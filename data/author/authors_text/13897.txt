An Algorithm for Anaphora Resolution in 
Spanish Texts 
Manuel Palomar* 
University of Alicante 
Lidia Moreno t
Valencia University of Technology 
Jesfis Peral* 
University of Alicante 
Rafael Mufioz* 
University of Alicante 
Antonio Ferr~indez* 
University of Alicante 
Patricio Martinez-Barco* 
University of Alicante 
Maximiliano Saiz-Noeda* 
University of Alicante 
This paper presents an algorithm for identifying noun phrase antecedents ofthird person personal 
pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) 
in unrestricted Spanish texts. We define a list of constraints and preferences for different ypes 
of pronominal expressions, and we document in detail the importance of each kind of knowledge 
(lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper 
also provides a definition for syntactic onditions on Spanish NP-pronoun oncoreference using 
partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved 
a success rate of 76.8%. We have also implemented four competitive algorithms and tested their 
performance in a blind evaluation on the same test corpus. This new approach could easily be 
extended to other languages uch as English, Portuguese, Italian, or Japanese. 
1. Introduction 
We present an algorithm for identifying noun phrase antecedents of personal pro- 
nouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pro- 
nouns) in Spanish. The algorithm identifies both intrasentential and intersentential 
antecedents and is applied to the syntactic analysis generated by the slot unifica- 
t ion parser (SUP) (Ferr~ndez, Palomar, and Moreno 1998b). It also combines different 
forms of knowledge by distinguishing between constraints and preferences. Whereas 
constraints are used as combinations of several kinds of knowledge (lexical, mor- 
phological, and syntactic), preferences are defined as a combination of heuristic rules 
extracted from a study of different corpora. 
We present he following main contributions in this paper: 
? an algorithm for anaphora resolution in Spanish texts that uses different 
kinds of knowledge 
* Department of Software and Computing Systems, Alicante, Spain. E-mail: (Palomar) 
mpalomar@dlsi.ua.es, (F rr~ndez) antonio@dlsi.ua.es, (Martfnez-Barco) patricio@dlsi.ua.es, (Peral) 
jperal@dlsi.ua.es, (Saiz-Noeda) max@dlsi.ua.es, (Mufioz) rafael@dlsi.ua.es 
t Department of Information Systems and Computation, Valencia, Spain. E-mail: hnoreno@dsic.upv.es 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
? an exhaustive study of the importance of each kind of knowledge in 
Spanish anaphora resolution 
? a proposal concerning syntactic onditions on NP-pronoun 
noncoreference in Spanish that can be evaluated on a partial parse tree 
? a proposal regarding preferences that are appropriate for resolving 
anaphora in Spanish and that could easily be extended to other 
languages 
? a blind test of the algorithm 
? a comparison with other approaches to anaphora resolution that we have 
applied to Spanish texts using the same blind test 
In Section 2, we show the classification scheme we used to identify the different ypes 
of anaphora that we would be resolving. In Section 3, we present he algorithm and 
discuss its main properties. In Section 4, we evaluate the algorithm. In Section 5, we 
compare our algorithm with several other approaches to anaphora resolution. Finally, 
we present our conclusions. 
2. Our Classification Scheme for Pronominal Expressions in Spanish 
In this section, we present our classification scheme for identifying the different ypes 
of anaphora that we will be resolving. Personal pronouns (PPR), demonstrative pro- 
nouns (DPR), reflexive pronouns (RPR), and omitted pronouns (OPa) are some of the 
most frequent ypes of anaphoric expressions found in Spanish and are the main 
subject of this study. Personal and demonstrative pronouns are further classified ac- 
cording to whether they appear within a prepositional phrase (PP) or whether they 
are complement personal pronouns (clitic pronouns1). We present examples for each 
of the four types of common anaphora. Each example is presented in three forms: as a 
Spanish sentence, as a word-to-word translation into English, and correctly translated 
into English. 2
2.1 Clitic Personal Pronouns (CPPR) 
In the case of clitic personal pronouns, I0, la, le 'him, her, it' and los, las, les 'them', we 
consider that the third person personal pronoun plays the role of the complement. 
(1) Ana abre \[la verja\]i y lai cierra tras de si. 
Ana opens \[the gate\]/ and it/ closes after herself 
'Ana opens the gate and closes it after herself.' 
2.2 Personal Pronouns Not Included in a PP (PPanotPP) 
We include in this class the personal pronouns ~l, ella, ello 'he, she, it' and ellas, ellos 
'they'. 
(2) Andr6si es mi vecino, t~li vive en el segundo piso. 
Andr6si is my neighbor Hei lives on the second floor 
'Andr6s is my neighbor. He lives on the second floor.' 
1 According to Mathews (1997), aclitic pronoun is a pronoun that is treated as an independent word in 
syntax but that forms a phonological unit with the verb that precedes or follows it. 
2 Coindexing indicates coreference b tween anaphor and antecedent. 
546 
Palomar et al Anaphora Resolution in Spanish Texts 
2.3 Personal Pronouns Included in a PP (PPRinPP) 
We include in this class the personal pronouns dl, ella, ello 'him, her, it' and ellas, ellos 
'them'. 
(3) Juan/ debe asistir pero Pedro lo har~i por 61i. 
Juani must attend but Pedro it will do for himi 
'Juan must attend but Pedro will do it for him.' 
2.4 Demonstrative Pronouns Not Included in a PP (DPRnotPP) 
We include in this class the demonstrative pronouns ~ste, dsta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqu~l, aqu~lla 'that'; and dsos, dsas, aqu~llos, aqudllas 'those'. 
(4) E1 Ferrarii gan6 al Ford. t~stei es el mejor. 
the Ferrarii beat the Ford This/ is the best 
'The Ferrari beat the Ford. This is the best.' 
2.5 Demonstrative Pronouns Included in a PP (DPRinPP) 
We include in this class the demonstrative pronouns ~ste, ~sta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqudl, aqudlla 'that'; and dsos, ~sas, aqu~llos, aqudllas 'those'. 
(5) Ana vive con Pacoi y cocina para 6stei diariamente. 
Ana lives with Pacoi and cooks for this/ every day 
'Ana lives with Paco and cooks for him every day.' 
2.6 Reflexive Pronouns (RPR) 
We include in this class the reflexive pronouns e, sL si mismo 'himself, herself, itself' 
and consigo, consigo mismo 'themselves'. 
(6) Anai abre la verja y la cierra tras de sfi. 
Anai opens the gate and it closes after herself/ 
'Ana opens the gate and closes it after herself.' 
2.7 Omitted Pronouns (Zero Pronouns OPa) 
The omitted pronoun is the most frequent ype of anaphoric expression in Spanish, as 
we will show in Section 4.2. Omitted pronouns occur when the pronominal subject is 
omitted. This kind of pronoun also occurs in other languages, such as Portuguese or 
Japanese; in these languages, it can also appear in object position, whereas in Spanish 
or Italian, it can appear only in subject position. In the following example, the omission 
is represented by the symbol 13 (the symbol does not appear in the correct ranslation 
into English). 
(7) Anai abre la verja y (~i la cierra tras de sf. 
Anai opens the gate and Oi it closes after herself 
'Ana opens the gate and she closes it after herself.' 
3. Anaphora Resolution Algorithm 
In the algorithm, all the types of anaphora are identified from left to right as they 
appear in the sentence. The most important proposals for anaphora resolution--such 
as those of Baldwin (1997), Lappin and Leass (1994), Hobbs (1978), or Kennedy and 
Boguraev (1996)--are based on a separation between constraints and preferences. 
547 
Computational Linguistics Volume 27, Number 4 
Constraints discard some of the candidates, whereas preferences simply sort the re- 
maining candidates. A constraint defines a property that must be satisfied in order 
for any candidate to be considered as a possible solution of the anaphor. For example, 
pronominal anaphors and antecedents must agree in person, gender, and number. 3 
Otherwise, the candidate is discarded as a possible solution. A preference is a charac- 
teristic that is not always satisfied by the solution of an anaphor. The application of 
preferences usually involves the use of heuristic rules in order to obtain a ranked list 
of candidates. 
Each type of anaphora has its own set of constraints and preferences, although 
they all follow the same general algorithm: constraints are applied first, followed by 
preferences. 
Based on the preceding description, our algorithm contains the following main 
components: 
? identification of the type of pronoun 
? constraints 
- -  morphological greement (person, gender, and number) 
- -  syntactic onditions on NP-pronoun oncoreference 
? preferences 
In order to apply this algorithm to unrestricted texts, it has been necessary to use 
partial parsing. In our partial-parsing scheme, as presented in Ferr~ndez, Palomar, and 
Moreno (1999), we only parse coordinated NPs and PPs, verbal chunks, pronouns, and 
what we have called free conjunctions (i.e., conjunctions that do not join coordinated 
NPs or PPs). Words that do not appear within these constituents are simply ignored. 
The NP constituents include coordinated adjectives, relative clauses, coordinated PPs, 
and appositives as modifiers. 
With this partial-parsing scheme, we divide a sentence into clauses by parsing first 
the free conjunction and then the verbs, as in the following example: 
(8) Pedro compr6 un regalo y se lo dio a Ana. 
Pedro bought a gift and her it gave to Ana 
'Pedro bought a gift and gave it to Ana.' 
In this example, we have parsed the following constituents: np(Pedro), v(comprO), np(un 
regalo),freeconj(y), pron(se), pron(lo), v(dio), pp(a Ana). We are able to divide this sentence 
into two clauses because it contains the free conjunction y 'and' and the two verbs 
compr6 'bought' and clio 'gave'. 
3.1 Identification of the Kind of Pronoun 
The algorithm uses partial-parse trees to automatically identify omitted pronouns by 
employing the following steps: 
? The sentence is divided into clauses (by parsing the free conjunction 
followed by the verbs). 
3 In our implementation, thismorphological information is extracted from the part-of-speech tagger. 
548 
Palomar et al Anaphora Resolution in Spanish Texts 
An NP or pronoun is sought for each clause by analyzing the clause 
constituents on the left-hand side of the verb, unless the verb is 
imperative or impersonal. The chosen NP or pronoun must agree in 
person and number with the clausal verb. (In evaluating this algorithm, 
Ferr~ndez and Peral \[2000\] achieved a success rate of 88% for detecting 
omitted pronouns.) 
The remaining pronouns are identified based on part-of-speech (POS) tagger out- 
puts. 
3.2 Morphological Agreement 
Person, gender, and number agreement are checked in order to discard potential an- 
tecedents. For example, in the sentence 
(9) Juanj vio a Rosa/. Ella/ estaba muy feliz. 
Juanj saw to Rosa/ Shei was very happy 
'Juan saw Rosa. She was very happy.' 
there are two possible antecedents for ella 'she', whose slot structures 4 are 
np (conc (sing, masc), X, Juan) 
np (conc (sing, fem), Y, Rosa) 
whereas the slot structure of the pronoun is 
pron (conc (sing, fem), Z, ella). 
In order to decide between the two antecedents, he unification of both slot struc- 
tures (pronoun and candidate) is carried out by the slot unification parser (Ferr~ndez, 
Palomar, and Moreno 1999). In this example, the candidate Juan is rejected by this 
morphological greement constraint. 
3.3 Syntactic Conditions on NP-Pronoun Noncoreference 
These conditions are based on c-command and minimal-governing-category constraints 
as formulated by Reinhart (1983) and on the noncoreference conditions of Lappin and 
Leass (1994). They are of great importance in any anaphora resolution system that 
does not use semantic information, as is the case with our proposal. In such systems, 
recency is important in selecting the antecedent of an anaphor. That is to say, the 
closest NP to the anaphor has a better chance of being selected as the solution. One 
problem, however, is that such constraints are formulated using full parsing, whereas 
if we want to work with unrestricted texts we should be using partial parsing, as 
previously defined. 
We have therefore proposed a set of noncoreference onditions for Spanish, using 
partial parsing, although they could easily be extended to other languages such as En- 
glish. In our system, the following types of pronouns are noncoreferential with a noun 
phrase (NP) under the conditions noted (noncoindexing indicates that a candidate is
rejected by these conditions). 
4 The term slot structure is defined in Ferr~ndez, Palomar, and Moreno (1998b). The slot structure stores 
morphological and syntactic information related to the different constituents of a sentence. 
549 
Computational Linguistics Volume 27, Number 4 
. 
(a) 
(b) 
(c) 
. 
(a) 
Reflexive pronouns are noncoreferential when: 
(b) 
(10) 
the NP is included in another constituent (e.g., the NP is 
included in a PP) 
Ante Luisj sei frot6 con la toalla. 
in front of Luisj himself/ rubbed with the towel 
'He rubbed himself with the towel in front of Luis.' 
In this sentence, we would have obtained the following sequence 
of constituents after our partial-parsing scheme: pp(prep(ante), 
np(Luis )) , pron(se) , v(frot6 ) , pp(prep( con) , np(la toalla) . Following 
the above-stated condition, the NP Luis cannot corefer with the 
reflexive pronoun se since Luis is included in a PP (ante Luis). 
the NP is in a different clause or sentence 
(11) Anaj trajo un cuchillo y Eva/ sei cort6. 
Anaj brought a knife and Eva/ herself/ cut 
'Ana brought a knife and Eva cut herself.' 
the NP appears after the verb and there is another NP in the 
same clause before the verb 
(12) 
(13) 
Juan/ sei cort6 con el cuchilloj. 
Juan/ himself/ cut with the knifej 
'Juan cut himself with the knife.' 
Under these conditions, coreference is allowed between the NP 
and the reflexive pronoun, since both are in the same clause. For 
example: 
Juan/ queria verlo por s~ mismoi. 
Juan/ wanted see it for himself/ 
'Juan wanted to see it for himself.' 
In this example, Juan and the reflexive pronoun si mismo 
'himself' corefer since Juan is in the same clause as the anaphor, 
it is not included in another constituent, and it appears before 
the verb. 
Clitic pronouns are noncoreferential when: 
the NP is included in a PP (except hose headed by the 
preposition a 'to') 
(14) Con Juan/ loj compr6. 
with Juan/ itj bought 
'I bought it with Juan.' 
the NP is located more than three constituents before the clitic 
pronoun in the same clause 
(15) En casai \[el martillo\]j no se loj di. 
at home/ \[the hammer\]j not him itj gave 
'I didn't give him the hammer at home.' 
550 
Palomar et al Anaphora Resolution in Spanish Texts 
. 
(a) 
(17) 
(b) 
In this example, the direct object el martillo 'the hammer '  has 
been moved from its common position after the verb, and it is 
necessary to fill the resulting gap with the pronoun lo 'it' even 
though it does not appear in the English translation. This 
phenomenon 5 can be considered an exception to the c-command 
constraints as formulated by Reinhart when applied to Spanish 
clitic pronouns. 
Moreover, if the last two conditions are not fulfilled by the NP and the 
verb is in the first or second person, then this NP will necessarily be the 
solution of the pronoun: 
(16) \[El boligrafo\]i 1Oi comprar~s en esa tienda. 
\[The pen\]/ iti will buy in that shop 
'You will buy the pen in that shop.' 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is in the same clause as the anaphor, and: 
the pronoun comes before the verb (in full parsing, this would 
mean that it is the subject of its clause) 
Ante Luisi 61j salud6 a Pedrok. 
in front of Luisi hey greeted to Pedrok 
'He greeted Pedro in front of Luis.' 
the pronoun comes after the verb (in full parsing, this would 
mean that it is the object of the verb) and the NP is not included 
in another NP 
(18) \[El padre de Juanj\]i le venci6 a 41j. 
\[Juanj's father\]/ him beat to himj 
'Juan's father beat him.' 
In this example, the pronoun ~I 'him' cannot corefer with the NP 
el padre de Juan 'Juan's father', but it can corefer with Juan since it 
is a modifier of the NP el padre de Juan. 
It should be mentioned that the clitic pronoun le is another 
form of the pronoun dl 'him'. This is a typical phenomenon i
Spanish, where clitic pronouns occupy the object position. 
Sometimes both the clitic pronoun and the object appear in the 
same clause, as occurs in the previous example and in the 
following one: 
(19) A Pedro/ yo lei vi ayer. 
to Pedroj I himi saw yesterday 
'I saw Pedro yesterday.' 
This example also illustrates the previously mentioned exception 
of c-command constraints for Spanish clitic pronouns. In this 
case, the direct object a Pedro 'to Pedro' has been moved before 
the verb, and the clitic pronoun le 'him' has been added. It 
should also be remarked that, as noted earlier, the clitic pronoun 
does not appear in the English translation. 
5 Mathews (1997) calls this phenomenon "clitic doubling" and defines it as the use of a clitic pronoun 
with the same referent and in the same syntactic function as another element in the same clause. 
551 
Computational Linguistics Volume 27, Number 4 
(c) the pronoun is included in a PP that is not included in another 
constituent and the NP is not included in another constituent 
(NP or PP) 
(20) \[El padre de Luisj\]i juega con 61j. 
\[Luisj's father\]/ plays with himj 
'Luis's father plays with him.' 
In this example, the pronoun ~I 'him' is included in a PP (which 
is not included in another constituent) and the NP el padre de 
Luis is not included in another NP or PP. Therefore, the NP 
cannot corefer with the pronoun. However, the NP Luis can 
corefer because it is included in the NP el padre de Luis. 
(d) the pronoun is included in an NP, so that the NP in which the 
pronoun is included cannot corefer with the pronoun 
(21) Pedro/ vio \[al hermano de ~li\] j. 
Pedro/ saw \[the brother of himi\]j 
'Pedro saw his brother.' 
(e) the pronoun is coordinated with other NPs, so that the other 
coordinated NPs cannot corefer with the pronoun 
(22) Juan/, \[el tio de Ana\]j, y 61k fueron de pesca. 
Juan/, \[Ana's uncle\]j, and hek went fishing 
'He, Juan, and Ana's uncle went fishing.' 
(f) the pronoun is included in a relative clause, and the following 
condition is met: 
. 
(24) 
i. the NP in which the relative clause is included does not 
corefer with the pronoun 
(23) Pedroj vio a \[un amigo que juega con 41j\]i. 
Pedroj saw to \[a friend that plays with himj\]i 
'Pedro saw a friend that he plays with.' 
ii. the NPs that are included in the relative clause follow 
the previous conditions 
iii. the remaining NPs outside the relative clause could 
corefer with the pronoun 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is not in the same clause as the pronoun. (In this case, the 
NP can corefer with the pronoun, except when this NP also appears in 
the same sentence and clause as the pronoun, in which case it will have 
been discarded by the previous noncoreference onditions.) 
Anaj y Evai son amigas. Evai lej ayuda mucho. 
Anay and Evai are friends Evai herj helps a lot 
'Ana and Eva are friends. Eva helps her a lot.' 
It is important o note that the above-mentioned conditions refer to those coor- 
dinated NPs and PPs that have been partially parsed. Moreover, as previously men- 
tioned, NPs can include relative clauses, appositives, coordinated PPs, and adjectives. 
552 
Palomar et al Anaphora Resolution in Spanish Texts 
We should also remark that we consider aconstituent A to be included in a constituent 
B if A modifies the head of B. Let us consider the following NP: 
(25) \[el hombre que ama a \[una mujer que lei ama\]j\]i 
\[the man who loves to \[a woman who him/ loveslj\]i 
'the man who loves a woman who loves him.' 
We consider that the pronoun le 'him' is included in the relative clause that mod- 
ifies the NP una mujer que le ama 'a woman who loves him', which then cannot corefer 
with it due to noncoreference ondition 3(f)i. Under condition 3(f)iii, however, the 
pronoun le 'him' could corefer with the entire NP el hombre que area a una mujer que le 
area 'the man who loves a woman who loves him'. 
Another example might be the following: 
(26) Eva/ tiene \[un tio que lei toma el pelo\]j. 
Evai has \[an uncle that heri teases\]j 
'Eva has an uncle who teases her.' 
In this example, the pronoun is included within the relative clause that modifies un 
tio 'an uncle', and therefore cannot corefer with it. But, following condition 3(f)iii, it 
can corefer with Eva. 
3.4 Preferences 
To obtain the different sets of preferences, we utilized the training corpus to identify 
the importance of each kind of knowledge that is used by humans when tracking 
down the NP antecedent of a pronoun. Our results are shown in Table 1. For our 
analysis, the antecedents for each pronoun in the text were identified, along with their 
configurational characteristics with reference to the pronoun. Thus, the table shows 
how often each configurational characteristic is valid for the solution of a particular 
pronoun. For example, the solution of a reflexive pronoun is a proper noun 53% of the 
time. The total number of pronoun occurrences in the study was 575. Thus, we were 
able to define the different patterns of Spanish pronoun resolution and apply them in 
order to obtain the evaluation results that are presented in this paper. The order of 
importance was determined by first sorting the preferences according to the percentage 
of each configurational characteristic; that is, preferences with higher percentages were 
applied before those with lower percentages. After several experiments on the training 
corpus, an optimal order--the one that produced the best performance--was obtained. 
Since in this evaluation phase we processed texts from different genres and by different 
authors, we can state that the final set of preferences obtained and their order of 
application can be used with confidence on any Spanish text. 
Based on the results presented in Table 1, we have extracted a set of preferences for 
each type of anaphora (listed below). We have distinguished between those pronouns 
that are included within PPs and those that are not. That is because when a pronoun 
is included in a PP, the preposition of this PP sets a preference. 
Preferences of omitted pronouns (OPR): 
1. NPs that are not of time, direction, quantity, or abstract type; that is to 
say, inanimate candidates are rejected (e.g., hal~past en, Market Street, 
three pounds, or a thing) 
2. NPs in the same sentence as the omitted pronotm 
553 
Computat ional  Linguistics Volume 27, Number  4 
Table 1 
Percentage validity of types of pronouns for different configuration characteristics of the 
training corpus (n = 575). 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Intrasentential 66 97 57 70 100 60 75 
Intersentential 34 3 43 30 0 40 25 
NPSentAnt ~ 9 3 4 16 50 9 38 
AntPPin b 7 9 14 27 50 20 25 
AntProper c 57 53 63 35 0 43 0 
AntIndef a 13 0 7 0 0 6 13 
AntRepeaff 72 66 79 65 50 71 50 
AntWithVerb f 14 94 20 24 0 26 25 
EqualPP g 100 100 100 78 100 97 100 
EqualPosVerb h 79 84 89 46 0 86 38 
BeforeVerb i 83 91 89 65 50 86 13 
NoTime d 100 100 100 100 100 100 100 
NoQuant i ty  k 100 100 100 100 100 97 100 
NoDirect ion I 100 100 100 97 100 100 100 
NoAbstract m 100 100 100 100 100 100 100 
NoCompany n 100 100 100 100 100 100 100 
a If the NP 
b If the NP 
c If the NP 
d If the NP 
e If the NP 
f If the NP 
g If the NP 
h If the NP 
i If the NP 
j If the NP 
k If the NP 
1 If the NP 
m If the NP 
n If the NP 
is included in another NP 
is included in a PP with the preposition en 'in' 
is a proper noun 
is an indefinite NP 
has been repeated more than once in the text 
has appeared with the verb of the anaphor more than once in the text 
has appeared in a PP more than once in the text 
occupies the same position with reference to the verb as the anaphor (before or after) 
appears before its verb 
is not a time-type 
is not a quantity-type 
is not a direction-type 
is not an abstract-type 
is not a company-type 
3. NPs  that  are in the same sentence  as the  anaphor  and  are also the  
so lu t ion  for  another  omi t ted  pronotm 
4. NPs  that  are in the prev ious  sentence  
5. NPs  that  are not  inc luded  in another  NP  (e.g., when they  appear  ins ide  
a re la t ive  c lause  or  appos i t i ve )  
6. NPs  that  are not  inc luded  in a PP or  are  inc luded  in a PP  when its 
p repos i t ion  is a ' to '  or  de ' o f '  
7. NPs  that  appear  be fore  the  verb  
8. NPs  that  have  been  repeated  more  than  once  in the  text  
Preferences of clitic personal pronouns (CPPR): 
1. NPs  that  are not  of  t ime,  d i rect ion ,  quant i ty ,  or  abst ract  type  
2. NPs  that  are in  the  same sentence  as the  anaphor  
554 
Palomar et al Anaphora Resolution in Spanish Texts 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a clause or appositive) 
5. NPs that are not included in a PP or are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. NPs that have appeared with the verb of the anaphor more than once 
Preferences of personal and demonstrative pronouns that are included in a PP 
(PPRinPP and DPRinPP): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that have been repeated more than once in the text 
6. NPs that are included in a PP 
7. NPs that occupy the same position (before or after) with respect o the 
verb as the anaphor 
Preferences of personal and demonstrative pronouns that are not included in a PP 
and of reflexive pronouns (PPRnotPP, DPRnotPP, and RPR): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that are not included in a PP or that are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. For the case of personal pronouns (PPRnotPP), NPs that are not 
included in a PP with the preposit ion en ' in' 
7. NPs that appear before their verbs (i.e., the verb of the sentence in 
which the NP appears) 
3.5 Resolution Procedure 
The resolution procedure consists of the following steps: 
1. Identify the type of anaphora: pronominal  (PPRinPP or PPRnotPP), 
demonstrat ive (DPRinPP or DPRnotPP), reflexive (RPR), or omitted 
(oPR). 
555 
Computational Linguistics Volume 27, Number 4 
2. Identify the NP candidate antecedents of a pronoun in order to create a 
list L. The list created will depend on the type of anaphor and the 
anaphoric accessibility space (empirically obtained from a deep study 
of the training corpus) and will be developed according to the 
following criteria: 
? For pronominal anaphora, demonstrative anaphora, and 
omitted pronouns, NP candidates will appear in the same 
sentence as the anaphor and in the four previous sentences. 
? For reflexive anaphora, NP candidates will appear in the same 
sentence as the anaphor. 
3. Apply constraints to L to obtain LI: 
(a) morphological agreement 
(b) syntactic onditions on NP-pronoun noncoreference 
4. If the number of elements of L1 - 1, then the solution is that element. 
5. If the number of elements of L1 = 0, then the solution is an exophor. 
6. If the number of elements of L1 > 1, then apply preferences to L1 to 
obtain L2. Depending on the type of anaphora, a different set and order 
of preferences will be applied (see Section 3.4). 
7. If the number of elements of L2 = 1, then the solution is that element. 
8. If the number of elements of L2 > 1, then apply the following three 
basic preferences in the order shown until only one candidate remains 
(these three preferences are common to all the pronouns): 
? NPs most repeated in the text 
? NPs that have appeared most with the verb of the anaphor 
? the first candidate of the remaining list (the closest one to the 
anaphor) 
After applying these basic preferences, the antecedent is obtained. 
4. Empirical Evaluation 
4.1 Description of Corpora 
We have tested the algorithm on both technical manuals and literary texts. In the first 
instance, we used a portion of the Spanish edition of the Blue Book corpus. 6 This 
corpus contains the handbook of the International Telecommunications Union CCITT, 
published in English, French, and Spanish; it is one of the most important collections of 
telecommunications texts available and contains 5,000,000 words automatically tagged 
by the Xerox tagger. In the second instance, the algorithm was tested on Lexesp, a 
corpus 7 that contains Spanish literary texts from different genres and by different 
6 CRATER (Proyecto CRATER 1994-1995) Corpus Resources and Terminology Extraction Project. Project 
supported by the European Community Commission (DG-XIII). Computational Linguistics Laboratory, 
Faculty of Philosophy and Fine Arts, Autonomous University of Madrid, Spain. 
7 The Lexesp corpus belongs to the project of the same name carried out by the Psychology Department 
of the University of Oviedo and developed by the Computational Linguistics Group of the University 
of Barcelona, with the collaboration fthe Language Processing Group of the Catalonia University of 
Technology, Spain. 
556 
Palomar et al Anaphora Resolution in Spanish Texts 
Table 2 
Pronoun occurrences in two types of texts. 
Total BB Corpus Lexesp Corpus 
Number of pronoun occurrences 
in the training corpus 575 123 
Number of pronoun occurrences 
in the test corpus 1,677 375 
452 
1,302 
authors. These texts were mainly obtained from newspapers and were automatically 
tagged by a different agger than the one used to tag the Blue Book. The portion of 
the Lexesp corpus that we processed contained various stories, related by a narrator, 
and written by different authors. As was the case for the Blue Book corpus, this 
corpus also contained 5,000,000 words. Since we worked on texts from different genres 
and by different authors, the applicability of our proposal to other kinds of texts is 
assured. 
We selected a subset of the Blue Book corpus and another subset of the Lex- 
esp corpus, and both were annotated with respect o coreference. One portion of the 
coreferentially tagged corpus (training corpus) was used for improving the rules for 
anaphora resolution (constraints and preferences), and another portion was reserved 
for test data (Table 2). 
The annotation phase was accomplished in the following manner: (1) two annota- 
tors were selected, (2) an agreement was reached between the annotators with regard 
to the annotation scheme, (3) each annotator annotated the corpus, and, finally, (4) a 
reliability test (Carletta et al 1997) was done on the annotation in order to guaran- 
tee the results. The reliability test used the kappa statistic that measures agreement 
between the annotations of two annotators in making judgments about categories. In 
this way, the annotation is considered a classification task consisting of defining an ad- 
equate solution among the candidate list. According to Vieira (1998), the classification 
task when tagging anaphora resolution can be reduced to a decision about whether 
each candidate is the solution or not. Thus, two different categories are considered 
for each anaphor: one for the correct antecedent and another for nonantecedents. Our 
experimentation showed one correct antecedent among an average of 14.5 possible 
candidates per anaphor after applying constraints. For computing the kappa statistic 
(k), see Siegel and Castellan (1988). 
According to Carletta et al (1997), a k measurement such as 0.68 < k < 0.8 allows 
us to draw encouraging conclusions, and a measurement k > 0.8 means there is to- 
tal reliability between the results of the two annotators. In our tests, we obtained a 
kappa measurement of k = 0.81. We therefore consider the annotation obtained for the 
evaluation to be totally reliable. 
4.2 Experimental Work 
We conducted a blind test over the entire test corpus of unrestricted Spanish texts by 
applying the algorithm to the partial syntactic structure generated by the slot unifica- 
tion parser. 
Over these corpora, our algorithm attained a success rate for anaphora resolution 
of 76.8%. We define "success rate" as the number of pronouns successfully resolved, 
divided by the total number of resolved pronouns. The total number of resolved pro- 
nouns was 1,677, including personal, demonstrative, reflexive, and omitted pronouns. 
557 
Computational Linguistics Volume 27, Number 4 
Table 3 
Results of blind test. 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP Total 
Num. of 
pronoun 
occurrences 228 80 1,099 107 20 94 49 1,677 
Num. of 
cases 
correctly 162 74 868 70 17 64 34 1,289 
resolved 
Success 
rate 71.0% 92.5% 78.9% 65.4% 85.0% 68.0% 69.3% 76.8% 
All of them were in the third person, with a noun phrase that appeared before the 
anaphor as their antecedent. Our algorithm's "recall percentage," defined as the num- 
ber of pronouns correctly resolved, divided by the total number of pronouns in the 
text, was therefore 76.8%. A breakdown of success rate results for each kind of pro- 
noun is also shown in Table 3. The pronouns were classified so as to provide the 
option of applying different kinds of knowledge to resolve each category of pronoun. 
One of the factors that affected the results was the complexity of the Lexesp corpus, 
due mainly to its complex narratives. On average, 16 words per sentence and 27 
candidates per anaphor were found in this corpus. 
In our experiment, a "successful resolution" occurred if the head of the solution 
offered by our algorithm was the same as that offered by two human experts. We 
adopted this definition of "success" because it allowed the system to be totally auto- 
matic: solutions given by the annotators were stored in a file and were later automat- 
ically compared with the solutions given by our system. Since semantic information 
was not used at all, PP attachments were not always correctly disambiguated. Hence, 
at times the differences imply corresponded to different subconstituents. 
After the evaluation process, we tested the results in order to identify the lim- 
itations of the algorithm with respect to the resolution process. We identified the 
following: 
? There were some mistakes in the POS tagging (causing an error rate of 
around 3%). 
? There were some mistakes in the partial parsing with respect o the 
identification of complex noun phrases (causing an error rate of around 
7%) (Palomar et al 1999). 
? Semantic information was not considered (causing an error rate of 
around 32%). An example of this type of error can be seen in the 
following text extracted from the Lexesp corpus: 
(27) Recuerdo, pot ejemplo, \[un pequefio claro en un bosque en 
medio de las montafias canadienses\]i, con tres lagunas diminutas 
que, a causa de los sedimentos del agua. tenfan distintos y chocantes 
colores. Esta rareza habia hecho del sitioi un espacio sagrado al que 
peregrinaron los indios durante siglos y seguramente antes los 
pobladores paleolfticos. Y eso se notaba. 
558 
Palomar et al Anaphora Resolution in Spanish Texts 
(28) 
Canad~i es un pals muy hermoso, y aqu41i no era, ni mucho 
rnenos, el lugar m~s bello: pero guardaba tranquilamente d ntro de sf 
toda su arrnonfa, como los melocotones guardan dentro de sf el duro 
hueso. 
'1 remember, for example, \[a small clearing in the woods in the 
middle of the Canadian mountains\]/, with three tiny lagoons that, 
due to the water sediments, had different and astonishing colors. 
This peculiarity had made the place/into a sacred site, to which the 
Indians made pilgrimages over the centuries, and surely even the 
Paleolithic Indians before them. And you could feel it. 
'Canada is a very beautiful country and that one/was by no 
means the most beautiful place: but it calmly kept within itself all of 
its harmony, like peaches that keep the hard seeds within.' 
In this text, the demonstrative pronoun aqudl 'that one' corefers with the 
antecedent un peque~o claro en un bosque n medio de las monta~as canadienses 
'a small clearing in the woods in the middle of the Canadian mountains', 
which is also linked to the definite noun phrase el sitio 'the place'. Our 
algorithm identified the proper noun Canadd, which is in the same 
sentence, as the anaphor, since the proper noun could only have been 
discarded by means of semantic information. 
As an example of an anaphor that was correctly resolved by the 
algorithm, we present he following sentence xtracted from the Blue 
Book corpus. In this case, the antecedent los sistemas de transmisidn 
analdgica 'the systems of analogue transmission' was correctly chosen for 
the personal pronoun ellos 'them': ' 
En las conexiones largas o de Iongitud media, es probable que la 
fuente principal de ruido de circuito estribe en \[los sistemas de 
transmisi6n anal6gica\]i, ya queen ellosi la potencia de ruido suele 
set proporcional  la Iongitud del circuito. 
'In long or medium connections, it is probable that the main source of 
circuit noise comes from \[the systems of analogue transmission\]/, 
since in them/the noise capacity is usually proportional to the length 
of the circuit.' 
The remainder of the errors were due to split antecedents (10%), 
cataphora (2%), exophora (3%), or exceptions in the application of 
preferences (43%). 
5. Comparison with Other Approaches to Anaphora Resolution 
5.1 Anaphora Resolution Approaches 
Common among all languages i the fact that the anaphora phenomenon requires im- 
ilar strategies for its resolution (e.g., pronouns or definite descriptions). All languages 
employ different kinds of knowledge, but their strategies differ only in the manner by 
which this knowledge is coordinated. For example, in some strategies just one kind 
of knowledge becomes the main selector for identifying the antecedent, with other 
kinds of knowledge being used merely to confirm or reject the proposed antecedent. 
In such cases, the typical kind of knowledge used as the selector is that of discourse 
structure. Centering theory, as employed by Strube and Hahn (1999) or Okumura and 
Tamura (1996), uses this type of approach. Other approaches, however, give equal 
559 
Computational Linguistics Volume 27, Number 4 
importance to each kind of knowledge and generally distinguish between constraints 
and preferences (Baldwin 1997; Lappin and Leass 1994; Carbonell and Brown 1988). 
Whereas constraints tend to be absolute and therefore discard possible antecedents, 
preferences tend to be relative and require the use of additional criteria (e.g., the use of 
heuristics that are not always satisfied by all antecedents). Nakaiwa and Shirai (1996) 
use this sort of resolution model, which involves the use of semantic and pragmatic 
constraints, such as constraints based on modal expressions, or constraints based on 
verbal semantic attributes or conjunctions. 
Our approach to anaphora resolution belongs in the latter category, since it com- 
bines different kinds of knowledge and no knowledge based on discourse structure 
is included. We choose to ignore discourse structure because obtaining this kind of 
knowledge requires not only an understanding of semantics but also knowledge about 
world affairs and the ability to almost perfectly parse any text under discussion (Az- 
zam, Humphreys, and Gaizauskas 1998). 
Still other approaches to anaphora resolution are based either on machine learn- 
ing techniques (Connolly, Burger, and Day 1994; Yamamoto and Sumita 1998; Paul, 
Yamamato, and Sumita 1999) or on the principles of uncertainty reasoning (Mitkov 
1995). 
Computational processing of semantic and domain information is relatively expen- 
sive when compared with other kinds of knowledge. Consequently, current anaphora 
resolution methods rely mainly on constraint and preference heuristics, which employ 
morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov 
\[1998\]). Such approaches have performed notably well. Lappin and Leass (1994) de- 
scribe an algorithm for pronominal anaphora resolution that achieves a high rate of 
correct analyses (85%). Their approach, however, operates almost exclusively on syn- 
tactic information. More recently, Kennedy and Boguraev (1996) proposed an algorithm 
for anaphora resolution that is actually a modified and extended version of the one 
developed by Lappin and Leass (1994). It works from the output of a POS tagger and 
achieves an accuracy rate of 75%. 
There are other approaches based on POS tagger outputs as well. For example, 
Mitkov and Stys (1997) propose a knowledge-poor approach to resolving pronouns 
in technical manuals in both English and Polish. The knowledge mployed in these 
approaches i limited to a small noun phrase grammar, a list of terms, and a set of 
antecedent indicators (definiteness, term preference, lexical reiteration, etc.). 
Still other approaches are based on statistical information, including the work of 
Dagan and Itai (1990, 1991) and Ge, Hale, and Charniak (1998), all of whom present a 
probabilistic model for pronoun resolution. 
We have adopted their ideas and adapted their algorithms to partial parsing and 
to Spanish texts in order to compare our results with their approaches. 
With reference to the differences between English and Spanish anaphora resolu- 
tion, we have made the following observations: 
Syntactic parallelism has played a more important role in English texts 
than in Spanish texts, since Spanish sentence structure is more flexible 
than English sentence structure. Spanish is a free-word-order language 
and has different syntactic onditions, which increases the difficulty of 
resolving Spanish pronouns (hence, the greater accuracy rate for English 
texts). 
? A greater number of possible antecedents was observed for Spanish 
pronouns than for English pronouns, due mainly to the greater average 
560 
Palomar et al Anaphora Resolution in Spanish Texts 
length of Spanish sentences (which also makes the resolution of Spanish 
pronouns more difficult). 
Spanish pronouns usually bear more morphological information. One 
result is that this constraint tends to discard more candidates in Spanish 
than in English. 
For comparison purposes, we implemented the following approaches on the same 
Spanish texts that were tested and described in Section 4.1. 
5.2 Hobbs's Algorithm 
Hobbs's algorithm (Hobbs 1978) is applied to the surface parse trees of sentences in 
a text. A surface parse tree represents the grammatical structure of a sentence. By 
reading the leaves of the parse tree from left to right, the original English sentence is
formed. The algorithm parses the tree in a predefined order and searches for a noun 
phrase of the correct gender and number. Hobbs tested his algorithm for the pronouns 
he, she, it, and they, using 100 examples taken from three different sources. Although 
the algorithm is very simple, it was successful 81.8% of the time. 
We implemented a version of Hobbs's algorithm for slot unification grammar for 
Spanish texts. Since full parsing was not done, our specifications for the algorithm 
were adjusted, as follows: 
? NPs were tested from left to right, as they were parsed in the sentence. 
? Afterward, the NPs that were included in an NP (breadth-first) were 
tested. 
? This test was interrupted when an NP agreed in gender and number 
with the anaphor. 
The problems we encountered in implementing Hobbs's algorithm are similar to 
those found in implementing other approaches: the adaptation to partial parsing, and 
the inherent difficulty of the Spanish language (i.e., its free-word-order characteristics). 
The results of our test of this version of Hobbs's algorithm on the test corpus 
appear in Table 4. 
5.3 Approaches Based on Constraints and Proximity Preference 
Our approach as also been compared with the typical baseline approach consisting of 
constraints and proximity preference; that is, the antecedent that appears closest o the 
anaphor is chosen from among those that satisfy the constraints. For this comparison, 
the same constraints that were used previously (i.e., morphological greement and 
syntactic onditions) were applied here. Then the antecedent a the head of the list of 
antecedents was proposed as the solution of the anaphor. These results are also listed 
in Table 4. As can be seen from the table, success rates were lower than those obtained 
through the joint application of all the preferences. 
5.4 Lappin and Leass's Algorithm 
An algorithm for identifying the noun phrase antecedents of third person pronouns 
and lexical anaphors (reflexive and reciprocal) is presented in Lappin and Leass (1994); 
this algorithm has exhibited a high rate (85%) of correct analyses in English texts. It 
relies on measures of salience that are derived from syntactic structures and on simple 
dynamic models of attentional state to select he antecedent oun phrase of a pronoun 
from a list of candidates. 
561 
Computational Linguistics Volume 27, Number 4 
We have implemented a version of Lappin and Leass's algorithm for Spanish texts. 
The original formulation of the algorithm proposes a syntactic filter on NP-pronoun 
coreference. This filter consists of six conditions for NP-pronoun oncoreference within 
any sentence (Lappin and Leass 1994, page 537). In applying this algorithm to Span- 
ish texts, we changed these conditions o as to capture the appropriate context. As 
mentioned previously, our algorithm does not have access to full syntactic knowledge. 
Accordingly, we employed partial parsing over the text in our application of Lappin 
and Leass's algorithm. The salience parameters were weighted (weight appears in 
parentheses) and applied in the following way: 
? Sentence recency (100): Applied when the NP appeared in the same 
sentence as the anaphor. 
? Subject emphasis (80): Applied when the NP was located before the 
verb of the clause in which it appeared. This heuristic was necessary 
because of our algorithm's lack of syntactic knowledge. It should be 
noted, however, that since Spanish is a nearly free-word-order language 
and the exchange of subject and object positions within Spanish 
sentences i common, the heuristic is often invalid. For example, the two 
Spanish sentences Pedro compr6 un regalo 'Pedro bought a present' and Un 
regalo compr6 Pedro 'A present bought Pedro' are equivalent to one 
another and to the English sentence Pedro bought a present. 
? Existential emphasis (70): In this instance, we applied the parameter in 
the same way as Lappin and Leass, since the entire NP was fully parsed, 
which allowed us to tell when it was a definite or an indefinite NP. 
? Accusative emphasis (50): Applied when the NP appeared after the verb 
of the clause in which it appeared and the NP did not appear inside 
another NP or PP. For example, in the sentence Pedro encontr6 el libro de 
Juana 'Pedro found Juana's book', a value was assigned to el libro de Juana 
'Juana's book' but not to Juana. Once again, it should be noted that this 
heuristic was necessary because of our algorithm's lack of syntactic 
knowledge. 
? Indirect object and oblique complement emphasis (40): Applied when 
the NP appeared in a PP with the Spanish preposition a 'to', which 
usually preceded the indirect object of its sentence. 
? Head noun emphasis (80): Applied when the NP was not contained in 
another NP. 
? Nonadverbial emphasis (50): Applied when the NP was not contained 
in an adverbial PP. In this case, its application depended on the kind of 
preposition in which the NP was included. 
? Parallelism reward (35): Applied when the NP occupied the same 
position as the anaphor with reference to the verb of the sentence (before 
or after the verb). 
Finally, we followed Lappin and Leass in assigning the additional salience value 
to NPs in the current sentence and in degrading the salience of NPs in preceding 
sentences. 
Our results exhibited some similarities with Lappin and Leass's experiments. 
For example, anaphora was strongly preferred over cataphora, and both approaches 
562 
Palomar et al Anaphora Resolution in Spanish Texts 
preferred intrasentential NPs to intersentential ones. These results can be seen in 
Table 4. 
5.5 Centering Approach 
The centering model proposed by Grosz, Joshi, and Weinstein (1983, 1995) provides 
a framework for modeling the local coherence of discourse. The model has two con- 
structs, a list of forward-looking centers and a backward-looking center, that can be 
assigned to each utterance Ui. The list of forward-looking centers Cf(Ui) ranks dis- 
course entities within the utterance Ui. The backward-looking center Cb(Ui+l) con- 
stitutes the most highly ranked element of Cf(Ui) that is finally realized in the next 
utterance Ui+l. In this way, the ranking imposed over Cf(Ui) must reflect he fact that 
the preferred center Cp(U/) (i.e., the most highly ranked element of Cf(Ui)) is most 
likely to be Cb(Ui+l). 
The ranking criteria used by Grosz, Joshi, and Weinstein (1995) order items in 
the Cf list using grammatical roles. Thus, entities with a subject role are preferred to 
entities with an object role, and objects are preferred to others (adjuncts, etc.). 
Grosz, Joshi, and Weinstein (1995) state that if any element of Cf(Ui) is realized 
by a pronoun in Ui+l, then Cb(Ui+l) must also be realized by a pronoun. 
Brennan, Friedman, and Pollard (1987) applied the centering model to pronoun 
resolution. They based their algorithm on the fact that centering transition relations 
will hold across adjacent utterances. 
Moreover, one crucial point in centering is the ranking of the forward-looking 
centers. Grosz, Joshi, and Weinstein (1995) state that Cf may be ordered using different 
factors, but they only use information about grammatical roles. However, both Strube 
(1998) and Strube and Hahn (1999) point out that it is difficult to define grammatical 
roles in free-word-order languages like German or Spanish. For languages like these, 
they propose other anking criteria dependent upon the information status of discourse 
entities. They claim that information about familiarity is crucial for the ranking of 
discourse ntities, at least in free-word-order languages. 
According to Strube's ranking criteria, two different sets of expressions, hearer- 
old discourse ntities (OLD) and hearer-new discourse ntities (NEW), can be distin- 
guished. OLD discourse ntities consist of evoked entities---coreferring resolved ex- 
pressions (pronominal and nominal anaphora, previously mentioned proper names, 
relative pronouns, appositives)--and unused entities (proper names and titles). The re- 
maining entities are assigned to the NEW set. The basic ranking criteria for pronominal 
anaphora resolution prefer OLD entities over NEW entities. 8 
Strube (1998) thus proposes the following adaptation to the centering model: 
The Cf list is replaced by the list of salient discourse ntities (S-list) 
containing discourse ntities that are realized in the current and previous 
utterance. 
? The elements of the S-list are ranked according to the basic ranking 
criteria and position information: 
If X E OLD and y C NEW, then x precedes y. 
If x, y ~ OLD or x, y E NEW, 
8 To resolve functional naphora,  third set, MED, which includes inferable information, must be added 
between the OLD and the NEW sets. However, this set is not needed to resolve pronominal naphora 
(Strube and Hahn 1999). 
563 
Computational Linguistics Volume 27, Number 4 
Table 4 
Comparative r sults of blind test. 
Total CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Num. of 
pronoun 1,677 228 80 1,099 107 20 94 49 
occurrences 
Hobbs's 
algorithm 62.7% 61% 85% 62% 62% 50% 66% 52% 
Lappin & 
Leass's 67.4% 66% 86% 67% 65% 60% 67% 60% 
algorithm 
Proximity 52.9% 55% 86% 47% 65% 85% 61% 65% 
Centering 
approach 62.6% 60% 85% 62% 61% 60% 62% 58% 
Our 
algorithm 76.8% 71% 92% 79% 65% 85% 68% 69% 
then if utterance(y) precedes utterance(x), then x precedes y, 
if utterance(y) = utterance(x) and pos(x) < pos(y), then x precedes y. 
Since there is not a clear definition of what an utterance is, the following 
criteria are assumed: tensed clauses are defined as utterances on their 
own and untensed clauses are processed with the main clause in order to 
constitute only one utterance. 
Incorporating these adaptations, Strube (1998) then proposes the following algo- 
rithm: 
1. If a referring expression is encountered, 
(a) if it is a pronoun, test the elements of the S-list in order until the 
test succeeds; 
(b) update the S-list using information about this referring 
expression. 
2. If the analysis of utterance U is finished, remove all discourse ntities 
from the S-list that are not realized in U. 
The evaluation of this algorithm was performed in Strube (1998) and obtained a 
precision of 85.4% for English, improving upon the results of the centering algorithm 
by Brennan, Friedman, and Pollard (1987), which achieved only 72.9% precision when 
it was applied to the same corpus. 
Consequently, in adapting the centering model to Spanish anaphora resolution, we 
followed Strube's indications. The success rate of the algorithm was not satisfactory, 
as can be seen in Table 4. 
6. Conclus ions 
In this paper, we have presented an algorithm for identifying noun phrase antecedents 
of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and 
564 
Palomar et al Anaphora Resolution in Spanish Texts 
omitted pronouns in Spanish. The algorithm is applied to the syntactic structure gen- 
erated by the slot unification parser--see Ferrdndez, Palomar, and Moreno (1998a, 
1998b, 1999)--and coordinates different kinds of knowledge (lexical, morphological, 
and syntactic) by distinguishing between constraints and preferences. 
The main contribution ofthis paper is the introduction ofan algorithm for anaphora 
resolution for Spanish. In our work, we have undertaken an exhaustive study of the 
importance of each kind of knowledge in anaphora resolution for Spanish. Moreover, 
we have developed a definition of syntactic onditions of NP-pronoun noncorefer- 
ence in Spanish with partial parsing. We have also adapted our anaphora resolution 
algorithm to the problem of partial syntactic knowledge, that is to say, when partial 
parsing of the text is accomplished. 
For unrestricted texts, our approach is somewhat less accurate, since semantic 
information is not taken into account. For such texts, we are dealing with the output 
of a POS tagger, which does not provide this sort of knowledge. In order to test our 
approach with texts of different genres by different authors, we have worked with 
two different Spanish corpora, literary texts (the Lexesp corpus) and technical texts 
(the Blue Book), containing a total of 1,677 pronoun occurrences. 
The algorithm successfully identified the antecedent of the pronoun for 76.8% 
of these pronoun occurrences. Other algorithms usually work with different kinds 
of knowledge, different texts, and different languages. In order to make a more valid 
comparison of our algorithm with others, we adapted the other algorithms so that they 
would operate using only partial-parsing knowledge. In this evaluation, our algorithm 
has always obtained better esults. 
Moreover, based on the results on our study of the importance of each kind 
of knowledge, we can emphasize that constraints are very important for resolving 
anaphora successfully, since they considerably reduce the number of possible candi- 
dates. 
In future studies, we will attempt to evaluate the importance of semantic informa- 
tion in unrestricted texts for anaphora resolution in Spanish texts (Saiz-Noeda, Su~rez, 
and Peral 1999). This information will be obtained from a lexical tool (e.g., Spanish 
WordNet), which can be automatically consulted (since the tagger does not provide 
this information). 
Acknowledgments 
The authors wish to thank Ferran Pla, 
Natividad Prieto, and Antonio Molina for 
contributing their tagger (Pla 2000); and 
Richard Evans, Mikel Forcada, and Rafael 
Carrasco for their helpful revisions of the 
ideas presented in this paper. We are also 
grateful to several anonymous reviewers of 
Computational Linguistics for helpful 
comments on earlier drafts of this paper. 
Our work has been supported by the 
Spanish government (CICYT) with Grant 
TIC97-0671-C02-01/02. 
References 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Evaluating a
focus-based approach to anaphora 
resolution. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics 
(COLING-ACL'98), pages 74-78, Montreal 
(Canada). 
Baldwin, Breck. 1997. CogNIAC: High 
precision coreference with limited 
knowledge and linguistic resources. In
Proceedings of the ACL/EACL Workshop on 
Operational Factors in Practical, Robust 
Anaphora Resolution for Unrestricted Texts, 
pages 38--45, Madrid (Spain). 
Brennan, Susan E., Marilyn W. Friedman, 
and Carl J. Pollard. 1987. A centering 
approach to pronouns. In Proceedings ofthe 
25th Annual Meeting of the Association for 
Computational Linguistics (ACL'87), pages 
155-162, Stanford, CA (USA). 
Carbonell, Jaime G. and Ralf D. Brown. 
1988. Anaphora resolution: A
multi-strategy approach. In Proceedings of
the 12th International Conference on 
565 
Computational Linguistics Volume 27, Number 4 
Computational Linguistics (COLING'88), 
pages 96-101, Budapest (Hungary). 
Carletta, Jean, Amy Isard, Stephen Isard, 
Jacqueline C. Kowtko, Gwyneth 
Doherty-Sneddon, and Anne H. 
Anderson. 1997. The reliability of a 
dialogue structure coding scheme. 
Computational Linguistics, 23(1):13-32. 
Connolly, Dennis, John D. Burger, and 
David S. Day. 1994. A machine learning 
approach to anaphoric reference. In
Proceedings ofthe International Conference on 
New Methods in Language Processing 
(NEMLAP'94), pages 255-261, Manchester 
(UK). 
Dagan, Ido and Alon Itai. 1990. Automatic 
processing of large corpora for the 
resolution of anaphora references. In
Proceedings ofthe 13th International 
Conference on Computational Linguistics 
(COLING'90), pages 330-332, Helsinki 
(Finland). 
Dagan, Ido and Alon Itai. 1991. A statistical 
filter for resolving pronoun references. In
Yishai A. Feldman and Alfred Bruckstein, 
editors, Artificial Intelligence and Computer 
Vision. Elsevier Science Publishers B. V. 
(North-Holland), Amsterdam, pages 
125-135. 
Ferrlindez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998a. A computational 
approach to pronominal anaphora, 
one-anaphora and surface count 
anaphora. In Proceedings ofthe Second 
Colloquium on Discourse Anaphora nd 
Anaphora Resolution (DAARC'98), pages 
117-128, Lancaster (UK). 
Ferr~ndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998b. Anaphora 
resolution in unrestricted texts with 
partial parsing. In Proceedings ofthe 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
385-391, Montreal (Canada). 
Ferr~fndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1999. An empirical 
approach to Spanish anaphora resolution. 
Machine Translation, 14(3/4):191-216. 
Ferr~indez, Antonio and Jestis Peral. 2000. A 
computational pproach to zero-pronouns 
in Spanish. In Proceedings ofthe 38th 
Annual Meeting of the Association for 
Computational Linguistics (ACL'O0), pages 
166-172, Hong Kong (China). 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Ven d Large Corpora, pages 
161-170, Montreal (Canada). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1983. Providing a unified 
account of definite noun phrases in 
discourse. In Proceedings ofthe 21st Annual 
Meeting of the Association for Computational 
Linguistics (ACL'83), pages 44-50, 
Cambridge, MA (USA). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1995. Centering: A framework 
for modeling the local coherence of 
discourse. Computational Linguistics, 
21(2):203-225. 
Hobbs, Jerry R. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kennedy, Christopher and Branimir 
Boguraev. 1996. Anaphora for everyone: 
Pronominal anaphora resolution without 
a parser. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics (COLING'96), pages 113-118, 
Copenhagen (Denmark). 
Lappin, Shalom and Herbert Leass. 1994. 
An algorithm for pronominal anaphora 
resolution. Computational Linguistics, 
20(4):535-561. 
Mathews, Peter H. 1997. The Concise Oxford 
Dictionary of Linguistics. Oxford University 
Press, Oxford (UK). 
Mitkov, Ruslan. 1995. An uncertainty 
reasoning approach to anaphora 
resolution. In Proceedings ofthe Natural 
Language Pacific Rim Symposium (NLPRS 
"95), pages 149-154, Seoul (Korea). 
Mitkov, Ruslan. 1998. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17 th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 869-875, 
Montreal (Canada). 
Mitkov, Ruslan and Malgorzata Stys. 1997. 
Robust reference resolution with limited 
knowledge: High precision genre-specific 
approach for English and Polish. In 
Proceedings ofthe International Conference on 
Recent Advances in Natural Language 
Processing (RANLP'97), pages 74-81, 
Tzigov Chark (Bulgaria). 
Nakaiwa, Hiromi and Satoshi Shirai. 1996. 
Anaphora resolution of Japanese zero 
pronouns with deictic reference. In
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 812-817, Copenhagen 
(Denmark). 
Okumura, Manabu and Kouji Tamura. 1996. 
Zero pronoun resolution in Japanese 
discourse based on centering theory. In 
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
566 
Palomar et al Anaphora Resolution in Spanish Texts 
(COLING'96), pages 871-876, Copenhagen 
(Denmark). 
Palomar, Manuel, Antonio Ferra'ndez, Lidia 
Moreno, Maximiliano Saiz-Noeda, Rafael 
Mu~oz, Patricio Martfnez-Barco, Jestis 
Peral, and Borja Navarro. 1999. A robust 
partial parsing strategy based on the slot 
unification grammars. In Proceedings ofthe 
6th Conference on Natural Language 
Processing (TALN'99), pages 263-272, 
Corsica (France). 
Paul, Michael, Kazuhide Yamamoto, and 
Eiichiro Sumita. 1999. Corpus-based 
anaphora resolution towards antecedent 
preference. In Proceedings ofthe ACL 
Workshop on Coreference and Its Applications, 
pages 47-52, College Park, MD (USA). 
Pla, Ferran. 2000. Etiquetado Ldxico y Andlisis 
Sintdctico Super~'cial Basado en Modelos 
Estadfsticos. Ph.D. thesis, Valencia 
University of Technology, Valencia 
(Spain). 
Proyecto CRATER. 1994-1995. Corpus 
Resources And Terminology ExtRaction. 
MLAP-93/20. http: //www.lllf.uam.es / 
proyectos/crater.html (page visited on 
04/17/01). 
Reinhart, Tanya. 1983. Anaphora nd Semantic 
Interpretation. Croom Hehn Linguistics 
series. Croom Helm Ltd., Beckenham, 
Kent (UK). 
Saiz-Noeda, Maximiliano, Armando Sudrez, 
and Jestis Peral. 1999. Propuesta de 
incorporacidn de informaci6n sem~ntica 
desde Wordnet alandlisis sintdctico 
parcial orientado a la resoluci6n de la 
an~ffora. Procesamiento del Lenguaje Natural, 
25:167-173. 
Siegel, Sidney and John N. Castellan. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill, New York, NY 
(USA), 2nd edition. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
1251-1257, Montreal (Canada). 
Strube, Michael and Udo Hahn. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Vieira, Renata. 1998. Processing of Definite 
Descriptions in Unrestricted Texts. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh (UK). 
Yamamoto, Kazuhide and Eiichiro Sumita. 
1998. Feasibility study for ellipsis 
resolution in dialogues by 
machine-learning technique. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 385-391, 
Montreal (Canada). 
567 

Splitting Complex Temporal Questions for Question Answering systems  
E. Saquete, P. Mart??nez-Barco, R. Mun?oz, J.L. Vicedo
Grupo de investigacio?n del Procesamiento del Lenguaje y Sistemas de Informacio?n.
Departamento de Lenguajes y Sistemas Informa?ticos. Universidad de Alicante.
Alicante, Spain

stela,patricio,rafael,vicedo  @dlsi.ua.es
Abstract
This paper presents a multi-layered Question An-
swering (Q.A.) architecture suitable for enhanc-
ing current Q.A. capabilities with the possibility of
processing complex questions. That is, questions
whose answer needs to be gathered from pieces
of factual information scattered in different docu-
ments. Specifically, we have designed a layer ori-
ented to process the different types of temporal
questions. Complex temporal questions are first de-
composed into simpler ones, according to the tem-
poral relationships expressed in the original ques-
tion.
In the same way, the answers of each simple ques-
tion are re-composed, fulfilling the temporal restric-
tions of the original complex question.
Using this architecture, a Temporal Q.A. system
has been developed.
In this paper, we focus on explaining the first part
of the process: the decomposition of the complex
questions. Furthermore, it has been evaluated with
the TERQAS question corpus of 112 temporal ques-
tions. For the task of question splitting our system
has performed, in terms of precision and recall, 85%
and 71%, respectively.
1 Introduction
Question Answering could be defined as the pro-
cess of computer-answering to precise or arbitrary
questions formulated by users. Q.A. systems are es-
pecially useful to obtain a specific piece of informa-
tion without the need of manually going through all
the available documentation related to the topic.
Research in Question Answering mainly focuses
on the treatment of factual questions. These require
as an answer very specific items of data, such as
dates, names of entities or quantities, e.g., ?What is
the capital of Brazil??.

This paper has been supported by the Spanish government,
projects FIT-150500-2002-244, FIT-150500-2002-416, TIC-
2003-07158-C04-01 and TIC2000-0664-C02-02.
Temporal Q.A. is not a trivial task due to the com-
plexity temporal questions may reach. Current op-
erational Q.A. systems can deal with simple factual
temporal questions. That is, questions requiring to
be answered with a date, e.g. ?When did Bob Mar-
ley die??. or questions that include simple temporal
expressions in their formulation, e.g., ?Who won the
U.S. Open in 1999??. Processing this sort of ques-
tions is usually performed by identifying explicit
temporal expressions in questions and relevant doc-
uments, in order to gather the necessary information
to answer the queries.
Even though, it seems necessary to emphasize
that the system described in (Breck et al, 2000) is
the only one also using implicit temporal expression
recognition for Q.A. purposes. It does so by apply-
ing the temporal tagger developed by Mani and Wil-
son (2000).
However, issues like addressing the temporal
properties or the ordering of events in questions, re-
main beyond the scope of current Q.A. systems:

?Who was spokesman of the Soviet Embassy
in Baghdad during the invasion of Kuwait??

?Is Bill Clinton currently the President of the
United States??
This work presents a Question Answering system
capable of answering complex temporal questions.
This approach tries to imitate human behavior when
responding this type of questions. For example, a
human that wants to answer the question: ?Who
was spokesman of the Soviet Embassy in Baghdad
during the invasion of Kuwait?? would follow this
process:
1. First, he would decompose this question into
two simpler ones: ?Who was spokesman of the
Soviet Embassy in Baghdad?? and ?When did
the invasion of Kuwait occur??.
2. He would look for all the possible answers
to the first simple question: ?Who was
spokesman of the Soviet Embassy in Bagh-
dad??.
3. After that, he would look for the answer to the
second simple question: ?When did the inva-
sion of Kuwait occur??
4. Finally, he would give as a final answer one
of the answers to the first question (if there is
any), whose associated date stays within the
period of dates implied by the answer to the
second question. That is, he would obtain
the final answer by discarding all answers to
the simple questions which do not accomplish
the restrictions imposed by the temporal signal
provided by the original question (during).
Therefore, the treatment of complex question is
based on the decomposition of these questions into
simpler ones, to be resolved using conventional
Question Answering systems. Answers to simple
questions are used to build the answer to the origi-
nal question.
This paper has been structured in the following
fashion: first of all, section 2 presents our proposal
of a taxonomy for temporal questions. Section 3
describes the general architecture of our temporal
Q.A. system. Section 4 deepens into the first part
of the system: the decomposition unit. Finally, the
evaluation of the decomposition unit and some con-
clusions are shown.
2 Proposal of a Temporal Questions
Taxonomy
Before explaining how to answer temporal ques-
tions, it is necessary to classify them, since the
way to solve them will be different in each case.
Our classification distinguishes first between simple
questions and complex questions. We will consider
as simple those questions that can be solved directly
by a current General Purpose Question Answering
system, since they are formed by a single event. On
the other hand, we will consider as complex those
questions that are formed by more than one event
related by a temporal signal which establishes an
order relation between these events.
Simple Temporal Questions:
Type 1: Single event temporal questions without
temporal expression (TE). This kind of questions
are formed by a single event and can be directly
resolved by a Q.A. System, without pre- or post-
processing them. There are not temporal expres-
sions in the question. Example: ?When did Jordan
close the port of Aqaba to Kuwait??
Type 2: Single event temporal questions with tem-
poral expression. There is a single event in the ques-
tion, but there are one or more temporal expressions
that need to be recognized, resolved and annotated.
Each piece of temporal information could help to
search for an answer. Example: ?Who won the 1988
New Hampshire republican primary??. TE: 1988
Complex Temporal Questions:
Type 3: Multiple events temporal questions with
temporal expression. Questions that contain two or
more events, related by a temporal signal. This sig-
nal establishes the order between the events in the
question. Moreover, there are one or more tempo-
ral expressions in the question. These temporal ex-
pressions need to be recognized, resolved and an-
notated, and they introduce temporal constraints to
the answers of the question. Example: ?What did
George Bush do after the U.N. Security Council or-
dered a global embargo on trade with Iraq in August
90?? In this example, the temporal signal is after
and the temporal constraint is ?between 8/1/1990
and 8/31/1990?. This question can be divided into
the following ones:
 Q1: What did George Bush do?
 Q2: When the U.N. Security Council ordered
a global embargo on trade with Iraq?
Type 4: Multiple events temporal questions with-
out temporal expression. Questions that consist
of two or more events, related by a temporal sig-
nal. This signal establishes the order between the
events in the question. Example: ?What happened
to world oil prices after the Iraqi annexation of
Kuwait??. In this example, the temporal signal is
after and the question would be decomposed into:
 Q1: What happened to world oil prices?
 Q2: When did the Iraqi ?annexation? of
Kuwait occur?
How to process each type will be explained in de-
tail in the following sections.
3 Multi-layered Question-Answering
System Architecture
Current Question Answering system architectures
do not allow to process complex questions. That is,
questions whose answer needs to be gathered from
pieces of factual information that is scattered in a
document or through different documents. In or-
der to be able to process these complex questions,
we propose a multi-layered architecture. This ar-
chitecture increases the functionality of the current
Question-Answering systems, allowing us to solve
any type of temporal questions. Moreover, this sys-
tem could be easily augmented with new layers to
cope with questions that need complex processing
and are not temporal oriented.
Some examples of complex questions are:
 Temporal questions like ?Where did Michael
Milken study before going to the University of
Pennsylvania??. This kind of questions needs
to use temporal information and event ordering
to obtain the right answer.
 Script questions like ?How do I assemble a bi-
cycle??. In these questions, the final answer is
a set of ordered answers.
 Template-based questions like ?Which are the
main biographical data of Nelson Mandela??.
This question should be divided in a number of
factual questions asking for different aspects of
Nelson Mandela?s biography. Gathering their
respective answers will make it possible to an-
swer the original question.
These three types of question have in common
the necessity of an additional processing in order
to be solved. Our proposal to deal with them is
to superpose an additional processing layer, one by
each type, to a current General Purpose Question
Answering system, as it is shown in Figure 1. This
layer will perform the following steps:
 Decomposition of the question into simple
events to generate simple questions (sub-
questions) and the ordering of the sub-
questions.
 Sending simple questions to a current General
Purpose Question Answering system.
 Receiving the answers to the simple questions
from the current General Purpose Question
Answering system.
 Filtering and comparison between sub-answers
to build the final complex answer.
	
	













 
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157?160,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Opinion and Generic Question Answering Systems: a Performance 
Analysis 
 
 
Alexandra Balahur 1,2 
1DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
2IPSC, EC Joint Research Centre 
Via E. Fermi, 21027, Ispra 
abalahur@dlsi.ua.es 
Ester Boldrini 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
patricio@dlsi.ua.es 
 
Abstract 
The importance of the new textual genres such 
as blogs or forum entries is growing in parallel 
with the evolution of the Social Web. This pa-
per presents two corpora of blog posts in Eng-
lish and in Spanish, annotated according to the 
EmotiBlog annotation scheme. Furthermore, 
we created 20 factual and opinionated ques-
tions for each language and also the Gold 
Standard for their answers in the corpus. The 
purpose of our work is to study the challenges 
involved in a mixed fact and opinion question 
answering setting by comparing the perform-
ance of two Question Answering (QA) sys-
tems as far as mixed opinion and factual set-
ting is concerned. The first one is open do-
main, while the second one is opinion-
oriented. We evaluate separately the two sys-
tems in both languages and propose possible 
solutions to improve QA systems that have to 
process mixed questions. 
Introduction and motivation 
In the last few years, the number of blogs has 
grown exponentially. Thus, the Web contains 
more and more subjective texts. A research from 
the Pew Institute shows that 75.000 blogs are 
created daily (Pang and Lee, 2008). They ap-
proach a great variety of topics (computer sci-
ence, sociology, political science or economics) 
and are written by different types of people, thus 
are a relevant resource for large community be-
havior analysis. Due to the high volume of data 
contained in blogs, new Natural Language Proc-
essing (NLP) resources, tools and methods are 
needed in order to manage their language under-
standing. Our fist contribution consists in carry-
ing out a multilingual research, for English and 
Spanish. Secondly, many sources are present in 
blogs, as people introduce quotes from newspa-
per articles or other information to support their 
arguments and make references to previous posts 
in the discussion thread. Thus, when performing 
a task such as Question Answering (QA), many 
new aspects have to be taken into consideration. 
Previous studies in the field (Stoyanov, Cardie 
and Wiebe, 2005) showed that certain types of 
queries, which are factual in nature, require the 
use of Opinion Mining (OM) resources and tech-
niques to retrieve the correct answers. A further 
contribution this paper brings is the analysis and 
definition of the criteria for the discrimination 
among types of factual versus opinionated ques-
tions. Previous researchers mainly concentrated 
on newspaper collections. We formulated and 
annotated of a set of questions and answers over 
a multilingual blog collection. A further contri-
bution is the evaluation and comparison of two 
different approaches to QA a fact-oriented one 
and another designed for opinion QA scenarios.  
Related work 
Research in building factoid QA systems has a 
long history. However, it is only recently that 
studies have started to focus also on the creation 
and development of QA systems for opinions. 
Recent years have seen the growth of interest in 
this field, both by the research performed and the 
publishing of various studies on the requirements 
157
and peculiarities of opinion QA systems (Stoy-
anov, Cardie and Wiebe, 2005), (Pustejovsky 
and Wiebe, 2006), as well as the organization of 
international conferences that promote the crea-
tion of effective QA systems both for general and 
subjective texts, as, for example, the Text Analy-
sis Conference (TAC)1. Last year?s TAC 2008 
Opinion QA track proposed a mixed setting of 
factoid (?rigid list?) and opinion questions 
(?squishy list?), to which the traditional systems 
had to be adapted. The Alyssa system (Shen et 
al., 2007), classified the polarity of the question 
and of the extracted answer snippet, using a Sup-
port Vector Machines classifier trained on the 
MPQA corpus (Wiebe, Wilson and Cardie, 
2005), English NTCIR2 data and rules based on 
the subjectivity lexicon (Wilson, Wiebe and 
Hoffman, 2005). The PolyU (Wenjie et al, 
2008) system determines the sentiment orienta-
tion with two estimated language models for the 
positive versus negative categories. The 
QUANTA (Li, 2008) system detects the opinion 
holder, the object and the polarity of the opinion 
using a semantic labeler based on PropBank3 and 
some manually defined patterns.  
Evaluation 
In order to carry out our evaluation, we em-
ployed a corpus of blog posts presented in 
(Boldrini et al, 2009). It is a collection of blog 
entries in English, Spanish and Italian. However, 
for this research we used the first two languages. 
We annotated it using EmotiBlog (Balahur et al, 
2009) and we also created a list of 20 questions 
for each language. Finally, we produced the Gold 
Standard, by labeling the corpus with the correct 
answers corresponding to the questions. 
1.1 Questions 
No TYPE QUESTION 
 
1 
 
F 
 
F 
What international organization do people criticize for 
its policy on carbon emissions? 
?Cu?l fue uno de los primeros pa?ses que se preocup? 
por el problema medioambiental? 
 
 
2 
 
 
O 
 
 
F 
What motivates people?s negative opinions on the 
Kyoto Protocol? 
?Cu?l es el pa?s con mayor responsabilidad de la 
contaminaci?n mundial seg?n la opini?n p?blica? 
 
 
3 
 
 
F 
 
 
F 
What country do people praise for not signing the 
Kyoto Protocol? 
?Qui?n piensa que la reducci?n de la contaminaci?n se 
deber?a apoyar en los consejos de los cient?ficos? 
 
 
4 
 
 
F 
 
 
F 
What is the nation that brings most criticism to the 
Kyoto Protocol? 
?Qu? administraci?n act?a totalmente en contra de la 
lucha contra el cambio clim?tico? 
                                                 
1 http://www.nist.gov/tac/ 
2 http://research.nii.ac.jp/ntcir/ 
3 http://verbs.colorado.edu/~mpalmer/projects/ace.html 
 
 
5 
 
 
O 
 
 
F 
What are the reasons for the success of the Kyoto 
Protocol? 
?Qu? personaje importante est? a favor de la 
colaboraci?n del estado en la lucha contra el 
calentamiento global? 
 
 
6 
 
 
O 
 
 
F 
What arguments do people bring for their criticism of 
media as far as the Kyoto Protocol is concerned? 
?A qu? pol?ticos americanos culpa la gente por la 
grave situaci?n en la que se encuentra el planeta? 
 
7 
 
O 
 
F 
Why do people criticize Richard Branson? 
?A qui?n reprocha la gente el fracaso del Protocolo de 
Kyoto? 
 
8 
 
F 
 
F 
What president is criticized worldwide for his reaction 
to the Kyoto Protocol? 
?Qui?n acusa a China por provocar el mayor da?o al 
medio ambiente? 
 
9 
 
F 
 
O 
What American politician is thought to have developed 
bad environmental policies? 
?C?mo ven los expertos el futuro? 
 
10 
 
F 
 
O 
What American politician has a positive opinion on the 
Kyoto protocol? 
C?mo se considera el atentado del 11 de septiembre? 
 
11 
 
O 
 
O 
What negative opinions do people have on Hilary 
Benn? 
?Cu?l es la opini?n sobre EEUU? 
 
12 
 
O 
 
O 
Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol and other environmental issues? 
?De d?nde viene la riqueza de EEUU? 
 
13 
 
F 
 
O 
What country disregards the importance of the Kyoto 
Protocol? 
?Por qu? la guerra es negativa? 
 
14 
 
F 
 
O 
What country is thought to have rejected the Kyoto 
Protocol due to corruption? 
?Por qu? Bush se retir? del Protocolo de Kyoto? 
 
15 
 
F/
O 
 
O 
What alternative environmental friendly resources do 
people suggest to use instead of gas en the future? 
?Cu?l fue la posici?n de EEUU sobre el Protocolo de 
Kyoto? 
 
16 
 
F/
O 
 
O 
 Is Arnold Schwarzenegger pro or against the reduction 
of CO2 emissions? 
?Qu? piensa Bush sobre el cambio clim?tico? 
 
17 
 
F 
 
O 
What American politician supports the reduction of 
CO2 emissions? 
?Qu? impresi?n da Bush? 
 
18 
 
F/
O 
 
O 
What improvements are proposed to the Kyoto Proto-
col? 
?Qu? piensa China del calentamiento global? 
 
19 
 
F/
O 
 
O 
What is Bush accused of as far as political measures 
are concerned? 
?Cu?l es la opini?n de Rusia sobre el Protocolo de 
Kyoto? 
 
20 
 
F/
O 
 
O 
What initiative of an international body is thought to be 
a good continuation for the Kyoto Protocol? 
?Qu? cree que es necesario hacer Yvo Boer? 
 
Table 1: List of question in English and Spanish 
 
As it can be seen in the table above, we created 
factoid (F) and opinion (O) queries for English 
and for Spanish; however, there are some that 
could be defined between factoid and opinion 
(F/O) and the system can retrieve multiple an-
swers after having selected, for example, the po-
larity of the sentences in the corpus. 
1.2 Performance of the two systems 
We evaluated and compared the generic QA sys-
tem of the University of Alicante (Moreda et al, 
2008) and the opinion QA system presented in 
(Balahur et al, 2008), in which Named Entity 
Recognition with LingPipe4 and FreeLing5 was 
                                                 
4 http://alias-i.com/lingpipe/ 
5 http://garraf.epsevg.upc.es/freeling/ 
158
added, in order to boost the scores of answers 
containing NEs of the question Expected Answer 
Type (EAT). Table 2 presents the results ob-
tained for English and Table 3 for Spanish. We 
indicate the id of the question (Q), the question 
type (T) and the number of answer of the Gold 
Standard (A). We present the number of the re-
trieved questions by the traditional system 
(TQA) and by the opinion one (OQA). We take 
into account the first 1, 5, 10 and 50 answers. 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
   TQA OQA TQA OQA TQA OQA TQA OQA 
1 F 5 0 0 0 2 0 3 4 4 
2 O 5 0 0 0 1 0 1 0 3 
3 F 2 1 1 2 1 2 1 2 1 
4 F 10 1 1 2 1 6 2 10 4  
5 O 11 0 0 0 0 0 0 0 0 
6 O 2 0 0 0 0 0 1 0 2 
7 O 5 0 0 0 0 0 1 0 3 
8 F 5 1 0 3 1 3 1 5 1 
9 F 5 0 1 0 2 0 2 1 3 
10 F 2 1 0 1 0 1 1 2 1 
11 O 2 0 1 0 1 0 1 0 1 
12 O 3 0 0 0 1 0 1 0 1 
13 F 1 0 0 0 0 0 0 0 1 
14 F 7 1 0 1 1 1 2 1 2 
15 F/O 1 0 0 0 0 0 1 0 1 
16 F/O 6 0 1 0 4 0 4 0 4 
17 F 10 0 1 0 1 4 1 0 2 
18 F/O 1 0 0 0 0 0 0 0 0 
19 F/O 27 0 1 0 5 0 6 0 18 
20 F/O 4 0 0 0 0 0 0 0 0 
 
Table 2: Results for English 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
    TQA  OQA  TQA  OQA  TQA  OQA  TQA  OQA 
1 F 9 1 0 0 1 1 1 1 3 
2 F 13 0 1 2 3 0 6 11 7 
3 F 2 0 1 0 2 0 2 2 2 
4 F 1 0 0 0 0 0 0 1 0 
5 F 3 0 0 0 0 0 0 1 0 
6 F 2 0 0 0 1 0 1 2 1 
7 F 4 0 0 0 0 1 0 4 0 
8 F 1 0 0 0 0 0 0 1 0 
9 O 5 0 1 0 2 0 2 0 4 
10 O 2 0 0 0 0 0 0 0 0 
11 O 5 0 0 0 1 0 2 0 3 
12 O 2 0 0 0 1 0 1 0 1 
13 O 8 0 1 0 2 0 2 0 4 
14 O 25 0 1 0 2 0 4 0 8 
15 O 36 0 1 0 2 0 6 0 15 
16 O 23 0 0 0 0 0 0 0 0 
17 O 50 0 1 0 5 0 6 0 10 
18 O 10 0 1 0 1 0 2 0 2 
19 O 4 0 1 0 1 0 1 0 1 
20 O 4 0 1 0 1 0 1 0 1 
 
Table 3: Results for Spanish 
1.3 Results and discussion 
There are many problems involved when trying 
to perform mixed fact and opinion QA. The first 
can be the ambiguity of the questions e.g. ?De 
d?nde viene la riqueza de EEUU?. The answer 
can be explicitly stated in one of the blog sen-
tences, or a system might have to infer them 
from assumptions made by the bloggers and their 
comments. Moreover, most of the opinion ques-
tions have longer answers, not just a phrase snip-
pet, but up to 2 or 3 sentences. As we can ob-
serve in Table 2, the questions for which the 
TQA system performed better were the pure fac-
tual ones (1, 3, 4, 8, 10 and 14), although in some 
cases (question number 14) the OQA system re-
trieved more correct answers.  At the same time, 
opinion queries, although revolving around NEs, 
were not answered by the traditional QA system, 
but were satisfactorily answered by the opinion 
QA system (2, 5, 6, 7, 11, 12). Questions 18 and 
20 were not correctly answered by any of the two 
systems. We believe the reason is that question 
18 was ambiguous as far as polarity of the opin-
ions expressed in the answer snippets (?im-
provement? does not translate to either ?positive? 
or ?negative?) and question 20 referred to the 
title of a project proposal that was not annotated 
by any of the tools used. Thus, as part of the fu-
ture work in our OQA system, we must add a 
component for the identification of quotes and 
titles, as well as explore a wider range of polar-
ity/opinion scales. Furthermore, questions 15, 16, 
18, 19 and 20 contain both factual as well as 
opinion aspects and the OQA system performed 
better than the TQA, although in some cases, 
answers were lost due to the artificial boosting of 
the queries containing NEs of the EAT (Ex-
pected Answer Type). Therefore, it is obvious 
that an extra method for answer ranking should 
be used, as Answer Validation techniques using 
Textual Entailment. In Table 3, the OQA missed 
some of the answers due to erroneous sentence 
splitting, either separating text into two sentences 
where it was not the case or concatenating two 
consecutive sentences; thus missing out on one 
of two consecutively annotated answers. Exam-
ples are questions number 16 and 17, where 
many blog entries enumerated the different ar-
guments in consecutive sentences. Another 
source of problems was the fact that we gave a 
high weight to the presence of the NE of the 
sought type within the retrieved snippet and in 
some cases the name was misspelled in the blog 
entries, whereas in other NER performed by 
159
FreeLing either attributed the wrong category to 
an entity, failed to annotate it or wrongfully an-
notated words as being NEs.  Not of less impor-
tance is the question duality aspect in question 
17. Bush is commented in more than 600 sen-
tences; therefore, when polarity is not specified, 
it is difficult to correctly rank the answers. Fi-
nally, also the problems of temporal expressions 
and the coreference need to be taken into ac-
count.  
Conclusions and future work 
In this article, we created a collection of both 
factual and opinion queries in Spanish and Eng-
lish. We labeled the Gold Standard of the an-
swers in the corpora and subsequently we em-
ployed two QA systems, one open domain, one 
for opinion questions. Our main objective was to 
compare the performances of these two systems 
and analyze their errors, proposing solutions to 
creating an effective QA system for both factoid 
an opinionated queries. We saw that, even using 
specialized resources, the task of QA is still chal-
lenging. Opinion QA can benefit from a snippet 
retrieval at a paragraph level, since in many 
cases the answers were not simple parts of sen-
tences, but consisted in two or more consecutive 
sentences. On the other hand, we have seen cases 
in which each of three different consecutive sen-
tences was a separate answer to a question. Our 
future work contemplates the study of the impact 
anaphora resolution and temporality on opinion 
QA, as well as the possibility to use Answer 
Validation techniques for answer re-ranking. 
 
Acknowledgments 
 
The authors would like to thank Paloma Moreda, 
Hector Llorens, Estela Saquete and Manuel 
Palomar for evaluating the questions on their QA 
system. This research has been partially funded 
by the Spanish Government under the project 
TEXT-MESS (TIN 2006-15265-C06-01), by the 
European project QALL-ME (FP6 IST 033860) 
and by the University of Alicante, through its 
doctoral scholarship. 
References 
Alexandra Balahur, Ester Boldrini, Andr?s Montoyo, 
and Patricio Mart?nez-Barco, 2009. Cross-topic 
Opinion Mining for Real-time Human-Computer 
Interaction. In Proceedings of the 6th Workshop in 
Natural Language Processing and Cognitive Sci-
ence, ICEIS 2009 Conference, Milan, Italy. 
Alexandra Balahur, Elena Lloret, Oscar Ferrandez, 
Andr?s Montoyo, Manuel Palomar, Rafael Mu?oz. 
2008. The DLSIUAES Team?s Participation in the 
TAC 2008 Tracks. In Proceedings of the Text 
Analysis Conference (TAC 2008). 
Ester Boldrini, Alexandra Balahur, Patricio Mart?nez-
Barco, and Andr?s Montoyo. 2009. EmotiBlog: An 
Annotation Scheme for Emotion Detection and 
Analysis in Non-Traditional Textual Genres. To 
appear in Proceedings of the 5th Conference on 
data Mining. Las Vegas, Nevada, USA. 
W. Li, Y. Ouyang, Y. Hu, F. Wei. PolyU at TAC 
2008. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Fangtao Li, Zhicheng Zheng, Tang Yang, Fan Bu, 
Rong Ge, Xiaoyan Zhu, Xian Zhang, and Minlie 
Huang. THU QUANTA at TAC 2008 QA and RTE 
track. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Bo Pang, and Lilian. Lee, Opinion mining and senti-
ment analysis. Foundations and Trends R. In In-
formation Retrieval Vol. 2, Nos. 1?2 (2008) 1?135, 
2008. 
James Pustejovsky and Janyce. Wiebe. Introduction 
to Special Issue on Advances in Question Answer-
ing. In Language Resources and Evaluation (2005) 
39: 119?122. Springer, 2006. 
Dan Shen, Jochen L. Leidner, Andreas Merkel, Diet-
rich Klakow. The Alyssa system at TREC QA 2007: 
Do we need Blog06? In Proceedings of The Six-
teenth Text Retrieval Conference (TREC 2007), 
Gaithersburg, MD, USA, 2007 
Vaselin, Stoyanov, Claire Cardie, Janyce Wiebe. 
Multi-Perspective Question Answering Using the 
OpQA Corpus. In Proceedings of HLT/EMNLP. 
2005. 
Paloma Moreda, Hector Llorens, Estela Saquete, 
Manuel Palomar. 2008. Automatic Generalization 
of a QA Answer Extraction Module Based on Se-
mantic Roles. In: AAI - IBERAMIA, Lisbon, Portu-
gal, pages 233-242, Springer. 
Janyce. Wiebe, Theresa Wilson, and Claire Cardie 
Annotating expressions of opinions and emotions 
in language. Language Resources and Evaluation, 
volume 39, issue 2-3, pp. 165-210, 2005. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
Recognising Contextual Polarity in Phrase-level 
sentiment Analysis. In Proceedings of Human lan-
guage Technologies Conference/Conference on 
Empirical methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2005. 
160
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 30?37,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluating Knowledge-based Approaches to the Multilingual Extension of
a Temporal Expression Normalizer
Matteo Negri
ITC-irst
Povo - Trento, Italy
negri@itc.it
Estela Saquete, Patricio Mart??nez-Barco, Rafael Mun?oz
DLSI, University of Alicante
Alicante, Spain
{stela,patricio,rafael}@dlsi.ua.es
Abstract
The extension to new languages is a well
known bottleneck for rule-based systems.
Considerable human effort, which typi-
cally consists in re-writing from scratch
huge amounts of rules, is in fact required
to transfer the knowledge available to the
system from one language to a new one.
Provided sufficient annotated data, ma-
chine learning algorithms allow to mini-
mize the costs of such knowledge trans-
fer but, up to date, proved to be ineffec-
tive for some specific tasks. Among these,
the recognition and normalization of tem-
poral expressions still remains out of their
reach. Focusing on this task, and still ad-
hering to the rule-based framework, this
paper presents a bunch of experiments on
the automatic porting to Italian of a system
originally developed for Spanish. Differ-
ent automatic rule translation strategies are
evaluated and discussed, providing a com-
prehensive overview of the challenge.
1 Introduction
In recent years, inspired by the success of MUC
evaluations, a growing number of initiatives (e.g.
TREC1, CLEF2, CoNLL3, Senseval4) have been
developed to boost research towards the automatic
understanding of textual data. Since 1999, the Au-
tomatic Content Extraction (ACE) program5 has
been contributing to broaden the varied scenario
of evaluation campaigns by proposing three main
1http://trec.nist.gov
2http://clef-campaign.org
3http://www.cnts.ua.ac.be/conll
4http://www.senseval.org
5http://www.nist.gov/speech/tests/ace
tasks, namely the recognition of entities, rela-
tions, and events. In 2004, the Timex2 Detec-
tion and Recognition task6 (also known as TERN,
for Time Expression Recognition and Normaliza-
tion) has been added to the ACE program, making
the whole evaluation exercise more complete. The
main goal of the task was to foster research on sys-
tems capable of automatically detecting temporal
expressions (TEs) present in an English text, and
normalizing them with respect to a specifically de-
fined annotation standard.
Within the above mentioned evaluation exer-
cises, the research activity on monolingual tasks
has gradually been complemented by a consid-
erable interest towards multilingual and cross-
language capabilities of NLP systems. This trend
confirms how portability across languages has
now become one of the key challenges for Natu-
ral Language Processing research, in the effort of
breaking the language barrier hampering systems?
application in many real use scenarios. In this di-
rection, machine learning techniques have become
the standard approach in many NLP areas. This
is motivated by several reasons, including i) the
fact that considerable amounts of annotated data,
indispensable to train ML-based algorithms, are
now available for many tasks, and ii) the difficulty,
inherent to rule-based approaches, of porting lan-
guage models from one language to new ones. In
fact, while supervised ML algorithms can be eas-
ily extended to new languages given an annotated
training corpus, rule-based approaches require to
redefine the set of rules, adapting them to each new
language. This is a time consuming and costly
work, as it usually consists in manually rewriting
from scratch huge amounts of rules.
6http://timex2.mitre.org
30
In spite of their effectiveness for some tasks,
ML techniques still fall short from providing ef-
fective solutions for others. This is confirmed by
the outcomes of the TERN 2004 evaluation, which
provide a clear picture of the situation. In spite
of the good results obtained in the TE recognition
task (Hacioglu et al, 2005), the normalization by
means of ML techniques has not been tackled yet,
and still remains an unresolved problem.
Considering the inadequacy of ML techniques
to deal with the normalization problem, and fo-
cusing on portability across languages, this pa-
per extends and completes the previous work pre-
sented in (Saquete et al, 2006b) and (Saquete et
al., 2006a). More specifically, we address the fol-
lowing crucial issue: how to minimize the costs
of building a rule-based TE recognition system
for a new language, given an already existing sys-
tem for another language. Our goal is to experi-
ment with different automatic porting procedures
to build temporal models for new languages, start-
ing from previously defined ones. Still adhering
to the rule-based paradigm, we analyse different
porting methodologies that automatically learn the
TE recognition model used by the system in one
language, adjusting the set of normalization rules
for the new target language.
In order to provide a clear and comprehen-
sive overview of the challenge, an incremental ap-
proach is proposed. Starting from the architecture
of an existing system developed for Spanish (Sa-
quete et al, 2005), we present a bunch of exper-
iments which take advantage of different knowl-
edge sources to build an homologous system for
Italian. Building on top of each other, such exper-
iments aim at incrementally analyzing the contri-
bution of additional information to attack the TE
normalization task. More specifically, the follow-
ing information will be considered:
? The output of online translators;
? The information mined from a manually an-
notated corpus;
? A combination of the two.
2 The task: TE recognition and
normalization
The TERN task consists in automatically detect-
ing, bracketing, and normalizing all the time ex-
pressions mentioned within an English text. The
recognized TEs are then annotated according to
the TIMEX2 annotation standard described in
(Ferro et al, 2005). Markable TEs include both
absolute (or explicit) expressions (e.g. ?April 15,
2006?), and relative (or anaphoric) expressions
(e.g. ?three years ago?). Also markable are du-
rations (e.g. ?two weeks?), event-anchored ex-
pressions (e.g. ?two days before departure?), and
sets of times (e.g. ?every week?). Detection and
bracketing concern systems? capability to recog-
nize TEs within an input text, and correctly deter-
mine their extension. Normalization concerns the
ability of the system to correctly assign, for each
detected TE, the correct values to the TIMEX2
normalization attributes. The meaning of these at-
tributes can be summarized as follows:
? VAL: contains the normalized value of a TE
(e.g. ?2004-05-06? for ?May 6th, 2004?)
? ANCHOR VAL: contains a normalized form
of an anchoring date-time.
? ANCHOR DIR: captures the relative
direction-orientation between VAL and
ANCHOR VAL.
? MOD: captures temporal modifiers (pos-
sible values include: ?approximately?,
?more than?, ?less than?)
? SET: identifies expressions denoting sets of
times (e.g. ?every year?).
2.1 The evaluation benchmark
Moving to a new language, an evaluation bench-
mark is necessary to test systems performances.
For this purpose, the temporal annotations of the
Italian Content Annotation Bank (I-CAB-temp7)
have been selected.
I-CAB consists of 525 news documents
taken from the Italian newspaper L?Adige
(http://www.adige.it), and contains around
182,500 words. Its 3,830 temporal expressions
(2,393 in the training part of the corpus, and 1,437
in the test part) have been manually annotated
following the TIMEX2 standard with some adap-
tations to the specific morpho-syntactic features
of Italian, which has a far richer morphology than
English (see (Magnini et al, 2006) for further
details).
7I-CAB is being developed as part of the three-year
project ONTOTEXT funded by the Provincia Autonoma di
Trento, Italy. See http://tcc.itc.it/projects/ontotext
31
3 The starting point: TERSEO
As a starting point for our experiments we used
TERSEO, a system originally developed for the
automatic annotation of TEs appearing in a Span-
ish written text in compliance with the TIMEX2
standard (see (Saquete, 2005) for a thorough de-
scription of TERSEO?s main features and func-
tionalities).
TEXT
POS 
TAGGER
RECOGNITION: 
PARSER
Lexical and
morphological
information
Temporal 
expression
recognition
DATE
ESTIMATION
Dictionary
Temporal
Expression
Grammar
TEMPORAL
EXPRESSION
NORMALIZATION
EVENT 
ORDERING
ORDERED
TEXT
Documental 
DataBase
Figure 1: System?s architecture.
Basically (see Figure 1), the TE recognition
and normalization process is carried out in two
phases. The first phase (recognition) includes a
pre-processing of the input text, which is tagged
with lexical and morphological information that
will be used as input to a temporal parser. The
temporal parser is implemented using an as-
cending technique (chart parser) and relies on a
language-specific temporal grammar. As TEs can
be divided into absolute and relative ones, such
grammar is tuned for discriminating between the
two groups. On the one hand, absolute TEs di-
rectly provide and fully describe a date. On the
other hand, relative TEs require some degree of
reasoning (as in the case of anaphora resolution).
In the second phase of the process, in order to
translate these expressions into their normalized
form, the lexical context in which they occur is
considered. At this stage, a normalization unit
is in charge of determining the appropriate refer-
ence date (anchor) associated to each anaphoric
TE, calculating its value, and finally generating the
corresponding TIMEX2 tag.
?From a multilingual perspective, an impor-
tant feature of TERSEO is the distinction between
recognition rules, which are language-specific,
and normalization rules, which are language-
independent and potentially reusable for any other
language. Taking the most from the modular ar-
chitecture of the system, a first multilingual exten-
sion has been evaluated over the English TERN
2004 test set. In that extension, the English
temporal model was automatically obtained from
the Spanish one, through the automatic transla-
tion into English8 of the Spanish TEs recognized
by the system (Saquete et al, 2004). The re-
sulting English TEs were then mapped onto the
corresponding language-independent normaliza-
tion rules, with good results (compared with other
participants to the competition) both in terms of
precision and recall. These results are shown in
Table 1.
Prec Rec F
timex2 0.673 0.728 0.699
anchor dir 0.658 0.877 0.752
anchor val 0.684 0.912 0.782
set 0.800 0.667 0.727
text 0.770 0.620 0.690
val 0.757 0.735 0.746
Table 1: Evaluation of English-TERSEO over the
TERN 2004 test set
The positive results of this experience demon-
strated the viability of the adopted solutions, and
motivate our further investigation with Italian as a
new target language.
4 Porting TERSEO to Italian
Due to the separation between language-specific
recognition rules and language-independent nor-
malization rules, the bulk of the porting process
relies on the adaptation of the recognition rules
to the new target language. Taking advantage of
different knowledge sources (either alone or in
combination), an incremental approach has been
adopted, in order to determine the contribution of
additional information on the performance of the
resulting system for Italian.
8Altavista Babel Fish Translation has been used for this
purpose (http://world.altavista.com).
32
4.1 Using online translators
As a first experiment, the same procedure adopted
for the extension to English has been followed.
This represents the simplest approach for porting
TERSEO to other languages, and will be consid-
ered as a baseline for comparison with the results
achieved in further experiments. The only minor
difference with respect to the original procedure
is that now, since two aligned sets of recognition
rules (i.e. for Spanish and for English) are avail-
able, both models have been used. The reason for
considering both models is the fact that they com-
plement each other: on the one hand, the Span-
ish model was obtained manually and showed high
precision values in detection (88%); on the other
hand, although the English model showed lower
precision results in detection (77%), the on-line
translators from English to Italian perform better
than translators from Spanish to Italian.
The process is carried out in the following four
steps.
1. Eng-Ita translation. All the English TEs
known by the system are translated into Ital-
ian9. Starting English, the probability of ob-
taining higher quality translations is maxi-
mized.
2. Spa-Ita translation. For each English TE
without an Italian translation, the correspond-
ing Spanish expression is translated into Ital-
ian. Also the Spanish TEs that do not have an
English equivalent are translated from Span-
ish10 into Italian. This way, the coverage
of the resulting model is maximized, becom-
ing comparable to the hand-crafted Spanish
model.
3. TE Filtering. A filtering module is used to
guarantee the correctness of the translations.
For this purpose, the translated expressions
are searched in the Web with Google. If an
expression is not found by Google it is given
up; otherwise it is considered as a valid Ital-
ian TE. The inconvenience of adopting this
simple filtering strategy occurs in case of am-
biguous expressions, i.e. when a correct ex-
pression is obtained through translation, and
9Also for English to Italian translation, Altavista Babel
Fish Translation has been used
10Using the Spanish-Italian translator available at
http://www.tranexp.com:2000/Translate/result.shtml
Google returns at least on document contain-
ing it, but the expression is not a tempo-
ral one. In these cases the system will er-
roneously store in its database non-temporal
expressions. In this experiment the results
returned by Google have not been analyzed
(only the number of hits has been taken into
account), nor the impact of these errors has
been estimated. A more precise analysis of
the output of the web search has been left as
a future improvement direction.
4. Normalization rules assignment. Finally,
the resulting Italian translations are mapped
onto the language-independent normalization
rules associated with the original English and
Spanish TEs.
The development of this first automatic porting
procedure required one person/week for software
implementation, and less than an hour to obtain
the new model for Italian. The performance of the
resulting system, evaluated over the test set of I-
CAB, is shown in table 2.
Prec Rec F
timex2 0.725 0.833 0.775
anchor dir 0.211 0.593 0.311
anchor val 0.203 0.571 0.300
set 0.152 1.000 0.263
text 0.217 0.249 0.232
val 0.364 0.351 0.357
Table 2: Porting to Italian based on translations
The results achieved by the translation-based
approach are controversial. On the one hand, we
observe a detection performance in line with the
English version of the system. The timex2 at-
tribute, which indicates the proportion of detected
TEs11, has even higher scores, both in terms of
precision (+5%) and recall (+11%), with respect
to the English system. On the other hand, both
bracketing (see the text attribute, which indicates
the quality of extent recognition) and normaliza-
tion (described by the other attributes) show a per-
formance drop. Unfortunately, the reasons of this
drop are still unclear. One possible explanation
is that, due to the intrinsic difficulties presented
by the Italian language, the translation-based ap-
proach falls short from providing an adequate cov-
erage of the many possible TE variants. While
11At least one overlapping character in the extent of the
reference and the system output is required for tag aligment.
33
the presence of lexical triggers denoting a TE ap-
pearing in a text (e.g. the Italian translations of
?years?, ?Monday?, ?afternoon?, ?yesterday?) can
be easily captured by this approach, the complex-
ity of many language-specific constructs is out of
its reach.
4.2 Using an annotated corpus
In a second experiment, the annotations of the
training portion of I-CAB have been used as a pri-
mary knowledge source. The main purpose of this
approach is to maximize the coverage of the Ital-
ian TEs, starting from language-specific knowl-
edge mined from the corpus. The basic hypothe-
sis is that a bottom-up porting methodology, led by
knowledge in the target language, is more effective
than the top-down approach based on knowledge
derived from models built for other languages.
The former, in fact, is in principle more suitable to
capture language-specific TE variations. In order
to test the validity of ths hypothesis, the following
two-step process has been set up:
1. TE Collection and translation. The Italian ex-
pressions are collected from the I-CAB train-
ing portion, and translated both into Spanish
and English.
2. Normalization rules assignment. Italian TEs
are assigned to the appropriate normalization
rules. For each Italian TE mined from the
corpus, the selection is done considering the
normalization rules assigned to its transla-
tions. If both the Spanish and English ex-
pressions are found in their respective mod-
els, and are associated with the same normal-
ization rule, then this rule is assigned also to
the Italian expression. Also, when only one
of the translated expressions is found in the
existing models, the normalization rule is as-
signed. In case of discrepancies, i.e. if both
expressions are found, but are not associated
to the same normalization rule, then one of
the languages must be prioritized. Since the
manually obtained Spanish model has shown
a higher precision, Spanish rules are pre-
ferred.
As the corpus-based approach is mostly built on
the same software used for the translation-based
porting procedure, it did not require additional
time for implementation. Also in this case, the
new model for Italian has been obtained in less
than one hour. Performance results calculated over
the I-CAB test set are reported in Table 3.
Prec Rec F
timex2 0.730 0.839 0.781
anchor dir 0.412 0.414 0.413
anchor val 0.339 0.340 0.339
set 0.030 1.000 0.059
text 0.222 0.255 0.238
val 0.285 0.274 0.279
Table 3: Porting based on corpus annotations
These results partially confirm our working hy-
pothesis, showing a performance increase in terms
of the Italian TEs correctly recognized by the sys-
tem. In fact, both the timex2 attribute, which
indicates the coverage of the system (detection),
and the text attribute, which refers to the TEs
extent determination (bracketing), are slightly in-
creased. This may lead to the conclusion that auto-
matic porting procedures can actually benefit from
language-specific knowledge derived from a cor-
pus.
However, looking at the other TIMEX2 at-
tributes, the situation is not so clear due to the less
coherent behaviour of the system on normaliza-
tion. While for two attributes (anchor dir and an-
chor val) the system performs better, for the other
two (set and val) a performance drop is observed.
A possible reason for that could be related to the
limited number of TE examples that can be ex-
tracted from the Italian corpus (whose dimensions
are relatively small compared to the annotated cor-
pora available for English). In fact, compared to
the sum of English and Spanish examples used for
the translation-based porting procedure, the Ital-
ian expressions present in the corpus are fewer and
repetitive. For instance, with 131, 140, and 30 oc-
currences, the expressions ?oggi? (?today?), ?ieri?
(?yesterday?), and ?domani? (?tomorrow?) repre-
sent around 12.5% of the 2,393 Italian TEs con-
tained in the I-CAB training set.
4.3 Combining online translators and an
annotated corpus
In light of the previous considerations, a third ex-
periment has been conducted combining the top-
down approach proposed in Section 4.1 and the
bottom-up approach proposed in Section 4.2. The
underlying hypothesis is that the induction of an
effective temporal model for Italian can bene-
fit from the combination of the large amount of
examples coming from translations on the one
34
side, and from the more precise language-specific
knowledge derived from the corpus on the other.
To check the validity of this hypothesis, the pro-
cess described in Section 4.2 has been modified
adding an additional phase. In this phase, the set
of TEs derived from I-CAB is augmented with the
expressions already available in the Spanish and
English TE sets. The new porting process is car-
ried out in the following steps:
1. TE Collection and translation. The Italian ex-
pressions are collected from the I-CAB train-
ing portion, and translated both into Spanish
and English.
2. Normalization rules assignment. With the
same methodology described in Section 4.2
(step 2), the Italian TEs mined from the cor-
pus are mapped onto the appropriate normal-
ization rules assigned to their translations.
3. TE set augmentation. The set of Italian TEs
is automatically augmented with new expres-
sions derived from the Spanish and English
TE sets. As described in Section 4.1, these
expressions are first translated into Italian us-
ing on-line translators, then filtered through
Web searches. The remaining TEs are in-
cluded in the Italian model, and related to the
same normalization rules assigned to the cor-
responding Spanish or English TEs.
Also this porting experiment was carried out
with minimal modifications of the existing code.
The automatic acquisition of the new model for
Italian required around one hour. Evaluation re-
sults, calculated over the I-CAB test set are pre-
sented in Table 4.
Prec Rec F
timex2 0.726 0.834 0.776
anchor dir 0.578 0.475 0.521
anchor val 0.516 0.424 0.465
set 0.182 1.000 0.308
text 0.258 0.296 0.276
val 0.564 0.545 0.555
Table 4: Porting based on corpus annotations and
online translators
As can be seen from the table, the combina-
tion of the two approaches leads to an overall per-
formance improvement with respect to the previ-
ous experiments. Apart from a slight decrease in
terms of detection (timex2 attribute), both brack-
eting and normalization performance benefit from
such combination. The improvement on bracket-
ing (text attribute) is around 4% with respect to
both the previous experiments. On average, the
improvement for the normalization attributes is
around 15% with respect to the translation-based
method (ranging from +4,5% for the set attribute,
to +20% for the val attribute), and 20% with re-
spect to the corpus-based method (ranging from
+11% for the anchor dir attribute, to +30% for
the set attribute). These performance improve-
ments are summarized in Table 5, which reports
the F-Measure scores achieved by the three port-
ing approaches.
F-Tran. F-Corpus F-Comb.
timex2 0.775 0.781 0.776
anchor dir 0.311 0.413 0.521
anchor val 0.300 0.339 0.465
set 0.152 0.059 0.308
text 0.263 0.238 0.276
val 0.232 0.279 0.555
Table 5: F-Measure scores comparison
These results confirm the validity of our work-
ing hypothesis, showing that:
? taken in isolation, both the knowledge de-
rived from models built for other languages,
and the language-specific knowledge derived
from an annotated corpus, have a limited im-
pact on the system?s performance;
? taken in combination, the top-down and the
bottom-up approaches can complement each
other, allowing to cope with the complexity
of the porting task.
5 Comparing TERSEO with a
language-specific system
For the sake of completeness, the results achieved
by our combined porting procedure have been
compared with those achieved, over the I-CAB
test set, by a system specifically designed for
Italian. The ITA-Chronos system (Negri and
Marseglia, 2004), a multilingual system for the
recognition and normalization of TEs in Italian
and English, has been used for this purpose. Up to
date, being among the two top performing systems
at TERN 2004, Chronos represents the state-of-
the-art with respect to the TERN task. In addition,
to the best of our knowledge, this is the only sys-
tem effectively dealing with the Italian language.
35
Like all the other state-of-the-art systems ad-
dressing the recognition/normalization task, ITA-
Chronos is a rule-based system. From a design
point of view, it shares with TERSEO a rather
similar architecture which relies on different sets
of rules. These are regular expressions that check
for specific features of the input text, such as the
presence of particular word senses, lemmas, parts
of speech, symbols, or strings satisfying specific
predicates12 . Each set of rules is in charge of
dealing with different aspects of the problem. In
particular, a set of around 350 rules is designed
for TE recognition and is capable of recognizing
with high Precision/Recall rates a broad variety of
TEs. Other sets of regular expressions, for a total
of around 700 rules, are used in the normalization
phase, and are in charge of handling each specific
TIMEX2 normalization attribute. The results ob-
tained by the Italian version of Chronos over the
I-CAB test set are shown in Table 6.
Prec Rec F F-Comb
timex2 0.925 0.908 0.917 0.776 (-14%)
anchor dir 0.733 0.636 0.681 0.521 (-16%)
anchor val 0.495 0.462 0.478 0.465 (-1.3%)
set 0.616 0.500 0.552 0.308 (-24%)
text 0.859 0.843 0.851 0.276 (-57%)
val 0.636 0.673 0.654 0.555 (-10%)
Table 6: Evaluation of ITA-Chronos over the I-
CAB test set
As expected, the distance between the results
obtained by ITA-Chronos and the best Italian sys-
tem automatically obtained from TERSEO (F-
Comb) is considerable. On average, in terms of
F-Measure, the scores obtained by ITA-TERSEO
are 20% lower, ranging from -1.3% for the an-
chor val attribute, to -57% for the text attribute.
However, going beyond the raw numbers, a com-
prehensive evaluation must also take into account
the great difference, in terms of the required time,
effort, and resources deployed in the development
of the two systems. While the implementation of
the manual one took several months, the automatic
porting procedure of TERSEO to Italian (in all the
three modalities described in this paper) is a very
fast process that can be accomplished in less than
an hour. Considering the trade-off between per-
formance and effort required for system?s devel-
12For instance, the predicates ?Weekday-p? and
?Time Unit-p? are respectively satisfied by strings such
as ?Monday?, ?Tuesday?, ..., ?Sunday?, and ?second?,
?minute?, ?hour?, ?day?, ..., ?century?. Of course, this also
holds for the Italian equivalents of these expressions
opment, the proposed methodology represents a
viable solution to attack the porting problem.
6 Conclusions
In this paper, the problem of automatically extend-
ing to new languages a rule-based system for TE
recognition and normalization has been addressed.
Adopting an incremental approach, different port-
ing strategies, for the creation of an Italian system
starting from an already available Spanish system,
have been evaluated and discussed. Each exper-
iment has been carried out considering the con-
tribution of different knowledge sources for rules
translation. Firstly, the contribution given by the
output of online translators has been evaluated,
showing detection performances in line with a pre-
viously developed English extension of the sys-
tem, but a performance drop in terms of normal-
ization performance. Then, the contribution of
knowledge mined from an annotated corpus has
been considered. Results show a performance in-
crease in terms of detection and bracketing, but
a less coherent behaviour in terms of normaliza-
tion. Finally, a combined approach has been ex-
perimented, resulting in an overall performance
increase. System?s performance is still far from
the results obtained by a state-of-the-art system for
Italian but, considering the trade-off between per-
formance and effort required for system?s devel-
opment, results are encouraging.
References
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. Tides.2005 standard for the annotation
of temporal expressions. Technical report, MITRE.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Time
Expression Labeling for English and Chinese Text.
In Proceedings of CICLing 2005, pages 548?559.
B. Magnini, E. Pianta, C. Girardi, M. Negri, L. Ro-
mano, M. Speranza, and R. Sprugnoli. 2006. I-
CAB: the Italian Content Annotation Bank. In Pro-
ceedings of LREC 2006. To appear.
M. Negri and L. Marseglia. 2004. Recognition and
normalization of time expressions: Itc-irst at tern
2004. Technical report, ITC-irst, Trento.
E. Saquete, P. Martnez-Barco, and R. Muoz. 2004.
Evaluation of the automatic multilinguality for time
expression resolution. In DEXA Workshops, pages
25?30. IEEE Computer Society.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2005.
Event ordering using TERSEO system. Data and
36
Knowledge Engineering Journal, page (To be pub-
lished).
E. Saquete, P. Martinez-Barco, R. Munoz R., M. Ne-
gri, M. Speranza, and R. Sprugnoli R. 2006a. Au-
tomatic resolution rule assignment to multilingual
temporal expressions using annotated corpora. In
Proceedings of the TIME 2006 International Sym-
posium on Temporal Representation and Reasoning.
To Appear.
E. Saquete, P. Martinez-Barco, R. Munoz R., M. Negri,
M. Speranza, and R. Sprugnoli R. 2006b. Multilin-
gual Extension of a Temporal Expression Normal-
izer using Annotated Corpora. In Proceedings of the
EACL Workshop on Cross-Language Knowledge In-
duction.
E. Saquete. 2005. Temporal information Resolution
and its application to Temporal Question Answer-
ing. Phd, Departamento de Lenguages y Sistemas
Informa?ticos. Universidad de Alicante, June.
37
Multilingual Extension of a Temporal Expression Normalizer using
Annotated Corpora
E. Saquete P. Mart??nez-Barco R. Mun?oz
gPLSI
DLSI. UA
Alicante, Spain
fstela,patricio,rafaelg@dlsi.ua.es
M. Negri M. Speranza
ITC-irst
Povo (TN), Italy
fnegri,mansperag@itc.it
R. Sprugnoli
CELCT
Trento, Italy
sprugnoli@celct.it
Abstract
This paper presents the automatic exten-
sion to other languages of TERSEO, a
knowledge-based system for the recogni-
tion and normalization of temporal ex-
pressions originally developed for Span-
ish1. TERSEO was first extended to En-
glish through the automatic translation of
the temporal expressions. Then, an im-
proved porting process was applied to Ital-
ian, where the automatic translation of
the temporal expressions from English and
from Spanish was combined with the ex-
traction of new expressions from an Ital-
ian annotated corpus. Experimental re-
sults demonstrate how, while still adher-
ing to the rule-based paradigm, the devel-
opment of automatic rule translation pro-
cedures allowed us to minimize the ef-
fort required for porting to new languages.
Relying on such procedures, and without
any manual effort or previous knowledge
of the target language, TERSEO recog-
nizes and normalizes temporal expressions
in Italian with good results (72% precision
and 83% recall for recognition).
1 Introduction
Recently, the Natural Language Processing com-
munity has become more and more interested
in developing language independent systems,
in the effort of breaking the language barrier
hampering their application in real use scenar-
ios. Such a strong interest in multilingual-
ity is demonstrated by the growing number of
1This research was partially funded by the Spanish Gov-
ernment (contract TIC2003-07158-C04-01)
international conferences and initiatives plac-
ing systems? multilingual/cross-language capabil-
ities among the hottest research topics, such as
the European Cross-Language Evaluation Forum2
(CLEF), a successful evaluation campaign which
aims at fostering research in different areas of
multilingual information retrieval. At the same
time, in the temporal expressions recognition and
normalization field, systems featuring multilin-
gual capabilities have been proposed. Among
others, (Moia, 2001; Wilson et al, 2001; Negri
and Marseglia, 2004) emphasized the potentiali-
ties of such applications for different information
retrieval related tasks.
As many other NLP areas, research in auto-
mated temporal reasoning has recently seen the
emergence of machine learning approaches trying
to overcome the difficulties of extending a lan-
guage model to other languages (Carpenter, 2004;
Ittycheriah et al, 2003). In this direction, the out-
comes of the first Time Expression Recognition
and Normalization Workshop (TERN 20043) pro-
vide a clear indication of the state of the field. In
spite of the good results obtained in the recog-
nition task, normalization by means of machine
learning techniques still shows relatively poor re-
sults with respect to rule-based approaches, and
still remains an unresolved problem.
The difficulty of porting systems to new lan-
guages (or domains) affects both rule-based and
machine learning approaches. With rule-based ap-
proaches (Schilder and Habel, 2001; Filatova and
Hovy, 2001), the main problems are related to
the fact that the porting process requires rewriting
from scratch, or adapting to each new language,
large numbers of rules, which is costly and time-
2http://www.clef-campaign.org/
3http://timex2.mitre.org/tern.html
1
consuming work. Machine learning approaches
(Setzer and Gaizauskas, 2002; Katz and Arosio,
2001), on the other hand, can be extended with
little human intervention through the use of lan-
guage corpora. However, the large annotated cor-
pora that are necessary to obtain high performance
are not always available. In this paper we describe
a new procedure to build temporal models for new
languages, starting from previously defined ones.
While still adhering to the rule-based paradigm, its
main contribution is the proposal of a simple, but
effective, methodology to automate the porting of
a system from one language to another. In this pro-
cedure, we take advantage of the architecture of an
existing system developed for Spanish (TERSEO,
see (Saquete et al, 2005)), where the recognition
model is language-dependent but the normalizing
procedure is completely independent. In this way,
the approach is capable of automatically learning
the recognition model by adjusting the set of nor-
malization rules.
The paper is structured as follows: Section 2
provides a short overview of TERSEO; Section 3
describes the automatic extension of the system to
Italian; Section 4 presents the results of our evalu-
ation experiments, comparing the performance of
Ita-TERSEO (i.e. our extended system) with the
performance of a state of the art system for Italian.
2 The TERSEO system architecture
TERSEO has been developed in order to automat-
ically recognize temporal expressions (TEs) ap-
pearing in a Spanish written text, and normalize
them according to the temporal model proposed
in (Saquete, 2005), which is compatible with the
ACE annotation standards for temporal expres-
sions (Ferro et al, 2005). As shown in Figure 1,
the first step (recognition) includes pre-processing
of the input texts, which are tagged with lexical
and morphological information that will be used
as input to the temporal parser. The temporal
parser is implemented using an ascending tech-
nique (chart parser) and is based on a temporal
grammar. Once the parser has recognized the TEs
in an input text, these are passed to the normaliza-
tion unit, which updates the value of the reference
according to the date they refer to, and generates
the XML tags for each expression.
As TEs can be categorized as explicit and im-
plicit, the grammar used by the parser is tuned for
discriminating between the two groups. On the
TEXT
POS 
TAGGER
RECOGNITION: 
PARSER
Lexical and
morphological
information
Temporal 
expression
recognition
DATE
ESTIMATION
Dictionary
Temporal
Expression
Grammar
TEMPORAL
EXPRESSION
NORMALIZATION
EVENT 
ORDERING
ORDERED
TEXT
Documental 
DataBase
Figure 1: Graphic representation of the TERSEO
architecture.
one hand, explicit temporal expressions directly
provide and fully describe a date which does not
require any further reasoning process to be inter-
preted (e.g. ?1st May 2005?, ?05/01/2005?). On
the other hand, implicit (or anaphoric) time ex-
pressions (e.g. ?yesterday?, ?three years later?)
require some degree of reasoning (as in the case
of anaphora resolution). In order to translate such
expressions into explicit dates, such reasoning ca-
pabilities consider the information provided by the
lexical context in which they occur (see (Saquete,
2005) for a thorough description of the reasoning
techniques used by TERSEO).
2.1 Recognition using a temporal expression
parser
The parser uses a grammar based on two differ-
ent sets of rules. The first set of rules is in charge
of date and time recognition (i.e. explicit dates,
such as ?05/01/2005?). For this type of TEs, the
grammar adopted by TERSEO recognizes a large
number of date and time formats (see Table 1 for
some examples).
The second set of rules is in charge of the recog-
nition of the temporal reference for implicit TEs,
2
fecha! dd+?/?+mm+?/?+(yy)yy (12/06/1975)
(06/12/1975)
fecha! dd+?-?+mes+?-?+(yy)yy (12-junio-1975)
(12-Jun.-1975)
fecha! dd+?de?+mm+?de?+(yy)yy (12 de junio de 1975)
Table 1: Sample of rules for Explicit Dates Recognition.
reference! ?ayer? (yesterday)
Implicit dates reference! ?man?ana? (tomorrow)
referring to Document Date reference! ?anteayer? (the day before yesterdary)
Concrete reference! ?el pro?ximo d??a? (the next day)
Implicit Dates reference! ?un mes despue?s? (a month later)
Previous Date Period reference! num+?an?os despue?s?(num years later)
Imp. Dates Prev.Date Concrete reference! ?un d??a antes? (a day before)
Implicit Dates reference! ?d??as despue?s? (some days later)
Previous Date Fuzzy reference! ?d??as antes? (some days before)
Table 2: Sample of rules for Implicit Dates recognition.
i.e. TEs that need to be related to an explicit TE
to be interpreted. These can be divided into time
adverbs (e.g. ?yesterday?, ?tomorrow?), and nom-
inal phrases that are referring to temporal relation-
ships (e.g. ?three years later?, ?the day before?).
Table 2 shows some of the rules used for the de-
tection of these kinds of references.
2.2 Normalization
When the system finds an explicit temporal ex-
pression, the normalization process is direct as no
resolution of the expression is necessary. For im-
plicit expressions, an inference engine that inter-
prets every reference previously found in the input
text is used. In some cases references are solved
using the newspaper?s date (FechaP). Other TEs
have to be interpreted by referring to a date named
before in the text that is being analyzed (FechaA).
In these cases, a temporal model that allows the
system to determine the reference date over which
the dictionary operations are going to be done, has
been defined. This model is based on the follow-
ing two rules:
1. The newspaper?s date, when available, is
used as a base temporal referent by default;
otherwise, the current date is used as anchor.
2. In case a non-anaphoric TE is found, it is
stored as FechaA. This value is updated ev-
ery time a non-anaphoric TE appears in the
text.
Table 3 shows some of the entries of the dictio-
nary used in the inference engine.
3 Extending TERSEO: from Spanish
and English to Italian
As stated before, the main purpose of this paper is
to describe a new procedure to automatically build
temporal models for new languages, starting from
previously defined models. In our case, an English
model has been automatically obtained from the
Spanish one through the automatic translation of
the Spanish temporal expressions to English. The
resulting system for the recognition and normal-
ization of English TEs obtains good results both
in terms of precision (P) and recall (R) (Saquete et
al., 2004). The comparison of the results between
the Spanish and the English system is shown in
Table 4.
SPANISH ENGLISH
DOCS 50 100
POS 199 634
ACT 156 511
CORRECT 138 393
INCORRECT 18 118
MISSED 43 123
P 88% 77%
R 69% 62%
F 77% 69%
Table 4: Comparison between Spanish TERSEO
and English TERSEO.
This section presents the procedure we followed
to extend our system to Italian, starting from the
Spanish and English models already available, and
a manually annotated corpus. In this case, both
models have been considered as they can be com-
plemented reciprocally. The Spanish model was
3
REFERENCE DICTIONARY ENTRY
?ayer? Day(FechaP)-1/Month(FechaP)/Year(FechaP)
(yesterday)
?man?ana? Day(FechaP)+1/Month(FechaP)/Year(FechaP)
(tomorrow)
?anteayer? Day(FechaP)-2/Month(FechaP)/Year(FechaP)
(the day before yesterday)
?el pro?ximo d??a? Day(FechaP)+1/Month(FechaP)/Year(FechaP)
(the next day)
?un mes despue?s? [DayI/Month(FechaA)+1/Year(FechaA)--
(a month later) DayF/Month(FechaA)+1/Year(FechaA)]
num+?an?os despue?s? [01/01/Year(FechaA)+num --
(num years later) 31/12/Year(FechaA)+num]
?un d??a antes? Day(FechaA)-1/Month(FechaA)/Year(FechaA)
(a day before)
?d??as despue?s? >>>>FechaA
(some days later)
?d??as antes? <<<<FechaA
(some days before)
Table 3: Normalization rules
manually obtained and evaluated showing high
scores for precision (88%), so better results could
be expected when it is used. However, in spite of
the fact that the English model has shown lower
results on precision (77%), the on-line transla-
tors between Italian and English have better re-
sults than Spanish to Italian translators. As a re-
sult, both models are considered in the following
steps for the multilingual extension:
 Firstly, a set of Italian temporal expressions
is extracted from an Italian annotated corpus
and stored in a database. The selected cor-
pus is the training part of I-CAB, the Ital-
ian Content Annotation Bank (Lavelli et al,
2005). More detailed information about I-
CAB is provided in Section 4.
 Secondly, the resulting set of Italian TEs
must be related to the appropriate normaliza-
tion rule. In order to do that, a double transla-
tion procedure has been developed. We first
translate all the expressions into English and
Spanish simultaneously; then, the normaliza-
tion rules related to the translated expressions
are obtained. If both the Spanish and En-
glish expressions are found in their respec-
tive models in agreement with the same nor-
malization rule, then this rule is also assigned
to the Italian expression. Also, when only
one of the translated expressions is found in
the existing models, the normalization rule
is assigned. In case of discrepancies, i.e. if
both expressions are found, but not coincid-
ing in the same normalization rule, then one
of the languages must be prioritized. As the
Spanish model was manually obtained and
has shown a higher precision, Spanish rules
are preferred. In other cases, the expression
is reserved for a manual assignment.
 Finally, the set is automatically augmented
using the Spanish and English sets of tem-
poral expressions. These expressions were
also translated into Italian by on-line ma-
chine translation systems (Spanish-Italian4 ,
English-Italian5). In this case, a filtering
module is used to guarantee that all the ex-
pressions were correctly translated. This
module searches the web with Google6 for
the translated expression. If the expression
is not frequently found, then the translation
is abandoned. After that, the new Italian ex-
pression is included in the model, and related
to the same normalization rule assigned to the
Spanish or English temporal expression.
The entire translation process has been com-
pleted with an automatic generalization process,
oriented to obtain generalized rules from the con-
crete cases that have been collected from the cor-
4http://www.tranexp.com:2000/Translate/result.shtml
5http://world.altavista.com/
6http://www.google.com/
4
pus. This generalization process has a double ef-
fect. On the one hand, it reduces the number of
recognition rules. On the other hand, it allows the
system to identify new expressions that were not
previously learned. For instance, the expression
?Dieci mesi dopo? (i.e. ?Ten months later?) could
be recognized if the expression ?Nove mesi dopo?
(i.e. Nine months later) was learned.
The multilingual extension procedure (Figure 3)
is carried out in three phases:
Spanish
Temporal 
Recognition
Model
Spanish-Italian
TRANSLATOR
TEs FILTER
KEYWORDS Unit
NEW TEs
FINDER
RULE
ASSIGNMENTS
Google
WordNet
Italian TEs
Italian TEs
Temporal  keywords
New Italian TEs
New Normalizer rule
Italian
Temporal 
Normalizer
Model
Online 
DictionariesITALIAN TEs
GRAMATICS 
Generator
English
Temporal 
Recognition
Model
Italian
I-CAB   
Corpus
English-Italian
TRANSLATOR
Italian-Spanish
TRANSLATOR
Italian-English
TRANSLATOR
Spanish
Temporal 
Normalizer
Model
English
Temporal 
Normalizer
Model
Italian TEs
Italian generalized TEs
Phase
1
Phase 
2 
Phase 
3
Figure 2: Multilingual extension procedure.
 Phase 1: TE Collection. During this phase,
the Italian temporal expressions are col-
lected from I-CAB (Italian Content Annota-
tion Bank), and the automatically translated
Italian TEs are derived from the set of Span-
ish and English TEs. In this case, the TEs
are filtered removing those not being found
by Google.
 Phase 2: TE Generalization. In this phase,
the TEs Gramatics Generator uses the mor-
phological and syntactical information from
the collected TEs to generate the grammat-
ical rules that generalize the recognition of
the TEs. Moreover, the keyword unit is able
to extract the temporal keywords that will be
used to build new TEs. These keywords are
augmented with their synonyms in WordNet
(Vossen, 2000) to generate new TEs.
 Phase 3: TE Normalizing Rule Assignment.
In the last phase, the translators are used to
relate the recognizing rule to the appropriate
normalization rule. For this purpose, the sys-
tem takes advantage of the previously defined
Spanish and English temporal models.
4 Evaluation
The automatic extension of the system to Italian
(Ita-TERSEO) has been evaluated using I-CAB,
which has been divided in two parts: training and
test. The training part has been used, first of all,
in order to automatically extend the system. Af-
ter this extension, the system was evaluated both
against the training and the test corpora. The pur-
pose of this double evaluation experiment was to
compare the recall obtained over the training cor-
pus with the value obtained over the test corpus.
An additional evaluation experiment has also
been carried out in order to compare the perfor-
mance of the automatically developed system with
a state of the art system specifically developed for
Italian and English, i.e. the Chronos system de-
scribed in (Negri and Marseglia, 2004).
In the following sections, more details about I-
CAB and the evaluation process are presented, to-
gether with the evaluation results.
4.1 The I-CAB Corpus
The evaluation has been performed on the tem-
poral annotations of I-CAB (I-CAB-temp) cre-
ated as part of the three-year project ONTOTEXT7
funded by the Provincia Autonoma di Trento.
I-CAB consists of 525 news documents
taken from the local newspaper L?Adige
(http://www.adige.it). The selected news sto-
ries belong to four different days (September, 7th
and 8th 2004 and October, 7th and 8th 2004) and
are grouped into five categories: News Stories,
Cultural News, Economic News, Sports News
and Local News. The corpus consists of around
182,500 words (on average 347 words per file).
The total number of annotated temporal expres-
sions is 4,553; the average length of a temporal
expression is 1.9 words.
The annotation of I-CAB has been carried out
adopting the standards developed within the ACE
program (Automatic Content Extraction8) for the
Time Expressions Recognition and Normalization
7http://tcc.itc.it/projects/ontotext
8http://www.nist.gov/speech/tests/ace
5
tasks, which allows for a semantically rich and
normalized annotation of different types of tempo-
ral expressions (for further details on the TIMEX2
annotation standard for English see (Ferro et al,
2005)).
The ACE guidelines have been adapted to
the specific morpho-syntactic features of Italian,
which has a far richer morphology than English.
In particular, some changes concerning the exten-
sion of the temporal expressions have been in-
troduced. According to the English guidelines,
in fact, definite and indefinite articles are consid-
ered as part of the textual realization of an entity,
while prepositions are not. As the annotation is
word-based, this does not account for Italian artic-
ulated prepositions, where a definite article and a
preposition are merged. Within I-CAB, this type
of preposition has been included as possible con-
stituents of an entity, so as to consistently include
all the articles.
An assessment of the inter-annotator agreement
based on the Dice coefficient has shown that the
task is a well-defined one, as the agreement is
95.5% for the recognition of temporal expressions.
4.2 Evaluation process
The evaluation of the automatic extension of
TERSEO to Italian has been performed in three
steps. First of all, the system has been evaluated
both against the training and the test corpora with
two main purposes:
 Determining if the recall obtained in the eval-
uation of the training part of the corpus is a
bit higher than the one obtained in the eval-
uation of the test part of I-CAB, due to the
fact that in the TE collection phase of the ex-
tension, temporal expressions were extracted
from this part of the corpus.
 Determining the performance of the automat-
ically extended system without any manual
revision of both the Italian translations and
the resolution rules automatically related to
the expressions.
Secondly, we were also interested in verifying
if the performance of the system in terms of pre-
cision could be improved through a manual revi-
sion of the automatically translated temporal ex-
pressions.
Finally, a comparison with a state of the art sys-
tem for Italian has been carried out in order to es-
timate the real potentialities of the proposed ap-
proach. All the evaluation results are compared
and presented in the following sections using the
same metrics adopted at the TERN2004 confer-
ence.
4.2.1 Evaluation of Ita-TERSEO
In the automatic extension of the system, a to-
tal of 1,183 Italian temporal expressions have been
stored in the database. As shown in Table 5, these
expressions have been obtained from the different
resources available:
 ENG ITA: This group of expressions has
been obtained from the automatic translation
into Italian of the English Temporal Expres-
sions stored in the knowledge DB.
 ESP ITA: This group of expressions has been
obtained from the automatic translation into
Italian of the Spanish Temporal Expressions
stored in the knowledge DB.
 CORPUS: This group of expressions has
been extracted directly from the training part
of the I-CAB corpus.
Source N %
ENG ITA 593 50.1
ESP ITA 358 30.3
CORPUS 232 19.6
TOTAL TEs 1183 100.0
Table 5: Italian TEs in the Knowledge DB.
Both the training part and the test part of I-CAB
have been used for evaluation. The results of pre-
cision (P), recall (R) and F-Measure (F) are pre-
sented in Table 6, which provides details about the
system performance over the general recognition
task (timex2), and the different normalization at-
tributes used by the TIMEX2 annotation standard.
As expected, recall performance over the train-
ing corpus is slightly higher. However, although
the temporal expressions have been extracted from
such corpus, in the automatic process of obtain-
ing the normalization rules for these expressions,
some errors could have been introduced.
Comparing these results with those obtained by
the automatic extension of TERSEO to English
and taking into account the recognition task (see
Table 4), precision (P) is slightly better for En-
glish (77% Vs. 72%) whereas recall (R) is better
in the Italian extension (62% Vs. 83%). This is
6
Ita-TERSEO: TRAINING Ita-TERSEO: TEST Chronos: TEST
Tag P R F P R F P R F
timex2 0.694 0.848 0.763 0.726 0.834 0.776 0.925 0.908 0.917
anchor dir 0.495 0.562 0.526 0.578 0.475 0.521 0.733 0.636 0.681
anchor val 0.464 0.527 0.493 0.516 0.424 0.465 0.495 0.462 0.478
set 0.308 0.903 0.459 0.182 1.000 0.308 0.616 0.5 0.552
text 0.265 0.324 0.292 0.258 0.296 0.276 0.859 0.843 0.851
val 0.581 0.564 0.573 0.564 0.545 0.555 0.636 0.673 0.654
Table 6: Results obtained over I-CAB by Ita-TERSEO and Chronos.
due to the fact that in the Italian extension, more
temporal expressions have been covered with re-
spect to the English extension. In this case, in
fact, Ita-TERSEO is not only using the temporal
expressions translated from the English or Spanish
knowledge database, but also the temporal expres-
sions extracted from the training part of I-CAB.
4.2.2 Manual revision of the acquired TEs
A manual revision of the Italian TEs stored in
the Knowledge DB has been done in two steps.
First of all, the incorrectly translated expressions
(from Spanish and English to Italian) were re-
moved from the database. A total of 334 expres-
sions were detected as wrong translated expres-
sions. After this, another revision was performed.
In this case, some expressions were modified be-
cause the expressions have some minor errors in
the translation. 213 expressions were modified in
this second revision cycle. Moreover, since pattern
constituents in Italian might have different ortho-
graphical features (e.g. masculine/feminine, ini-
tial vowel/consonant, etc.), new patterns had to be
introduced to capture such variants. For exam-
ple, as months? names in Italian could start with
a vowel, the temporal expression pattern ?nell?-
MONTH? has been inserted in the Knowledge
DB. After these changes, the total amount of ex-
pressions stored in the DB are shown in Table 7.
Source N %
ING ITA 416 47.9
ESP ITA 201 23.1
CORPUS 232 26.7
REV MAN 20 2.3
TOTAL TEs 869 100.0
Table 7: Italian TEs in the Knowledge DB after
manual revision.
In order to evaluate the system after this manual
revision, the training and the test part of I-CAB
have been used. However, the results of preci-
sion (PREC), recall (REC) and F-Measure were
exactly the same as presented in Table 6. That
is not really surprising. The existence of wrong
expressions in the knowledge database does not
affect the final results of the system, as they will
never be used for recognition or resolution. This
is because these expressions will not appear in real
documents, and are redundant as the correct ex-
pression is also stored in the Knowledge DB.
4.2.3 Comparing Italian TERSEO with a
language-specific system
Finally, in order to compare Ita-TERSEO with
a state of the art system specifically designed for
Italian, we chose Chronos (Negri and Marseglia,
2004), a multilingual system for the recognition
and normalization of TEs in Italian and English.
Like all the other state of the art systems address-
ing the recognition/normalization task, Chronos
is a rule-based system. From a design point of
view, it shares with TERSEO a rather similar ar-
chitecture which relies on different sets of rules.
These are regular expressions that check for spe-
cific features of the input text, such as the pres-
ence of particular word senses, lemmas, parts
of speech, symbols, or strings satisfying specific
predicates. Each set of rules is in charge of deal-
ing with different aspects of the problem. In
particular, a set of around 350 rules is designed
for TE recognition and is capable of recogniz-
ing with high Precision/Recall rates both explicit
and implicit TEs. Other sets of regular expres-
sions, for a total of around 700 rules, are used
in the normalization phase, and are in charge of
handling a specific TIMEX2 attribute (i.e. VAL,
SET, ANCHOR VAL, and ANCHOR DIR). The
results obtained by the Italian version of Chronos
over the test part of I-CAB are shown in the last
three columns of Table 6.
As expected, the distance between the results
obtained by the two systems is considerable. How-
ever, the following considerations should be taken
into account. First, there is a great difference, both
7
in terms of the required time and effort, in the de-
velopment of the two systems. While the imple-
mentation of the manual one took several months,
the porting procedure of TERSEO to Italian is a
very fast process that can be accomplished in less
than an hour. Second, even if an annotated corpus
for a new language is not available, the automatic
porting procedure we present still remains feasi-
ble. In fact, most of the TEs for a new language
that are stored in the Knowledge DB are the result
of the translation of the Spanish/English TEs into
such a target language. In our case, as shown in
Table 5, more than 80% of the acquired Italian TEs
result from the automatic translation of the expres-
sions already stored in the DB. This makes the pro-
posed approach a viable solution which allows for
a rapid porting of the system to other languages,
while just requiring an on-line translator (note that
the Altavista Babel Fish translator9 provides trans-
lations from English to 12 target languages). In
light of these considerations, the results obtained
by Ita-TERSEO are encouraging.
5 Conclusions
In this paper we have presented an automatic ex-
tension of a rule-based approach to TEs recogni-
tion and normalization. The procedure is based
on building temporal models for new languages
starting from previously defined ones. This proce-
dure is able to fill the gap left by machine learning
systems that, up to date, are still far from provid-
ing acceptable performance on this task. As re-
sults illustrate, the proposed methodology (even
though with a lower performance with respect to
language-specific systems) is a viable and effec-
tive solution for a rapid and automatic porting of
an existing system to new languages.
References
B. Carpenter. 2004. Phrasal Queries with LingPipe
and Lucene. In 13th Text REtrieval Conference,
NIST Special Publication. National Institute of Stan-
dards and Technology.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 Standard for the annotation
of temporal expressions. Technical report, MITRE.
E. Filatova and E. Hovy. 2001. Assigning time-stamps
to event-clauses. In ACL, editor, Proceedings of the
2001 ACL Workshop on Temporal and Spatial Infor-
mation Processing, pages 88?95, Toulouse, France.
9http://world.altavista.com/
A. Ittycheriah, L.V. Lita, N. Kambhatla, N. Nicolov,
S. Roukos, and M. Stys. 2003. Identifying and
Tracking Entity Mentions in a Maximum Entropy
Framework. In ACL, editor, Proceedings of the
NAACL Workshop WordNet and Other Lexical Re-
sources: Applications, Extensions and Customiza-
tions.
G. Katz and F. Arosio. 2001. The annotation of tem-
poral information in natural language sentences. In
ACL, editor, Proceedings of the 2001 ACL Work-
shop on Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France.
A. Lavelli, B. Magnini, M. Negri, E. Pianta, M. Sper-
anza, and R. Sprugnoli. 2005. Italian Content An-
notation Bank (I-CAB): Temporal expressions (v.
1.0.): T-0505-12. Technical report, ITC-irst, Trento.
T. Moia. 2001. Telling apart temporal locating adver-
bials and time-denoting expressions. In ACL, editor,
Proceedings of the 2001 ACL Workshop on Tempo-
ral and Spatial Information Processing, Toulouse,
France.
M. Negri and L. Marseglia. 2004. Recognition and
normalization of time expressions: Itc-irst at TERN
2004. Technical report, ITC-irst, Trento.
E. Saquete, P. Mart??nez-Barco, and R. Mun?oz. 2004.
Evaluation of the automatic multilinguality for time
expression resolution. In DEXA Workshops, pages
25?30. IEEE Computer Society.
E. Saquete, R. Mun?oz, and P. Mart??nez-Barco. 2005.
Event ordering using terseo system. Data and
Knowledge Engineering Journal, page (To be pub-
lished).
E. Saquete. 2005. Temporal information Resolution
and its application to Temporal Question Answer-
ing. Phd, Departamento de Lenguages y Sistemas
Informa?ticos. Universidad de Alicante, June.
F. Schilder and C. Habel. 2001. From temporal expres-
sions to temporal information: Semantic tagging of
news messages. In ACL, editor, Proceedings of the
2001 ACL Workshop on Temporal and Spatial Infor-
mation Processing, pages 65?72, Toulouse, France.
A. Setzer and R. Gaizauskas. 2002. On the impor-
tance of annotating event-event temporal relations
in text. In LREC, editor, Proceedings of the LREC
Workshop on Temporal Annotation Standards, 2002,
pages 52?60, Las Palmas de Gran Canaria,Spain.
P. Vossen. 2000. EuroWordNet: Building a Multilin-
gual Database with WordNets in 8 European Lan-
guages. The ELRA Newsletter, 5(1):9?10.
G. Wilson, I. Mani, B. Sundheim, and L. Ferro. 2001.
A multilingual approach to annotating and extract-
ing temporal information. In ACL, editor, Pro-
ceedings of the 2001 ACL Workshop on Temporal
and Spatial Information Processing, pages 81?87,
Toulouse, France.
8
Coling 2010: Poster Volume, pages 27?35,
Beijing, August 2010
Going Beyond Traditional QA Systems: Challenges and Keys 
in Opinion Question Answering 
Alexandra Balahur 
Dept. of Software and Computing Systems  
University of Alicante 
abalahur@dlsi.ua.es 
Ester Boldrini  
Dept. of Software and Computing Systems  
University of Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
Dept. of Software and Computing Systems  
University of Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
Dept. of Software and Computing Systems  
University of Alicante 
patricio@dlsi.ua.es 
Abstract  
The treatment of factual data has been 
widely studied in different areas of Nat-
ural Language Processing (NLP). How-
ever, processing subjective information 
still poses important challenges. This 
paper presents research aimed at assess-
ing techniques that have been suggested 
as appropriate in the context of subjec-
tive - Opinion Question Answering 
(OQA). We evaluate the performance of 
an OQA with these new components 
and propose methods to optimally tackle 
the issues encountered. We assess the 
impact of including additional resources 
and processes with the purpose of im-
proving the system performance on two 
distinct blog datasets. The improve-
ments obtained for the different combi-
nation of tools are statistically signifi-
cant. We thus conclude that the pro-
posed approach is adequate for the OQA 
task, offering a good strategy to deal 
with opinionated questions. 
1 Introduction 
The State of the Blogosphere 2009 survey pub-
lished by Technorati 1 concludes that in the past 
years the blogosphere has gained a high influ-
ence on a high variety of topics, ranging from 
cooking and gardening, to economics, politics 
and scientific achievements. The development 
                                                 
1 http://technorati.com/ 
of the Social Web and the new communication 
frameworks also influenced the way informa-
tion is transmitted through communities. Blogs 
are part of the so-called new textual genres. 
They have distinctive features when compared 
to the traditional ones, such as newspaper ar-
ticles. Blog language contains formal and in-
formal expressions, and other elements, as re-
peated punctuation or emoticons (used to stress 
upon different text elements). With the growth 
in the content of the blogosphere, the quantity 
of subjective data of the Web is increasing ex-
ponentially (Cui et al, 2006). As it is being up-
dated in real-time, this data becomes a source of 
timely information on many topics, exploitable 
by different applications. In order to properly 
manage the content of this subjective informa-
tion, its processing must be automated. The 
NLP task, which deals with the classification of 
opinionated content is called Sentiment Analy-
sis (SA). Research in this field aims at discover-
ing appropriate mechanisms to properly re-
trieve, extract and classify opinions expressed in 
text. While techniques to retrieve objective in-
formation have been widely studied, imple-
mented and evaluated, opinion-related tasks still 
represent an important challenge. As a conse-
quence, the aim of our research is to study, im-
plement and evaluate appropriate methods for 
the task of Question Answering (QA) in the 
opinion treatment framework.  
2 Motivation and Contribution 
Research in opinion-related tasks gained impor-
tance in the past years. However, there are still 
many aspects that require analysis and im-
27
provement, especially for approaches that com-
bine SA with other NLP tasks such as QA or 
automatic summarization. The TAC 2008 Opi-
nion Pilot task and the subsequent research per-
formed on the competition data have demon-
strated that answering opinionated questions 
and summarizing subjective information are 
significantly different from the equivalent tasks 
in the same context, but dealing with factual 
data.  This finding was confirmed by the recent 
work by (Kabadjov et al, 2009). The first moti-
vation of our work is the need to detect and ex-
plore the challenges raised by opinion QA 
(OQA), as compared to factual QA. To this aim, 
we analyze the improvements that can be 
brought at the different steps of the OQA 
process: question treatment (identification of 
expected polarity ? EPT, expected source ? ES 
and expected target ?ET-), opinion retrieval (at 
the level of one and three-sentences long snip-
pets, using topic-related words or using paraph-
rases), opinion analysis (using topic detection 
and anaphora resolution). This preliminary re-
search is motivated by the conclusions drawn by 
previous studies (Balahur et al, 2009). Our pur-
pose is to verify if the inclusion of new ele-
ments and methods - source and target detection 
(using semantic role labeling (SRL)), topic de-
tection (using Latent Semantic Analysis), pa-
raphrasing and joint topic-sentiment analysis 
(classification of the opinion expressed only in 
sentences related to the topic), followed by ana-
phora resolution (using a system whose perfor-
mance is not optimal), affects the results of the 
system and how. Our contribution to this respect 
is the identification of the challenges related to 
OQA compared to traditional QA. A further 
contribution consists in adding the appropriate 
methods, tools and resources to resolve the 
identified challenges. With the purpose of test-
ing the effect of each tool, resource and tech-
nique, we carry out a separate and a global 
evaluation. An additional motivation of our 
work is the fact that although previous ap-
proaches showed that opinion questions have 
longer answers than factual ones, the research 
done in OQA so far has only considered a sen-
tence-level approach. Another contribution this 
paper brings is the retrieval at 1 and 3-sentence 
level and the retrieval based on similarity to 
query paraphrases enriched with topic-related 
words). We believe retrieving longer text could 
cause additional problems such as redundancy, 
coreference and temporal expressions or the 
need to apply contextual information. Paraph-
rasing, on the other hand, had account for lan-
guage variability in a more robust manner; 
however, the paraphrase collections that are 
available at the moment are known to be noisy. 
The following sections are structured as fol-
lows: Section 3 presents the related work in the 
field and the competitions organized for systems 
tackling the OQA task. In Section 4 we describe 
the corpora used for the experiments we carried 
out and the set of questions asked over each of 
them. Section 5 presents the experimental set-
tings and the different system configurations we 
assessed. Section 6 shows the results of the 
evaluations, discusses the improvements and 
drops in performance using different configura-
tions. We finally conclude on our approaches in 
Section 7, proposing the lines for future work. 
3 Related Work 
QA can be defined as the task in which given a 
set of questions and a collection of documents, 
an automatic NLP system is employed to re-
trieve the answer to the queries in Natural Lan-
guage (NL). Research focused on building fac-
toid QA systems has a long tradition; however, 
it is only recently that researchers have started 
to focus on the development of OQA systems. 
(Stoyanov et al, 2005) and (Pustejovsky and 
Wiebe, 2006) studied the peculiarities of opi-
nion questions. (Cardie et al, 2003) employed 
opinion summarization to support a Multi-
Perspective QA system, aiming at identifying 
the opinion-oriented answers for a given set of 
questions. (Yu and Hatzivassiloglou, 2003) se-
parated opinions from facts and summarized 
them as answer to opinion questions. (Kim and 
Hovy, 2005) identified opinion holders, which 
are a key component in retrieving the correct 
answers to opinion questions. Due to the rea-
lized importance of blog data, recent years have 
also marked the beginning of NLP research fo-
cused on the development of opinion QA sys-
tems and the organization of international con-
ferences encouraging the creation of effective 
QA systems both for fact and subjective texts. 
The TAC 20082 QA track proposed a collection 
                                                 
2 http://www.nist.gov/tac/ 
28
of factoid and opinion queries called ?rigid list? 
(factoid) and ?squishy list? (opinion) respective-
ly, to which the traditional QA systems had to 
be adapted. Some participating systems treated 
opinionated questions as ?other? and thus they 
did not employ opinion specific methods. How-
ever, systems that performed better in the 
?squishy list? questions than in the ?rigid list? 
implemented additional components to classify 
the polarity of the question and of the extracted 
answer snippet. The Alyssa system (Shen et al 
2007) uses a Support Vector Machines (SVM) 
classifier trained on the MPQA corpus (Wiebe 
et al, 2005), English NTCIR3 data and rules 
based on the subjectivity lexicon (Wilson et al, 
2005). (Varma et al, 2008) performed query 
analysis to detect the polarity of the question 
using defined rules. Furthermore, they filter 
opinion from fact retrieved snippets using a 
classifier based on Na?ve Bayes with unigram 
features, assigning for each sentence a score that 
is a linear combination between the opinion and 
the polarity scores. The PolyU (Venjie et al, 
2008) system determines the sentiment orienta-
tion of the sentence using the Kullback-Leibler 
divergence measure with the two estimated lan-
guage models for the positive versus negative 
categories. The QUANTA (Li et al, 2008) sys-
tem performs opinion question sentiment analy-
sis by detecting the opinion holder, the object 
and the polarity of the opinion. It uses a seman-
tic labeler based on PropBank 4  and manually 
defined patterns. Regarding the sentiment clas-
sification, they extract and classify the opinion 
words. Finally, for the answer retrieval, they 
score the retrieved snippets depending on the 
presence of topic and opinion words and only 
choose as answer the top ranking results. Other 
related work concerns opinion holder and target 
detection. NTCIR 7 and 8 organized MOAT 
(the Multilingual Opinion Analysis Task), in 
which most participants employed machine 
learning approaches using syntactic patterns 
learned on the MPQA corpus (Wiebe et al, 
2005). Starting from the abovementioned re-
search, our aim is to take a step forward to 
present approaches and employ opinion specific 
methods focused on improving the performance 
of our OQA. We perform the retrieval at 1 sen-
                                                 
3 http://research.nii.ac.jp/ntcir/ 
4http://verbs.colorado.edu/~mpalmer/projects/ace.html 
tence and 3 sentence-level and also determine 
the expected source (ES) and the expected tar-
get (ET) of the questions, which are fundamen-
tal to properly retrieve the correct answer. These 
two elements are selected employing semantic 
roles (SR). The expected answer type (EAT) is 
determined using Machine Learning (ML) using 
Support Vector Machine (SVM), by taking into 
account the interrogation formula, the subjectiv-
ity of the verb and the presence of polarity 
words in the target SR. In the case of expected 
opinionated answers, we also compute the ex-
pected polarity type (EPT) ? by applying opi-
nion mining (OM) on the affirmative version of 
the question (e.g. for the question ?Why do 
people prefer Starbucks to Dunkin Donuts??, 
the affirmative version is ?People prefer Star-
bucks to Dunkin Donuts because X?). These 
experiments are presented in more detail in  
Section 5.  
4 Corpora 
In order to carry out the present research for 
detecting and solving the complexities of opi-
nion QA, we employed two corpora of blog 
posts: EmotiBlog (Boldrini et al, 2009a) and 
the TAC 2008 Opinion Pilot test collection (part 
of the Blog06 corpus). 
The TAC 2008 Opinion Pilot test collection is 
composed by documents with the answers to the 
opinion questions given on 25 targets. EmotiB-
log is a collection of blog posts in English ex-
tracted form the Web. As a consequence, it 
represents a genuine example of this textual ge-
nre. It consists in a monothematic corpus about 
the Kyoto Protocol, annotated with the im-
proved version of EmotiBlog (Boldrini et al, 
2009b). It is well know that Opinion Mining 
(OM) is a very complex task due to the high 
variability of the language employed. Thus, our 
objective is to build an annotation model that is 
able to capture the whole range of phenomena 
specific to subjectivity expression. Additional 
criteria employed when choosing the elements 
to be annotated were effectiveness and noise 
minimization. Thus, from the first version of the 
model, the elements which did not prove to be 
statistically relevant have been eliminated. The 
elements that compose the improved version of 
the annotation model are presented in Table 1.   
 
29
Elements Description 
Obj. speech Confidence, comment, source, target. 
Subj. speech Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Adjec-
tives/Adverbs 
Confidence, comment, level, emotion, 
phenomenon, modifier/not, polarity, 
source and target. 
Verbs/ Names Confidence, comment, level, emotion, 
phenomenon, polarity, mode, source 
and target. 
Anaphora Confidence, comment, type, source and 
target. 
Capital letter/ 
Punctuation 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Phenomenon Confidence, comment, type, colloca-
tion, saying, slang, title, and rhetoric. 
Reader/Author 
Interpr. (obj.) 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Emotions Confidence, comment, accept, anger, 
anticipation, anxiety, appreciation, bad, 
bewilderment, comfort, compassion? 
Table 1: EmotiBlog improved structure 
 
The first distinction consists in separating objec-
tive and subjective speech. Subsequently, a fin-
er-grained annotation is employed for each of 
the two types of data. Objective sentences are 
annotated with source and target (when neces-
sary, also the level of confidence of the annota-
tor and a comment). Subjective elements can be 
annotated at a sentence level, but they also have 
to be labeled at a word and/or phrase level. 
EmotiBlog also contains annotations of anapho-
ra at a cross-document level (to interpret the 
storyline of the posts) and the sentence type 
(simple sentence or title, but also saying or col-
location). Finally, the Reader and the Writer 
interpretation have to be marked in objective 
sentences. These elements are employed to 
mark and interpret correctly an apparent objec-
tive discourse, whose aim is to implicitly ex-
press an opinion (e.g. ?The camera broke in two 
days?). The first is useful to extract what is the 
interpretation of the reader (for example if the 
writer says The result of their governing was an 
increase of 3.4% in the unemployment rate in-
stead of The result of their governing was a dis-
aster for the unemployment rate) and the second 
to understand the background of the reader (i.e.. 
These criminals are not able to govern instead 
of saying the x party is not able to govern). 
From this sentence, for example, the reader can 
deduce the political ideas of the writer. The 
questions whose answers are annotated with 
EmotiBlog are the subset of opinion questions in 
English presented in (Balahur et al, 2009). The 
complete list of questions is shown in Table 2.  
 
N Question 
2 What motivates people?s negative opinions on the 
Kyoto Protocol? 
5 What are the reasons for the success of the Kyoto 
Protocol? 
6 What arguments do people bring for their criticism 
of media as far as the Kyoto Protocol is concerned? 
7 Why do people criticize Richard Branson? 
11 What negative opinions do people have on Hilary 
Benn? 
12 Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol? 
15 What alternative environmental friendly resources 
do people suggest to use instead of gas en the future? 
16 Is Arnold Schwarzenegger pro or against the reduc-
tion of CO2 emissions? 
18 What improvements are proposed to the Kyoto Pro-
tocol? 
19 What is Bush accused of as far as political measures 
are concerned? 
20 What initiative of an international body is thought to 
be a good continuation for the Kyoto Protocol? 
Table 2: Questions over the EmotiBlog  
corpus 
 
The main difference between the two corpora 
employed is that Emotiblog is monothematic, 
containing only posts about the Kyoto Protocol, 
while the TAC 2008 corpus contains documents 
on a multitude of subjects. Therefore, different 
techniques must be adjusted in order to treat 
each of them.  
5 Experiments 
5.1 Question Analysis 
In order to be able to extract the correct answer 
to opinion questions, different elements must be 
considered. As stated in (Balahur et al, 2009) 
we need to determine both the expected answer 
type (EAT) of the question ? as in the case of 
factoid ones - as well as new elements ? such as 
expected polarity type (EPT). However, opi-
nions are directional ? i.e., they suppose the ex-
istence of a source and a target to which they 
are addressed. Thus, we introduce two new 
elements in the question analysis ? expected 
source (ES) and expected target (ET). These 
two elements are selected by applying SR and 
choosing the source as the agent in the sentence 
and the direct object (patient) as the target of the 
opinion. Of course, the source and target of the 
30
opinions expressed can also be found in other 
roles, but at this stage we only consider these 
cases. The expected answer type (EAT) (e.g. 
opinion or other) is determined using Machine 
Learning (ML) using Support Vector Machine 
(SVM), by taking into account the interrogation 
formula, the subjectivity of the verb and the 
presence of polarity words in the target SR. In 
the case of expected opinionated answers, we 
also compute the expected polarity type (EPT) ? 
by applying OM on the affirmative version of 
the question. An example of such a transforma-
tion is: given the question ?What are the rea-
sons for the success of the Kyoto Protocol??, 
the affirmative version of the question is ?The 
reasons for the success of the Kyoto Protocol 
are X?.  
5.2 Candidate Snippet Retrieval 
In the answer retrieval stage, we employ four 
strategies:  
1. Using the JIRS (JAVA Information Re-
trieval System) IR engine (G?mez et al, 
2007) to find relevant snippets. JIRS re-
trieves passages (of the desired length), 
based on searching the question struc-
tures (n-grams) instead of the keywords, 
and comparing them.  
2. Using the ?Yahoo? search engine to re-
trieve the first 20 documents that are 
most related to the query. Subsequently, 
we apply LSA on the retrieved docu-
ments and extract the words that are 
most related to the topic. Finally, we 
expand the query using words that are 
very similar to the topic and retrieve 
snippets that contain at least one of 
them and the ET. 
3. Generating equivalent expressions for 
the query, using the DIRT paraphrase 
collection (Lin and Pantel, 2001) and 
retrieving candidate snippets of length 1 
and 3 (length refers to the number of 
sentences retrieved) that are similar to 
each of the new generated queries and 
contain the ET. Similarity is computed 
using the cosine measure. Examples of 
alternative queries for ?People like 
George Clooney? are ?People adore 
George Clooney?, ?People enjoy 
George Clooney?, ?People prefer 
George Clooney?. 
4. Enriching the equivalent expressions for 
the query in 3. with the topic-related 
words discovered in 2. using LSA. 
5.3 Polarity and topic-polarity classifica-
tion of snippets 
In order to determine the correct answers from 
the collection of retrieved snippets, we must 
filter for the next processing stage only the can-
didates that have the same polarity as the ques-
tion EPT. For polarity detection, we use a com-
bined system employing SVM ML on unigram 
and bigram features trained on the NTCIR 
MOAT 7 data and an unsupervised lexicon-
based system. In order to compute the features 
for each of the unigrams and bigrams, we com-
pute the tf-idf scores. 
The unsupervised system uses the Opinion 
Finder lexicon to filter out subjective sentences 
? that contain more than two subjective words 
or a subjective word and a valence shifter (ob-
tained from the General Inquirer resource). Sub-
sequently, it accounts for the presence of opi-
nionated words from four different lexicons ? 
MicroWordNet (Cerini et al, 2007), WordNet 
Affect (Strapparava and Valitutti, 2004) Emo-
tion Triggers (Balahur and Montoyo, 2008) and 
General Inquirer (Stone et al, 1966). For the 
joint topic-polarity analysis, we first employ 
LSA to determine the words that are strongly 
associated to the topic, as described in Section 
5.2 (second list item). Consequently, we com-
pute the polarity of the sentences that contain at 
least one topic word and the question target. 
5.4 Filtering using SR 
Finally, answers are filtered using the Semrol 
system for SR labeling described in (Moreda, 
2008). Subsequently, we filter all snippets with 
the required target and source as agent or pa-
tient. Semrol receives as input plain text with 
information about grammar, syntax, word 
senses, Named Entities and constituents of each 
verb. The system output is the given text, in 
which the semantic roles information of each 
constituent is marked. Ambiguity is resolved 
31
depending on the machine algorithm employed, 
which in this case is TIMBL5. 
6 Evaluation and Discussion 
We evaluate our approaches on both the Emo-
tiBlog question collection, as well as on the 
TAC 2008 Opinion Pilot test set. We compare 
them against the performance of the system eva-
luated in (Balahur et al, 2009) and the best 
(Copeck et al, 2008) and worst (Varma et al, 
2008) scoring systems (as far as F-measure is 
concerned) in the TAC 2008 task.  For both the 
TAC 2008 and EmotiBlog sets of questions, we 
employ the SR system in SA and determine the 
ES, ET and EPT. Subsequently, for each of the 
two corpora, we retrieve 1-phrase and 3-phrase 
snippets. The retrieval of the of the EmotiBlog 
candidate snippets is done using query expan-
sion with LSA and filtering according to the ET. 
Further on, we apply sentiment analysis (SA) 
using the approach described in Section 5.3 and 
select only the snippets whose polarity is the 
same as the determined question EPT. The re-
sults are presented in Table 3.  
 
Q 
N
o. 
N
o.  
A 
Baseline 
(Balahur et al, 
2009) 
1 phrase + 
ET+SA 
3 phrases 
+ET+SA 
  @ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@
2
0 
2 5 0 2 3 4 1 2 3 4 1 2 3 4 
5 1
1 
0 0 0 0 0 2 2 2 1 2 3 4 
6 2 0 0 1 2 1 1 2 2 0 1 2 2 
7 5 0 0 1 3 1 1 1 3 0 2 2 4 
1
1 
2 1 1 1 1 0 0 0 0 0 0 0 1 
1
2 
3 0 1 1 1 0 1 2 3 0 0 1 2 
1
5 
1 0 0 1 1 0 0 1 1 1 1 1 1 
1
6 
6 1 4 4 4 0 1 1 2 1 2 2 6 
1
8 
1 0 0 0 0 0 0 0 0 0 0 0 0 
1
9 
2
7 
1 5 6 1
8 
0 1 1 2 0 1 1 1 
2
0 
4 0 0 0 0 0 0 1 1 0 0 1 2 
Table 3: Results for questions over  
EmotiBlog 
 
                                                 
5
http://ilk.uvt.nl/downloads/pub/papers/Timbl_6.2_Manual
.pdf and http://ilk.uvt.nl/timbl/ 
The retrieval of the TAC 2008 1-phrase and 3-
phrase candidate snippets was done using JIRS 
and, in a second approach, using the cosine si-
milarity measure between alternative queries 
generated using paraphrases and candidate 
snippets. Subsequently, we performed different 
evaluations, in order to assess the impact of us-
ing different resources and tools. Since the TAC 
2008 had a limit of the output of 7000 charac-
ters, in order to compute a comparable F-
measure, at the end of each processing chain, 
we only considered the snippets for the 1-phrase 
retrieval and for the 3-phases one until this limit 
was reached. 
1. In the first evaluation, we only apply the 
sentiment analysis tool and select the snip-
pets that have the same polarity as the ques-
tion EPT and the ET is found in the snippet.  
(i.e. What motivates peoples negative opi-
nions on the Kyoto Protocol? The Kyoto 
Protocol becomes deterrence to economic 
development and international cooperation/ 
Secondly, in terms of administrative aspect, 
the Kyoto Protocol is difficult to implement.  
- same EPT and ET) 
We also detected cases of same polarity but 
no ET, e.g. These attempts mean annual ex-
penditures of $700 million in tax credits in 
order to endorse technologies, $3 billion in 
developing research and $200 million in 
settling technology into developing coun-
tries ? EPT negative but not same ET. 
2. In the second evaluation, we add the result 
of the LSA process to filter out the snippets 
from 1., containing the words related to the 
topic starting from the retrieval performed 
by Yahoo, which extracts the first 20 docu-
ments about the topic. 
3. In the third evaluation, we filter the results 
in 2 by applying the Semrol system and set-
ting the condition that the ET and ES are the 
agent or the patient of the snippet. 
4. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases (as explained in 
the third method in section 5.2.). We subse-
quently filtered these results based on their 
polarity  (so that it corresponds to the EPT) 
and on the condition that the source and tar-
get of the opinion (identified through SRL 
using Semrol) correspond to the ES and ET.  
32
5. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases, enriched with the 
topic words determined using LSA. We 
subsequently filtered these results based on 
their polarity (so that it corresponds to the 
EPT) and on the condition that the source 
and target of the opinion (identified through 
SRL using Semrol) correspond to the ES 
and ET.  
 
System F-measure 
Best TAC 0.534 
Worst TAC 0.101 
JIRS + SA+ET (1 phrase)  0.377 
JIRS + SA+ET (3 phrases)  0.431 
JIRS + SA+ET+LSA (1 phrase)  0.489 
JIRS + SA+ET+LSA (3 phrases)  0.505 
JIRS + SA+ET+LSA+SR (1 
phrase)  
0. 533 
JIRS + SA+ET+LSA+SR (3 
phrases) 
0.571 
PAR+SA+ET+SR(1 phrase) 0.345 
PAR+SA+ET+SR(2 phrase) 0.386 
PAR_LSA+SA+ET+SR (1 phra-
se) 
0.453 
PAR_LSA+SA+ET+SR (3 phra-
ses) 
0.434 
Table 4: Results for the TAC 2008 test set 
 
From the results obtained (Table 3 and Table 4), 
we can draw the following conclusions. Firstly, 
the hypothesis that OQA requires the retrieval 
of longer snippets was confirmed by the im-
proved results, both in the case of EmotiBlog, as 
well as the TAC 2008 corpus. Secondly, opi-
nion questions require the use of joint topic-
sentiment analysis. As we can see from the re-
sults, the use of topic-related words when com-
puting of the affect influences the results in a 
positive manner and joint topic-sentiment anal-
ysis is especially useful for the cases of ques-
tions asked on a monothematic corpus. Thirdly, 
another conclusion that we can draw is that tar-
get and source detection are highly relevant 
steps at the time of answer filtering, not only 
helping the more accurate retrieval of answers, 
but also at placing at the top of the retrieval the 
relevant results (as more relevant information is 
contained within these 7000 characters). The 
use of paraphrases at the retrieval stage was 
shown to produce a significant drop in results, 
which we explain by the noise introduced and 
the fact that more non-relevant answer candi-
dates were introduced among the results. None-
theless, as we can see from the overall relatively 
low improvement in the results, much remains 
to be done in order to appropriately tackle 
OQA. As seen in the results, there are still ques-
tions for which no answer is found (e.g. 18). 
This is due to the fact that the treatment of such 
questions requires the use of inference tech-
niques that are presently unavailable (i.e. define 
terms such as ?improvement?, possibly as ?X 
better than Y?, in which case opinion extraction 
from comparative sentences should be intro-
duced in the model).  
The results obtained when using all the compo-
nents for the 3-sentence long snippets signifi-
cantly improve the results obtained by the best 
system participating in the TAC 2008 Opinion 
Pilot competition (determined using a paired t-
test for statistical significance, with confidence 
level 5%). Finally, from the analysis of the er-
rors, we could see that even though some tools 
are in theory useful and should produce higher 
improvements ? such as SR ? their performance 
in reality does not produce drastically higher 
results. The idea to use paraphrases for query 
expansion also proved to decrease the system 
performance. From preliminary results obtained 
using JavaRap6  for coreference resolution, we 
also noticed that the performance of the OQA 
lowered, although theoretically it should have 
improved. 
7 Conclusions ad Future Work 
In this paper, we presented and evaluated differ-
ent methods and techniques with the objective 
of improving the task of QA in the context of 
opinion data. From the evaluations performed 
using different NLP resources and tools, we 
concluded that joint topic-sentiment analysis, as 
well as the target and source identification, are 
crucial for the correct performance of this task. 
We have also demonstrated that by retrieving 
longer answers, the results have improved. We 
tested, within a simple setting, the impact of 
using paraphrases in the context of opinion 
questions and saw that their use lowered the 
system results. Although such paraphrase col-
                                                 
6http://wing.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.ht
m 
33
lections include a lot of noise and have been 
shown to decrease system performance even in 
the case of factual questions, we believe that 
other types of paraphrasing methods should be 
investigated in the context of OQA. We thus 
showed that opinion QA requires the develop-
ment of appropriate strategies at the different 
stages of the task (recognition of subjective 
questions, detection of subjective content of the 
questions, source and target identification, re-
trieval and classification of the candidate an-
swer data). Due to the high level of complexity 
of subjective language, our future work will be 
focused on testing higher-performing tools for 
coreference resolution, other (opinion) paraph-
rases collections and paraphrasing methods and 
the employment of external knowledge sources 
that refine the semantics of queries. We also 
plan to include other SA methods and extend 
the semantic roles considered for ET and ES, 
with the purpose of checking if they improve or 
not the performance of the QA system. 
 
Acknowledgements 
This paper has been partially supported by Mi-
nisterio de Ciencia e Innovaci?n - Spanish Gov-
ernment (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat Va-
lenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/286). 
References 
Balahur, A. and Montoyo, A. 2008. Applying a 
Culture Dependent Emotion Triggers Data-
base for Text Valence and Emotion 
Classification. In Proceedings of the AISB 
2008 Symposium on Affective Language in 
Human and Machine, Aberdeen, Scotland. 
Balahur, A., Lloret, E., Ferr?ndez, O., Montoyo, 
A., Palomar, M., and Mu?oz, R. 2008. The 
DLSIUAES Team?s Participation in the TAC 
2008 Tracks. In Proceedings of the Text 
Analysis Conference 2008 Workshop. 
Balahur, A., Boldrini, E., Montoyo A. and 
Mart?nez-Barco P. 2009. Opinion and Generic 
Question Answering Systems: a Performance 
Analysis. In Proceedings of ACL. Singapur.  
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and  Montoyo. A. 2009a. EmotiBlog: an An-
notation Scheme for Emotion Detection and 
Analysis in Non-traditional Textual Genre. In 
Proceedings of DMIN 2009, Las Vegas. Ne-
vada. 
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and Montoyo. A. 2009b. EmotiBlog: a fine-
grained model for emotion detection in non-
traditional textual genre. In Proceedings of 
WOMSA 2009. Seville. 
Cardie, C., Wiebe, J., Wilson, T. and Litman, D. 
2003. Combining Low-Level and Summary 
Representations of Opinions for Multi-
Perspective Question Answering. AAAI 
Spring Symposium on New Directions in 
Question Answering. 
Cerini, S., Compagnoni, V., Demontis, A., 
Formentelli, M. and Gandini, C. 2007. Mi-
cro-WNOp: A gold standard for the evalua-
tion of automatically compiledlexical re-
sources for opinion mining. In: A.Sanso 
(ed.): Language resources and linguistic 
theory: Typology, Second Language Acqui-
sition, English Linguistics. Milano. IT. 
Copeck, T.,  Kazantseva, A., Kennedy, A., 
Kunadze, A., Inkpen, D. and Szpakowicz, 
S. 2008. Update Summary Update. In Pro-
ceedings of the Text Analysis Conference 
(TAC) 2008. 
Cui, H., Mittal, V. and Datar, M. 2006. Com-
parative Experiments on Sentiment Classifi-
cation for Online Product Review. Proceed-
ings, The Twenty-First National Conference 
on Artificial Intelligence and the Eighteenth 
Innovative Applications of Artificial Intelli-
gence Conference. Boston, Massachusetts, 
USA. 
G?mez, J.M., Rosso, P. and Sanchis, E. 2007. 
JIRS Language-Independent Passage Re-
trieval System: A Comparative Study. 5th 
International Conference on Natural 
Language Proceeding (ICON 2007). 
Kabadjov, M., Balahur, A. And Boldrini, E. 
2009. Sentiment Intensity: Is It a Good 
Summary Indicator?. Proceedings of the 4th 
Language Technology Conference LTC, pp. 
380-384. Poznan, Poland, 6-8.11.2009. 
Kim, S. M. and Hovy, E. 2005. Identifying 
Opinion Holders for Question Answering in 
Opinion Texts. Proceedings of the 
Workshop on Question Answering in 
Restricted Domain at the Conference of the 
American Association of Artificial 
Intelligence (AAAI-05).  Pittsburgh, PA. 
34
Li, F., Zheng, Z.,Yang T., Bu, F., Ge, R., Zhu, 
X., Zhang, X., and Huang, M. 2008. THU 
QUANTA at TAC 2008. QA and RTE track. 
In Proceedings of the Text Analysis 
Conference (TAC). 
Lin, D. and Pantel, P. 2001. Discovery of 
Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-
360. 
Moreda. P. 2008. Los Roles Sem?nticos en la 
Tecnolog?a del Lengauje Humano: Anota-
ci?n y Aplicaci?n. Doctoral Thesis. Univer-
sity of Alicante. 
Pustejovsky, J. and Wiebe, J. 2006. Introduction 
to Special Issue on Advances in Question 
Answering. Language Resources and Eval-
uation (2005), (39). 
Shen, D., Wiegand, M., Merkel, A., Kazalski, 
S., Hunsicker, S., Leidner, J. L. and 
Klakow, D. 2007. The Alyssa System at 
TREC QA 2007: Do We Need Blog06? In 
Proceedings of the Sixteenth Text Retrieval 
Conference (TREC 2007), Gaithersburg, 
MD, USA. 
Strapparava, C. and Valitutti, A. 2004. Word-
Net-Affect: an affective extension of Word-
Net. In Proceedings of 4th International Con-
ference on Language Resources and Evalua-
tion (LREC 2004), pages 1083 ? 1086, Lis-
bon. 
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. 
Multiperspective question answering using 
the opqa corpus. In Proceedings of the 
Human Language Technology Conference 
and the Conference on Empirical Methods 
in Natural Language Processing 
(HLT/EMNLP 2005). 
Varma, V., Pingali, P., Katragadda, S., Krishna, 
R., Ganesh, S., Sarvabhotla, K. Garapati, 
H., Gopisetty, H., Reddy, K. and 
Bharadwaj, R. 2008. IIIT Hyderabad at 
TAC 2008. In Proceedings of Text Analysis 
Conference (TAC).  
Wenjie, L., Ouyang, Y., Hu, Y. and Wei, F. 
2008. PolyU at TAC 2008. In Proceedings 
of the Text Analysis Conference (TAC). 
Wiebe, J., Wilson, T., and Cardie, C. 2005. 
Annotating expressions of opinions and 
emotions in language. Language Resources 
and Evaluation, volume 39, issue 2-3, pp. 
165-210. 
Wilson, T., J. Wiebe, and Hoffmann, P. 2005. 
Recognizing Contextual Polarity in Phrase-
level sentiment Analysis. In Proceedings of 
the Human Language Technologies 
Conference/Conference on Empirical 
Methods in Natural Language Processing 
(HLT/ EMNLP). 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating 
Facts from Opinions. In Proceedings of 
EMNLP-03. 
Wiebe, J., Wilson, T., and Cardie, C. (2005). 
Annotating expressions of opinions and 
emotions in language. In Language 
Resources and Evaluation. Vol. 39. 
35
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
EmotiBlog: a finer-grained and more precise learning of subjectivity expression models 
Ester Boldrini University of Alicante, Department of Software and Computing Systems eboldrini@dlsi.ua.es 
Alexandra Balahur University of Alicante, Department of Software and Computing Systems abalahur@dlsi.ua.es Patricio Mart?nez-Barco University of Alicante, Department of Software and Computing Systems patricio@dlsi.ua.es 
Andr?s Montoyo University of Alicante, Department of Software and Computing Systems montoyo@dlsi.ua.es  Abstract  
The exponential growth of the subjective in-formation in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog ? a fine-grained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the sys-tems using it for training, through the experi-ments we carried out on opinion mining and emotion detection. We employ corpora of dif-ferent textual genres ?a set of annotated re-ported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life self-expressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detec-tion. 1 Credits This paper has been supported by Ministe-rio de Ciencia e Innovaci?n- Spanish Gov-ernment (grant no. TIN2009-13391-C04-01), and Conselleria d'Educaci?n-Generalitat Valenciana (grant no. PRO-METEO/2009/119 and A-COMP/2010/288).  
2 Introduction The exponential growth of the subjective infor-mation with Web 2.0 created the need to develop new Natural Language Processing (NLP) tools to automatically process and manage the content available on the Internet. Apart from the tradi-tional textual genres, at present we have new ones such as blogs, forums and reviews. The main difference between them is that the latter are predominantly subjective, containing per-sonal judgments. At the moment, NLP tools and methods for analyzing objective information have a better performance than the new ones the research community is creating for managing the subjective content. The survey called ?The State of the Blogosphere 2009?, published by Tech-norati 1 , demonstrates that users are blogging more than ever. Furthermore, in contrast to the general idea about bloggers, each day it is more and more the number of professionals who de-cide to use this means of communication, contra-dicting the common belief about the predomi-nance of an informal editing (Balahur et al, 2009). Due to the growing interest in this text type, the subjective data of the Web is increasing on a daily basis, becoming a reflection of peo-ple?s opinion about a wide range of topics. (Cui, Mittal and Datar, 2006). Blogs represent an im-portant source of real-time, unbiased informa-tion, useful for the development of many applica-tions for concrete purposes. Given the proved importance of automatically processing this data, a new task has appeared in NLP task, dealing with the treatment of subjective data: Sentiment Analysis (SA). The main objective of this paper is to present EmotiBlog (Boldrini et al, 2009), a fine-grained annotation scheme for labeling sub-jectivity in the new textual genres. Subjectivity                                                 1 http://technorati.com/ 
1
can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2  and opinions, generally denomi-nated ?private states? (Uspensky, 1973), not open to verification (Wiebe, 1994). We per-formed a series of experiments focused on dem-onstrating that EmotiBlog represents a step for-ward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models. Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, mov-ing a step forward the mere polarity recognition. In particular, the experiments concern expres-sions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and classification. To that aim, we employ corpora of different textual genres? a set of annotated reported speech extracted from news articles (denominated JRC quotes) (Bala-hur et al, 2010) and the set of news titles anno-tated with polarity and emotion from the SemE-val 2007 Task No. 14 (Strapparava and Mihal-cea, 2007), as well as a corpus of real-life self-expressed emotion entitled ISEAR (Scherer and Walbott, 1999). We subsequently show, through the quality of the results obtained, that Emoti-Blog, through its structure and annotation para-digm, offers high quality training for systems dealing both with opinion mining, as well as emotion detection.  3 Motivation and Contribution The main motivation of this research is the dem-onstrated necessity to work towards the harmoni-zation and interoperability of the increasingly large number of tools and frameworks that sup-port the creation, instantiation, manipulation, querying, and exploitation of annotated resource. This necessity is stressed by the new tools and resources, which have been recently created for processing the subjectivity in the new-textual genres born with the Web 2.0. Such predomi-nantly subjective data is increasing at an expo-nential rate (about 75000 new blogs are reported to be created every day) and contains opinions on the most diverse set of topics. Given its world-wide availability, the subjective data on the Web has become a primary source of information (Balahur et al, 2009). As a consequence, new mechanisms have to be implemented so that this                                                 2 http://dictionary.cambridge.org/ 
data is effectively analyzed and processed. The main challenge of the opinionated content is that, unlike the objective one, which presents facts, the subjective information is most of the times difficult and complex to extract and classify us-ing in grammatically static and fixed rules. Ex-pression of subjectivity is more spontaneous and even if the majority is quite formal, new means of expressivity can be encountered, such as the use of colloquialisms, sayings, collocations or anomalies in the use of punctuation; this is moti-vated by the fact that subjectivity expression is part of our daily life. For example, at the time of taking a decision, people search for information and opinions expressed on the Web on their mat-ter of interest and base their final decision on the information found. At the same time, when using a product, people often write reviews on it, so that others can have a better idea of the perform-ance of that product before purchasing it. There-fore, on the one hand, the growing volume of opinion information available on the Web allows for better and more informed decisions of the users. On the other hand, the amount of data to be analyzed requires the development of special-ized NLP systems that automatically extract, classify and summarize the data available on the Web on different topics. (Esuli and Sebastiani, 2006) define OM as a recent discipline at the crossroads of Information Retrieval and Compu-tational Linguistics, which is concerned not with the topic a document is about, but with the opin-ion it expresses. Research in this field has proven the task to be very difficult, due to the high se-mantic variability of affective language. Differ-ent authors have addressed the problem of ex-tracting and classifying opinion from different perspectives and at different levels, depending on a series of factors which can be level of interest (overall/specific), querying formula (?Nokia E65?/?Why do people buy Nokia E65??), type of text (review on forum/blog/dialogue/press arti-cle), and manner of expression of opinion - di-rectly (using opinion statements, e.g. ?I think this product is wonderful!?/?This is a bright initia-tive?), indirectly (using affect vocabulary, e.g. ?I love the pictures this camera takes!?/?Personally, I am shocked one can pro-pose such a law!?) or implicitly (using adjectives and evaluative expressions, e.g. ?It?s light as a feather and fits right into my pocket!?). While determining the overall opinion on a movie is sufficient for taking the decision to watch it or not, when buying a product, people are interested in the individual opinions on the different prod-
2
uct characteristics. When discussing a person, one can judge and give opinion on the person?s actions. Moreover, the approaches taken can vary depending on the manner in which a user asks for the data (general formula such as ?opinions on X? or a specific question ?Why do people like X?? and the text source that needs to be queried). Retrieving opinion information in newspaper articles or blogs posts is more complex, because it involves the detection of different discussion topics, the subjective phrases present and subse-quently their classification according to polarity. Especially in the blog area, determining points of view expressed in dialogues together with the mixture of quotes and pastes from newspapers on a topic can, additionally, involve determining the persons present and whether or not the opinion expressed is on the required topic or on a point previously made by another speaker. This diffi-cult NLP problem requires the use of specialized data for system training and tuning, gathered, annotated and tested within the different text spheres. At the present moment, these specialized resources are scarce and when they exist, they are rather simplistically annotated or highly domain-dependent. Moreover, most of these resources created are for the English. The contribution we describe in this paper intends to propose solutions to the above-mentioned problems, and consists of the following points: first of all, we overcome the problem of corpora scarcity in other languages except English and also improve the English ones; we present the manner in which we compiled a multilingual corpus of blog posts on different topics of interest in three languages-Spanish, Italian and English. The second issue we tried to solve was the coarse-grained annotation schemas employed in other annotation schema. Thus, we describe the new annotation model, EmotiBlog built up in order to capture the different subjectivity/objectivity, emotion/opinion/attitude aspects we are interested in at a finer-grained level. We justify the need for a more detailed annotation model, the sources and the reasons taken into consideration when constructing the corpus and its annotation. Thirdly, we address an aspect strongly related to blogs annotation: due the presence of ?copy and pastes? from news articles or other blogs, the frequent quotes, we include the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We discuss on the problems encountered at different stages and comment upon some of the conclusions we have reached while performing this research. 
this research. Finally, we conclude on our ap-proach and propose the lines for future work. 4 Related Work In recent years, different researchers have ad-dressed the needs and possible methodologies from the linguistic, theoretical and practical points of view. Thus, the first step involved re-sided in building lexical resources of affect, such as WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et. Al, 2007) or ?emotion triggers? (Balahur and Montoyo, 2009). All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context. We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion. Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeni-able influence of the context. The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giv-ing a method to annotate a corpus depending on these two aspects ? MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the senti-ment task, as part of Opinion Information Re-trieval  (last three editions of the TREC Blog tracks 3  competitions, the TAC 2008 competi-tion4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al, 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial dis-crimination, but we also add a deeper level of emotion annotation. Since expressions of emo-tion are also highly related to opinions, related work also includes customer review classifica-tion at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations                                                 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/ 
3
and other attributes with SVM (Mullena and Col-lier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sen-timent classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in ana-lysing sentiment at a sentence level using boot-strapping techniques (Riloff, Wiebe, 2003), con-sidering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then ap-plying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opin-ion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orienta-tion of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, fine-grained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into ac-count modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail. They are extremely useful and a poll for discussing about any topic with the world. For this reason, the first corpus object of our study is a collection of blog posts extracted from the Web. The texts we selected have distinctive features, extremely different from traditional tex-tual ones. In fact people writing a post can use an informal language colloquialism, emoticons, etc. to express their feelings and it is not rare to find a mix of sources in the same post; people usually mention some facts or discourses and then they give their opinion about them. As we can deduce, 
the source detection represents one of the most complex tasks. As we mentioned above, we car-ried out a multilingual research, collecting texts in three languages: Spanish, Italian, and English about three subjects of interest. The first one contains blog posts commenting upon the signing of the Kyoto Protocol against global warming, the second collection consists of blog entries about the Mugabe government in Zimbabwe, and finally we selected a series of blog posts discuss-ing the issues related to the 2008 USA presiden-tial elections. For each of the abovementioned topics, we have gathered 100 texts, summing up a total of 30.000 words approximately for each language. However in this research we start with English but consider as future work labeling the other languages we have. The second corpus we employed for this research is a collection of 1592 quotes extracted from the news in April 2008. As a consequence they are about many different top-ics and in English (Balahur and Steinberg, 2009). Both of these corpora have been annotated with EmotiBlog that is presented in the next section. 6 EmotiBlog Annotation Model Our annotation schema can be defined as a fine-grained model for labelling subjectivity of the new-textual genres born with the Web 2.0. As mentioned above, it represents a step forward to previous research and it is focused on detecting the linguistic elements, which give subjectivity to the text. The EmotiBlog annotation is divided into different levels (Figure 1).  
 Figure 1: General structure of EmotiBlog.  As we can observe in Figure 1, the first distinc-tion to be made is between objective and subjec-tive speech. If we are labelling an objective sen-tence, we insert the source element, while if we are annotating a subjective discourse, a list of elements with the corresponding attributes have to be added. We select among the list of subjec-tive elements and specify the element?s attrib-
4
utes. Table 1 presents the annotation model in detail.  Elem. Description Obj. speech Confidence, comment, source, target. Subj. speech Confidence, comment, level, emotion, phenomenon, polarity, source and target. Adjectives Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Adverbs Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Verbs Confidence, comment, level, emotion, phenomenon, polarity, mode, source and target. Anaphora Confidence, comment, type, source and target. Capital letter Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Punctuation Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Names Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, and source. Phenomenon Confidence, comment, type: collocation, saying, slang, title, and rhetoric. Reader Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Author Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Emotions Confidence, comment, accept, anger, anticipation, anxiety, appreciation, bad, bewilderment, comfort, ? Table 1: EmotiBlog structure  Each element of the discourse has its own attrib-utes with a series of features, which have to be annotated. Due to space reasons it is impossible to detail each one of them, however we would like to underline the most innovative and rel-evant. For each element we are labelling the an-notator has to insert his level of confidence. In this way we will assign each label a weight that will be computed for future evaluations. More-over, the annotator has to insert the polarity, which can be positive or negative, the level (high, medium, low) and also the sentiment this element is expressing. Table 2 presents a com-plete list of the emotions we selected to be part of EmotiBlog. We grouped all sentiments into subgroups in order to help the evaluation pro-cess. In fact emotions of the same subgroup will have less impact when calculating the inter-annotation agreement. In order to make this sub-division proper and effective division, we were inspired by (Scherer, 2005) who created an alter-native dimensional structure of the semantic space for emotions. The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in two-
dimensional valence by activity/arousal space (upper-case terms). As we can appreciate, the circle is divided by 4 axes. Moreover, Scherer distinguishes between positive and negative sen-timents and after that between active and passive. Furthermore emotions are grouped between ob-structive and conductive, and finally between high power and low power control. We started form this classification, grouping sentiments into positive and negative, but we divided them as high/low power control, obstructive/conductive and active/passive. Further on, we distributed the sentiments within our list into the Scherer slots creating other smaller categories included in the abovementioned general ones. The result of this division is shown in Table 2: 
Table 2: Alternative dimensional structures of the semantic space for emotions  Following with the description of the model, we said that the first distinction to be made is be-tween objective and subjective speech. Analys-ing the texts we collected, we realised that even if the writer uses an objective speech, sometimes it is just apparently objective and for this reason we added two elements: reader and author inter-pretation. The first one is the impres-sion/feeling/reaction the reader has reading the intervention and what s/he can deduce from the piece of text and the author interpretation is what we can understand from the author (politic orien-tation, preferences). All this information can be deduced form some linguistic elements that ap-parently are not so objective as they may appear. Another innovative element we inserted in the model is the coreference but just at a cross-post level. It is necessary because blogs are composed by posts linked between them and thus cross-
Group Emotions Criticism Sarcasm, irony, incorrect, criticism, objection, opposition, scepticism. Happiness Joy, joke. Support Accept, correct, good, hope, support, trust, rapture, respect, patience, appreciation, excuse. Importance Important, interesting, will, justice, longing, anticipation, revenge. Gratitude Thank. Guilt Guilt, vexation. Fear Fear, fright, troubledness, anxiety. Surprise Surprise, bewilderment, disappoint-ment, consternation. Anger Rage, hatred, enmity, wrath, force, anger, revendication. Envy Envy, rivalry, jealousy. Indifference Unimportant, yield, sluggishness. Pity Compassion, shame, grief. Pain Sadness, lament, remorse, mourning, depression, despondency. Shyness Timidity. Bad Bad, malice, disgust, greed. 
5
document coreference can help the reader to fol-low the conversations. We also label the unusual usage of capital letters and repeated punctuation. In fact, it is very common in blogs to find words written in capital letter or with no conventional usage of punctuation; these features usually mean shouts or a particular mood of the writer. Using EmotiBlog, we annotate the single ele-ments, but we also mark sayings or collocations, representative of each language. A saying is a well-known and wise statement, which often has a meaning, different from the simple meanings of the words it contains5; while a collocation is a word or phrase, which is frequently used with another word or phrase, in a way that sounds cor-rect to native speakers, but might not be expected from the individual words? meanings6. Finally we insert for each element the source and topic. An example of annotation can be:  <phenomenon target="Kyoto Protocol" category="phrase" degree="medium" source="w" polarity="positive" emotion="good">The Onion has a <adjective target="Kyoto Protocol" phenomenon="phrase" de-gree="medium" polarity="positive" emotion="good" source="w" ismodifier="yes">great</adjective> story today titled ?Bush Told to Sign Birthday Treaty for Someone Named Kyoto." </phenomenon> 7 Experiments and Evaluation In order to evaluate the appropriateness of the EmotiBlog annotation scheme and to prove that the fine-grained level it aims at has a positive impact on the performance of the systems em-ploying it as training, we performed several ex-periments. Given that a) EmotiBlog contains an-notations for individual words, as well as for multi-word expressions and at a sentence level, and b) they are labeled with polarity, but also emotion, our experiments show how the anno-tated elements can be used as training for the opinion mining and polarity classification task, as well as for emotion detection. Moreover, tak-ing into consideration the fact that EmotiBlog labels the intensity level of the annotated ele-ments, we performed a brief experiment on de-termining the sentiment intensity, measured on a three-level scale: low, medium and high. In order to perform these three different evaluations, we chose three different corpora. The first one is a collection of quotes (reported speech) from newspaper articles presented in (Balahur et al, 2010), enriched with the manual fine-grained 
                                                5  Definition according to the Cambridge Advanced Learner?s Dictionary 6   Definition according to the Cambridge Advanced Learner?s Dictionary 
annotation of EmotiBlog7; the second one is the collection of newspaper titles in the test set of the SemEval 2007 task number 14 ? Affective Text. Finally, the third one is a corpus of self-reported emotional response ? ISEAR (Scherer and Wal-bott, 1999). The intensity classification task is evaluated only on the second corpus, given that it is the only one in which scores between -100 and 0 and 0 and 100, respectively, are given for the polarity of the titles.  6.1 Creation of training models For the OM and polarity classification task, we first extracted the Named Entities contained in the annotations using Lingpipe and united through a ?_? all the tokens pertaining to the NE. All the annotations of punctuation signs that had a specific meaning together were also united un-der a single punctuation sign. Subsequently, we processed the annotated data, using Minipar. We compute, for each word in a sentence, a series of features (some of these features are used in (Choi et al, 2005): ? the part of speech (POS)  ? capitalization (if all letters are in capitals, if only the first letter is in capitals, and if it is a NE or not) ? opinionatedness/intensity/emotion - if the word is annotated as opinion word, its polar-ity, i.e. 1 and -1 if the word is positive or negative, respectively and 0 if it is not an opinion word, its intensity (1.2 or 3) and 0 if it is not a subjective word, its emotion (if it has, none otherwise) ? syntactic relatedness with other opinion word ? if it is directly dependent of an opin-ion word or modifier (0 or 1), plus the polar-ity/intensity and emotion of this word (0 for all the components otherwise) ?  role in 2-word, 3-word and 4-word annota-tions: opinionatedness, intensity and emo-tion of the other words contained in the an-notation, direct dependency relations with them if they exist and 0 otherwise.  We compute the length of the longest sentence in EmotiBlog. The feature vector for each of the sentences contains the feature vectors of each of its words and 0s for the corresponding feature vectors of the words, which the current sentence has less than the longest annotated sentence. Fi-nally, we add for each sentence as feature binary features for subjectivity and polarity, the value corresponding to the intensity of opinion and the                                                 7 Freely available on request to the authors. 
6
general emotion. These feature vectors are fed into the Weka8 SVM SMO ML algorithm and a model is created (EmotiBlog I). A second model (EmotiBlog II) is created by adding to the collec-tion of single opinion and emotion words anno-tated in EmotiBlog, the Opinion Finder lexicon and the opinion words found in MicroWordNet, the General Inquirer resource and WordNet Af-fect.   6.2 Evaluation of models on test sets  In order to evaluate the performance of the mod-els extracted from the features of the annotations in EmotiBlog, we performed different tests. The first one regarded the evaluation of the polarity and intensity classification task using the Emoit-blog I and II constructed models on two test sets ? the JRC quotes collection and the SemEval 2007 Task Number 14 test set. Since the quotes often contain more than a sentence, we consider the polarity and intensity of the entire quote as the most frequent result in each class, corre-sponding to its constituent sentences. Also, given the fact that the SemEval Affective Text head-lines were given intensity values between -100 and 100, we mapped the values contained in the Gold Standard of the task into three categories: [-100, -67] is high (value 3 in intensity) and nega-tive (value -1 in polarity), [-66, 34] medium negative and [33, 1] is low negative. The values between [1 and 100] are mapped in the same manner to the positive category. 0 was consid-ered objective, so containing the value 0 for in-tensity. The results are presented in Table 3 (the values I and II correspond to the models Emoti-Blog I and EmotiBlog II):   Test  Corpus Evaluation type Precision Recall Polarity 32.13 54.09 JRC quotes I Intensity 36.00 53.2 Polarity 36.4 51.00 JRC quotes II Intensity 38.7 57.81 Polarity 38.57 51.3 SemEval I Intensity 37.39 50.9 Polarity 35.8 58.68 SemEval II Intensity 32.3 50.4 Table 3. Results for polarity and intensity classifi-cation using the models built from the EmotiBlog annotations The results shown in Table 2 show a signifi-cantly high improvement over the results ob-tained in the SemEval task in 2007. This is ex-plainable, on the one hand, by the fact that sys-                                                8 http://www.cs.waikato.ac.nz/ml/weka/ 
tems performing the opinion task did not have at their disposal the lexical resources for opinion employed in the EmotiBlog II model, but also because of the fact that they did not use machine learning on a corpus comparable to EmotiBlog (as seen from the results obtained when using solely the EmotiBlog I corpus). Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in preci-sion, with a recall that is comparable to most of the participating systems. In the second experi-ment, we tested the performance of emotion clas-sification using the two models built using Emo-tiBlog on the three corpora ? JRC quotes, SemE-val 2007 Task No.14 test set and the ISEAR cor-pus. The JRC quotes are labeled using Emoti-Blog; however, the other two are labeled with a small set of emotions ? 6 in the case of the Se-mEval data (joy, surprise, anger, fear, sadness, disgust) and 7 in ISEAR (joy, sadness, anger, fear, guilt, shame, disgust). Moreover, the Se-mEval data contains more than one emotion per title in the Gold Standard, therefore we consider as correct any of the classifications containing one of them. In order to unify the results and ob-tain comparable evaluations, we assessed the performance of the system using the alternative dimensional structures defined in Table 1. The ones not overlapping with the category of any of the 8 different emotions in SemEval and ISEAR are considered as ?Other? and are not included either in the training, nor test set. The results of the evaluation are presented in Table 4. Again, the values I and II correspond to the models EmotiBlog I and II. The ?Emotions? category contains the following emotions: joy, sadness, anger, fear, guilt, shame, disgust, surprise.  Test  corpus Evaluation  type Precision Recall JRC quotes I Emotions   24.7 15.08 
JRC quotes II Emotions  33.65 18.98 
SemEval I Emotions 29.03 18.89 SemEval II Emotions 32.98 18.45 ISEAR I Emotions 22.31 15.01 ISEAR II Emotions 25.62 17.83 Table 4. Results for emotion classification using the models built from the EmotiBlog annotations. The best results for emotion detection were ob-tained for the ?anger? category, where the preci-sion was around 35 percent, for a recall of 19 percent. The worst results obtained were for the ISEAR category of ?shame?, where precision was around 12 percent, with a recall of 15 per-
7
cent. We believe this is due to the fact that the latter emotion is a combination of more complex affective states and it can be easily misclassified to other categories of emotion. Moreover, from the analysis performed on the errors, we realized that many of the affective phenomena presented were more explicit in the case of texts expressing strong emotions such as ?joy? and ?anger?, and were mostly related to common-sense interpreta-tion of the facts presented in the weaker ones. As it can be seen in Table 3, results for the texts per-taining to the news category obtain better results, most of all news titles. This is due to the fact that such texts, although they contain a few words, have a more direct and stronger emotional charge than direct speech (which may be biased by the need to be diplomatic, find the best suited words etc.). Finally, the error analysis showed that emo-tion that is directly reported by the persons expe-riencing is more ?hidden?, in the use of words carrying special signification or related to gen-eral human experience. This fact makes emotion detection in such texts a harder task. Neverthe-less, the results in all corpora are comparable, showing that the approach is robust enough to handle different text types. All in all, the results obtained using the fine and coarse-grained anno-tations in EmotiBlog increased the performance of emotion detection as compared to the systems in the SemEval competition.   6.3 Discussion on the overall results  From the results obtained, we can see that this approach combining the features extracted from the EmotiBlog fine and coarse-grained annota-tions helps to balance between the results ob-tained for precision and recall. The impact of using additional resources that contain opinion words is that of increasing the recall of the sys-tem, at the cost of a slight drop in precision, which proves that the approach is robust enough so that additional knowledge sources can be added. Although the corpus is small, the results obtained show that the phenomena it captures is relevant in the OM task, not only for the blog sphere, but also for other types of text (newspa-per articles, self-reported affect). 8 Conclusions and future work Due to the exponential increase of the subjective information result of the high-level usage of the Internet and the Web 2.0, NLP able to process this data are required. In this paper we presented 
the procedure by which we compiled a multilin-gual corpus of blog posts on different topics of interest in three languages: Spanish, Italian and English. Further on, we explained the need to create a finer-grained annotation schema that can be used to improve the performance of subjectiv-ity mining systems. Thus, we presented the new annotation model, EmotiBlog and justified the benefits of this detailed annotation schema, pre-senting the sources and the reasons taken into consideration when building up the corpus and its labeling. Furthermore, we addressed the pres-ence of ?copy and pastes? from news articles or other blogs, the frequent quotes. For solving this possible ambiguity we included the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We performed several experiments on three dif-ferent corpora, aimed at finding and classifying both the opinion, as well as the expressions of emotion they contained; we showed that the fine and coarse-grained levels of annotation that EmotiBlog contains offers important information on the structure of affective texts, leading to an improvement of the performance of systems trained on it. Although the EmotiBlog corpus is small, the results obtained are promising and show that the phenomena it captures are relevant in the OM task, not only for the blog sphere, but also for other textual-genres. It is well known that OM is an extremely challenging task and a young discipline, thus there is room for im-provement above all to solve linguistic phenom-ena such as the correference resolution at a cross document level, temporal expression recognition. In addition to this, more experiments would need to be done in order to verify the complete ro-bustness of EmotiBlog. Last but not least, our idea is to include the existing tools for a more effective semi-supervised annotation. After the training of the ML system we obtain automati-cally some markables which have to be validated or not by the annotator and the ideal option would be to connect these terms the system de-tects automatically with tools, such as the map-ping with an opinion lexicon based on WordNet (SentiWordNet, WordNet Affect, MicroWord-Net), in order to automatically annotate all the synonyms and antonyms with the same or the opposite polarity respectively and assigning them some other elements contemplated into the Emo-tiBlog annotation schema. This would mean an important step forward for saving time during the annotation process and it will also assure a high quality annotation due to the human supervision. 
8
References Balahur A., Steinberger R., Kabadjov M., Zavarella V., van der Goot E., Halkia M., Pouliquen B., and Belyaeva J. 2010. Sentiment Analysis in the News.  In Proceedings of LREC 2010. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. A Comparative Study of Open Domain and Opinion Question Answering Systems for Fac-tual and Opinionated Queries. In Proceedings of the Recent Advances in Natural Language Proc-essing. Balahur A., Montoyo A. 2008. Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification. In Proceed-ings of the AISB 2008 Symposium on Affective Language in Human and Machine, Aberdeen, Scot-land. Balahur A., Steinberger R., Rethinking Sentiment Analysis in the News: from Theory to Practice and back. In Proceeding of WOMSA 2009. Seville. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. Summarizing Threads in Blogs Using Opinion Polarity. In Proceedings of ETTS work-shop. RANLP. 2009. Boldrini E., Balahur A., Mart?nez-Barco P., Montoyo A. 2009. EmotiBlog: a fine-grained model for emotion detection in non-traditional textual gen-res. In Proceedings of WOMSA. Seville, Spain. Boldrini E., Fern?ndez J., G?mez J.M., Mart?nez-Barco P. 2009. Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres. In Proceedings of WOMSA 2009. Seville, Spain. Chaovalit P, Zhou L. 2005. Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches. In Proceedings of HICSS-05. Carletta J. 1996. Assessing agreement on classification task: the kappa statistic. Computa-tional Linguistics, 22(2): 249?254. Cui H., Mittal V., Datar M. 2006. Comparative Ex-periments on Sentiment Classification for Online Product Reviews. In Proceedings of the 21st Na-tional Conference on Artificial Intelligence AAAI. Cerini S., Compagnoni V., Demontis A., Formentelli M., and Gandini G. 2007. Language resources and linguistic theory: Typology, second language ac-quisition. English linguistics (Forthcoming), chap-ter Micro-WNOp: A gold standard for the evalua-tion of automatically compiled lexical resources for opinion mining. Franco Angeli Editore, Milano, IT. Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005. Identifying Sources of Opinions with Conditional Random  Fields and Extraction Patterns.  In Pro-ceedings of the HLT/EMNLP.  Dave K., Lawrence S., Pennock, D. ?Mining the Pea-nut Gallery: Opinion Extraction and Semantic Classification of Product Reviews?. In Proceedings of WWW-03. 2003. 
Esuli A., Sebastiani F. 2006. SentiWordNet: A Pub-licly Available Resource for Opinion Mining. In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy.  Gamon M., Aue S., Corston-Oliver S., Ringger E. 2005. Mining Customer Opinions from Free Text. Lecture Notes in Computer Science. Goldberg A.B., Zhu J. 2006. Seeing stars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Process-ing. Hu M., Liu B. 2004. Mining Opinion Features in Cus-tomer Reviews. In Proceedings of Nineteenth Na-tional Conference on Artificial Intelligence AAAI. Hatzivassiloglou V., Wiebe J. 2000. Effects of adjec-tive orientation and gradability on sentence subjec-tivity. In Proceedings of COLING.  Kim S.M., Hovy E. 2004. Determining the Sentiment of Opinions. In Proceedings of COLING. Mullen T., Collier N. 2006. Sentiment Analysis Using Support Vector Machines with Diverse Information Sources. In Proceedings of EMNLP. 2004. Lin, W.H., Wilson, T., Wiebe, J., Hauptman, A. ?Which Side are You On? Identifying Perspectives at the Document and Sentence Levels?. In Proceedings of the Tenth Conference on Natural Language Learn-ing CoNLL.2006.  Ng V., Dasgupta S. and Arifin S. M. 2006. Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Re-views. In the proceedings of the ACL, Sydney. Pang B., Lee L., Vaithyanathan S. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Lan-guage Processing. Riloff E., Wiebe J. 2003. Learning Extraction Pat-terns for Subjective Expressions. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.  Strapparava C. Valitutti A. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings ofthe 4th International  Conference on Language Resources and Evaluation, LREC. Russell J.A. 1983. Pancultural aspects of the human conceptual organization of emotions. Journal of Personality and Social Psychology 45: 1281?8. Scherer K. R. 2005. What are emotions? And how can they be measured? Social Science Information, 44(4), 693?727. Stoyanov V. and Cardie C. 2006. Toward Opinion Summarization: Linking the Sources. COLING-ACL. Workshop on Sentiment and Subjectivity in Text. Stoyanov V., Cardie C., Litman D., and Wiebe J. 2004. Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and An-
9
swer Corpus. AAAI Spring Symposium on Explor-ing Attitude and Affect in Text.  Strapparava and Mihalcea, 2007 - SemEval 2007 Task 14: Affective Text. In  Proceedings of the ACL.   Turney P. 2002. Thumbs Up or Thumbs Down? Se-mantic Orientation Applied to Unsupervised Clas-sification of Reviews. ACL 2002: 417-424. Turney P., Littman M. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems 21. Uspensky B. 1973. A Poetics of Composition. Univer-sity of California Press, Berkeley, California. Wiebe J. M. 1994. Tracking point of view in narra-tive. Computational Linguistics, vol. 20, pp. 233?287. Wiebe J., Wilson T. and Cardie C. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation. Wilson T., Wiebe J., Hwa R. 2004. Just how mad are you? Finding strong and weak opinion clauses. In: Proceedings of AAAI. Wiebe J., Wilson T. and Cardie C. 2005. ?Annotation Expressions of Opinions and Emotions in Lan-guage. Language Resources and Evaluation.  Wiebe J., Riloff E. 2005. Creating Subjective and Objective Sentence Classifiers from Unannotated Texts. In Proceedings of the 6th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLing).      
10
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 153?160,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
EMOCause: An Easy-adaptable Approach to Emotion Cause Contexts 
 Irene Russo  Tommaso Caselli Francesco Rubino ILC ?A.Zampolli? ? CNR  Via G. Moruzzi, 1- 56124 Pisa {irene.russo}{tommaso.caselli}{francesco.rubino}@ilc.cnr.it Ester Boldrini  Patricio Mart?nez-Barco DSLI ? University of Alicante Ap. de Correos, 99 ? 03080 Alicante {eboldrini}{patricio}@dlsi.ua.es    Abstract 
In this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from Italian newspaper articles (La Repubblica Corpus). Our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. Our approach has been evaluated with respect to manually annotated data. The results obtained so far are satisfying and support the validity of the methodology proposed. 
1 Introduction As it has been demonstrated in Balahur et al (2010), mining the web to discriminate between objective and subjective content and extract the relevant opinions about a specific target is today a crucial as well as a challenging task due to the growing amount of available information.  Opinions are just a part of the subjective content, which is expressed in texts. Emotions and emotional states are a further set of subjective data. Natural Language is commonly used to express emotions, attribute them and, most importantly, to indicate their cause(s).  Due to the importance of linking the emotion to its cause, a recent subtask of Sentiment Analysis 
(SA) consists in the detection of the emotion cause event (ECE, Lee et al, 2010; Chen et al, 2010) and focuses on the identification of the phrase (if present, as in 1 in bold) mentioning the event that is related to the emotional state (in italics):   (1) Non poteva mancare un accenno alla strage di Bologna, che costringe l' animo a infinita vergogna. [There was a mention of Bologna massacre,   that forces us to feel ashamed.]  This kind of information is extremely interesting, since it can provide pragmatic knowledge about content words and their emotional/subjective polarity and consequently it can be employed for building up useful applications with practical purposes.  The paper focuses on the development of a method for the identification of Italian sentences which contain an emotion cause phrase. Our approach is based on the interplay between linguistic patterns which allow the retrieval of emotion ? emotion cause phrase couples and on the exploitation of an associated incremental repository of commonsense knowledge about events which elicit emotions or emotional states. The methodology is only partially language dependent and this approach can be easily extended to other languages such as Spanish. The repository is one of the main results of this work. It allows the discovery of pragmatic knowledge associated with various content 
153
words and can assign them a polarity value which can be further exploited in more complex SA and Opinion Mining tasks. The present paper is structured as follows. Section 2 shortly describes related work and state of the art on this task. Section 3 focuses on the description of the methodology. Section 4 describes the annotation scheme and the corpus used for the creation of the test set. Section 5 reports on the experiments and their results. Conclusions and future works are described in Section 6. 1 Related Works Emotional states are often triggered by the perception of external events (pre-events) (Wierzbicka, 1999). In addition to this, emotional states can also be the cause of events (post-events; Chun-Ren, 2010). This suggests to consider emotional states as a pivot and structure the relations between emotional states and related events as a tri-tuple of two pairs:  (2) <<pre-events, emotional state> <emotional state, post-event>>  This study focuses on the relationship between the first pair of the tri-tuple, namely pre-events (or ECE), and emotional states.  Previous works on this task have been carried out for Chinese (Lee et al, 2009, Chen et al, 2009, Lee et al, 2010). ECE can be explicitly expressed as arguments, events, propositions, nominalizations and nominals. Lee et al(2010) restrict the definition of ECE as the immediate cause of the emotional state which does not necessarily correspond to the actual emotional state trigger or what leads to the emotional state.  Their work considers all possible linguistic realization of EKs (nouns, verbs, adjectives, prepositional phrases) and ECEs.  On the basis of an annotated corpus, correlations between emotional states and ECEs have been studied in terms of different linguistic cues (e.g. position of the cause events, presence of epistemic markers...) thus identifying seven groups of cues. After that, they have been implemented in a rule-based system, which is able to identify: i.) the EK; ii.) the ECE and its position (same sentence as the EK, previous sentence with 
respect to the EK, following sentence with respect to the EK) and (iii.) the experiencer of the emotional state(s).The system evaluation has been performed on the annotated corpus in two phases: firstly, identifying those sentences containing a co-occurrence of EK and ECE; secondly, for those contexts where an EK and ECE co-occurs, identifying the correct ECE. Standard Precision, Recall and F-measure have been used. The baseline is computed by assuming that the first verb on the left of the EK is the ECE. The system outperforms the baseline f-score by 0.19. Although the results are not very high, the system accuracy for the detection of ECEs is reported to be three times more accurate than the baseline. 2 Emotional states between linguistic patterns and commonsense knowledge The work of Lee et al (2010) represents the starting point for the development of our method. We depart from their approach in the following points: i.) use of data mining techniques (clustering plus a classifier) to automatically induce the rules for sentential contexts in which an event cause phrase is expressed; and ii.) exploitation of a commonsense repository of EK - eliciting ECE noun couples for the identification of the correct ECE noun. The remaining of this section will describe in details the creation of the repository and the methodology we have adopted. 2.1 A source for commonsense knowledge of EKs and ECEs in Italian Recently crowdsourcing techniques that exploit the functionalities of the Web 2.0 have been used in AI and NLP for reducing the efforts, costs and time for the creation of Language Resources. We have exploited the data from an on-line initiative launched in December 2010 by the Italian newspaper ?Il Corriere della Sera? which asked its readers to describe the year 2010 with 10 words. 2,378 people participated in the data collection for a total of 22,469 words. We exploited these data to identify preliminary couples of emotional states and cause events, and thus create a repository of affective commonsense knowledge, by extracting all 
154
bigrams realized by nouns for a total of 18,240 couples noun1-noun2. After this operation, an adapted Italian version of WN-Affect (Strapparava ? Valitutti, 2004) obtained by means of mapping procedures through MultiWordNet (MWN) has been applied to each item of the bigrams. By means of a simple query, we have extracted all bigrams where at least one item has an associated sense corresponding to the ?emotion? category in WN-Affect. We have applied WN-Affect again to these results and extracted only those bigrams where the unclassified item corresponded to the WN-Affect label of ?emotion eliciting situation?. Finally, two lists of keywords have been obtained: one denoting EKs (133 lemmas) and the other denoting possible ECEs associated with a specific EK. The possible ECEs have been extended by exploiting MWN synsets and lexical relations of similar-to, pertains-to, attribute and is-value-of. We have filtered the set of ECE keywords by selecting only those nouns whose top nodes uniquely belongs to the following ontological classes, namely: event, state, phenomenon, and act. After this operation we have 161 nominal lemmas of possible ECEs.  2.2 Exploiting the repository for pattern induction The preliminary version of the repository of EK - ECE couples has been exploited in order to identify relevant syntagmatic patterns for the detection of nominal ECEs. The pattern induction phase has been performed on a parsed version of a large corpus of Italian, the La Repubblica Corpus (Baroni et al, 2004).  We have implemented a pattern extractor that takes as input the couples of the seed words from the commonsense repository and extracted all combinations of EKs and its/their associated ECEs occurring in the same sentence, with a distance ranging from 1 to 8 possible intervening parts-of-speech. We have thus obtained 1,339 possible patterns. This set has been cleaned both on the basis of pattern frequencies and with manual exploration. In total 47 patterns were selected and were settled among the features for the clustering and classifier ensemble which will be exploited for the identification of the 
sentential contexts which may contain an emotion cause phrase (see Section 5 for details). 3 Developing a gold standard and related annotation scheme With the purpose of evaluating the validity and reliability of our approach, a reference annotated corpus (gold standard) has been created.  The data collection has been performed in a semi-automatic way. In particular, we have extracted from an Italian lexicon, SIMPLE/CLIPS (Ruimy et al, 2003), all nouns marked with semantic type ?Sentiment? to avoid biases for the evaluation and measure the coverage of the commonsense repository. The keywords have been used to query the La Repubblica Corpus and thus creating the corpus collection. We have restricted the length of the documents to be annotated to a maximum of three sentences, namely the sentence containing the emotion keyword, the one preceding it and the sentence immediately following. As a justification for this choice, we have assumed that causes are a local focus discourse phenomenon and should not be found at a long distance with respect to their effects (i.e. the emotion keyword). Finally, the corpus is composed by 6,000 text snippets for a total of 738,558 tokens.  The corresponding annotation scheme, It-EmoCause, is based on recommendations and previous experience in event annotation (ISO-TimeML), emotion event annotation (Lee et al, 2009, Chen et al, 2010), emotion and affective computing annotation (EARL1, the HUMAINE Emotion Annotation and Representation Language, EmotiBlog, Boldrini et al 2010). The scheme applies at two levels: phrase level and token level and it allows nested tags. Figure 1 reports the BNF description of the scheme.  Text consuming markables are  <emotionWord>, <causePhrase> and <causeEmotion> tags, which are responsible, respectively, for marking the emotion keyword, the phrase expressing the cause emotion event and the token expressing the cause emotion. The values of the attribute emotionClass is derived from Ekman                                                            1 http://emotion-research.net/earl 
155
(1972)'s classification and extended with the value UNDERSPECIFIED. This value is used as a cover term for all other types of emotion reducing disagreement and allowing further classifications on the basis of more detailed and different lists of emotions that each user can specify. Finally, the non-text consuming <EmLink> link puts in relation the cause emotion event or phrase with the emotion keyword.  entry ::= <emotionWord> <causePhrase>+ <ELink>*  <emotionWord> ::= ewid lemma emotionClass appraisalDimension, emotionHolder polarity comment ewid ::= ew<digit> lemma ::= CDATA emotionClass ::= HAPPINESS | ANGER | FEAR | SURPRISE| SADNESS| DISGUST |               UNDERSPECIFIED appraisalDimension ::= CDATA emotionHolder ::= CDATA polarity ::= POSITIVE | NEGATIVE comment ::= CDATA  <causePhrase> ::= epid <causeEmotion>+ epid ::= ep<digit> <causeEmotion> ::= eid lemma eid ::= e<digit> lemma ::= CDATA  <EmLink> ::= elid linkType emotionInstanceID causeEventInstanceID causePhraseID comment elid ::= el<digit> linkType ::= POSITIVE | NEGATIVE  relatedToEmotion ::= IDREF {relatedToEmotion ::= ewid} causeEventID ::= IDREF {causeEventID ::= eid} causePhraseID  ::= IDREF {causePhraseID ::= epid} comment ::= CDATA Figure 1 ? BNF description of the EmoContext Scheme  The annotation has been performed by two expert linguists and validated by a judge. The tool used for the annotation is the Brandeis Annotation Tool (BAT)2. The corpus is currently under annotation and we concentrated mainly on the development of a test set. Not all markables and attributes have been annotated in this phase.  
                                                           2 http://www.batcaves.org/bat/tool/ 
The inter-annotator agreement (IAA)3 on the detection of the cause event and the cause phrase are not satisfactory. To have reliable data, we have adopted a correction strategy by asking the annotators to assign a common value to disagreements. This has increased the IAA on cause emotion to K=0.45, and P&R= 0.46. A revision procedure of the annotation guidelines is necessary and annotation specifications must be developed so that the disagreement can be further reduced. Table 1 reports the figures about the annotated data so far.  It-EmoContext Corpus # of tokens 32,525 # of emotion keyword 356 # of cause emotion 84 # of causePhrase emotion  104 # emotion ? cause emotion couples 95 # of emotion ? cause phrase couples 121 Agreement on emotion keyword detection K = 0.91 P&R = 0.91 Agreement on cause emotion detection K = 0.34 P&R = 0.33 Agreement on causePhrase detection K = 0.21 P&R = 0.26 Table 1 - It-EmoContext Corpus Figures 4 Emotion cause detection: experiments and results In order to find out a set of rules for the detection of emotion cause phrase contexts, we experimented a combination of Machine Learning techniques, namely clustering and rule induction classifier algorithms. In particular, we want to exploit the output of a clustering algorithm as input to a rule learner classifier both available in the Weka platform (Witten and Frank, 2005). The clustering algorithm is the Expectation-Maximization algorithm (EM; Hofmann and Puzicha, 1998). The EM is an unsupervised algorithm, commonly used for model-based                                                            3 Cohen's Kappa, Precision and Recall have been used for computing the IAA. 
156
clustering and also applied in SA tasks (Takamura et al 2006). In this work, we equipped the EM clustering model with syntagmatic, lexical and contextual features. The clustering algorithm has been trained on 2,000 corpus instances of the potential EK - ECE couples of the repository from the La Repubblica corpus along with a three sentence context (i.e the sentence immediately preceding and that immediately following the sentence containing the EK). Four groups of features have been identified: the first set of features corresponds to a re-adaptation of the rules implemented in Lee et al (2010); the second set of features implements the 47 syntagmatic patterns that specifically codify the relation between the EK and the ECE (see Section 3.2); the last two set of features are composed, respectively, by a list of intra-sentential bigrams, trigrams and fourgrams for a total of 364 different part-of-speech sequences with the EK as the first element and by a list of 6 relevant collocational patterns which express cause-effect relationship between the  ECE and the EK, manually identified on the basis of the authors' intuitions. In Table 2 some examples of each group of features are reported4.   Group of feature Instance Re-adaptation of Lee et al, 2010's rules Presence of an ECE after the EK in the same sentence Syntagmatic patterns manually identified S E S | S E RI S | S V RI A S ... Bigrams, trigrams and fourgrams POS sequences S EA | S EA AP | S EA AP S  Relevant collocational patterns S A per RD/RI S ... Table 2 ? Features for the EM cluster.  We expected two data clusters, one which includes cause emotion sentential contexts where the EK and the emotion cause co-occurs in the same sentence and another where either 
                                                           4 The tags S, EA, RI and similar reported for the last three groups of features are abbreviations for the POS used by the parser. The complete list can be found at http://medialab.di.unipi.it/wiki/Tanl_POS_Tagset 
the emotion cause it is not present or it occurs in a different sentence (i.e. the one before the EK or in the one following it). In order to evaluate the goodness of the cluster configuration created by the Weka version of the EM algorithm, we have run different clustering experiments. The results of each clustering analysis have been passed to the Weka PART rule-induction classifier. The best results were those which confirmed our working hypothesis, i.e. two clusters. The first cluster contains 869 items while the second 1,131 items.  The PART classifier provided a total of 49 detection rules for the detection of EK ? ECE contexts. The classifier identifies the occurrence of a cause phrase in the same sentence but is not able to identify the noun which corresponds to the ECE. The evaluation of the classifier has been performed on the 121 couples of EK ? cause phrase of the test set. As we are aiming at spotting nominal causes of EKs, we have computed the baseline by considering as the correct phrase containing the ECE the first noun phrase occurring at the right of the emotion keyword and in the same sentence since this kind of ECEs tends to occur mostly at this position. In this way the baseline has an accuracy of  0.14 (only 33 NPs were correct over a total of 227 NPs at the right of the EKs). By applying the rules of the PART classifier, we have obtained an overall accuracy of 0.71, outperforming the baseline. As for the identification of the EK - cause phrase couples occurring in the same sentence, we computed standard Precision, Recall and F-measure. The results are reported in Table 3. The system tends to have a high precision (0.70) and a low recall (0.58).    Total Correct P R F EK ? cause phrase couple 121 85 0.70 0.58 0.63 Table 3 ? Evaluation of the classifier in detecting EF ? cause phrase couples.  After this, we tried to identify the correct nominal ECE in the cause phrase. Provided the reduced dimensions of the annotated corpus, no training set was available to train a further 
157
classifier. Thus, to perform this task we decided to exploit the commonsense repository. However, the first version of the repository is too small to obtain any relevant results. We enlarged it by applying two set of features (the syntagmatic patterns manually identified and the collocational patterns used for the clustering analysis). 4.1 Incrementing the repository and discovering EK ? ECE couples Our hypothesis is that the identification of the ECE(s) in context could be performed by looking for a plausible set of nouns which are associated with a specific EK and assumed to be its cause. This type of information is exactly the one contained in the repository described in Section 3.1. In order to work with a larger data set of ECE entries per emotion keyword, we have applied the syntagmatic patterns manually identified and the collocational patterns on two corpora: i.) La Repubblica and ii.) ItWaC5 (Baroni et al, 2009). For each EK - ECE couple identified we have kept track of the co-occurrence frequencies and computed the Mutual Information (MI). Frequency and MI are extremely relevant because they provide a reliability threshold for each couple of EK and ECE. In Table 4 we report some co-occurrences of the EK ?ansia? [anxiety] and ECEs.  ECE Frequency (La Repubblica Corpus) Mutual  Information crisi [crisis] 119 5,514 angoscia [anguish] 80 8.762 guerra [war] 185 6.609 pianificazione [planning] 1 4.117 ricostruzione [reconstruction] 19 5.630 Table 4- ECEs co-occurrences with EK ?ansia?[anxiety].  Each ECE has been associated to a probability measure of eventivity derived from MWN top                                                            5 http://wacky.sslmit.unibo.it 
ontological classes, obtained from the ratio between 1 and the sum of all top ontological classes associated to the ECE lemma. The top nodes ?event?, ?state?, ?phenomenon?, and ?act? have been considered as a unique top class by applying the TimeML definition of event6. This measure is useful in case more than one ECEs is occurring in the context in analysis as a disambiguation strategy. In fact, if more than one ECEs is present, that with the higher frequency, MI and eventivity score should be preferred.  Furthermore, to make the repository more effective and also to associate an emotional polarity to the ECEs (i.e. whether they have positive, negative or neutral values) we have further extended the set of information by exploiting WN-Affect 1.1. In particular we have associated each EK to its emotional category (e.g. despondency, resentment, joy) and its emotional superclass (e.g. positive-emotion, negative-emotion, ambiguous-emotion).  This extended version of the repository has been applied to identify the correct ECE noun for the 95 couples of EK ? ECE in the test set. We have splitted the whole set of EK ? ECE couples into two subgroups: i.) EK ? ECE couples occurring in the same sentence (82/95); and ii.) EK ? ECE couples occurring in different sentences (13/95). By applying the repository to the first group, we were able to correctly identify 50% (41/82) of the ECE nouns for each specific EK when occurring in the same sentence. Moreover, we applied the repository also to the EK ? ECE couples of the second group: a rough 30.76% (4/13) of the ECE occurring in sentences other that the one containing the EK can be correctly retrieved without increasing the number of false positives. This is possible thanks to the probability score computed by means of MWN top ontological classes, even if the number of annotated examples is too small to justify strong conclusions. 
                                                           6 To clarify, the ECE ?guerra? [war] has four senses in MWN. Three of them belong to the top ontological class of ?event? and one to ?state?. This possible ECE has 1 top ontological node, and its eventivity mesure is 1. 
158
5 Conclusions and future works  In this paper we describe a methodology based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge of EK ? ECE couples, which can be integrated into more complex systems for SA and Opinion Mining. The experimental results show that clustering techniques (EM clustering model) and a rule learner classifier (the PART classifier) can be efficiently combined to select and induce relevant linguistic patterns for the discovery of EK ? ECE couples in the same sentence. The information thus collected has been organized into the repository of commonsense knowledge about emotions and their possible causes. The repository has been extended by using corpora of varying dimensions (la Repubblica and ItWaC) and effectively used to identify ECEs of specific emotion keywords.  One interesting aspect of this approach is represented by the reduced manual effort both for the identification of linguistic patterns for the extraction of reliable information and for the maintenance and extension of specific language resources which can be applied also to domains other than SA. In addition to this, the method can be extended and applied to identify ECE realized by other POS, such as verbs and adjectives. As future works, we aim to extend the repository by extracting data from the Web and connecting it to SentiWordNet and WN-Affect. In particular, the connection to the existing language resources could be used to spot possible misclassifications and polarity values. Acknowledgments The authors want to thank the RSC Media Group. This work has been partially founded by the projects TEXTMESS 2.0 (TIN2009-13391-C04-01), Prometeo (PROMETEO/2009/199), the Generalitat valenciana (ACOMP/2011/001) and the EU FP7 project METANET (grant agreement n? 249119) References  Baccianella S., A. Esuli and F. Sebastiani. (2010) . SentiWordNet 3.0: An Enhanced Lexical Resource 
for Sentiment Analysis and Opinion Mining. In:  Proceedings of the 7th conference on International Language Resources and Evaluation (LREC 2010), Malta, May 2010 Balahur A., R. Steinberger, M.A. Kabadjov, V. Zavarella, E. van der Goot, M. Halkia, B. Pouliquen, J. Belyaeva. (2010). Sentiment Analysis in the News. In: Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), Malta, May 2010.  Baroni, M., Bernardini, S., Comastri, F., Piccioni, L., Volpi, A., Aston, G.,Mazzoleni, M. (2004). Introducing the ?la Repubblica? corpus: A large, annotated, TEI(XML)-compliant corpus of newspaper italian. In: Proceedings of the 4th International conference on Language Resources and Evaluation (LREC-04), Lisbon, May 2004.  Boldrini E, A. Balahur, P. Mart?nez-Barco and A. Montoyo. (2010). EmotiBlog: a finer-grained and more precise learning of subjectivity expression models. In: Proceedings of the Fourth Linguistic Annotation Workshop (LAW IV '10). Association for Computational Linguistics. Chen Y., S.Y.M. Lee, S. Li, and C. Huang. (2010) Emotion Cause Detection with Linguistic Constructions. In: Proceeding of the 23rd International Conference on Computational Linguistics (COLING 2010). Ekman, P. (1972). Universals And Cultural Differences In Facial Expressions Of Emotions.In: J. Cole (ed.), Nebraska Symposium on Motivation, 1971. Lincoln, Neb.: University of Nebraska Press, 1972. pp. 207- 283.3. Huang, C. (2010). Emotions as Events (and Cause as Pre-Events). Communication at the Chinese Temporal/discourse annotation workshop, Los Angeles, June 2010,. Lee S.Y.M., Y. Chen, C. Huang. (2010). A Text-driven Rule-based System for Emotion Cause Detection. In: Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. Pianta, E., Bentivogli, L., Girardi, C. (2002). Multiwordnet: Developing and aligned multilingual database. In: Proceedings of the First International Conference on Global WordNet, Mysore, India, January 2002. Pustejovsky, J., Castao, J., Saur`?, R., Ingria, R., Gaizauskas, R., Setzer, A., Katz, G. (2003). TimeML: Robust specification of event and temporal expressions in text. In: Proceedings of 
159
the 5th International Workshop on Computational Semantics (IWCS-5). Ruimy, N., Monachini, M., Gola, E., Calzolari, N., Fiorentino, M.D., Ulivieri, M., Rossi, S. (2003). A computational semantic lexicon of italian: SIMPLE. In: Linguistica Computazionale XVIII-XIX, Pisa, pp. 821?64 Schroeder M., H. Pirker and M. Lamolle. (2006). First Suggestion for an Emotion Annotation and Representation Language. In: Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa, May 2006.  
Strapparava C. and A. Valitutti. (2004) WordNet-Affect: an affective extension ofWordNet". In: Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, May 2004. Takamura H., I. Takashi, M. Okumura. (2006). Latent Variables Models for Semantic Orientation of Phrases. In: Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2006).  Wierzbicka, A. (1999) Emotion Across Languages and Cultures Diversity and Universals. Cambidge.CUP. 
160
