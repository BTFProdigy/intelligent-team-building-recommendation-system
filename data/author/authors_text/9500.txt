N-gram-based Machine Translation
Jose? B. Marin?o?
Rafael E. Banchs?
Josep M. Crego?
Adria` de Gispert?
Patrik Lambert?
Jose? A. R. Fonollosa?
Marta R. Costa-jussa`?
Universitat Polite`cnica de Catalunya
This article describes in detail an n-gram approach to statistical machine translation. This ap-
proach consists of a log-linear combination of a translation model based on n-grams of bilingual
units, which are referred to as tuples, along with four specific feature functions. Translation
performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English
and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
1. Introduction
The beginnings of statistical machine translation (SMT) can be traced back to the early
fifties, closely related to the ideas from which information theory arose (Shannon and
Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during
World War II. According to this view, machine translation was conceived as the problem
of finding a sentence by decoding a given ?encrypted? version of it (Weaver 1955).
Although the idea seemed very feasible, enthusiasm faded shortly afterward because of
the computational limitations of the time (Hutchins 1986). Finally, during the nineties,
two factors made it possible for SMT to become an actual and practical technology:
first, significant increment in both the computational power and storage capacity of
computers, and second, the availability of large volumes of bilingual data.
The first SMT systems were developed in the early nineties (Brown et al 1990, 1993).
These systems were based on the so-called noisy channel approach, which models the
probability of a target language sentence T given a source language sentence S as the
product of a translation-model probability p(S|T), which accounts for adequacy of trans-
lation contents, times a target language probability p(T), which accounts for fluency
of target constructions. For these first SMT systems, translation-model probabilities at
the sentence level were approximated from word-based translation models that were
trained by using bilingual corpora (Brown et al 1993). In the case of target language
probabilities, these were generally trained from monolingual data by using n-grams.
Present SMT systems have evolved from the original ones in such a way that
mainly differ from them in two respects: first, word-based translation models have been
? Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain.
Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for
publication: 5 July 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och,
and Marcu 2003) which are directly estimated from aligned bilingual corpora by consid-
ering relative frequencies, and second, the noisy channel approach has been expanded
to a more general maximum entropy approach in which a log-linear combination of
multiple feature functions is implemented (Och and Ney 2002).
As an extension of the machine translation problem, technological advances in the
fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it
possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron,
and Norvig 1992). According to this, SMT has also been approached from a finite-state
point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini,
and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi
2000). In this SMT approach, translation models are implemented by means of finite-
state transducers for which transition probabilities are learned from bilingual data.
As opposed to phrase-based translation models, which consider probabilities between
target and source units referred to as phrases, finite-state translation models rely on
probabilities among sequences of bilingual units, which are defined by the transitions
of the transducer.
The translation system described in this article implements a translation model that
has been derived from the finite-state perspective?more specifically, from the work of
Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier
work the translation model is implemented by using a finite-state transducer, in the sys-
tem presented here the translation model is implemented by using n-grams. In this way,
the proposed translation system can take full advantage of the smoothing and consist-
ency provided by standard back-off n-gram models. The translation model presented
here actually constitutes a language model of a sort of ?bilanguage? composed of bilin-
gual units, which will be referred to as tuples (de Gispert and Marin?o 2002). An alterna-
tive approach, which relies on bilingual-unit unigram probabilities, was developed by
Tillmann and Xia (2003); in contrast, the approach presented here considers bilingual-
unit n-gram probabilities. In addition to the tuple n-gram translation model, the
translation system presented here implements four specific feature functions that are
log-linearly combined along with the translation model for performing the decoding
(Marin?o et al 2005).
This article is intended to provide a detailed description of the n-gram-based
translation system, as well as to demonstrate the system performance in a wide-
domain, large-vocabulary translation task. The article is structured as follows. First,
Section 2 presents a complete description of the n-gram-based translation model. Then,
Section 3 describes in detail the additional feature functions that, along with the trans-
lation model, compose the n-gram-based SMT system implemented. Section 4 describes
the European Parliament Plenary Session (EPPS) data, as well as the most relevant
details about the translation tasks considered. Section 5 presents and discusses the
translation experiments and their results. Finally, Section 6 presents some conclusions
and intended further work.
2. The Tuple N-gram Model
This section describes in detail the tuple n-gram translation model, which constitutes
the core model implemented by the n-gram-based SMT system. First, the bilingual unit
definition and model computation are presented in Section 2.1. Then, some important
refinements to the basic translation model are provided and discussed in Section 2.2.
Finally, Section 2.3 discusses issues related to n-gram-based decoding.
528
Marin?o et al N-gram-based Machine Translation
2.1 Tuple Extraction and Model Computation
As already mentioned, the translation model implemented by the described SMT sys-
tem is based on bilingual n-grams. This model actually constitutes a language model of
a particular bilanguage composed of bilingual units that are referred to as tuples. In this
way, the translation model probabilities at the sentence level are approximated by using
n-grams of tuples, such as described by the following equation:
p(T, S) ?
K
?
k=1
p((t, s)k|(t, s)k?1, (t, s)k?2, . . . , (t, s)k?n+1) (1)
where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual
sentence pair. It is important to note that since both languages are linked up in tuples,
the context information provided by this translation model is bilingual.
Tuples are extracted from a word-to-word aligned corpus in such a way that a
unique segmentation of the bilingual corpus is achieved. Although in principle any
Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary
depends highly on the particular alignment set considered, and this impacts the trans-
lation results. According to our experience, the best performance is achieved when
the union of the source-to-target and target-to-source alignment sets (IBM models;
Brown et al [1993]) is used for tuple extraction (some experimental results regarding
this issue are presented in Section 4.2.2). Additionally, the use of the union can also
be justified from a theoretical point of view by considering that the union set typically
exhibits higher recall values than do other alignment sets such as the intersection and
source-to-target.
In this way, as opposed to other implementations, where one-to-one (Bangalore
and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used,
tuples are extracted from many-to-many alignments. This implementation produces
a monotonic segmentation of bilingual sentence pairs, which allows for simulta-
neously capturing contextual and reordering information into the bilingual translation
unit structures. This segmentation also allows for estimating the n-gram probabil-
ities appearing in (1). In order to guarantee a unique segmentation of the corpus,
tuple extraction is performed according to the following constraints (Crego, Marin?o,
and de Gispert 2004):
 a monotonic segmentation of each bilingual sentence pair is produced,
 no word inside the tuple is aligned to words outside the tuple, and
 no smaller tuples can be extracted without violating the previous
constraints.
Notice that, according to this, tuples can be formally defined as the set of shortest
phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1
presents a simple example illustrating the unique tuple segmentation for a given pair of
sentences, as well as the complete phrase set.
The first important observation from Figure 1 is related to the possible occurrence
of tuples containing unaligned elements on the target side. This is the case for tuple 1.
Tuples of this kind should be handled in an alternative way for the system to be able
to provide appropriate translations for such unaligned elements. The problem of how
529
Computational Linguistics Volume 32, Number 4
Figure 1
Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the
set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence
pair is achieved.
to handle this kind of situation, which we refer to as involving source-nulled tuples, is
discussed in detail in Section 2.2.2.
Also, as observed from Figure 1, the total number of tuples is significantly lower
than the total number of phrases, and, in most of the cases, longer phrases can be
constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10,
and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple
representation is not able to provide translations for individual words that appear tied
to other words unless they occur alone in some other tuple. This problem, which we
refer to as embedded words, is discussed in detail in Section 2.2.1.
Another important observation from Figure 1 is that each tuple length is implicitly
defined by the word links in the alignment. As opposed to phrase-extraction proce-
dures, for which a maximum phrase length should be defined to avoid a vocabulary
explosion, tuple extraction procedures do not have any control over tuple lengths.
According to this, the tuple approach will strongly benefit from the structural similarity
between the languages under consideration. Then, for close language pairs, tuples are
expected to successfully handle those short reordering patterns that are included in
the tuple structure, as in the case of ?traducciones perfectas : perfect translations?
presented in Figure 1. On the other hand, in the case of distant pairs of languages, for
which a large number of long tuples are expected to occur, the approach will more easily
fail to provide a good translation model due to tuple sparseness.
2.2 Translation Model Refinements
The basic n-gram translation model, as defined in the previous section, exhibits some
important limitations that can be easily overcome by incorporating specific changes in
530
Marin?o et al N-gram-based Machine Translation
either the tuple vocabulary or the n-gram model. This section describes such limitations
and provides a detailed description of the implemented refinements.
2.2.1 Embedded Words. The first issue regarding the n-gram translation model is related
to the already mentioned problem of embedded words, which refers to the fact that
the tuple representation is not able to provide translations for individual words all the
time. Embedded words can become a serious drawback when they occur in relatively
significant numbers in the tuple vocabulary.
Consider for example the word translations in Figure 1. As seen from the figure, this
word appears embedded into tuple ?traducciones perfectas : perfect translations.? If a
similar situation is encountered for all other occurrences of that word in the training
corpus, then no translation probability for an independent occurrence of that word
will exist. A more relevant example would be the case of the embedded word perfect
since this adjective always moves relative to the noun it is modifying. In this case,
providing the translation system with a word-to-word translation probability for ?per-
fectas : perfect? only guarantees that the decoder will have a translation option for an
isolated occurrence of such words but does not guarantee anything about word order.
So, certainly, any adjective?noun combination including the word perfect, which has not
been seen during the training stage, will be translated in the wrong order. Accordingly,
the problem resulting from embedded words can be partially solved by incorporating a
bilingual dictionary able to provide word-to-word translation when required by the
translation system. A more complete treatment for this problem must consider the
implementation of a word-reordering strategy for the proposed SMT approach (as will
be discussed in Section 6, this constitutes one of the main concerns for our further
research).
In our n-gram-based SMT implementation, the following strategy for handling em-
bedded words is considered. First, one-word tuples for each detected embedded word
are extracted from the training data and their corresponding word-to-word translation
probabilities are computed by using relative frequencies. Then, the tuple n-gram model
is enhanced by including all embedded-word tuples as unigrams into the model. Since
a high-precision alignment set is desirable for extracting such one-word tuples and
estimating their probabilities, the intersection of both alignments, source to target and
target-to-source, is used instead of the union.
In the particular case of the EPPS tasks considered in this work, embedded words
do not constitute a real problem because of the great amount of training material and
the reduced size of the test data set (see Section 4.1 for a detailed description of the
EPPS data set). On the contrary, in other translation tasks with less available training
material, the embedded-word handling strategy described above has been very useful
(de Gispert, Marin?o, and Crego 2004).
2.2.2 Tuples with Empty Source Sides. The second important issue regarding the
n-gram translation model is related to tuples with empty source sides, hereinafter
referred to as source-nulled tuples. In the tuple n-gram model implementation, it fre-
quently happens that some target words linked to NULL end up producing tuples with
NULL source sides. Consider, for example, the first tuple of the example presented in
Figure 1. In this example, ?NULL : we? is a source-nulled tuple if Spanish is considered
to be the source language. Notice that tuples of this kind cannot be allowed since no
NULL is expected to occur in a translation input.
The classical solution to this problem in the finite-state transducer framework is
the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi
531
Computational Linguistics Volume 32, Number 4
2000). However, epsilon arcs significantly increase decoding complexity. In our n-gram
system implementation, this problem is easily solved by preprocessing the union set of
alignments before extracting tuples, in such a way that any target word that is linked
to NULL is attached to either its preceding word or its following word. In this way, no
target word remains linked to NULL, and source-nulled tuples will not occur during
tuple extraction.
Some different strategies for handling target words aligned to NULL have been
considered. In the simplest strategy, which will be referred to as the attach-to-right strat-
egy, target words aligned to NULL are always attached to their following word. This
simple strategy happens to provide better results, for English-to-Spanish and Spanish-
to-English translations, than the opposite one (attachment to the previous word), and
also better than a more sophisticated strategy that considers bigram probabilities for
deciding whether a given word should be attached to the following or to the pre-
vious one.
Notice that in the particular cases of Spanish and English, the attach-to-right strat-
egy can be justified heuristically. Indeed, when translating from Spanish to English,
most of the source-nulled tuples result from omitted verbal subjects, which is a very
common situation in Spanish. This is the case for the first tuple in Figure 1. Suppose,
for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the
tuple ?quisie?ramos : would like? will be replaced by the new tuple ?quisie?ramos : we
would like,? which actually makes a better translation unit, at least from a grammatical
point of view. Similarly, some common situations can be identified for translations in
the English-to-Spanish direction, such as omitted determiners (e.g., ?I want information
about European countries : quiero informacio?n sobre los pa??ses Europeos?). Again,
the attach-to-right strategy for the unaligned Spanish determiner los seems to be the
best one.
Experimental results comparing the attach-to-right strategy to an additional strat-
egy based on a statistical translation lexicon are provided in Section 5.1.3.
2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram transla-
tion model is related to the computational costs resulting from the tuple vocabulary size
during decoding. The idea behind this refinement is to reduce both computation time
and storage requirements without degrading translation performance. In our n-gram-
based SMT system implementation, the tuple vocabulary is pruned by using histogram
counts. This pruning is performed by keeping the N most frequent tuples with common
source sides.
Notice that such a pruning, because it is performed before computing tuple n-gram
probabilities, has a direct impact on the translation model probabilities and then on
the overall system performance. For this reason, the pruning parameter N is critical
for efficient usage of the translation system. While a low value of N will significantly
decrease translation quality, on the other hand, a large value of N will provide the
same translation quality than a more adequate N, but with a significant increment in
computational costs. The optimal value for this parameter depends on data and should
be adjusted empirically for each considered translation task.
2.3 N-gram-based Decoding
Decoding for the n-gram-based translation model is slightly different from phrase-
based decoding. For this reason, a specific decoding tool had to be implemented. This
532
Marin?o et al N-gram-based Machine Translation
section briefly describes MARIE, the n-gram based search engine developed for our
SMT system (Crego, Marin?o, and de Gispert 2005a).
MARIE implements a beam-search strategy based on dynamic programming. The
decoding is performed monotonically and is guided by the source. During decoding,
partial-translation hypotheses are arranged into different stacks according to the total
number of source words they cover. In this way, a given hypothesis only competes with
those hypotheses that provide the same source-word coverage. At every translation
step, stacks are pruned to keep decoding tractable. MARIE allows for two different
pruning methods:
 Threshold pruning: for which all partial-translation hypotheses scoring
below a predetermined threshold value are eliminated.
 Histogram pruning: for which the maximum number of partial-translation
hypotheses to be considered is limited to the K-best ranked ones.
Additionally, MARIE allows for hypothesis recombination, which provides a more
efficient search. In the implemented algorithm, partial-translation hypotheses are re-
combined if they coincide exactly in both the present tuple and the tuple trigram history.
MARIE also allows for considering additional feature functions during decoding.
All these models are taken into account simultaneously, along with the n-gram trans-
lation model. In our SMT system implementation, four additional feature functions are
considered. These functions are described in detail in Section 3.2.
3. Feature Functions for the N-gram-based SMT System
This section describes in detail some feature functions that are implemented along with
the n-gram translation model for the complete translation system. First, in subsection
3.1, the log-linear combination framework and the implemented optimization proce-
dure are discussed. Then, four specific feature functions that constitute our SMT system
are detailed in Section 3.2.
3.1 Log-linear Combination Framework
As mentioned in the Introduction, in recent translation systems the noisy channel ap-
proach has been replaced by a more general approach, which is founded on the princi-
ples of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach,
the corresponding translation for a given source language sentence S is defined by the
target language sentence that maximizes a log-linear combination of multiple feature
functions hi(S, T) (Och and Ney 2002), such as described by the following equation:
argmax
T
?
m
?mhm(S, T) (2)
where ?m represents the coefficient of the mth feature function hm(S, T), which ac-
tually corresponds to a log-scaled version of the mth-model probabilities. Optimal
values for the ?m coefficients are estimated via an optimization procedure by using a
development data set.
533
Computational Linguistics Volume 32, Number 4
3.2 Translation System Features
In addition to the tuple n-gram translation model, our n-gram-based SMT system
implements four feature functions: a target-language model, a word-bonus model, and
two lexicon models. These system features are described next.
3.2.1 Target-language Model. This feature provides information about the target lan-
guage structure and fluency. It favors those partial-translation hypotheses that are more
likely to constitute correctly structured target sentences over those that are not. The
model is implemented by using a word n-gram model of the target language, which is
computed according to the following expression:
hTL(T, S) = hTL(T) = log
K
?
k=1
p(wk|wk?1, wk?2, . . . , wk?n+1) (3)
where wk refers to the kth word in the considered partial-translation hypothesis. Notice
that this model only depends on the target side of the data, and can in fact be trained by
including additional information from other available monolingual corpora.
3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partial-
translation hypothesis length. This is done to compensate for the system preference for
short translations over large ones. The model is implemented through a bonus factor
that directly depends on the total number of words contained in the partial-translation
hypothesis, and it is computed as follows:
hWP(T, S) = hWP(T) = M (4)
where M is the number of words contained in the partial-translation hypothesis.
3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complemen-
tary translation model. This model provides, for a given tuple, a translation probability
estimate between its source and target sides. This feature is implemented by using the
IBM-1 lexical parameters (Brown et al 1993; Och et al 2004). Accordingly, the source-
to-target lexicon probability is computed for each tuple according to the following
equation:
hLF(T, S) = log 1(I + 1)J
J
?
j=1
I
?
i=0
q(tnj |sni ) (5)
where sni and t
n
j are the ith and jth words in the source and target sides of tuple (t, s)n,
with I and J the corresponding total number of words in each side. In the equation,
q(.) refers to IBM-1 lexical parameters, which are estimated from alignments computed
in the source-to-target direction.
3.2.4 Target-to-Source Lexicon Model. Similar to the previous feature, this feature
function constitutes a complementary translation model too. It is computed in ex-
534
Marin?o et al N-gram-based Machine Translation
actly the same way the previous model is, with the only difference that IBM-1 lexical
parameters are estimated from alignments computed in the target-to-source direction
instead.
4. EPPS Translation Task
This section describes in detail the most relevant issues about the translation tasks con-
sidered. Section 4.1 describes the EPPS data set that is used, and Section 4.2 presents the
overall implementation details in regard to preprocessing, training, and optimization.
4.1 Corpus Description
The EPPS data set is composed of the official plenary session transcriptions of the Eu-
ropean Parliament, which are currently available in eleven different languages (Koehn
2002). However, in the case of the results presented here, we have used the Spanish and
English versions of the EPPS data that have been prepared by RWTH Aachen University
in the context of the European Project TC-STAR. The training, development, and test
data used include session transcriptions from April 1996 until September 2004, from
October 21 until October 28, 2004, and from November 15 until November 18, 2004,
respectively.
Table 1 presents the basic statistics for the training, development, and test data sets
for each considered language. More specifically, the statistics shown in Table 1 are the
number of sentences, the number of words, the vocabulary size (or number of distinct
words), the average sentence length in number of words, and the number of available
translation references.
As seen from Table 1, although the total number of words in the training set is
very similar for both languages, vocabulary sizes are substantially different. Indeed,
the Spanish vocabulary is approximately 60% larger than the English vocabulary. This
can be explained by the more inflected nature of Spanish, which is particularly evident
in the case of nouns, adjectives, and verbs, which may have many different forms de-
pending on gender, number, tense, and mode. As will be seen from results presented in
Section 5, this difference in vocabulary size has important consequences in translation
quality for the English-to-Spanish direction.
Regarding the development data set, only 1, 008 sentences were considered. Notice
from Table 1 that in this case, the Spanish vocabulary is 20% larger than the English
Table 1
Basic statistics for the training, development, and test data sets (M and k stand for millions and
thousands, respectively; Lmean refers to the average sentence length in number of words, and
Ref. to the number of available translation references).
Set Language Sentences Words Vocabulary Lmean Ref.
Train English 1.22 M 33.4 M 105 k 23.7 1
Spanish 1.22 M 34.8 M 169 k 28.4 1
Dev. English 1008 26.0 k 3.2 k 25.8 3
Spanish 1008 25.7 k 3.9 k 25.5 3
Test English 1094 26.8 k 3.9 k 24.5 2
Spanish 840 22.7 k 4.0 k 27.0 2
535
Computational Linguistics Volume 32, Number 4
vocabulary. Another important issue regarding the development data set is the number
of unseen words, that is, those words present in the development data that are not
present in the training data. In this case, 35 words (0.13%) out of the total number of
words in the English development set did not occur in the training data. From these 35
words, only 30 corresponded to different words. Similarly, 61 words (0.24%) out of the
total number of words in the Spanish development set were not in the training data. In
this case, 57 different words occurred.
Notice also in Table 1 that a different test set was used for each translation direction,
and although a different number of sentences is considered in each case, vocabulary
sizes are almost equivalent. Regarding unseen words, in this case, 112 words (0.42%) out
of the total number of words in the English test set did not occur in the training data.
From these 112 words, only 81 corresponded to different words. Similarly, 46 words
(0.20%) out of the total number of words in the Spanish test were not in the training
data. In this case, 40 different words occurred.
4.2 Preprocessing, Training, and System Optimization
This section presents the overall implementation details in regard to preprocessing,
training, and optimization of the translation system. Two languages, English and Span-
ish, and both translation directions between them are considered for several different
system configurations.
4.2.1 Preprocessing and Alignment. The training data are preprocessed by using stan-
dard tools for tokenizing and filtering. In the filtering stage, some sentence pairs are
removed from the training data to allow for a better performance of the alignment tool.
Sentence pairs are removed according to the following two criteria:
 Fertility filtering: removes sentence pairs with a word ratio larger than a
predefined threshold value.
 Length filtering: removes sentence pairs with at least one sentence of more
than 100 words in length. This helps to maintain bounded alignment
computational times.
After preprocessing, word-to-word alignments are performed in both directions,
source-to-target and target-to-source. In our system implementation, GIZA++ (Och and
Ney 2000) is used for computing the alignments. A total of five iterations for models
IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed.
Then, the obtained alignment sets are used for computing the intersection and the
union of alignments from which tuples and embedded-word tuples are extracted,
respectively.
4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is ex-
tracted from the union set of alignments while avoiding source-nulled tuples by using
the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are
pruned according to the procedure described in Section 2.2.3. In the case of the EPPS
data under consideration, pruning parameter values of N = 20 and N = 30 are used for
Spanish-to-English and English-to-Spanish, respectively.
In order to better justify such alignment set and pruning parameter selections,
Tables 2 and 3 present model sizes and translation accuracies for the tuple n-gram model
536
Marin?o et al N-gram-based Machine Translation
Table 2
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy when tuples are extracted from different alignment sets. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model alone.
Direction Alignment set Tuple voc. Bigrams Trigrams BLEU
ES ? EN Source-to-target 1.920 6.426 2.353 0.4424
union 2.040 6.009 1.798 0.4745
refined 2.111 6.851 2.398 0.4594
EN ? ES Source-to-target 1.813 6.263 2.268 0.4152
union 2.023 6.092 1.747 0.4276
refined 2.081 6.920 2.323 0.4193
when tuples are extracted from different alignment sets and when different pruning
parameters are used, respectively. Translation accuracy is measured in terms of the
BLEU score (Papineni et al 2002), which is computed here for translations generated
by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2,
in the case of Table 3. Both translation directions, Spanish to English (ES ? EN) and
English to Spanish (EN ? ES), are considered in each table.
In the case of Table 2, model size and translation accuracy are evaluated against
the type of alignment set used for extracting tuples. Three different alignment sets are
considered: source-to-target, the union of source-to-target and target-to-source, and the
?refined? alignment method described by Och and Ney (2003). For the results presented
in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English
direction, while a value of N = 30 was used for the English-to-Spanish direction.
As can be clearly seen in Table 2, the union alignment set happens to be the most
favorable one for extracting tuples in both translation directions since it provides a
significantly better translation accuracy, in terms of BLEU score, than the other two
alignment sets considered. Notice also in Table 2 that the union set is the one providing
the smallest model sizes according to the number of bigrams and trigrams. This might
explain the improvement observed in translation accuracy, with respect to the other two
cases, in terms of model sparseness.
Table 3
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy for different pruning values and both translation directions. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2.
Direction Pruning Tuple voc. Bigrams Trigrams BLEU
ES ? EN N = 30 2.109 6.233 1.805 0.5440
N = 20 2.040 6.009 1.798 0.5434
N = 10 1.921 5.567 1.759 0.5399
EN ? ES N = 30 2.023 6.092 1.747 0.4688
N = 20 1.956 5.840 1.733 0.4671
N = 10 1.843 5.342 1.677 0.4595
537
Computational Linguistics Volume 32, Number 4
In the case of Table 3, model size and translation accuracy are compared for three
different pruning conditions: N = 30, N = 20, and N = 10. For all the cases presented in
the table, tuples were extracted from the union set of alignments.
Notice in Table 3 how translation accuracy is clearly affected by pruning. In the
case of Spanish to English, values of N = 20 and N = 10, while providing tuple vo-
cabulary reductions of 3.27% and 8.91% with respect to N = 30, respectively, produce
a translation BLEU score reductions of 0.11% and 0.75%. On the other hand, in the
case of English to Spanish, values of N = 20 and N = 10 provide tuple vocabulary
reductions of 3.31% and 8.89% and a translation BLEU score reductions of 0.36% and
1.98% with respect to N = 30, respectively. According to these results, a similar tuple
vocabulary reduction seems to affect English-to-Spanish translations more than it af-
fects Spanish-to-English translations. For this reason, we finally adopted N = 20 and
N = 30 as the pruning parameter values for Spanish to English and English to Spanish,
respectively.
Another important observation derived from Table 3 is the higher BLEU score
values with respect to the ones presented in Table 2. This is because, as mentioned
above, the results presented in Table 3 were obtained by considering a full translation
system that implements the tuple n-gram model along with the additional four feature
functions described in Section 3.2. The relative impact of the described feature functions
on translation accuracy is studied in detail in Section 5.1.1.
4.2.3 Translation Model and Feature Function Training. After pruning, a tuple n-gram
model is trained for each translation direction by using the SRI Language Modeling
toolkit (Stolcke 2002). The options for Kneser?Ney smoothing (Kneser and Ney 1995)
and interpolation of higher and lower n-grams are used in these trainings. Then, each
tuple n-gram translation model is finally enhanced by including the unigram probabil-
ities for the embedded-word tuples such as described in Section 2.2.2.
Similarly, a word n-gram target language model is trained for each translation
direction by using the SRI Language Modeling toolkit. Again, as in the case of the
tuple n-gram model, Kneser?Ney smoothing and interpolation of higher and lower
n-grams are used. Extended target language models might also be obtained by adding
additional information from other available monolingual corpora. However, in the
translation tasks described here, target language models are estimated by using only
the information contained in the target side of the training data set.
In our SMT system implementation, trigram models are considered for both the
tuple translation model and the target language model. This selection is based on
perplexity measurements (over the development data set) obtained for n-gram models
computed from the EPPS training data by using different n-gram sizes. Table 4 presents
Table 4
Perplexity measurements for translation and target language models of different n-gram sizes.
Type of model Language Bigram Trigram 4-gram 5-gram
Translation ES ? EN 201.75 161.26 156.88 157.24
Translation EN ? ES 223.94 179.12 174.10 174.49
Language Spanish 81.98 52.49 48.03 47.54
Language English 78.91 50.59 46.22 45.59
538
Marin?o et al N-gram-based Machine Translation
perplexity values obtained for translation and target language models with different
n-gram sizes.
Although our system implements trigram models, the performance of translation
systems using different n-gram sized models is also evaluated. These results are pre-
sented and discussed in Section 5.1.2.
Finally, the source-to-target and target-to-source lexicon models are computed for
each translation direction according to the procedure described in Section 3.2.3. For each
considered lexicon model, either the alignment set in the source-to-target direction or
the alignment set in the target-to-source direction is used, accordingly.
4.2.4 System Optimization. Once the models are computed, a set of optimal log-linear
coefficients is estimated for each translation direction and system configuration via
an optimization procedure, which is described as follows. First, a development data
set that does not overlap either the training set or the test set is required. Then, trans-
lation quality over the development set is maximized by iteratively varying the set of
coefficients. In our SMT system implementation, this optimization procedure is per-
formed by using a tool developed in-house, which is based on a simplex method (Press
et al 2002), and the BLEU score (Papineni et al 2002) is used as a translation quality
measurement.
As will be described in the next section, several different system configurations
are considered in the experiments. For all these optimizations, the development data
described in Table 1 are used. As presented in the table, the development data included
three translation references for both English and Spanish, which are used to compute
the BLEU score at each iteration of the optimization procedures.
The same decoder settings are used for all system optimizations. These settings are
the following:
 decoding is performed monotonically, that is, no reordering capabilities
are used,
 decoding is guided by the source sentence to be translated,
 although available in the decoder, threshold pruning is not used, and
 a value of K = 50 for during-decoding histogram pruning is used.
5. Translation Experiments and Error Analysis
This section presents all translation experiments performed and a brief error analysis
of the obtained results. In order to evaluate the relative contributions of different
system elements to the overall performance of the n-gram-based translation system,
three different experimental settings are considered. The experiments and their re-
sults are described in Section 5.1, and a brief error analysis of results is presented in
Section 5.2. Finally, a comparison between n-gram-based SMT and state-of-the-art
phrase-based translation systems is presented in Section 5.3.
5.1 Translation Experiments and Results
As already mentioned, three experimental settings are considered. For each setting,
the impact on translation quality of a different system parameter is evaluated, namely,
539
Computational Linguistics Volume 32, Number 4
feature function, n-gram size, and the source-nulled tuple strategy. Evaluations in all
three experimental settings are performed with respect to the same standard system
configuration, which is defined in terms of the following parameters:
 Alignment set used for tuple extraction: UNION
 Tuple vocabulary pruning parameter: N = 20 for Spanish to English, and
N = 30 for English to Spanish
 N-gram size used in translation model: 3
 N-gram size used in target language model: 3
 Expanded translation model with embedded-word tuples: YES
 Source-nulled tuple handling strategy: attach-to-right
 Feature functions considered: target language, word-bonus,
source-to-target lexicon, and target-to-source lexicon
In the three experimental settings considered, which are presented in the following
subsections, a total of seven different system configurations are evaluated in both
translation directions, English to Spanish and Spanish to English. Thus, a total of 14
different translation experiments are performed. For each of these cases, the corre-
sponding test set is translated by using the corresponding estimated models and set
of optimal coefficients. The same decoder settings (which were previously described in
Section 4.2.4) that were used during the optimizations are used for all translation
experiments. Translation results are evaluated in terms of mWER and BLEU by using
the two references available for each language test set.
5.1.1 Feature Function Contributions. This experiment is designed to evaluate the
relative contribution of feature functions to the overall system performance. In this
section, four different systems are evaluated. These systems are:
 System A. This constitutes the basic n-gram translation system, which
implements the tuple trigram translation model alone, that is, no
additional feature function is used.
 System B. This is a target-reinforced system. In this system, the translation
model is used along with the target-language and word-bonus models.
 System C. This is a lexicon-reinforced system. In this system, the
translation model is used along with the source-to-target and
target-to-source lexicon models.
 System D. This constitutes the full system, that is, the translation model is
used along with all four additional feature functions. This system
corresponds to the standard system configuration that was defined at the
beginning of Section 5.1.
Table 5 summarizes the results of this evaluation, in terms of BLEU and mWER, for
the four systems considered. As can be seen from the table, both translation directions,
540
Marin?o et al N-gram-based Machine Translation
Table 5
Evaluation results for experiments on feature function contribution.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN A ? ? ? ? 39.71 0.4745
B 0.29 0.31 ? ? 39.51 0.4856
C ? ? 0.77 0.08 35.77 0.5356
D 0.49 0.30 0.94 0.25 34.94 0.5434
EN ? ES A ? ? ? ? 44.46 0.4276
B 0.33 0.27 ? ? 44.67 0.4367
C ? ? 0.29 0.15 41.69 0.4482
D 0.66 0.73 0.32 0.47 40.34 0.4688
Spanish to English and English to Spanish, are considered. Table 5 also presents the
optimized log-linear coefficients associated with the features considered in each system
configuration (the log-linear weight of the translation model has been omitted from the
table because its value is fixed to 1 in all cases).
As can be observed in Table 5, the inclusion of the four feature functions into
the translation system definitively produces a significant improvement in translation
quality in both translation directions. In particular, it becomes evident that the features
with the most impact on translation quality are the lexicon models. The target language
model and the word bonus also contribute to improving translation quality, but to a
lesser degree.
Also, although it is more evident in the English-to-Spanish direction than in the
opposite one, it can be noticed from the presented results that the contribution of
target-language and word-bonus models is more relevant when the lexicon mod-
els are used (full system). In fact, as seen from the ?lm values in Table 5, when
the lexicon models are not included, the target-language model contribution to the
overall translation system becomes much less significant. A comparative analysis of
the resulting translations suggests that including the lexicon models tends to favor
short tuples over long ones, so the target-language model becomes more important
for providing target context information when the lexicon models are used. How-
ever, more experimentation and research are required for fully understanding this
interesting result.
Another important observation, which follows from comparing results between
both translation directions, is that in all cases the Spanish-to-English translations are
consistently and significantly better than the English-to-Spanish translations. This is
clearly due to the more inflected nature of Spanish vocabulary. For example, the single
English word the can generate any of the four Spanish words el, la, los, and las. Similar
situations occur with nouns, adjectives, and verbs that may have many different forms
in Spanish. This would suggest that the English-to-Spanish translation task is more
difficult than the Spanish-to-English task.
5.1.2 Translation and Language N-gram Size. This experiment is designed to evaluate
the impact of translation- and language-model n-gram sizes on overall system perform-
ance. In this section, the full system (System D in the previous experiment) is com-
pared with two similar systems for which 4-grams are used for training the translation
541
Computational Linguistics Volume 32, Number 4
model and/or the target language model. More specifically, the three systems compared
in this experiment are:
 System D, which implements a tuple trigram translation model and a word
trigram target language model. This system corresponds to the standard
system configuration that was defined at the beginning of Section 5.1.
 System E, which implements a tuple trigram translation model and a word
4-gram target language model.
 System F, which implements a tuple 4-gram translation model and a word
4-gram target language model.
Table 6 summarizes the results of this evaluation for Systems E, F, and D. Again, both
translation directions are considered and the optimized coefficients associated with the
four feature functions are also presented for each system configuration.
As can be seen in Table 6, the use of 4-grams for model computation does not
provide a clear improvement in translation quality. This is more evident in the English-
to-Spanish direction for which System F happens to be the worst ranked one, while
System D is the one obtaining the best mWER score and system E is the one obtaining
the best BLEU score. On the other hand, in the Spanish-to-English direction, it seems
that a little improvement with respect to System D is achieved by using 4-grams.
However, it is not clear which system performs the best since System E obtains the
best BLEU score while System F obtains the best mWER score.
According to these results, more experimentation and research are required to fully
understand the interaction between the n-gram sizes of translation and target language
models. Notice that in the particular case of the n-gram SMT system described here,
such an interaction is not evident at all since the n-gram-based translation model itself
contains some of the target language model information.
5.1.3 Source-nulled Tuple Strategy Comparison. This experiment is designed to eval-
uate a different strategy for handling source-nulled tuples. In this section, the standard
system configuration (System D) presented at the beginning of Section 5.1, which imple-
ments the attach-to-right strategy described in Section 2.2.2, is compared with a similar
system (referred to as System G) implementing a more complex strategy for handling
those tuples with NULL source sides. More specifically, the latter system uses the
IBM-1 lexical parameters (Brown et al 1993) for computing the translation probabilities
of two possible new tuples: the one resulting when the null-aligned-word is attached to
Table 6
Evaluation results for experiments on n-gram size incidence.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
E 0.50 0.54 0.66 0.45 34.66 0.5483
F 0.66 0.50 1.01 0.57 34.59 0.5464
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
E 0.57 0.45 0.51 0.26 40.55 0.4714
F 1.24 1.07 0.99 0.57 40.91 0.4688
542
Marin?o et al N-gram-based Machine Translation
the previous word and the one resulting when it is attached to the following one. Then,
the attachment direction is selected according to the tuple with the highest translation
probability.
Table 7 summarizes the results of evaluation Systems D and G. Again, both trans-
lation directions are considered and the optimized coefficients associated with the four
feature functions are also presented for each system configuration.
As can be seen in Table 7, consistently better results are obtained in both translation
tasks when using IBM-1 lexicon probabilities to handle tuples with a NULL source
side. Even though slight improvements are achieved in both cases, especially with
the English-to-Spanish translation task, the results show how the initial attach-to-right
strategy is easily improved by making use of some bilingual knowledge.
5.2 Error Analysis
In this last section, we present a brief description of an error analysis performed
on some of the outputs provided by the standard system configuration that was de-
scribed in Section 5.1 (system D). More specifically, a detailed review of 100 trans-
lated sentences and their corresponding source sentences, in each direction, was
conducted. This analysis was very useful since it allowed us to identify the most com-
mon errors and problems related to our n-gram based SMT system in each translation
direction.
A detailed analysis of all the reviewed translations reveals that most translation
problems encountered are typically related to four basic different types of errors:
 Verbal forms: A significant number of wrong verbal tenses and auxiliary
forms were detected. This problem turned out to be the most common
one, reflecting the difficulty of the current statistical approach to capture
the linguistic phenomena that shape head verbs, auxiliary verbs, and
pronouns into full verbal forms in each language, especially given the
inflected nature of the Spanish language.
 Omitted translations: A large number of translations involving tuples with
NULL target sides were detected. Although in some cases these situations
corresponded to correct translations, most of the time they resulted in
omitted-word errors.
 Reordering problems: The two specific situations that most commonly
occurred were problems related to adjective?noun and subject?verb
structures.
Table 7
Evaluation results for experiments on strategies for handling source-nulled tuples.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
G 0.49 0.45 0.78 0.39 34.15 0.5451
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
G 0.96 0.93 0.53 0.44 40.12 0.4694
543
Computational Linguistics Volume 32, Number 4
 Concordance problems: Inconsistencies related to gender and number
were the most commonly found.
Table 8 presents the relative number of occurrences for each of the four types of errors
identified in both translation directions.
Notice in Table 8 that the most common errors in both translation directions are
those related to verbal forms. However, it is important to mention that 29.5% of verbal-
form errors in the English-to-Spanish direction actually correspond to verbal omissions.
Similarly, 12.8% of verbal-form errors in the Spanish-to-English direction are verbal
omissions. According to this, if errors due to omitted translations and to omitted verbal
forms are considered together, it is evident that errors involving omissions constitute
the most important group, especially in the case of English-to-Spanish translations. It
is also interesting to note that the Spanish-to-English direction exhibits more omitted-
translation errors that are not related to verbal forms than the English-to-Spanish
direction.
Also in Table 8, it can be seen that concordance errors affect more than twice as many
English-to-Spanish translations as Spanish-to-English ones. This result can be explained
by the more inflected nature of Spanish.
Finally, as an illustrative example, three Spanish-to-English translation outputs are
presented below. For each presented example, errors have been boldfaced and correct
translations are provided in brackets:
Example 1
The policy of the European Union on Cuba NULL must [must not] change.
Example 2
To achieve these purposes, it is necessary NULL for the governments to be allocated
[to allocate], at least, 60,000 million NULL dollars a year . . .
Example 3
In the UK we have NULL [already] laws enough [enough laws], but we want to encourage
NULL other States . . .
5.3 N-gram-based SMT Compared with Phrase-Based SMT
The n-gram-based translation system here described has been also evaluated and com-
pared to other phrase-based translation systems in the context of the European Project
Table 8
Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English
translations that were studied.
Type of error English-to-Spanish Spanish-to-English
Verbal forms 31.3% 29.9%
Omitted translations 22.0% 26.1%
Reordering problems 15.9% 19.7%
Concordance problems 10.8% 4.6%
Other errors 20.0% 19.7%
544
Marin?o et al N-gram-based Machine Translation
TC-STAR. A detailed description of the first evaluation campaign (including the main
characteristics of every system) is available through the consortium?s Web site as a
progress report (Ney et al 2005).
Table 9 presents the four best BLEU results for the EPPS translation task in the
first TC-STAR?s evaluation campaign, where the results corresponding to our n-gram-
based translation system are provided in brackets. A total of six systems were evaluated
in this evaluation campaign. The task consisted of two translation directions: English
to Spanish and Spanish to English, and three different evaluation conditions: final
text edition, verbatim, and ASR output. The final text edition condition corresponds
to the official transcripts of the EPPS, so it is actually a written-language translation
condition. On the other hand, the other two conditions are spoken-language transla-
tion conditions. More specifically, the verbatim condition corresponds to literal tran-
scriptions of parliamentary speeches, which include hesitations, repeated words, and
other spontaneous speech effects; and the ASR output condition corresponds to the
output of an automatic speech recognition system, so it additionally includes speech-
recognition errors.
As can be seen in Table 9, performance of the n-gram-based translation system is
among the three best systems for the translation directions and conditions considered
in the first TC-STAR evaluation campaign.
Another independent comparison of the translation system proposed here with
other phrase-based translation systems is available through the results of the second
shared task of the ACL 2005 workshop on ?Building and using parallel texts: Data-
driven machine translation and beyond.? In this shared task, which was entitled ?Ex-
ploiting Parallel Texts for Statistical Machine Translation,? our n-gram-based translation
system was evaluated in four different translation directions: Spanish to English, French
to English, German to English, and Finish to English (Banchs et al 2005). The domain
of this task was also the European Parliament; however, the data set considered in this
evaluation was different from the one used in TC-STAR?s evaluation campaign. The
final text edition condition (official transcripts) was the only one considered here. A total
of twelve different systems participated in this shared task. Table 10 presents the four
best BLEU results for each of the four translation directions considered in the shared
task. Again, results corresponding to our n-gram-based translation system are provided
in brackets.
As can be seen in Table 10, the performance of the n-gram-based translation system
is among the three best systems for the four translation directions considered in the
ACL 2005 workshop shared task. The third system in Table 10 for ES to EN translation
Table 9
The four best BLEU results for the EPPS translation task in TC-STAR?s first evaluation campaign.
N-gram based system results are provided in brackets. All BLEU values presented here have
been taken from TC-STAR?s SLT Progress Report, available at: http://www.tc-star.org/.
Direction Condition First Second Third Fourth
ES ? EN Final text edition [53.3] 53.1 47.5 46.1
Verbatim 45.9 44.1 [42.1] 38.1
ASR output 41.5 39.7 [37.7] 34.7
EN ? ES Final text edition [46.2] 45.2 38.9 37.6
Verbatim 42.5 [38.1] 36.8 33.4
ASR output 38.7 34.3 [33.8] 33.0
545
Computational Linguistics Volume 32, Number 4
Table 10
The four best BLEU results for the four translation directions considered in the shared task
?Exploiting Parallel Texts for Statistical Machine Translation? (ACL 2005 workshop on
?Building and using parallel texts: Data-driven machine translation and beyond?). N-gram-
based system results are provided in brackets. All BLEU values presented here have been
taken from the shared task?s Web site: http://www.statmt.org/wpt05/mt-shared-task/.
Direction Condition First Second Third Fourth
FR ? EN Final text edition 30.27 [30.20] 29.53 28.89
ES ? EN Final text edition 30.95 [30.07] 29.84 29.08
DE ? EN Final text edition 24.77 [24.26] 23.21 22.91
FI ? EN Final text edition 22.01 20.95 [20.31] 18.87
deserves some comment. This system is a conventional phrase-based system sharing
the same decoder MARIE, IBM features, word bonus, and target-language model as the
n-gram-based system. The specific characteristics of the phrase-based system are direct
and inverse phrase conditional probabilities and phrase penalty. Additional compar-
isons between an n-gram system and a phrase-based system sharing a common decoder
and training and test framework can be found in Crego et al (2005c).
6. Conclusions and Further Work
As can be concluded from the results presented, the tuple n-gram translation model,
when used along with additional feature functions, provides state-of-the-art transla-
tions for the considered translation directions.
Another important result is that the quality of Spanish-to-English translations is
significantly and consistently better than those obtained in English-to-Spanish transla-
tions. Consequently, significant efforts should be dedicated towards properly exploiting
morphological analysis and synthesis methods for improving English-to-Spanish trans-
lation quality.
Additionally, four commonly occurring types of translation errors were identified
by reviewing a significant number of translated sentence pairs. This analysis has pro-
vided us with useful hints for future research and improvement of our SMT system.
However, more evaluation and discussion are required in this area in order to fully
understand these common translation failures and then implementing appropriate
solutions.
All the experiments presented in this work were performed using monotone de-
coding, and no reordering strategies were implemented. Although this system con-
figuration proved to provide state-of-the-art translations for the tasks presented, this
may not hold for tasks involving more distant language pairs for which reordering
capabilities must be implemented. Accordingly, along with other results obtained in
the present work, we consider that further research on n-gram SMT should focus on the
following issues:
 Reordering strategies, as well as non-monotonous decoding schemes, for
the proposed SMT system must be developed and tested. As mentioned
before, reordering problems specifically related to adjective?noun and
subject?verb structures occur very often in Spanish-to-English and
546
Marin?o et al N-gram-based Machine Translation
English-to-Spanish translations. Preliminary results concerning the use of
word class deterministic reordering and POS-tag-based reordering
patterns can be found in Costa-jussa`, Fonollosa, and Monte (2006) and
Crego and Marin?o (2006), respectively.
 An effective long-tuple unfolding strategy must be developed to avoid
the occurrence of long tuples resulting from long alignment links, which
happens to be a common situation when dealing with translations
between distant pairs of languages. This problem is closely related to
reordering, and some preliminary results have been presented by Crego,
Marin?o, and de Gispert (2005b).
 The definition of the tuple as a bilingual pair will be revised in order to
better handle unaligned words in both the source and the target sides. As
mentioned above, a better strategy for dealing with target words aligned
to NULL is required. Similarly, a better handling of NULLs in the target
side will result in fewer omitted-translation errors.
 The extension of the embedded-word concept to the more general idea of
embedded n-grams should be evaluated and implemented. Accordingly, a
translation probability should be estimated for those groups of words
that always occur embedded in tuples. This would guarantee that the
decoder will always have a translation option for any given word or word
combination previously seen in the training data. Further work is required
to determine the relative impact of these embedded n-grams on the
translation model, and the most appropriate strategy for handling them.
 Linguistic information must be used to cope with the observed
morphological problems in the English-to-Spanish translation direction,
as well as the more general problem of incorrect verbal form translations.
In this regard, ongoing research on linguistic tuples classification is
being done in order to improve translation results. Preliminary results
on detecting and classifying verb forms have been presented by
de Gispert (2005).
 A more detailed error analysis than the one presented in Section 5.2 is
required to fully understand the n-gram SMT system behavior and the
specific causes of each resulting type of error. It would be very useful for
improving our translation system performance to clearly identify whether
these errors are due to unseen information while training, to modeling
problems, or to decoding errors.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-STAR (Technology and Corpora for
Speech to Speech Translation) (IST-2002-
FP6-506738, http://www.tc-star.org), the
Spanish Department of Education and
Science (MEC), the Department of
Universities, Research and Information
Society (Generalitat de Catalunya), and
the Universitat Polite`cnica de Catalunya.
References
Banchs, Rachel E., Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, and
Jose? Bernardo Marin?o. 2005. Statistical
machine translation of Euparl data by
using bilingual n-grams. In ACL Workshop
on Data-Driven Machine Translation and
Beyond, pages 133?136, Ann Arbor, MI.
Bangalore, Srinivas and Giuseppe Riccardi.
2000. Stochastic finite-state models for
spoken language machine translation.
547
Computational Linguistics Volume 32, Number 4
In Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Berger, Adam, Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Brown, Peter, John Cocke, Stephen Della
Pietra, Vincent Della Pietra, Frederick
Jelinek, John Lafferty, Robert Mercer, and
Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter, Stephen Della Pietra, Vincent
Della Pietra, and Robert Mercer. 1993.
The mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Casacuberta, Francisco. 2001. Finite-state
transducers for speech input translation. In
Proceedings IEEE ASRU, pages 375?380,
Madonna di Campiglio, Italy.
Casacuberta, Francisco and Enrique Vidal.
2004. Machine translation with inferred
stochastic finite-state transducers.
Computational Linguistics, 30(2):205?225.
Costa-jussa`, Marta Ruiz, Jose? Adria?n
Rodriguez Fonollosa, and Enric Monte.
2006. Using reordering in statistical
machine translation based on alignment
block classification. Internal Report.
http://gps-tsc.upc.es/veu/personal/
mruiz/docs/br06.pdf.
Crego, Josep Maria, Jose? Bernardo
Marin?o, and Adria` de Gispert. 2004.
Finite-state-based and phrase-based
statistical machine translation. In
Proceedings of the 8th International
Conference on Spoken Language
Processing, pages 37?40, Jeju, Korea.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005a. An
Ngram-based statistical machine
translation decoder. In INTERSPEECH
2005, pages 3185?3188, Lisbon, Portugal.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005b. Reordered
search and tuple unfolding for Ngram-
based SMT. Proceedings of the Tenth
Machine Translation Summit, pages 283?289,
Phuket, Thailand.
Crego, Josep Maria, Marta Ruiz Costa-jussa`,
Jose? Bernardo Marin?o, and Jose? Adria?n
Rodriguez Fonollosa. 2005c. Ngram-
based versus phrase-based statistical
machine translation. In Proceedings of the
International Workshop on Spoken Language
Translation, pages 177?184, Pittsburgh, PA.
Crego, Josep Maria and Jose? Bernardo
Marin?o. 2006. Integration of POStag-based
source reordering into SMT decoding by
an extended search graph. In Proceedings of
the 7th Biennial Conference of the Association
for Machine Translation in the Americas,
Boston, MA.
de Gispert, Adria` and Jose? Bernardo Marin?o.
2002. Using X-grams for speech-to-
speech translation. In Proceedings of the
7th International Conference on Spoken
Language Processing, pages 1885?1888,
Denver, CO.
de Gispert, Adria`, Jose? Bernardo Marin?o, and
Josep Maria Crego. 2004. TALP:
Xgram-based spoken language translation
system. In Proceedings of the International
Workshop on Spoken Language Translation,
pages 85?90, Kyoto, Japan.
de Gispert, Adria`. 2005. Phrase linguistic
classification and generalization for
improving statistical machine translation.
In ACL?05 Student Workshop, pages 67?72,
Ann Arbor, MI.
Hutchins, John. 1986. Machine Translation:
Past, Present and Future. Ellis Horwood,
Chichester, England.
Kay, Martin, Jean Mark Gawron, and Peter
Norvig. 1992. Verbmobil: A Translation
System for Face-to-Face Dialog. CSLI.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for m-gram
language modeling. In IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 49?52, Detroit, MI.
Knight, Kevin and Yaser Al-Onaizan.
1998. Translation with finite-state
devices. In AI Lecture Notes in Artificial
Intelligence, volume 1529, Springer-Verlag,
pages 421?437.
Koehn, Philippe, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
chapter of the ACL, pages 48?54, Edmonton,
Alberta, Canada.
Koehn, Philippe. 2002. Europarl: A
multilingual corpus for evaluation
of machine translation. Available
online at: http://people.csail.mit.edu/
people/koehn/publications/europarl/.
Marin?o, Jose? Bernardo, Rafael E. Banchs,
Josep Maria Crego, Adria` de Gispert,
Patrik Lambert, Jose? Adria?n Rodriguez
Fonollosa, and Marta Ruiz. 2005. Bilingual
N-gram statistical machine translation.
In Proceedings of the Tenth Machine
Translation Summit, pages 275?282,
Phuket, Thailand.
548
Marin?o et al N-gram-based Machine Translation
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, Jorge Gonza?lez,
Young-suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. SLT progress report.
TC-STAR Deliverable D5, European
Community project no. FP6-506738.
Available online at: http://www.
tc-star.org/pages/f documents.htm.
Och, Franz Joseph and Hermann Ney.
2000. Improved statistical alignment
models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440?447,
Hong Kong, China.
Och, Franz Joseph and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302,
Philadelphia, PA.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Och, Franz Joseph, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenji Yamada,
Alexander Fraser, Shankar Kumar, Libin
Shen, David Smith, Katharine Eng, Viren
Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
NAACL, pages 161?168, Boston, MA, May.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the ACL,
pages 311?318, Philadelphia, PA.
Press, William H., Saul Teukolsky, William
Vetterling, and Brian P. Flannery.
2002. Numerical Recipes in C++: The
Art of Scientific Computing, Cambridge
University Press.
Riccardi, Giuseppe, Roberto Pieraccini, and
Enrico Bocchieri. 1996. Stochastic automata
for language modeling. Computer Speech
and Language, 10(4):265?293.
Shannon, Claude E. 1949. Communication
theory of secrecy systems. Bell System
Technical Journal, 28:656?715.
Shannon, Claude E. 1951. Prediction and
entropy of printed English. Bell System
Technical Journal, 30:50?64.
Shannon, Claude E. and Warren Weaver.
1949. The Mathematical Theory of
Communication, University of Illinois
Press, Urbana, IL.
Stolcke, Andreas 2002. SRLIM: An extensible
language modeling toolkit. In Proceedings
of the International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Tillmann, Christoph and Fei Xia. 2003. A
phrase-based unigram model for statistical
machine translation. In Proceedings of
HLT-NAACL - Short Papers, pages 106?108,
Edmonton, Alberta, Canada.
Vidal, Enrique. 1997. Finite-state speech-to-
speech translation. In Proceedings of 1997
IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 111?114,
Munich, Germany.
Weaver, Warren. 1955. Translation. In
William Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. John Wiley & Sons, New
York, pages 15?23.
Zens, Richard, Franz Joseph Och, and
Hermann Ney. 2002. Phrase-based
statistical machine translation. In
25th German Conference on Artificial
Intelligence, pages 18?32, September.
Aachen, Springer Verlag.
549

Proceedings of NAACL HLT 2007, Companion Volume, pages 85?88,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Discriminative Alignment Training without Annotated Data
for Machine Translation
Patrik Lambert, Rafael E. Banchs and Josep M. Crego
TALP Research Center
Jordi Girona Salgado 1?3
08034 Barcelona, Spain
{lambert, rbanchs, jmcrego}@gps.tsc.upc.edu
Abstract
In present Statistical Machine Translation
(SMT) systems, alignment is trained in a
previous stage as the translation model.
Consequently, alignment model parame-
ters are not tuned in function of the trans-
lation task, but only indirectly. In this
paper, we propose a novel framework for
discriminative training of alignment mod-
els with automated translation metrics as
maximization criterion. In this approach,
alignments are optimized for the transla-
tion task. In addition, no link labels at the
word level are needed. This framework
is evaluated in terms of automatic trans-
lation evaluation metrics, and an improve-
ment of translation quality is observed.
1 Introduction
In the first SMT systems (Brown et al, 1993), word
alignment was introduced as a hidden variable of
the translation model. When word-based translation
models have been replaced by phrase-based mod-
els (Zens et al, 2002), alignment1 and translation
model training have become two separated tasks.
The system of Brown et al was based on the
noisy channel approach. Present SMT systems use a
more general maximum entropy approach in which a
log-linear combination of multiple feature functions
is implemented (Och and Ney, 2002). Within this
1Hereinafter, alignment will refer to word alignment, unless
otherwise stated.
new framework translation quality can be tuned by
adjusting the weight of each feature function in the
log-linear combination. In order to improve transla-
tion quality, this tuning can be effectively performed
by minimizing translation error over a development
corpus for which manually translated references are
available (Och, 2003). As a separate first stage of the
process, alignment is not in practice directly tuned in
function of the machine translation task.
Tuning alignment for an MT system is subject to
practical difficulties. Unsupervised systems (Och
and Ney, 2003; Liang et al, 2006) are based on gen-
erative models trained with the EM algorithm. They
require large computational resources, and incorpo-
rating new features is difficult. In contrast, adding
new features to some supervised systems (Liu et al,
2005; Moore, 2005; Ittycheriah and Roukos, 2005)
is easy, but the need of annotated data is a problem.
A more general difficulty, however, is that of find-
ing an alignment evaluation metric favoring align-
ments which benefit Machine Translation. The fact
that the required alignment characteristics depend
on each particular system makes it even more dif-
ficult. It seems that high precision alignments are
better for phrase-based SMT (Chen and Federico,
2006; Ayan and Dorr, 2006), whereas high recall
alignments are more suited to N-gram SMT (Marin?o
et al, 2006). In this context, alignment quality im-
provements does not necessarily imply translation
quality improvements. This is in agreement with
the observation of a poor correlation between word
alignment error rate (AER (Och and Ney, 2000)) and
automatic translation evaluation metrics (Ittycheriah
and Roukos, 2005; Vilar et al, 2006).
85
Recently some alignment evaluation metrics have
been proposed which are more informative when
the alignments are used to extract translation
units (Fraser and Marcu, 2006; Ayan and Dorr,
2006). However, these metrics assess translation
quality very indirectly.
In this paper, we propose a novel framework for
discriminative training of alignment models with au-
tomated translation metrics as maximization crite-
rion. Thus we just need a reference aligned at the
sentence level instead of link labels at the word level.
The paper is structured as follows. Section 2 ex-
plains the models used in our word aligner, focusing
on the features designed to account for the specifici-
ties of the SMT system. In section 3, our minimum
error training procedure is described and experimen-
tal results are shown. Finally, some concluding re-
marks and lines of further research are given.
2 Bilingual Word Aligner
For versatility and efficiency requirements, we im-
plemented BIA, a BIlingual word Aligner similar
to that of Moore (2005). BIA consists in a beam-
search decoder searching, for each sentence pair, the
alignment which minimizes the cost of a linear com-
bination of various models. The differences with
the system of Moore lie in the features, which we
specially designed to suit our translation system (N-
gram SMT (Marin?o et al, 2006)). Its particularity
is the translation model, which is based on a 4-gram
language model of bilingual units referred to as tu-
ples. Two issues regarding this translation model can
be dealt with at the alignment stage.
Firstly, in order to estimate the bilingual n-gram
model, only one monotonic segmentation of each
sentence pair is performed. Thus long reorderings
cause long and sparse tuples to be extracted. For ex-
ample, if the first source word is linked to the last
target word, only one tuple can be extracted, which
contains the whole sentence pair. This kind of tuple
is not reusable, and the data between its two extreme
words are lost.
Secondly, it occurs very often that unlinked words
(i.e. linked to NULL) end up producing tuples with
NULL source sides. This cannot be allowed since
no NULL is expected to occur in a translation input.
This problem is solved by preprocessing alignments
before tuple extraction such that any unlinked target
word is attached to either its precedent or its follow-
ing word.
Taking theses issues into account, we imple-
mented the following features:
? distinct source and target unlinked word penal-
ties: since unlinked words have a different im-
pact whether they appear in the source or target
language, we introduced an unlinked word fea-
ture for each side of the sentence pair.
? link bonus: in order to accommodate the N-
gram model preference for higher recall align-
ment, we introduced a feature which adds a
bonus for each link in the alignment.
? embedded word position penalty: this feature
penalizes situations like the one depicted in fig-
ure 1. In this example, the bilingual units s2-t2
and s3-t3 cannot be extracted because word po-
sitions s2 and s3 are embedded between links
s1-t1 and s4-t1. Thus the link s4-t1 may intro-
duces data sparseness in the translation model,
although it may be a correct link. So we want
to have a feature which counts the number of
embedded word positions in an alignment.
Figure 1: Word positions embedded in a tuple.
In addition to the embedded word position feature,
we used the same two distortion features as Moore
to penalize reorderings in the alignment (one sums
the number of crossing links, and the other one sums
the amplitude of crossing links). We also used the ?2
score (Gale and Church, 1991) as a word association
model, and as a POS-tags association model.
3 Experimental Work
For these experiments we used the Chinese-
English data provided for IWSLT?06 evaluation
campaign (Paul, 2006). The training set contains
46000 sentences (of 6.7 and 7.0 average length). Pa-
rameters were tuned over the development set (dev4)
provided, consisting of 489 sentences of 11.2 words
in average, with 7 references. Our test set was a se-
lection of 500 sentences (of 6 words in average, with
16 references) among dev1, dev2 and dev3 sets.
86
3.1 Optimization Procedure
Once the alignment models were computed, a set of
optimal log-linear coefficients was estimated via the
optimization procedure depicted in Figure 2.
Figure 2: Optimization loop.
The training corpus was aligned with a set of ini-
tial parameters ?1, . . . , ?7. This alignment was used
to extract tuples and build a bilingual N-gram trans-
lation model (TM). A baseline SMT system, consist-
ing of MARIE decoder and this translation model as
unique feature2, was used to produce a translation
(OUT) of the development source set. Then, trans-
lation quality over the development set is maximized
by iteratively varying the set of coefficients.
The optimization procedure was performed by us-
ing the SPSA algorithm (Spall, 1992). SPSA is a
stochastic implementation of the conjugate gradient
method which requires only two evaluations of the
objective function. It was observed to be more ro-
bust than the Downhill Simplex method when tuning
SMT coefficients (Lambert and Banchs, 2006).
Each function evaluation required to align the
training corpus and build a new translation model.
The algorithm converged after about 80 evaluations,
lasting each 17 minutes with a 3 GHz processor.
Alignment decoding was performed with a beam of
10 (it took 50 seconds and required 8 MB memory).
Finally, the corpus was aligned with the opti-
mum set of coefficients, and a full SMT system was
build, with a target language model (trained on the
provided training data), a word bonus model and
two lexical models. SMT models weights were op-
timized with a standard Minimum Error Training
(MET) strategy3 and the test corpus was translated
2An N-gram SMT system can produce good translations
without additional target language model since the target lan-
guage is modeled inside the bilingual N-gram model.
3SMT parameters are not optimized together with alignment
with the full system. To contrast the results, full
translation systems were also build extracting tuples
from various combinations of GIZA++ alignments
(trained with 50 classes and respectively 4,5 and 4
iterations of models 1,HMM and 4). In order to limit
the error introduced by MET, we translated the test
corpus with three sets of SMT model weights, and
took the average and standard deviation.
3.2 Results
Table 1 shows results obtained with the full SMT
system on the test corpus, with GIZA++ alignments,
and BIA alignments optimized in function of three
metrics: BLEU, NIST, and BLEU+4*NIST. The
standard deviation is indicated in parentheses. Al-
though results for systems trained with different BIA
alignments present more variability than systems
trained with GIZA++ alignments, they achieve bet-
ter average scores, and one of them obtains much
higher scores. Unexpectedly, BIA alignments tuned
with NIST yield the system with worse NIST score.
4 Conclusions and further work
We proposed a novel framework for discriminative
training of alignment models with automated trans-
lation metrics as maximization criterion. Accord-
ing to this type of metrics, the translation systems
trained from the optimized alignments clearly per-
formed better than the ones trained from Giza++
alignment combinations.
In addition, this first version of the alignment
system has very basic models and could be im-
proved. We could certainly improve the association
score model, for example adding discount factors or
adding more association score types, or dictionaries.
During the alignment coefficient optimization de-
picted in Figure 2, only the baseline SMT system
is used. In future work, we could consider using
various SMT features (as would be required for a
phrase-based SMT system).
Our approach, as it is, cannot be applied to a large
corpus, since it requires to align the whole training
corpus at each iteration. Thus an interesting further
research would consist in determining whether the
parameters for two main reasons. Firstly, translation is more
sensitive to variations of SMT parameters. Secondly, alignment
is optimized over the full training set, whereas SMT is tuned
over the development set.
87
System BLEU NIST PER WER
GIZA++ union 42.7 (1.1) 8.82 (0.07) 34.7 (0.2) 43.7 (0.4)
GIZA++ intersection 42.4 (0.9) 8.53 (0.07) 37.0 (0.9) 45.0 (1.3)
GIZA++ Zh?En 43.7 (0.9) 8.90 (0.2) 37.2 (1.4) 45.5 (2.0)
BIA (BLEU) 44.8 (0.4) 9.00 (0.04) 35.7 (0.07) 43.8 (0.09)
BIA (BLEU+4*NIST) 47.0 (1.5) 8.83 (0.4) 32.9 (0.8) 40.9 (0.5)
BIA (NIST) 44.8 (0.1) 8.55 (0.14) 33.0 (0.2) 41.4 (0.5)
Table 1: Automatic translation evaluation results.
alignment parameters trained on a part of the corpus
are valid for the whole corpus.
Finally, some Giza++ parameters may also be
tuned, in the same way as for BIA parameters.
5 Acknowledgments
This work has been partially funded by the Euro-
pean Union under the integrated project TC-STAR
- Technology and Corpora for Speech to Speech
Translation -(IST-2002-FP6-506738, http://www.tc-
star.org) and by the Spanish Government under grant
TEC2006-13964-C03 (AVIVAVOZ project).
References
Necip F. Ayan and Bonnie J. Dorr. 2006. Going Beyond
AER: An Extensive Analysis of Word Alignments and
Their Impact on MT. In Proc. COLING-ACL, pages
9?16, Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Boxing Chen and Marcello Federico. 2006. Improving
phrase-based statistical translation through combina-
tion of word alignment. In Proc. FinTAL, Turku, Fin-
land.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proc. COLING-ACL, pages 769?776, Sydney, Aus-
tralia.
W. Gale and K. W. Church. 1991. Identifying word cor-
respondences in parallel texts. In DARPA Speech and
Natural Language Workshop, Asilomar, CA.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In Proc. HLT-EMNLP, pages 89?96, Van-
couver, Canada.
Patrik Lambert and Rafael E. Banchs. 2006. Tuning
Machine Translation Parameters with SPSA. In Proc.
IWSLT, pages 190?196, Kyoto, Japan.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. the HLT-NAACL, pages
104?111, New York City, USA.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proc. ACL, pages 459?
466, Ann Arbor, Michigan.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N-gram based machine
translation. Computational Linguistics, 32(4):527?
549.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proc. HLT-EMNLP,
pages 81?88, Vancouver, Canada.
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine trans-
lation. In Proc. COLING, pages 1086?1090, Saar-
brucken,Germany.
F.J. Och and H. Ney. 2002. Dicriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302, Philadel-
phia, PA.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, pages 160?167.
Michael Paul. 2006. Overview of the IWSLT 2006 Eval-
uation Campaign. In Proc. IWSLT, pages 1?15, Kyoto,
Japan.
James C. Spall. 1992. Multivariate stochastic approxi-
mation using a simultaneous perturbation gradient ap-
proximation. IEEE Trans. Automat. Control, 37:332?
341.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ?improve? our alignments? In
Proc. IWSLT, pages 205?212, Kyoto, Japan.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Springer Verlag, editor,
Proc. German Conf. on Artificial Intelligence (KI).
88
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 133?136,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Statistical Machine Translation of Euparl Data by using Bilingual N-grams
Rafael E. Banchs Josep M. Crego Adria` de Gispert
Department of Signal Theory and Communications
Universitat Polite`cnica de Catalunya, Barcelona 08034, Spain
{rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.edu
Patrik Lambert Jose? B. Marin?o
Abstract
This work discusses translation results for
the four Euparl data sets which were made
available for the shared task ?Exploit-
ing Parallel Texts for Statistical Machine
Translation?. All results presented were
generated by using a statistical machine
translation system which implements a
log-linear combination of feature func-
tions along with a bilingual n-gram trans-
lation model.
1 Introduction
During the last decade, statistical machine transla-
tion (SMT) systems have evolved from the orig-
inal word-based approach (Brown et al, 1993)
into phrase-based translation systems (Koehn et al,
2003). Similarly, the noisy channel approach has
been expanded to a more general maximum entropy
approach in which a log-linear combination of mul-
tiple models is implemented (Och and Ney, 2002).
The SMT approach used in this work implements
a log-linear combination of feature functions along
with a translation model which is based on bilingual
n-grams. This translation model was developed by
de Gispert and Marin?o (2002), and it differs from the
well known phrase-based translation model in two
basic issues: first, training data is monotonously seg-
mented into bilingual units; and second, the model
considers n-gram probabilities instead of relative
frequencies. This model is described in section 2.
Translation results from the four source languages
made available for the shared task (es: Spanish, fr:
French, de: German, and fi: Finnish) into English
(en) are presented and discussed.
The paper is structured as follows. Section 2 de-
scribes the bilingual n-gram translation model. Sec-
tion 3 presents a brief overview of the whole SMT
procedure. Section 4 presents and discusses the
shared task results and other interesting experimen-
tation. Finally, section 5 presents some conclusions
and further work.
2 Bilingual N-gram Translation Model
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units which
are referred to as tuples (de Gispert and Marin?o,
2002). This model approximates the joint probabil-
ity between source and target languages by using 3-
grams as it is described in the following equation:
p(T, S) ?
N
?
n=1
p((t, s)n|(t, s)n?2, (t, s)n?1) (1)
where t refers to target, s to source and (t, s)n to the
nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, the produced segmentation is maximal in the
sense that no smaller tuples can be extracted with-
out violating the previous constraint (Crego et al,
2004). According to this, tuple extraction provides a
unique segmentation for a given bilingual sentence
pair alignment. Figure 1 illustrates this idea with a
simple example.
133
We would like to achieve perfect translations
NULL quisieramos lograr traducciones perfectas
t1 t2 t3 t4
Figure 1: Example of tuple extraction from an
aligned sentence pair.
Two important issues regarding this translation
model must be mentioned. First, when extracting
tuples, some words always appear embedded into tu-
ples containing two or more words, so no translation
probability for an independent occurrence of such
words exists. To overcome this problem, the tuple
3-gram model is enhanced by incorporating 1-gram
translation probabilities for all the embedded words
(de Gispert et al, 2004).
Second, some words linked to NULL end up pro-
ducing tuples with NULL source sides. This cannot
be allowed since no NULL is expected to occur in a
translation input. This problem is solved by prepro-
cessing alignments before tuple extraction such that
any target word that is linked to NULL is attached
to either its precedent or its following word.
3 SMT Procedure Description
This section describes the procedure followed for
preprocessing the data, training the models and op-
timizing the translation system parameters.
3.1 Preprocessing and Alignment
The Euparl data provided for this shared task (Eu-
parl, 2003) was preprocessed for eliminating all sen-
tence pairs with a word ratio larger than 2.4. As a
result of this preprocessing, the number of sentences
in each training set was slightly reduced. However,
no significant reduction was produced.
In the case of French, a re-tokenizing procedure
was performed in which all apostrophes appearing
alone were attached to their corresponding words.
For example, pairs of tokens such as l ? and qu ?
were reduced to single tokens such as l? and qu?.
Once the training data was preprocessed, a word-
to-word alignment was performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2000). As an approxi-
mation to the most probable alignment, the Viterbi
alignment was considered. Then, the intersection
and union of alignment sets in both directions were
computed for each training set.
3.2 Feature Function Computation
The considered translation system implements a to-
tal of five feature functions. The first of these mod-
els is the tuple 3-gram model, which was already de-
scribed in section 2. Tuples for the translation model
were extracted from the union set of alignments as
shown in Figure 1. Once tuples had been extracted,
the tuple vocabulary was pruned by using histogram
pruning. The same pruning parameter, which was
actually estimated for Spanish-English, was used for
the other three language pairs. After pruning, the
tuple 3-gram model was trained by using the SRI
Language Modeling toolkit (Stolcke, 2002). Finally,
the obtained model was enhanced by incorporating
1-gram probabilities for the embedded word tuples,
which were extracted from the intersection set of
alignments.
Table 1 presents the total number of running
words, distinct tokens and tuples, for each of the four
training data sets.
Table 1: Total number of running words, distinct to-
kens and tuples in training.
source running distinct tuple
language words tokens vocabulary
Spanish 15670801 113570 1288770
French 14844465 78408 1173424
German 15207550 204949 1391425
Finnish 11228947 389223 1496417
The second feature function considered was a tar-
get language model. This feature actually consisted
of a word 3-gram model, which was trained from the
target side of the bilingual corpus by using the SRI
Language Modeling toolkit.
The third feature function was given by a word
penalty model. This function introduces a sentence
length penalization in order to compensate the sys-
134
tem preference for short output sentences. More
specifically, the penalization factor was given by the
total number of words contained in the translation
hypothesis.
Finally, the fourth and fifth feature functions cor-
responded to two lexicon models based on IBM
Model 1 lexical parameters p(t|s) (Brown et al,
1993). These lexicon models were calculated for
each tuple according to the following equation:
plexicon((t, s)n) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(tin|sjn) (2)
where sjn and tin are the jth and ith words in the
source and target sides of tuple (t, s)n, being J and
I the corresponding total number words in each side
of it.
The forward lexicon model uses IBM Model 1 pa-
rameters obtained from source-to-target algnments,
while the backward lexicon model uses parameters
obtained from target-to-source alignments.
3.3 Decoding and Optimization
The search engine for this translation system was
developed by Crego et al (2005). It implements
a beam-search strategy based on dynamic program-
ming and takes into account all the five feature func-
tions described above simultaneously. It also allows
for three different pruning methods: threshold prun-
ing, histogram pruning, and hypothesis recombina-
tion. For all the results presented in this work the
decoder?s monotonic search modality was used.
An optimization tool, which is based on a simplex
method (Press et al, 2002), was developed and used
for computing log-linear weights for each of the fea-
ture functions described above. This algorithm ad-
justs the log-linear weights so that BLEU (Papineni
et al, 2002) is maximized over a given development
set. One optimization for each language pair was
performed by using the 2000-sentence development
sets made available for the shared task.
4 Shared Task Results
Table 2 presents the BLEU scores obtained for the
shared task test data. Each test set consisted of 2000
sentences. The computed BLEU scores were case
insensitive and used one translation reference.
Table 2: BLEU scores (shared task test sets).
es - en fr - en de - en fi - en
0.3007 0.3020 0.2426 0.2031
As can be seen from Table 2 the best ranked trans-
lations were those obtained for French, followed by
Spanish, German and Finnish. A big difference is
observed between the best and the worst results.
Differences can be observed from translation out-
puts too. Consider, for example, the following seg-
ments taken from one of the test sentences:
es-en: We know very well that the present Treaties are not
enough and that , in the future , it will be necessary to develop
a structure better and different for the European Union...
fr-en: We know very well that the Treaties in their current
are not enough and that it will be necessary for the future to
develop a structure more effective and different for the Union...
de-en: We very much aware that the relevant treaties are
inadequate and , in future to another , more efficient structure
for the European Union that must be developed...
fi-en: We know full well that the current Treaties are not
sufficient and that , in the future , it is necessary to develop the
Union better and a different structure...
It is evident from these translation outputs that
translation quality decreases when moving from
Spanish and French to German and Finnish. A
detailed observation of translation outputs reveals
that there are basically two problems related to this
degradation in quality. The first has to do with re-
ordering, which seems to be affecting Finnish and,
specially, German translations.
The second problem has to do with vocabulary. It
is well known that large vocabularies produce data
sparseness problems (Koehn, 2002). As can be con-
firmed from Tables 1 and 2, translation quality de-
creases as vocabulary size increases. However, it is
not clear yet, in which degree such degradation is
due to monotonic decoding and/or vocabulary size.
Finally, we also evaluated how much the full fea-
ture function system differs from the baseline tu-
ple 3-gram model alone. In this way, BLEU scores
were computed for translation outputs obtained for
the baseline system and the full system. Since the
English reference for the test set was not available,
we computed translations and BLEU scores over de-
135
velopment sets. Table 3 presents the results for both
the full system and the baseline.1
Table 3: Baseline- and full-system BLEU scores
(computed over development sets).
language pair baseline full
es - en 0.2588 0.3004
fr - en 0.2547 0.2938
de - en 0.1844 0.2350
fi - en 0.1526 0.1989
From Table 3, it is evident that the four additional
feature functions produce important improvements
in translation quality.
5 Conclusions and Further Work
As can be concluded from the presented results, per-
formance of the translation system used is much bet-
ter for French and Spanish than for German and
Finnish. As some results suggest, reordering and
vocabulary size are the most important problems re-
lated to the low translation quality achieved for Ger-
man and Finnish.
It is also evident that the bilingual n-gram model
used requires the additional feature functions to pro-
duce better translations. However, more experimen-
tation is required in order to fully understand each
individual feature?s influence on the overall log-
linear model performance.
6 Acknowledgments
This work has been funded by the European Union
under the integrated project TC-STAR - Technology
and Corpora for Speech to Speech Translation -(IST-
2002-FP6-506738, http://www.tc-star.org).
The authors also want to thank Jose? A. R. Fonol-
losa and Marta Ruiz Costa-jussa` for their participa-
tion in discussions related to this work.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. ?The mathemat-
1Differently from BLEU scores presented in Table 2, which
are case insensitive, BLEU scores presented in Table 3 are case
sensitive.
ics of statistical machine translation: parameter esti-
mation?. Computational Linguistics, 19(2):263?311.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2004. ?Finite-state-based and phrase-based statistical
machine translation?. Proc. of the 8th Int. Conf. on
Spoken Language Processing, :37?40, October.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. ?A Ngram-based Statistical Machine Transla-
tion Decoder?. Submitted to INTERSPEECH 2005.
Adria` de Gispert, and Jose? B. Marin?o. 2002. ?Using X-
grams for speech-to-speech translation?. Proc. of the
7th Int. Conf. on Spoken Language Processing.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2004. ?TALP: Xgram-based spoken language transla-
tion system?. Proc. of the Int. Workshop on Spoken
Language Translation, :85?90. Kyoto, Japan, October.
EUPARL: European Parliament Proceedings Parallel
Corpus 1996-2003. Available on-line at: http://
people.csail.mit.edu/people/koehn/public
ations/europarl/
Philipp Koehn. 2002. ?Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation?. Avail-
able on-line at: http://people.csail.mit.edu/
people/koehn/publications/europarl/
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
?Statistical phrase-based translation?. Proc. of the
2003 Meeting of the North American chapter of the
ACL, Edmonton, Alberta.
Franz J. Och and Hermann Ney. 2000. ?Improved statis-
tical alignment models?. Proc. of the 38th Ann. Meet-
ing of the ACL, Hong Kong, China, October.
Franz J. Och and Hermann Ney. 2002. ?Discriminative
training and maximum entropy models for statistical
machine translation?. Proc. of the 40th Ann. Meeting
of the ACL, :295?302, Philadelphia, PA, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. ?Bleu: a method for automatic eval-
uation of machine translation?. Proc. of the 40th Ann.
Conf. of the ACL, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing, Cambridge
University Press.
Andreas Stolcke. 2002. ?SRLIM: an extensible language
modeling toolkit?. Proc. of the Int. Conf. on Spoken
Language Processing :901?904, Denver, CO, Septem-
ber. Available on line at: http://www.speech.sr
i.com/projects/srilm/
136
Grouping Multi-word Expressions According to Part-Of-Speech in
Statistical Machine Translation
Patrik Lambert
TALP Research Center
Jordi Girona Salgado, 1-3
08034 Barcelona, Spain
lambert@gps.tsc.upc.edu
Rafael Banchs
TALP Research Center
Jordi Girona Salgado, 1-3
08034 Barcelona, Spain
rbanchs@gps.tsc.upc.edu
Abstract
This paper studies a strategy for identify-
ing and using multi-word expressions in
Statistical Machine Translation. The per-
formance of the proposed strategy for var-
ious types of multi-word expressions (like
nouns or verbs) is evaluated in terms of
alignment quality as well as translation ac-
curacy. Evaluations are performed by us-
ing real-life data, namely the European
Parliament corpus. Results from trans-
lation tasks from English-to-Spanish and
from Spanish-to-English are presented and
discussed.
1 Introduction
Statistical machine translation (SMT) was origi-
nally focused on word to word translation and was
based on the noisy channel approach (Brown et
al., 1993). Present SMT systems have evolved
from the original ones in such a way that mainly
differ from them in two issues: first, word-based
translation models have been replaced by phrase-
based translation models (Zens et al, 2002) and
(Koehn et al, 2003); and second, the noisy chan-
nel approach has been expanded to a more general
maximum entropy approach in which a log-linear
combination of multiple feature functions is im-
plemented (Och and Ney, 2002).
Nevertheless, it is interesting to call the atten-
tion about one important fact. Despite the change
from a word-based to a phrase-based translation
approach, word to word approaches for inferring
alignment models from bilingual data (Vogel et al,
1996; Och and Ney, 2003) continue to be widely
used.
On the other hand, from observing bilingual
data sets, it becomes evident that in some cases it
is just impossible to perform a word to word align-
ment between two phrases that are translations of
each other. For example, certain combination of
words might convey a meaning which is somehow
independent from the words it contains. This is
the case of bilingual pairs such as ?fire engine?
and ?camio?n de bomberos?.
Notice that a word-to-word alignment strategy
would most probably1 provide the follow-
ing Viterbi alignments for words contained
in the previous example: ?camio?n:truck?,
?bomberos:firefighters?, ?fuego:fire?, and
?ma?quina:engine?.
Of course, it cannot be concluded from these
examples that a SMT system which uses a word
to word alignment strategy will not be able to han-
dle properly the kind of word expression described
above. This is because there are other models and
feature functions involved which can actually help
the SMT system to get the right translation.
However these ideas motivate for exploring
alternatives for using multi-word expression in-
formation in order to improve alignment quality
and consequently translation accuracy. In this
sense, our idea of a multi-word expression (here-
after MWE) refers in principle to word sequences
which cannot be translated literally word-to-word.
However, the automatic technique studied in this
work for extracting and identifying MWEs does
not necessarily follow this definition rigorously.
In a preliminary study (Lambert and Banchs,
2005), we presented a technique for extracting
bilingual multi-word expressions (BMWE) from
parallel corpora. In that study, BMWEs identified
in a small corpus2 were grouped as a unique to-
1Of course, alignment results strongly depends on corpus
statistics.
2VERBMOBIL (Arranz et al, 2003)
9
ken before training alignment models. As a re-
sult, both alignment quality and translation accu-
racy were slightly improved.
In this paper we applied the same BMWE ex-
traction technique, with various improvements, to
a large corpus (EPPS, described in section 4.1).
Since this is a statistical technique, and frequen-
cies of multi-word expressions are low (Baldwin
and Villavicencio, 2002), the size of the corpus is
an important factor. A few very basic rules based
on part-of-speech have also been added to filter out
noisy entries in the dictionary. Finally, BMWEs
have been classified into three categories (nouns,
verbs and others). In addition to the impact of the
whole set, the impact of each category has been
evaluated separately.
The technique will be explained in section 3, af-
ter presenting the baseline translation system used
(section 2). Experimental results are presented in
section 4. Finally some conclusions are presented
and further work in this area is depicted.
2 Baseline Translation System
This section describes the SMT approach that was
used in this work. A more detailed description
of the presented translation system is available in
Marin?o et al (2005). This approach implements
a translation model which is based on bilingual
n-grams, and was developed by de Gispert and
Marin?o (2002).
The bilingual n-gram translation model actu-
ally constitutes a language model of bilingual units
which are referred to as tuples. This model ap-
proximates the joint probability between source
and target languages by using 3-grams as it is de-
scribed in the following equation:
p(T, S) ?
N?
n=1
p((t, s)n|(t, s)n?2, (t, s)n?1) (1)
where t refers to target, s to source and (t, s)n to
the nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word
aligned corpus. More specifically, word-to-
word alignments are performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2003), and tuples are
extracted from the union set of alignments accord-
ing to the following constraints (de Gispert and
Marin?o, 2004):
? a monotonous segmentation of each bilingual
sentence pairs is produced,
? no word inside the tuple is aligned to words
outside the tuple, and
? no smaller tuples can be extracted without vi-
olating the previous constraints.
As a consequence of these constraints, only one
segmentation is possible for a given sentence pair.
Figure 1 presents a simple example illustrating the
tuple extraction process.
Figure 1: Example of tuple extraction from an
aligned bilingual sentence pair.
A tuple set is extracted for each transla-
tion direction, Spanish-to-English and English-to-
Spanish. Then the tuple 3-gram models are trained
by using the SRI Language Modelling toolkit
(Stolcke, 2002).
The search engine for this translation system
was developed by Crego et al (2005). It imple-
ments a beam-search strategy based on dynamic
programming. The decoder?s monotonic search
modality was used.
This decoder was designed to take into account
various different models simultaneously, so trans-
lation hypotheses are evaluated by considering a
log-linear combination of feature functions. These
feature functions are the translation model, a tar-
get language model, a word bonus model, a lexical
model and an inverse lexical model.
3 Experimental Procedure
In this section we describe the technique used to
see the effect of multi-words information on the
translation model described in section 2.
3.1 Bilingual Multi-words Extraction
First, BMWEs were automatically extracted from
the parallel training corpus and the most relevant
ones were stored in a dictionary.
3.1.1 Asymmetry Based Extraction
For BMWE extraction, the method proposed
by Lambert and Castell (2004) was used. This
10
verdad . . . . . . +
es . . . . . + .
esto . . . . + . .
; . . . + . . .
siento . ?+ . . .
lo ? ? . . . .
I ?m so
rr
y
, th
is
is tru
e
Figure 2: There is an asymmetry in the word-to-
word alignments of the idiomatic expression ?lo
siento ? I ?m sorry?. Source-target and target-
source links are represented respectively by hor-
izontal and vertical dashes.
method is based on word-to-word alignments
which are different in the source-target and target-
source directions, such as the alignments trained
to extract tuples (section 2). Multi-words like id-
iomatic expressions or collocations can typically
not be aligned word-to-word, and cause a (source-
target and target-source) asymmetry in the align-
ment matrix. An asymmetry in the alignment ma-
trix is a sub-matrix where source-target and target-
source links are different. If a word is part of an
asymmetry, all words linked to it are also part of
this asymmetry. An example is depicted in figure
2.
In this method, asymmetries in the training cor-
pus are detected and stored as possible BMWEs.
Accurate statistics are needed to score each
BMWE entry. In the identification phase (sec-
tion 3.3), these scores permit to prioritise for
the selection of some entries with respect to oth-
ers. Previous experiments (Lambert and Banchs,
2005) have shown than the large set of bilingual
phrases described in the following section pro-
vides better statistics than the set of asymmetry-
based BMWEs.
3.1.2 Scoring Based on Bilingual Phrases
Here we refer to Bilingual Phrase (BP) as the
bilingual phrases used by Och and Ney (2004).
The BP are pairs of word groups which are sup-
posed to be the translation of each other. The set
of BP is consistent with the alignment and con-
sists of all phrase pairs in which all words within
the target language are only aligned to the words
of the source language and vice versa. At least
one word of the target language phrase has to be
aligned with at least one word of the source lan-
guage phrase. Finally, the algorithm takes into ac-
count possibly unaligned words at the boundaries
of the target or source language phrases.
We extracted all BP of length up to four words,
with the algorithm described by Och and Ney.
Then we estimated the phrase translation proba-
bility distribution by relative frequency:
p(t|s) =
N(t, s)
N(s)
(2)
In equation 2, s and t stand for the source and
target side of the BP, respectively. N(t, s) is the
number of times the phrase s is translated by t,
and N(s) is the number of times s occurs in the
corpus. We took the minimum of both direct and
inverse relative frequencies as probability of a BP.
If this minimum was below some threshold, the
BP was pruned. Otherwise, this probability was
multiplied by the number of occurrences N(t, s)
of this phrase pair in the whole corpus. A weight
? was introduced to balance the respective impor-
tance of relative frequency and number of occur-
rences, as shown in equation 3:
score = min(p(t|s), p(s|t)) N(t, s)?
= min(
N(t, s)1+?
N(s)
,
N(t, s)1+?
N(t)
)
(3)
We performed the intersection between the en-
tire BP set and the entire asymmetry based multi-
words set, keeping BP scores. Notice that the en-
tire set of BP is not adequate for our dictionary be-
cause BP are extracted from all parts of the align-
ment (and not in asymmetries only), so most BP
are not BMWEs but word sequences that can be
decomposed and translated word to word.
3.2 Lexical and Morpho-syntactic Filters
In English and Spanish, a list of stop words3
(respectively 19 and 26) was established. The
BMWE dictionary was also processed by a Part-
Of-Speech (POS) tagger and eight rules were writ-
ten to filter out noisy entries. These rules depend
on the tag set used. Examples of criteria to reject
a BMWE include:
? Its source or target side only contains stop
words
? Its source or target side ends with a coordina-
tion conjunction
3frequently occurring, semantically insignificant words
like ?in?, ?of?, ?on?.
11
? Its source or target side begins with a coordi-
nation conjunction (except ?nor?, in English)
? Its source or target side ends with an indefi-
nite determiner
English data have been POS-tagged using the TnT
tagger (Brants, 2000), after the lemmas have been
extracted with wnmorph, included in the Wordnet
package (Miller et al, 1991). POS-tagging for
Spanish has been performed using the FreeLing
analysis tool (Carreras et al, 2004).
Finally, the BMWE set has been divided in three
subsets, according to the following criteria, ap-
plied in this order:
? If source AND target sides of a BMWE con-
tain at least a verb, it is assigned to the ?verb?
class.
? If source AND target sides of a BMWE con-
tain at least a noun, it is assigned to the
?noun? class.
? Otherwise, it is assigned to the ?misc? class
(miscellaneous). Note that this class is
mainly composed of adverbial phrases.
3.3 Multi-Words Identification
Identification consists, first, of the detection of all
possible BMWE(s) in the corpus, and second, of
the selection of the relevant candidates.
The detection part simply means matching the
entries of the dictionaries described in the previ-
ous subsections. In the example of figure 2, the
following BMWEs would have been detected (the
number on the right is the score):
i am sorry ||| lo siento ||| 1566
am sorry ||| siento ||| 890
it is ||| es ||| 1004407
it is ||| esto es ||| 269
true ||| es verdad ||| 63
Then, selection in a sentence pair runs as fol-
lows. First, the BMWE with highest score among
the possible candidates is considered and its cor-
responding positions are set as covered. If this
BMWE satisfies the selection criterion, the corre-
sponding words in the source and target sentences
are grouped as a unique token. This process is re-
peated until all word positions are covered in the
sentence pair, or until no BMWE matches the po-
sitions remaining to cover.
The selection criterion rejects candidates whose
words are linked to exactly one word. Thus in the
example, ?esto ? this is? would not be selected.
This is correct, because the subject ?esto? (this)
of the verb ?es? (is) in Spanish is not omitted, so
that ?this is ? es? does not act as BMWE (?esto?
should be translated to ?this? and ?is? to ?es?).
At the end of the identification process the sen-
tence pair of figure 2 would be the following:
?lo siento ; esto es verdad ? I ?m sorry , this is
true?.
In order to increase the recall, BMWE detec-
tion was insensitive to the case of the first letter
of each multi-word. The detection engine also al-
lows a search based on lemmas. Two strategies
are possible. In the first one, search is first carried
out with full forms, so that lemmas are resorted to
only if no match is found with full forms. In the
second strategy, only lemmas are considered.
3.4 Re-alignment
The modified training corpus, with identified
BMWEs grouped in a unique ?super-token? was
aligned again in the same way as explained in sec-
tion 2. By grouping multi-words, we increased the
size of the vocabulary and thus the sparseness of
data. However, we expect that if the meaning of
the multi-words expressions we grouped is effec-
tively different from the meaning of the words they
contain, the individual word probabilities should
be improved.
After re-aligning, we unjoined the super-tokens
that had been grouped in the previous stage, cor-
recting the alignment set accordingly. More pre-
cisely, if two super-tokens A and B were linked
together, after ungrouping them into various to-
kens, every word of A was linked to every word
of B. Translation units were extracted from this
corrected alignment, with the unjoined sentence
pairs (i.e.the same as in the baseline). So the only
difference with respect to the baseline lied in the
alignment, and thus in the distribution of transla-
tion units and in lexical model probabilities.
4 Experimental Results
4.1 Training and Test Data
Our task was word alignment and translation of
parliamentary session transcriptions of the Eu-
ropean Parliament (EPPS). These data are cur-
rently available at the Parliament?s website.4 They
were distributed through the TC-STAR consor-
tium.5 The training and translation test data used
4http://www.euro parl.eu.int/
5http: //www.tc-star.org/
12
included session transcriptions from April 1996
until September 2004, and from November 15th
until November 18th, 2004, respectively. Transla-
tion test data include two reference sets. Align-
ment test data was a subset of the training data
(Lambert et al, 2006).
Table 1 presents some statistics of the various
data sets for each considered language: English
(eng) and Spanish (spa). More specifically, the
statistics presented in Table 1 are, the total num-
ber of sentences, the total number of words, the
vocabulary size (or total number of distinct words)
and the average number of words per sentence.
1.a.- Training data set
Lang. Sentences Words Vocab. Aver.
Eng 1.22 M 33.4 M 105 k 27.3
Spa 1.22 M 35.0 M 151 k 28.6
1.b.- Test data set for translation
Lang. Sentences Words Vocab. Aver.
Eng 1094 26.8 k 3.9 k 24.5
Spa 840 22.7 k 4.0 k 27.0
1.c.- Word alignment reference
Lang. Sentences Words Vocab. Aver.
Eng 400 11.7 k 2.7 k 29.1
Spa 400 12.3 k 3.1 k 30.4
Table 1: Basic statistics for the considered training
(a) translation test (b) and alignment test (c) data
sets (M and k stands for millions and thousands,
respectively).
4.2 Evaluation measures
Details about alignment evaluation can be found
in Lambert et al (2006). The alignment test data
contain unambiguous links (called S or Sure) and
ambiguous links (called P or Possible). If there
is a P link between two words in the reference, a
computed link (i.e. to be evaluated) between these
words is acceptable, but not compulsory. On the
contrary, if there would be an S link between these
words in the reference, a computed link would
be compulsory. In this paper, precision refers to
the proportion of computed links that are present
in the reference. Recall refers to the proportion
of reference Sure links that were computed. The
alignment error rate (AER) is given by the follow-
ing formula:
AER = 1?
|A ? GS |+ |A ? G|
|A|+ |GS |
(4)
where A is the set of computed links, GS is the set
of Sure reference links and G is the entire set of
reference links.
As for translation evaluation, we used the fol-
lowing measures:
WER (word error rate) or mWER (multi-
reference word error rate) The WER is the
minimum number of substitution, insertion
and deletion operations that must be per-
formed to convert the generated sentence into
the reference target sentence. For the mWER,
a whole set of reference translations is used.
In this case, for each translation hypothesis,
the edit distance to the most similar sentence
is calculated.
BLEU score This score measures the precision of
unigrams, bigrams, trigrams, and fourgrams
with respect to a whole set of reference trans-
lations, and with a penalty for too short sen-
tences (Papineni et al, 2001). BLEU mea-
sures accuracy, thus larger scores are better.
4.3 Multi-words in Training Data
In this section we describe the results of the
BMWE extraction and detection techniques ap-
plied to the training data.
4.3.1 Description of the BMWE dictionaries
Parameters of the extraction process have been
optimised with the alignment development corpus
available with the alignment test corpus. With
these parameters, a dictionary of 60k entries was
extracted. After applying the lexical and morpho-
syntactic filters, 45k entries were left. The best
30k entries (hereinafter referred to as all) have
been selected for the experiments and divided in
the three groups mentioned in section 3.2. verb,
noun and misc (miscellaneous) dictionaries con-
tained respectively 11797, 9709 and 8494 entries.
Table 2 shows recall and precision for the
BMWEs identified with each dictionary. The
first line is the evaluation of the MWEs obtained
with the best 30k entries of the dictionary before
filtering. Alignments evaluated in table 2 con-
tained only links corresponding to the identified
BMWEs. For an identified BMWE, a link was in-
troduced between each word of the source side and
each word of the target side. Nevertheless, the test
data contained the whole set of links.
From table 2 we see the dramatic effect of the
filters. The precision for nouns is lower than for
13
Recall Precision
Best 30k (no filters) 13.6 53.6
Best 30k (filters) 11.4 79.3
VERB (filters) 3.7 81.8
NOUN (filters) 4.0 72.8
MISC (filters) 4.1 80.8
Table 2: Quality of the BMWEs identified from
the various dictionaries.
the other categories because many word groups
which were identified, like ?European Parliament -
Parlamento europeo?, are not aligned as a group in
the alignment reference. Notice also that the data
in table 2 reflects the precision of bilingual MWE,
which is a lower bound of the precision of ?super-
tokens? formed in each sentence, the quantity that
matters in our experiment.
Identification of BMWE based on lemmas has
also been experimented. However, with lemmas,
the selection phase is more delicate. With our ba-
sic selection criterion (see section 3.3), the quality
of MWEs identified was worse so we based iden-
tification on full forms.
Figure 3 shows the first 10 entries in the misc
dictionary, along with their renormalised score.
Notice that ?the EU - la UE?, ?young people -
jo?venes? and ?the WTO - la OMC? have been in-
correctly classified due to POS-tagging errors.
the EU ||| la UE ||| 770731
secondly ||| en segundo lugar ||| 610599
however ||| sin embargo ||| 443042
finally ||| por u?ltimo ||| 421879
firstly ||| en primer lugar ||| 324396
thirdly ||| en tercer lugar ||| 286924
young people ||| jo?venes ||| 178571
the WTO ||| la OMC ||| 174496
once again ||| una vez ma?s ||| 169317
once ||| una vez ||| 150139
Figure 3: Examples of BMWEs of the misc cate-
gory.
4.3.2 BMWE Identification Statistics
Table 3 shows, for each language, the MWE vo-
cabulary size after the identification process, and
how many times a MWE has been grouped as a
unique token (instances). The different number
of instances between Spanish and English corre-
spond to one-to-many BMWEs. In general more
MWEs are grouped in the Spanish side, because
English is a denser language. However, the omis-
sion of the subject in Spanish causes the inverse
situation for verbs.
Vocabulary Instances
ENG SPA ENG SPA
ALL 12.2k 12.6k 1.28M 1.56M
VERB 6.0k 3.3k 738k 237k
NOUN 3.9k 5.9k 288k 827k
MISC 3.1k 4.3k 336k 557k
Table 3: Statistics for the BMWEs identified from
the various dictionaries. ALL refers to the 30k best
entries with filters.
4.4 Alignment and Translation Results
Tables 4 and 5 show the effect of aligning the cor-
pus when the various categories of multi-words
have been previously grouped.
IBM1 lexical probabilities baseline All
p(in other words|es decir) - 0.94
p(words|decir) 0.23 0.0013
p(other|decir) 0.026 6 10?5
p(say|decir) 0.45 0.49
Table 4: Single word lexical probabilities of the
alignment model in the baseline and after group-
ing MWE with all dictionary entries. The multi-
word tokens ?in other words? and ?es decir? do
not exist in the baseline.
In table 4 we see how word-to-word lexi-
cal probabilities of the alignment model can be
favourably modified. In the baseline, due to pres-
ence of the fixed expression ?in other words -
es decir?, the probability of ?words? given ?de-
cir? (?say? in English) is high. With this ex-
pression grouped, probabilities p(words|decir) and
p(other|decir) vanish, while p(say|decir) is rein-
forced. These observations allowed to expect that
with many individual probabilities improved, a
global improvement of the alignment would occur.
However, table 5 shows that alignment is not
better when trained with BMWEs grouped as a
unique token.
A closer insight into alignments confirms that
they have not been improved globally. Changes
with respect to the baseline are very localised and
correspond directly to the grouping of the BMWEs
present in each sentence pair.
Table 6 presents the automatic translation eval-
14
Recall Precision AER
Baseline 76.3 85.0 19.4
All 78.0 82.0 19.9
Verb 77.0 84.5 19.3
Noun 76.8 83.0 20.0
Misc 77.0 84.1 19.4
Table 5: Alignment results
uation results. In the Spanish to English direc-
tion, BMWEs seem to have a negative influence.
In the English to Spanish direction, no significant
improvement or worsening is observed.
S?E E?S
mWER BLEU mWER BLEU
Baseline 34.4 0.547 40.2 0.472
All 36.4 0.517 40.7 0.470
Verb 35.1 0.537 40.2 0.472
Noun 35.1 0.537 40.7 0.469
Misc 35.8 0.527 41.1 0.466
Table 6: Translation results in Spanish-to-English
(S?E) and English-to-Spanish (E?S) directions.
In order to understand these results better, we
performed a manual error analysis for the first 50
sentences of the test corpus. We analysed, for the
experiment with all dictionary entries (?All? line
of table 6), the changes in translation with respect
to the baseline. We counted how many changes
had a neutral, positive or negative effect on trans-
lation quality. Results are shown in table 7. Notice
that approximatively half of these changes were
directly related to the presence some BMWE.
This study permitted to see interesting qualita-
tive features. First, BMWEs have a clear influence
on translation, sometimes positive and sometimes
negative, with a balance which appears to be null
in this experiment. In many examples BMWEs
allowed a group translation instead of an incor-
rect word to word literal translation. For instance,
?Red Crescent? was translated by ?Media Luna
Roja? instead of ?Cruz Luna? (cross moon).
Two main types of error were observed. The
first ones are related to the quality of BMWEs. De-
terminers, or particles like ?of?, which are present
in BMWEs are mistakenly inserted in the trans-
lations. Some errors are caused by inadequate
BMWEs. For example ?looking at ? si anal-
izamos? (?if we analyse?) cannot be used in the
sense of looking with the eyes. The second type of
error is related to the rigidity and data sparseness
introduced in the bilingual n-gram model. For ex-
ample, when inflected forms are encapsulated in
a BMWE, the model looses flexibility to trans-
late the correct inflection. Another typical error
is caused by the use of back-off (n-1)-grams in
the bilingual language model, when the n-gram is
not any more available because of increased data
sparseness.
The error analysis did not give explanation for
why the effect of BMWEs is so different for dif-
ferent translation directions. A possible hypothe-
sis would be that BMWEs help in translating from
a denser language. However, in this case, verbs
would be expected to help relatively more in the
Spanish to English direction, since there are more
verb group instances in the English side.
Neutral Positive Negative
S?E 43 20 22
E?S 49 19 17
Table 7: Effect on quality of differences in the
translations between the baseline and the BMWE
experiment with ?ALL? dictionary. S and E stand
for Spanish and English, respectively.
5 Conclusions and Further work
We applied a technique for extracting and using
BMWEs in Statistical Machine Translation. This
technique is based on grouping BMWEs before
performing statistical alignment. On a large cor-
pus with real-life data, this technique failed to
clearly improve alignment quality or translation
accuracy.
After performing a detailed error analysis, we
believe that when the considered MWEs are fixed
expressions, grouping them before training helps
for their correct translation in test. However,
grouping MWEs which could in fact be translated
word to word, doesn?t help and introduces unnec-
essary rigidity and data sparseness in the models.
The main strength of the n-gram translation
model (its history capability) is reduced when tu-
ples become longer. So we plan to run this experi-
ment with a phrase-based translation model. Since
these models use unigrams, they are more flexible
and less sensitive to data sparseness.
Some errors were also caused by noise in the
automatic generation of BMWEs. Thus filter-
15
ing techniques should be improved, and differ-
ent methods for extracting and identifying MWEs
must be developed and evaluated. Resources build
manually, like Wordnet multi-word expressions,
should also be considered.
The proposed method considers the bilingual
multi-words as units ; the use of each side of the
BMWEs as independent monolingual multi-words
must be considered and evaluated.
Acknowledgements
This work has been partially funded by the Eu-
ropean Union under the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation -(IST-2002-FP6-506738,
http://www.tc-star.org).
References
V. Arranz, N. Castell, and J. Gime?nez. 2003. Devel-
opment of language resources for speech-to-speech
translation. In Proc. of the International Conference
on Recent Advances in Natural Language Process-
ing (RANLP), Borovets, Bulgary, September, 10-12.
T. Baldwin and A. Villavicencio. 2002. Extracting the
unextractable: A case study on verb-particles. In
Computational Natural Language Learning Work-
shop (CoNLL).
T. Brants. 2000. Tnt ? a statistical part-of-speech
tagger. In Proc. of Applied Natural Language Pro-
cessing (ANLP), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
Xavier Carreras, I. Chao, L. Padro?, and M. Padro?.
2004. Freeling: An open-source suite of lan-
guage analyzers. In Proc. of the 4th International
Conference on Linguistic Resources and Evaluation
(LREC), Lisbon, Portugal, May.
J. M. Crego, J. Marin?o, and A. de Gispert. 2005. A
ngram-based statistical machine translation decoder.
In Proc. of the 9th European Conf. on Speech Com-
munication and Technology (Interspeech), pages
3185?88, Lisbon, Portugal.
A. de Gispert and J. Marin?o. 2002. Using X-grams for
speech-to-speech translation. Proc. of the 7th Int.
Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert and J. Marin?o. 2004. Talp: Xgram-
based spoken language translation system. Proc. of
the Int. Workshop on Spoken Language Translation,
IWSLT?04, pages 85?90, October.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the 41th An-
nual Meeting of the Association for Computational
Linguistics.
P. Lambert and R. Banchs. 2005. Data inferred multi-
word expressions for statistical machine translation.
In Proc. of Machine Translation Summit X, pages
396?403, Phuket, Tailandia.
P. Lambert and N. Castell. 2004. Alignment of parallel
corpora exploiting asymmetrically aligned phrases.
In Proc. of the LREC 2004 Workshop on the Amaz-
ing Utility of Parallel and Comparable Corpora,
Lisbon, Portugal, May 25.
P. Lambert, A. de Gispert, R. Banchs, and J. Marin?o.
2006. Guidelines for word alignment and manual
alignment. Accepted for publication in Language
Resources and Evaluation.
J. Marin?o, R. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J.A. Fonollosa, and M. Ruiz. 2005.
Bilingual n-gram statistical machine translation. In
Proc. of Machine Translation Summit X, pages 275?
82, Phuket, Thailand.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
K. Miller, and R. Tengi. 1991. Five papers on word-
net. Special Issue of International Journal of Lexi-
cography, 3(4):235?312.
F.J. Och and H. Ney. 2002. Dicriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 295?302, Philadelphia, PA, July.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Re-
port, RC22176, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING?96: The 16thInt. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Springer Verlag,
editor, Proc. German Conference on Artificial Intel-
ligence (KI), september.
16
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
TALP Phrase-based statistical translation system for European language
pairs
Marta R. Costa-jussa`
Patrik Lambert
Jose? B. Marin?o
Josep M. Crego
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.edu
Adria` de Gispert
Rafael E. Banchs
Abstract
This paper reports translation results for
the ?Exploiting Parallel Texts for Statis-
tical Machine Translation? (HLT-NAACL
Workshop on Parallel Texts 2006). We
have studied different techniques to im-
prove the standard Phrase-Based transla-
tion system. Mainly we introduce two re-
ordering approaches and add morphologi-
cal information.
1 Introduction
Nowadays most Statistical Machine Translation
(SMT) systems use phrases as translation units. In
addition, the decision rule is commonly modelled
through a log-linear maximum entropy framework
which is based on several feature functions (in-
cluding the translation model), hm. Each feature
function models the probability that a sentence e in
the target language is a translation of a given sen-
tence f in the source language. The weights, ?i,
of each feature function are typically optimized to
maximize a scoring function. It has the advantage
that additional features functions can be easily in-
tegrated in the overall system.
This paper describes a Phrase-Based system
whose baseline is similar to the system in Costa-
jussa` and Fonollosa (2005). Here we introduce
two reordering approaches and add morphological
information. Translation results for all six trans-
lation directions proposed in the shared task are
presented and discussed. More specifically, four
different languages are considered: English (en),
Spanish (es), French (fr) and German (de); and
both translation directions are considered for the
pairs: EnEs, EnFr, and EnDe. The paper is orga-
nized as follows: Section 2 describes the system;
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project) and the TALP
Research Center (under a TALP-UPC-Recerca grant).
Section 3 presents the shared task results; and, fi-
nally, in Section 4, we conclude.
2 System Description
This section describes the system procedure fol-
lowed for the data provided.
2.1 Alignment
Given a bilingual corpus, we use GIZA++ (Och,
2003) as word alignment core algorithm. During
word alignment, we use 50 classes per language
estimated by ?mkcls?, a freely-available tool along
with GIZA++. Before aligning we work with low-
ercase text (which leads to an Alignment Error
Rate reduction) and we recover truecase after the
alignment is done.
In addition, the alignment (in specific pairs of
languages) was improved using two strategies:
Full verb forms The morphology of the verbs
usually differs in each language. Therefore, it is
interesting to classify the verbs in order to address
the rich variety of verbal forms. Each verb is re-
duced into its base form and reduced POS tag as
explained in (de Gispert, 2005). This transforma-
tion is only done for the alignment, and its goal
is to simplify the work of the word alignment im-
proving its quality.
Block reordering (br) The difference in word
order between two languages is one of the most
significant sources of error in SMT. Related works
either deal with reordering in general as (Kanthak
et al, 2005) or deal with local reordering as (Till-
mann and Ney, 2003). We report a local reorder-
ing technique, which is implemented as a pre-
processing stage, with two applications: (1) to im-
prove only alignment quality, and (2) to improve
alignment quality and to infer reordering in trans-
lation. Here, we present a short explanation of the
algorithm, for further details see Costa-jussa` and
Fonollosa (2006).
142
Figure 1: Example of an Alignment Block, i.e. a
pair of consecutive blocks whose target translation
is swapped
This reordering strategy is intended to infer the
most probable reordering for sequences of words,
which are referred to as blocks, in order to mono-
tonize current data alignments and generalize re-
ordering for unseen pairs of blocks.
Given a word alignment, we identify those pairs
of consecutive source blocks whose translation is
swapped, i.e. those blocks which, if swapped,
generate a correct monotone translation. Figure 1
shows an example of these pairs (hereinafter called
Alignment Blocks).
Then, the list of Alignment Blocks (LAB) is
processed in order to decide whether two consec-
utive blocks have to be reordered or not. By using
the classification algorithm, see the Appendix, we
divide the LAB in groups (Gn, n = 1 . . . N ). In-
side the same group, we allow new internal com-
bination in order to generalize the reordering to
unseen pairs of blocks (i.e. new Alignment Blocks
are created). Based on this information, the source
side of the bilingual corpora are reordered.
In case of applying the reordering technique for
purpose (1), we modify only the source training
corpora to realign and then we recover the origi-
nal order of the training corpora. In case of using
Block Reordering for purpose (2), we modify all
the source corpora (both training and test), and we
use the new training corpora to realign and build
the final translation system.
2.2 Phrase Extraction
Given a sentence pair and a corresponding word
alignment, phrases are extracted following the cri-
terion in Och and Ney (2004). A phrase (or
bilingual phrase) is any pair of m source words
and n target words that satisfies two basic con-
straints: words are consecutive along both sides
of the bilingual phrase, and no word on either side
of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to
7. The huge increase in computational and storage
cost of including longer phrases does not provide
a significant improvement in quality (Koehn et al,
2003) as the probability of reappearance of larger
phrases decreases.
2.3 Feature functions
Conditional and posterior probability (cp, pp)
Given the collected phrase pairs, we estimate the
phrase translation probability distribution by rela-
tive frequency in both directions.
The target language model (lm) consists of an
n-gram model, in which the probability of a trans-
lation hypothesis is approximated by the product
of word n-gram probabilities. As default language
model feature, we use a standard word-based 5-
gram language model generated with Kneser-Ney
smoothing and interpolation of higher and lower
order n-grams (Stolcke, 2002).
The POS target language model (tpos) con-
sists of an N-gram language model estimated over
the same target-side of the training corpus but us-
ing POS tags instead of raw words.
The forward and backwards lexicon mod-
els (ibm1, ibm1?1) provide lexicon translation
probabilities for each phrase based on the word
IBM model 1 probabilities. For computing the
forward lexicon model, IBM model 1 probabili-
ties from GIZA++ source-to-target algnments are
used. In the case of the backwards lexicon model,
target-to-source alignments are used instead.
The word bonus model (wb) introduces a sen-
tence length bonus in order to compensate the sys-
tem preference for short output sentences.
The phrase bonus model (pb) introduces a con-
stant bonus per produced phrase.
2.4 Decoding
The search engine for this translation system is de-
scribed in Crego et al (2005) which takes into ac-
count the features described above.
Using reordering in the decoder (rgraph) A
highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
143
extend the monotone search graph with additional
arcs. See the details in Crego et al (2006).
2.5 Optimization
It is based on a simplex method (Nelder and
Mead, 1965). This algorithm adjusts the log-
linear weights in order to maximize a non-linear
combination of translation BLEU and NIST: 10 ?
log10((BLEU ? 100) + 1) + NIST. The max-
imization is done over the provided development
set for each of the six translation directions under
consideration. We have experimented an improve-
ment in the coherence between all the automatic
figures by integrating two of these figures in the
optimization function.
3 Shared Task Results
3.1 Data
The data provided for this shared task corresponds
to a subset of the official transcriptions of the
European Parliament Plenary Sessions, and it
is available through the shared task website at:
http://www.statmt.org/wmt06/shared-task/.
The development set used to tune the system
consists of a subset (500 first sentences) of the
official development set made available for the
Shared Task.
We carried out a morphological analysis of the
data. The English POS-tagging has been carried
out using freely available TNT tagger (Brants,
2000). In the Spanish case, we have used the
Freeling (Carreras et al, 2004) analysis tool
which generates the POS-tagging for each input
word.
3.2 Systems configurations
The baseline system is the same for all tasks and
includes the following features functions: cp, pp,
lm, ibm1, ibm1?1, wb, pb. The POStag target
language model has been used in those tasks for
which the tagger was available. Table 1 shows the
reordering configuration used for each task.
The Block Reordering (application 2) has been
used when the source language belongs to the Ro-
manic family. The length of the block is lim-
ited to 1 (i.e. it allows the swapping of single
words). The main reason is that specific errors are
solved in the tasks from a Romanic language to
a Germanic language (as the common reorder of
Noun + Adjective that turns into Adjective +
Noun). Although the Block Reordering approach
Task Reordering Configuration
Es2En br2
En2Es br1 + rgraph
Fr2En br2
En2Fr br1 + rgraph
De2En -
En2De -
Table 1: Additional reordering models for each
task: br1 (br2) stands for Block Reordering ap-
plication 1 (application 2); and rgraph refers to
the reordering integrated in the decoder
does not depend on the task, we have not done
the corresponding experiments to observe its ef-
ficiency in all the pairs used in this evaluation.
The rgraph has been applied in those cases
where: we do not use br2 (there is no sense in
applying them simultaneously); and we have the
tagger for the source language model available.
In the case of the pair GeEn, we have not exper-
imented any reordering, we left the application of
both reordering approaches as future work.
3.3 Discussion
Table 2 presents the BLEU scores evaluated on the
test set (using TRUECASE) for each configuration.
The official results were slightly better because a
lowercase evaluation was used, see (Koehn and
Monz, 2006).
For both, Es2En and Fr2En tasks, br helps
slightly. The improvement of the approach de-
pends on the quality of the alignment. The better
alignments allow to extract higher quality Align-
ment Blocks (Costa-jussa` and Fonollosa, 2006).
The En2Es task is improved when adding both
br1 and rgraph. Similarly, the En2Fr task seems to
perform fairly well when using the rgraph. In this
case, the improvement of the approach depends on
the quality of the alignment patterns (Crego et al,
2006). However, it has the advantage of delay-
ing the final decision of reordering to the overall
search, where all models are used to take a fully
informed decision.
Finally, the tpos does not help much when trans-
lating to English. It is not surprising because it was
used in order to improve the gender and number
agreement, and in English there is no need. How-
ever, in the direction to Spanish, the tpos added
to the corresponding reordering helps more as the
Spanish language has gender and number agree-
ment.
144
Task Baseline +tpos +rc +tpos+rc
Es2En 29.08 29.08 29.89 29.98
En2Es 27.73 27.66 28.79 28.99
Fr2En 27.05 27.06 27.43 27.23
En2Fr 26.16 - 27.80 -
De2En 21.59 21.33 - -
En2De 15.20 - - -
Table 2: Results evaluated using TRUECASE on
the test set for each conguration: rc stands for
Reordering Conguration and refers to Table 1.
The bold results were the congurations submit-
ted.
4 Conclusions
Reordering is important when using a Phrase-
Based system. Although local reordering is sup-
posed to be included in the phrase structure, per-
forming local reordering improves the translation
quality. In fact, local reordering, provided by the
reordering approaches, allows for those general-
izations which phrases could not achieve. Re-
ordering in the DeEn task is left as further work.
References
T. Brants. 2000. Tnt - a statistical part-of-speech tag-
ger. Proceedings of the Sixth Applied Natural Lan-
guage Processing.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyz-
ers. 4th Int. Conf. on Language Resources and Eval-
uation, LREC?04.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2005. Im-
proving the phrase-based statistical translation by
modifying phrase extraction and including new fea-
tures. Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2006. Using
reordering in statistical machine translation based on
alignment block classification. Internal Report.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
J. M. Crego, A. de Gispert, P. Lambert, M. R.
Costa-jussa`, M. Khalilov, J. Marin?o, J. A. Fonol-
losa, and R. Banchs. 2006. Ngram-based smt
system enhanced with reordering patterns. HLT-
NAACL06 Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, June.
A. de Gispert. 2005. Phrase linguistic classification for
improving statistical machine translation. ACL 2005
Students Workshop, June.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133, March.
A Appendix
Here we describe the classification algorithm used
in Section 1.
1. Initialization: set n? 1 and LAB ? ? LAB.
2. Main part: while LAB ? is not empty do
? Gn = {(?k, ?k)} where (?k, ?k) is any
element of LAB ?, i.e. ?k is the first
block and ?k is the second block of the
Alignment Block k of the LAB ?.
? Recursively, move elements (?i, ?i)
from LAB? to Gn if there is an element
(?j , ?j) ? Gn such that ?i = ?j or
?i = ?j
? Increase n (i.e. n? n + 1)
3. Ending: For each Gn, construct the two sets
An and Bn which consists on the first and
second element of the pairs in Gn, respec-
tively.
145
Proceedings of the Workshop on Statistical Machine Translation, pages 162?165,
New York City, June 2006. c?2006 Association for Computational Linguistics
N-gram-based SMT System Enhanced with Reordering Patterns
Josep M. Crego
Marta R. Costa-jussa`
Jose? B. Marin?o
Adria` de Gispert
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
{jmcrego,agispert,lambert,mruiz,khalilov,rbanchs,canton,adrian}@gps.tsc.upc.edu
Patrik Lambert
Rafael E. Banchs
Abstract
This work presents translation results for
the three data sets made available in the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? of the
HLT-NAACL 2006 Workshop on Statisti-
cal Machine Translation. All results pre-
sented were generated by using the N-
gram-based statistical machine translation
system which has been enhanced from the
last year?s evaluation with a tagged target
language model (using Part-Of-Speech
tags). For both Spanish-English transla-
tion directions and the English-to-French
translation task, the baseline system al-
lows for linguistically motivated source-
side reorderings.
1 Introduction
The statistical machine translation approach used
in this work implements a log-linear combination
of feature functions along with a translation model
which is based on bilingual n-grams (de Gispert and
Marin?o, 2002).
This translation model differs from the well
known phrase-based translation approach (Koehn
et al, 2003) in two basic issues: first, training data
is monotonously segmented into bilingual units; and
second, the model considers n-gram probabilities in-
stead of relative frequencies. This translation ap-
proach is described in detail in (Marin?o et al, 2005).
For those translation tasks with Spanish or En-
glish as target language, an additional tagged (us-
ing POS information) target language model is used.
Additionally a reordering strategy that includes POS
information is described and evaluated.
Translation results for all six translation directions
proposed in the shared task are presented and dis-
cussed. Both translation directions are considered
for the pairs: English-Spanish, English-French,
and English-German.
The paper is structured as follows: Section 2
briefly outlines the baseline system. Section 3 de-
scribes in detail the implemented POS-based re-
ordering strategy. Section 4 presents and discusses
the shared task results and, finally, section 5 presents
some conclusions and further work.
2 Baseline N-gram-based SMT System
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units, referred
to as tuples, which approximates the joint probabil-
ity between source and target languages by using
bilingual n-grams (de Gispert and Marin?o, 2002).
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, no smaller tuples can be extracted without vi-
olating the previous constraint. See (Crego et al,
2004) for further details.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tu-
ples. In addition to this bilingual n-gram translation
model, the baseline system implements a log linear
combination of five feature functions.
162
These five additional models are:
? A target language model. 5-gram of the target
side of the bilingual corpus.
? A word bonus. Based on the number of tar-
get words in the partial-translation hypothesis,
to compensate the LM preference for short sen-
tences.
? A Source-to-target lexicon model. Based on
IBM Model 1 lexical parameters(Brown et al,
1993), providing a complementary probability
for each tuple in the translation table. These
parameters are obtained from source-to-target
alignments.
? A Target-to-source lexicon model. Analo-
gous to the previous feature, but obtained from
target-to-source alignments.
? A Tagged (POS) target language model. This
feature implements a 5-gram language model
of target POS-tags. In this case, each trans-
lation unit carried the information of its target
side POS-tags, though this is not used for trans-
lation model estimation (only in order to eval-
uate the target POS language model at decod-
ing time). Due to the non-availability of POS-
taggers for French and German, it was not pos-
sible to incorporate this feature in all transla-
tion tasks considered, being only used for those
translation tasks with Spanish and English as
target languages.
The search engine for this translation system is
described in (Crego et al, 2005) and implements
a beam-search strategy based on dynamic program-
ming, taking into account all feature functions de-
scribed above, along with the bilingual n-gram trans-
lation model. Monotone search is performed, in-
cluding histogram and threshold pruning and hy-
pothesis recombination.
An optimization tool, which is based on a down-
hill simplex method was developed and used for
computing log-linear weights for each of the feature
functions. This algorithm adjusts the weights so that
a non-linear combination of BLEU and NIST scores
is maximized over the development set for each of
the six translation directions considered.
This baseline system is actually very similar to
the system used for last year?s shared task ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion? of ACL?05 Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Translation
and Beyond (Banchs et al, 2005), whose results
are available at: http://www.statmt.org/wpt05/
mt-shared-task/. A more detailed description of
the system can be found in (2005).
The tools used for POS-tagging were Freel-
ing (Carreras et al, 2004) for Spanish and
TnT (Brants, 2000) for English. All language mod-
els were estimated using the SRI language mod-
eling toolkit. Word-to-word alignments were ex-
tracted with GIZA++. Improvements in word-to-
word alignments were achieved through verb group
classification as described in (de Gispert, 2005).
3 Reordering Framework
In this section we outline the reordering framework
used for the experiments (Crego and Marin?o, 2006).
A highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
extend the monotone search graph with additional
arcs.
To extract patterns, we use the word-to-word
alignments (the union of both alignment directions)
and source-side POS tags. The main procedure con-
sists of identifying all crossings produced in the
Figure 1: Reordering patterns are extracted using
word-to-word alignments. The generalization power
is achieved through the POS tags. Three instances of
different patterns are extracted using the sentences
in the example.
163
word-to-word alignments. Once a crossing has been
detected, its source POS tags and alignments are
used to account for a new instance of pattern. The
target side of a pattern (source-side positions after
reordering), is computed using the original order
of the target words to which the source words are
aligned. See figure 1 for a clarifying example of
pattern extraction.
The monotone search graph is extended with re-
orderings following the patterns found in training.
The procedure identifies first the sequences of words
in the input sentence that match any available pat-
tern. Then, each of the matchings implies the ad-
dition of an arc into the search graph (encoding the
reordering learnt in the pattern). However, this ad-
dition of a new arc is not performed if a translation
unit with the same source-side words already exists
in the training. Figure 2 shows an example of the
procedure.
Figure 2: Three additional arcs have been added
to the original monotone graph (bold arcs) given
the reordering patterns found matching any of the
source POS tags sequence.
Once the search graph is built, the decoder tra-
verses the graph looking for the best translation.
Hence, the winner hypothesis is computed using
all the available information (the whole SMT mod-
els). The reordering strategy is additionally sup-
ported by a 5-gram language model of reordered
source POS-tags. In training, POS-tags are re-
ordered according with the extracted reordering pat-
terns and word-to-word links. The resulting se-
quence of source POS-tags are used to train the n-
gram LM.
Notice that this reordering framework has only
been used for some translation tasks (Spanish-
to-English, English-to-Spanish and English-to-
French). The reason is double: first, because we
did not have available a French POS-tagger. Second,
because the technique used to learn reorderings (de-
tailed below) does not seem to apply for language
pairs like German-English, because the agglutina-
tive characteristic of German (words are formed by
joining morphemes together).
Table 1: BLEU, NIST and mWER scores (com-
puted using two reference translations) obtained for
both translation directions (Spanish-to-English and
English-to-Spanish).
Conf BLEU NIST mWER
Spanish-to-English
base 55.23 10.69 34.40
+rgraph 55.59 10.70 34.23
+pos 56.39 10.75 33.75
English-to-Spanish
base 48.03 9.84 41.18
+rgraph 48.53 9.81 41.15
+pos 48.91 9.91 40.29
Table 1 shows the improvement of the original
baseline system described in section 2 (base), en-
hanced using reordering graphs (+rgraph) and pro-
vided the tagged-source language model (+pos).
The experiments in table 1 were not carried out over
the official corpus of this shared task. The Spanish-
English corpus of the TC-Star 2005 Evaluation was
used. Due to the high similarities between both cor-
pus (this shared task corpus consists of a subset of
the whole corpus used in the TC-Star 2005 Evalua-
tion), it makes sense to think that comparable results
would be obtained.
It is worth mentioning that the official corpus of
the shared task (HLT-NAACL 2006) was used when
building and tuning the present shared task system.
4 Shared Task Results
The data provided for this shared task corresponds
to a subset of the official transcriptions of the Euro-
pean Parliament Plenary Sessions. The development
set used to tune the system consists of a subset (500
first sentences) of the official development set made
available for the Shared Task.
164
Table 2 presents the BLEU, NIST and mWER
scores obtained for the development-test data set.
The last column shows whether the target POS lan-
guage model feature was used or not. Computed
scores are case sensitive and compare to one refer-
ence translation. Tasks in bold were conducted al-
lowing for the reordering framework. For French-
to-English task, block reordering strategy was used,
which is described in (Costa-jussa` et al, 2006). As it
can be seen, for the English-to-German task we did
not use any of the previous enhancements.
Table 2: Translation results
Task BLEU NIST mWER tPOS
en ? es 29.50 7.32 58.95 yes
es ? en 30.29 7.51 57.72 yes
en ? fr 30.23 7.40 59.76 no
fr ? en 30.21 7.61 56.97 yes
en ? de 17.40 5.61 71.18 no
de ? en 23.78 6.70 65.83 yes
Important differences can be observed between
the German-English and the rest of translation tasks.
They result from the greater differences in word
order present in this language pair (the German-
English results are obtained under monotone decod-
ing conditions). Also because the greater vocabulary
of words of German, which increases sparseness in
any task where German is envolved. As expected,
differences in translation accuracy between Spanish-
English and French-English are smaller.
5 Conclusions and Further Work
As it can be concluded from the presented results,
although in principle some language pairs (Spanish-
English-French) seem to have very little need for re-
orderings (due to their similar word order), the use
of linguistically-based reorderings proves to be use-
ful to improve translation accuracy.
Additional work is to be conducted to allow for
reorderings when translating from/to German.
6 Acknowledgments
This work was partly funded by the European Union
under the integrated project TC-STAR1: Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738) and the European Social Fund.
1http://www.tc-star.org
References
R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, and
J. B. Marin?o. 2005. Statistical machine translation of
euparl data by using bilingual n-grams. Proc. of the
ACL Workshop on Building and Using Parallel Texts
(ACL?05/Wkshp), pages 67?72, June.
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
M.R. Costa-jussa`, J.M. Crego, A. de Gispert, P. Lam-
bert, M. Khalilov, R. Banchs, J.B. Marin?o, and J.A.R.
Fonollosa. 2006. Talp phrase-based statistical transla-
tion system for european language pairs. Proc. of the
HLT/NAACL Workshop on Statistical Machine Trans-
lation, June.
J. M. Crego and J. Marin?o. 2006. A reordering frame-
work for statistical machine translation. Internal Re-
port.
J. M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
J. M. Crego, J. Marin?o, and A. Gispert. 2005. An ngram-
based statistical machine translation decoder. Proc. of
the 9th European Conference on Speech Communica-
tion and Technology, Interspeech?05, September.
A. de Gispert and J. Marin?o. 2002. Using X-grams
for speech-to-speech translation. Proc. of the 7th
Int. Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert. 2005. Phrase linguistic classification and
generalization for improving statistical machine trans-
lation. Proc. of the ACL Student Research Workshop
(ACL?05/SRW), June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R Banchs, J.M. Crego, A. de Gispert,
P. Lambert, M. R. Costa-jussa`, and J.A.R. Fonollosa.
2005. Bilingual n?gram statistical machine transla-
tion. Proc. of the MT Summit X, September.
165
Proceedings of the Second Workshop on Statistical Machine Translation, pages 96?103,
Prague, June 2007. c?2007 Association for Computational Linguistics
Human Evaluation of Machine Translation Through Binary System
Comparisons
David Vilar, Gregor Leusch
and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,leusch,ney}@cs.rwth-aachen.de
Rafael E. Banchs
D. of Signal Theory and Communications
Universitat Polite`cnica de Catalunya
08034 Barcelona, Spain
rbanchs@gps.tsc.upc.edu
Abstract
We introduce a novel evaluation scheme for
the human evaluation of different machine
translation systems. Our method is based
on direct comparison of two sentences at a
time by human judges. These binary judg-
ments are then used to decide between all
possible rankings of the systems. The ad-
vantages of this new method are the lower
dependency on extensive evaluation guide-
lines, and a tighter focus on a typical eval-
uation task, namely the ranking of systems.
Furthermore we argue that machine transla-
tion evaluations should be regarded as sta-
tistical processes, both for human and au-
tomatic evaluation. We show how confi-
dence ranges for state-of-the-art evaluation
measures such as WER and TER can be
computed accurately and efficiently without
having to resort to Monte Carlo estimates.
We give an example of our new evaluation
scheme, as well as a comparison with classi-
cal automatic and human evaluation on data
from a recent international evaluation cam-
paign.
1 Introduction
Evaluation of machine translation (MT) output is a
difficult and still open problem. As in other natu-
ral language processing tasks, automatic measures
which try to asses the quality of the translation
can be computed. The most widely known are the
Word Error Rate (WER), the Position independent
word Error Rate (PER), the NIST score (Dodding-
ton, 2002) and, especially in recent years, the BLEU
score (Papineni et al, 2002) and the Translation Er-
ror Rate (TER) (Snover et al, 2005). All of the-
ses measures compare the system output with one
or more gold standard references and produce a nu-
merical value (score or error rate) which measures
the similarity between the machine translation and a
human produced one. Once such reference transla-
tions are available, the evaluation can be carried out
in a quick, efficient and reproducible manner.
However, automatic measures also have big dis-
advantages; (Callison-Burch et al, 2006) describes
some of them. A major problem is that a given sen-
tence in one language can have several correct trans-
lations in another language and thus, the measure of
similarity with one or even a small amount of ref-
erence translations will never be flexible enough to
truly reflect the wide range of correct possibilities of
a translation. 1 This holds in particular for long sen-
tences and wide- or open-domain tasks like the ones
dealt with in current MT projects and evaluations.
If the actual quality of a translation in terms of
usefulness for human users is to be evaluated, human
evaluation needs to be carried out. This is however
a costly and very time-consuming process. In this
work we present a novel approach to human evalu-
ation that simplifies the task for human judges. In-
stead of having to assign numerical scores to each
sentence to be evaluated, as is done in current evalu-
ation procedures, human judges choose the best one
out of two candidate translations. We show how this
method can be used to rank an arbitrary number of
systems and present a detailed analysis of the statis-
tical significance of the method.
1Compare this with speech recognition, where apart from
orthographic variance there is only one correct reference.
96
2 State-of-the-art
The standard procedure for carrying out a human
evaluation of machine translation output is based on
the manual scoring of each sentence with two nu-
merical values between 1 and 5. The first one mea-
sures the fluency of the sentence, that is its readabil-
ity and understandability. This is a monolingual fea-
ture which does not take the source sentence into
account. The second one reflects the adequacy, that
is whether the translated sentence is a correct trans-
lation of the original sentence in the sense that the
meaning is transferred. Since humans will be the
end users of the generated output,2 it can be ex-
pected that these human-produced measures will re-
flect the usability and appropriateness of MT output
better than any automatic measure.
This kind of human evaluation has however addi-
tional problems. It is much more time consuming
than the automatic evaluation, and because it is sub-
jective, results are not reproducible, even from the
same group of evaluators. Furthermore, there can
be biases among the human judges. Large amounts
of sentences must therefore be evaluated and proce-
dures like evaluation normalization must be carried
out before significant conclusions from the evalua-
tion can be drawn. Another important drawback,
which is also one of the causes of the aforemen-
tioned problems, is that it is very difficult to define
the meaning of the numerical scores precisely. Even
if human judges have explicit evaluation guidelines
at hand, they still find it difficult to assign a numeri-
cal value which represents the quality of the transla-
tion for many sentences (Koehn and Monz, 2006).
In this paper we present an alternative to this eval-
uation scheme. Our method starts from the obser-
vation that normally the final objective of a human
evaluation is to find a ?ranking? of different systems,
and the absolute score for each system is not relevant
(and it can even not be comparable between differ-
ent evaluations). We focus on a method that aims to
simplify the task of the judges and allows to rank the
systems according to their translation quality.
3 Binary System Comparisons
The main idea of our method relies in the fact
that a human evaluator, when presented two differ-
ent translations of the same sentence, can normally
choose the best one out of them in a more or less
2With the exception of cross-language information retrieval
and similar tasks.
definite way. In social sciences, a similar method
has been proposed by (Thurstone, 1927).
3.1 Comparison of Two Systems
For the comparison of two MT systems, a set of
translated sentence pairs is selected. Each of these
pairs consists of the translations of a particular
source sentence from the two systems. The human
judge is then asked to select the ?best? translation of
these two, or to mark the translations to be equally
good. We are aware that the definition of ?best? here
is fuzzy. In our experiments, we made a point of not
giving the evaluators explicit guidelines on how to
decide between both translations. As a consequence,
the judges were not to make a distinction between
fluency and adequacy of the translation. This has a
two-fold purpose: on the one hand it simplifies the
decision procedure for the judges, as in most of the
cases the decision is quite natural and they do not
need to think explicitly in terms of fluency and ade-
quacy. On the other hand, one should keep in mind
that the final goal of an MT system is its usefulness
for a human user, which is why we do not want to
impose artificial constraints on the evaluation proce-
dure. If only certain quality aspects of the systems
are relevant for the ranking, for example if we want
to focus on the fluency of the translations, explicit
guidelines can be given to the judges. If the evalua-
tors are bilingual they can use the original sentences
to judge whether the information was preserved in
the translation.
After our experiment, the human judges provided
feedback on the evaluation process. We learned
that the evaluators normally selected the translation
which preserved most of the information from the
original sentence. Thus, we expect to have a slight
preference for adequacy over fluency in this evalu-
ation process. Note however that adequacy and flu-
ency have shown a high correlation3 in previous ex-
periments. This can be explained by noting that a
low fluency renders the text incomprehensible and
thus the adequacy score will also be low.
The difference in the amount of selected sen-
tences of each system is an indicator of the differ-
ence in quality between the systems. Statistics can
be carried out in order to decide whether this differ-
ence is statistically significant; we will describe this
in more detail in Section 3.4.
3At least for ?sensible? translation systems. Academic
counter-examples could easily be constructed.
97
3.2 Evaluation of Multiple Systems
We can generalize our method to find a ranking of
several systems as follows: In this setting, we have
a set of n systems. Furthermore, we have defined an
order relationship ?is better than? between pairs of
these systems. Our goal now is to find an ordering
of the systems, such that each system is better than
its predecessor. In other words, this is just a sorting
problem ? as widely known in computer science.
Several efficient sorting algorithms can be found
in the literature. Generally, the efficiency of sort-
ing algorithms is measured in terms of the number
of comparisons carried out. State-of-the-art sort-
ing algorithms have a worst-case running time of
O(n log n), where n is the number of elements to
sort. In our case, because such binary comparisons
are very time consuming, we want to minimize the
absolute number of comparisons needed. This mini-
mization should be carried out in the strict sense, not
just in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail. It is
relatively straightforward to show that, in the worst
case, the minimum number of comparisons to be
carried out to sort n elements is at least dlog n!e
(for which n log n is an approximation). It is not
always possible to reach this minimum, however, as
was proven e.g. for the case n = 12 in (Wells, 1971)
and for n = 13 in (Peczarski, 2002). (Ford Jr and
Johnson, 1959) propose an algorithm called merge
insertion which comes very close to the theoretical
limit. This algorithm is sketched in Figure 1. There
are also algorithms with a better asymptotic runtime
(Bui and Thanh, 1985), but they only take effect for
values of n too large for our purposes (e.g., more
than 100). Thus, using the algorithm from Figure 1
we can obtain the ordering of the systems with a
(nearly) optimal number of comparisons.
3.3 Further Considerations
In Section 3.1 we described how to carry out the
comparison between two systems when there is only
one human judge carrying out this comparison. The
comparison of systems is a very time consuming
task. Therefore it is hardly possible for one judge
to carry out the evaluation on a whole test corpus.
Usually, subsets of these test corpora are selected
for human evaluations instead. In order to obtain
a better coverage of the test corpus, but also to try
to alleviate the possible bias of a single evaluator, it
is advantageous to have several evaluators carrying
out the comparison between two systems. However,
there are two points that must be considered.
The first one is the selection of sentences each hu-
man judge should evaluate. Assume that we have al-
ready decided the amount of sentences m each eval-
uator has to work with (in our case m = 100). One
possibility is that all human judges evaluate the same
set of sentences, which presumably will cancel pos-
sible biases of the evaluators. A second possibility is
to give each judge a disjunct set of sentences. In this
way we benefit from a higher coverage of the corpus,
but do not have an explicit bias compensation.
In our experiments, we decided for a middle
course: Each evaluator receives a randomly selected
set of sentences. There are no restrictions on the se-
lection process. This implicitly produces some over-
lap while at the same time allowing for a larger set
of sentences to be evaluated. To maintain the same
conditions for each comparison, we also decided
that each human judge should evaluate the same set
of sentences for each system pair.
The other point to consider is how the evaluation
results of each of the human judges should be com-
bined into a decision for the whole system. One
possibility would be to take only a ?majority vote?
among the evaluators to decide which system is the
best. By doing this, however, possible quantitative
information on the quality difference of the systems
is not taken into account. Consequently, the output is
strongly influenced by statistical fluctuations of the
data and/or of the selected set of sentences to eval-
uate. Thus, in order to combine the evaluations we
just summed over all decisions to get a total count of
sentences for each system.
3.4 Statistical Significance
The evaluation of MT systems by evaluating trans-
lations of test sentences ? be it automatic evaluation
or human evaluation ? must always be regarded as
a statistical process: Whereas the outcome, or score
R, of an evaluation is considered to hold for ?all?
possible sentences from a given domain, a test cor-
pus naturally consists of only a sample from all these
sentences. Consequently, R depends on that sam-
ple of test sentences. Furthermore, both a human
evaluation score and an automatic evaluation score
for a hypothesis sentence are by itself noisy: Hu-
man evaluation is subjective, and as such is subject
to ?human noise?, as described in Section 2. Each
automatic score, on the other hand, depends heavily
on the ambiguous selection of reference translations.
Accordingly, evaluation scores underly a probability
98
1. Make pairwise comparisons of bn/2c disjoint pairs of elements. (If n is odd, leave one element out).
2. Sort the bn/2c larger elements found in step 1, recursively by merge insertion.
3. Name the bn/2c elements found in step 2 a1, a2, . . . , abn/2c and the rest b1, b2, . . . , bdn/2e, such that
a1 ? a2 ? ? ? ? ? abn/2c and bi ? ai for 1 ? i ? bn/2c. Call b1 and the a?s the ?main chain?.
4. Insert the remaining b?s into the main chain, using binary insertion, in the following order (ignore the
bj such that j > dn/2e): b3, b2; b5, b4; b11, . . . , b6; . . . ; btk , . . . , btk?1+1; . . . with tk =
2k+1+(?1)k
3 .
Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).
distribution, and each evaluation result we achieve
must be considered as a sample from that distribu-
tion. Consequently, both human and automatic eval-
uation results must undergo statistical analysis be-
fore conclusions can be drawn from them.
A typical application of MT evaluation ? for ex-
ample in the method described in this paper ? is to
decide whether a given MT system X , represented
by a set of translated sentences, is significantly better
than another system Y with respect to a given eval-
uation measure. This outcome is traditionally called
the alternative hypothesis. The opposite outcome,
namely that the two systems are equal, is known
as the null hypothesis. We say that certain values
of RX , RY confirm the alternative hypothesis if the
null hypothesis can be rejected with a given level
of certainty, e.g. 95%. In the case of comparing
two MT systems, the null hypothesis would be ?both
systems are equal with regard to the evaluation mea-
sure; that is, both evaluation scoresR, R? come from
the same distribution R0?.
As R is randomly distributed, it has an expecta-
tion E[R] and a standard error se[R]. Assuming a
normal distribution for R, we can reject the null hy-
pothesis with a confidence of 95% if the sampled
score R is more than 1.96 times the standard error
away from the null hypothesis expectation:
R significant ? |E[R0] ? R| > 1.96 se[R0] (1)
The question we have to solve is: How can we es-
timate E[R0] and se[R0]? The first step is that we
consider R and R0 to share the same standard error
se[R0] = se[R]. This value can then be estimated
from the test data. In a second step, we give an es-
timate for E[R0], either inherent in the evaluation
measure (see below), or from the estimate for the
comparison system R?.
A universal estimation method is the bootstrap
estimate: The core idea is to create replications of
R by random sampling from the data set (Bisani
and Ney, 2004). Bootstrapping is generally possi-
ble for all evaluation measures. With a high number
of replicates, se[R] and E[R0] can be estimated with
satisfactory precision.
For a certain class of evaluation measures, these
parameters can be estimated more accurately and ef-
ficiently from the evaluation data without resorting
to Monte Carlo estimates. This is the class of er-
rors based on the arithmetic mean over a sentence-
wise score: In our binary comparison experiments,
each judge was given hypothesis translations ei,X ,
ei,Y . She could then judge ei,X to be better than,
equal to, or worse than ei,Y . All these judgments
were counted over the systems. We define a sentence
score ri,X,Y for this evaluation method as follows:
ri,X,Y :=
?
??
??
+1 ei,X is better than ei,Y
0 ei,X is equal to ei,Y
?1 ei,X is worse than ei,Y
. (2)
Then, the total evaluation score for a binary com-
parison of systems X and Y is
RX,Y :=
1
m
m?
i=1
ri,X,Y , (3)
with m the number of evaluated sentences.
For this case, namelyR being an arithmetic mean,
(Efron and Tibshirani, 1993) gives an explicit for-
mula for the estimated standard error of the score
RX,Y . To simplify the notation, we will use R in-
stead of RX,Y from now on, and ri instead of ri,X,Y .
se[R] =
1
m ? 1
?
?
?
?
m?
i=1
(ri ? R)2 . (4)
With x denoting the number of sentences where
ri = 1, and y denoting the number of sentences
99
where ri = ?1,
R =
x ? y
m
(5)
and with basic algebra
se[R] =
1
m ? 1
?
x + y ?
(x ? y)2
m
. (6)
Moreover, we can explicitly give an estimate for
E[R0]: The null hypothesis is that both systems are
?equally good?. Then, we should expect as many
sentences where X is better than Y as vice versa,
i.e. x = y. Thus, E[R0] = 0.
Using Equation 4, we calculate se[R] and thus a
significance range for adequacy and fluency judg-
ments. When comparing two systems X and Y ,
we assume for the null hypothesis that se[R0] =
se[RX ] and E[R0] = E[RY ] (or vice versa).
A very useful (and to our knowledge new) result
for MT evaluation is that se[R] can also be explic-
itly estimated for weighted means ? such as WER,
PER, and TER. These measures are defined as fol-
lows: Let di, i = 1, . . . ,m denote the number of ?er-
rors? (edit operations) of the translation candidate ei
with regard to a reference translation with length li.
Then, the total error rate will be computed as
R :=
1
L
m?
i=1
di (7)
where
L :=
m?
i=1
li (8)
As a result, each sentence ei affects the overall score
with weight li ? the effect of leaving out a sen-
tence with length 40 is four times higher than that
of leaving out one with length 10. Consequently,
these weights must be considered when estimating
the standard error of R:
se[R] =
?
?
?
? 1
(m ? 1)(L ? 1)
m?
i=1
(
di
li
? R
)2
? li
(9)
With this Equation, Monte-Carlo-estimates are no
longer necessary for examining the significance of
WER, PER, TER, etc. Unfortunately, we do not ex-
pect such a short explicit formula to exist for the
standard BLEU score. Still, a confidence range
for BLEU can be estimated by bootstrapping (Och,
2003; Zhang and Vogel, 2004).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Singletons 63K 46K
Test Sentences 1 117
Words 26K
OOV Words 72
Table 1: Statistics of the EPPS Corpus.
4 Evaluation Setup
The evaluation procedure was carried out on the data
generated in the second evaluation campaign of the
TC-STAR project4. The goal of this project is to
build a speech-to-speech translation system that can
deal with real life data. Three translation directions
are dealt with in the project: Spanish to English, En-
glish to Spanish and Chinese to English. For the sys-
tem comparison we concentrated only in the English
to Spanish direction.
The corpus for the Spanish?English language pair
consists of the official version of the speeches held in
the European Parliament Plenary Sessions (EPPS),
as available on the web page of the European Parlia-
ment. A more detailed description of the EPPS data
can be found in (Vilar et al, 2005). Table 1 shows
the statistics of the corpus.
A total of 9 different MT systems participated in
this condition in the evaluation campaign that took
place in February 2006. We selected five representa-
tive systems for our study. Henceforth we shall refer
to these systems as System A through System E. We
restricted the number of systems in order to keep the
evaluation effort manageable for a first experimental
setup to test the feasibility of our method. The rank-
ing of 5 systems can be carried out with as few as 7
comparisons, but the ranking of 9 systems requires
19 comparisons.
5 Evaluation Results
Seven human bilingual evaluators (6 native speakers
and one near-native speaker of Spanish) carried out
the evaluation. 100 sentences were randomly cho-
sen and assigned to each of the evaluators for every
system comparison, as discussed in Section 3.3. The
results can be seen in Table 2 and Figure 2. Counts
4http://www.tc-star.org/
100
0 10 20 30 40 50 60 70
0
10
20
30
40
50
60
70
l
l
l
l
ll
l
# "First system better"
# "S
eco
nd s
yste
m b
ette
r" l
B?AD?CA?CE?AE?BB?DD?A
(a) Each judge.
0 100 200 300 400
0
100
200
300
400
# "First system better"
# "S
eco
nd s
yste
m b
ette
r"
lB?AD?C
A?C E?A
E?B
B?D D?A
(b) All judges.
Figure 2: Results of the binary comparisons. Number of times the winning system was really judged ?better?
vs. number of times it was judged ?worse?. Results in hatched area can not reject null hypothesis, i.e. would
be considered insignificant.
missing to 100 and 700 respectively denote ?same
quality? decisions.
As can be seen from the results, in most of the
cases the judges clearly favor one of the systems.
The most notable exception is found when compar-
ing systems A and C, where a difference of only 3
sentences is clearly not enough to decide between
the two. Thus, the two bottom positions in the final
ranking could be swapped.
Figure 2(a) shows the outcome for the binary
comparisons separately for each judge, together with
an analysis of the statistical significance of the re-
sults. As can be seen, the number of samples (100)
would have been too low to show significant re-
sults in many experiments (data points in the hatched
area). In some cases, the evaluator even judged bet-
ter the system which was scored to be worse by the
majority of the other evaluators (data points above
the bisector). As Figure 2(b) shows, ?the only thing
better than data is more data?: When we summarize
R over all judges, we see a significant difference
(with a confidence of 95%) at all comparisons but
two (A vs. C, and E vs. B). It is interesting to note
that exactly these two pairs do not show a significant
difference when using a majority vote strategy.
Table 3 shows also the standard evaluation met-
rics. Three BLEU scores are given in this table, the
one computed on the whole corpus, the one com-
puted on the set used for standard adequacy and flu-
ency computations and the ones on the set we se-
lected for this task5. It can be seen that the BLEU
scores are consistent across all data subsets. In this
case the ranking according to this automatic measure
matches exactly the ranking found by our method.
When comparing with the adequacy and fluency
scores, however, the ranking of the systems changes
considerably: B D E C A. However, the difference
between the three top systems is quite small. This
can be seen in Figure 3, which shows some auto-
matic and human scores for the five systems in our
experiments, along with the estimated 95% confi-
dence range. The bigger difference is found when
comparing the bottom systems, namely System A
and System C. While our method produces nearly
no difference the adequacy and fluency scores indi-
cate System C as clearly superior to System A. It is
worth noting that the both groups use quite different
translation approaches (statistical vs. rule-based).
5Regretfully these two last sets were not the same. This is
due to the fact that the ?AF Test Set? was further used for eval-
uating Text-to-Speech systems, and thus a targeted subset of
sentences was selected.
101
Sys E1 E2 E3 E4 E5 E6 E7
?
A 29 19 38 17 32 29 41 205
B 40 59 48 53 63 64 45 372
C 32 22 29 23 32 34 42 214
D 39 61 59 50 64 58 46 377
A 32 31 31 31 47 38 40 250
C 37 29 32 22 39 45 43 247
A 36 28 17 28 34 37 31 211
E 41 47 44 43 53 45 58 331
B 26 29 18 24 43 36 33 209
E 34 33 28 27 32 29 43 226
B 34 28 30 31 40 41 48 252
D 23 17 23 17 24 28 38 170
A 36 14 27 9 31 30 34 181
D 34 50 40 50 57 61 57 349
Final ranking (best?worst): E B D A C
Table 2: Result of the binary system comparison.
Numbers of sentences for which each system was
judged better by each evaluator (E1-E7).
Subset: Whole A+F Binary
Sys BLEU BLEU A F BLEU
A 36.3 36.2 2.93 2.46 36.3
B 49.4 49.3 3.74 3.58 49.2
C 36.3 36.2 3.53 3.31 36.1
D 48.2 46.8 3.68 3.48 47.7
E 49.8 49.6 3.67 3.46 49.4
Table 3: BLEU scores and Adequacy and Fluency
scores for the different systems and subsets of the
whole test set. BLEU values in %, Adequacy (A)
and Fluency (F) from 1 (worst) to 5 (best).
6 Discussion
In this section we will review the main drawbacks of
the human evaluation listed in Section 2 and analyze
how our approach deals with them. The first one
was the use of explicit numerical scores, which are
difficult to define exactly. Our system was mainly
designed for the elimination of this issue.
Our evaluation continues to be time consuming.
Even more, the number of individual comparisons
needed is in the order of log(n!), in contrast with the
standard adequacy-fluency evaluation which needs
2n individual evaluations (two evaluations per sys-
tem, one for fluency, another one for adequacy). For
n in the range of 1 up to 20 (a realistic number of
systems for current evaluation campaigns) these two
quantities are comparable. And actually each of our
CA
DB
E
CA
DB
E
CA
DB
E
CA
DB
E
ll
ll
l
ll
ll
l
ll
ll
l
ll
l l
lFluency
Adequacy
1?WER
BLEU
0.3 0.4 0.5 0.6 0.7
          worse <?  normalized score  ?> better
Me
asu
re &
 Sys
tem
Figure 3: Normalized evaluation scores. Higher
scores are better. Solid lines show the 95% con-
fidence range. Automatic scores calculated on the
whole test set, human scores on the A+F subset.
evaluations should be simpler than the standard ad-
equacy and fluency ones. Therefore the time needed
for both evaluation procedures is probably similar.
Reproducibility of the evaluation is also an impor-
tant concern. We computed the number of ?errors?
in the evaluation process, i.e. the number of sen-
tences evaluated by two or more evaluators where
the evaluators? judgement was different. Only in
10% of the cases the evaluation was contradictory,
in the sense that one evaluator chose one sentence as
better than the other, while the other evaluator chose
the other one. In 30% of the cases, however, one
evaluator estimated both sentences to be of the same
quality while the other judged one sentence as supe-
rior to the other one. As comparison, for the fluency-
adequacy judgement nearly one third of the com-
mon evaluations have a difference in score greater or
equal than two (where the maximum would be four),
and another third a score difference of one point6.
With respect to biases, we feel that it is almost im-
possible to eliminate them if humans are involved. If
one of the judges prefers one kind of structure, there
will a bias for a system producing such output, in-
dependently of the evaluation procedure. However,
the suppression of explicit numerical scores elimi-
nates an additional bias of evaluators. It has been
observed that human judges often give scores within
6Note however that possible evaluator biases can have a
great influence in these statistics.
102
a certain range (e.g. in the mid-range or only ex-
treme values), which constitute an additional diffi-
culty when carrying out the evaluation (Leusch et
al., 2005). Our method suppresses this kind of bias.
Another advantage of our method is the possibil-
ity of assessing improvements within one system.
With one evaluation we can decide if some modi-
fications actually improve performance. This eval-
uation even gives us a confidence interval to weight
the significance of an improvement. Carrying out
a full adequacy-fluency analysis would require a lot
more effort, without giving more useful results.
7 Conclusion
We presented a novel human evaluation technique
that simplifies the task of the evaluators. Our method
relies on two basic observations. The first one is that
in most evaluations the final goal is to find a ranking
of different systems, the absolute scores are usually
not so relevant. Especially when considering human
evaluation, the scores are not even comparable be-
tween two evaluation campaigns. The second one
is the fact that a human judge can normally choose
the best one out of two translations, and this is a
much easier process than the assessment of numeri-
cal scores whose definition is not at all clear. Taking
this into consideration we suggested a method that
aims at finding a ranking of different MT systems
based on the comparison of pairs of translation can-
didates for a set of sentences to be evaluated.
A detailed analysis of the statistical significance
of the method is presented and also applied to some
wide-spread automatic measures. The evaluation
methodology was applied for the ranking of 5 sys-
tems that participated in the second evaluation cam-
paign of the TC-STAR project and comparison with
standard evaluation measures was performed.
8 Acknowledgements
We would like to thank the human judges who par-
ticipated in the evaluation. This work has been
funded by the integrated project TC-STAR? Tech-
nology and Corpora for Speech-to-Speech Transla-
tion ? (IST-2002-FP6-506738).
References
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluationx.
IEEE ICASSP, pages 409?412, Montreal, Canada,
May.
T. Bui and M. Thanh. 1985. Significant improvements to
the Ford-Johnson algorithm for sorting. BIT Numeri-
cal Mathematics, 25(1):70?75.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. Proceeding of the 11th Conference of the Eu-
ropean Chapter of the ACL: EACL 2006, pages 249?
256, Trento, Italy, Apr.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Proc. ARPA Workshop on Human Language
Technology.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York and
London.
L. Ford Jr and S. Johnson. 1959. A Tournament Problem.
The American Mathematical Monthly, 66(5):387?389.
D. E. Knuth. 1973. The Art of Computer Programming,
volume 3. Addison-Wesley, 1st edition. Sorting and
Searching.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York
City, Jun.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic evalu-
ation of machine translation. 43rd ACL: Proc. Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 17?24, Ann Ar-
bor, Michigan, Jun.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. Proc. of the 41st ACL, pages
160?167, Sapporo, Japan, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. Proc. of the 40th ACL, pages 311?318,
Philadelphia, PA, Jul.
M. Peczarski. 2002. Sorting 13 elements requires 34
comparisons. LNCS, 2461/2002:785?794, Sep.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2005. A study of translation
error rate with targeted human annotation. Technical
Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-
2005-58, University of Maryland, College Park, MD.
L. Thurstone. 1927. The method of paired comparisons
for social values. Journal of Abnormal and Social Psy-
chology, 21:384?400.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical Machine Translation of European
Parliamentary Speeches. Proceedings of MT Summit
X, pages 259?266, Phuket, Thailand, Sep.
M. Wells. 1971. Elements of combinatorial computing.
Pergamon Press.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, pages 4?6, Baltimore, MD.
103
Proceedings of the Second Workshop on Statistical Machine Translation, pages 167?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Ngram-based statistical machine translation enhanced with multiple
weighted reordering hypotheses
Marta R. Costa-jussa`, Josep M. Crego, Patrik Lambert, Maxim Khalilov
Jose? A. R. Fonollosa, Jose? B. Marin?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu
Abstract
This paper describes the 2007 Ngram-based sta-
tistical machine translation system developed at
the TALP Research Center of the UPC (Uni-
versitat Polite`cnica de Catalunya) in Barcelona.
Emphasis is put on improvements and extensions
of the previous years system, being highlighted
and empirically compared. Mainly, these include
a novel word ordering strategy based on: (1) sta-
tistically monotonizing the training source cor-
pus and (2) a novel reordering approach based
on weighted reordering graphs. In addition, this
system introduces a target language model based
on statistical classes, a feature for out-of-domain
units and an improved optimization procedure.
The paper provides details of this system par-
ticipation in the ACL 2007 SECOND WORK-
SHOP ON STATISTICAL MACHINE TRANSLA-
TION. Results on three pairs of languages are
reported, namely from Spanish, French and Ger-
man into English (and the other way round) for
both the in-domain and out-of-domain tasks.
1 Introduction
Based on estimating a joint-probability model between
the source and the target languages, Ngram-based SMT
has proved to be a very competitive alternatively to
phrase-based and other state-of-the-art systems in previ-
ous evaluation campaigns, as shown in (Koehn and Monz,
2005; Koehn and Monz, 2006).
Given the challenge of domain adaptation, efforts have
been focused on improving strategies for Ngram-based
SMT which could generalize better. Specifically, a novel
reordering strategy is explored. It is based on extending
the search by using precomputed statistical information.
Results are promising while keeping computational ex-
penses at a similar level as monotonic search. Addition-
ally, a bonus for tuples from the out-of-domain corpus is
introduced, as well as a target language model based on
statistical classes. One of the advantages of working with
statistical classes is that they can easily be used for any
pair of languages.
This paper is organized as follows. Section 2 briefly
reviews last year?s system, including tuple definition and
extraction, translation model and feature functions, de-
coding tool and optimization criterion. Section 3 delves
into the word ordering problem, by contrasting last year
strategy with the novel weighted reordering input graph.
Section 4 focuses on new features: both tuple-domain
bonus and target language model based on classes. Later
on, Section 5 reports on all experiments carried out for
WMT 2007. Finally, Section 6 sums up the main conclu-
sions from the paper and discusses future research lines.
2 Baseline N-gram-based SMT System
The translation model is based on bilingual n-grams. It
actually constitutes a language model of bilingual units,
referred to as tuples, which approximates the joint proba-
bility between source and target languages by using bilin-
gual n-grams.
Tuples are extracted from a word-to-word aligned cor-
pus according to the following two constraints: first, tu-
ple extraction should produce a monotonic segmentation
of bilingual sentence pairs; and second, no smaller tuples
can be extracted without violating the previous constraint.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tuples.
In addition to this bilingual n-gram translation model, the
baseline system implements a log linear combination of
four feature functions. These four additional models are:
a target language model (a 5-gram model of words);
a word bonus; a source-to-target lexicon model and a
target-to-source lexicon model, both features provide a
complementary probability for each tuple in the transla-
tion table.
The decoder (called MARIE) for this translation sys-
167
tem is based on a beam search 1.
This baseline system is actually the same system used
for the first shared task ?Exploiting Parallel Texts for Sta-
tistical Machine Translation? of the ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-Driven
Machine Translation and Beyond. A more detailed de-
scription of the system can be found in (Marin?o et al,
2006).
3 Baseline System Enhanced with a
Weighted Reordering Input Graph
This section briefly describes the statistical machine re-
ordering (SMR) technique. Further details on the archi-
tecture of SMR system can be found on (Costa-jussa` and
Fonollosa, 2006).
3.1 Concept
The SMR system can be seen as a SMT system which
translates from an original source language (S) to a re-
ordered source language (S?), given a target language
(T). The SMR technique works with statistical word
classes (Och, 1999) instead of words themselves (partic-
ularly, we have used 200 classes in all experiments).
Figure 1: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
3.2 Using SMR technique to improve SMT training
The original source corpus S is translated into the re-
ordered source corpus S? with the SMR system. Fig-
ure 1 (A) shows the corresponding block diagram. The
reordered training source corpus and the original training
target corpus are used to build the SMT system.
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T original task.
Figure 2 (A) shows an example of the alignment com-
puted on the original training corpus. Figure 2 (B) shows
the same links but with the source training corpus in a
different order (this training corpus comes from the SMR
output). Although, the quality in alignment is the same,
the tuples that can be extracted change (notice that the
tuple extraction is monotonic). We are able to extract
1http://gps-tsc.upc.es/veu/soft/soft/marie/
smaller tuples which reduces the translation vocabulary
sparseness. These new tuples are used to build the SMT
system.
Figure 2: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
3.3 Using SMR technique to generate multiple
weighted reordering hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 1 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear framework. Figure 3 shows the
weighted reordering graph.
The main difference with the reordering technique for
WMT06 (Crego et al, 2006) lies in (1) the tuples are ex-
tracted from the word alignment between the reordered
source training corpus and the given target training cor-
pus and (2) the graph structure: the SMR graph provides
weights for each reordering path.
4 Other features and functionalities
In addition to the novel reordering strategy, we consider
two new features functions.
4.1 Target Language Model based on Statistical
Classes
This feature implements a 5-gram language model of tar-
get statistical classes (Och, 1999). This model is trained
by considering statistical classes, instead of words, for
168
Figure 3: Weighted reordering input graph for SMT sys-
tem.
the target side of the training corpus. Accordingly, the tu-
ple translation unit is redefined in terms of a triplet which
includes: a source string containing the source side of
the tuple, a target string containing the target side of the
tuple, and a class string containing the statistical classes
corresponding to the words in the target strings.
4.2 Bonus for out-of-domain tuples
This feature adds a bonus to those tuples which comes
from the training of the out-of-domain task. This feature
is added when optimizing with the development of the
out-of-domain task.
4.3 Optimization
Finally, a n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/. This procedure
allows for a faster and more efficient adjustment of model
weights by means of a double-loop optimization, which
provides significant reduction of the number of transla-
tions that should be carried out. The current optimization
procedure uses the Simplex algorithm.
5 Shared Task Framework
5.1 Data
The data provided for this shared task corresponds to a
subset of the official transcriptions of the European Par-
liament Plenary Sessions 2. Additionally, there was avail-
able a smaller corpus called News-Commentary. For all
tasks and domains, our training corpus was the catenation
of both.
2http://www.statmt.org/wmt07/shared-task/
5.2 Processing details
Word Alignment. The word alignment is automati-
cally computed by using GIZA++ 3 in both directions,
which are symmetrized by using the union operation. In-
stead of aligning words themselves, stems are used for
aligning. Afterwards case sensitive words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al are splited into de el or a
el. As a post-processing, in the En2Es direction we used
a POS target language model as a feature (instead of the
target language model based on classes) that allowed to
recover the segmentations (de Gispert, 2006).
Language Model Interpolation. In other to better
adapt the system to the out-of-domain condition, the
target language model feature was built by combining
two 5-gram target language models (using SRILM 4).
One was trained from the EuroParl training data set, and
the other from the available, but much smaller, news-
commentary data set. The combination weights for the
EuroParl and news-commentary language models were
empirically adjusted by following a minimum perplexity
criterion. A relative perplexity reduction around 10-15%
respect to original EuroParl language model was achieved
in all the tasks.
5.3 Experiments and Results
The main difference between this year?s and last year?s
systems are: the amount of data provided; the word align-
ment; the Spanish morphology reduction; the reordering
technique; the extra target language model based on sta-
tistical classes (except for the En2Es); and the bonus for
the out-of-domain task (only for the En2Es task).
Among them, the most important is the reordering
technique. That is why we provide a fair comparison be-
tween the reordering patterns (Crego and Marin?o, 2006)
technique and the SMR reordering technique. Table 1
shows the system described above using either reorder-
ing patterns or the SMR technique. The BLEU calcula-
tion was case insensitive and sensitive to tokenization.
Table 2 presents the BLEU score obtained for the 2006
test data set comparing last year?s and this year?s systems.
The computed BLEU scores are case insensitive, sensi-
tive to tokenization and uses one translation reference.
The improvement in BLEU results shown from UPC-jm
3http://www.fjoch.com/GIZA++.html
4http://www.speech.sri.com/projects/srilm/
169
Task Reordering patterns SMR technique
es2en 31.21 33.34
en2es 31.67 32.33
Table 1: BLEU comparison: reordering patterns vs. SMR
technique.
Task UPC-jm 2006 UPC 2007
in-d out-d in-d out-d
es2en 31.01 27.92 33.34 32.85
en2es 30.44 25.59 32.33 33.07
fr2en 30.42 21.79 32.44 26.93
en2fr 31.75 23.30 32.30 27.03
de2en 24.43 17.57 26.54 21.63
en2de 17.73 10.96 19.74 15.06
Table 2: BLEU scores for each of the six translation di-
rections considered (computed over 2006 test set) com-
paring last year?s and this year?s system results (in-
domain and out-domain).
2006 Table 2 and reordering patterns Table 1 in the En-
glish/Spanish in-domain task comes from the combina-
tion of: the additional corpora, the word alignment, the
Spanish morphology reduction and the extra target lan-
guage model based on classes (only in the Es2En direc-
tion).
6 Conclusions and Further Work
This paper describes the UPC system for the WMT07
Evaluation. In the framework of Ngram-based system, a
novel reordering strategy which can be used for any pair
of languages has been presented and it has been showed
to significantly improve translation performance. Ad-
ditionally two features has been added to the log-lineal
scheme: the target language model based on classes and
the bonus for out-of-domain translation units.
7 Acknowledgments
This work has been funded by the European Union un-
der the TC-STAR project (IST-2002-FP6-506738) and
the Spanish Government under grant TEC2006-13964-
C03 (AVIVAVOZ project).
References
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In EMNLP, pages 71?77, Sydney,
July. ACL.
J.M. Crego and J.B. Marin?o. 2006. Reordering experi-
ments for n-gram-based smt. In SLT, pages 242?245,
Aruba.
Josep M. Crego, Adria` de Gispert, Patrik Lambert,
Marta R. Costa-jussa`, Maxim Khalilov, Rafael Banchs,
Jose? B. Marin?o, and Jose? A. R. Fonollosa. 2006. N-
gram-based smt system enhanced with reordering pat-
terns. In WMT, pages 162?165, New York City, June.
ACL.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge in Statistical Machine Translation. Ph.D. thesis,
Universitat Polite`cnica de Catalunya, December.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In WMT, pages 119?124, Michigan, June.
ACL.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In WMT, pages 102?121, New
York City, June. ACL.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549, December.
F.J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In EACL, pages 71?76,
Bergen, Norway, June.
170
Proceedings of the Third Workshop on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The TALP-UPC Ngram-based statistical machine translation system for
ACL-WMT 2008
Maxim Khalilov, Adolfo Hern?ndez H., Marta R. Costa-juss?,
Josep M. Crego, Carlos A. Henr?quez Q., Patrik Lambert,
Jos? A. R. Fonollosa, Jos? B. Mari?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.edu
Abstract
This paper reports on the participation of the TALP
Research Center of the UPC (Universitat Polit?cnica
de Catalunya) to the ACL WMT 2008 evaluation
campaign.
This year?s system is the evolution of the one we em-
ployed for the 2007 campaign. Main updates and
extensions involve linguistically motivated word re-
ordering based on the reordering patterns technique.
In addition, this system introduces a target language
model, based on linguistic classes (Part-of-Speech),
morphology reduction for an inflectional language
(Spanish) and an improved optimization procedure.
Results obtained over the development and test sets
on Spanish to English (and the other way round)
translations for both the traditional Europarl and
a challenging News stories tasks are analyzed and
commented.
1 Introduction
Over the past few years, the Statistical Machine Transla-
tion (SMT) group of the TALP-UPC has been develop-
ing the Ngram-based SMT system (Mari?o et al, 2006).
In previous evaluation campaigns the Ngram-based ap-
proach has proved to be comparable with the state-of-
the-art phrase-based systems, as shown in Koehn and
Monz(2006), Callison-Burch et al (2007).
We present a summary of the TALP-UPC Ngram-
based SMT system used for this shared task. We dis-
cuss the system configuration and novel features, namely
linguistically motivated reordering technique, which is
applied on the decoding step. Additionally, the reorder-
ing procedure is supported by an Ngram language model
(LM) of reordered source Part-of-Speech tags (POS).
In this year?s evaluation we submitted systems for
Spanish-English and English-Spanish language pairs for
the traditional (Europarl) and challenging (News) tasks.
In each case, we used only the supplied data for each lan-
guage pair for models training and optimization.
This paper is organized as follows. Section 2 briefly
outlines the 2008 system, including tuple definition and
extraction, translation model and additional feature mod-
els, decoding tool and optimization procedure. Section 3
describes the word reordering problem and presents the
proposed technique of reordering patterns learning and
application. Later on, Section 4 reports on the experi-
mental setups of the WMT 2008 evaluation campaign. In
Section 5 we sum up the main conclusions from the pa-
per.
2 Ngram-based SMT System
Our translation system implements a log-linear model in
which a foreign language sentence fJ1 = f1, f2, ..., fJ
is translated into another language eI1 = f1, f2, ..., eI by
searching for the translation hypothesis e?I1 maximizing a
log-linear combination of several feature models (Brown
et al, 1990):
e?I1 = argmax
eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system models
and the set of ?m refers to the weights corresponding to
these models.
The core part of the system constructed in that way
is a translation model, which is based on bilingual n-
grams. It actually constitutes an Ngram-based LM of
bilingual units (called tuples), which approximates the
joint probability between the languages under consider-
ation. The procedure of tuples extraction from a word-
to-word alignment according to certain constraints is ex-
plained in detail in Mari?o et al (2006).
The Ngram-based approach differs from the phrase-
based SMT mainly by distinct representating of the bilin-
gual units defined by word alignment and using a higher
127
order HMM of the translation process. While regular
phrase-based SMT considers context only for phrase re-
ordering but not for translation, the N-gram based ap-
proach conditions translation decisions on previous trans-
lation decisions.
The TALP-UPC 2008 translation system, besides the
bilingual translation model, which consists of a 4-gram
LM of tuples with Kneser-Ney discounting (estimated
with SRI Language Modeling Toolkit1), implements a
log-linear combination of five additional feature models:
? a target language model (a 4-gram model of words,
estimated with Kneser-Ney smoothing);
? a POS target language model (a 4-gram model of
tags with Good-Turing discounting (TPOS));
? a word bonus model, which is used to compensate
the system?s preference for short output sentences;
? a source-to-target lexicon model and a target-to-
source lexicon model, these models use word-to-
word IBM Model 1 probabilities (Och and Ney,
2004) to estimate the lexical weights for each tuple
in the translation table.
Decisions on the particular LM configuration and
smoothing technique were taken on the minimal-
perplexity and maximal-BLEU bases.
The decoder (called MARIE), an open source tool2,
implementing a beam search strategy with distortion ca-
pabilities was used in the translation system.
Given the development set and references, the log-
linear combination of weights was adjusted using a sim-
plex optimization method (with the optimization criteria
of the highest BLEU score ) and an n-best re-ranking
just as described in http://www.statmt.org/jhuws/. This
strategy allows for a faster and more efficient adjustment
of model weights by means of a double-loop optimiza-
tion, which provides significant reduction of the number
of translations that should be carried out.
3 Reordering framework
For a great number of translation tasks a certain reorder-
ing strategy is required. This is especially important
when the translation is performed between pairs of lan-
guages with non-monotonic word order. There are var-
ious types of distortion models, simplifying bilingual
translation. In our system we use an extended monotone
reordering model based on automatically learned reorder-
ing rules. A detailed description can be found in Crego
and Mari?o (2006).
1http://www.speech.sri.com/projects/srilm/
2http://gps-tsc.upc.es/veu/soft/soft/marie/
Apart from that, tuples were extracted by an unfold-
ing technique: this means that the tuples are broken into
smaller tuples, and these are sequenced in the order of the
target words.
3.1 Reordering patterns
Word movements are realized according to the reordering
rewrite rules, which have the form of:
t1, ..., tn 7? i1, ..., in
where t1, ..., tn is a sequence of POS tags (relating a
sequence of source words), and i1, ..., in indicates which
order of the source words generate monotonically the tar-
get words.
Patterns are extracted in training from the crossed links
found in the word alignment, in other words, found in
translation tuples (as no word within a tuple can be linked
to a word out of it (Crego and Mari?o, 2006)).
Having all the instances of rewrite patterns, a score for
each pattern on the basis of relative frequency is calcu-
lated as shown below:
p(t1, ..., tn 7? i1, ..., in) =
N(t1, ..., tn 7? i1, ..., in)
NN(t1, ..., tn)
3.2 Search graph extension and source POS model
The monotone search graph is extended with reorderings
following the patterns found in training. Once the search
graph is built, the decoder traverses the graph looking for
the best translation. Hence, the winning hypothesis is
computed using all the available information (the whole
SMT models).
Figure 1: Search graph extension. NC, CC and AQ stand re-
spectively for name, conjunction and adjective.
The procedure identifies first the sequences of words
in the input sentence that match any available pattern.
Then, each of the matchings implies the addition of an arc
into the search graph (encoding the reordering learned in
the pattern). However, this addition of a new arc is not
128
Task BL BL+SPOS
Europarl News Europarl News
es2en 32.79 36.09 32.88 36.36
en2es 32.05 33.91 32.10 33.63
Table 1: BLEU comparison demonstrating the impact of the
source-side POS tags model.
performed if a translation unit with the same source-side
words already exists in the training. Figure 1 shows how
two rewrite rules applied over an input sentence extend
the search graph given the reordering patterns that match
the source POS tag sequence.
The reordering strategy is additionally supported by
a 4-gram language model (estimated with Good-Turing
smoothing) of reordered source POS tags (SPOS). In
training, POS tags are reordered according with the ex-
tracted reordering patterns and word-to-word links. The
resulting sequence of source POS tags is used to train the
Ngram LM.
Table 1 presents the effect of the source POS LM in-
troduction to the reordering module of the Ngram-based
SMT. As it can be seen, the impactya le h of the source-
side POS LM is minimal, however we decided to consider
the model aiming at improving it in future. The reported
results are related to the Europarl and News Commen-
tary (News) development sets. BLEU calculation is case
insensitive and insensitive to tokenization. BL (baseline)
refers to the presented Ngram-based system considering
all the features, apart from the target and source POS
models.
4 WMT 2008 Evaluation Framework
4.1 Corpus
An extraction of the official transcriptions of the 3rd re-
lease of the European Parliament Plenary Sessions3 was
provided for the ACL WMT 2008 shared translation task.
About 40 times smaller corpus from news domain (called
News Commentary) was also available. For both tasks,
our training corpus was the catenation of the Europarl and
News Commentary corpora.
TALP UPC participated in the constraint to the
provided training data track for Spanish-English and
English-Spanish translation tasks. We used the same
training material for the traditional and challenging tasks,
while the development sets used to tune the system were
distinct (2000 sentences for Europarl task and 1057
for News Commentary, one reference translation for
each of them). A brief training and development corpora
statistics is presented in Table 2.
3http://www.statmt.org/wmt08/shared-task.html
Spanish English
Train
Sentences 1.3 M 1.3 M
Words 38.2 M 35.8 K
Vocabulary 156 K 120 K
Development Europarl
Sentences 2000 2000
Words 61.8 K 58.7 K
Vocabulary 8 K 6.5 K
Development News Commentary
Sentences 1057 1057
Words 29.8 K 25.8 K
Vocabulary 5.4 K 4.9 K
Table 2: Basic statistics of ACL WMT 2008 corpus.
4.2 Processing details
The training data was preprocessed by using provided
tools for tokenizing and filtering.
POS tagging. POS information for the source and the
target languages was considered for both translation tasks
that we have participated. The software tools available
for performing POS-tagging were Freeling (Carreras et
al., 2004) for Spanish and TnT (Brants, 2000) for En-
glish. The number of classes for English is 44, while
Spanish is considered as a more inflectional language,
and the tag set contains 376 different tags.
Word Alignment. The word alignment is automati-
cally computed by using GIZA++4(Och and Ney, 2000)
in both directions, which are symmetrized by using the
union operation. Instead of aligning words themselves,
stems are used for aligning. Afterwards case sensitive
words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al were splitted into de el or
a el. As a post-processing, in the En2Es direction we
used a POS target LM as a feature (instead of the target
language model based on classes) that allowed to recover
the segmentations (de Gispert, 2006).
4.3 Experiments and Results
In contrast to the last year?s system where statistical
classes were used to train the target-side tags LM, this
year we used linguistically motivated word classes
4http://code.google.com/p/giza-pp/
129
Task BL+SPOS BL+SPOS+TPOS
(UPC 2008)
Europarl News Europarl News
es2en 32.88 36.36 32.89 36.31
en2es 31.52 34.13 30.72 32.72
en2es "clean"5 32.10 33.63 32.09 35.04
Table 3: BLEU scores for Spanish-English and English-Spanish
2008 development corpora (Europarl and News Commentary).
Task UPC 2008
Europarl News
es2en 32.80 19.61
en2es 31.31 19.28
en2es "clean"5 32.34 20.05
Table 4: BLEU scores for official tests 2008.
(POS) which were considered to train the POS target LM
and extract the reordering patterns. Other characteristics
of this year?s system are:
? reordering patterns technique;
? source POS model, supporting word reordering;
? no LM interpolation. For this year?s evaluation, we
trained two separate LMs for each domain-specific
corpus (i.e., Europarl and News Commentary tasks).
It is important to mention that 2008 training material is
identical to the one provided for the 2007 shared transla-
tion task.
Table 3 presents the BLEU score obtained for the 2008
development data sets and shows the impact of the target-
side POS LM introduction, which can be characterized as
highly corpus- and language-dependent feature. BL refers
to the same system configuration as described in subsec-
tion 3.2. The computed BLEU scores are case insensitive,
insensitive to tokenization and use one translation refer-
ence.
After submitting the systems we discovered a bug re-
lated to incorrect implementation of the target LMs of
words and tags for Spanish, it caused serious reduction
of translation quality (1.4 BLEU points for development
set in case of English-to-Spanish Europarl task and 2.3
points in case of the corresponding News Commentary
task). The last raw of table 3 (en2es "clean") repre-
sents the results corresponding to the UPC 2008 post-
evaluation system, while the previous one (en2es) refers
to the "bugged" system submitted to the evaluation.
The experiments presented in Table 4 correspond to the
2008 test evaluation sets.
5Corrected post-evaluation results (see subsection 4.3.)
5 Conclusions
In this paper we introduced the TALP UPC Ngram-based
SMT system participating in the WMT08 evaluation.
Apart from briefly summarizing the decoding and opti-
mization processes, we have presented the feature mod-
els that were taken into account, along with the bilingual
Ngram translation model. A reordering strategy based on
linguistically-motivated reordering patterns to harmonize
the source and target word order has been presented in
the framework of the Ngram-based system.
6 Acknowledgments
This work has been funded by the Spanish Government
under grant TEC2006-13964-C03 (AVIVAVOZ project).
The authors want to thank Adri? de Gispert (Cambridge
University) for his contribution to this work.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger. In
Proceedings of the 6th Applied Natural Language Processing
(ANLP-2000).
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,
J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational Lin-
guistics, 16(2):79?85.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine trans-
lation. In Proceedings of the ACL 2007 Workshop on Statis-
tical and Hybrid methods for Machine Translation (WMT),
pages 136?158.
X. Carreras, I. Chao, L. Padr?, and M. Padr?. 2004. Freeling:
An open-source suite of language analyzers. In Proceedings
of the 4th Int. Conf. on Language Resources and Evaluation
(LREC?04).
J. M. Crego and J. B. Mari?o. 2006. Improving statistical MT
by coupling reordering and decoding. Machine Translation,
20(3):199?215.
A. de Gispert. 2006. Introducing linguistic knowledge into
statistical machine translation. Ph.D. thesis, Universitat
Polit?cnica de Catalunya, December.
P. Koehn and C. Monz. 2006. Manual and automatic eval-
uation of machine translation between european languages.
In Proceedings of the ACL 2006 Workshop on Statistical and
Hybrid methods for Machine Translation (WMT), pages 102?
121.
J. B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-
bert, J. A. R. Fonollosa, and M. R. Costa-juss?. 2006. N-
gram based machine translation. Computational Linguistics,
32(4):527?549, December.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the the 38th Annual Meeting
on Association for Computational Linguistics (ACL), pages
440?447.
F. Och and H. Ney. 2004. The alignment template approach to
statistical machine translation. 30(4):417 ? 449, December.
130
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85?89,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The TALP-UPC phrase-based translation system for EACL-WMT 2009
Jos? A.R. Fonollosa and Maxim Khalilov and Marta R. Costa-juss? and
Jos? B. Mari?o and Carlos A. Henr?quez Q. and Adolfo Hern?ndez H. and
Rafael E. Banchs
TALP Research Center
Universitat Polit?cnica de Catalunya, Barcelona 08034
{adrian,khalilov,mruiz,canton,carloshq,adolfohh,rbanchs}@talp.upc.edu
Abstract
This study presents the TALP-UPC sub-
mission to the EACL Fourth Worskhop
on Statistical Machine Translation 2009
evaluation campaign. It outlines the ar-
chitecture and configuration of the 2009
phrase-based statistical machine transla-
tion (SMT) system, putting emphasis on
the major novelty of this year: combina-
tion of SMT systems implementing differ-
ent word reordering algorithms.
Traditionally, we have concentrated on
the Spanish-to-English and English-to-
Spanish News Commentary translation
tasks.
1 Introduction
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Polit?cnica de Catalunya) is a permanent par-
ticipant of the ACL WMT shared translations
tasks, traditionally concentrating on the Spanish-
to-English and vice versa language pairs. In this
paper, we describe the 2009 system?s architecture
and design describing individual components and
distinguishing features of our model.
This year?s system stands aside from the
previous years? configurations which were per-
formed following an N -gram-based (tuple-based)
approach to SMT. By contrast to them, this
year we investigate the translation models (TMs)
interpolation for a state-of-the-art phrase-based
translation system. Inspired by the work pre-
sented in (Schwenk and Est?ve, 2008), we attack
this challenge using the coefficients obtained for
the corresponding monolingual language models
(LMs) for TMs interpolation.
On the second step, we have performed
additional word reordering experiments, com-
paring the results obtained with a statisti-
cal method (R. Costa-juss? and R. Fonollosa,
2009) and syntax-based algorithm (Khalilov and
R. Fonollosa, 2008). Further the outputs of
the systems were combined selecting the trans-
lation with the Minimum Bayes Risk (MBR) al-
gorithm (Kumar, 2004) that allowed significantly
outperforming the baseline configuration.
The remainder of this paper is organized as
follows: Section 2 presents the TALP-UPC?09
phrase-based system, along with the translation
models interpolation procedure and other minor
novelties of this year. Section 3 reports on the ex-
perimental setups and outlines the results of the
participation in the EACL WMT 2009 evaluation
campaign. Section 4 concludes the paper with dis-
cussions.
2 TALP-UPC phrase-based SMT
The system developed for this year?s shared
task is based on a state-of-the-art SMT sys-
tem implemented within the open-source MOSES
toolkit (Koehn et al, 2007). A phrase-based trans-
lation is considered as a three step algorithm:
(1) the source sequence of words is segmented
in phrases, (2) each phrase is translated into tar-
get language using translation table, (3) the target
phrases are reordered to be inherent in the target
language.
A bilingual phrase (which in the context of SMT
do not necessarily coincide with their linguistic
analogies) is any pair of m source words and n
target words that satisfies two basic constraints:
(1) words are consecutive along both sides of the
bilingual phrase and (2) no word on either side of
the phrase is aligned to a word outside the phrase.
Given a sentence pair and a corresponding word-
to-word alignment, phrases are extracted follow-
ing the criterion in (Och and Ney, 2004). The
probability of the phrases is estimated by relative
frequencies of their appearance in the training cor-
pus.
85
Classically, a phrase-based translation system
implements a log-linear model in which a foreign
language sentence fJ1 = f1, f2, ..., fJ is trans-
lated into another language eI1 = e1, e2, ..., eI by
searching for the translation hypothesis e?I1 maxi-
mizing a log-linear combination of several feature
models (Brown et al, 1990):
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system
models and the set of ?m refers to the weights cor-
responding to these models.
2.1 Translation models interpolation
We implemented a TM interpolation strategy fol-
lowing the ideas proposed in (Schwenk and Es-
t?ve, 2008), where the authors present a promis-
ing technique of target LMs linear interpolation;
in (Koehn and Schroeder, 2007) where a log-linear
combination of TMs is performed; and specifi-
cally in (Foster and Kuhn, 2007) where the authors
present various ways of TM combination and ana-
lyze in detail the TM domain adaptation.
In the framework of the evaluation campaign,
there were two Spanish-to-English parallel train-
ing corpora available: Europarl v.4 corpus (about
50M tokens) and News Commentary (NC) corpus
(about 2M tokens). The test dataset provided by
the organizers this year was from the news do-
main, so we considered the Europarl training cor-
pus as "out-of-domain" data and the News Com-
mentary as "in-domain" training material. Unfor-
tunately, the in-domain corpus is much smaller in
size, however the Europarl corpus can be also used
to increase the final translation and reordering ta-
bles in spite of its different nature.
A straightforward approach to the TM interpo-
lation would be an iterative TM reconstruction ad-
justing scale coefficients on each step of the loop
with use of the highest BLEU score as a maxi-
mization criterion.
However, we did not expect a significant gain
from this time-consumption strategy and we de-
cided to follow a simpler approach. In the pre-
sented results, we obtained the best interpola-
tion weight following the standard entropy-based
optimization of the target-side LM. We adjust
the weight coefficient ?Europarl (?NC = 1 ?
?Europarl) of the linear interpolation of the target-
side LMs:
P (w) = ?Europarl ? PwEuroparl + ?NC ? PwNC (1)
where PwEuroparl and PwNC are probabilities as-
signed to the word sequence w by the LM esti-
mated on Europarl and NC data, respectively.
The scale factor values are automatically opti-
mized to obtain the lowest perplexity ppl(w) pro-
duced by the interpolated LM P (w). We used the
standard script compute ? best ? mix from the
SRI LM package (Stolcke, 2002) for optimization.
On the next step, the optimized coefficients
?Europarl and ?NC are generalized on the interpo-
lated translation and reordering models. In other
words, reordering and translation models are in-
terpolated using the same weights which yield the
lowest perplexity for LM interpolation.
The word-to-word alignment was obtained from
the joint (merged) database (Europarl + NC).
Then, we separately computed the translation and
reordering tables corresponding to the in- and out-
of-domain parts of the joint alignment. The final
tables, as well as the final target LM were obtained
using linear interpolation. The weights were se-
lected using a minimum perplexity criterion esti-
mated on the corresponding interpolated combina-
tion of the target-side LMs.
The optimized coefficient values are: for Span-
ish: NC weight = 0.526, Europarl weight = 0.474;
for English: NC weight = 0.503, Europarl weight
= 0.497. The perplexity results obtained using
monolingual LMs and the 2009 development set
(English and Spanish references) can be found in
Table 1, while the corresponding improvement in
BLEU score is presented in Section 3.3 and sum-
mary of the obtained results (Table 4).
Europarl NC Interpolated
English 463.439 489.915 353.305
Spanish 308.802 347.092 246.573
Table 1: Perplexity results obtained on the Dev
2009 corpus and the monolingual LMs.
Note that the corresponding reordering models
are interpolated with the same weights.
2.2 Statistical Machine Reordering
The idea of the Statistical Machine Reordering
(SMR) stems from the idea of using the power-
ful techniques developed for SMT and to translate
86
the source language (S) into a reordered source
language (S?), which more closely matches the
order of the target language. To infer more re-
orderings, it makes use of word classes. To cor-
rectly integrate the SMT and SMR systems, both
are concatenated by using a word graph which of-
fers weighted reordering hypotheses to the SMT
system. The details are described in (?).
2.3 Syntax-based Reordering
Syntax-based Reordering (SBR) approach deals
with the word reordering problem and is based on
non-isomorphic parse subtree transfer as described
in details in (Khalilov and R. Fonollosa, 2008).
Local and long-range word reorderings are
driven by automatically extracted permutation pat-
terns operating with source language constituents.
Once the reordering patterns are extracted, they
are further applied to monotonize the bilingual
corpus in the same way as shown in the previ-
ous subsection. The target-side parse tree is con-
sidered as a filter constraining reordering rules to
the set of patterns covered both by the source- and
target-side subtrees.
2.4 System Combination
Over the past few years the MBR algorithm uti-
lization to find the best consensus outputs of dif-
ferent translation systems has proved to improve
the translation accuracy (Kumar, 2004). The sys-
tem combination is performed on the 200-best
lists which are generated by the three systems:
(1) MOSES-based system without pre-translation
monotonization (baseline), (2) MOSES-based
SMT enhanced with SMR monotonization and (3)
MOSES-based SMT augmented with SBR mono-
tonization. The results presented in Table 4 show
that the combined output significantly outperforms
the baseline system configuration.
3 Experiments and results
We followed the evaluation baseline instructions 1
to train the MOSES-based translation system.
In some experiments we used MBR decod-
ing (Kumar and Byrne, 2004) with the smoothed
BLEU score as a similarity criteria, that al-
lowed gaining 0.2 BLEU points comparing to the
standard procedure of outputting the translation
with the highest probability (HP). We applied the
Moses implementation of this algorithm to the list
1http://www.statmt.org/wmt09/baseline.html
of 200 best translations generated by the TALP-
UPC system. The results obtained over the official
2009 Test dataset can be found in Table 2.
Task HP MBR
EsEn 24.48 24.62
EnEs 23.46 23.64
Table 2: MBR versus MERT decoding.
The "recase" script provided within the base-
line was supplemented with and additional mod-
ule, which restore the original case for unknown
words (many of them are proper names and loos-
ing of case information leads to a significant per-
formance degradation).
3.1 Language models
The target-side language models were estimated
using the SRILM toolkit (Stolcke, 2002). We tried
to use all the available in-domain training mate-
rial: apart from the corresponding portions of the
bilingual NC corpora we involved the following
monolingual corpora:
? News monolingual corpus (49M tokens for
English and 49M for Spanish)
? Europarl monolingual corpus (about 504M
tokens for English and 463M for Spanish)
? A collection of News development and test
sets from previous evaluations (151K tokens
for English and 175K for Spanish)
? A collection of Europarl development and
test sets from previous evaluations (295K to-
kens for English and 311K for Spanish)
Five LMs per language were estimated on the
corresponding datasets and interpolated follow-
ing the maximum perplexity criteria. Hence, the
larger LMs incorporating in- and out-of-domain
data were used in decoding.
3.2 Spanish enclitics separation
For the Spanish portion of the corpus we imple-
mented an enclitics separation procedure on the
preprocessing step, i.e. the pronouns attached to
the verb were separated and contractions as del
or al were splitted into de el or a el. Conse-
quently, training data sparseness due to Spanish
morphology was reduced improving the perfor-
mance of the overall translation system. As a
87
post-processing, the segmentation was recovered
in the English-to-Spanish direction using target-
side Part-of-Speech tags (de Gispert, 2006).
3.3 Results
The automatic scores provided by the WMT?09
organizers for TALP-UPC submissions calculated
over the News 2009 dataset can be found in Ta-
ble 3. BLEU and NIST case-insensitive (CI) and
case-sensitive (CS) metrics are considered.
Task Bleu CI Bleu CS NIST CI NIST CS
EsEn 25.93 24.54 7.275 7.017
EnEs 24.85 23.37 6.963 6.689
Table 3: BLEU and NIST scores for preliminary
official test dataset 2009 (primary submission)
with 500 sentences excluded.
The TALP-UPC primary submission was
ranked the 3rd among 28 presented translations
for the Spanish-to-English task and the 4th for the
English-to-Spanish task among 9 systems.
The following system configurations and the in-
ternal results obtained are reported:
? Baseline: Moses-based SMT, as proposed
on the web-page of the evaluation campaign
with Spanish enclitics separation and modi-
fied version of ?recase? tool,
? Baseline+TMI: Baseline enhanced with TM
interpolation as described in subsection 2.1,
? Baseline+TMI+MBR: the same as the latter
but with MBR decoding,
? Baseline+TMI+SMR: the same as Base-
line+TMI but with SMR technique applied to
monotonize the source portion of the corpus,
as described in subsection 2.2,
? Baseline+SBR: the same as Baseline but with
SBR algorithm applied to monotonize the
source portion of the corpus, as described in
subsection 2.3,
? System Combination: a combined output of
the 3 previous systems done with the MBR
algorithm, as described in subsection 2.4.
Impact of TM interpolation and MBR decod-
ing is more significant for the English-to-Spanish
translation task, for which the target-side mono-
lingual corpus is smaller than for the Spanish-to-
English translation.
We did not have time to meet the evalua-
tion deadline for providing the system combi-
nation output. Nevertheless, during the post-
evaluation period we performed the experiments
reported in the last three lines of Table 4 (Base-
line+TMI+SMR, Baseline+SBR and System com-
bination).
Note that the results presented in Table 4 differ
from the ones which can be found the Table 3 due
to selective conditions of preliminary evaluation
done by the Shared Task organizers.
System News 2009 Test CI News 2009 Test CS
Spanish-to-English
Baseline 25.82 24.37
Baseline+TMI 25.84 24.47
Baseline+TMI+MBR (Primary) 26.04 24.62
Baseline+SMR 24.95 23.62
Baseline+SBR 24.24 22.89
System combination 26.44 25.00
English-to-Spanish
Baseline 24.56 23.05
Baseline+TMI 25.01 23.41
Baseline+TMI+MBR (Primary) 25.16 23.64
Baseline+SMR 24.09 22.65
Baseline+SBR 23.52 22.05
System combination 25.39 23.86
Table 4: Experiments summary.
88
4 Conclusions
In this paper, we present the TALP-UPC phrase-
based translation system developed for the EACL-
WMT 2009 evaluation campaign. The major nov-
elties of this year are translation models interpola-
tion done in linear way and combination of SMT
systems implementing different word reordering
algorithms. The system was ranked pretty well for
both translation tasks in which our institution has
participated.
Unfortunately, the promising reordering tech-
niques and the combination of their outputs were
not applied within the evaluation deadline, how-
ever we report the obtained results in the paper.
5 Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
References
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J.D. Lafferty, R. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?
85.
A. de Gispert. 2006. Introducing linguistic knowledge
into Statistical Machine Translation. Ph.D. thesis,
Universitat Polit?cnica de Catalunya, December.
G. Foster and R. Kuhn. 2007. Mixture-model adap-
tation for SMT. In In Annual Meeting of the Asso-
ciation for Computational Linguistics: Proc. of the
Second Workshop on Statistical Machine Transla-
tion (WMT), pages 128?135, Prague, Czech Repub-
lic, June.
M. Khalilov and J. R. Fonollosa. 2008. A new subtree-
transfer approach to syntax-based reordering for sta-
tistical machine translation. Technical report, Uni-
versitat Polit?cnica de Catalunya.
Ph. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In In Annual Meeting of the Association for Compu-
tational Linguistics: Proc. of the Second Workshop
on Statistical Machine Translation (WMT), pages
224?227, Prague, Czech Republic, June.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL) 2007, pages 177?180.
Sh. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In In
HLTNAACL?04, pages 169?176.
Sh. Kumar. 2004. Minimum Bayes-Risk Techniques in
Automatic Speech Recognition and Statistical Ma-
chine Translation. Ph.D. thesis, Johns Hopkins Uni-
versity.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 3(4):417?449, December.
M. R. Costa-juss? and J. R. Fonollosa. 2009. An
Ngram reordering model. Computer Speech and
Language. ISSN 0885-2308, accepted for publica-
tion.
H. Schwenk and Y. Est?ve. 2008. Data selection and
smoothing in an open-source system for the 2008
nist machine translation evaluation. In Proceedings
of the Interspeech?08, pages 2727?2730, Brisbane,
Australia, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of the Int. Conf.
on Spoken Language Processing, pages 901?904.
89
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 153?158,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
AM-FM: A Semantic Framework for Translation Quality Assessment 
 
 
Rafael E. Banchs Haizhou Li 
Human Language Technology Department Human Language Technology Department 
Institute for Infocomm Research Institute for Infocomm Research 
1 Fusionopolis Way, Singapore 138632 1 Fusionopolis Way, Singapore 138632 
rembanchs@i2r.a-star.edu.sg hli@i2r.a-star.edu.sg 
 
 
 
 
 
 
Abstract 
This work introduces AM-FM, a semantic 
framework for machine translation evalua-
tion. Based upon this framework, a new 
evaluation metric, which is able to operate 
without the need for reference translations, 
is implemented and evaluated. The metric 
is based on the concepts of adequacy and 
fluency, which are independently assessed 
by using a cross-language latent semantic 
indexing approach and an n-gram based 
language model approach, respectively. 
Comparative analyses with conventional 
evaluation metrics are conducted on two 
different evaluation tasks (overall quality 
assessment and comparative ranking) over 
a large collection of human evaluations in-
volving five European languages. Finally, 
the main pros and cons of the proposed 
framework are discussed along with future 
research directions. 
1 Introduction 
Evaluation has always been one of the major issues 
in Machine Translation research, as both human 
and automatic evaluation methods exhibit very 
important limitations. On the one hand, although 
highly reliable, in addition to being expensive and 
time consuming, human evaluation suffers from 
inconsistency problems due to inter- and intra-
annotator agreement issues. On the other hand, 
while being consistent, fast and cheap, automatic 
evaluation has the major disadvantage of requiring 
reference translations. This makes automatic eval-
uation not reliable in the sense that good transla-
tions not matching the available references are 
evaluated as poor or bad translations.  
The main objective of this work is to propose 
and evaluate AM-FM, a semantic framework for 
assessing translation quality without the need for 
reference translations. The proposed framework is 
theoretically grounded on the classical concepts of 
adequacy and fluency, and it is designed to account 
for these two components of translation quality in 
an independent manner. First, a cross-language la-
tent semantic indexing model is used for assessing 
the adequacy component by directly comparing the 
output translation with the input sentence it was 
generated from. Second, an n-gram based language 
model of the target language is used for assessing 
the fluency component.  
Both components of the metric are evaluated at 
the sentence level, providing the means for defin-
ing and implementing a sentence-based evaluation 
metric. Finally, the two components are combined 
into a single measure by implementing a weighted 
harmonic mean, for which the weighting factor can 
be adjusted for optimizing the metric performance.  
The rest of the paper is organized as follows. 
Section 2, presents some background work and the 
specific dataset that has been used in the experi-
mental work. Section 3, provides details on the 
proposed AM-FM framework and the specific met-
ric implementation. Section 4 presents the results 
of the conducted comparative evaluations. Finally, 
section 5 presents the main conclusions and rele-
vant issues to be dealt with in future research. 
153
2 Related Work and Dataset 
Although BLEU (Papineni et al, 2002) has be-
come a de facto standard for machine translation 
evaluation, other metrics such as NIST (Dodding-
ton, 2002) and, more recently, Meteor (Banerjee 
and Lavie, 2005), are commonly used too. Regard-
ing the specific idea of evaluating machine trans-
lation without using reference translations, several 
works have proposed and evaluated different ap-
proaches, including round-trip translation (Somers, 
2005; Rapp, 2009), as well as other regression- and 
classification-based approaches (Quirk, 2004; Ga-
mon et al, 2005; Albrecht and Hwa, 2007; Specia 
et al, 2009). 
As part of the recent efforts on machine transla-
tion evaluation, two workshops have been organiz-
ing shared-tasks and evaluation campaigns over the 
last four years: the NIST Metrics for Machine 
Translation Challenge 1  (MetricsMATR) and the 
Workshop on Statistical Machine Translation 2  
(WMT); which were actually held as one single 
event in their most recent edition in 2010. 
The dataset used in this work corresponds to 
WMT-07. This dataset is used, instead of a more 
recent one, because no human judgments on ade-
quacy and fluency have been conducted in WMT 
after year 2007, and human evaluation data is not 
freely available from MetricsMATR. 
In this dataset, translation outputs are available 
for fourteen tasks involving five European lan-
guages: English (EN), Spanish (ES), German (DE), 
French (FR) and Czech (CZ); and two domains: 
News Commentaries (News) and European Par-
liament Debates (EPPS). A complete description 
on WMT-07 evaluation campaign and dataset is 
available in Callison-Burch et al (2007). 
System outputs for fourteen of the fifteen sys-
tems that participated in the evaluation are availa-
ble. This accounts for 86 independent system 
outputs with a total of 172,315 individual sentence 
translations, from which only 10,754 were rated 
for both adequacy and fluency by human judges.  
The specific vote standardization procedure de-
scribed in section 5.4 of Blatz et al (2003) was 
applied to all adequacy and fluency scores for re-
moving individual voting patterns and averaging 
votes. Table 1 provides information on the corre-
sponding domain, and source and target languages 
                                                          
1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/  
2 http://www.statmt.org/wmt10/  
for each of the fourteen translation tasks, along 
with their corresponding number of system outputs 
and the amount of sentence translations for which 
human evaluations are available. 
 
Task Domain Src. Tgt. Syst. Sent. 
T1 News CZ EN 3 727 
T2 News EN CZ 2 806 
T3 EPPS EN FR 7 577 
T4 News EN FR 8 561 
T5 EPPS EN DE 6 924 
T6 News EN DE 6 892 
T7 EPPS EN ES 6 703 
T8 News EN ES 7 832 
T9 EPPS FR EN 7 624 
T10 News FR EN 7 740 
T11 EPPS DE EN 7 949 
T12 News DE EN 5 939 
T13 EPPS ES EN 8 812 
T14 News ES EN 7 668 
 
Table 1: Domain, source language, target lan-
guage, system outputs and total amount of sentence 
translations (with both adequacy and fluency hu-
man assessments) included in the WMT-07 dataset 
3 Semantic Evaluation Framework  
The framework proposed in this work (AM-FM) 
aims at assessing translation quality without the 
need for reference translations, while maintaining 
consistency with human quality assessments. Dif-
ferent from other approaches not using reference 
translations, we rely on a cross-language version of 
latent semantic indexing (Dumais et al, 1997) for 
creating a semantic space where translation outputs 
and inputs can be directly compared.  
A two-component evaluation metric, based on 
the concepts of adequacy and fluency (White et al, 
1994) is defined. While adequacy accounts for the 
amount of source meaning being preserved by the 
translation (5:all, 4:most, 3:much, 2:little, 1:none), 
fluency accounts for the quality of the target lan-
guage in the translation (5:flawless, 4:good, 3:non-
native, 2:disfluent, 1:incomprehensible). 
3.1 Metric Definition 
For implementing the adequacy-oriented compo-
nent (AM) of the metric, the cross-language latent 
semantic indexing approach is used (Dumais et al, 
1997), in which the source sentence originating the 
translation is used as evaluation reference. Accord-
154
ing to this, the AM component can be regarded to 
be mainly adequacy-oriented as it is computed on a 
cross-language semantic space. 
For implementing the fluency-oriented compo-
nent (FM) of the proposed metric, an n-gram based 
language model approach is used (Manning and 
Schutze, 1999). This component can be regarded to 
be mainly fluency-oriented as it is computed on the 
target language side in a manner that is totally in-
dependent from the source language.  
For combining both components into a single 
metric, a weighted harmonic mean is proposed: 
 
AM-FM = AM FM / (? AM + (1-?) FM) (1) 
 
where ? is a weighting factor ranging from ?=0 
(pure AM component) to ?=1 (pure FM compo-
nent), which can be adjusted for maximizing the 
correlation between the proposed metric AM-FM 
and human evaluation scores. 
3.2 Implementation Details 
The adequacy-oriented component of the metric 
(AM) was implemented by following the proce-
dure proposed by Dumais et al (1997), where a 
bilingual collection of data is used to generate a 
cross-language projection matrix for a vector-space 
representation of texts (Salton et al, 1975) by 
using singular value decomposition: SVD (Golub 
and Kahan, 1965).  
According to this formulation, a bilingual term-
document matrix Xab of dimensions M*N, where 
M=(Ma+Mb) are vocabulary terms in languages a 
and b, and N are documents (sentences in our 
case), can be decomposed as follows:  
 
Xab = [Xa;Xb] = Uab ?ab Vab T (2) 
 
where [Xa;Xb] is the concatenation of the two 
monolingual term-document matrices Xa and Xb 
(of dimensions Ma*N and Mb*N) corresponding to 
the available parallel training collection, Uab and 
Vab are unitary matrices of dimensions M*M and 
N*N, respectively, and ? is an M*N diagonal matrix 
containing the singular values associated to the de-
composition. 
From the singular value decomposition depicted 
in (2), a low-dimensional representation for any 
sentence vector xa or xb, in language a or b, can be 
computed as follows: 
ya T  =  [xa ;0] T  UabM*L (3.a) 
 
yb T  =  [0; xb] T  UabM*L (3.b) 
 
where ya and yb represent the L-dimensional vec-
tors corresponding to the projections of the full-
dimensional sentence vectors xa and xb, respective-
ly; and UabM*L is a cross-language projection matrix 
composed of the first L column vectors of the 
unitary matrix Uab obtained in (2).  
Notice, from (3a) and (3b), how both sentence 
vectors xa and xb are padded with zeros at each 
corresponding other-language vocabulary locations 
for performing the cross-language projections. As 
similar terms in different languages would have 
similar occurrence patterns, theoretically, a close 
representation in the cross-language reduced space 
should be obtained for terms and sentences that are 
semantically related. Therefore, sentences can be 
compared across languages in the reduced space. 
The AM component of the metric is finally com-
puted in the projected space by using the cosine 
similarity between the source and target sentences:  
 
AM = [s;0]TP ([0;t]TP)T / |[s;0]TP| / |[0;t]TP| (4) 
 
where P is the projection matrix UabM*L described 
in (3a) and (3b), [s;0] and [0;t] are vector space 
representations of the source and target sentences 
being compared (with their target and source 
vocabulary elements set to zero, respectively), and 
| | is the L2-norm operator. In a final implementa-
tion stage, the range of AM is restricted to the 
interval [0,1] by truncating negative results.  
For computing the projection matrices, random 
sets of 10,000 parallel sentences3 were drawn from 
the available training datasets. The only restriction 
we imposed to the extracted sentences was that 
each should contain at least 10 words. Seven pro-
jection matrices were constructed in total, one for 
each different combination of domain and lan-
guage pair. TF-IDF weighting was applied to the 
constructed term-document matrices while main-
taining all words in the vocabularies (i.e. no stop-
words were removed). All computations related to 
SVD, sentence projections and cosine similarities 
were conducted with MATLAB. 
                                                          
3 Although this accounts for a small proportion of the datasets 
(20% of News and 1% of European Parliament), it allowed for 
maintaining computational requirements under control while 
still providing a good vocabulary coverage. 
155
The fluency-oriented component FM is imple-
mented by using an n-gram language model. In 
order to avoid possible effects derived from dif-
ferences in sentence lengths, a compensation factor 
is introduced in log-probability space. According 
to this, the FM component is computed as follows: 
 
FM  =  exp(?n=1:N log(p(wn|wn-1,?))/N) (5) 
 
where p(wn|wn-1,?) represent the target language 
n-gram probabilities and N is the total number of 
words in the target sentence being evaluated.  
By construction, the values of FM are also re-
stricted to the interval [0,1]; so, both component 
values range within the same interval.  
Fourteen language models were trained in total, 
one per task, by using the available training data-
sets. The models were computed with the SRILM 
toolbox (Stolcke, 2002). 
As seen from (4) and (5), different from con-
ventional metrics that compute matches between 
translation outputs and references, in the AM-FM 
framework, a semantic embedding is used for as-
sessing the similarities between outputs and inputs 
(4) and, independently, an n-gram model is used 
for evaluating output language quality (5). 
4 Comparative Evaluations   
In order to evaluate the AM-FM framework, two 
comparative evaluations with standard metrics 
were conducted. More specifically, BLEU, NIST 
and Meteor were considered, as they are the met-
rics most frequently used in machine translation 
evaluation campaigns.  
4.1 Correlation with Human Scores 
In this first evaluation, AM-FM is compared with 
standard evaluation metrics in terms of their corre-
lations with human-generated scores. Different 
from Callison-Burch et al (2007), where Spear-
man?s correlation coefficients were used, we use 
here Pearson?s coefficients as, instead of focusing 
on ranking; this first evaluation exercise focuses on 
evaluating the significance and noisiness of the 
association, if any, between the automatic metrics 
and human-generated scores. 
Three parameters should be adjusted for the 
AM-FM implementation described in (1): the di-
mensionality of the reduced space for AM, the or-
der of n-gram model for FM, and the harmonic 
mean weighting parameter ?. Such parameters can 
be adjusted for maximizing the correlation coeffi-
cient between the AM-FM metric and human-
generated scores. 4  After exploring the solution 
space, the following values were selected, dimen-
sionality for AM: 1,000; order of n-gram model for 
FM: 3; and, weighting parameter ?: 0.30 
In the comparative evaluation presented here, 
correlation coefficients between the automatic met-
rics and human-generated scores were computed at 
the system level (i.e. the units of analysis were sys-
tem outputs), by considering all 86 available sys-
tem outputs (see Table 1). For computing human 
scores and AM-FM at the system level, average 
values of sentence-based scores for each system 
output were considered.  
Table 2 presents the Pearson?s correlation coef-
ficients computed between the automatic metrics 
(BLEU, NIST, Meteor and our proposed AM-FM) 
and the human-generated scores (adequacy, fluen-
cy and the harmonic mean of both; i.e. 2af/(a+f)). 
All correlation coefficients presented in the table 
are statistically significant with p<0.01 (where p is 
the probability of getting the same correlation 
coefficient, with a similar number of 86 samples, 
by chance).
 
Metric Adequacy Fluency H Mean 
BLEU 0.4232 0.4670 0.4516 
NIST 0.3178 0.3490 0.3396 
Meteor 0.4048 0.3920 0.4065 
AM-FM 0.3719 0.4558 0.4170 
 
Table 2: Pearson?s correlation coefficients (com-
puted at the system level) between automatic met-
rics and human-generated scores 
 
As seen from the table, BLEU is the metric ex-
hibiting the largest correlation coefficients with 
human-generated scores, followed by Meteor and 
AM-FM, while NIST exhibits the lowest correla-
tion coefficient values. Recall that our proposed 
AM-FM metric is not using reference translations 
for assessing translation quality, while the other 
three metrics are. 
In a similar exercise, the correlation coefficients 
were also computed at the sentence level (i.e. the 
units of analysis were sentences). These results are 
summarized in Table 3. As metrics are computed 
                                                          
4 As no development dataset was available for this particular 
task, a subset of the same evaluation dataset had to be used. 
156
at the sentence level, smoothed-bleu (Lin and Och, 
2004) was used in this case. Again, all correlation 
coefficients presented in the table are statistically 
significant with p<0.01.
 
Metric Adequacy Fluency H Mean 
sBLEU 0.3089 0.3361 0.3486 
NIST 0.1208 0.0834 0.1201 
Meteor 0.3220 0.3065 0.3405 
AM-FM 0.2142 0.2256 0.2406 
 
Table 3: Pearson?s correlation coefficients (com-
puted at the sentence level) between automatic 
metrics and human-generated scores 
 
As seen from the table, in this case, BLEU and 
Meteor are the metrics exhibiting the largest 
correlation coefficients, followed by AM-FM and 
NIST.
4.2 Reproducing Rankings   
In addition to adequacy and fluency, the WMT-07 
dataset includes rankings of sentence translations. 
To evaluate the usefulness of AM-FM and its 
components in a different evaluation setting, we 
also conducted a comparative evaluation on their 
capacity for predicting human-generated rankings.   
As ranking evaluations allowed for ties among 
sentence translations, we restricted our analysis to 
evaluate whether automatic metrics were able to 
predict the best, the worst and both sentence trans-
lations for each of the 4,060 available rankings5. 
The number of items per ranking varies from 2 to 
5, with an average of 4.11 items per ranking. Table 
4 presents the results of the comparative evaluation 
on predicting rankings. 
As seen from the table, Meteor is the automatic 
metric exhibiting the largest ranking prediction 
capability, followed by BLEU and NIST, while our 
proposed AM-FM metric exhibits the lowest rank-
ing prediction capability. However, it still performs 
well above random chance predictions, which, for 
the given average of 4 items per ranking, is about 
25% for best and worst ranking predictions, and 
about 8.33% for both. Again, recall that the AM-
FM metric is not using reference translations, 
while the other three metrics are. Also, it is worth 
mentioning that human rankings were conducted 
                                                          
5 We discarded those rankings involving the translation system 
for which translation outputs were not available that, conse-
quently, only had one translation output left. 
by looking at the reference translations and not the 
source. See Callison-Burch et al (2007) for details 
on the human evaluation task. 
 
Metric Best Worst Both 
sBLEU 51.08% 54.90% 37.86% 
NIST 49.56% 54.98% 37.36% 
Meteor 52.83% 58.03% 39.85% 
AM-FM 35.25% 41.11% 25.20% 
AM 37.19% 46.92% 28.47% 
FM 34.01% 39.01% 24.11% 
 
Table 4: Percentage of cases in which each auto-
matic metric is able to predict the best, the worst, 
and both ranked sentence translations 
 
Additionally, results for the individual compo-
nents, AM and FM, are also presented in the table. 
Notice how the AM component exhibits a better 
ranking capability than the FM component. 
5 Conclusions and Future Work 
This work presented AM-FM, a semantic frame-
work for translation quality assessment. Two com-
parative evaluations with standard metrics have 
been conducted over a large collection of human-
generated scores involving different languages. 
Although the obtained performance is below stand-
ard metrics, the proposed method has the main 
advantage of not requiring reference translations. 
Notice that a monolingual version of AM-FM is 
also possible by using monolingual latent semantic 
indexing (Landauer et al, 1998) along with a set of 
reference translations. A detailed evaluation of a 
monolingual implementation of AM-FM can be 
found in Banchs and Li (2011).  
As future research, we plan to study the impact 
of different dataset sizes and vector space model 
parameters for improving the performance of the 
AM component of the metric. This will include the 
study of learning curves based on the amount of 
training data used, and the evaluation of different 
vector model construction strategies, such as re-
moving stop-words and considering bigrams and 
word categories in addition to individual words.   
Finally, we also plan to study alternative uses of 
AM-FM within the context of statistical machine 
translation as, for example, a metric for MERT 
optimization, or using the AM component alone as 
an additional feature for decoding, rescoring and/or 
confidence estimation.
157
References  
Joshua S. Albrecht and Rebeca Hwa. 2007. Regression 
for sentence-level MT evaluation with pseudo 
references. In Proceedings of the 45th Annual 
Meeting of the Association of Computational 
Linguistics, 296-303. 
Rafael E. Banchs and Haizhou Li. 2011. Monolingual 
AM-FM: a two-dimensional machine translation 
evaluation method. Submitted to the Conference on 
Empirical Methods in Natural Language Processing. 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
an automatic metric for MT evaluation with 
improved correlation with human judgments. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization, 65-72.  
John Blatz, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto 
Sanchis and Nicola Ueffing. 2003. Confidence 
estimation for machine translation. Final Report 
WS2003 CLSP Summer Workshop, Johns Hopkins 
University   
Chris Callison-Burch, Cameron Fordyce,Philipp Koehn, 
Christof Monz and Josh Schroeder. 2007. (Meta-) 
evaluation of machine translation. In Proceedings of 
Statistical Machine Translation Workshop, 136-158. 
George Doddington. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Human 
Language Technology Conference. 
Susan Dumais, Thomas K. Landauer and Michael L. 
Littman. 1997. Automatic cross-linguistic 
information retrieval using latent semantic indexing. 
In Proceedings of the SIGIR Workshop on Cross-
Lingual Information Retrieval, 16-23. 
Michael Gamon, Anthony Aue and Martine Smets. 
2005. Sentence-level MT evaluation without 
reference translations: beyond language modeling. In 
Proceedings of the 10th Annual Conference of the 
European Association for Machine Translation, 103-
111. 
G. H. Golub and W. Kahan. 1965. Calculating the 
singular values and pseudo-inverse of a matrix. 
Journal of the Society for Industrial and Applied 
Mathematics: Numerical Analysis, 2(2):205-224. 
 
 
Thomas K. Landauer, Peter W. Foltz and Darrell 
Laham. 1998. Introduction to Latent Semantic 
Analysis. Discourse Processes, 25:259-284. 
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a 
method for evaluating automatic evaluation metrics 
for machine translation. In Proceedings of the 20th 
international conference on Computational 
Linguistics, pp 501, Morristown, NJ. 
Christopher D. Manning and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language 
Processing (Chapter 6). Cambridge, MA: The MIT 
Press. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jung Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, 311-
318.  
Christopher B. Quirk. 2004. Training a sentence-level 
machine translation confidence measure. In 
Proceedings of the 4th International Conference on 
Language Resources and Evaluation, 825-828. 
Reinhard Rapp. 2009. The back-translation score: 
automatic MT evaluation at the sentences level 
without reference translations. In Proceedings of the 
ACL-IJCNLP, 133-136. 
Gerard M. Salton, Andrew K. Wong and C. S. Yang. 
1975. A vector space model for automatic indexing. 
Communications of the ACM, 18(11):613-620. 
Harold Somers. 2005. Round-trip translation: what is it 
good for? In proceedings of the Australasian 
Language Technology Workshop, 127-133. 
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran 
Wang and John Shawe-Taylor. 2009. Improving the 
confidence of machine translation quality estimates. 
In Proceedings of MT Summit XII. Ottawa, Canada. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing.  
John S. White, Theresa O?Cornell and Francis O?Nava. 
1994. The ARPA MT evaluation methodologies: 
evolution, lessons and future approaches. In 
Proceedings of the Association for Machine 
Translation in the Americas, 193-205. 
 
158
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 114?121,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Opinion Mining of Spanish Customer Comments with Non-Expert
Annotations on Mechanical Turk
Bart Mellebeek, Francesc Benavent, Jens Grivolla,
Joan Codina, Marta R. Costa-jussa` and Rafael Banchs
Barcelona Media Innovation Center
Av. Diagonal, 177, planta 9
08018 Barcelona, Spain
{bart.mellebeek|francesc.benavent|jens.grivolla|joan.codina|
marta.ruiz|rafael.banchs}@barcelonamedia.org
Abstract
One of the major bottlenecks in the develop-
ment of data-driven AI Systems is the cost of
reliable human annotations. The recent ad-
vent of several crowdsourcing platforms such
as Amazon?s Mechanical Turk, allowing re-
questers the access to affordable and rapid re-
sults of a global workforce, greatly facilitates
the creation of massive training data. Most
of the available studies on the effectiveness of
crowdsourcing report on English data. We use
Mechanical Turk annotations to train an Opin-
ion Mining System to classify Spanish con-
sumer comments. We design three different
Human Intelligence Task (HIT) strategies and
report high inter-annotator agreement between
non-experts and expert annotators. We evalu-
ate the advantages/drawbacks of each HIT de-
sign and show that, in our case, the use of
non-expert annotations is a viable and cost-
effective alternative to expert annotations.
1 Introduction
Obtaining reliable human annotations to train data-
driven AI systems is often an arduous and expensive
process. For this reason, crowdsourcing platforms
such as Amazon?s Mechanical Turk1, Crowdflower2
and others have recently attracted a lot of attention
from both companies and academia. Crowdsourc-
ing enables requesters to tap from a global pool of
non-experts to obtain rapid and affordable answers
to simple Human Intelligence Tasks (HITs), which
1https://www.mturk.com
2http://crowdflower.com/
can be subsequently used to train data-driven appli-
cations.
A number of recent papers on this subject point
out that non-expert annotations, if produced in a suf-
ficient quantity, can rival and even surpass the qual-
ity of expert annotations, often at a much lower cost
(Snow et al, 2008), (Su et al, 2007). However, this
possible increase in quality depends on the task at
hand and on an adequate HIT design (Kittur et al,
2008).
In this paper, we evaluate the usefulness of MTurk
annotations to train an Opinion Mining System to
detect opinionated contents (Polarity Detection) in
Spanish customer comments on car brands. Cur-
rently, a large majority of MTurk tasks is designed
for English speakers. One of our reasons for partic-
ipating in this shared task was to find out how easy
it is to obtain annotated data for Spanish. In addi-
tion, we want to find out how useful these data are
by comparing them to expert annotations and using
them as training data of an Opinion Mining System
for polarity detection.
This paper is structured as follows. Section 2 con-
tains an explanation of the task outline and our goals.
Section 3 contains a description of three different
HIT designs that we used in this task. In Section
4, we provide a detailed analysis of the retrieved
HITs and focus on geographical information of the
workers, the correlation between the different HIT
designs, the quality of the retrieved answers and on
the cost-effectiveness of the experiment. In Section
5, we evaluate the incidence of MTurk-generated an-
notations on a polarity classification task using two
different experimental settings. Finally, we conclude
114
in Section 6.
2 Task Outline and Goals
We compare different HIT design strategies by eval-
uating the usefulness of resulting Mechanical Turk
(MTurk) annotations to train an Opinion Mining
System on Spanish consumer data. More specifi-
cally, we address the following research questions:
(i) Annotation quality: how do the different
MTurk annotations compare to expert annotations?
(ii) Annotation applicability: how does the per-
formance of an Opinion Mining classifier vary after
training on different (sub)sets of MTurk and expert
annotations?
(iii) Return on Investment: how does the use of
MTurk annotations compare economically against
the use of expert annotations?
(iv) Language barriers: currently, most MTurk
tasks are designed for English speakers. How easy
is it to obtain reliable MTurk results for Spanish?
3 HIT Design
We selected a dataset of 1000 sentences contain-
ing user opinions on cars from the automotive sec-
tion of www.ciao.es (Spanish). This website was
chosen because it contains a large and varied pool
of Spanish customer comments suitable to train an
Opinion Mining System and because opinions in-
clude simultaneously global numeric and specific
ratings over particular attributes of the subject mat-
ter. Section 5.1 contains more detailed information
about the selection of the dataset. An example of a
sentence from the data set can be found in (1):
(1) ?No te lo pienses ma?s, co?mpratelo!?
(= ?Don?t think twice, buy it!?)
The sentences in the dataset were presented to
the MTurk workers in three different HIT designs.
Each HIT design contains a single sentence to be
evaluated. HIT1 is a simple categorization scheme
in which workers are asked to classify the sentence
as being either positive, negative or neutral, as is
shown in Figure 1b. HIT2 is a graded categorization
template in which workers had to assign a score be-
tween -5 (negative) and +5 (positive) to the example
sentence, as is shown in Figure 1c. Finally, HIT3 is
a continuous triangular scoring template that allows
Figure 1: An example sentence (a) and the three HIT
designs used in the experiments: (b) HIT1: a simple
categorization scheme, (c) HIT2: a graded categoriza-
tion scheme, and (d) HIT3: a continuous triangular scor-
ing scheme containing both a horizontal positive-negative
axis and a vertical subjective-objective axis.
workers to use both a horizontal positive-negative
axis and a vertical subjective-objective axis by plac-
ing the example sentence anywhere inside the trian-
gle. The subjective-objective axis expresses the de-
gree to which the sentence contains opinionated con-
tent and was earlier used by (Esuli and Sebastiani,
2006). For example, the sentence ?I think this is a
wonderful car? clearly marks an opinion and should
be positioned towards the subjective end, while the
sentence ?The car has six cilinders? should be lo-
cated towards the objective end. Figure 1d contains
an example of HIT3. In order not to burden the
workers with overly complex instructions, we did
not mention this subjective-objective axis but asked
them instead to place ambiguous sentences towards
the center of the horizontal positive-negative axis
and more objective, non-opinionated sentences to-
wards the lower neutral tip of the triangle.
115
For each of the three HIT designs, we speci-
fied the requirement of three different unique as-
signments per HIT, which led to a total amount of
3 ? 3 ? 1000 = 9000 HIT assignments being up-
loaded on MTurk. Mind that setting the requirement
of unique assigments ensures a number of unique
workers per individual HIT, but does not ensure a
consistency of workers over a single batch of 1000
HITs. This is in the line with the philosophy of
crowdsourcing, which allows many different people
to participate in the same task.
4 Annotation Task Results and Analysis
After designing the HITs, we uploaded 30 random
samples for testing purposes. These HITs were com-
pleted in a matter of seconds, mostly by workers in
India. After a brief inspection of the results, it was
obvious that most answers corresponded to random
clicks. Therefore, we decided to include a small
competence test to ensure that future workers would
possess the necessary linguistic skills to perform the
task. The test consists of six simple categorisation
questions of the type of HIT1 that a skilled worker
would be able to perform in under a minute. In order
to discourage the use of automatic translation tools,
a time limit of two minutes was imposed and most
test sentences contain idiomatic constructions that
are known to pose problems to Machine Translation
Systems.
4.1 HIT Statistics
Table 1 contains statistics on the workers who com-
pleted our HITs. A total of 19 workers passed the
competence test and submitted at least one HIT. Of
those, four workers completed HITs belonging to
two different designs and six submitted HITs in all
three designs. Twelve workers are located in the US
(64%), three in Spain (16%), one in Mexico (5%),
Equador (5%), The Netherlands (5%) and an un-
known location (5%).
As to a comparison of completion times, it took
a worker on average 11 seconds to complete an in-
stance of HIT1, and 9 seconds to complete an in-
stance of HIT2 and HIT3. At first sight, this result
might seem surprising, since conceptually there is an
increase in complexity when moving from HIT1 to
HIT2 and from HIT2 to HIT3. These results might
Overall HIT1 HIT2 HIT3
ID C % # sec. # sec. # sec.
1 mx 29.9 794 11.0 967 8.6 930 11.6
2 us 27.6 980 8.3 507 7.8 994 7.4
3 nl 11.0 85 8.3 573 10.9 333 11.4
4 us 9.5 853 16.8 - - - -
5 es 9.4 - - 579 9.1 265 8.0
6 ec 4.1 151 9.4 14 16.7 200 13.0
7 us 3.6 3 15.7 139 8.5 133 11.6
8 us 2.2 77 8.2 106 7.3 11 10.5
9 us 0.6 - - - - 50 11.2
10 us 0.5 43 5.3 1 5 - -
11 us 0.4 - - 38 25.2 - -
12 us 0.4 - - 10 9.5 27 10.8
13 es 0.4 - - - - 35 15.1
14 es 0.3 - - 30 13.5 - -
15 us 0.3 8 24.7 18 21.5 - -
16 us 0.2 - - - - 22 8.9
17 us 0.2 - - 17 16.5 - -
18 ? 0.1 6 20 - - - -
19 us 0.1 - - 1 33 - -
Table 1: Statistics on MTurk workers for all three HIT
designs: (fictional) worker ID, country code, % of total
number of HITs completed, number of HITs completed
per design and average completion time.
suggest that users find it easier to classify items
on a graded or continuous scale such as HIT2 and
HIT3, which allows for a certain degree of flexibil-
ity, than on a stricter categorical template such as
HIT1, where there is no room for error.
4.2 Annotation Distributions
In order to get an overview of distribution of the re-
sults of each HIT, a histogram was plotted for each
different task. Figure 2a shows a uniform distribu-
tion of the three categories used in the simple cat-
egorization scheme of HIT1, as could be expected
from a balanced dataset.
Figure 2b shows the distribution of the graded cat-
egorization template of HIT2. Compared to the dis-
tribution in 2a, two observations can be made: (i)
the proportion of the zero values is almost identical
to the proportion of the neutral category in Figure
2a, and (ii) the proportion of the sum of the positive
values [+1,+5] and the proportion of the sum of the
negative values [-5,-1] are equally similar to the pro-
portion of the positive and negative categories in 2a.
This suggests that in order to map the graded annota-
tions of HIT2 to the categories of HIT1, an intuitive
partitioning of the graded scale into three equal parts
should be avoided. Instead, a more adequate alterna-
tive would consist of mapping [-5,-1] to negative, 0
116
Figure 2: Overview of HIT results: a) distribution of the three categories used in HIT1, b) distribution of results in the
scaled format of HIT2, c) heat map of the distribution of results in the HIT3 triangle, d) distribution of projection of
triangle data points onto the X-axis (positive/negative).
to neutral and [+1,+5] to positive. This means that
even slightly positive/negative grades correspond to
positive/negative categories.
Figure 2c shows a heat map that plots the distri-
bution of the annotations in the triangle of HIT3. It
appears that worker annotations show a spontaneous
tendency of clustering, despite the continuous nature
of the design. This suggests that this HIT design,
originally conceived as continuous, was transformed
by the workers as a simpler categorization task using
five labels: negative, ambiguous and positive at the
top, neutral at the bottom, and other in the center.
Figure 2d shows the distribution of all data-
points in the triangle of Figure 2c, projected onto
the X-axis (positive/negative). Although similar to
the graded scale in HIT2, the distribution shows a
slightly higher polarization.
These results suggest that, out of all three HIT de-
signs, HIT2 is the one that contains the best balance
between the amount of information that can be ob-
tained and the simplicity of a one-dimensional an-
notation.
4.3 Annotation Quality
The annotation quality of MTurk workers can be
measured by comparing them to expert annotations.
This is usually done by calculating inter-annotator
agreement (ITA) scores. Note that, since a single
HIT can contain more than one assignment and each
assignment is typically performed by more than one
annotator, we can only calculate ITA scores between
batches of assignments, rather than between individ-
ual workers. Therefore, we describe the ITA scores
in terms of batches. In Table 4.4, we present a com-
parison of standard kappa3 calculations (Eugenio
and Glass, 2004) between batches of assignments in
HIT1 and expert annotations.
We found an inter-batch ITA score of 0.598,
which indicates a moderate agreement due to fairly
consistent annotations between workers. When
comparing individual batches with expert annota-
tions, we found similar ITA scores, in the range be-
tween 0.628 and 0.649. This increase with respect
to the inter-batch score suggests a higher variability
among MTurk workers than between workers and
experts. In order to filter out noise in worker annota-
tions, we applied a simple majority voting procedure
in which we selected, for each sentence in HIT1, the
most voted category. This results in an additional
3In reality, we found that fixed and free margin Kappa values
were almost identical, which reflects the balanced distribution
of the dataset.
117
batch of annotations. This batch, refered in Table
4.4 as Majority, produced a considerably higher ITA
score of 0.716, which confirms the validity of the
majority voting scheme to obtain better annotations.
In addition, we calculated ITA scores between
three expert annotators on a separate, 500-sentence
dataset, randomly selected from the same corpus as
described at the start of Section 3. This collection
was later used as test set in the experiments de-
scribed in Section 5. The inter-expert ITA scores
on this separate dataset contains values of 0.725 for
?1 and 0.729 for ?2, only marginally higher than the
Majority ITA scores. Although we are comparing
results on different data sets, these results seem to
indicate that multiple MTurk annotations are able to
produce a similar quality to expert annotations. This
might suggest that a further increase in the number
of HIT assignments would outperform expert ITA
scores, as was previously reported in (Snow et al,
2008).
4.4 Annotation Costs
As explained in Section 3, a total amount of 9000
assignments were uploaded on MTurk. At a reward
of .02$ per assignment, a total sum of 225$ (180$
+ 45$ Amazon fees) was spent on the task. Work-
ers perceived an average hourly rate of 6.5$/hour for
HIT1 and 8$/hour for HIT2 and HIT3. These fig-
ures suggest that, at least for assignments of type
HIT2 and HIT3, a lower reward/assignment might
have been considered. This would also be consis-
tent with the recommendations of (Mason and Watts,
2009), who claim that lower rewards might have an
effect on the speed at which the task will be com-
pleted - more workers will be competing for the task
at any given moment - but not on the quality. Since
we were not certain whether a large enough crowd
existed with the necessary skills to perform our task,
we explicitly decided not to try to offer the lowest
possible price.
An in-house expert annotator (working at approx-
imately 70$/hour, including overhead) finished a
batch of 1000 HIT assignments in approximately
three hours, which leads to a total expert annotator
cost of 210$. By comparing this figure to the cost
of uploading 3 ? 1000 HIT assignments (75$), we
saved 210 ? 75 = 135$, which constitutes almost
65% of the cost of an expert annotator. These figures
do not take into account the costs of preparing the
data and HIT templates, but it can be assumed that
these costs will be marginal when large data sets are
used. Moreover, most of this effort is equally needed
for preparing data for in-house annotation.
?1 ?2
Inter-batch 0.598 0.598
Batch 1 vs. Expert 0.628 0.628
Batch 2 vs. Expert 0.649 0.649
Batch 3 vs. Expert 0.626 0.626
Majority vs. Expert 0.716 0.716
Experts4 0.725 0.729
Table 2: Interannotation Agreement as a measure of qual-
ity of the annotations in HIT1. ?1 = Fixed Margin
Kappa. ?2 = Free Margin Kappa.
5 Incidence of annotations on supervised
polarity classification
This section intends to evaluate the incidence of
MTurk-generated annotations on a polarity classifi-
cation task. We present two different evaluations.
In section 5.2, we compare the results of training
a polarity classification system with noisy available
metadata and with MTurk generated annotations of
HIT1. In section 5.3, we compare the results of
training several polarity classifiers using different
training sets, comparing expert annotations to those
obtained with MTurk.
5.1 Description of datasets
As was mentioned in Section 3, all sentences were
extracted from a corpus of user opinions on cars
from the automotive section of www.ciao.es
(Spanish). For conducting the experimental evalu-
ation, the following datasets were used:
1. Baseline: constitutes the dataset used for train-
ing the baseline or reference classifiers in Ex-
periment 1. Automatic annotation for this
dataset was obtained by using the following
naive approach: those sentences extracted from
comments with ratings5 equal to 5 were as-
signed to category ?positive?, those extracted
5The corpus at www.ciao.es contains consumer opinions
marked with a score between 1 (negative) and 5 (positive).
118
from comments with ratings equal to 3 were
assigned to ?neutral?, and those extracted from
comments with ratings equal to 1 were assigned
to ?negative?. This dataset contains a total of
5570 sentences, with a vocabulary coverage of
11797 words.
2. MTurk Annotated: constitutes the dataset that
was manually annotated by MTurk workers in
HIT1. This dataset is used for training the con-
trastive classifiers which are to be compared
with the baseline system in Experiment 1. It
is also used in various ways in Experiment 2.
The three independent annotations generated
by MTurk workers for each sentence within this
dataset were consolidated into one unique an-
notation by majority voting: if the three pro-
vided annotations happened to be different6,
the sentence was assigned to category ?neutral?;
otherwise, the sentence was assigned to the cat-
egory with at least two annotation agreements.
This dataset contains a total of 1000 sentences,
with a vocabulary coverage of 3022 words.
3. Expert Annotated: this dataset contains the
same sentences as the MTurk Annotated one,
but with annotations produced internally by
known reliable annotators7. Each sentence re-
ceived one annotation, while the dataset was
split between a total of five annotators.
4. Evaluation: constitutes the gold standard used
for evaluating the performance of classifiers.
This dataset was manually annotated by three
experts in an independent manner. The gold
standard annotation was consolidated by using
the same criterion used in the case of the pre-
vious dataset8. This dataset contains a total of
500 sentences, with a vocabulary coverage of
2004 words.
6This kind of total disagreement among annotators occurred
only in 13 sentences out of 1000.
7While annotations of this kind are necessarily somewhat
subjective, these annotations are guaranteed to have been pro-
duced in good faith by competent annotators with an excellent
understanding of the Spanish language (native or near-native
speakers)
8In this case, annotator inter-agreement was above 80%, and
total disagreement among annotators occurred only in 1 sen-
tence out of 500
Baseline Annotated Evaluation
Positive 1882 341 200
Negative 1876 323 137
Neutral 1812 336 161
Totals 5570 1000 500
Table 3: Sentence-per-category distributions for baseline,
annotated and evaluation datasets.
These three datasets were constructed by ran-
domly extracting sample sentences from an origi-
nal corpus of over 25000 user comments contain-
ing more than 1000000 sentences in total. The sam-
pling was conducted with the following constraints
in mind: (i) the three resulting datasets should not
overlap, (ii) only sentences containing more than
3 tokens are considered, and (iii) each resulting
dataset must be balanced, as much as possible, in
terms of the amount of sentences per category. Table
3 presents the distribution of sentences per category
for each of the three considered datasets.
5.2 Experiment one: MTurk annotations vs.
original Ciao annotations
A simple SVM-based supervised classification ap-
proach was considered for the polarity detection task
under consideration. According to this, two dif-
ferent groups of classifiers were used: a baseline
or reference group, and a contrastive group. Clas-
sifiers within these two groups were trained with
data samples extracted from the baseline and anno-
tated datasets, respectively. Within each group of
classifiers, three different binary classification sub-
tasks were considered: positive/not positive, nega-
tive/not negative and neutral/not neutral. All trained
binary classifiers were evaluated by computing pre-
cision and recall for each considered category, as
well as overall classification accuracy, over the eval-
uation dataset.
A feature space model representation of the data
was constructed by considering the standard bag-of-
words approach. In this way, a sparse vector was ob-
tained for each sentence in the datasets. Stop-word
removal was not conducted before computing vec-
tor models, and standard normalization and TF-IDF
weighting schemes were used.
Multiple-fold cross-validation was used in all
conducted experiments to tackle with statistical vari-
119
classifier baseline annotated
positive/not positive 59.63 (3.04) 69.53 (1.70)
negative/not negative 60.09 (2.90) 63.73 (1.60)
neutral/not neutral 51.27 (2.49) 62.57 (2.08)
Table 4: Mean accuracy over 20 independent simula-
tions (with standard deviations provided in parenthesis)
for each classification subtasks trained with either the
baseline or the annotated dataset.
ability of the data. In this sense, twenty independent
realizations were actually conducted for each exper-
iment presented and, instead of individual output re-
sults, mean values and standard deviations of evalu-
ation metrics are reported.
Each binary classifier realization was trained with
a random subsample set of 600 sentences extracted
from the training dataset corresponding to the clas-
sifier group, i.e. baseline dataset for reference sys-
tems, and annotated dataset for contrastive systems.
Training subsample sets were always balanced with
respect to the original three categories: ?positive?,
?negative? and ?neutral?.
Table 4 presents the resulting mean values of
accuracy for each considered subtask in classifiers
trained with either the baseline or the annotated
dataset. As observed in the table, all subtasks ben-
efit from using the annotated dataset for training
the classifiers; however, it is important to mention
that while similar absolute gains are observed for
the ?positive/not positive? and ?neutral/not neutral?
subtasks, this is not the case for the subtask ?neg-
ative/not negative?, which actually gains much less
than the other two subtasks.
After considering all evaluation metrics, the bene-
fit provided by human-annotated data availability for
categories ?neutral? and ?positive? is evident. How-
ever, in the case of category ?negative?, although
some gain is also observed, the benefit of human-
annotated data does not seem to be as much as for
the two other categories. This, along with the fact
that the ?negative/not negative? subtask is actually
the best performing one (in terms of accuracy) when
baseline training data is used, might suggest that
low rating comments contains a better representa-
tion of sentences belonging to category ?negative?
than medium and high rating comments do with re-
spect to classes ?neutral? and ?positive?.
In any case, this experimental work only verifies
the feasibility of constructing training datasets for
opinionated content analysis, as well as it provides
an approximated idea of costs involved in the gener-
ation of this type of resources, by using MTurk.
5.3 Experiment two: MTurk annotations vs.
expert annotations
In this section, we compare the results of training
several polarity classifiers on six different training
sets, each of them generated from the MTurk anno-
tations of HIT1. The different training sets are: (i)
the original dataset of 1000 sentences annotated by
experts (Experts), (ii) the first set of 1000 MTurk re-
sults (Batch1), (iii) the second set of 1000 MTurk
results (Batch2), (iv) the third set of 1000 MTurk
results (Batch3), (v) the batch obtained by major-
ity voting between Batch1, Batch2 and Batch3 (Ma-
jority), and (vi) a batch of 3000 training instances
obtained by aggregating Batch1, Batch2 and Batch3
(All). We used classifiers as implemented in Mal-
let (McCallum, 2002) and Weka (Hall et al, 2009),
based on a simple bag-of-words representation of
the sentences. As the objective was not to obtain
optimum performance but only to evaluate the dif-
ferences between different sets of annotations, all
classifiers were used with their default settings.
Table 5 contains results of four different clas-
sifiers (Maxent, C45, Winnow and SVM), trained
on these six different datasets and evaluated on the
same 500-sentence test set as explained in Section
5.1. Classification using expert annotations usu-
ally outperforms classification using a single batch
(one annotation per sentence) of annotations pro-
duced using MTurk. Using the tree annotations per
sentence available from MTurk, all classifiers reach
similar or better performance compared to the sin-
gle set of expert annotations, at a much lower cost
(as explained in section 4.4).
It is interesting to note that most classifiers bene-
fit from using the full 3000 training examples (1000
sentences with 3 annotations each), which intu-
itively makes sense as the unanimously labeled ex-
amples will have more weight in defining the model
of the corresponding class, whereas ambiguous or
unclear cases will have their impact reduced as their
characteristics are attributed to various classes.
On the contrary, Support Vector Machines show
120
System
E
xp
er
ts
B
at
ch
1
B
at
ch
2
B
at
ch
3
M
aj
or
it
y
A
ll
Winnow 44.2 43.6 40.4 47.6 46.2 50.6
SVM 57.6 53.0 55.4 54.0 57.2 52.8
C45 42.2 33.6 42.0 41.2 41.6 45.0
Maxent 59.2 55.8 57.6 54.0 57.6 58.6
Table 5: Accuracy figures of four different classifiers
(Winnow, SVM, C45 and Maxent) trained on six different
datasets (see text for details).
an important drop in performance when using mul-
tiple annotations, but perform well when using the
majority vote. As a first intuition, this may be due to
the fact that SVMs focus on detecting class bound-
aries (and optimizing the margin between classes)
rather than developing a model of each class. As
such, having the same data point appear several
times with the same label will not aid in finding ap-
propriate support vectors, whereas having the same
data point with conflicting labels may have a nega-
tive impact on the margin maximization.
Having only evaluated each classifier (and train-
ing set) once on a static test set it is unfortunately not
possible to reliably infer the significance of the per-
formance differences (or determine confidence in-
tervals, etc.). For a more in-depth analysis it might
be interesting to use bootstrapping or similar tech-
niques to evaluate the robustness of the results.
6 Conclusions
In this paper we have examined the usefulness of
non-expert annotations on Amazon?s Mechanical
Turk to annotate the polarity of Spanish consumer
comments. We discussed the advantages/drawbacks
of three different HIT designs, ranging from a sim-
ple categorization scheme to a continous scoring
template. We report high inter-annotator agree-
ment scores between non-experts and expert anno-
tators and show that training an Opinion Mining
System with non-expert MTurk annotations outper-
forms original noisy annotations and obtains com-
petitive results when compared to expert annotations
using a variety of classifiers. In conclusion, we
found that, in our case, the use of non-expert anno-
tations through crowdsourcing is a viable and cost-
effective alternative to the use of expert annotations.
In the classification experiments reported in this
paper, we have relied exclusively on MTurk anno-
tations from HIT1. Further work is needed to fully
analyze the impact of each of the HIT designs for
Opinion Mining tasks. We hope that the added rich-
ness of annotation of HIT2 and HIT3 will enable us
to use more sophisticated classification methods.
References
A. Esuli and F. Sebastiani. 2006. SentiWordNet: a pub-
licly available lexical resource for opinion mining. In
Proceedings of LREC, volume 6.
B. D Eugenio and M. Glass. 2004. The kappa statistic: A
second look. Computational linguistics, 30(1):95101.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
A. Kittur, E. H Chi, and B. Suh. 2008. Crowdsourcing
user studies with mechanical turk.
W. Mason and D. J Watts. 2009. Financial incentives
and the performance of crowds. In Proceedings of
the ACM SIGKDD Workshop on Human Computation,
pages 77?85.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y Ng. 2008.
Cheap and fastbut is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263.
Q. Su, D. Pavlov, J. H Chow, and W. C Baker. 2007.
Internet-scale collection of human-reviewed data. In
Proceedings of the 16th international conference on
World Wide Web, pages 231?240.
121
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 98?102,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Using collocation segmentation to augment the phrase table
Carlos A. Henr?quez Q.
?
, Marta R. Costa-juss?
?
, Vidas Daudaravicius?
Rafael E. Banchs
?
, Jos? B. Mari?o
?
?
TALP Research Center, Universitat Polit?cnica de Catalunya, Barcelona, Spain
{carlos.henriquez,jose.marino}@upc.edu
?
Barcelona Media Innovation Center, Barcelona, Spain
{marta.ruiz,rafael.banchs}@barcelonamedia.org
?Faculty of Informatics, Vytautas Magnus University, Kaunas, Lithuania
vidas@donelaitis.vdu.lt
Abstract
This paper describes the 2010 phrase-based
statistical machine translation system de-
veloped at the TALP Research Center of
the UPC
1
in cooperation with BMIC
2
and
VMU
3
. In phrase-based SMT, the phrase
table is the main tool in translation. It is
created extracting phrases from an aligned
parallel corpus and then computing trans-
lation model scores with them. Performing
a collocation segmentation over the source
and target corpus before the alignment
causes that different and larger phrases
are extracted from the same original doc-
uments. We performed this segmentation
and used the union of this phrase set with
the phrase set extracted from the non-
segmented corpus to compute the phrase
table. We present the configurations con-
sidered and also report results obtained
with internal and official test sets.
1 Introduction
The TALP Research Center of the UPC
1
in coop-
eration with BMIC
2
and VMU
3
participated in the
Spanish-to-English WMT task. Our primary sub-
mission was a phrase-based SMT system enhanced
with POS tags and our contrastive submission was
an augmented phrase-based system using colloca-
tion segmentation (Costa-juss? et al, 2010), which
mainly is a way of introducing new phrases in the
translation table. This paper presents the descrip-
tion of both systems together with the results that
we obtained in the evaluation task and is organized
as follows: first, Section 2 and 3 present a brief de-
scription of a phrase-based SMT, followed by a gen-
eral explanation of collocation segmentation. Sec-
tion 4 presents the experimental framework, corpus
used and a description of the different systems built
for the translation task; the section ends showing
the results we obtained over the official test set. Fi-
nally, section 5 presents the conclusions obtained
from the experiments.
1
Universitat Polit?cnica de Catalunya
2
Barcelona Media Innovation Center
3
Vytautas Magnus University
2 Phrase-based SMT
This approach to SMT performs the translation
splitting the source sentence in segments and as-
signing to each segment a bilingual phrase from
a phrase-table. Bilingual phrases are translation
units that contain source words and target words,
e.g. < unidad de traduccio?n | translation unit >,
and have different scores associated to them. These
bilingual phrases are then sorted in order to max-
imize a linear combination of feature functions.
Such strategy is known as the log-linear model
(Och and Ney, 2003) and it is formally defined as:
e? = arg max
e
[
M?
m=1
?mhm (e, f)
]
(1)
where hm are different feature functions with
weights ?m. The two main feature functions
are the translation model (TM) and the target
language model (LM). Additional models include
POS target language models, lexical weights, word
penalty and reordering models among others.
3 Collocation segmentation
Collocation segmentation is the process of de-
tecting boundaries between collocation segments
within a text (Daudaravicius and Marcinkeviciene,
2004). A collocation segment is a piece of text be-
tween boundaries. The boundaries are established
in two steps using two different measures: the Dice
score and a Average Minimum Law (AML).
The Dice score is used to measure the associa-
tion strength between two words. It has been used
before in the collocation compiler XTract (Smadja,
1993) and in the lexicon extraction system Cham-
pollion (Smadja et al, 1996). It is defined as fol-
lows:
Dice (x; y) =
2f (x, y)
f (x) + f (y)
(2)
where f (x, y) is the frequency of co-occurrence of
x and y, and f (x) and f (y) the frequencies of
occurrence of x and y anywhere in the text. It gives
high scores when x and y occur in conjunction.
The first step then establishes a boundary between
98
two adjacent words when the Dice score is lower
than a threshold t = exp (?8). Such a threshold
was established following the results obtained in
(Costa-juss? et al, 2010), where an integration of
this technique and a SMT system was performed
over the Bible corpus.
The second step of the procedure uses the AML.
It defines a boundary between words xi?1 and xi
when:
Dice (xi?2;xi?1) +Dice (xi;xi+1)
2
> Dice (xi?1;xi)
(3)
That is, the boundary is set when the Dice value
between words xi and xi?1 is lower than the aver-
age of preceding and following values.
4 Experimental Framework
All systems were built using Moses (Koehn et al,
2007), a state-of-the-art software for phrase-based
SMT. For preprocessing Spanish, we used Freeling
(Atserias et al, 2006), an open source library of
natural language analyzers. For English, we used
TnT (Brants, 2000) and Moses' tokenizer. The
language models were built using SRILM (Stolcke,
2002).
4.1 Corpus
This year, the translation task provided four dif-
ferent sources to collect corpora for the Spanish-
English pair. Bilingual corpora included version 5
of the Europarl Corpus (Koehn, 2005), the News
Commentary corpus and the United Nations cor-
pus. Additional English corpora was available from
the News corpus. The organizers also allowed the
use of the English Gigaword Third and Fourth Edi-
tion, released by the LDC. As for development
and internal test, the test sets from 2008 and 2009
translation tasks were available.
For our experiments, we selected as training data
the union of the Europarl and the News Commen-
tary. Development was performed with a section
of the 2008 test set and the 2009 test set was se-
lected as internal test. We deleted all empty lines,
removed pairs that were longer than 40 words, ei-
ther in Spanish or English; and also removed pairs
whose ratio between number of words were bigger
than 3.
As a preprocess, all corpora were lower-cased
and tokenized. The Spanish corpus was tokenized
and POS tags were extracted using Freeling, which
split clitics from verbs and also separated words
like del into de el. In order to build a POS tar-
get language model, we also obtained POS tags
from the English corpus using the TnT tagger.
Statistics of the selected corpus can be seen in Ta-
ble 1.
Corpora Spanish English
Training sent 1, 180, 623 1, 180, 623
Running words 26, 454, 280 25, 291, 370
Vocabulary 118, 073 89, 248
Development sent 1, 729 1, 729
Running words 37, 092 34, 774
Vocabulary 7, 025 6, 199
Internal test sent 2, 525 2, 525
Running words 69, 565 65, 595
Vocabulary 10, 539 8, 907
Official test sent 2, 489 -
Running words 66, 714 -
Vocabulary 10, 725 -
Table 1: Statistics for the training, development
and test sets.
Internal test Official test
Adjectives 137 72
Common nouns 369 188
Proper nouns 408 2, 106
Verbs 213 128
Others 119 168
Total 1246 2662
Table 2: Unknown words found in internal and
official test sets
It is important to notice that neither the United
Nations nor the Gigaword corpus were used for
bilingual training. Nevertheless, the English part
from the United Nations and the monolingual
News corpus were used to build the language model
of our systems.
4.1.1 Unknown words
We analyzed the content from the internal and of-
ficial test and realized that they both contained
many words that were not seen in the training data.
Table 2 shows the number of unknown words found
in both sets, classified according to their POS.
In average, we may expect an unknown word
every two sentences in the internal test and more
than one per sentence in the official test set. It can
also be seen that most of those unknown words are
proper nouns, representing 32% and 79% of the
unknown sets, respectively. Common nouns were
the second most frequent type of unknown words,
followed by verbs and adjectives.
4.2 Systems
We submitted two different systems for the trans-
lation task. First a baseline using the training data
mentioned before; and then an augmented system,
where the baseline-extracted phrase list was ex-
tended with additional phrases coming from a seg-
mented version of the training corpus.
We also considered an additional system built
99
with two different decoding path, a standard path
from words to words and POS and an alternative
path from stems to words and POS in the target
side. At the end, we did not submit this system
to the translation task because it did not provide
better results than the previous two in our internal
test.
The set of feature functions used include: source-
to-target and target-to-source relative frequen-
cies, source-to-target and target-to-source lexical
weights, word and phrase penalties, a target lan-
guage model, a POS target language model, and a
lexicalized reordering model (Tillman, 2004).
4.2.1 Considering stems as an alternate
decoding path.
Using Moses' framework for factored translation
models we defined a system with two decoding
paths: one decoding path using words and the
other decoding path using stems in the source lan-
guage and words in the target language. Both de-
coding paths only had a single translation step.
The possibility of using multiple alternative decod-
ing path was developed by Birch et. al. (2007).
This system tried to solve the problem with the
unknown words. Because Spanish is morphologi-
cally richer than English, this alternative decoding
path allowed the decoder translate words that were
not seen in the training data and shared the same
root with other known words.
4.2.2 Expanding the phrase table using
collocation segmentation.
In order to build the augmented phrase table with
the technique mentioned in section 3, we seg-
mented each language of the bilingual corpus in-
dependently and then, using the collocation seg-
ments as words, we aligned the corpus and ex-
tracted the phrases from it. Once the phrases were
extracted, the segments of each phrase were split
again in words to have standard phrases. Finally,
we use the union of this phrases and the phrases
extracted from the baseline system to compute the
final phrase table. A diagram of the whole proce-
dure can be seen in figure 1.
The objective of this integration is to add new
phrases in the translation table and to enhance
the relative frequency of the phrases that were ex-
tracted from both methods.
4.2.3 Language model interpolation.
Because SMT systems are trained with a bilingual
corpus, they ended highly tied to the domain the
corpus belong to. Therefore, when the documents
we want to translate belong to a different domain,
additional domain adaptation techniques are rec-
ommended to build the system. Those techniques
usually employ additional corpora that correspond
to the domain we want to translate from.
internal test
baseline 24.25
baseline+stem 23.45
augmented 23.9
Table 3: Internal test results.
test testcased?detok
baseline 26.1 25.1
augmented 26.1 25.1
Table 4: Results from translation task
The test set for this translation task comes from
the news domain, but most of our bilingual cor-
pora belonged to a political domain, the Europarl.
Therefore we use the additional monolingual cor-
pus to adapt the language model to the news do-
main.
The strategy used followed the experiment per-
formed last year in (R. Fonollosa et al, 2009).
We used SRILM during the whole process. All
language models were order five and used modi-
fied Kneser-Ney discount and interpolation. First,
we build three different language models accord-
ing to their domain: Europarl, United Nations and
news; then, we obtained the perplexity of each lan-
guage model over the News Commentary develop-
ment corpus; next, we used compute-best-mix to
obtain weights for each language model that di-
minish the global perplexity. Finally, the models
were combined using those weights.
In our experiments all systems used the resulting
language model, therefore the difference obtained
in our results were cause only by the translation
model.
4.3 Results
We present results from the three systems devel-
oped this year. First, the baseline, which included
all the features mentioned in section 4.2; then, the
system with an alternative decoding path, called
baseline+stem; and finally the augmented system,
which integrated collocation segmentation to the
baseline. Internal test results can be seen in table
3. Automatic scores provided by the WMT 2010
organizers for the official test can be found in ta-
ble 4. All BLEU scores are case-insensitive and
tokenized except for the official test set which also
contains case-sensitive and non-tokenized score.
We obtained a BLEU score of 26.1 and 25.1 for
our case-insensitive and sensitive outputs, respec-
tively. The highest score was obtained by Uni-
versity of Cambridge, with 30.5 and 29.1 BLEU
points.
100
Figure 1: Example of the expansion of the phrase table using collocation segmentation. New phrases
added by the collocation-based system are marked with a ??.
4.3.1 Comparing systems
Once we obtained the translation outputs from the
baseline and the augmented system, we performed
a manual comparison of them. Even though we
did not find any significant advantages of the aug-
mented system over the baseline, the collocation
segmentation strategy chose a better morphologi-
cal structures in some cases as can be seen in Table
5 (only sentence sub-segments are shown):
5 Conclusion
We presented two different submissions for the
Spanish-English language pair. The language
model for both system was built interpolating two
big out-of-domain language models and one smaller
in-domain language model. The first system was a
baseline with POS target language model; and the
second one an augmented system, that integrates
the baseline with collocation segmentation. Re-
sults over the official test set showed no difference
in BLEU between these two, even though internal
results showed that the baseline obtained a better
score.
We also considered adding an additional decod-
ing path from stems to words in the baseline but
internal tests showed that it did not improve trans-
lation quality either. The high number of unknown
words found in Spanish suggested us that consider-
ing in parallel the simple form of stems could help
us achieve better results. Nevertheless, a deeper
study of the unknown set showed us that most
of those words were proper nouns, which do not
have inflection and therefore cannot benefited from
stems.
Finally, despite that internal test did not showed
an improvement with the augmented system, we
submitted it as a secondary run looking for the
effect these phrases could have over human evalu-
ation.
Acknowledgment
The research leading to these results has received
funding from the European Community's Seventh
Framework Programme (FP7/2007-2013) under
grant agreement number 247762, from the Span-
ish Ministry of Science and Innovation through the
Buceador project (TEC2009-14094-C04-01) and
the Juan de la Cierva fellowship program. The
authors also wants to thank the Barcelona Media
Innovation Centre for its support and permission
to publish this research.
References
Jordi Atserias, Bernardino Casas, Elisabet
Comelles, Meritxell Gonz?lez, Llu?s Padr?, and
Muntsa Padr?. 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP
101
Original: sabiendo que est? recibiendo el premio
Baseline: knowing that it receive the prize
Augmented: knowing that he is receiving the prize
Original: muchos de mis amigos prefieren no separarla.
Baseline: many of my friends prefer not to separate them.
Augmented: many of my friends prefer not to separate it.
Original: Los estadounidenses contar?n con un tel?fono m?vil
Baseline: The Americans have a mobile phone
Augmented: The Americans will have a mobile phone
Original: es plenamente consciente del camino m?s largo que debe emprender
Baseline: is fully aware of the longest journey must undertake
Augmented: is fully aware of the longest journey that need to be taken
Table 5: Comparison between baseline and augmented outputs
library. In Proceedings of the fifth interna-
tional conference on Language Resources and
Evaluation (LREC 2006), ELRA, Genoa, Italy,
May.
Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2007. Ccg supertags in factored statis-
tical machine translation. In StatMT '07: Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 916, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Thorsten Brants. 2000. TnT  a statistical part-
of-speech tagger. In Proceedings of the Sixth
Applied Natural Language Processing (ANLP-
2000), Seattle, WA.
Marta R. Costa-juss?, Vidas Daudaravicius, and
Rafael E. Banchs. 2010. Integration of statisti-
cal collocation segmentations in a phrase-based
statistical machine translation system. In 14th
Annual Conference of the European Association
for Machine Translation.
Vidas Daudaravicius and Ruta Marcinkeviciene.
2004. Gravity counts for the boundaries of col-
locations. International Journal of Corpus Lin-
guistics, 9:321348(28).
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL '07: Proceedings of
the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions, pages
177180, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models. Computational Linguistics, 29:19
51.
Jos? A. R. Fonollosa, Maxim Khalilov, Marta R.
Costa-juss?, Jos? B. Mari?o, Carlos A. Hen-
r?quez Q., Adolfo Hern?ndez H., and Rafael E.
Banchs. 2009. The TALP-UPC phrase-based
translation system for EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 8589, Athens,
Greece, March. Association for Computational
Linguistics.
Frank A. Smadja, Kathleen McKeown, and
Vasileios Hatzivassiloglou. 1996. Translating
collocations for bilingual lexicons: A statistical
approach. Computational Linguistics, 22(1):1
38.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Comput. Linguist., 19(1):143177.
Andreas Stolcke. 2002. SRILM  an extensible
language modeling toolkit. pages 901904.
Christoph Tillman. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In
HLT-NAACL.
102
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 126?134,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
A Semantic Feature for Statistical Machine Translation 
 
 
Rafael E. Banchs Marta R. Costa-juss? 
Institute for Infocomm Research Barcelona Media Innovation Centre 
1 Fusionopolis Way, 21-01, Singapore 138632 Av. Diagonal 177, planta 9, 08018 Barcelona 
rembanchs@i2r.a-star.edu.sg marta.ruiz@barcelonamedia.org 
 
 
 
 
Abstract 
A semantic feature for statistical machine trans-
lation, based on Latent Semantic Indexing, is 
proposed and evaluated. The objective of the 
proposed feature is to account for the degree of 
similarity between a given input sentence and 
each individual sentence in the training dataset. 
This similarity is computed in a reduced vector-
space constructed by means of the Latent Se-
mantic Indexing decomposition. The computed 
similarity values are used as an additional fea-
ture in the log-linear model combination ap-
proach to statistical machine translation. In our 
implementation, the proposed feature is dy-
namically adjusted for each translation unit in 
the translation table according to the current in-
put sentence to be translated. This model aims 
at favoring those translation units that were ex-
tracted from training sentences that are seman-
tically related to the current input sentence 
being translated. Experimental results on a 
Spanish-to-English translation task on the Bible 
corpus demonstrate a significant improvement 
on translation quality with respect to a baseline 
system. 
1 Introduction  
In recent years, the statistical approach to machine 
translation has gained a lot of attention from both 
the scientific and the commercial perspective. This 
has basically been a consequence of the increasing 
availability of bilingual training material as well as 
the increasing storage and processing capabilities 
of current computational systems, which have al-
lowed for the construction of machine translation 
systems with general-public acceptance quality. 
For several reasons, the most prominent statisti-
cal machine translation paradigm currently used is 
the phrase-based approach (Koehn et al, 2003), 
which has been derived from the IBM?s word-
based approach originally proposed in the early 
90?s (Brown et al, 1993). This original approach 
was heavily rooted on the noisy-channel model 
framework, which, in our view, continues to play 
an important role in the fundamental conception of 
current statistical machine translation. 
While one of the major assumptions of the 
noisy-channel model approach is the independence 
between decoding and source language probabili-
ties, there exists strong evidence on the important 
role played by source language structure and con-
text within the task of human translation (Padilla & 
Bajo, 1998). In this sense, the inability of main-
stream statistical machine translation to tackle with 
source-context information in a reliable way has 
been already recognized as a major drawback of 
the statistical approach, whereas the use of source-
context information has been proven to be effec-
tive in the case of example-based machine transla-
tion (Carl & Way, 2003). In this regard, attempts 
for incorporating source-context information into 
the phrase-based machine translation framework 
have been already reported (Carpuat & Wu, 2007; 
Carpuat & Wu, 2008; Haque et al, 2009; Espa?a-
Bonet et al, 2009; Haque et al, 2010; Costa-juss? 
& Banchs, 2010). However, as far as we know, no 
transcendental improvements in performance have 
been achieved or, at least, reported yet. 
In this work, we elaborate deeper on the ideas 
we have recently presented and discussed in Costa-
juss? & Banchs (2010), where we used a similarity 
metric between the source sentence to be translated 
and all the sentences in the training set as an addi-
126
tional feature in the log-linear combination (Och & 
Ney, 2002) of models of a phrase-based translation 
system. Such a feature, which is dynamic in the 
sense that depends on the input sentence to be 
translated, is intended to favor those translation 
units which were extracted from training sentences 
that are similar to the current input sentence over 
those translation units which were extracted from 
different or unrelated sentences. Different from our 
original methodology, where sentence similarities 
were assessed over a term-document matrix repre-
sentation for words and statistical classes of words, 
here we compute sentence similarities in a low-
dimensional vector space constructed by means of 
Latent Semantic Indexing (Landauer et al, 1998). 
The rest of the paper is organized as follows. 
Section 2 presents an overview of some recent ap-
proaches attempting to introduce source-context 
information into the statistical machine translation 
framework. Then, section 3 introduces the meth-
odology that is proposed and evaluated in this 
work, and section 4 focuses on some implementa-
tion issues. Section 5 describes the experimental 
settings and results. Section 6 presents a manual 
evaluation of a selected sample of system transla-
tions and discusses the most relevant findings and 
observations. Finally, section 7 presents the most 
relevant conclusions of this work and provides 
guidelines for further research in this area. 
2 Related Work  
Several attempts for incorporating source-context 
information into the statistical machine translation 
framework have been reported in the literature dur-
ing the last few years. Without attempting to be 
comprehensive, we provide a brief overlook of 
some of the most sounded recent works within this 
area which are relevant to the phrase-based statisti-
cal machine translation approach. For a more com-
prehensive review of the state-of-the-art, the reader 
can refer to Haque et al (2010). 
On the one hand, there are some semantic ap-
proaches. In Carpuat & Wu (2007), for instance, 
word sense disambiguation techniques are intro-
duced into statistical machine translation; and in 
Carpuat & Wu (2008), dynamically-built context-
dependant phrasal translation lexicons are shown 
to be more useful for phrase-based machine trans-
lation than conventional static phrasal translation 
lexicons, which ignore all contextual information. 
On the other hand, there are approaches which 
use machine learning techniques. In Haque et al 
(2009), different syntactic and lexical features are 
proposed for incorporating information about the 
neighbouring words; and in Espa?a-Bonet et al 
(2009), local classifiers are trained, using linguistic 
and context information, to translate a phrase. 
Finally, our recent approach, which is inspired 
on information retrieval techniques for measuring 
the source-context similarity between the input 
sentence to be translated and the original training 
material, was presented in Costa-juss? & Banchs 
(2010). As our present methodology is closely re-
lated to this approach, more details are provided in 
the following section. 
3 Proposed Methodology  
As already mentioned, the methodology proposed 
and evaluated in this work is based on the source-
context similarity approach we presented in Costa-
juss? & Banchs (2010). Different from that work, 
here we introduce the use Latent Semantic Index-
ing (Landauer et al, 1998) to construct a vector-
space model representation of the data collection in 
a reduced-dimensionality space before computing 
source sentence similarities. First, in subsection 
3.1, we review the source-context similarity ap-
proach. Then, in subsection 3.2 we present the ba-
sics of Latent Semantic Indexing.  
3.1 The Source-Context Similarity Approach 
The method we proposed in Costa-juss? & Banchs 
(2010) introduces and extended concept of transla-
tion unit or phrase by defining a tuple of three ele-
ments: phrase-source-side, phrase-target-side, and 
source-context: 
 
TU = {PSS ||| PTS ||| SC} . (1) 
    
In the most simplistic approach, the source-
context element of a given translation unit can be 
approximated by the complete source sentence the 
translation unit was originally extracted from. To 
illustrate this point, consider the following conven-
tional translation unit {vino|||wine} which has been 
extracted from the training sentence sus ojos est?n 
brillantes por el vino y sus dientes blancos por la 
leche (his eyes shall be red with wine and his teeth 
white with milk). According to (1), the extended 
translation unit TU is defined as {vino|||wine|||sus 
127
ojos est?n brillantes por el vino y sus dientes blan-
cos por la leche}. Notice that, from this definition, 
identical source-target phrase pairs that have been 
extracted from different training sentences are re-
garded as different translation units! 
According to this definition, the relatedness of 
contexts between any translation unit and an input 
sentence to be translated can be computed by 
means of some distance or similarity metric over a 
semantic space representation for sentences. This 
idea is implemented in practice by means of the 
following dynamic feature function: 
 
F(TU,IN) = SIM(TU,IN) = SIM(SC,IN) , (2) 
 
where TU refers to a given translation unit, IN re-
fers to the input sentence to be translated, SC refers 
to the source-context component of translation unit 
TU (which in our implementation is the source 
training sentence which the translation unit was 
extracted from), and SIM is a similarity metric over 
a given model space.  
As implied in (2), the source-context feature to 
be implemented consists of a similarity measure-
ment between the input sentence to be translated 
IN and the source-context component SC of the 
available translation units.  
In Costa-juss? & Banchs (2010), we used the 
cosine of the angle between vectors in a term-
sentence matrix representation (Salton et al, 1975) 
for computing the source-context similarity feature 
described in (2). In this work, we use Latent Se-
mantic Indexing (Landauer et al, 1998) for pro-
jecting the term-sentence matrix representation 
into a low-dimensional space and use the cosine of 
the angle between vectors in the resulting reduced 
space for computing the source-context similarity 
feature. With this, we expect to reduce the noise 
resulting from data sparseness problems in the 
original full-dimensional representation. 
To better illustrate the concepts discussed here, 
let us consider the Spanish word vino and the cor-
responding English translations for its two senses: 
wine and came. Both translations can be automati-
cally inferred from training data; and Table 1 illus-
trates the resulting probability values derived for 
both senses of the Spanish word vino from the ac-
tual training dataset used in this work (a detailed 
description of the dataset is given in section 5).  
Notice from the table, how in general the most 
probable sense of vino in our considered dataset is 
wine. This actually happens because the English 
word wine is always related to the Spanish word 
vino, whereas the English word came can refer to 
many different inflections of the same Spanish 
word: vine, viniste, vino, vinimos, vinieron, etc. 
 
phrase ?(f|e) lex(f|e) ?(e|f) lex(e|f) 
{vino|||wine} 0.665198 0.721612 0.273551 0.329431
{vino|||came} 0.253568 0.131398 0.418478 0.446488
 
Table 1: Actual probability values for the two pos-
sible translations of the Spanish word vino. 
 
The idea of the proposed source-context feature 
is to use the contextual similarity between the input 
sentence to be translated and the sentences in the 
training dataset as an additional source of informa-
tion that should be helpful during decoding.  
Consider for instance the following two sen-
tences corresponding to the wine sense of vino:  
 
SC1: No hab?is comido pan ni tomado vino ni licor , para que se-
p?is que yo soy Jehovah vuestro Dios . (Ye have not eaten bread , 
neither have ye drunk wine or strong drink : that ye might know 
that I am the Lord your God .) 
 
SC2: Cuando fue divulgada esta orden , los hijos de Israel dieron 
muchas primicias de grano , vino nuevo , aceite , miel y de todos 
los frutos de la tierra . (And as soon as the commandment came 
abroad , the children of Israel brought in abundance the firstfruits 
of corn , wine , and oil , and honey , and of all the increase of the 
field .) 
 
and the following two sentences corresponding to 
the came sense of vino: 
 
SC3: Al tercer d?a vino Jeroboam con todo el pueblo a Roboam , 
como el rey hab?a hablado diciendo : Volved a m? al tercer d?a . 
(So Jeroboam and all the people came to Rehoboam the third day , 
as the king had appointed , saying , Come to me again the third 
day .) 
 
SC4: Ella vino y ha estado desde la ma?ana hasta ahora . No ha 
vuelto a casa ni por un momento . (She came , and hath continued 
even from the morning until now , that she tarried a little in the 
house .) 
 
As the context for a given word is generally de-
termined by its surrounding words, we should be 
able to infer the correct sense for the word vino in 
a new Spanish sentence by considering its similar-
ity to sentences SC1, SC2, SC3 and SC4. Now, sup-
pose we want to translate the following two input 
sentences into English: 
 
IN1: Hasta que yo venga y os lleve a una tierra como la vuestra , 
tierra de grano y de vino , tierra de pan y de vi?as , tierra de aceite 
de olivo y de miel . (Until I come and take you away to a land like 
your own land , a land of corn and wine , a land of bread and 
vineyards , a land of oil olive and of honey .) 
128
IN2: Cuando amanec?a , la mujer vino y cay? delante de la puerta 
de la casa de aquel hombre donde estaba su se?or , hasta que fue 
de d?a . (Then came the woman in the dawning of the day , and fell 
down at the door of the man 's house where her lord was , till it 
was light .) 
 
We can select the appropriate sense for vino in 
each case by considering the sentence similarity 
between each of these two sentences and ?training? 
sentences SC1, SC2, SC3 and SC4. The actual similar-
ity values are presented in Table 2. 
 
 SC1 SC2 SC3 SC4 
sense {vino|||wine} {vino|||came} 
IN1 0.0636 0.2666 0.0351 0.0310 
IN2 0.0023 0.0513 0.0888 0.0774 
 
Table 2: Actual similarity values between input 
and training sentences containing the word vino. 
 
As seen from the table, the source-context simi-
larity feature is actually giving preference to the 
phrase pair {vino|||wine} in the case of input sen-
tence IN1 and to {vino|||came} in the case of IN2. 
Notice that more than one similarity value is gen-
erally available for each phrase pair. In our pro-
posed implementation, the largest similarity value 
is the one that is retained. More details on how we 
compute these sentence similarities are given in the 
following subsection.   
3.2 Latent Semantic Indexing 
Latent Semantic Indexing (Landauer et al, 1998) 
can be regarded as the text mining equivalent of 
Principal Component Analysis (Pearson, 1901). 
Both methods are based on the singular value de-
composition (SVD) of a matrix (Golub & Kahan, 
1965), according to which a rectangular matrix X 
of dimensions MxN can be factorized as follows: 
 
X = U ? VT , (3) 
 
where U and V are unitary matrices of dimensions 
MxM and NxN, respectively, and ? is a diagonal 
matrix containing the singular values associated to 
the decomposition.  
According to Landauer et al (1998), a low-
dimensional representation of a given document 
vector x can be obtained by means of the SVD de-
composition depicted in (3) as follows: 
 
yT = xT UMxL , (4) 
where y is the L-dimensional document vector cor-
responding to the projection of an M-dimensional 
document vector x, and UMxL is a matrix contain-
ing the L first column vectors of the unitary matrix 
U obtained from (3). 
Finally, the feature F(TU,IN) described in (2) is 
implemented as the internal product between nor-
malized versions of the vector projections obtained 
in (4). In our case, a vector-space model represen-
tation is constructed for sentences, instead of 
documents, and the source-context similarity val-
ues between translation units and input sentences 
are computed accordingly: 
 
F (TU, IN) = (5)  
<scT UMxL / |scTUMxL| , inT UMxL / |inTUMxL|> 
 
While the value of M is given by the vocabulary 
size in the data collection under consideration, se-
veral implementation questions arise regarding the 
most appropriate values for N (amount of sen-
tences to be used for estimating the projection op-
erator U) and L (the dimensionality of the reduced 
space). These and other implementation issues are 
discussed in detail in the following section.  
4 Implementation Issues  
This section discusses some important implemen-
tation issues that have to be dealt with in order to 
implement and evaluate the proposed approach. 
First, in subsection 4.1, the problem of implement-
ing a dynamic feature in a standard phrase-based 
machine translation framework is discussed. Then, 
in subsections 4.2 and 4.3, the problems of deter-
mining the amount of data required for estimating 
the Latent Semantic Indexing projection operator 
and the most appropriate dimensionality size for 
the reduced space representation are discussed.  
4.1 Implementing a Dynamic Feature 
As defined in (2), the value of the proposed source-
context similarity feature depends on each individ-
ual input sentence to be translated by the system. 
This definition implies a major difference between 
this feature and other conventional phrase-based 
translation features: it is a dynamic feature in the 
sense that it cannot be computed in advance before 
the input sentences to be translated are known. 
This on-the-fly requirement, along with the ex-
tended translation unit definition presented in (1), 
129
makes it not possible to directly implement the 
proposed methodology within a standard phrase-
based machine translation framework such as 
MOSES (Koehn et al, 2007). As it is not our in-
tention to develop a customized decoding tool for 
implementing and testing our proposed feature, we 
followed or previous implementation of an off-line 
version of the proposed methodology (Costa-juss? 
& Banchs, 2010), which, although very inefficient 
in the practice, allows us to evaluate the impact of 
the source-context feature on a state-of-the-art 
phrase-based translation system.  
According to this, our practical implementation 
is a follows: 
? Two sentence similarity matrices are com-
puted: one between sentences in the devel-
opment and training sets, and the other 
between sentences in the test and training 
datasets.  
? Each matrix entry mij should contain the 
similarity score between the ith sentence in 
the training set and the jth sentence in the 
development (or test) set. 
? For each sentence s in the test and develop-
ment sets, a phrase list LS of all potential 
phrases that can be used during decoding is 
extracted from the aligned training set. 
? The corresponding source-context similarity 
values are assigned to each phrase in lists LS 
according to values in the corresponding 
similarity matrices. 
? Each phrase list LS is collapsed into a phrase 
table TS by removing repetitions (when re-
moving repeated entries in the list, the larg-
est value of the source-context similarity 
feature is retained). 
? Each phrase table is completed by adding 
standard feature values (which are computed 
in the standard manner).  
? MOSES is used on a sentence-per-sentence 
basis, using a different translation table for 
each development (or test) sentence.  
4.2 Dataset for Latent Semantic Indexing 
Another important implementation issue that re-
quires attention is the computation of the Singular 
Value Decomposition described in (3). Ideally, the 
term-sentence matrix X to be decomposed should 
include all available data, i.e. training, develop-
ment and test sentences; however, in the practice, 
this is not possible because of two reasons. First, 
the sizes of typical datasets and vocabularies used 
in statistical machine translation systems are large 
enough to make Singular Value Decomposition 
unfeasible from a computational point of view 1 . 
Second, in a practical application system, the ?test 
set? is actually unknown during the system con-
struction and training phases. In this way, a realis-
tic implementation should be able to work with 
previously unseen data. 
In order to overcome the problem of applying 
the Singular Value Decomposition described in (3) 
to the full term-sentence matrix of all available 
data, we implemented an approximated procedure. 
In our approximation, we compute the similarity 
matrix between two set of sentences as the average 
of several similarity matrices that are computed 
over reduced space projections estimated with dif-
ferent random samples of the training data sen-
tences. In this way, our source-context similarity 
feature, previously defined in (5), becomes: 
 
F (TU, IN) ? (6)  
    1/K ?k <scTUkMxL/|scTUkMxL| , inTUkMxL/|inTUkMxL|>  
 
where UkMxL refers to a projection operator that has 
been computed by means of the Singular Valued 
Decomposition of a term-sentence matrix Xk con-
structed with a random sample of N sentences. 
Note that a total of K different similarity scores are 
averaged in (6). 
In order to evaluate the variability of the similar-
ity values estimated by this approximation, several 
experiments were conducted for different values of 
N and L, where the variance of the estimates over 
K=10 different realizations were computed. Figure 
1 shows the resulting standard deviations for simi-
larity values estimated for different values of L 
when varying N (upper panel), and for different 
values of N when varying L (lower panel). 
As seen from the figure, the range 500<N<1000 
seems to constitute a good compromise between 
the size of selected random sentence sets and the 
observed variability for similarity value estimates, 
as it provides a significant reduction in the com-
puted standard deviations with respect to N=100, 
and not important improvement is observed when 
                                                          
1 Even in the case of a small dataset such as the one consid-
ered here (see details in section 5) the Singular Value Decom-
position of the full term-sentence matrix can take several 
weeks to be completed in and standard Linux-based server. 
130
N>1000. According to this, we selected N=1000 
for our proposed approximation described in (6). 
 
 
 
Figure 1: Standard deviations (STD) for similarity 
values between development and test datasets (de-
scribed in section 5) estimated for different values 
of L when varying N (upper panel), and for differ-
ent values of N when varying L (lower panel). In 
all cases K=10. 
4.3 Reduced Space Dimensionality 
The third and final implementation issue to be dis-
cussed is the selection of the reduced space dimen-
sionality. It have been reported in the literature that 
dimensionality reduction, by means of Latent Se-
mantic Indexing, into the range between 100 and 
1000 provides good space representations for word 
and sentence association applications (Landauer et 
al., 1998). Although it is reasonable to assume this 
condition to be valid also for the application under 
consideration, we conducted a more detailed ex-
ploratory analysis for selecting the dimensionality 
L to be used in our experiments. 
First, we studied the distributions of context-
similarity values computed according to (6) over 
the available data. Figure 2 shows the average dis-
tributions of similarities between sentences in the 
development and training datasets (see data de-
scription in section 5) at different dimensionality 
values. As can be seen from the figure, a dimen-
sionality value of L=100 exhibits a very nice dis-
tribution of similarity values; however, according 
to the results depicted in Figure 1 (lower panel), 
the variability of estimates for such a low dimen-
sionality is relatively high. On the other hand, no-
tice again from Figure 2, how a much larger 
dimensionality value such as L=5000 already starts 
to exhibit a distribution of similarities that is heav-
ily biased towards the low similarity region. Ac-
cording to this result, and taking also into account 
the results in Figure 1, we finally decided setting 
the dimensionality of the reduced space to L=500. 
 
 
 
Figure 2: Average distributions of similarity values 
between development and training sentences com-
puted at different dimensionality values. For all 
cases presented here N=500 and K=10. 
5 Experimental Work     
This section describes the experimental work con-
ducted to evaluate the incidence of the proposed 
source-context similarity feature on translation 
quality for a state-of-the-art phrase-based statistical 
machine translation. First, subsection 5.1 describes 
the dataset and experimental setting. Then, subsec-
tion 5.2 presents and discusses the results. 
5.1 Experimental Setting 
The proposed methodology is evaluated on the Bi-
ble dataset (Chew et al, 2006) Spanish-to-English 
translation task, using the MOSES framework as 
baseline phrase-based statistical machine transla-
tion system (Koehn et al, 2007). Table 3 presents 
the main statistics of the bilingual corpus used. 
 
dataset lang. sentences tokens vocab av. lenght 
Train Spa 28,887 781,113 28,178 27 
Train Eng 28,887 848,776 13,126 29 
Test Spa 500 13,312 2,879 27 
Test Eng 500 14,562 2,156 29 
Dev Spa 500 13,170 2,862 26 
Dev Eng 500 14,537 2,095 29 
 
Table 3: Main statistics of the bilingual corpus un-
der consideration (number of sentences, tokens, 
vocabulary, and average sentence length) 
 
Regarding the baseline system, we used the de-
fault parameters of MOSES, which include the 
131
grow-final-diagonal alignment symmetrisation, the 
lexicalized reordering, a 5-gram language model 
using Kneser-Ney smoothing, and phrases up to 
length 10, among others. The optimization was 
done using the standard MERT procedure (Och & 
Ney, 2002). 
5.2 Experimental Results 
Table 4 presents the translation BLEU, measured 
over the development and test sets, for three differ-
ent system implementations: the baseline system, a 
second system implementing the source-context 
similarity feature over the full-dimensional vector 
space (FVS), just as we implemented it in Costa-
juss? & Banchs (2010), and a third system imple-
menting the source-context similarity feature based 
on Latent Semantic Indexing (LSI). 
 
 Development Test 
Baseline 39.92 38.92 
Source-context (FVS) 40.61 39.43 
Source-context (LSI) 40.80 39.86 
 
Table 4: BLEU scores over development and test 
datasets corresponding to three system implemen-
tations: baseline, and source-context similarity fea-
ture at full-dimensional vector space (FVS) and by 
means of Latent Semantic Indexing (LSI).   
 
As seen from the table, the system implementing 
the Latent Semantic Indexing based source-context 
similarity feature outperforms the baseline system 
by almost one absolute BLEU point, and the full-
dimensional vector space system by some less than 
a half absolute BLEU point. An analysis of signifi-
cance (Koehn, 2004) showed that the differences 
among the systems are statistically significant.  
A more comprehensive manual analysis of both 
the baseline and source-context LSI system outputs 
was required to better asses the incidence of the 
implemented source-context similarity feature on 
the generated translations. The result of this analy-
sis is presented in the following section. 
6 Manual Evaluation 
This section presents and discusses the results of a 
manual evaluation that was conducted over a sam-
ple set of translations. Previous to the manual 
evaluation, we performed a sentence-based auto-
matic evaluation using BLEU for the 500 sen-
tences in the test dataset. We obtained that our 
proposed approach is better than the baseline sys-
tem in 208 sentences, while the baseline is better 
than our system in 173 sentences and the remain-
ing 119 had the same BLEU scores. 
Some output sentences were randomly selected, 
regardless of which system performed better, for 
conducting a manual inspection. From these sen-
tences, we have extracted some segments that illus-
trate specific cases in which our proposed source-
context feature is actually helping to select a better 
translation unit according to the context of the in-
put sentence being translated. Five of these seg-
ments are presented in Table 5, where the relevant 
fragments within the segments are shown in bold.  
 
Example 1  
source No des sue?o a tus ojos ni dejes dormitar tus p?rpa-
dos . 
reference Give not sleep to thine eyes , nor slumber to thine 
eyelids . 
baseline Not sleep in thy sight , Let neither slumber thy 
eyelids . 
LSI-context
 
Give not sleep to thine eyes neither slumber , Let 
thine eyelids . 
Example 2  
source Entonces ellos se acercaron , echaron mano a Jes?s 
y le prendieron ? 
reference Then came they , and laid hands on Jesus , and took 
him ? 
baseline And they came near , and cast hand to Jesus , and 
took him ? 
LSI-context
 
And they came near , and laid hands on Jesus , and 
took him ? 
Example 3  
source Y al tercer d?a , he aqu? que un hombre vino del 
campamento de Sa?l ? 
reference It came even to pass on the third day , that , behold , a 
man came out of the camp from Saul ? 
baseline And the third day , behold , a man wine of the camp 
of Saul ? 
LSI-context And the third day , behold , there came a man of the 
camp of Saul ? 
Example 4  
source ? sed confortados ; sed de un mismo sentir ? 
reference ? be of good comfort , be of one mind ? 
baseline ? thirst confortados ; thirst of one mind 's sake ? 
LSI-context ? be ye confortados : be ye of one mind 's sake ? 
Example 5  
source ? seg?n sus familias , seg?n sus idiomas , en sus 
territorios y en sus naciones . 
reference ? after their families , after their tongues , in their 
countries , and in their nations . 
baseline ? according to their families , after their tongues , in 
their coasts , and in their nations . 
LSI-context ? after their families , after their tongues , in their 
lands , and in their nations . 
 
Table 5: Sample segments where the LSI-based 
source-context feature has helped to accomplish 
better translation unit selections. 
132
As seen from the table, the LSI-based source-
context system is clearly accomplishing more ap-
propriate unit selections. However, in most of the 
cases this does not imply either a better overall 
translation or a closer match to the available refer-
ence translation. This can explain the relative low 
BLEU gain achieved by the method.  
Similarly, we also extracted some segments that 
illustrate specific cases in which our proposed 
source-context feature fails in helping to select a 
better translation unit. Table 6 presents four of 
these cases. 
 
Example 1  
source ? yo he sido enviado con malas noticias para ti . 
reference ? for I am sent to thee with heavy tidings . 
baseline ? for I have sent with evil tidings unto thee . 
LSI-context ? I am sent with evil tidings unto thee . 
Example 2  
source ? heredad de Jehovah son los hijos ; recompensa es 
el fruto del vientre . 
reference ? children are an heritage of the Lord : and the fruit 
of the womb is his reward . 
baseline ? the inheritance of the Lord , are the children ; 
reward is the fruit of the belly . 
LSI-context ? the inheritance of the Lord are the children , and 
reward is the fruit of the belly . 
Example 3  
source ? y que hab?a enaltecido su reino por amor a su 
pueblo Israel . 
reference ? and that he had exalted his kingdom for his 
people Israel 's sake . 
baseline ? and for his kingdom was lifted up his people 
Israel . 
LSI-context ? and for his kingdom was lifted up unto his 
people Israel . 
Example 4  
source Y suceder? que a causa de la abundancia de leche , 
comer? leche cuajada ? 
reference And it shall come to pass , for the abundance of 
milk that he shall eat butter ? 
baseline And it shall come to pass , that by reason of the 
multitude of milk , shall eat with milk cuajada ? 
LSI-context And it shall come to pass by reason of the multitude 
of milk , and shall eat with milk cuajada ? 
 
Table 6: Sample segments where the LSI-based 
source-context feature has failed to accomplish 
better translation unit selections. 
 
In the latter examples in Table 6, the proposed 
source-context feature is clearly failing to provide 
better lexical selections. In some cases, this seems 
to be due to the lack of enough source-context in-
formation in the input sentence to be translated. 
However, in other cases, it is because the source-
context feature alone is not able to compensate the 
system?s bias towards more frequent translations.   
7 Conclusions and Future Work  
A new semantically-motivated feature for statisti-
cal machine translation based on Latent Semantic 
Indexing has been proposed and evaluated. The 
objective of the proposed feature is to account for 
the degree of similarity between a given input sen-
tence and each individual sentence in the training 
dataset. This similarity is computed in a reduced 
vector-space constructed by means of the Latent 
Semantic Indexing decomposition.  
The computed similarity values are used as an 
additional feature in the log-linear model combina-
tion approach to statistical machine translation. In 
our implementation, the proposed feature is dy-
namically adjusted for each translation unit in the 
translation table according to the current input sen-
tence to be translated. 
Experimental results on a Spanish-to-English 
translation task on the Bible corpus showed sig-
nificant improvements of almost 1 and 0.5 absolute 
BLEU points with respect to a baseline system and 
a similar system evaluating sentence similarity at 
the full-dimensional vector space, respectively. A 
manual evaluation revealed that the proposed fea-
ture is actually helping the translation system to 
perform a better selection of translation units on a 
semantic basis.  
As future work, we intend to evaluate different 
association and distance metrics, as well as to ex-
tend the current notion of source-context from the 
input sentence to be translated to any other kind of 
available information beyond the input sentence 
limits. Similarly, different paradigms of semantic 
space representations, including those statistically 
motivated, will be studied and evaluated.  
Implementation issues are also to be revisited 
for better evaluating the impact of both the amount 
of training data and the dimensionality of the re-
duced space on the method?s performance. Finally, 
an on-line version of the method must be imple-
mented in order to be able to evaluate the proposed 
methodology over larger data collections.  
 
Acknowledgments 
The authors would like to thank the Institute for 
Infocomm Research, as well as Barcelona Media 
Innovation Centre and the Juan de la Cierva fel-
lowship program, for their support and permission 
to publish this work. 
133
References  
Brown, P., Della-Pietra, S., Della-Pietra, V., Mercer, R. 
(1993) The Mathematics of Statistical Machine 
Translation: Computational Linguistics 19(2), 263--
311 
Carl, M., Way, A. (2003) Recent Advances in Example-
Based Machine Translation. Kluwer Academic 
Carpuat, M., Wu, D. (2007) How Phrase Sense Disam-
biguation Outperforms Word Sense Disambiguation 
for Statistical Machine Translation. In: 11th Interna-
tional Conference on Theoretical and Methodological 
Issues in Machine Translation. Skovde 
Carpuat, M., Wu, D. (2008) Evaluation of Context-
Dependent Phrasal Translation Lexicons for Statisti-
cal Machine Translation. In: 6th International Con-
ference on Language Resources and Evaluation 
(LREC). Marrakech 
Chew, P. A., Verzi, S. J., Bauer, T. L., McClain, J. T. 
(2006) Evaluation of the Bible as a Resource for 
Cross-Language Information Retrieval. In: Workshop 
on Multilingual Language Resources and Interopera-
bility, pp. 68--74, Sydney 
Costa-juss?, M. R., Banchs, R.E. (2010) A Vector-
Space Dynamic Feature for Phrase-Based Statistical 
Machine Translation. Journal of Intelligent Informa-
tion Systems 
Espa?a-Bonet, C., Gimenez, J., Marquez, L. (2009) 
Discriminative Phrase-Based Models for Arabic Ma-
chine Translation. ACM Transactions on Asian Lan-
guage Information Processing Journal (Special Issue 
on Arabic Natural Language Processing)  
Golub, G. H., Kahan, W. (1965) Calculating the Singu-
lar Values and Pseudo-Inverse of a Matrix. Journal of 
the Society for Industrial and Applied Mathematics: 
Numerical Analysis 2(2), 205--224  
Haque, R., Naskar, S. K., Ma, Y., Way, A. (2009) Using 
Supertags as Source Language Context in SMT. In: 
13th Annual Conference of the European Association 
for Machine Translation, pp. 234--241. Barcelona  
Haque, R., Naskar, S. K., van den Bosh, A., Way, A. 
(2010) Supertags as Source Language Context in Hi-
erarchical Phrase-Based SMT. In: 9th Conference of 
the Association for Machine Translation in the 
Americas (AMTA) 
Koehn, P., Och, F. J., Marcu, D. (2003) Statistical 
Phrase-Based Translation. In: Human Language 
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing (HLT-
EMNLP), pp. 48--54. Edmonton 
 
Koehn, P. (2004) Statistical Significance Test for Ma-
chine Translation Evaluation. In: Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP) 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., Cowan, B., Shen, W., 
Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, 
A., Herbst, E. (2007) Moses: Open Source Toolkit 
for Statistical Machine Translation. In: 45th Annual 
Metting of the Association for Computational Lin-
guistics, pp. 177--180. Prague 
Landauer, T. K., Laham, D., Foltz, P. (1998) Learning 
Human-Like Knowledge by Singular Value Decom-
position: A Progress Report. In: Conference on Ad-
vances in Neural Information Processing Systems, 
pp. 45--51. Denver 
Landauer, T. K., Foltz, P.W., Laham, D. (1998) Intro-
duction to Latent Semantic Analysis. Discourse 
Processes 25, 259--284  
Och, F. J., Ney, H. (2002) Discriminative Training and 
Maximum Entropy Models for Statistical Machine 
Translation. In: 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 295--302 
Padilla, P., Bajo, T. (1998) Hacia un Modelo de Memo-
ria y Atenci?n en la Interpretaci?n Simult?nea. Quad-
erns: Revista de Traducci? 2, 107--117  
Pearson, K. (1901) On Lines and Planes of Closest Fit 
to Systems of Points in Space. Philosophical Maga-
zine 2(6), 559--572  
Salton, G., Wong, A., Yang, C. S. (1975) A Vector 
Space Model for Automatic Indexing. Communica-
tions of the ACM 18(11), 613--620  
 
 
134
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 452?456,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The BM-I2R Haitian-Cre?ole-to-English translation system description
for the WMT 2011 evaluation campaign
Marta R. Costa-jussa`
Barcelona Media Innovation Center
Av Diagonal, 177, 9th floor
08018 Barcelona
marta.ruiz@barcelonamedia.org
Rafael E. Banchs
Institute for Infocomm Research
1 Fusionopolis Way 21-01
Singapore 138632
rembanchs@i2r.a-star.edu.sg
Abstract
This work describes the Haitian-Cre?ole to En-
glish statistical machine translation system
built by Barcelona Media Innovation Center
(BM) and Institute for Infocomm Research
(I2R) for the 6th Workshop on Statistical Ma-
chine Translation (WMT 2011). Our sys-
tem carefully processes the available data and
uses it in a standard phrase-based system en-
hanced with a source context semantic feature
that helps conducting a better lexical selection
and a feature orthogonalization procedure that
helps making MERT optimization more reli-
able and stable. Our system was ranked first
(among a total of 9 participant systems) by the
conducted human evaluation.
1 Introduction
During years there has been a big effort to produce
natural language processing tools that try to under-
stand well written sentences, but the question is how
well do these tools work to analyze the contents
of SMS. For example, not even syntactic tools like
stemming can bring to common stems words that
have been shortened (like Xmas or Christmas).
This paper describes our participation on the 6th
Workshop on Statistical Machine Translation (WMT
2011). The featured task from the workshop was
to translate Haitian-Cre?ole SMS messages into En-
glish. According to the WMT 2011 organizers, these
text messages (SMS) were sent by people in Haiti in
the aftermath of the January 2010 earthquake. Our
objective in this featured task is to translate from
Haitian-Cre?ole into English either using raw or clean
data.
We propose to build an SMT system which could
be used for both raw and clean data. Our base-
line system is an standard phrase-based SMT sys-
tem built with Moses (Koehn et al, 2007). Starting
from this system we propose to introduce a semantic
feature function based on latent semantic indexing
(Landauer et al, 1998). Additionally, as a total dif-
ferent approximation, we propose to orthogonalize
the standard feature functions of the phrase-based
table using the Gram-Schmidt methodology (Greub,
1975). Then, we experimentally combine both en-
hancements.
The only difference among the raw and clean
SMT system were the training sentences. In order
to translate the clean data, we propose to normalize
the corpus of short messages given very scarce re-
sources. We only count with a small set of parallel
corpus at the level of sentence of chat and standard
language. A nice normalization methodology can
allow to make the task of communication easier. We
propose a statistical normalization technique using
the scarce resources we have based on a combina-
tion of statistical machine translation techniques.
The rest of this paper is organized as follows. Sec-
tion 2 briefly describes the phrase-based SMT sys-
tem which is used as a reference system. Next, sec-
tion 3 describes our approximation to introduce se-
mantics in the baseline system. Section 4 reports our
idea of orthogonalizing the feature functions in the
translation table. Section 5 details the data process-
ing and the data conversion from raw to clean. As
follows, section 6 shows the translation results. Fi-
nally, section 7 reports most relevant conclusions of
this work.
452
2 Phrase-based SMT baseline system
The phrase-based approach to SMT performs the
translation splitting the source sentence in segments
and assigning to each segment a bilingual phrase
from a phrase-table. Bilingual phrases are trans-
lation units that contain source words and target
words, e.g. unite? de traduction ? translation unit,
and have different scores associated to them. These
bilingual phrases are then selected in order to maxi-
mize a linear combination of feature functions. Such
strategy is known as the log-linear model (Och,
2003) and it is formally defined as:
e? = argmax
e
[
M?
m=1
?mhm (e, f)
]
(1)
where hm are different feature functions with
weights ?m. The two main feature functions are
the translation model (TM) and the target lan-
guage model (LM). Additional models include lexi-
cal weights, phrase and word penalty and reordering.
3 Semantic feature function
Source context information is generally disregarded
in phrase-based systems given that all training sen-
tences contribute equally to the final translation.
The main objective in this section is to motivate
the use of a semantic feature function we have re-
cently proposed (Banchs and Costa-jussa`, 2011) for
incorporating source context information into the
phrase-based statistical machine translation frame-
work. Such a feature is based on the use of a sim-
ilarity metric for assessing the degree of similarity
between the sentences to be translated and the sen-
tences in the original training dataset.
The measured similarity is used to favour those
translation units that have been extracted from train-
ing sentences that are similar to the current sen-
tence to be translated and to penalize those trans-
lation units than have been extracted from unrelated
or dissimilar training sentences. In the proposed fea-
ture, sentence similarity is measured by means of the
cosine distance in a reduced dimension vector-space
model, which is constructed by using Latent Seman-
tic Indexing (Landauer et al, 1998), a well know
dimensionality reduction technique that is based on
the singular value decomposition of a matrix (Golub
and Kahan, 1965).
The main motivation of this semantic feature is
the fact that source context information is actually
helpful for disambiguating the sense of a given word
during the translation process. Consider for instance
the Spanish word banco which can be translated into
English as either bank or bench depending on the
specific context it occurs. By comparing a given
input sentence containing the Spanish word banco
with all training sentences from which phrases in-
cluding this word where extracted, we can figure
out which is the most appropriated sense for this
word in the given sentence. This is because for the
sense bank the Spanish word banco will be more
like to co-occur with words such as dinero (money),
cuenta (account), intereses (interest), etc., while for
the sense bench it would be more likely to co-occur
with words such as plaza (square), parque (park),
mesa (table), etc; and the chances are high for such
disambiguating words to appear in one or more of
the training sentences from which bilingual phrases
containing banco has been extracted.
In the particular case of translation tasks where
multi-domain corpora is used for training machine
translation systems, such as the Haitian-Creole-to-
English task considered here, the proposed seman-
tic feature has proven to contribute to a better lexi-
cal selection during the decoding process. However,
in tasks considering mono-domain corpora the se-
mantic feature does not improves translation quality
as the most frequent translation pairs learned by the
system are actually the correct ones.
Another important issue related to the semantic
feature discussed here is that it is a dynamic feature
in the sense that it is computed for each potential
translation unit according to the current input sen-
tence being translated. This makes the implementa-
tion of this semantic feature very expensive from a
computational point of view. At this moment, we do
not have an efficient implementation, which makes it
unfeasible in the practice to apply this methodology
to large training corpora.
As the training corpus available for the Haitian-
Creole-to-English is both small in size and multi-
domain in nature, it constitutes the perfect scenario
for experimenting with the recently proposed source
context semantic feature. For more details about im-
453
plementation and performance of this methodology
in a different translation task, the reader should refer
to (Banchs and Costa-jussa`, 2011).
4 Heuristic enhacement
The phrase-based SMT baseline system contains,
by default, 5 feature functions which are the con-
ditional and posterior probabilities, the direct and
indirect lexical scores and the phrase penalty. Usu-
ally, these feature functions are not statistical inde-
pendent from each other. Based on the analogy be-
tween the statistical and geometrical concepts of in-
dependence and orthogonality, and given that, dur-
ing MERT, the optimization of feature combination
is conducted on log-probability space; we decided
to explore the effect of using a set of orthogonal fea-
tures during MERT optimization.
It is well know in both spectral analysis and vec-
tor space decomposition that orthogonal bases allow
for optimal representations of signals and variables,
as they allow for each individual natural component
to be represented independently of the others. In
linear lattice predictors, for instance, each filter co-
efficient can be optimized independently from the
others while convergence to the optimal solution is
guarantied (Haykin, 1996). In the case of statisti-
cal machine translation, the linear nature of feature
combination in log-probability space suggested us
that transforming the features into a set of orthog-
onal features could make MERT optimization more
robust and efficient.
According to this, we used Gram-Schmidt
(Greub, 1975) to transform all available feature
functions into an orthogonal set of feature func-
tions. This orthogonalization process was con-
ducted directly over the log-probability space, i.e,
given the five vectors representing the feature
functions h1, h2, h3, h4, h5, we used the Gram-
Schmidt algorithm to construct an orthogonal basis
v1, v2, v3, v4, v5. The resulting set of features con-
sisted of 5 vectors that form an orthogonal basis.
This new orthogonal set of features was used for
MERT optimization and decoding.
5 Experimental framework
In this section we report the details of the used data
preprocessing and raw to clean data conversion.
5.1 Data preprocessing
The WMT evaluation provided a high variety of
data. Our preprocessing consisted of the following:
? Lowercase and tokenize all files using the
scripts from Moses.
? In the case of the haitian-Creole side of the
data, replace all stressed vowels by their plain
forms.
? Filter out those sentences which had no words
or more than 120.
Table 1 shows the data statistics of the different
sources before and after this preprocessing. The dif-
ferent sources of the table include: in-domain SMS
data (SMS); medical domain (medical); newswire
domain (newswire); united nations (un); state de-
partment (state depart.); guidelines for approapriate
international disaster donations (guidelines); kren-
gle senetences (krengle) and a glossary includes
wikipedia name entities and haitisurf dictionary.
The sources of this material are specified in the web
page of the workshop.
All data from table 1 was concatenated and used
as training corpus. The English part of this data was
used to build the language model. As development
and test corpus we used the data provided by the
organization. Both development and test contained
900 sentences.
Finally, in the evaluation, we included develop-
ment and tests as part of the training corpus, and
then, we translated the evaluation set.
5.2 Raw to clean data conversion
This featured task contained two subtasks. One was
to translate raw data and the other was to translate
clean data. Therefore, we have to build two sys-
tems. Our raw data system was built using the train-
ing data from table 1. The clean data system was
built using all training data from table 1 except in-
domain SMS data. Particularly, a modified version
of the in-domain SMS data was included in the clean
data system. The modification consisted in cleaning
the original in-domain SMS data using an standard
Moses SMT system. We built an SMT system to
translate from raw data to clean data. This SMT sys-
tem was built with the development, test and evalu-
ation data which in total were 2700 sentences. We
454
Statistics
before after
SMS
sentences 17,192 16,594
words 386.0k 383.0k
medical
sentences 1,619 1,619
words 10.4k 10.4k
newswire
sentences 13,517 13,508
words 326.9k 326.7k
wikipedia
sentences 8,476 8,476
words 113.9k 113.9k
un
sentences 91 91
words 1,906 1,906
state depart.
sentences 56 14
words 450 355
guidelines
sentences 60 9
words 795 206
krengle
sentences 658 655
words 4.2k 4.2k
bible
sentences 30,715 30,677
words 946k 944k
glossary
sentences 49,990 49,980
words 126.4k 126.3k
Table 1: Data Statistics before and after training prepro-
cessing. Number of words are from the English side.
used 2500 sentences as training data and 200 sen-
tences for development to adjust weights. The raw
and clean systems were tuned with their respective
developments and tested on their respective tests.
6 Experimental results
In this section we report the results of the approaches
proposed in previous sections. Table 2 and 3 report
the results on the development and test sets on the
raw and clean subtask, respectively.
First row on both tables report the results of the
baseline system briefly described in section 2. Sec-
ond row and third row on both tables report the per-
formance of the semantic feature function and on the
heuristic approach of orthogonalization (orthofea-
tures) respectively. Finally, the last row on both
tables report the performance of both semantic and
heuristic features when combined.
Results shown in tables 2 and 3 do not show
coherent improvements when introducing the new
System Dev Test
baseline 32.00 31.01
+semanticfeature 32.34 30.68
+orthofeatures 31.63 29.90
+semanticfeature+orthofeatures 32.21 30.34
Table 2: BLEU results for the raw data. Best results in
bold.
System Dev Test
baseline 35.86 33.78
+semanticfeature 35.98 33.90
+orthofeatures 35.57 34.10
+semanticfeature+orthofeatures 36.28 33.53
Table 3: BLEU results for the clean data. Best results in
bold.
methodologies proposed. The clean data seems to
benefit from the semantic features and the orthofea-
tures separately. However, the raw data seems not to
benefit from the orthofeatures and keep the similar
performance to the baseline system when using the
semantic feature. Although, this trend is clear, the
results are not conclusive. Therefore, we decided to
participate in the evaluation with the full system (in-
cluding the semantic features and orthofeatures) in
the clean track and with the system including the se-
mantic feature in the raw track. Actually, we used
those systems that performed best in the develop-
ment set. Additionally, results with the semantic
feature may not be significantly better than the base-
line system, but we have seen it actually heps to im-
prove lexical selection in practice in previous works
(Banchs and Costa-jussa`, 2011).
7 Conclusions
This paper reports the BM-I2R system description in
the Haitian-Cre?ole to English translation task. This
system was ranked first in the WMT 2011 by the
conducted human evaluation. The translation sys-
tem uses a PBSMT system enhanced with two dif-
ferent methodologies. First, we experiment with the
introduction of a semantic feature which is capa-
ble of introducing source context information. Sec-
ond, we propose to transform the five standard fea-
ture functions used in the translation model of the
PBSMT system into five orthogonal feature func-
455
tions using the Gram-Schmidt methodology. Results
show that the first methodology can be used for both
raw and clean data. Whereas the second seems to
only benefit clean data.
Acknowledgments
The research leading to these results has received
funding from the Spanish Ministry of Science and
Innovation through the Juan de la Cierva fellowship
program. The authors would like to thank Barcelona
Media Innovation Center and Institute for Infocomm
Research for their support and permission to publish
this research.
References
R. Banchs and M.R. Costa-jussa`. 2011. A semantic fea-
ture for statistical machine translation. In 5th Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (at ACL HLT 2011), Portland.
G. H. Golub and W. Kahan. 1965. Calculating the sin-
gular values and pseudo-inverse of a matrix. journal of
the society for industrial and applied mathematics. In
Numerical Analysis 2(2), pages 205?224.
W. Greub. 1975. Linear Algebra. Springer.
S. Haykin. 1996. Adaptive Filter Theory. Prentice Hall.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 177?180, Prague, Czech Republic,
June.
T. K. Landauer, D. Laham, and P. Foltz. 1998. Learning
human-like knowledge by singular value decomposi-
tion: A progress report. In Conference on Advances in
Neural Information Processing Systems, pages 45?51,
Denver.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41th Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, July.
456
