Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1113?1120
Manchester, August 2008
A Hybrid Generative/Discriminative Framework to Train a Semantic
Parser from an Un-annotated Corpus
Deyu Zhou and Yulan He
Information Research Centre
The University of Reading
Reading, RG6 6BX, UK
d.zhou@rdg.ac.uk, y.he@rdg.ac.uk
Abstract
We propose a hybrid genera-
tive/discriminative framework for se-
mantic parsing which combines the hidden
vector state (HVS) model and the hidden
Markov support vector machines (HM-
SVMs). The HVS model is an extension of
the basic discrete Markov model in which
context is encoded as a stack-oriented
state vector. The HM-SVMs combine the
advantages of the hidden Markov models
and the support vector machines. By
employing a modified K-means clustering
method, a small set of most representative
sentences can be automatically selected
from an un-annotated corpus. These
sentences together with their abstract an-
notations are used to train an HVS model
which could be subsequently applied on
the whole corpus to generate semantic
parsing results. The most confident
semantic parsing results are selected to
generate a fully-annotated corpus which is
used to train the HM-SVMs. The proposed
framework has been tested on the DARPA
Communicator Data. Experimental results
show that an improvement over the base-
line HVS parser has been observed using
the hybrid framework. When compared
with the HM-SVMs trained from the fully-
annotated corpus, the hybrid framework
gave a comparable performance with only
a small set of lightly annotated sentences.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
Semantic parsing maps the natural language sen-
tences to complete formal meaning representa-
tions. Traditionally, research in the field of se-
mantic parsing can be divided into two categories:
rule-based approaches and statistical approaches.
Based on hand-crafted semantic grammar rules,
rule-based approaches fill slots in semantic frames
using word pattern and semantic tokens (Dowding
et al, 1994; Ward and Issar, 1994). Such rule-
based approaches are typically domain-specific
and often fragile. Statistical approaches are gen-
erally based on stochastic models. They can be
further categorized into three types: generative
approaches, discriminative approaches and a hy-
brid of the two. Generative approaches learn the
joint probability model, P (W,C), of input sen-
tence W and its semantic tag sequence C, com-
pute P (C|W ) using the Bayes rule, and then take
the most probable tag sequence C. The hidden
Morkov model (HMM), being a generative model,
has been predominantly used in statistical seman-
tic parsing. It models sequential dependencies by
treating a semantic parse sequence as a Markov
chain, which leads to an efficient dynamic pro-
gramming formulation for inference and learning.
The hidden vector state (HVS) model (He and
Young, 2005) is a discrete HMM model in which
each HMM state represents the state of a push-
down automaton with a finite stack size. State
transitions are factored into separate stack pop
and push operations constrained to give a tractable
search space. The result is a model which is
complex enough to capture hierarchical structure
but which can be trained automatically from only
lightly annotated data. Discriminative approaches
directly model posterior probability P (C|W ) and
1113
learn mappings from W to C. One representa-
tive example is support vector machines (SVMs)
(Vapnik, 1995). More recently, the hidden Markov
support vector machines (HM-SVMs) (Altun et al,
2003) have been proposed which combine the flex-
ibility of kernel methods with the idea of HMMs to
predict a label sequence given an input sequence.
However, HM-SVMs require full annotated cor-
pora for training which are difficult to obtain in
practical applications. On the other hand, the HVS
model can be easily trained from only lightly an-
notated corpora. It is thus interesting to explore the
feasibility to combine the advantages of the HVS
model and the HM-SVMs.
We propose a hybrid generative/discriminative
framework here where a modified K-means clus-
tering method is first applied to select a small
set of the most representative sentences automat-
ically from an un-annotated corpus. These sen-
tences together with their abstract annotations are
used to train an HVS model which could be sub-
sequently applied on the whole corpus to generate
semantic parsing results. The most confident se-
mantic parsing results are selected to generate a
fully-annotated corpus which is used to train the
HM-SVMs. Experimental results show that an im-
provement over the baseline HVS parser has been
achieved using the hybrid framework. When com-
pared with the HM-SVMs trained from the fully-
annotated corpus, the hybrid framework gave a
comparable performance with only a small set of
lighted annotated sentences.
The rest of this paper is organized as follows.
Section 2 reviews other proposed hybrid gener-
ative/discriminative frameworks in recent years.
Section 3 briefly describes the HVS model and
the HM-SVMs followed by the presentation of the
proposed hybrid framework. In Section 4, exper-
imental setup and results are discussed. Finally,
Section 5 concludes the paper.
2 Related Work
Combination of generative and discriminative
models for data classification has recently attracted
much interests in the machine learning community.
It has been shown theoretically and experimentally
in (Jaakkola and Haussler, 1998; Ng and Jordan,
2002; Bouchard and Triggs, 2004) that the hy-
brid model combines the complementary powers
of the both models. The first extensive study on hy-
brid models were discussed in (Jaakkola and Haus-
sler, 1998) where discriminative features were ex-
tracted using generative models and were later
used in discriminative models. More recently, the
HM-SVMs (Altun et al, 2003) have been proposed
which incorporate kernel methods into HMMs to
predict a label sequence given an input sequence.
There have also been several studies on explor-
ing the hybrid generative/discriminative frame-
works which combine the generative and discrim-
inative models in a pipelined way. One exam-
ple is the hybrid framework proposed in (Abou-
Moustafa et al, 2004) for sequential data classi-
fication. The framework employs HMMs to map
the variable length sequential data into a fixed size
P -dimensional vector that can be classified us-
ing any discriminative model. Experiments were
conducted on the NIST database for handwritten
digits and results showed a better recognition rate
than that of standard HMMs. Another example is
the hybrid generative/discriminative approach pro-
posed in (Holub et al, 2008) for detecting and
classifying object categories in the machine ver-
sion domain. In this approach, ?Fisher Kernels?
were used to retain most of the desirable prop-
erties of generative methods and a discriminative
setting was used to increase the classification per-
formance. Experimental results showed signifi-
cant performance improvement over the generative
counterpart.
3 Methodologies
This section first introduces the hidden vector
state (HVS) model and the hidden Markov sup-
port vector machines (HM-SVMs) followed by
the presentation of the proposed hybrid genera-
tive/discriminative framework.
3.1 Hidden Vector State Model
Given a model and an observed word sequence
W = (w
1
? ? ?w
T
), semantic parsing can be
viewed as a pattern recognition problem and the
most likely semantic representation can be found
through statistical decoding. If assuming that the
hidden data take the form of a semantic parse tree
C then the model should be a push-down automata
which can generate the pair ?W,C? through some
canonical sequence of moves D = (d
1
? ? ? d
T
).
That is,
P (W,C) =
T
?
t=1
P (d
t
|d
t?1
? ? ? d
1
) (1)
1114
For the general case of an unconstrained hierarchi-
cal model, D will consist of three types of proba-
bilistic move:
1. popping semantic category labels off the
stack;
2. pushing one or more non-terminal semantic
category label onto the stack;
3. generating the next word.
When considering a constrained form of au-
tomata where the stack is finite depth and ?W,C?
is built by repeatedly popping 0 to n labels off
the stack, pushing exactly one new label onto the
stack and then generating the next word, it defines
the Hidden Vector State (HVS) model in which
conventional grammar rules are replaced by three
probability tables.
Given a word sequence W , concept vector se-
quence C and a sequence of stack pop operations
N , the joint probability of P (W,C,N) can be de-
composed as
P (W,C,N) =
T
?
t=1
P (n
t
|c
t?1
)P (c
t
[1]|c
t
[2 ? ? ?D
t
])
P (w
t
|c
t
) (2)
where c
t
, the vector state at word position t, is a
vector of D
t
semantic concept labels (tags), i.e.
c
t
= [c
t
[1], c
t
[2], ..c
t
[D
t
]] where c
t
[1] is the preter-
minal concept label and c
t
[D
t
] is the root concept
label, n
t
is the vector stack shift operation at word
position t and take values in the range 0, . . . , D
t?1
and c
t
[1] = c
w
t
is the new preterminal semantic
tag assigned to word w
t
at word position t.
3.2 Hidden Markov Support Vector
Machines
To learn a function that assigns to a sequence of
words W = (w
1
? ? ?w
T
), w
i
? W, i = 1, . . . T a
sequence of semantic tags C = c
1
c
2
. . . c
T
, c
i
?
C, i = 1, . . . T , a common approach is to deter-
mine a discriminant function F : W?C ? R that
assigns a score to every input W ? W := W
?
and
every semantic tag sequence C ? C := C
?
, where
W
?
denotes the Kleene closure of W. In order to
obtain a prediction f(W ) ? C, the function is max-
imized with respect to f(W ) = argmax
C?C
F (W,C).
In particular, the function F (W,C) is assumed
to be linear in some combined feature represen-
tation of W and C in HM-SVMs (Altun et al,
2003), F (W,C) := ?w,?(W,C)?. Given a set
of training data (W
i
, C
i
), i = 1, . . . N , the param-
eters w are adjusted so that the true semantic tag
sequence C
i
scores higher than all other tag se-
quences C ? C
i
:= C\C
i
with a large margin. To
achieve the goal, the following optimization prob-
lem is solved instead.
min
?
i
?R,w?F
Cons
?
i
?
i
+
1
2
?w?
2
(3)
s.t. ?w,?(W,C
i
)? ? ?w,?(W,C)? ? 1? ?
i
,
?i = 1, . . . N and C ? C\C
i
where ?
i
is non-negative slack variables allowing
one to increase the global margin by paying a lo-
cal penalty on some outlying examples, and Cons
dictates the desired trade off between margin size
and outliers. To solve the equation 3, the dual of
the equation is solved instead. The solution w
?
can
be written as
w
?
=
N
?
i=1
?
C?C
?
i
(C)?(W
i
, C), (4)
where ?
i
(C) is the Lagrange multiplier of the con-
straint associated with example i and C
i
.
3.3 A Hybrid Generative/Discriminative
Framework for Semantic Parsing
The framework of combining the HVS model and
the HM-SVMs is illustrated in Figure 1. It consists
of three main stages, Representative Sentences Se-
lection, Fully Annotated Corpus Generation, and
HM-SVM Training and Testing. Each of them is
discussed in details below.
? Representative Sentences Selection. Given an
un-annotated corpus, the modified K-means
clustering algorithm is first employed to se-
lect the most representative sentences for an-
notation. This is to avoid annotating the
whole corpus and hopefully the model trained
from the subset of the original corpus would
still give a similar performance when com-
pared with the model trained from the full
corpus. The modified K-means clustering al-
gorithm is described in Figure 3.3.
Initially, k different sentences are randomly
selected as the initial centroids. Then, each
sentence s
i
in the training data is assigned to
one of the k clusters based on the similarity
measurement which will be discussed later.
1115
HM-SVM Training and Testing
Fully Annotated Corpus Generation
Representative Sentence Selection
Un-annotated 
corpus
Test Data(Sentences)
HVS 
model
Parsing results filtering
HVS trainingHVS parsing
Clustering Annotating
HM-SVM Training
Classification
Results
Sentences and 
their annotations
Fully annotated 
corpus
Sentences and their 
parsing sequences
Representative 
sentences
HM-SVM 
Classifier
Figure 1: The hybrid generative/discriminative framework for semantic parsing.
After that, the centroids of the k clusters are
recalculated. The above process repeats until
there are no further changes in the centroids.
The similarity between two sentences is cal-
culated based on sequence alignment. Sup-
pose a = a
1
a
2
? ? ? a
n
and b = b
1
b
2
? ? ? b
m
are
the two word sequences of length of n and m,
Sim(i, j) is defined as the score of the op-
timal alignment between the initial segment
from a
1
to a
i
of a and the initial segment from
b
1
to b
j
of b, where Sim(i, j) is recursively
calculated as follows:
Sim(i, 0) = 0, i = 1, 2, ...n
Sim(0, j) = 0, j = 1, 2, ...m
Sim(i, j) = max
?
?
?
?
?
?
?
?
?
?
?
0,
Sim(i? 1, j ? 1) + s(a
i
, b
j
),
Sim(i? 1, j) + s(a
i
,
?
?
?
),
Sim(i, j ? 1) + s(
?
?
?
, b
j
)
Here s(a
i
, b
j
) is the score of aligning a
i
with
b
j
and is defined as:
s(a
i
, b
j
) = log
[
p(a
i
, b
j
)
p(a
i
)? p(b
j
)
]
(5)
where, p(a
i
) denotes the occurrence probabil-
ity of the word a
i
and p(a
i
, b
j
) denotes the
probability that a
i
and b
j
appear at the same
position in two aligned sequences.
To ensure that content words containing key
information are weighted more heavily than
the less relevant words such as function
words, a score matrix can then be built and
dynamic programming is used to find the
largest score between two sequences. The
distance between the two sentences is de-
fined as the negation of the similarity between
them.
After generating k clusters, the centroid in
each of the clusters is selected as the represen-
tative sentence for annotation. This results in
an exactly k sentences being selected. There
are however two ways to construct the anno-
tated corpus depending on the neighborhood
threshold value d. When d = 1, the anno-
tated corpus only contains k sentences. When
d < 1, both the centroid and some of its near-
est neighboring sentences in each cluster will
receive the same abstract annotation. Thus,
the annotated corpus will contain more than
k sentences. It has to be noted that in both
cases, only k sentences (centroids) need to be
annotated.
? Fully Annotated Corpus Generation. An
HVS model is trained from the annotated cor-
pus constructed in the previous stage which
is then applied to the original un-annotated
corpus to generate a semantic tag sequence
for each sentence. The generated semantic
tag sequences essentially define the explicit
word/tag alignments which could serve as the
full annotation required by HM-SVMs train-
ing. However, the HVS model does not guar-
antee to parse the sentences with 100% accu-
racy. Based on a user-defined parse probabil-
1116
Input: A set of sentences S = {s
i
, i = 1, . . . , N}, a distance threshold ?, a neighborhood threshold d
Output: A set of representative sentences R = {r
j
, j = 1, . . . ,M}, and a set of the centroids of the
generated clusters Cent
Algorithm:
1. For each s
i
? S, set Flag
i
= 1. Initialize R and Cent to be empty sets.
2. Select sentences from S with Flag equal to 1 , then reconstruct
?
S = {s
j
|Flag
j
= 1, j =
1, . . . ,
?
N},
?
N is the number of sentences with Flag equal to 1 in S.
3. Randomly select k different sentences c
k
from
?
S, the default value of k is 1000. Construct k
clusters C = {c
l
}, l = 1, . . . , k. Set NumOfFlag = ?
?
S?
4. Loop for each sentences s
i
?
?
S
Loop for each cluster c
l
Calculate the distance between s
i
and the centroid of c
l
. Dist
il
= Distance(s
i
, c
l
).
If Dist
il
< ?, then c
l
= c
l
?
s
i
, set Flag
i
= 0, ExitLoop.
EndLoop
If Flag
i
6= 0, then find the cluster l
?
= argmin
l
{Dist
il
, l = 1, . . . , k}, c
l
?
= c
l
?
?
s
i
EndLoop
If ?{s
i
|s
i
?
?
S, F lag
i
= 1}? <NumOfFlag, then set NumOfFlag = ?{s
i
|s
i
?
?
S, F lag
i
= 1}?,
go to Step 4.
Cent = Cent
?
Cent
l
, l = 1, . . . , k, ?c
l
? 6= 0.
5. If NumOfFlag > 0 then Go to step 2.
Else
R = R
?
Cent.
Construct ?Cent? clusters
?
C = {c
l
}, l = 1, . . . , ?Cent?.
Loop for each sentences s
i
? S
Find the cluster l
?
= argmin
l
{Dist
il
, l = 1, . . . , ?Cent?}, c
?
l
= c
?
l
?
s
i
If Dist
il
?
< d, then R = R
?
s
i
EndLoop
EndIf
Figure 2: A modified K-means clustering method.
ity threshold, the most confident parse results
are selected for the construction of the fully
annotated corpus.
? HM-SVMs Training and Testing. Given the
fully annotated corpus generated in the previ-
ous stage, the HM-SVMs can then be trained
which could later be used to derive the seman-
tic tag sequences for the test data.
4 Experiment
Experiments have been conducted on the DARPA
Communicator data (CUData, 2004) which are
available to the public as open source download.
The data contain utterance transcriptions and the
semantic parse results from the rule-based Phoenix
parser
1
. The DARPA Communicator data were
collected in 461 days. From these, 46 days were
randomly selected for use as test set data and the
remainder were used for training. After cleaning
up the data, the training set consist of 12702 utter-
ances while the test set contains 1178 utterances.
1
http://communicator.colorado.edu/phoenix
The abstract annotation used for training and the
reference annotation needed for testing were de-
rived by hand correcting the Phoenix parse results.
For example, for the sentence ?Show me flights
from Boston to New York?, the abstract annota-
tion would be
FLIGHT(FROMLOC(CITY) TOLOC(CITY)).
Such an annotation need only list a set of valid se-
mantic concepts and the dominance relationships
between them without considering the actual real-
ized concept sequence or attempting to identify ex-
plicit word/concept pairs. Thus, it avoids the need
for expensive tree-bank style annotations.
To evaluate the performance of the model, a ref-
erence frame structure was derived for every test
set sentence consisting of slot/value pairs. An ex-
ample of a reference frame is:
Show me flights from Boston to New York.
Frame: FLIGHT
Slots: FROMLOC.CITY = Boston
TOLOC.CITY = New York
Performance was then measured in terms of
F -measure on slot/value pairs, which combines
1117
Table 1: Feature templates used in HM-SVMs. w
i
is the current word, and w
1
, . . . , w
n
is the entire
sentence.
Current word w
i
Previous word w
i?1
Word two back w
i?2
Next word w
i+1
Word two ahead w
i+2
Bigram features w
i?2
, w
i?1
w
i?1
, w
i
w
i
, w
i+1
w
i+1
, w
i+2
Table 2: The number of representative sentences
vs the different settings of ? and d.
H
H
H
H
H
H
d
?
0.5 0.6 0.7 0.8 0.9
1 350 663 1068 1743 2763
0.6 6878 7596 9810 9640 11872
the precision (P) and recall (R) values with equal
weight and is defined as F = (P +R)/2PR.
In all the subsequent experiments, the open
source SVM
hmm
(Tsochantaridis et al, 2005)
2
has been used to train and test the HM-SVMs. The
features used in the HM-SVMs are listed in Ta-
ble 1.
4.1 Comparison between HVS and
HM-SVMs
In the modified K-means clustering algorithm de-
scribed in Figure 3.3, the number of representative
sentences depends on both the distance threshold ?
and the neighborhood threshold d. Table 2 shows
the number of representative sentences obtained by
varying ? and d.
First, a set of experiments were conducted to
compare the performance of the HVS model with
the HM-SVMs only without incorporating the hy-
brid framework. Based on the different values of
d and ?, we constructed different sets of the anno-
tated corpus. For example, when ? = 0.5, there
are 350 clusters generated from a total of 12702
sentences in the un-annotated corpus. The centroid
from each of the cluster is then selected for annota-
tion. These 350 sentences were annotated with ab-
stract annotation for the HVS model training. And
they were also fully annotated by providing word-
level annotations for HM-SVMs training.
2
http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
0.5 0.6 0.7 0.8 0.90.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
?
F?
me
asu
re 
 
 
HM?SVM
HVS (d = 1)
HVS (d = 0.6)
Figure 3: Comparisons of the performance of HVS
and HM-SVMs on different ?.
Since only abstract annotations need to be pro-
vided for the HVS model training, it is possible to
automatically enlarge the annotated corpus by in-
cluding some of the neighboring sentences if the
same annotation of the centroid can be assigned to
them. This is controlled by varying the neighbor-
hood threshold d. If d = 1, then the annotated
corpus only contains 350 sentences with their ab-
stract annotations. If varying d to 0.6, then for each
cluster, some of the neighboring sentences also re-
ceive the same abstract annotation as that of the
centroid, thus the annotated corpus is enlarged to
contain 6878 sentences.
The performance comparison of the HVS and
the HM-SVMs is shown in Figure 3. It can be
observed that in general, HM-SVMs outperforms
HVS. This is not surprising as HM-SVMs was
trained from the fully annotated corpus. The HVS
model based on d = 0.6 achieved better perfor-
mance over the one based on d = 1 since the en-
larged annotated corpus was used for training. The
best performance given by HM-SVMs is 92.5%
of F-measure when ? = 0.9 and 2793 annotated
sentences were used for training, while the HVS
model gave an F-measure of 86.9%.
Though HM-SVMs outperforms HVS by 5.5%,
it should be noted that the time consumed for
preparing the fully annotated corpus for HM-
SVMs is far more than the time spent for abstract
annotation for HVS as shown in Figure 4. When
? = 0.5, annotating 350 sentences with the ex-
plicit word/semantic tag mappings took about 17.5
hours while abstract annotation only took about 3
hours. When ? = 0.9, the time spent on fully an-
notating 2763 sentences is almost six times that of
abstract annotation.
1118
0
20
40
60
80
100
120
140
1 2 3 4 5
alpha
ho
urs

Abstract Annotation Word level Annotation
Figure 4: Comparison of time Consuming in
preparing training data for the HVS model and the
HM-SVMs.
4.2 Comparison between HVS and the
Hybrid Framework with Clustering
Figure 5 shows the performance comparison be-
tween the HVS model and the hybrid framework
by varying ?. It can be observed that when the size
of the annotated corpus is small, the HVS model
outperforms the hybrid framework. However, with
increased number of annotated sentences, the hy-
brid framework achieves the better performance.
For both the HVS model and the hybrid frame-
work, improved performance is observed by train-
ing the model/framework from the augmented an-
notated corpus with the neighboring sentences au-
tomatically added in (cf. Figure 5(a) and (b)).
We notice from Figure 5(a) that when the num-
ber of annotated sentences increases from 1743 to
2763, the performances of both the HVS model
and the hybrid framework decrease. By analyz-
ing the clustering results generated from the mod-
ified K-means algorithm, we found that some of
the clusters formed actually contain those rare sen-
tences and they represent the outliers of the orig-
inal training set. This therefore leads to the de-
creased performance of the HVS model and the
hybrid framework.
With only 2763 annotated sentences, the hybrid
framework trained under d = 0.6 achieves 88.5%
in F-measure which results in a relative error re-
duction of 12% when compared with the HVS
model where the F-measure value of 87.0% was
obtained.
4.3 Comparison between HVS and the
Hybrid Framework without Clustering
Experiments have also been conducted to compare
the performance of the HVS model and the hy-
0.5 0.6 0.7 0.8 0.90.75
0.760.77
0.780.79
0.80.81
0.820.83
0.840.85
0.860.87
0.880.89
0.9
?
F?m
eas
ure
 
 
HVS
Hybrid framework
(a) d = 1.
0.5 0.6 0.7 0.8 0.90.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
?
F?m
eas
ure
 
 
 
HVS
Hybrid framework
(b) d = 0.6.
Figure 5: Comparison of the performance of the
HVS model and the hybrid framework
brid framework without employing the modified
K-means clustering algorithm to automatically se-
lect the most representative sentences for annota-
tion. That is, the whole corpus of 12702 sentences
were provided with abstract annotations. Both
the HVS model and the hybrid framework were
trained from the training set which was formed by
randomly selecting the annotated sentences from
the original corpus. Figure 6 illustrates the perfor-
mance of the HVS model and the hybrid frame-
work versus the varying sizes of the training data.
Here 10-fold cross validation was performed and
the F-measure value at each point of the curve in
Figure 6 was calculated by averaging the perfor-
mance of the 10 experiments each time with dif-
ferent training set of the same size.
It can be observed that the performance of both
the HVS model and the proposed hybrid frame-
work increases with the increased size of the train-
ing data. The hybrid framework outperforms the
HVS model when the size of the training data is
beyond 6000. The improvement is more substan-
tial by incorporating more training data.
The best performance achieved by the HVS
model and the proposed hybrid framework is listed
1119
1 2 3 4 5 6 7 8 9 10 11 120.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
Number of Sentences in the Training data (*1000)
F?m
eas
ure
 
 
HVS
Hybrid framework
Figure 6: Performance of the HVS model and the
hybrid framework vs the size of the training data.
in Table 3. It can be observed that the hybrid
framework gives the relative error reduction of
22% when compared with the performance of the
HVS model where only 87.97% was achieved.
Table 3: Performance comparison between HVS
and the hybrid framework.
Measurement HVS Hybrid Framework
Recall 87.81% 90.99 %
Precision 88.13% 90.25%
F-measure 87.97% 90.62%
It should also be noted that the best performance
of 87.97% in F-measure was obtained when the
HVS model was trained on the whole annotated
corpus of 12702 sentences. By employing the clus-
tering method to select the most representative sen-
tences, the hybrid framework trained with less than
one fourth of the original training data (2763 an-
notated sentences) achieves 88.53% in F-measure,
which is better than that of the HVS model.
5 Conclusions
This paper presented a hybrid framework by com-
bining the HVS model and the HM-SVMs for se-
mantic parsing. Experimental results show that
22% relative error reduction in F-measure was ob-
tained using the hybrid framework on the DARPA
Communicator Data when compared with the per-
formance of the HVS model. Furthermore, em-
ploying the modified K-means clustering algo-
rithm to automatically select the most representa-
tive sentences for annotation greatly reduces the
human effort for annotation. With only 2763
annotated sentences, the hybrid framework gives
the better performance compared with the HVS
model trained on the full 12702 annotated sen-
tences. Also, the hybrid framework gives the com-
parable performance with that of the HM-SVMs
but without the use of the expensive word-level an-
notations.
References
Abou-Moustafa, K.T., C.Y. Suen, and M. Cheriet.
2004. A generative-discriminative hybrid for se-
quential data classification. In Acoustics, Speech,
and Signal Processing, 2004 (ICASSP ?04), vol-
ume 5, pages 805?808.
Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003.
Hidden markov support vector machines. In Inter-
national Conference in Machine Learning, pages 3?
10.
Bouchard, Guillaume and Bill Triggs. 2004. The trade-
off between generative and discriminative classifiers.
In Proc. of COMPSTAT 2004, pages 721?728.
CUData. 2004. Darpa communicator travel data.
university of colorado at boulder. Avaiable from
http://communicator.colorado.edu/phoenix.
Dowding, J., R. Moore, F. Andry, and D. Moran. 1994.
Interleaving syntax and semantics in an efficient
bottom-up parser. In Proc. of the 32th Annual Meet-
ing of the Association for Computational Linguistics,
pages 110?116, Las Cruces, New Mexico, USA.
He, Yulan and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. Computer
Speech and Language, 19(1):85?106.
Holub, Alex D., Max Welling, and Pietro Perona1.
2008. Hybrid generative-discriminative visual cat-
egorization. International Journal of Computer Vi-
sion, 77:239?258.
Jaakkola, T. and D. Haussler. 1998. Exploiting genera-
tive models in discriminative classifiers. In Proc. of
Advances in Neural Information Processing 11.
Ng, A. and M. Jordan. 2002. On generative vs. dis-
criminative classifiers: A comparison of logistic re-
gression and naive bayes. In Proc. of Advances in
Neural Information Processing 15, pages 841?848.
Tsochantaridis, Ioannis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent output
variables. J. Mach. Learn. Res., 6:1453?1484.
Vapnik, Vladimir N. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Ward, W. and S. Issar. 1994. Recent improvements in
the cmu spoken language understanding system. In
Proc. of the workshop on Human Language Technol-
ogy, pages 213?216, Plainsboro, New Jerey, USA.
1120
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 98?99,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting Protein-Protein Interaction based on Discriminative Training of
the Hidden Vector State Model
Deyu Zhou and Yulan He
Informatics Research Centre, The University of Reading, Reading RG6 6BX, UK
Email:d.zhou@reading.ac.uk, y.he@reading.ac.uk
1 Introduction
The knowledge about gene clusters and protein in-
teractions is important for biological researchers
to unveil the mechanism of life. However, large
quantity of the knowledge often hides in the liter-
ature, such as journal articles, reports, books and
so on. Many approaches focusing on extracting in-
formation from unstructured text, such as pattern
matching, shallow and deep parsing, have been pro-
posed especially for extracting protein-protein inter-
actions (Zhou and He, 2008).
A semantic parser based on the Hidden Vector
State (HVS) model for extracting protein-protein in-
teractions is presented in (Zhou et al, 2008). The
HVS model is an extension of the basic discrete
Markov model in which context is encoded as a
stack-oriented state vector. Maximum Likelihood
estimation (MLE) is used to derive the parameters
of the HVS model. In this paper, we propose a dis-
criminative approach based on parse error measure
to train the HVS model. To adjust the HVS model to
achieve minimum parse error rate, the generalized
probabilistic descent (GPD) algorithm (Kuo et al,
2002) is used. Experiments have been conducted on
the GENIA corpus. The results demonstrate mod-
est improvements when the discriminatively trained
HVS model outperforms its MLE trained counter-
part by 2.5% in F-measure on the GENIA corpus.
2 Methodologies
The Hidden Vector State (HVS) model (He and
Young, 2005) is a discrete Hidden Markov Model
(HMM) in which each HMM state represents the
state of a push-down automaton with a finite stack
size.
Normally, MLE is used for generative probabil-
ity model training in which only the correct model
needs to be updated during training. It is be-
lieved that improvement can be achieved by train-
ing the generative model based on a discriminative
optimization criteria (Klein and Manning, 2002) in
which the training procedure is designed to maxi-
mize the conditional probability of the parses given
the sentences in the training corpus. That is, not only
the likelihood for the correct model should be in-
creased but also the likelihood for the incorrect mod-
els should be decreased.Assuming the most likely semantic parse tree
C? = Cj and there are altogether M semantic parse
hypotheses for a particular sentence W , a parse er-
ror measure (Juang et al, 1993; Chou et al, 1993;
Chen and Soong, 1994) can be defined as
d(W ) = ? logP (W,Cj) + log[
1
M ? 1
?
i,i6=j
P (W,Ci)? ]
1
? (1)
where ? is a positive number and is used to se-
lect competing semantic parses. When ? = 1,
the competing semantic parse term is the average
of all the competing semantic parse scores. When
? ? ?, the competing semantic parse term be-
comes max
i.i6=j
P (W,Ci) which is the score for the top
competing semantic parse. By varying the value of
?, we can take all the competing semantic parses into
consideration. d(W ) > 0 implies classification er-
ror and d(W ) ? 0 implies correct decision.
The sigmoid function can be used to normalize
d(W ) in a smooth zero-one range and the loss func-
tion is thus defined as (Juang et al, 1993):
`(W ) = sigmoid(d(W )) (2)
98
where
sigmoid(x) = 1
1 + e??x
(3)
Here, ? is a constant which controls the slope of the
sigmoid function.
The update formula is given by:
?k+1 = ?k ? ?k?`(Wi, ?k) (4)
where ?k is the step size.
Using the definition of `(Wi, ?k) and after work-
ing out the mathematics, we get the update formu-
lae 5, 6, 7,
(
logP (n|c?)
)? = logP (n|c?)? ??`(di)(1? `(di))
?
??I(Cj , n, c?) +
?
i,i6=j
I(Ci, n, c?)
P (Wi, Ci, ?)??
i,i6=j P (Wi, Ci, ?)?
?
? (5)
(logP (c[1]|c[2..D]))? = logP (c[1]|c[2..D])? ??`(di)(1? `(di))
?
??I(Cj , c[1], c[2..D]) +
?
i,i6=j
I(Ci, c[1], c[2..D])
P (Wi, Ci, ?)??
i,i6=j P (Wi, Ci, ?)?
?
?
(6)
(logP (w|c))? = logP (w|c)? ??`(di)(1? `(di))
?
??I(Cj , w, c) +
?
i,i6=j
I(Ci, w, c)
P (Wi, Ci, ?)??
i,i6=j P (Wi, Ci, ?)?
?
? (7)
where I(Ci, n, c?) denotes the number of times
the operation of popping up n semantic tags at
the current vector state c? in the Ci parse tree,
I(Ci, c[1], c[2..D]) denotes the number of times the
operation of pushing the semantic tag c[1] at the cur-
rent vector state c[2..D] in the Ci parse tree and
I(Ci, w, c) denotes the number of times of emitting
the word w at the state c in the parse tree Ci.
3 Experimental Setup and Results
GENIA (Kim et al, 2003) is a collection of 2000 re-
search abstracts selected from the search results of
MEDLINE database using keywords (MESH terms)
?human, blood cells and transcription factors?. All
these abstracts were then split into sentences and
those containing more than two protein names and
at least one interaction keyword were kept. Alto-
gether 3533 sentences were left and 2500 sentences
were sampled to build our data set.
The results using MLE and discriminative train-
ing are listed in Table 1. Discriminative training
improves on the MLE by relatively 2.5% where N
Table 1: Performance comparison of MLE versus Dis-
criminative training
Measurement GENIA
MLE Discriminative
Recall 61.78% 64.59%
Precision 61.16% 61.51%
F-measure 61.47% 63.01%
and I are set to 5 and 200 individually. Here N de-
notes the number of semantic parse hypotheses and
I denotes the the number of sentences in the training
data.
References
J.K. Chen and F.K. Soong. 1994. An n-best candidates-
based discriminative training for speech recognition
applications. IEEE Transactions on Speech and Audio
Processing, 2:206 ? 216.
W. Chou, C.H. Lee, and B.H. Juang. 1993. Minimum
error rate training based on n-best string models. In
Acoustics, Speech, and Signal Processing, IEEE Inter-
national Conference on ICASSP ?93, volume 2, pages
652 ? 655.
Y. He and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and
Language, 19(1):85?106.
B.H. Juang, W. Chou, and C.H. Lee. 1993. Statistical
and discriminative methods for speech recognition. In
Rubio, editor, Speech Recognition and Understanding,
NATO ASI Series, Berlin. Springer-Verlag.
JD. Kim, T. Ohta, Y. Tateisi, and J Tsujii. 2003. GE-
NIA corpus?semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl 1):i180?2.
D. Klein and C. D. Manning. 2002. Conditional struc-
ture versus conditional estimation in nlp models. In
Proc. the ACL-02 conference on Empirical methods in
natural language processing, pages 9?16, University
of Pennsylvania, PA.
H.-K.J. Kuo, E. Fosle-Lussier, H. Jiang, and C.H. Lee.
2002. Discriminative training of language models
for speech recognition. In Acoustics, Speech, and
Signal Processing, IEEE International Conference on
ICASSP ?02, volume 1, pages 325 ? 328.
Deyu Zhou and Yulan He. 2008. Extracting Interac-
tions between Proteins from the Literature. Journal
of Biomedical Informatics, 41:393?407.
Deyu Zhou, Yulan He, and Chee Keong Kwoh. 2008.
Extracting Protein-Protein Interactions from the Liter-
ature using the Hidden Vector State Model. Interna-
tional Journal of Bioinformatics Research and Appli-
cations, 4(1):64?80.
99
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700?705,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Simple Bayesian Modelling Approach to Event Extraction from Twitter
Deyu Zhou
??
Liangyu Chen
?
Yulan He
?
?
School of Computer Science and Engineering, Key Laboratory of Computer Network
and Information Integration, Ministry of Education, Southeast University, China
?
State Key Laboratory for Novel Software Technology, Nanjing University, China
?
School of Engineering and Applied Science, Aston University, UK
d.zhou@seu.edu.cn, cly1cn@126.com, y.he@cantab.net
Abstract
With the proliferation of social media
sites, social streams have proven to con-
tain the most up-to-date information on
current events. Therefore, it is crucial to
extract events from the social streams such
as tweets. However, it is not straight-
forward to adapt the existing event ex-
traction systems since texts in social me-
dia are fragmented and noisy. In this pa-
per we propose a simple and yet effec-
tive Bayesian model, called Latent Event
Model (LEM), to extract structured rep-
resentation of events from social media.
LEM is fully unsupervised and does not
require annotated data for training. We
evaluate LEM on a Twitter corpus. Ex-
perimental results show that the proposed
model achieves 83% in F-measure, and
outperforms the state-of-the-art baseline
by over 7%.
1 Introduction
Event extraction is to automatically identify events
from text with information about what happened,
when, where, to whom, and why. Previous work in
event extraction has focused largely on news ar-
ticles, as the newswire texts have been the best
source of information on current events (Hogen-
boom et al, 2011). Approaches for event ex-
traction include knowledge-based (Piskorski et al,
2007; Tanev et al, 2008), data-driven (Piskorski
et al, 2008) and a combination of the above two
categories (Grishman et al, 2005). Knowledge-
based approaches often rely on linguistic and lexi-
cographic patterns which represent expert domain
knowledge for particular event types. They lack
the flexibility of porting to new domains since ex-
traction patterns often need to be re-defined. Data-
driven approaches require large annotated data to
train statistical models that approximate linguistic
phenomena. Nevertheless, it is expensive to obtain
annotated data in practice.
With the increasing popularity of social media,
social networking sites such as Twitter have be-
come an important source of event information.
As reported in (Petrovic et al, 2013), even 1% of
the public stream of Twitter contains around 95%
of all the events reported in the newswire. Never-
theless, the social stream data such as Twitter data
pose new challenges. Social media messages are
often short and evolve rapidly over time. As such,
it is not possible to know the event types a priori
and hence violates the use of existing event extrac-
tion approaches.
Approaches to event extraction from Twitter
make use of a graphical model to extract canonical
entertainment events from tweets by aggregating
information across multiple messages (Benson et
al., 2011). In (Liu et al, 2012), social events in-
volving two persons are extracted from multiple
similar tweets using a factor graph by harvesting
the redundancy in tweets. Ritter et al (2012) pre-
sented a system called TwiCal which extracts an
open-domain calendar of significant events repre-
sented by a 4-tuple set including a named entity,
event phrase, calendar date, and event type from
Twitter.
In our work here, we notice a very important
property in social media data that the same event
could be referenced by high volume messages.
This property allows us resort to statistical mod-
els that can group similar events based on the co-
occurrence patterns of their event elements. Here,
event elements include named entities such as per-
son, company, organization, date/time, location,
and the relations among them. We can treat an
event as a latent variable and model the genera-
tion of an event as a joint distribution of its indi-
vidual event elements. We thus propose a Latent
Event Model (LEM) which can automatically de-
tect events from social media without the use of
labeled data.
700
Latent 
Event
Model
Post-
processing
Tweets Pre-processing
POS
Tagging
Named Entity 
Recognition Stemming
Temporal Resolution
Extracted Events
Name Entity Time Location Key words
...
Amy
Winehouse
2011/0
7/23 London
Die, 
Death, ..
Space Shuttle 
Atlantis
2011/0
7/08
Kennedy 
Space Center Land ...
Figure 1: The proposed framework for event extraction from tweets.
Our work is similar to TwiCal in the sense that
we also focus on the extraction of structured repre-
sentation of events from Twitter. However, TwiCal
relies on a supervised sequence labeler trained
on tweets annotated with event mentions for the
identification of event-related phrases. We pro-
pose a simple Bayesian modelling approach which
is able to directly extract event-related keywords
from tweets without supervised learning. Also,
TwiCal uses G
2
test to choose an entity y with
the strongest association with a date d to form a
binary tuple ?y, d? to represent an event. On the
contrary, the structured representation of events
can be directly extracted from the output of our
LEM model. We have conducted experiments on
a Twitter corpus and the results show that our pro-
posed approach outperforms TwiCal, the state-of-
the-art open event extraction system, by 7.7% in
F-measure.
2 Methodology
Events extracted in our proposed framework are
represented as a 4-tuple ?y, d, l, k?, where y stands
for a non-location named entity, d for a date, l for a
location, and k for an event-related keyword. Each
event mentioned in tweets can be closely depicted
by this representation. It should be noted that for
some events, one or more elements in their corre-
sponding tuples might be absent as their related in-
formation is not available in tweets. As illustrated
in Figure 1, our proposed framework consists of
three main steps, pre-processing, event extraction
based on the LEM model and post-processing.
The details of our proposed framework are de-
scribed below.
2.1 Pre-processing
Tweets are pre-processed by time expression
recognition, named entity recognition, POS tag-
ging and stemming.
Time Expression Recognition. Twitter users
might represent the same date in various forms.
For example, ?tomorrow?, ?next Monday?, ? Au-
gust 23th? in tweets might all refer to the same
day, depending on the date that users wrote the
tweets. To resolve the ambiguity of the time ex-
pressions, SUTime
1
(Chang and Manning, 2012)
is employed, which takes text and a reference date
as input and outputs a more accurate date which
the time expression refers to.
Named Entity Recognition. Named entity
recognition (NER) is a crucial step since the
results would directly impact the final extracted
4-tuple ?y, d, l, k?. It is not easy to accurately
identify named entities in the Twitter data since
tweets contain a lot of misspellings and abbrevi-
ations. However, it is often observed that events
mentioned in tweets are also reported in news
articles in the same period (Petrovic et al, 2013).
Therefore, named entities mentioned in tweets are
likely to appear in news articles as well. We thus
perform named entity recognition in the following
way. First, a traditional NER tool such as the
Stanford Named Entity Recognizer
2
is used to
identify named entities from the news articles
crawled from BBC and CNN during the same
period that the tweets were published. The recog-
nised named entities from news are then used to
build a dictionary. Named entities from tweets
are extracted by looking up the dictionary through
fuzzy matching. We have also used a named
entity tagger trained specifically on the Twitter
data
3
(Ritter et al, 2011) to directly extract named
entities from tweets. However, as will be shown
in Section 3 that using our constructed dictionary
for named entity extraction gives better results.
We distinguish between location entities, denoted
as l, and non-location entities such as person or
organization, denoted as y.
1
http://nlp.stanford.edu/software/
sutime.shtml
2
http://nlp.stanford.edu/software/
CRF-NER.shtml
3
http://github.com/aritter/twitter-nlp
701
Finally, we use a POS tagger
4
trained on
tweets (Gimpel et al, 2011) to perform POS tag-
ging on the tweets data and apart from the pre-
viously recognised named entities, only words
tagged with nouns, verbs or adjectives are kept.
These remaining words are subsequently stemmed
and words occurred less than 3 times are filtered.
After the pre-processing step, non-location enti-
ties y, locations l, dates d and candidate keywords
of the tweets are collected as the input to the LEM
model for event extraction.
2.2 Event Extraction using the Latent Event
Model (LEM)
We propose an unsupervised latent variable model,
called the Latent Event Model (LEM), to extract
events from tweets. The graphical model of LEM
is shown in Figure 2.
MN
y
?
?
d l k
e
E? ? ? ?
? ? ? ?
Figure 2: Laten Event Model (LEM).
In this model, we assume that each tweet mes-
sage m ? {1..M} is assigned to one event in-
stance e, while e is modeled as a joint distribution
over the named entities y, the date/time d when
the event occurred, the location l where the event
occurred and the event-related keywords k. This
assumption essentially encourages events that in-
volve the same named entities, occur at the same
time and in the same location and have similar
keyword to be assigned with the same event.
The generative process of LEM is shown below.
? Draw the event distribution pi
e
?
Dirichlet(?)
? For each event e ? {1..E}, draw multino-
mial distributions ?
e
? Dirichlet(?),?
e
?
Dirichlet(?),?
e
? Dirichlet(?),?
e
?
Dirichlet(?).
4
http://www.ark.cs.cmu.edu/TweetNLP
? For each tweet w
? Choose an event e ? Multinomial(pi),
? For each named entity occur in tweet
w, choose a named entity y ?
Multinomial(?
e
),
? For each date occur in tweet w, choose
a date d ? Multinomial(?
e
),
? For each location occur in tweet w,
choose a location l ? Multinomial(?
e
),
? For other words in tweet w, choose a
word k ? Multinomial(?
e
).
We use Collapsed Gibbs Sampling (Griffiths
and Steyvers, 2004) to infer the parameters of the
model and the latent class assignments for events,
given observed data D and the total likelihood.
Gibbs sampling allows us repeatedly sample from
a Markov chain whose stationary distribution is
the posterior of e
m
from the distribution over that
variable given the current values of all other vari-
ables and the data. Such samples can be used to
empirically estimate the target distribution. Let-
ting the subscript ?m denote a quantity that ex-
cludes data from mth tweet , the conditional pos-
terior for e
m
is:
P (e
m
= t|e
?m
,y,d, l,z,?) ?
n
?m
t
+ ?
M + E?
?
Y?
y=1
?
n
(m)
t,y
b=1
(n
t,y
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+ Y ?)
?
D?
d=1
?
n
(m)
t,d
b=1
(n
t,d
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+D?)
?
L?
l=1
?
n
(m)
t,l
b=1
(n
t,l
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+ L?)
?
V?
k=1
?
n
(m)
t,k
b=1
(n
t,k
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+ V ?)
where n
t
is the number of tweets that have been
assigned to the event t; M is the total number of
tweets, n
t,y
is the number of times named entity y
has been associated with event t; n
t,d
is the num-
ber of times dates d has been associated with event
t; n
t,l
is the number of times locations l has been
assigned with event t; n
t,k
is the number of times
keyword k has associated with event t, counts with
(m) notation denote the counts relating to tweet
m only. Y,D,L, V are the total numbers of dis-
tinct named entities, dates, locations, and words
appeared in the whole Twitter corpus respectively.
E is the total number of events which needs to be
set.
Once the class assignments for all events are
known, we can easily estimate the model param-
eters {pi,?,?,?,?}. We set the hyperparame-
ters ? = ? = ? = ? = ? = 0.5 and run Gibbs
702
sampler for 10,000 iterations and stop the iteration
once the log-likelihood of the training data con-
verges under the learned model. Finally we select
an entity, a date, a location, and the top 2 keywords
of the highest probability of every event to form a
4-tuple as the representation of that event.
2.3 Post-processing
To improve the precision of event extraction, we
remove the least confident event element from the
4-tuples using the following rule. If P (element)
is less than
1
?
P (S), where P (S) is the sum of
probabilities of the other three elements and ? is a
threshold value and is set to 5 empirically, the ele-
ment will be removed from the extracted results.
3 Experiments
In this section, we first describe the Twitter corpus
used in our experiments and then present how we
build a baseline based on the previously proposed
TwiCal system (Ritter et al, 2012), the state-of-
the-art open event extraction system on tweets. Fi-
nally, we present our experimental results.
3.1 Dataset
We use the First Story Detection (FSD)
dataset (Petrovic et al, 2013) in our experi-
ment. It consists of 2,499 tweets which are
manually annotated with the corresponding event
instances resulting in a total of 27 events. The
tweets were published between 7th July and 12th
September 2011. These events cover a range of
categories, from celebrity news to accidents, and
from natural disasters to science discoveries. It
should be noted here that some event elements
such as location is not always available in the
tweets. Automatically inferring geolocation of the
tweets is a challenging task and will be considered
in our future work. For the tweets without time
expressions, we used the tweets? publication dates
as a default. The number of tweets for each event
ranges from 2 to around 1000. We believe that in
reality, events which are mentioned in very few
tweets are less likely to be significant. Therefore,
the dataset was filtered by removing the events
which are mentioned in less than 10 tweets. This
results in a final dataset containing 2468 tweets
annotated with 21 events.
3.2 Baseline construction
The baseline we chose is TwiCal (Ritter et al,
2012). The events extracted in the baseline are
represented as a 3-tuple ?y, d, k?
5
, where y stands
for a non-location named entity, d for a date and
k for an event phrase. We re-implemented the
system and evaluate the performance of the base-
line on the correctness of the exacted three ele-
ments excluding the location element. In the base-
line approach, the tuple ?y, d, k? are extracted in
the following ways. Firstly, a named entity rec-
ognizer (Ritter et al, 2011) is employed to iden-
tify named entities. The TempEx (Mani and Wil-
son, 2000) is used to resolve temporal expressions.
For each date, the baseline approach chose the en-
tity y with the strongest association with the date
and form the binary tuple ?y, d? to represent an
event. An event phrase extractor trained on an-
notated tweets is required to extract event-related
phrases. Due to the difficulties of re-implementing
the sequence labeler without knowing the actual
features set and the annotated training data, we as-
sume all the event-related phrases are identified
correctly and simply use the event trigger words
annotated in the FSD corpus as k to form the event
3-tuples. It is worth noting that the F-measure re-
ported for the event phrase extraction is only 64%
in the baseline approach (Ritter et al, 2012).
3.3 Evaluation Metric
To evaluate the performance of the propose ap-
proach, we use precison, recall, and F ?
measure as in general information extraction sys-
tems (Makhoul et al, 1999). For the 4-tuple
?y, d, l, k?, the precision is calculated based on the
following criteria:
1. Do the entity y, location l and date d that we
have extracted refer to the same event?
2. Are the keywords k in accord with the event
that other extracted elements y, l, d refer to
and are they informative enough to tell us
what happened?
If the extracted representation does not contain
keywords, its precision is calculated by check-
ing the criteria 1. If the extracted representation
contains keywords, its precision is calculated by
checking both criteria 1 and 2.
3.4 Experimental Results
The number of events, E, in the LEM model
is set to 25. The performance of the proposed
5
TwiCal also groups event instances into event types such
as ?Sport? or ?Politics? using LinkLDA which is not consid-
ered here.
703
Method Tuple Evaluated Precision Recall F-measure
Baseline ?y, d, k? 75% 76.19% 75.59%
Proposed ?y, d, l? 96% 80.95% 87.83%
Proposed ?y, d, l, k? 92% 76.19% 83.35%
Table 1: Comparison of the performance of event
extraction on the FSD dataset.
Method Tuple Evaluated Precision Recall F-measure
TW-NER ?y, d, l? 88% 76.19% 80.35%
TW-NER ?y, d, l, k? 84% 76.19% 79.90%
NW-NER ?y, d, l? 96% 80.95% 87.83%
NW-NER ?y, d, l, k? 92% 76.19% 83.35%
Table 2: Comparison of the performance of event
extraction using different NER method.
framework is presented in Table 1. The base-
line re-implemented here can only output 3-tuples
?y, d, k? and we simply use the gold standard event
trigger words to assign to k. Still, we observe
that compared to the baseline approach, the per-
formance of our proposed framework evaluated on
the 4-tuple achieves nearly 17% improvement on
precision. The overall improvement on F-measure
is around 7.76%.
3.5 Impact of Named Entity Recognition
We experimented with two approaches for named
entity recognition (NER) in preprocessing. One
is to use the NER tool trained specifically on the
Twitter data (Ritter et al, 2011), denoted as ?TW-
NER? in Table 2. The other uses the traditional
Stanford NER to extract named entities from news
articles published in the same period and then
perform fuzzy matching to identify named enti-
ties from tweets. The latter method is denoted
as ?NW-NER? in Table 2. It can be observed
from Table 2 that by using NW-NER, the per-
formance of event extraction system is improved
significantly by 7.5% and 3% respectively on F-
measure when evaluated on 3-tuples (without key-
words) or 4-tuples (with keywords).
3.6 Impact of the Number of Events E
We need to set the number of events E in the
LEM model. Figure 3 shows the performance of
event extraction versus different value of E. It can
be observed that the performance of the proposed
framework improves with the increase of the value
ofE until it reaches 25, which is close to the actual
number of events in our data. If further increasing
E, we notice more balanced precision/recall val-
ues and a relatively stable F-measure. This shows
that our LEM model is less sensitive to the num-
ber of events E so long as E is set to a relatively
larger value.
10 15 20 25 30 35 40
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
p
e
r
f
o
r
m
a
n
c
e
E
 Precision
 Recall
 F-meature
Figure 3: The performance of the proposed frame-
work with different number of events E.
4 Conclusions and Future Work
In this paper we have proposed an unsupervised
Bayesian model, called the Latent Event Model
(LEM), to extract the structured representation of
events from social media data. Instead of em-
ploying labeled corpora for training, the proposed
model only requires the identification of named
entities, locations and time expressions. After that,
the model can automatically extract events which
involving a named entity at certain time, location,
and with event-related keywords based on the co-
occurrence patterns of the event elements. Our
proposed model has been evaluated on the FSD
corpus. Experimental results show our proposed
framework outperforms the state-of-the-art base-
line by over 7% in F-measure. In future work,
we plan to investigate inferring geolocations au-
tomatically from tweets. We also intend to study
a better method to infer date more accurately from
tweets and explore efficient ranking strategies to
rank evens extracted for a better presentation of
results.
Acknowledgments
This work was funded by the National Natural
Science Foundation of China (61103077), Ph.D.
Programs Foundation of Ministry of Education
of China for Young Faculties (20100092120031),
Scientific Research Foundation for the Returned
Overseas Chinese Scholars, State Education Min-
istry, the Fundamental Research Funds for the
Central Universities, and the UK?s EPSRC grant
EP/L010690/1.
704
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
389?398, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
Sutime: A library for recognizing and normaliz-
ing time expressions. In 8th International Confer-
ence on Language Resources and Evaluation (LREC
2012).
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the Na-
tional Academy of Sciences 101 (Suppl. 1), page
5228C5235.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description.
In ACE 05 Evaluation Workshop.
Frederik Hogenboom, Flavius Frasincar, Uzay Kay-
mak, and Franciska de Jong. 2011. An overview of
event extraction from text. In Workshop on Detec-
tion, Representation, and Exploitation of Events in
the Semantic Web (DeRiVE 2011) at Tenth Interna-
tional Semantic Web Conference (ISWC2011), pages
48?57.
Xiaohua Liu, Xiangyang Zhou, Zhongyang Fu, Furu
Wei, and Ming Zhou. 2012. Exacting social events
for tweets using a factor graph. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial In-
telligence, pages 1692?1698.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proceedings of DARPA
Broadcast News Workshop.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 69?76, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sasa Petrovic, Miles Osborne, Richard McCreadie,
Craig Macdonald, Iadh Ounis, and Luke Shrimpton.
2013. Can twitter replace newswire for breaking
news? In Proceedings of ICWSM?13.
J. Piskorski, H. Tanev, and P. Oezden Wennerberg.
2007. Extracting violent events from on-line news
for ontology population. In Business Information
Systems, pages 287?300.
J. Piskorski, H. Tanev, M. Atkinson, and E. Van
Der Goot. 2008. Cluster-centric approach to news
event extraction. In International Conference on
New Trends in Multimedia and Network Information
Systems, pages 276?290.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, KDD ?12, pages 1104?1112, New
York, NY, USA. ACM.
H. Tanev, J. Piskorski, and M. Atkinson. 2008. Real-
time news event extraction for global crisis monitor-
ing. In 13th International Conference on Applica-
tions of Natural Language to Information Systems
(NLDB), pages 207?218.
705
Exploring English Lexicon Knowledge for Chinese Sentiment Analysis
Yulan He Harith Alani
Knowledge Media Institute
The Open University
Milton Keynes MK6 6AA, UK
{y.he, h.alani}@open.ac.uk
Deyu Zhou
School of Computer Science and Engineering
Southeast University
Nanjing, China
d.zhou@seu.edu.cn
Abstract
This paper presents a weakly-supervised
method for Chinese sentiment analysis
by incorporating lexical prior knowledge
obtained from English sentiment lexi-
cons through machine translation. A
mechanism is introduced to incorpo-
rate the prior information about polarity-
bearing words obtained from existing
sentiment lexicons into latent Dirichlet
allocation (LDA) where sentiment labels
are considered as topics. Experiments
on Chinese product reviews on mobile
phones, digital cameras, MP3 players,
and monitors demonstrate the feasibil-
ity and effectiveness of the proposed ap-
proach and show that the weakly su-
pervised LDA model performs as well
as supervised classifiers such as Naive
Bayes and Support vector Machines with
an average of 83% accuracy achieved
over a total of 5484 review documents.
Moreover, the LDA model is able to
extract highly domain-salient polarity
words from text.
1 Introduction
Sentiment analysis aims to understand subjec-
tive information such as opinions, attitudes, and
feelings expressed in text. It has become a hot
topic in recent years because of the explosion in
availability of people?s attitudes and opinions ex-
pressed in social media including blogs, discus-
sion forums, tweets, etc. Research in sentiment
analysis has mainly focused on the English lan-
guage. There have been few studies in sentiment
analysis in other languages due to the lack of re-
sources, such as subjectivity lexicons consisting
of a list of words marked with their respective
polarity (positive, negative or neutral) and manu-
ally labeled subjectivity corpora with documents
labeled with their polarity.
Pilot studies on cross-lingual sentiment anal-
ysis utilize machine translation to perform senti-
ment analysis on the English translation of for-
eign language text (Banea et al, 2008; Bautin
et al, 2008; Wan, 2009). The major problem
is that they cannot be generalized well when
there is a domain mismatch between the source
and target languages. There have also been in-
creasing interests in exploiting bootstrapping-
style approaches for weakly-supervised senti-
ment classification in languages other than En-
glish (Zagibalov and Carroll, 2008b; Zagibalov
and Carroll, 2008a; Qiu et al, 2009). Other
approaches use ensemble techniques by either
combining lexicon-based and corpus-based algo-
rithms (Tan et al, 2008) or combining sentiment
classification outputs from different experimen-
tal settings (Wan, 2008). Nevertheless, all these
approaches are either complex or require careful
tuning of domain and data specific parameters.
This paper proposes a weakly-supervised ap-
proach for Chinese sentiment classification by
incorporating language-specific lexical knowl-
edge obtained from available English senti-
ment lexicons through machine translation. Un-
like other cross-lingual sentiment classification
methods which often require labeled corpora for
training and therefore hinder their applicability
for cross-domain sentiment analysis, the pro-
posed approach does not require labeled docu-
ments. Moreover, as opposed to existing weakly-
supervised sentiment classification approaches
which are rather complex, slow, and require care-
ful parameter tuning, the proposed approach is
simple and computationally efficient; rendering
more suitable for online and real-time sentiment
classification from the Web.
Our experimental results on the Chinese re-
views of four different product types show that
the LDA model performs as well as the super-
vised classifiers such as Naive Bayes and Sup-
port Vector Machines trained from labeled cor-
pora. Although this paper primarily studies sen-
timent analysis in Chinese, the proposed ap-
proach is applicable to any other language so
long as a machine translation engine is available
between the selected language and English.
The remainder of the paper is organized as
follows. Related work on cross-lingual senti-
ment classification and weakly-supervised sen-
timent classification in languages other than En-
glish are discussed in Section 2. The proposed
mechanism of incorporating prior word polarity
knowledge into the LDA model is introduced in
Section 3. The experimental setup and results of
sentiment classification on the Chinese reviews
of four different products are presented in Sec-
tion 4 and 5 respectively. Finally, Section 6 con-
cludes the paper.
2 Related Work
Pilot studies on cross-lingual sentiment analysis
rely on English corpora for subjectivity classifi-
cation in other languages. For example, Mihal-
cea et al (2007) make use of a bilingual lexicon
and a manually translated parallel text to gener-
ate the resources to build subjectivity classifiers
based on Support Vector Machines (SVMs) and
Naive Bayes (NB) in a new language; Banea et
al. (2008) use machine translation to produce a
corpus in a new language and train SVMs and
NB for subjectivity classification in the new lan-
guage. Bautin et al (2008) also utilize machine
translation to perform sentiment analysis on the
English translation of a foreign language text.
More recently, Wan (2009) proposed a co-
training approach to tackle the problem of cross-
lingual sentiment classification by leveraging an
available English corpus for Chinese sentiment
classification. Similar to the approach proposed
in (Banea et al, 2008), Wan?s method also uses
machine translation to produced a labeled Chi-
nese review corpus from the available labeled
English review data. However, in order to allevi-
ate the language gap problem that the underlying
distributions between the source and target lan-
guage are different, Wan builds two SVM classi-
fiers, one based on English features and the other
based on Chinese features, and uses a bootstrap-
ping method based on co-training to iteratively
improve classifiers until convergence.
The major problem of the aforementioned
cross-lingual sentiment analysis algorithms is
that they all utilize supervised learning to train
sentiment classifiers from annotated English cor-
pora (or the translated target language corpora
generated by machine translation). As such, they
cannot be generalized well when there is a do-
main mismatch between the source and target
language. For example, For example, the word
?compact? might express positive polarity when
used to describe a digital camera, but it could
have negative orientation if it is used to describe
a hotel room. Thus, classifiers trained on one
domain often fail to produce satisfactory results
when shifting to another domain.
Recent efforts have also been made for
weakly-supervised sentiment classification in
Chinese. Zagibalov and Carroll (2008b) starts
with a one-word sentiment seed vocabulary and
use iterative retraining to gradually enlarge the
seed vocabulary by adding more sentiment-
bearing lexical items based on their relative fre-
quency in both the positive and negative parts
of the current training data. Sentiment direction
of a document is then determined by the sum
of sentiment scores of all the sentiment-bearing
lexical items found in the document. The prob-
lem with this approach is that there is no princi-
pal way to set the optimal number of iterations.
They then suggested an iteration control method
in (Zagibalov and Carroll, 2008a) where itera-
tive training stops when there is no change to the
classification of any document over the previous
two iterations. However, this does not necessar-
ily correlate to the best classification accuracy.
Similar to (Zagibalov and Carroll, 2008b),
Qiu et al (2009) also uses a lexicon-based iter-
ative process as the first phase to iteratively en-
large an initial sentiment dictionary. But instead
of using a one-word seed dictionary as in (Za-
gibalov and Carroll, 2008b), they started with a
much larger HowNet Chinese sentiment dictio-
nary1 as the initial lexicon. Documents classified
by the first phase are taken as the training set to
train the SVMs which are subsequently used to
revise the results produced by the first phase.
Other researchers investigated ensemble tech-
niques for weakly-supervised sentiment classifi-
cation. Tan et al (2008) proposed a combination
of lexicon-based and corpus-based approaches
that first labels some examples from a give do-
main using a sentiment lexicon and then trains
a supervised classifier based on the labeled ones
from the first stage. Wan (2008) combined sen-
timent scores calculated from Chinese product
reviews using the Chinese HowNet sentiment
dictionary and from the English translation of
Chinese reviews using the English MPQA sub-
jectivity lexicon2. Various weighting strategies
were explored to combine sentiment classifica-
tion outputs from different experimental settings
in order to improve classification accuracy.
Nevertheless, all these weakly-supervised
sentiment classification approaches are rather
complex and require either iterative training or
careful tuning of domain and data specific pa-
rameters, and hence unsuitable for online and
real-time sentiment analysis in practical applica-
tions.
3 Incorporating Prior Word Polarity
Knowledge into LDA
Unlike existing approaches, we view sentiment
classification as a generative problem that when
an author writes a review document, he/she first
decides on the overall sentiment or polarity (pos-
itive, negative, or neutral) of a document, then
for each sentiment, decides on the words to be
used. We use LDA to model a mixture of only
three topics or sentiment labels, i.e. positive,
negative and neutral.
Assuming that we have a total number of S
sentiment labels; a corpus with a collection of D
1http://www.keenage.com/download/
sentiment.rar
2http://www.cs.pitt.edu/mpqa/
documents is denoted by C = {d1, d2, ..., dD};
each document in the corpus is a sequence of Nd
words denoted by d = (w1, w2, ..., wNd), and
each word in the document is an item from a vo-
cabulary index with V distinct terms denoted by
{1, 2, ..., V }. The generative process is as fol-
lows:
? Choose distributions ? ? Dir(?).
? For each document d ? [1, D], choose dis-
tributions pid ? Dir(?).
? For each of the Nd word posi-
tion wt, choose a sentiment label
lt ? Multinomial(pid), and then choose a
word wt ?Multinomial(?lt).
The joint probability of words and sentiment
label assignment in LDA can be factored into
two terms:
P (w, l) = P (w|l)P (l|d). (1)
Letting the superscript ?t denote a quantity that
excludes data from the tth position, the condi-
tional posterior for lt by marginalizing out the
random variables ? and pi is
P (lt = k|w, l?t, ?,?) ?
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
, (2)
where Nwt,k is the number of times word wt has
associated with sentiment label k; Nk is the the
number of times words in the corpus assigned to
sentiment label k; Nk,d is the number of times
sentiment label k has been assigned to some
word tokens in document d; Nd is the total num-
ber of words in the document collection.
Each words in documents can either bear pos-
itive polarity (lt = 1), or negative polarity (lt =
2), or is neutral (lt = 0). We now show how
to incorporate polarized words in sentiment lex-
icons as prior information in the Gibbs sampling
process. Let
Qt,k =
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
(3)
We can then modify the Gibbs sampling equa-
tion as follows:
P (lt = k|w, l?t, ?,?) ?
{
1I(k = S(wt))?Qt,k if S(wt) is defined
Qt,k otherwise
(4)
where the function S(wt) returns the prior senti-
ment label of wt in a sentiment lexicon and it is
defined if word wt is found in the sentiment lex-
icon. 1I(k = S(wt)) is an indicator function that
takes on value 1 if k = S(wt) and 0 otherwise.
Equation 4 in fact applies a hard constraint
that when a word is found in a sentiment lexi-
con, its sampled sentiment label is restricted to
be the same as its prior sentiment label defined
in the lexicon. This constraint can be relaxed by
introducing a parameter to control the strength of
the constraint such that when wordwt is found in
the sentiment lexicon, Equation 4 becomes
P (lt = k|w, l?t, ?,?) ?
(1? ?)?Qt,k + ?? 1I(k = S(wt))?Qt,k
(5)
where 0 ? ? ? 1. When ? = 1, the hard con-
straint will be applied; when ? = 0, Equation 5
is reduced to the original unconstrained Gibbs
sampling as defined in Equation 2.
While sentiment prior information is incor-
porated by modifying conditional probabilities
used in Gibbs sampling here, it is also possible to
explore other mechanisms to define expectation
or posterior constraints, for example, using the
generalized expectation criteria (McCallum et
al., 2007) to express preferences on expectations
of sentiment labels of those lexicon words. We
leave the exploitation of other mechanisms of in-
corporating prior knowledge into model training
as future work.
The document sentiment is classified based on
P (l|d), the probability of sentiment label given
document, which can be directly obtained from
the document-sentiment distribution. We de-
fine that a document d is classified as positive
if P (lpos|d) > P (lneg|d), and vice versa.
Table 2: Data statistics of the four Chinese prod-
uct reviews corpora.
No. of Reviews Vocab
Corpus positive Negative Size
Mobile 1159 1158 8945
DigiCam 853 852 5716
MP3 390 389 4324
Monitor 341 342 4712
4 Experimental Setup
We conducted experiments on the four corpora3
which were derived from product reviews har-
vested from the website IT1684 with each cor-
responding to different types of product reviews
including mobile phones, digital cameras, MP3
players, and monitors. All the reviews were
tagged by their authors as either positive or neg-
ative overall. The statistics of the four corpora
are shown in Table 2.
We explored three widely used English sen-
timent lexicons in our experiments, namely the
MPQA subjectivity lexicon, the appraisal lexi-
con5, and the SentiWordNet6 (Esuli and Sebas-
tiani, 2006). For all these lexicons, we only ex-
tracted words bearing positive or negative polar-
ities and discarded words bearing neutral polar-
ity. For SentiWordNet, as it consists of words
marked with positive and negative orientation
scores ranging from 0 to 1, we extracted a subset
of 8,780 opinionated words, by selecting those
whose orientation strength is above a threshold
of 0.6.
We used Google translator toolkit7 to translate
these three English lexicons into Chinese. After
translation, duplicate entries, words that failed to
translate, and words with contradictory polarities
were removed. For comparison, we also tested a
Chinese sentiment lexicon, NTU Sentiment Dic-
tionary (NTUSD)8 (Ku and Chen, 2007) which
3http://www.informatics.sussex.ac.uk/
users/tz21/dataZH.tar.gz
4http://product.it168.com
5http://lingcog.iit.edu/arc/
appraisal_lexicon_2007b.tar.gz
6http://sentiwordnet.isti.cnr.it/
7http://translate.google.com
8http://nlg18.csie.ntu.edu.tw:
Table 1: Matched polarity words statistics (positive/negative).
Lexicon
Chinese English
Mobile DigiCam MP3 Monitors Mobile DigiCam MP3 Monitors
(a)MPQA 261/253 183/174 162/135 169/147 293/331 220/241 201/153 210/174
(b)Appraisal 279/165 206/127 180/104 198/105 392/271 330/206 304/153 324/157
(c)SentiWN 304/365 222/276 202/213 222/236 394/497 306/397 276/310 313/331
(d)NTUSD 338/319 263/242 239/167 277/241 ?
(a)+(c) 425/465 307/337 274/268 296/289 516/607 400/468 356/345 396/381
(a)+(b)+(c) 495/481 364/353 312/280 344/302 624/634 496/482 447/356 494/389
(a)+(c)+(d) 586/608 429/452 382/336 421/410 ?
was automatically generated by enlarging an ini-
tial manually created seed vocabulary by con-
sulting two thesauri, tong2yi4ci2ci2lin2 and the
Academia Sinica Bilingual Ontological Word-
Net 3.
Chinese word segmentation was performed on
the four corpora using the conditional random
fields based Chinese Word Segmenter9. The to-
tal numbers of matched polarity words in each
corpus using different lexicon are shown in Ta-
ble 1 with the left half showing the statistics
against the Chinese lexicons (the original En-
glish lexicons have been translated into Chinese)
and the right half listing the statistics against the
English lexicons. We did not translate the Chi-
nese lexicon NTUSD into English since we fo-
cused on Chinese sentiment classification here.
It can be easily seen from the table that in gen-
eral the matched positive words outnumbered the
matched negative words using any single lexi-
con except SentiWordNet. But the combination
of the lexicons results in more matched polarity
words and thus gives more balanced number of
positive and negative words. We also observed
the increasing number of the matched polarity
words on the translated English corpora com-
pared to their original Chinese corpora. How-
ever, as will be discussed in Section 5.2 that the
increasing number of the matched polarity words
does not necessarily lead to the improvement of
the sentiment classification accuracy.
We modified GibbsLDA++ package10 for the
model implementation and only used hard con-
8080/opinion/pub1.html
9http://nlp.stanford.edu/software/
stanford-chinese-segmenter-2008-05-21.
tar.gz
10http://gibbslda.sourceforge.net/
straints as defined in Equation 4 in our experi-
ments. The word prior polarity information was
also utilized during the initialization stage that
if a word can be found in a sentiment lexicon,
the word token is assigned with its correspond-
ing sentiment label. Otherwise, a sentiment label
is randomly sampled for the word. Symmetric
Dirichlet prior ? was used for sentiment-word
distribution and was set to 0.01, while asym-
metric Dirichlet prior ? was used for document-
sentiment distribution and was set to 0.01 for
positive and neutral sentiment labels, and 0.05
for negative sentiment label.
5 Experimental Results
This section presents the experimental results
obtained under two different settings: LDA
model with translated English lexicons tested on
the original Chinese product review corpora; and
LDA model with original English lexicons tested
on the translated product review corpora.
5.1 Results with Different Sentiment
Lexicons
Table 3 gives the classification accuracy results
using the LDA model with prior sentiment la-
bel information provided by different sentiment
lexicons. Since we did not use any labeled in-
formation, the accuracies were averaged over 5
runs and on the whole corpora. For comparison
purposes, we have also implemented a baseline
model which simply assigns a score +1 and -1
to any matched positive and negative word re-
spectively based on a sentiment lexicon. A re-
view document is then classified as either posi-
tive or negative according to the aggregated sen-
timent scores. The baseline results were shown
in brackets in Table 3 .
Table 3: Sentiment classification accuracy (%) by LDA, numbers in brackets are baseline results.
Lexicon Mobile DigiCam MP3 Monitors Average
(a)MPQA 82.00 (63.53) 80.93 (67.59) 78.31 (68.42) 81.41 (64.86) 80.66 (66.10)
(b)Appraisal 71.95 (56.28) 80.46 (60.54) 77.28 (61.36) 80.67 (57.98) 77.59 (59.04)
(c)SentiWN 81.10 (62.45) 78.52 (57.13) 79.08 (64.57) 75.55 (55.34) 78.56 (59.87)
(d)NTUSD 82.61 (71.21) 78.70 (68.23) 78.69 (75.87) 84.63 (74.96) 81.16 (72.57)
(a)+(c) 81.18 (65.95) 78.70 (65.18) 83.83 (67.52) 80.53 (62.08) 81.06 (65.18)
(a)+(b)+(c) 81.48 (62.84) 80.22 (65.88) 80.23 (65.60) 78.62 (61.35) 80.14 (63.92)
(a)+(c)+(d) 82.48 (69.96) 84.33 (69.58) 83.70 (71.12) 82.72 (65.59) 83.31 (69.06)
Naive Bayes 86.52 82.27 82.64 86.21 84.41
SVMs 84.49 82.04 79.43 83.87 82.46
It can be observed from Table 3 that the
LDA model performs significantly better than
the baseline model. The improvement ranges be-
tween 9% and 19% and this roughly corresponds
to how much the model learned from the data.
We can thus speculate that LDA is indeed able to
learn the sentiment-word distributions from data.
Translated English sentiment lexicons per-
form comparably with the Chinese sentiment
lexicon NTUSD. As for the individual lexicon,
using MPQA subjectivity lexicon gives the best
result among all the English lexicons on all the
corpora except the MP3 corpus where MPQA
performs slightly worse than SentiWordNet. The
combination of MPQA and SentiWordNet per-
forms significantly better than other lexicons on
the MP3 corpus, with almost 5% improvement
compared to the second best result. We also
notice that the combination of all the three En-
glish lexicons does not lead to the improvement
of classification accuracy which implies that the
quality of a sentiment lexicon is indeed impor-
tant to sentiment classification. The above re-
sults suggest that in the absence of any Chinese
sentiment lexicon, MPQA subjectivity lexicon
appears to be the best candidate to be used to
provide sentiment prior information to the LDA
model for Chinese sentiment classification.
We also conducted experiments by includ-
ing the Chinese sentiment lexicon NTUSD and
found that the combination of MPQA, Senti-
WordNet, and NTUSD gives the best overall
classification accuracy with 83.31% achieved.
For comparison purposes, we list the 10-fold
cross validation results obtained using the super-
vised classifiers, Naive Bayes and SVMs, trained
on the labeled corpora as previously reported in
(Zagibalov and Carroll, 2008a). It can be ob-
served that using only English lexicons (the com-
bination of MPQA and SentiWordNet), we ob-
tain better results than both NB and SVMs on
the MP3 corpus. With an additional inclusion
of NTUSD, LDA outperforms NB and SVMs
on both DigiCam and MP3. Furthermore, LDA
gives a better overall accuracy when compared
to SVMs. Thus, we may conclude that the un-
supervised LDA model performs as well as the
supervised classifiers such as NB and SVMs on
the Chinese product review corpora.
5.2 Results with Translated Corpora
We ran a second set of experiments on the trans-
lated Chinese product review corpora using the
original English sentiment lexicons. Both the
translated corpora and the sentiment lexicons
have gone through stopword removal and stem-
ming in order to reduce the vocabulary size and
thereby alleviate data sparseness problem. It can
be observed from Figure 1 that in general senti-
ment classification on the original Chinese cor-
pora using the translated English sentiment lex-
icons gives better results than classifying on the
translated review corpora using the original En-
glish lexicons on both the Mobile and Digicam
corpora. However, reversed results are observed
on the Monitor corpus that classifying on the
translated review corpus using the English sen-
timent lexicons outperforms classifying on the
85
Mobi
le
8085
y?(%)
Mobi
le
70758085
Accurac
Mobi
le
6570758085
()M
PQA
(b)A
il
()S
iWN
()(
)
()(b
)()
Mobi
le
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
seCo
rpora
Englis
hCor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
se?Co
rpora
Englis
h?Cor
pora
85
DigiC
am
8085
y?(%)
DigiC
am
70758085
Accurac
DigiC
am
6570758085
(a)M
PQA
(b)Ap
praisa
l(c)
SentiW
N
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
85
MP3
8085
y?(%)
MP3
70758085
Accurac
MP3
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
85
Moni
tor
8085
y?(%)
Moni
tor
70758085
Accurac
Moni
tor
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
Figure 1: Comparison of the performance on the Chinese corpora and their translated corpora in
English.
original Chinese review corpus using the trans-
lated sentiment lexicons. In particular, the com-
bination of the MPQA subjectivity lexicon and
SentiWordNet gives the best result of 84% on
the Monitor corpus. As for the MP3 corpus,
classifying on the original Chinese reviews or on
the translated reviews does not differ much ex-
cept that a better result is obtained on the Chi-
nese corpus when using the combination of the
MPQA subjectivity lexicon and SentiWordNet.
The above results can be partially explained by
the ambiguities and changes of meanings intro-
duced in the translation. The Mobile and Digi-
Cam corpora are relatively larger than the MP3
and Monitors corpora and we therefore expect
more ambiguities being introduced which might
result in the change of document polarities.
5.3 Extracted Polarity-Bearing Words
LDA is able to extract polarity-bearing words.
Table 4 lists some of the polarity words identi-
fied by the LDA model which are not found in
the original sentiment lexicons. We can see that
LDA is indeed able to recognize domain-specific
positive or negative words, for example, ?Y
(bluetooth) for mobile phones, ? (compact)
for digital cameras,?^ (metallic) for MP3,?
s (flat screen) and?b (deformation) for mon-
itors.
The iterative approach proposed in (Zagibalov
and Carroll, 2008a) can also automatically ac-
quire polarity words from data. However, it ap-
pears that only positive words were identified
by their approach. Our proposed LDA model
can extract both positive and negative words and
most of them are highly domain-salient as can be
seen from Table 4.
6 Conclusions
This paper has proposed a mechanism to incor-
porate prior information about polarity words
from English sentiment lexicons into LDA
model learning for weakly-supervised Chinese
sentiment classification. Experimental results of
sentiment classification on Chinese product re-
views show that in the absence of a language-
specific sentiment lexicon, the translated En-
glish lexicons can still produce satisfactory re-
sults with the sentiment classification accuracy
of 81% achieved averaging over four different
types of product reviews. With the incorpora-
tion of the Chinese sentiment lexicon NTUSD,
the classification accuracy is further improved to
83%. Compared to the existing approaches to
cross-lingual sentiment classification which ei-
ther rely on labeled corpora for classifier learn-
ing or iterative training for performance gains,
the proposed approach is simple and readily to
Table 4: Extracted example polarity words by LDA.
Corpus Positive Negative
Mobile ? (advantage), ' (large), }( (easy to
use),? (fast), (comfortable),?Y (blue-
tooth),? (new),? (easy)
O (bad), ? (poor), b (slow), ? (no;not), ?
(difficult;hard), (less),?/ (but),? (repair)
DigiCam  ? (advantage),  ? (compact), :
(strong;strength), & (telephoto), ? (dy-
namic), h (comprehensive), Semantic Parsing for Biomedical Event Extraction
Deyu Zhou1 and Yulan He2
1School of Computer Science and Engineering, Southeast University,China
2Knowledge Media Institute, The Open University, UK
Abstract
We propose a biomedical event extraction system, HVS-BioEvent, which employs the hidden
vector state (HVS) model for semantic parsing. Biomedical events extraction needs to deal with
complex events consisting of embedded or hierarchical relations among proteins, events, and their
textual triggers. In HVS-BioEvent, we further propose novel machine learning approaches for event
trigger word identification, and for biomedical events extraction from the HVS parse results. Our
proposed system achieves an F-score of 49.57% on the corpus used in the BioNLP?09 shared task,
which is only two points lower than the best performing system by UTurku. Nevertheless, HVS-
BioEvent outperforms UTurku on the extraction of complex event types. The results suggest that the
HVS model with the hierarchical hidden state structure is indeed more suitable for complex event
extraction since it can naturally model embedded structural context in sentences.
1 Introduction
In the past few years, there has been a surge of interests in utilizing text mining techniques to pro-
vide in-depth bio-related information services. With an increasing number of publications reporting on
protein-protein interactions (PPIs), much effort has been made in extracting information from biomedical
articles using natural language processing (NLP) techniques. Several shared tasks, such as LLL [7] and
BioCreative [4], have been arranged for the BioNLP community to compare different methodologies for
biomedical information extraction.
Comparing to LLL and BioCreative which primarily focus on a simple representation of relations of
bio-molecules, i.e. protein-protein interaction, the BioNLP?09 Shared Task [5] involves the recognition
of bio-molecular events in scientific abstracts, such as gene expression, transcription, protein catabolism,
localization and binding, plus (positive or negative) regulation of proteins. The task concerns the detailed
behavior of bio-molecules, and can be used to support the development of biomedical-related databases.
In the BioNLP?09 shared task evaluation, the system constructed by UTurku [2] achieved an F-score of
51.95% on the core task, the best results among all the participants.
In this paper, we describe a system, called HVS-BioEvent, which employs the hidden vector state
model (HVS) to automatically extract biomedical events from biomedical literature. The HVS model has
been successfully employed to extract PPIs [9]. However, it is not straightforward to extend the usage
of the HVS model for biomedical events extraction. There are two main challenges. First, comparing
to the trigger words used for PPIs which are often expressed as single words or at most two words, the
trigger words for biomedical event are more complex. For example, controlled at transcriptional and
post-transcriptional levels, spanning over 6 words, is considered as the trigger word for the regulation
event. In addition, the same word can be the trigger word for different types of biomedical events in
different context. Second, biomedical events consist of both simple events and complex events. While
simple events are more similar to PPIs which only involve binary or pairwise relations, complex events
involve both n-ary (n > 2) and nested relations. For example, a regulation event may take another
event as its theme or cause which represents a structurally more complex relation. Being able to handle
both simple and complex events thus poses a huge challenge to the development of our HVS-BioEvent
system.
The rest of the paper is organized as follows. Section 2 presents the overall process of the HVS-
BioEvent system, which consists of three steps, trigger words identification, semantic parsing based on
395
the HVS model, and biomedical events extraction from the HVS parse results. Experimental results are
discussed in section 3. Finally, section 4 concludes the paper.
2 Biomedical Event Extraction
We perform biomedical event extraction with the following steps. At the beginning, abstracts are re-
trieved from MEDLINE and split into sentences. Protein names, gene names, trigger words for biomed-
ical events are then identified. After that, each sentence is parsed by the HVS semantic parser. Finally,
biomedical events are extracted from the HVS parse results using a hybrid method based on rules and
machine learning. All these steps process one sentence at a time. Since 95% of all annotated events
are fully annotated within a single sentence, this does not incur a large performance penalty but greatly
reduces the size and complexity of the problem. The remainder of the section will discuss each of the
steps in details.
2.1 Event Trigger Words Identification
Event trigger words are crucial to biomedical events extraction. In our system, we employ two ap-
proaches for event trigger words identification, one is a hybrid approach using both rules and a dictio-
nary, the other treats trigger words identification as a sequence labeling problem and uses a Maximum
Entropy Markov Model (MEMM) to detect trigger words.
For the hybrid approach using both rules and a dictionary, firstly, we constructed a trigger dictionary
from the original GENIA event corpus [6] by extracting the annotated trigger words. These trigger words
were subsequently lemmatized and stemmed. However, the wide variety of potential lexicalized triggers
for an event means that lots of triggers lack discriminative power relative to individual event types. For
example, in certain context, through is the trigger word for the binding event type and are is the trigger
word for localization. Such words are too common and cause potential ambiguities and therefore lead to
many false positive events extracted. We could perform disambiguation by counting the co-occurrence
of a event trigger and a particular event type from the training data and discard those event triggers whose
co-occurrence counts are lower than certain threshold for that event type. After this filtering stage, still,
there might be cases where one trigger might representing multiple event types, we thus define a set of
rules to further process the trigger words matched from the constructed dictionary.
In the second approach, we treat trigger words identification as a sequence labeling problem and train
a first-order MEMM model [8] from the BioNLP?09 shared task training data. As in typical named entity
recognition tasks, the training data are converted into BIO format where ?B? refers to the word which is
the beginning word of an event trigger, ?I? indicates the rest of the words (if the trigger contains more
than one words) and ?O? refers to the other words which are not event triggers. The features used in the
MEMM model was extracted from the surface string and the part-of-speech information of the words
corresponding to (or adjacent to) the target BIO tags.
2.2 Semantic Parsing using the HVS Model
The Hidden Vector State (HVS) model [3] is a discrete Hidden Markov Model (HMM) in which each
HMM state represents the state of a push-down automaton with a finite stack size. State transitions
are factored into separate stack pop and push operations constrained to give a tractable search space.
The sequence of HVS stack states corresponding to the given parse tree is illustrated in Figure 1. The
result is a model which is complex enough to capture hierarchical structure but which can be trained
automatically from only lightly annotated data.
In the HVS-based semantic parser, conventional grammar rules are replaced by three probability
tables. Let each state at time t be denoted by a vector of Dt semantic concept labels (tags) ct =
[ct[1], ct[2], ..ct[Dt]] where ct[1] is the preterminal concept label and ct[Dt] is the root concept label
(SS in Figure 3). Given a word sequence W , concept vector sequence C and a sequence of stack pop
operations N , the joint probability of P (W,C, N) can be decomposed as
P (W,C, N) =
T
?
t=1
P (nt|ct?1)P (ct[1]|ct[2 ? ? ?Dt])P (wt|ct) (1)
396
enhanced tyrosine phosphorylation of STAT1
Positive_regulation
Phosphorylation
Site
ProteinDummy
Positive_regulation
SS
Site
Positive_regulation
SS
Phosphorylation
Site
Positive_regulation
SS
Dummy
Phosphorylation
Site
Positive_regulation
SS
Protein
Phosphorylation
Site
Positive_regulation
SS
IFN-alpha
Dummy
SS
sent_start sent_end
SS
Dummy
SS SE
SS
SE
Figure 1: Example of a parse tree and its vector state equivalent.
where nt is the vector stack shift operation and takes values in the range 0, ? ? ? , Dt?1, and ct[1] = cwt is
the new pre-terminal semantic label assigned to word wt at word position t.
Thus, the HVS model consists of three types of probabilistic move, each move being determined by a
discrete probability table: (1) popping semantic labels off the stack - P (n|c); (2) pushing a pre-terminal
semantic label onto the stack - P (c[1]|c[2 ? ? ?D]); (3) generating the next word - P (w|c). Each of these
tables are estimated in training using an EM algorithm and then used to compute parse trees at run-time
using Viterbi decoding. In training, each word string W is marked with the set of semantic concepts
C that it contains. For example, the sentence IFN-alpha enhanced tyrosine phosphorylation of STAT1
contains the semantic concept/value pairs as shown in Figure 1. Its corresponding abstract semantic
annotation is:
Positive regulation(Site(Phosphorylation(protein)))
where brackets denote the hierarchical relations among semantic concepts1. For each word wk of a
training sentence W , EM training uses the forward-backward algorithm to compute the probability of
the model being in stack state c when wk is processed. Without any constraints, the set of possible stack
states would be intractably large. However, in the HVS model this problem can be avoided by pruning
out all states which are inconsistent with the semantic concepts associated with W . The details of how
this is done are given in [3].
For the sentences in the BioNLP?09 shared task, only event information is provided. However, the
abstract semantic annotation as shown above is required for training the HVS model. We propose Algo-
rithm 1 to automatically convert the annotated event information into the abstract semantic annotations.
An example of how the abstract annotations are generated is given as follows.
Sentence: According to current models the inhibitory capacity of I(kappa)B(alpha) would be mediated
through the retention of Rel/NF-kappaB proteins in the cytosol.
Corresponding Events: E1 Negative regulation: inhibitory capacity Theme: I(kappa)B(alpha)
E2 Positive regulation: mediated Theme: E1
Candidate annotation generation (Steps 1-4 of Algorithm 1):
Negative regulation(Protein) Negative regulation(Protein(Positive regulation))
Abstract annotation pruning (Steps 5-14 of Algorithm 1):
Negative regulation(Protein(Positive regulation))
2.3 Biomedical Events Extraction From HVS Parse Results
Based on HVS parse results, it seems straightforward to extract the event information. However, after
detailed investigation, we found that sentences having the same semantic tags might contain different
events information. For example, the two sentences shown in Table 1 have the same semantic parsing
results but with different event information.
This problem can be solved by classification. For the semantic tags which can represent multiple
event information, we considered each event information as a class and employed hiddenMarkov support
vector machines (HM-SVMs) [1] for disambiguation among possible events. The features used in HM-
SVMs are extracted from surface strings and part-of-speech information of the words corresponding to
(or adjacent to) trigger words.
1We omit SS and SE here which denote sentence start and end.
397
Algorithm 1 Abstract semantic annotation generation.
Input: A sentence W =< w1, w2, ? ? ? , wn >, and its event information Ev =< e1, e2, ? ? ? , em >
Output: Abstract semantic annotation A
1: for each event ei =<Event type:Trigger words Theme:Protein name ...> do
2: Sort the Trigger words, Protein name, and other arguments based on their positions in W and get
a sorted list t1, t2, ..., tk
3: Generate an annotation as t1(t2(..tk)), add it into the annotation list A
4: end for
5: for each annotation ai ? A do
6: if ai contains another event then
7: Replace the event with its corresponding annotation am
8: end if
9: end for
10: for each annotation ai ? A do
11: if ai is a subset of another annotation in A then
12: Remove ai from the annotation list A
13: end if
14: end for
15: Reorder annotations in A based on their positions in W
Sentence We concluded that CTCF expression and activity is con-
trolled at transcriptional and post-transcriptional levels
CONCLUSION: IL-5 synthesis by human helper T cells
is regulated at the transcriptional level
Parse
results
SS+Protein(CTCF) SS+Protein+Gene Expression(expression)
SS+Protein+Gene Expression+Regulation( controlled...levels)
SS+Protein(IL-5) SS+Protein+Gene Expression(synthesis)
SS+Protein+Gene Expression+Regulation( regulated)
Events E1 Gene expression:expression Theme: CTCF E1 Gene expression: synthesis Theme: IL-5
E2 Regulation: controlled...levels Theme: E1 E2 Regulation: regulated Theme: E1
E3 Regulation: controlled...levels Theme: CTCF
Table 1: An example of the same semantic parse results denoting different event information
3 Results and Discussion
Experiments have been conducted on the training data of the BioNLP?09 shared task which consists of
800 abstracts. After cleaning up the sentences which do not contain biomedical events information, 2893
sentences were kept. We split the 2893 sentences randomly into the training set and the test set at the
ratio of 9:1 and conducted the experiments ten times with different training and test data each round.
Method Recall (%) Precision (%) F-score (%)
Trigger Word Identification
Dictionary+Rules 46.31 53.34 49.57
MEMM 45.43 40.91 42.99
Event Extraction from HVS Parse Results
No classification 43.57 52.85 47.77
With Classification 46.31 53.34 49.57
Table 2: Experimental results based on 10 fold cross-validation.
Table 2 shows the performance evaluated using the approximate recursive matching method adopted
from the BioNLP?09 share task evaluation mode. To evaluate the performance impact of trigger word
identification, we also report the overall performance of the system using the two approaches we pro-
posed, dictionary+rules and MEMM. The results show that the hybrid approach combining a trigger
dictionary and rules gives better performance than MEMM which only achieved a F-score around 43%.
For biomedical event extraction from HVS parse results, employing the classification method presented
in Section 2.3 improves the overall performance from 47.77% to 49.57%.
The best performance that HVS-BioEvent achieved is an F-score of 49.57%, which is only two points
lower than UTurku, the best performing system in the BioNLP?09 share task. It should be noted that our
results are based on 10-fold cross validation on the BioNLP?09 shared task training data only since we
don?t have the access to the BioNLP?09 test set while the results generated by UTurku were evaluated
on the BioNLP?09 test set. Although a direct comparison is not possible, we could still speculate that
398
Simple Events Complex Events
Event Class HVS-BioEvent UTurku Event Class HVS-BioEvent UTurku
localization 61.40 61.65 binding 49.90 44.41
gene expression 72.44 73.90 regulation 36.57 30.52
transcription 68.30 50.23 negative regulation 40.61 38.99
protein catabolism 70.27 52.17
phosphorylation 56.52 77.58
Table 3: Per-class performance comparison in F-score (%) between HVS-BioEvent and UTurku.
HVS-BioEvent is comparable to the best performing system in the BioNLP?09 shared task.
The results on the five event types involving only a single theme argument are shown in Table 3
as Simple Events. For the complex events such as ?binding?, ?regulation? and ?negative regulation?
events, the results are shown in Table 3 as Complex Events. We notice that HVS-BioEvent outperforms
UTurku on the extraction of the complex event types, with the performance gain ranging between 2%
and 7%. The results suggest that the HVS model with the hierarchical hidden state structure is indeed
more suitable for complex event extraction since it could naturally model embedded structural context in
sentences.
4 Conclusions
In this paper, we have presented HVS-BioEvent which uses the HVS model to automatically extract
biomedical events from text. The system is able to offer comparable performance compared with the
best performing system in the BioNLP?09 shared task. Moreover, it outperforms the existing systems
on complex events extraction which shows the ability of the HVS model in capturing embedded and
hierarchical relations among named entities. In future work we will explore incorporating arbitrary
lexical features into the HVS model training in order to further improve the extraction accuracy.
References
[1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In Interna-
tional Conference in Machine Learning, pages 3?10, 2003.
[2] Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkla, and Tapio Salakoski. Extract-
ing complex biological events with rich graph-based feature sets. In Proceedings of the Workshop
on BioNLP, pages 10?18, 2009.
[3] Yulan He and Steve Young. Semantic processing using the hidden vector state model. Computer
Speech and Language, 19(1):85?106, 2005.
[4] Lynette Hirschman, Alexander Yeh, Christian Blaschke, and Alfonso Valencia. Overview of biocre-
ative: critical assessment of information extraction for biology. BMC Bioinformatics, 2005.
[5] Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun?ichi Tsujii. Overview of
bionlp?09 shared task on event extraction. In Proceedings of the Workshop on BioNLP, pages 1?9,
2009.
[6] Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. Corpus annotation for mining biomedical events
from literature. BMC Bioinformatics, 9(10), 2008.
[7] Claire Ne?dellec. Learning Language in Logic - Genic Interaction Extraction Challenge. In Learning
Language in Logic workshop (LLL05), pages 31?37, 2005.
[8] Nam Nguyen and Yunsong Guo. Comparisons of sequence labeling algorithms and extensions. In
Proceedings of the ICML, pages 681?688, 2007.
[9] Deyu Zhou, Yulan He, and Chee Keong Kwoh. Extracting protein-protein interactions from medline
using the hidden vector state model. International Journal of Bioinformatics Research and Applica-
tions, 4(1):64?80, 2008.
399
