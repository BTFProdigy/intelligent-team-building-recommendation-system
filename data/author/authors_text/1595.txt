Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 708?716, Prague, June 2007. c?2007 Association for Computational Linguistics
Large-Scale Named Entity Disambiguation 
Based on Wikipedia Data 
 
Silviu Cucerzan 
Microsoft Research 
One Microsoft Way, Redmond, WA 98052, USA 
silviu@microsoft.com 
 
Abstract 
This paper presents a large-scale system for the 
recognition and semantic disambiguation of 
named entities based on information extracted 
from a large encyclopedic collection and Web 
search results. It describes in detail the disam-
biguation paradigm employed and the information 
extraction process from Wikipedia. Through a 
process of maximizing the agreement between the 
contextual information extracted from Wikipedia 
and the context of a document, as well as the 
agreement among the category tags associated 
with the candidate entities, the implemented sys-
tem shows high disambiguation accuracy on both 
news stories and Wikipedia articles. 
1 Introduction and Related Work 
The ability to identify the named entities (such as 
people and locations) has been established as an 
important task in several areas, including topic de-
tection and tracking, machine translation, and in-
formation retrieval. Its goal is the identification of 
mentions of entities in text (also referred to as sur-
face forms henceforth), and their labeling with one 
of several entity type labels. Note that an entity 
(such as George W. Bush, the current president of 
the U.S.) can be referred to by multiple surface 
forms (e.g., ?George Bush? and ?Bush?) and a sur-
face form (e.g., ?Bush?) can refer to multiple enti-
ties (e.g., two U.S. presidents, the football player 
Reggie Bush, and the rock band called Bush). 
When it was introduced, in the 6th Message Un-
derstanding Conference (Grishman and Sundheim, 
1996), the named entity recognition task comprised 
three entity identification and labeling subtasks: 
ENAMEX (proper names and acronyms designat-
ing persons, locations, and organizations), TIMEX 
(absolute temporal terms) and NUMEX (numeric 
expressions, monetary expressions, and percent-
ages).  Since 1995, other similar named entity rec-
ognition tasks have been defined, among which 
CoNLL (e.g., Tjong Kim Sang and De Meulder, 
2003) and ACE (Doddington et al, 2004). In addi-
tion to structural disambiguation (e.g., does ?the 
Alliance for Democracy in Mali? mention one, 
two, or three entities?) and entity labeling (e.g., 
does ?Washington went ahead? mention a person, 
a place, or an organization?), MUC and ACE also 
included a within document coreference task, of 
grouping all the mentions of an entity in a docu-
ment together (Hirschman and Chinchor, 1997). 
When breaking the document boundary and scal-
ing entity tracking to a large document collection 
or the Web, resolving semantic ambiguity becomes 
of central importance, as many surface forms turn 
out to be ambiguous. For example, the surface 
form ?Texas? is used to refer to more than twenty 
different named entities in Wikipedia. In the con-
text ?former Texas quarterback James Street?, 
Texas refers to the University of Texas at Austin; 
in the context ?in 2000, Texas released a greatest 
hits album?, Texas refers to the British pop band; 
in the context ?Texas borders Oklahoma on the 
north?, it refers to the U.S. state; while in the con-
text ?the characters in Texas include both real and 
fictional explorers?, the same surface form refers 
to the novel written by James A. Michener. 
Bagga and Baldwin (1998) tackled the problem 
of cross-document coreference by comparing, for 
any pair of entities in two documents, the word 
vectors built from all the sentences containing 
mentions of the targeted entities. Ravin and Kazi 
(1999) further refined the method of solving co-
reference through measuring context similarity and 
integrated it into Nominator (Wacholder et al, 
1997), which was one of the first successful sys-
tems for named entity recognition and co-reference 
resolution. However, both studies targeted the clus-
tering of all mentions of an entity across a given 
document collection rather than the mapping of 
these mentions to a given reference list of entities. 
A body of work that did employ reference entity 
lists targeted the resolution of geographic names in 
708
text. Woodruff and Plaunt (1994) used a list of 80k 
geographic entities and achieved a disambiguation 
precision of 75%. Kanada (1999) employed a list 
of 96k entities and reported 96% precision for geo-
graphic name disambiguation in Japanese text. 
Smith and Crane (2002) used the Cruchley?s and 
the Getty thesauri, in conjunction with heuristics 
inspired from the Nominator work, and obtained 
between 74% and 93% precision at recall levels of 
89-99% on five different history text corpora. 
Overell and R?ger (2006) also employed the Getty 
thesaurus as reference and used Wikipedia to develop 
a co-occurrence model and to test their system. 
In many respects, the problem of resolving am-
biguous surface forms based on a reference list of 
entities is similar to the lexical sample task in word 
sense disambiguation (WSD). This task, which has 
supported large-scale evaluations ? SENSEVAL 1-3 
(Kilgarriff and Rosenzweig, 2000; Edmonds and 
Cotton, 2001; Mihalcea et al, 2004) ? aims to as-
sign dictionary meanings to all the instances of a 
predetermined set of polysemous words in a corpus 
(for example, choose whether the word ?church? 
refers to a building or an institution in a given con-
text). However, these evaluations did not include 
proper noun disambiguation and omitted named 
entity meanings from the targeted semantic labels 
and the development and test contexts (e.g., 
?Church and Gale showed that the frequency [..]?).  
The problem of resolving ambiguous names also 
arises naturally in Web search. For queries such as 
?Jim Clark? or ?Michael Jordan?, search engines 
return blended sets of results referring to many 
different people. Mann and Yarowsky (2003) ad-
dressed the task of clustering the Web search re-
sults for a set of ambiguous personal names by 
employing a rich feature space of biographic facts 
obtained via bootstrapped extraction patterns. They 
reported 88% precision and 73% recall in a three-way 
classification (most common, secondary, and other uses). 
Raghavan et al (2004) explored the use of entity 
language models for tasks such as clustering enti-
ties by profession and classifying politicians as 
liberal or conservative. To build the models, they 
recognized the named entities in the TREC-8 cor-
pus and computed the probability distributions 
over words occurring within a certain distance of 
any instance labeled as Person of the canonical 
surface form of 162 famous people. 
Our aim has been to build a named entity recog-
nition and disambiguation system that employs a 
comprehensive list of entities and a vast amount of 
world knowledge. Thus, we turned our attention to 
the Wikipedia collection, the largest organized 
knowledge repository on the Web (Remy, 2002). 
Wikipedia was successfully employed previously 
by Strube and Ponzetto (2006) and Gabrilovich and 
Markovitch (2007) to devise methods for computing 
semantic relatedness of documents, WikiRelate! 
and Explicit Semantic Analysis (ESA), respec-
tively. For any pair of words, WikiRelate! attempts 
to find a pair of articles with titles that contain 
those words and then computes their relatedness 
from the word-based similarity of the articles and 
the distance between the articles? categories in the 
Wikipedia category tree. ESA works by first build-
ing an inverted index from words to all Wikipedia 
articles that contain them. Then, it estimates a re-
latedness score for any two documents by using the 
inverted index to build a vector over Wikipedia 
articles for each document and by computing the 
cosine similarity between the two vectors. 
The most similar work to date was published by 
Bunescu and Paca (2006). They employed several 
of the disambiguation resources discussed in this 
paper (Wikipedia entity pages, redirection pages, 
categories, and hyperlinks) and built a context-
article cosine similarity model and an SVM based 
on a taxonomy kernel. They evaluated their models 
for person name disambiguation over 110, 540, 
and 2,847 categories, reporting accuracies between 
55.4% and 84.8% on (55-word context, entity) 
pairs extracted from Wikipedia, depending on the 
model and the development/test data employed. 
The system discussed in this paper performs both 
named entity identification and disambiguation. 
The entity identification and in-document corefer-
ence components resemble the Nominator system 
(Wacholder et al, 1997). However, while Nomina-
tor made heavy use of heuristics and lexical clues 
to solve the structural ambiguity of entity men-
tions, we employ statistics extracted from Wikipe-
dia and Web search results. The disambiguation 
component, which constitutes the main focus of the 
paper, employs a vast amount of contextual and 
category information automatically extracted from 
Wikipedia over a space of 1.4 million distinct enti-
ties/concepts, making extensive use of the highly 
interlinked structure of this collection. We aug-
ment the Wikipedia category information with in-
formation automatically extracted from Wikipedia 
list pages and use it in conjunction with the context 
information in a vectorial model that employs a 
novel disambiguation method. 
709
2 The Disambiguation Paradigm 
We present in this section an overview of the pro-
posed disambiguation model and the world knowl-
edge data employed in the instantiation of the 
model discussed in this paper. The formal model is 
discussed in detailed in Section 5. 
The world knowledge used includes the known 
entities (most articles in Wikipedia are associated 
to an entity/concept), their entity class when avail-
able (Person, Location, Organization, and Miscel-
laneous), their known surface forms (terms that are 
used to mention the entities in text), contextual 
evidence (words or other entities that describe or 
co-occur with an entity), and category tags (which 
describe topics to which an entity belongs to). 
For example, Figure 1 shows nine of the over 70 
different entities that are referred to as ?Columbia? 
in Wikipedia and some of the category and contex-
tual information associated with one of these enti-
ties, the Space Shuttle Columbia. 
The disambiguation process uses the data associ-
ated with the known surface forms identified in a 
document and all their possible entity disambigua-
tions to maximize the agreement between the con-
text data stored for the candidate entities and the 
contextual information in the document, and also, 
the agreement among the category tags of the can-
didate entities. For example, a document that con-
tains the surface forms ?Columbia? and 
?Discovery? is likely to refer to the Space Shuttle 
Columbia and the Space Shuttle Discovery because 
these candidate entities share the category tags 
LIST_astronomical_topics, CAT_Manned_space-
craft, CAT_Space_Shuttles (the extraction of such 
tags is presented in Section 3.2), while other entity 
disambiguations, such as Columbia Pictures and 
Space Shuttle Discovery, do not share any com-
mon category tags. The agreement maximization 
process is discussed in depth in Section 5. 
This process is based on the assumption that 
typically, all instances of a surface form in a 
document have the same meaning. Nonetheless, 
there are a non-negligible number of cases in 
which the one sense per discourse assumption 
(Gale et al, 1992) does not hold. To address this 
problem, we employ an iterative approach, of 
shrinking the context size used to disambiguate 
surface forms for which there is no dominating 
entity disambiguation at document level, perform-
ing the disambiguation at the paragraph level and 
then at the sentence level if necessary. 
 
Figure 1. The model of storing the information ex-
tracted from Wikipedia into two databases. 
3 Information Extraction from Wikipedia 
We discuss now the extraction of entities and the 
three main types of disambiguation clues (entity 
surface forms, category tags, and contexts) used by 
the implemented system. While this information 
extraction was performed on the English version of 
the Wikipedia collection, versions in other lan-
guages or other collections, such as Encarta or 
WebMD, could be targeted in a similar manner. 
When processing the Wikipedia collection, we 
distinguish among four types of articles: entity 
pages, redirecting pages, disambiguation pages, 
and list pages. The characteristics of these articles 
and the processing applied to each type to extract 
the three sets of clues employed by the disam-
biguation model are discussed in the next three 
subsections. 
3.1 Surface Form to Entity Mappings 
There are four sources that we use to extract entity 
surface forms: the titles of entity pages, the titles of 
redirecting pages, the disambiguation pages, and 
the references to entity pages in other Wikipedia 
articles. An entity page is an article that contains 
information focused on one single entity, such as a 
person, a place, or a work of art. For example, 
Wikipedia contains a page titled ?Texas (TV se-
ries)?, which offers information about the soap 
opera that aired on NBC from 1980 until 1982. A 
redirecting page typically contains only a refer-
ence to an entity page. For example, the article  
titled ?Another World in Texas? contains a redirec-
Columbia 
 
Colombia 
Columbia University 
Columbia River 
Columbia Pictures 
Columbia Bicycles 
Space Shuttle Columbia  
USS Columbia 
Columbia, Maryland 
Columbia, California 
... 
Space Shuttle Columbia 
 
Tags: 
Manned spacecraft 
Space program fatalities 
Space Shuttles 
 
Contexts: 
NASA 
Kennedy Space Center 
Eileen Collins 
Entities Surface Forms 
710
tion to the article titled ?Texas (TV series)?. From 
these two articles, we extract the entity Texas (TV 
series) and its surface forms Texas (TV series), 
Texas and Another World in Texas. As shown in 
this example, we store not only the exact article 
titles but also the corresponding forms from which 
we eliminate appositives (either within parentheses 
or following a comma). 
We also extract surface form to entity mappings 
from Wikipedia disambiguation pages, which are 
specially marked articles having as title a surface 
form, typically followed by the word ?disambigua-
tion? (e.g., ?Texas (disambiguation)?), and con-
taining a list of references to pages for entities that 
are typically mentioned using that surface form. 
Additionally, we extract all the surface forms 
used at least in two articles to refer to a Wikipedia 
entity page. Illustratively, the article for Pam Long 
contains the following Wikitext, which uses the 
surface form ?Texas? to refer to Texas (TV series): 
After graduation, she went to [[New York City]] and 
played Ashley Linden on [[Texas (TV series)|Texas]] 
from [[1981]] to [[1982]]. 
In Wikitext, the references to other Wikipedia ar-
ticles are within pairs of double square brackets. If 
a reference contains a vertical bar then the text at 
the left of the bar is the name of the referred article 
(e.g. ?Texas (TV Series)?), while the text at the 
right of the bar (e.g., ?Texas?) is the surface form 
that is displayed (also referred to as the anchor text 
of the link). Otherwise, the surface form shown in 
the text is identical to the title of the Wikipedia 
article referred (e.g., ?New York City?). 
Using these four sources, we extracted more than 
1.4 million entities, with an average of 2.4 surface 
forms per entity. We obtained 377k entities with 
one surface form, 166k entities with two surface 
forms, and 79k entities with three surface forms. 
At the other extreme, we extracted one entity with 
no less than 99 surface forms. 
3.2 Category Information 
All articles that are titled ?List of [?]? or ?Table 
of [?]? are treated separately as list pages. They 
were built by Wikipedia contributors to group enti-
ties of the same type together (e.g., ?List of an-
thropologists?, ?List of animated television series?, 
etc.) and are used by our system to extract category 
tags for the entities listed in these articles. The tags 
are named after the title of the Wikipedia list page. 
For example, from the article ?List of band name 
etymologies?, the system extracts the category tag 
LIST_band_name_etymologies and labels all the 
entities referenced in the list, including Texas 
(band), with this tag. This process resulted in the 
extraction of more than 1 million (entity, tag) pairs. 
After a post-processing phase that discards tempo-
ral tags, as well as several types of non-useful tags 
such as ?people by name? and ?places by name?, 
we obtained a filtered list of 540 thousand pairs. 
We also exploit the fact that Wikipedia enables 
contributors to assign categories to each article, 
which are defined as ?major topics that are likely 
to be useful to someone reading the article?. Be-
cause any Wikipedia contributor can add a cate-
gory to any article and the work of filtering out 
bogus assignments is tedious, these categories 
seem to be noisier than the lists, but they can still 
provide a tremendous amount of information. We 
extracted the categories of each entity page and 
assigned them as tags to the corresponding entity. 
Again, we employed some basic filtering to discard 
meta-categories (e.g., ?Articles with unsourced 
statements?) and categories not useful for the proc-
ess of disambiguation through tag agreement (e.g., 
?Living people?, ?1929 births?). This extraction 
process resulted in 2.65 million (entity, tag) pairs 
over a space of 139,029 category tags. 
We also attempted to extract category tags based 
on lexicosyntactic patterns, more specifically from 
enumerations of entities. For example, the para-
graph titled ?Music of Scotland? (shown below in 
Wikitext) in the Wikipedia article on Scotland con-
tains an enumeration of entities, which can be la-
beled ENUM_Scotland_PAR_Music_of_Scotland: 
Modern Scottish [[pop music]] has produced many 
international bands including the [[Bay City Rollers]], 
[[Primal Scream]], [[Simple Minds]], [[The Proclaim-
ers]], [[Deacon Blue]], [[Texas (band)|Texas]], [[Franz 
Ferdinand]], [[Belle and Sebastian]], and [[Travis 
(band)|Travis]], as well as individual artists such as 
[[Gerry Rafferty]], [[Lulu]], [[Annie Lennox]] and [[Lloyd 
Cole]], and world-famous Gaelic groups such as 
[[Runrig]] and [[Capercaillie (band)|Capercaillie]]. 
Lexicosyntactic patterns have been employed 
successfully in the past (e.g., Hearst, 1992; Roark 
and Charniak, 1998; Cederberg and Widdows, 
2003), and this type of tag extraction is still a 
promising direction for the future. However, the 
brute force approach we tried ? of indiscriminately 
tagging the entities of enumerations of four or 
more entities ?  was found to introduce a large 
amount of noise into the system in our develop-
ment experiments. 
711
3.3 Contexts 
To extract contextual clues for an entity, we use 
the information present in that entity?s page and in 
the other articles that explicitly refer to that entity. 
First, the appositives in the titles of entity pages, 
which are eliminated to derive entity surface forms 
(as discussed in Section 3.1) are saved as contex-
tual clues. For example, ?TV series? becomes a 
context for the entity Texas (TV series). 
We then extract all the entity references in the 
entity page. For example, from the article on Texas 
(band), for which a snippet in Wikitext is shown 
below, we extract as contexts the references pop 
music, Glasgow, Scotland, and so on: 
'''Texas''' is a [[pop music]] band from [[Glasgow]], 
[[Scotland]], [[United Kingdom]]. They were founded 
by [[Johnny McElhone]] in [[1986 in music|1986]] and 
had their performing debut in [[March]] [[1988]] at [?] 
Reciprocally, we also extract from the same ar-
ticle that the entity Texas (band) is a good context 
for pop music, Glasgow, Scotland, etc. 
The number of contexts extracted in this manner 
is overwhelming and had to be reduced to a man-
ageable size. In our development experiments, we 
explored various ways of reducing the context in-
formation, for example, by extracting only entities 
with a certain number of mentions in an article, or 
by discarding mentions with low TF*IDF scores 
(Salton, 1989). In the end, we chose a strategy in 
which we employ as contexts for an entity two 
category of references: those mentioned in the first 
paragraph of the targeted entity page, and those for 
which the corresponding pages refer back to the 
targeted entity. For example, Pam Long and Texas 
(TV series) are extracted as relevant contexts for 
each other because their corresponding Wikipedia 
articles reference one another ? a relevant snippet 
from the Pam Long article is cited in Section 3.1 
and a snippet from the article for Texas (TV se-
ries) that references Pam Long is shown below:  
In 1982 [[Gail Kobe]] became executive producer and 
[[Pam Long]] became headwriter. 
In this manner, we extracted approximately 38 
million (entity, context) pairs. 
4 Document Analysis 
In this section, we describe concisely the main text 
processing and entity identification components of 
the implemented system. We will then focus on the 
novel entity disambiguation component, which we 
propose and evaluate in this paper, in Section 5. 
 
Figure 2. An overview of the processes employed by 
the proposed system. 
Figure 2 outlines the processes and the resources 
that are employed by the implemented system in 
the analysis of text documents. First, the system 
splits a document into sentences and truecases the 
beginning of each sentence, hypothesizing whether 
the first word is part of an entity or it is capitalized 
because of orthographic conventions. It also identi-
fies titles and hypothesizes the correct case for all 
words in the titles. This is done based on statistics 
extracted from a one-billion-word corpus, with 
back-off to Web statistics. 
In a second stage, a hybrid named-entity recog-
nizer based on capitalization rules, Web statistics, 
and statistics extracted from the CoNLL 2003 
shared task data (Tjong Kim Sang and De 
Meulder, 2003) identifies the  boundaries of  the 
entity  mentions in the text and assigns each set of 
mentions sharing the same surface form a probabil-
ity distribution over four labels: Person, Location, 
Organization, and Miscellaneous.1 The named en-
tity recognition component resolves the structural 
ambiguity with regard to conjunctions (e.g., ?Bar-
nes and Noble?, ?Lewis and Clark?), possessives 
(e.g., ?Alice's Adventures in Wonderland?, ?Brit-
ain's Tony Blair?), and prepositional attachment 
(e.g., ?Whitney Museum of American Art?, 
?Whitney Museum in New York?) by using the 
surface form information extracted from Wikipe-
dia, when available, with back-off to co-occurrence 
counts on the Web, in a similar way to Lapata and 
Keller (2004). Recursively, for each ambiguous 
term T0 of the form T1 Particle T2, where Particle 
is one of a possessive pronoun, a coordinative con-
junction, and a preposition, optionally followed by 
a determiner, and the terms T1 and T2 are se-
                                                          
1
 While the named entity labels are used only to solve in-
document coreferences by the current system, as described 
further in this section, preliminary experiments of probabilisti-
cally labeling the Wikipedia pages show that the these labels 
could also be used successfully in the disambiguation process. 
Truecaser and 
Sentence 
Breaker 
Entity 
Recognizer 
Entity 
Disambiguator 
 
	






	
Augmenting Wikipedia with Named Entity Tags 
Wisam Dakka 
Columbia University 
1214 Amsterdam Avenue 
New York, NY 10027 
wisam@cs.columbia.edu 
Silviu Cucerzan 
Microsoft Research 
1 Microsoft Way 
Redmond, WA 98052 
silviu@microsoft.com 
 
 
Abstract 
Wikipedia is the largest organized knowledge 
repository on the Web, increasingly employed 
by natural language processing and search tools. 
In this paper, we investigate the task of labeling 
Wikipedia pages with standard named entity 
tags, which can be used further by a range of in-
formation extraction and language processing 
tools. To train the classifiers, we manually anno-
tated a small set of Wikipedia pages and then ex-
trapolated the annotations using the Wikipedia 
category information to a much larger training 
set. We employed several distinct features for 
each page: bag-of-words, page structure, ab-
stract, titles, and entity mentions. We report high 
accuracies for several of the classifiers built. As 
a result of this work, a Web service that classi-
fies any Wikipedia page has been made available 
to the academic community. 
1 Introduction 
Wikipedia, one of the most frequently visited web 
sites nowadays, contains the largest amount of 
knowledge ever gathered in one place by volunteer 
contributors around the world (Poe, 2006). Each 
Wikipedia article contains information about one 
entity or concept, gathers information about 
entities of one particular type of entities (the so-
called list pages), or provides information about 
homonyms (disambiguation pages). As of July 
2007, Wikipedia contains close to two million 
articles in English. In addition to the English-
language version, there are 200 versions in other 
languages. Wikipedia has about 5 million 
registered contributors, averaging more than 10 
edits per contributor. 
Natural language processing and search tools can 
greatly benefit from Wikipedia by using it as an 
authoritative source of common knowledge and by 
exploiting its interlinked structure and 
disambiguation pages, or by extracting concept co-
occurrence information. This paper presents a 
successful study on enriching the Wikipedia data 
with named entity tags. Such tags could be 
employed by disambiguation systems such as 
Bunescu and Paca (2006) and Cucerzan (2007), in 
mining relationships between named entities, or in 
extracting useful facet terms from news articles 
(e.g., Dakka and Ipeirotis, 2008). 
In this work, we classify the Wikipedia pages 
into categories similar to those used in the CoNLL 
shared tasks (Tjong Kim Sang, 2002; Tjong Kim 
Sang and De Meulder, 2003) and ACE 
(Doddington et al, 2004). To the best of our 
knowledge, this is the first attempt to perform such 
classification on the English language version of 
the collection. 1  Although the task settings are 
different, the results we obtained are comparable 
with those previously reported in document 
classification tasks. 
We examined the Wikipedia pages to extract 
several feature groups for our classification task. 
We also observed that each entity/concept has at 
least two pseudo-independent views (page-based 
features and link-based features), which allow the 
use a co-training method to boost the performance 
of classifiers trained separately on each view. 
The classifier that achieved the best accuracy on 
out test set was applied then to all Wikipedia pages 
and its classifications are provided to the academic 
community for use in future studies through a Web 
service.2 
                                                 
1
 Watanabe et al (2007) have reported recently experi-
ments on categorizing named entities in the Japanese 
version of Wikipedia using a graph-based approach. 
2
 The Web service is available at wikinet.stern.nyu.edu. 
545
2 Related Work 
This study is related to the area of named entity 
recognition, which has supported extensive evalua-
tions (CoNLL and ACE). Since the introduction of 
this task in MUC-6 (Grishman and Sundheim, 
1996), numerous systems using various ways of 
exploiting entity-specific and local context features 
were proposed, from relatively simple character-
based models such as Cucerzan and Yarowsky 
(2002) and Klein et al (2003) to complex models 
making use of various lexical, syntactic, morpho-
logical, and orthographical information, such as 
Wacholder et al (1997), Fleischman and Hovy 
(2002), and Florian et al (2003). While the task we 
address is not the conventional named entity rec-
ognition but rather document classification, our 
classes are a derived from the labels traditionally 
employed in named entity recognition, following 
the CoNLL and ACE guidelines, as described in 
Section 3. 
The areas of text categorization and document 
classification have also been extensively re-
searched over time. These task have the goal of 
assigning to each document in a collection one or 
several labels from a given set, such as News-
groups (Lang, 1995), Reuters (Reuters, 1997), Ya-
hoo! (Mladenic, 1998), Open Directory Project 
(Chakrabarti et al, 2002), and Hoover?s Online 
(Yang et al, 2002). Various supervised machine 
learning algorithms have been applied successfully 
to the document classification problem (e.g., 
Joachims, 1999; Quinlan, 1993; Cohen, 1995). 
Dumais et al (1998) and Yang and Liu (1999) re-
ported that support vector machines (SVM) and K-
Nearest Neighbor performed the best in text cate-
gorization. We adopted SVM as our algorithm of 
choice because of these findings and also because 
SVMs have been shown robust to noise in the fea-
ture set in several studies. While Joachims (1998) 
and Rogati and Yang (2002) reported no improve-
ment in SVM performance after applying a feature 
selection step, Gabrilovich and Markovitch (2004) 
showed that for collection with numerous redun-
dant features, aggressive feature selection allowed 
SVMs to actually improve their performance. 
However, performing an extensive investigation of 
classification performance across various machine 
learning algorithms has been beyond the purpose 
of this work, in which we ran classification ex-
periments using SVMs and compared them only 
with the results of similar systems employing    
Na?ve Bayes. 
In addition to the traditional bag-of-words, 
which has been extensively used for the document 
classification task (e.g. Sebastiani, 2002), we em-
ployed various other Wikipedia-specific feature 
sets. Some of these have been previously employed 
for various tasks by Gabrilovich and Markovitch, 
(2006); Overell and Ruger (2006), Cucerzan 
(2007), and Suchanek et al (2007). 
3 Classifying Wikipedia Pages 
The Wikipedia pages that we analyzed in this study 
can be divided into three types: 
Disambiguation Page (DIS): is a special kind of 
page that usually contains the word ?disambigua-
tion? in its title, and that contains several possible 
disambiguations of a term. 
Common Page (COMM): refers to a common 
object rather than a named entity. Generally, if the 
name of an object or concept appears non-
capitalized in text then it is very likely that the ob-
ject or the concept is of common nature (heuristic 
previously employed by Bunescu and Paca, 2006). 
For example, the Wikipedia page ?Guitar? refers to 
a common object rather than a named entity. 
Named Entity Page: refers to a specific object 
or set of objects in the world, which is/are com-
monly referred to using a certain proper noun 
phrase. For example, any particular person is a 
named entity, though the concept of ?people? is 
not a named entity. Note that most names are am-
biguous. ?Apollo? can refer to more than 30 differ-
ent entities of different types, for example, the Fin-
nish rock band of the late 1960s/early 1970s , the 
Greek god of light, healing, and poetry, and the 
series of space missions run by NASA. 
To classify the named entities in Wikipedia, we 
adopted a restricted version of the ACE guidelines 
(ACE), using four main entity classes (also similar 
to the classes employed in the CoNLL evaluations): 
Animated Entities (PER): An animate entity 
can be either of type human or non-human. Hu-
man entities are either humans that are known to 
have lived (e.g., ?Leonardo da Vinci?, ?Britney 
Spears?, ?Gotthard of Hildesheim?, ?Saint 
Godehard?) or humanoid individuals in fictional 
works, such as books, movies, TV shows, and 
comics (e.g., ?Harry Potter?, ?Batman?, ?Sonny? 
546
the robot from the movie ?I, Robot?). Fictional 
characters also include mythological figures and 
deities (e.g. ?Zeus?, ?Apollo?, ?Jupiter?). The fic-
tional nature of a character must be explicitly indi-
cated. Non-human entities are any particular ani-
mal or alien that has lived or that is described in a 
fictional work and can be singled out using a name. 
 Organization Entities (ORG): An organization 
entity must have some formally established asso-
ciation. Typical examples are businesses (e.g., 
?Microsoft?, ?Ford?), governmental bodies (e.g., 
?United States Congress?), non-governmental or-
ganizations (e.g., ?Republican Party?, ?American 
Bar Association?), science and health units (e.g., 
?Massachusetts General Hospital?), sports organi-
zations and teams (e.g., ?Angolan Football Federa-
tion?, ?San Francisco 49ers?), religious organiza-
tions (e.g., ?Church of Christ?), and entertainment 
organizations, including formally organized music 
groups (e.g., ?San Francisco Mime Troupe?, the 
rock band ?The Police?). Industrial sectors and 
industries (e.g., ?Petroleum industry?) are also 
treated as organization entities, as well as all media 
and publications. 
Location Entities (LOC): These are physical lo-
cations (regions in space) defined by geographical, 
astronomical, or political criteria. They are of three 
types: Geo-Political entities are composite entities 
comprised of a physical location, a population, a 
government, and a nation (or province, state, 
county, city, etc.). A Wikipedia page that mentions 
all these components should be labeled as Geo-
Political Entity (e.g., ?Hawaii?, ?European Union?, 
?Australia?, and ?Washington, D.C.?). Locations 
are places defined on a geographical or astronomi-
cal basis and do not constitute a political entity. 
These include mountains, rivers, seas, islands, con-
tinents (e.g., ?the Solar system?, ?Mars?, ?Hudson 
River?, and ?Mount Rainier?). Facilities are arti-
facts in the domain of architecture and civil engi-
neering, such as buildings and other permanent 
man-made structures and real estate improvements: 
airports, highways, streets, etc. 
Miscellaneous Entities (MISC): About 25% of 
the named entities in Wikipedia are not of the 
types listed above. By examining several hundred 
examples, we concluded that the majority of these 
named entities can be classified in one of the fol-
lowing classes: Events refer to historical events or 
actions with some certain duration, such as wars, 
sport events, and trials (e.g., ?Gulf War?, ?2006 
FIFA World Cup?, ?Olympic Games?, ?O.J. Simp-
son trial?). Works of art refer to named works that 
are imaginative in nature. Examples include books, 
movies, TV programs, etc. (e.g., the ?Batman? 
movie, ?The Tonight Show?, the ?Harry Potter? 
books). Artifacts refer to man-made objects or 
products that have a name and cannot generally be 
labeled as art. This includes mass-produced mer-
chandise and lines of products (e.g. the camera 
?Canon PowerShot Pro1?, the series ?Canon Pow-
erShot?, the type of car ?Ford Mustang?, the soft-
ware ?Windows XP?). Finally Processes include 
all named physical and chemical processes (e.g., 
?Ettinghausen effect?). Abstract formulas or algo-
rithms that have a name are also labeled as proc-
esses (e.g., ?Naive Bayes classifier?). 
4 Features Used. Independent Views 
When creating a Wikipedia page and introducing 
a new entity, contributors can refer to other related 
Wikipedia entities, which may or may not have 
corresponding Wikipedia pages. This way of gen-
erating content creates an internal web graph and, 
interesting, results in the presence of two different 
and pseudo-independent views for each entity. We 
can represent an entity using the content written on 
the entity page, or alternatively, using the context 
from a reference on the related page. For example, 
Figures 1 and 2 show the two independent views of 
the entity ?Gwen Stefani?. 
 
 
Figure 1. A partial list of contextual references taken 
from Wikipedia for the named entity ?Gwen Stefani?. 
(There are over 600 such references.) 
1 such as ?Let Me Blow Ya Mind? by Eve and [[Gwen 
Stefani]] (whom he would produce 
2 In the video ?[[Cool (song)?Cool]]?, [[Gwen Stefani]] 
is made-up as Monroe. 
3 ?[[South Side (song)?South Side]]? (featuring [[Gwen 
Stefani]]) #14 US 
4  [[1969]] - [[Gwen Stefani]], American singer ([[No 
Doubt]]) 
5 [[Rosie Gaines]], [[Carmen Electra]], [[Gwen Stefani]], 
[[Chuck D]], [[Angie Stone]], 
6 In late [[2004]], [[Gwen Stefani]] released a hit song 
called ?Rich Girl? which 
7 [[Gwen Stefani]] - lead singer of the band [[No 
Doubt]], who is now a successful 
8 [[Social Distortion]], and [[TSOL]]. [[Gwen Stefani]], 
lead vocalist of the [[alternative rock]] 
9 main proponents (along with [[Gwen Stefani]] and 
[[Ashley Judd]]) in bringing back the 
10 The [[United States?American]] singer [[Gwen 
Stefani]] references Harajuku in several 
547
 Figure 2. Wikipedia page for the named entity ?Gwen 
Stefani?. Other than the regular text, information such 
as surface and disambiguated entities, structure proper-
ties, and section titles can be easily extracted. 
 
We utilize this important observation to extract our 
features based on these two independent views: 
page-based features and context features. We dis-
cuss these in greater detail next. 
4.1 Page-Based Features 
A typical Wikipedia page is usually written and 
edited by several contributors. Each page includes 
a rich set of information including the following 
elements: titles, section titles, paragraphs, multi-
media objects, hyperlinks, structure data, surface 
entities and their disambiguations. Figure 2 shows 
some of these elements in the page dedicated to 
singer ?Gwen Stefani?. We use the Wikipedia page 
XML syntax to draw a set of different page-based 
feature vectors, including the following: 
Bag of Words (BOW): This vector is the term 
frequency representation of the entire page. 
Structured Data (STRUCT): Many Wikipedia 
pages contain useful data organized in tables and 
other structural representations. In Figure 2, we see 
that contributors have used a table representation 
to list different properties about Gwen Stefani. We 
extract for each page, using the Wikipedia syntax, 
the bag-of-words feature vector that corresponds to 
this structured data only. 
 
Figure 3. The abstract provided by Wikipedia for 
?Gwen Stefani?. Note the concatenation of ?Stefani? 
and ?Some?, which results in a new word, and is a rele-
vant example of noise encountered in Wikipedia text. 
 
First Paragraph (FPAR): We examined several 
hundred pages, and observed that a human could 
label most of the pages by reading only the first 
paragraph. Therefore, we built the feature vector 
that contains the bag-of-word representation of the 
page?s first paragraph. 
Abstract (ABS): For each page, Wikipedia pro-
vides a summary of several lines about the entity 
described on the page. We use this summary to 
draw another bag-of-word feature vector based on 
the provided abstracts only. For example, Figure 3 
shows the abstract for the entity ?Gwen Stefani?. 
Surface Forms and Disambiguations (SFD): 
Contributors use the Wikipedia syntax to link from 
one entity page to another. In the page of Figure 2, 
for example, we have references to several other 
Wikipedia entities, such as ?hip hop?, ?R&B?, and 
?Bush?. Wikipedia page syntax lets us extract the 
disambiguated meaning of each of these references, 
which are ?Hip hop music,? ?Rhythm and blues,? 
and ?Bush band?, respectively. For each page, we 
extract all the surface forms used by contributors in 
text (such as ?hip hop?) and their disambiguated 
meanings (such as ?Hip hop music?), and build 
feature vectors to represent them. 
4.2 Context Features 
Figure 1 shows some of the ways contributors to 
Wikipedia refer to the entity ?Gwen Stefani?. The 
Wikipedia version that we analyzed contains about 
35 million references to entities in the collection. 
On average, each page has five references to other 
entities. 
We decided to make use of the text surrounding 
these references to draw contextual features, which 
can capture both syntactic and semantic properties 
of the referenced entity. For each entity reference, 
we compute the feature vectors by using a text 
window of three words to the left and to the right 
of the reference. 
<abstract> 
Gwen Rene StefaniSome sources give Stefani?s first name 
as Gwendolyn, but her first name is simply Gwen. Her list-
ing on the California Birth Index from the Center for Health 
Statistics gives a birth name of Gwen Rene Stefani. 
</abstract> 
548
BOW 1,821,966 ABS 372,909 
SFD 847,857 BCON 35,178,120 
STRUCT 159,645 FPAR 781,938 
Table 1. Number of features in each group, as obtained 
by examining all the Wikipedia pages. 
 
We derived a unigram context model and a bigram 
context model, following the findings of previous 
work that such models benefit from employing 
information about the position of words relative to 
the targeted term: 
Unigram Context (UCON): The feature vector 
is constructed in a way that preserves the positional 
information of words in the context. Each feature fti 
in the vector represents the total number of times a 
term t appears in position i around the entity. 
Bigram Context (BCON): The bigram-based 
context model was built in a similar way to UCON, 
so that relative positional information is preserved. 
5 Challenges 
For our classification task, we faced several 
challenges. First, many Wikipedia entities have 
only a partial list of the feature groups discussed 
above. For example, contributors may refer to enti-
ties that do not exist in Wikipedia but might be 
added in the future. Also, not all the page-based 
features groups are available for every entity page. 
For instance, abstracts and structure features are 
only available for 68% and 79% of the pages, re-
spectively. Second, we only had available several 
hundred labeled examples (as described in Section 
6.1). Third, the feature space is very large com-
pared to the typical text classification problem (see 
Table 1), and a substantial amount of noise plagues 
the data. A further investigation revealed that the 
difference in the dimensionality compared to text 
classification stems from the way Wikipedia pages 
are created: contributors make spelling errors, in-
troduce new words, and frequently use slang, acro-
nyms, and other languages than English. 
We utilize all the features groups described in 
Section 4 and various combinations of them. This 
provides us with greater flexibility to use classifi-
ers trained on different feature groups when 
Wikipedia entities miss certain types of features. 
In addition, we try to take advantage of the inde-
pendent views of each entity by employing a co-
training procedure (Blum and Mitchell, 1998; Ni-
gam and Ghani, 2000). In previous work, this has 
been shown to boost the performance of the weak 
classifiers on certain feature groups. For example, 
it is interesting to determine whether we can use 
the STRUCT view of a Wikipedia pages to boost 
the performance of the classifiers based on context. 
Alternatively, we can employ co-training on the 
STRUCT and SFD features, hypothesized as two 
independent views of the data. 
6 Experiments and Findings 
6.1 Training Data 
We experimented with two data sets: Human 
Judged Data (HJD): This set was obtained in an 
annotation effort that followed the guidelines pre-
sented in Section 3. Due to the cost of the labeling 
procedure, this set was limited to a small random 
set of 800 Wikipedia pages. Human Judged Data 
Extended (HJDE): The initial classification results 
obtained using a small subset of HJD hinted to the 
need for more training data. Therefore, we devised 
a procedure that takes advantage of the fact that 
Wikipedia contributors have assigned many of the 
pages to one or more lists. For example, the page 
?List of novelists? contains a reference to ?Orhan 
Pamuk?, which is part of the HJD and is labeled as 
PER. Our extension procedure first uses the pages 
in the training set from HJD to extract the lists in 
Wikipedia that contain references to them and then 
projects the entity labels of the seeds to all ele-
ments in the lists. Unfortunately, not all the 
Wikipedia lists contain only references named enti-
ties of the same category. Furthermore, some lists 
are hierarchical and include sub-lists of different 
classes. To overcome these issues, we examined 
only leaf lists and manually filtered all the lists that 
by definition could have pages of different catego-
ries. Finally, we filtered out all list pages that con-
tain entities in two or more entity classes (as de-
scribed in Section 3). 
Our partially manual extension procedure is as 
follows: 1) Pick a random sample of 400 entities 
from HJD along with their human judged labels; 2) 
Extract all the lists that contain any entity from this 
labeled sample; 3) Filter out the lists that contain 
entities from different entity classes (PER, ORG, 
LOC, MISC, and COM); 4) propagate the entity 
labels of the known entities in the lists to the other 
referenced entities; 5) Choose a random sample 
from all labeled pages with respect to the entity 
class distribution observed in HJD. 
549
PER MISC ORG LOC COMM 
41% 25.1% 11.2% 11.7% 11% 
Table 2. The distribution of labels in the HJDE data set. 
Our extension procedure resulted initially in 770 
lists, which were then reduced to 501. In step (5), 
we chose a maximal random sample from all la-
beled pages in HJDE so that it matched the entity 
class distribution in the original HJD training set 
(shown in Table 2). 
6.2 Classification 
From the numerous machine learning algorithms 
available for our classification task (e.g., Joachims, 
1999; Quinlan, 1993; Cohen, 1995), we chose to 
the SVMs (Vapnik, 1995), and the Na?ve Bayes 
(John and Langley, 1995) algorithms because both 
can output probability estimates for their predic-
tions, which are necessary for the co-training pro-
cedure. We use an implementation of SVM (Platt, 
1999) with linear kernels and the Na?ve Bayes im-
plementation from the machine learning toolkit 
Weka3. Our implementation of co-training fol-
lowed that of Nigam and Ghani (2000). 
Using the HJDE data, we experimented with 
learning a classifier for each feature group dis-
cussed in Section 4. We report the results for two 
classification tasks: binary classification to identify 
all the Wikipedia pages of type PER, and 5-fold 
classification (PER, COM, ORG, LOC, and MISC). 
To reduce the feature space, we built a term fre-
quency dictionary taken from one year?s worth of 
news data and restrict our feature space to contain 
only terms with frequency values higher than 10. 
6.3 Results on Bag-of-words 
This feature group is of particular interest, since it 
has been widely used for document classification 
and also, because every Wikipedia page has a 
BOW representation. We experimented with the 
two classification tasks for this feature group. For 
the binary classification task, both SVM and Na?ve 
Bayes performed remarkably well, obtaining accu-
racies of 0.962 and 0.914, respectively. Table 3 
shows detailed performance numbers for SVM and 
Na?ve Bayes for the multi-class task. Unlike in the 
binary case, Na?ve Bayes falls short of achieving 
results similar to those from SVM, which obtains 
an average F-measure of 0.928 and an average pre-
cision of 0.931. 
 Precision Recall F-measure 
 SVM NB SVM NB SVM NB 
PER 0.944 0.918 0.959 0.771 0.951 0.838 
MISC 0.927 0.824 0.920 0.687 0.924 0.750 
ORG 0.940 0.709 0.928 0.701 0.934 0.705 
LOC 0.958 0.459 0.949 0.863 0.954 0.599 
COMM 0.887 0.680 0.869 0.714 0.878 0.697 
Table 3. Precision, recall, and F1 measure for the multi-
class classification task. Results are obtained using 
SVM and Na?ve Bayes after a stratified cross-validation 
using HJDE data set and the bag-of-words features. 
 
SFD 83.14% ABS 68.96% 
STRUCT 79.55% BCON 83.57% 
Table 4. Percentage of available examples HJDE for 
each feature group. 
 
 Precision Recall F-measure 
 SVM NB SVM NB SVM NB 
BOW 0.901 0.858 0.894 0.880 0.897 0.869 
SFD 0.851 0.775 0.830 0.882 0.840 0.825 
STRUCT 0.888 0.840 0.875 0.856 0.881 0.848 
FPAR 0.867 0.872 0.854 0.896 0.860 0.884 
ABS 0.861 0.833 0.852 0.885 0.857 0.858 
BCON 0.311 0.245 0.291 0.334 0.300 0.283 
Table 5. Average precision, recall, and F1 measure val-
ues for the multi-class task. Results are obtained using 
SVM and Na?ve Bayes across the different feature 
groups on the test set of HJDE. 
6.4 Results on Other Feature Groups 
We present now the results obtained using other 
groups of features. We omit the results on UCON 
due to their similarity with BCON. Recall that 
these features may not be present in all Wikipedia 
pages. Table 4 shows the availability of these fea-
tures in the HJDE set. The lack of one feature 
group has a negative impact on the results of the 
corresponding classifier, as shown in Table 5. No-
ticeably, the results of the STRUCT features are 
very encouraging and confirm our hypothesis that 
such features are distinctive in identifying the type 
of the page. While results using STRUCT and 
FPAR are high, they are lower than the results ob-
tained on BOW. In general, using SVM with BOW 
performed better than any other feature set, averag-
ing 0.897 F-measure on test set. This could be be-
cause when using BOW, we have a larger training 
set than any other feature group. SVM with 
STRUCT and Na?ve Bayes with FPAR performed 
550
second and third best, with average F1 measure 
values of 0.881 and 0.860, respectively. The results 
also show that it is difficult to learn if a page is 
COMM in all learning combination. This could be 
related to the membership complexity of that class. 
Finally, the results on the bigram contextual fea-
tures, namely BCON, for both SVM and Na?ve 
Bayes are not encouraging and surprisingly low. 
6.5 Results for Co-training 
Motivated by the fact that some feature groups can 
be seen as independent views of the data, we used 
a co-training procedure to boost the classification 
accuracy. One combination of views that we exam-
ined is BCON with BOW, hoping to boost the 
classification performance of the bigram context 
features, as this classifier could be used for entities 
in any new text, not only for Wikipedia pages . 
Unfortunately, the results were not encouraging in 
either of the cases (SVM and Na?ve Bayes) and for 
none of the other feature groups used instead of 
BOW. This indicates that the context features ex-
tracted have limited power and that further investi-
gation of extracting relevant context features from 
Wikipedia is necessary. 
7 Conclusions and Future Work 
In this paper, we presented a study on the classifi-
cation of Wikipedia pages with named entity labels. 
We explored several alternatives for extracting 
useful page-based and context-based features such 
as the traditional bag-of-words, page structure, hy-
perlink text, abstracts, section titles, and n-gram 
contextual features. While the classification with 
page features resulted in high classification accu-
racy, context-based and structural features did not 
work similarly well, either alone or in a co-training 
setup. This motivates future work to extract better 
such features. We plan to examine employing more 
sophisticated ways both for extracting contextual 
features and for using the implicit Wikipedia graph 
structure in a co-training setup. 
Recently, the Wikipedia foundation has been 
taken steps toward enforcing a more systematic 
way to add useful structured data on each page by 
suggesting templates to use when a new page gets 
added to the collection. This suggests that in a not-
so-distant future, we may be able to utilize the 
structured data features as attribute-value pairs 
rather than as bags of words, which is prone to los-
ing valuable semantic information. 
Finally, we have applied our classifier to all 
Wikipedia pages to determine their labels and 
made these data available in the form of a Web 
service, which can positively contribute to future 
studies that employ the Wikipedia collection. 
References 
ACE Project. At http://www.nist.gov/speech/history/in-
dex.htm 
Reuters-1997. 1997. Reuters-21578 text categorization 
test collection. At http://www.daviddlewis.com/re-
sources/testcollections/reuters21578 
A. Blum and T. Mitchell. 1998. Combining labeled and 
unlabeled data with co-training. In Proceedings of  
COLT?98, pages 92?100. 
A. Borthwick, J. Sterling, E. Agichtein, and R. Grish-
man. 1998. NYU: Description of the MENE named 
entity system as used in MUC. In Proceedings of 
MUC-7. 
R. Bunescu and M. Pasca. 2006. Using encyclopedic 
knowledge for named entity disambiguation. In Pro-
ceedings of EACL-2006, pages 9?16. 
S. Chakrabarti, M.M. Joshi, K. Punera, and D.M. Pen-
nock. 2002. The structure of broad topics on the web. 
In: Proceedings of WWW ?02, pages 251?262. 
W.W. Cohen. 1995. Fast effective rule induction. In 
Proceedings of ICML?95. 
S. Cucerzan. 2007. Large-Scale Named Entity Disam-
biguation Based on Wikipedia Data. In Proceedings 
of EMNLP-CoNLL 2007, pages 708?716. 
S. Cucerzan and D. Yarowsky. 2002. Language Inde-
pendent NER using a Unified Model of Internal and 
Contextual Evidence, in Proceedings of CoNLL 2002, 
pages 171?174. 
W. Dakka and P. G. Ipeirotis. 2008. Automatic Extrac-
tion of Useful Facet Terms from Text Documents. In 
Proceedings of ICDE 2008 (to appear). 
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, 
S. Strassel, and R. Weischedel. 2004. ACE pro-
gram ? task definitions and performance measures. In 
Proceedings of LREC, pages 837?840. 
S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 
1998. Inductive learning algorithms and representa-
tions for text categorization. In Proceedings of 
CIKM ?98, pages 148?155. 
M. Fleischman and E. Hovy. 2002. Fine Grained Classi-
fication of Named Entities. In Proceedings of COL-
ING?02, pages 267?273. 
551
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang, 
Named Entity Recognition through Classifier Com-
bination, in Proceedings of CoNLL 2003, pages 168?
171. 
E. Gabrilovich and S. Markovitch. 2004. Text categori-
zation with many redundant features: using aggres-
sive feature selection to make SVMs competitive 
with c4.5. In Proceedings of ICML ?04, page 41. 
E. Gabrilovich and S. Markovitch. 2006. Overcoming-
the brittleness bottleneck using Wikipedia: Enhanc-
ing text categorization with encyclopedic knowledge. 
In Proceedings of AAI 2006. 
R. Grishman and B. Sundheim. 1996. Message Under-
standing Conference - 6: A brief history. In Proceed-
ings of COLING, 466-471. 
T. Joachims. 1998. Text categorization with support 
vector machines: Learning with many relevant fea-
tures. In Proceedings of ECML ?98, pages 137?142. 
T. Joachims. 1999. Making large-scale support vector 
machine learning practical. Advances in kernel meth-
ods: support vector learning, pages 169?184. 
G.H. John and P. Langley. 1995. Estimating continuous 
distributions in Bayesian classifiers. Proceedings of 
the Eleventh Conference on Uncertainty in Artificial 
Intelligence, pages 338?345. 
D. Klein, J. Smarr, H. Nguyen, and C. D. Manning. 
2003. Named Entity Recognition with Character-
Level Models, in Proceedings of CoNLL 2003. 
K. Lang. 1995. NewsWeeder: Learning to filter netnews. 
In Proceedings of ICML?95, pages 331?339. 
D. Mladenic. 1998. Feature subset selection in text 
learning. In Proceedings of ECML ?98, pages 95?100. 
K. Nigam and R. Ghani. 2000. Analyzing the effective-
ness and applicability of co-training. In Proceedings 
of CIKM?00, pages 86?93. 
S.E. Overell and S. Ruger. 2006. Identifying and 
grounding descriptions of places. In Workshop on 
Geographic Information Retrieval, SIGIR 2006. 
J.C. Platt. 1999. Fast training of support vector ma-
chines using sequential minimal optimization. Ad-
vances in kernel methods: support vector learning, 
pages 185?208. 
M. Poe. 2006. The hive: Can thousands of wikipedians 
be wrong? How an attempt to build an online ency-
clopedia touched off history?s biggest experiment in 
collaborative knowledge. The Atlantic Monthly, Sep-
tember 2006. 
J.R. Quinlan. 1993. C4.5: programs for machine learn-
ing. Morgan Kaufmann Publishers Inc. 
M. Rogati and Y. Yang. 2002. High-performing feature 
selection for text classification. In Proceedings of 
CIKM ?02, pages 659?661. 
F. Sebastiani. 2002. Machine learning in automated text 
categorization. ACM Computing. Surveys, 34(1):1?47. 
F.M. Suchanek, G. Kasneci, and G. Weikum. 2007. 
Yago: A Core of Semantic Knowledge. In Proceed-
ings of WWW 2007. 
E.F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of CoNLL-2003, pages 142?147. 
E. F. Tjong Kim Sang. 2002. Introduction to the 
CoNLL-2002 shared task: Language-independent 
named entity recognition. In Proceedings of CoNLL-
2002, pages 155?158. 
V.N. Vapnik. 1995. The nature of statistical learning 
theory. Springer-Verlag New York, Inc. 
N. Wacholder., Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text. In Proceedings of 
ANLP?97, pages 202-208. 
Y. Watanabe, M. Asahara, and Y. Matsumoto. 2007. A 
Graph-based Approach to Named Entity Categoriza-
tion in Wikipedia using Conditional Random Fields. 
In Proc. of EMNLP-CoNLL 2007, pages 649-657. 
Y. Yang and X. Liu. 1999. A re-examination of text 
categorization methods. In Proceedings of SIGIR ?99, 
pages 42?49. 
Y. Yang, S. Slattery, and R. Ghani. 2002. A study of 
approaches to hypertext categorization. Journal of. 
Intelligent. Information. Systems., 18(2-3):219?241. 
 
552
Spelling correction as an iterative process 
that exploits the collective knowledge of web users 
 
Silviu Cucerzan and Eric Brill 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{silviu,brill}@microsoft.com 
 
 
 
Abstract 
Logs of user queries to an internet search engine pro-
vide a large amount of implicit and explicit informa-
tion about language. In this paper, we investigate 
their use in spelling correction of search queries, a 
task which poses many additional challenges beyond 
the traditional spelling correction problem. We pre-
sent an approach that uses an iterative transformation 
of the input query strings into other strings that corre-
spond to more and more likely queries according to 
statistics extracted from internet search query logs. 
1 Introduction 
The task of general purpose spelling correction has 
a long history (e.g. Damerau, 1964; Rieseman and 
Hanson, 1974; McIlroy, 1982), traditionally focus-
ing on resolving typographical errors such as in-
sertions, deletions, substitutions, and 
transpositions of letters that result in unknown 
words (i.e. words not found in a trusted lexicon of 
the language). Typical word processing spell 
checkers compute for each unknown word a small 
set of in-lexicon alternatives to be proposed as 
possible corrections, relying on information about 
in-lexicon-word frequencies and about the most 
common keyboard mistakes (such as typing m in-
stead of n) and phonetic/cognitive mistakes, both 
at word level (e.g. the use of acceptible instead of 
acceptable) and at character level (e.g. the misuse 
of f instead of ph). Very few spell checkers attempt 
to detect and correct word substitution errors, 
which refer to the use of in-lexicon words in inap-
propriate contexts and can also be the result of 
both typographical mistakes (such as typing coed 
instead of cord) and cognitive mistakes (e.g. prin-
cipal and principle). Some research efforts to 
tackle this problem have been made; for example 
Heidorn et al (1982) and Garside et al (1987) de-
veloped systems that rely on syntactic patterns to 
detect substitution errors, while Mays et al (1991) 
employed word co-occurrence evidence from a 
large corpus to detect and correct such errors.   
The former approaches were based on the imprac-
tical assumption that all possible syntactic uses    
of all words (i.e. part-of-speech) are known, and 
presented both recall and precision problems be-
cause many of the substitution errors are not syn-
tactically anomalous and many unusual syntactic 
constructions do not contain errors. The latter ap-
proach had very limited success under the assump-
tions that each sentence contains at most one 
misspelled word, each misspelling is the result of a 
single point change (insertion, deletion, substitu-
tion, or transposition), and the defect rate (the rela-
tive number of errors in the text) is known. A 
different body of work (e.g. Golding, 1995; Gold-
ing and Roth, 1996; Mangu and Brill, 1997) fo-
cused on resolving a limited number of cognitive 
substitution errors, in the framework of context 
sensitive spelling correction (CSSC). Although 
promising results were obtained (92-95% accu-
racy), the scope of this work was very limited as it 
only addressed known sets of commonly confused 
words, such as {peace, piece}). 
1.1 Spell Checking of Search Engine Queries 
The task of web-query spelling correction ad-
dressed in this work has many similarities to tradi-
tional spelling correction but also poses additional 
challenges. Both the frequency and severity of 
spelling errors for search queries are significantly 
greater than in word processing.  Roughly 10-15% 
of the queries sent to search engines contain errors. 
Typically, the validity of a query cannot be de-
cided by lexicon look-up or by checking its gram-
maticality. Because web queries are very short (on 
average, less than 3 words), techniques that use a 
multitude of features based on relatively wide con-
text windows, such as those investigated in CSSC, 
are difficult to apply. Rather than being well-
formed sentences, most queries consist of one con-
cept or an enumeration of concepts, many times 
containing legitimate words that are not found in 
any traditional lexicon. 
 Just defining what a valid web query is represents 
a difficult enterprise. We clearly cannot use only a 
static trusted lexicon, as many new names and 
concepts (such as aznar, blog, naboo, nimh, nsync, 
and shrek) become popular every day and it would 
be extremely difficult if not impossible to maintain 
a high-coverage lexicon. In addition, employing 
very large lexicons can result in more errors sur-
facing as word substitutions, which are very diffi-
cult to detect, rather than as unknown words. 
 One alternative investigated in this work is to ex-
ploit the continuously evolving expertise of mil-
lions of people that use web search engines, as 
collected in search query logs (seen as histograms 
over the queries received by a search engine). In 
some sense, we could say that the validity of a 
word can be inferred from its frequency in what 
people are querying for, similarly to Wittgen-
stein?s (1968) observation that ?the meaning of a 
word is its use in the language?. Such an approach 
has its own caveats. For example, it would be er-
roneous to simply extract from web-query logs all 
the queries whose frequencies are above a certain 
value and consider them valid. Misspelled queries 
such as britny spears are much more popular than 
correctly spelled queries such as bayesian nets and 
amd processors. Our challenge is to try to utilize 
query logs to learn what queries are valid, and to 
build a model for valid query probabilities, despite 
the fact that a large percentage of the logged que-
ries are misspelled and there is no trivial way to 
determine the valid from invalid queries. 
2 Problem Formulation. Prior Work 
Comprehensive reviews of the spelling correction 
literature were provided by Peterson (1980), 
Kukich (1992), and Jurafsky and Martin (2000). In 
this section, we survey a few lexicon-based spell-
ing correction approaches by using a series of for-
mal definitions of the task and presenting concrete 
examples showing the strengths and the limits cor-
responding to each situation. We iteratively rede-
fine the problem, starting from an approach purely 
based on a trusted lexicon and ending up with an 
approach in which the role of the trusted lexicon is 
greatly diminished. While doing so, we also make 
concrete forward steps in our attempt to provide a 
definition of valid web queries.  
 Let ?  be the alphabet of a language and *??L a 
broad-coverage lexicon of the language. The sim-
plest and historically the first definition of lexicon-
based spelling correction (Damerau, 1964) is: 
Given an unknown word Lw \*?? , find Lw ?'  
such that ),(min)',( vwdistwwdist
Lv?
= . 
i.e. for any out-of-lexicon word in a text, find the 
closest word form(s) in the available lexicon and 
hypothesize it as the correct spelling alternative. 
dist  can be any string-based function; for exam-
ple, it can be the ratio between the number of let-
ters two words do not have in common and the 
number of letters they share.1 The two most used 
classes of distances in spelling correction are edit 
distances, as proposed by Damerau (1964) and 
Levenshtein (1965), and correlation matrix dis-
tances (Cherkassky et al, 1974). In our study, we 
use a modified version of the Damerau-Lev-
enshtein edit distance, as presented in Section 3. 
 One flaw of the preceding formulation is that it 
does not take into account the frequency of words 
in a language. A simple solution to this problem is 
to compute the probability of words in the target 
language as maximum likelihood estimates (MLE) 
over a large corpus and reformulate the general 
spelling-correction problem as follows: 
Given Lw \*?? , find Lw ?'  such that 
??)',( wwdist and )(max)'(
),(:
vPwP
vwdistLv ???
= . 
 In this formulation, all in-lexicon words that are 
within some ?reasonable? distance ?  of the un-
known word are considered as good candidates, 
the correction being chosen based on its prior 
probability in the language. While there is an im-
plicit conditioning on the original spelling because 
of the domain on which the best correction is 
searched, this objective function only uses the 
prior probability of words in the language and not 
the actual distances between each candidate and 
the input word  
 One solution that allows using a probabilistic edit 
distance is to condition the probability of a correc-
tion on the original spelling )|( wvP :  
Given Lw \*?? , find Lw ?'  such that 
??)',( wwdist and )|(max)|'(
),(:
wvPwwP
vwdistLv ???
= . 
  In a noisy channel model framework, as em-
ployed for spelling correction by Kernigham et al 
(1990), the objective function can be written by 
using Bayesian inversion as the product between 
the prior probability of words in a language )(vP  
(the language model), and the likelihood of mis-
spelling a word v as w, )|( vwP  (which models the 
noisy channel and will be called the error model). 
In the above formulations, unknown words are   
corrected in isolation. This is a rather major flaw 
because context is extremely important for spelling 
correction, as illustrated in the following example: 
power crd  power cord 
video crd  video card 
                                                          
1
 Note that the function does not have to be symmetric; thus, 
the notation dist(w,w?) is used with a loose sense. 
 The misspelled word crd should be corrected to 
two different words depending on its contexts.2   
 A formulation of the spelling correction problem 
that takes into account context is the following: 
Given a string *??s , rl wccs = , with Lw \
*??  
and *, Lcc rl ? , find Lw ?'  such that ??)',( wwdist  
and )|(max)|'(
),(: rlvwdistLvrl
wccvPwccwP
???
= . 
 Spaces and other word delimiters are ignored in 
this formulation and the subsequent formulations 
for simplicity, although text tokenization repre-
sents an important part of the spelling-correction 
process, as discussed in Sections 5 and 6. 
 The task definitions enumerated up to this point 
(on which most traditional spelling correction sys-
tems are based) ignore word substitution errors. In 
the case of web searches, it is extremely important 
to provide correction suggestions for valid words 
when they are more meaningful as a search query 
than the original query, for example: 
golf war  gulf war 
sap opera  soap opera 
 This problem is partially addressed by the task of 
CSSC, which can be formalized as follows: 
Given a set of confusable valid word forms          
in a language },...,,{ 21 nwwwW =  and a string 
ril cwcs = , choose Ww j ?  such that 
)|(max)|(
..1 rilknkrilj
cwcwPcwcwP
=
= . 
 In the CSSC literature, the sets of confusables are 
presumed known, but they could also be built for 
each in-lexicon word w as all words 'w  with 
??)',( wwdist , similarly to the approach investi-
gated by Mays et al (1991), in which they chose a 
1=?  and employed an edit distance with all point 
changes having the same cost 1. 
 The generalized problem of phrasal spelling cor-
rection can then be formulated as follows: 
Given *??s , find *' Ls ?  such that ??)',( ssdist  
and )|(max)|'(
),(:*
stPssP
tsdistLt ???
= . 
 Typically, a correction is desirable when *Ls ?  
(i.e. at least one of the component words is un-
known) but, as shown above, there are frequent 
cases (e.g. golf war) when sequences of valid 
words should be changed to other word sequences. 
Note that word boundaries are hidden in this latter 
                                                          
2
 To simplify the exposition, we only consider two highly 
probable corrections, but other valid alternatives exist, e.g. 
video cd. 
formulation, making it more general and allowing 
it to cover two other important spelling error 
classes, concatenation and splitting, e.g.: 
power point slides  powerpoint slides 
chat inspanich   chat in spanish 
 Yet, it still does not account for another important 
class of cases in web query correction which is 
represented by out-of-lexicon words that are valid 
in certain contexts (therefore, *' Ls ? ), for example: 
amd processors  amd processors (no change) 
 The above phrase represents a legitimate query, 
despite the fact that it may contain unknown words 
when employing a traditional English lexicon.  
 Some even more interesting cases not handled by 
traditional spellers and also not covered by the 
latter formulation are those in which in-lexicon 
words should be changed to out-of-lexicon words, 
as in the following examples, where two valid 
words must be concatenated into an out of lexicon 
word: 
gun dam planet  gundam planet 
limp biz kit  limp bizkit 
 These observations lead to an even more general 
formulation of the spelling-correction problem: 
Given *??s , find *' ??s  such that ??)',( ssdist  
and )|(max)|'(
),(:*
stPssP
tsdistt ????
= . 
 For the first time, the formulation no longer 
makes explicit use of a lexicon of the language.3 In 
some sense, the actual language in which the web 
queries are expressed becomes less important than 
the query-log data from which the string probabili-
ties are estimated. This probability model can be 
seen as a substitute for a measure of the meaning-
fulness of strings as web-queries. For example, an 
implausible random noun phrase in any of the tra-
ditional corpora such as sad tomatoes is meaning-
ful in the context of web search (being the name of 
a somewhat popular music band). 
3 The Error Model. String Edit Functions 
All formulations of the spelling correction task 
given in the previous section used a string distance 
function and a threshold to restrict the space in 
which alternative spellings are searched. Various 
previous work has addressed the problem of 
choosing appropriate functions (e.g. Kernigham et 
al. 1990, Brill and Moore, 2002; Toutanova and 
Moore, 2003). 
                                                          
3
 A trusted lexicon may still be used in the estimation of the 
language model probability for the computation of )|( stP . 
 The choice of distance function d and threshold ? 
could be extremely important for the accuracy of a 
speller. At one extreme, the use of a too restrictive 
function/threshold combination can result in not 
finding the best correction for a given query. For 
example, using the vanilla Damerau-Levenshtein 
edit distance (defined as the minimum number of 
point changes required to transform a string into 
another, where a point change is one of the follow-
ing operations: insertion of a letter, deletion of a 
letter, and substitution of one letter with another 
letter) and a threshold 1=? , the correction donadl 
duck  donald duck would not be possible. At the 
other extreme, the use of a less limiting function 
might have as consequence suggesting very 
unlikely corrections. For example, using the same 
classical Levenshtein distance and 2=?  would 
allow the correction of the string donadl duck, but 
will also lead to bad corrections such as log wood 
 dog food (based on the frequency of the queries, 
as incorporated in )(sP ).  Nonetheless, large dis-
tance corrections are still desirable in a diversity of 
situations, for example: 
platnuin rings   platinum rings 
ditroitigers   detroit tigers 
 The system described in this paper makes use of a 
modified context-dependent weighted Damerau-
Levenshtein edit function which allows insertion, 
deletion, substitution, immediate transposition, and 
long-distance movement of letters as point 
changes, for which the weights were interactively 
refined using statistics from query logs. 
4 The Language Model. Exploiting Large 
Web Query Logs 
A misspelling such as ditroitigers is far from the 
correct alternative and thus, it might be extremely 
difficult to find its correct spelling based solely on 
edit distance. Nonetheless, the correct alternative 
could be reached by allowing intermediate valid 
correction steps, such as ditroitigers  detroitti-
gers  detroit tigers. But what makes detroittigers 
a valid correction step? Recall that the last formu-
lation of spelling correction in Section 3 did not 
explicitly use a lexicon of the language. Rather, 
any string that appears in the query log used for 
training can be considered a valid correction and 
can be suggested as an alternative to the current 
web query based on the relative frequency of the 
query and the alternative spelling. Thus, a spell 
checker built according to this formulation could 
suggest the correction detroittigers because this 
alternative occurs frequently enough in the em-
ployed query log. However, detroittigers itself 
could be corrected to detroit tigers if presented as 
a stand-alone query to this spell checker, based on 
similar query-log frequency facts, which naturally 
leads to the idea of an iterative correction ap-
proach. 
 
albert einstein 4834 
albert einstien 525 
albert einstine 149 
albert einsten 27 
albert einsteins 25 
albert einstain 11 
albert einstin 10 
albert eintein 9 
albeart einstein 6 
aolbert einstein 6 
alber einstein 4 
albert einseint 3 
albert einsteirn 3 
albert einsterin 3 
albert eintien 3 
alberto einstein 3 
albrecht einstein 3 
alvert einstein 3 
Table 1. Counts of different (mis)spellings of Albert          
Einstein?s name in a web query log.  
 Essential to such an approach are three typical 
properties of the query logs (e.g. see Table 1): 
? words in the query logs are misspelled in vari-
ous ways, from relatively easy-to-correct mis-
spellings to very-difficult-to-correct ones, that 
make the user?s intent almost impossible to 
recognize;  
? the less malign (difficult to correct) a misspell-
ing is the more frequent it is; 
? the correct spellings tend to be more frequent 
than misspellings. 
 In this context, the spelling correction problem 
can be given the following iterative formulation: 
Given a string *0 ??s , find a sequence    
*
21 ,..., ??nsss   such that  ??+ ),( 1ii ssdist , 
)|(max)|(
),(:1 * itsdisttii
stPssP
i ????
+ = , 1..0 ??? ni , 
and )|(max)|(
),(:* ntsdisttnn
stPssP
n ????
= . 
 An example of correction that can be made by   
iteratively applying the base spell checker is: 
anol scwartegger   arnold schwarzenegger 
Misspelled query: anol scwartegger 
First iteration: arnold schwartnegger 
Second iteration: arnold schwarznegger 
Third iteration: arnold schwarzenegger 
Fourth iteration: no further correction 
 Up to this point, we underspecified the notion of 
string in the task formulations given. One possibil-
ity is to consider whole queries as the strings to be 
corrected and iteratively search for better logged 
queries according to the agreement between their 
relative frequencies and the character error model. 
This is equivalent to identifying all queries in the 
query log that are misspellings of other queries and 
for any new query, find a correction sequence of 
logged queries. While such an approach exploits 
the vast information available in web-query logs, it 
only covers exact matches of the queries that ap-
pear in these logs and provides a low coverage of 
infrequent queries. For example, a query such as 
britnet spear inconcert could not be corrected if 
the correction britney spears in concert does not 
appear in the employed query log, although the 
substring britnet spear could be corrected to brit-
ney spears. 
 To address the shortcomings of such an approach, 
we propose a system based on the following for-
mulation, which uses query substrings: 
Given *0 ??s , find a sequence 
*
21 ,..., ??nsss , 
such that for each 1..0 ?? ni  there exist the de-
compositions ii lii
l
iii wwwws 1,1
1
1,11i0,
1
0, ...s ,... +++ == , 
where k hjw ,  are words or groups of words such that 
??+ ),( 1,10, kiki wwdist , ilkni ..1  ,1..0 ?????  and 
)|(max)|(
** ),(:1 itsdisttii
stPssP
i ????
+ = , 1..0 ??? ni , 
and )|(max)|(
** ),(: ntsdisttnn
stPssP
n ????
= . 
Note that the length of the string decomposition 
may vary from one iteration to the next one, for 
example: 
 
 In the implementation evaluated in this paper, we 
allowed decompositions of query strings into 
words and word bigrams. The tokenization process 
uses space and punctuation delimiters in addition 
to the information provided about multi-word 
compounds (e.g. add-on and back-up) by a trusted 
English lexicon with approximately 200k entries. 
By using the tokenization process described above, 
we extracted word unigram and bigram statistics 
from query logs to be used as the system?s lan-
guage model. 
5 Query Correction 
An input query is tokenized using the same space 
and word-delimiter information in addition to the 
available lexical information as used for process-
ing the query log. For each token, a set of alterna-
tives is computed using the weighted Levenshtein 
distance function described in Section 3 and two 
different thresholds for in-lexicon and out-of-
lexicon tokens 
 Matches are searched in the space of word uni-
grams and bigrams extracted from query logs in 
addition to the trusted lexicon. Unigrams and bi-
grams are stored in the same data structure on 
which the search for correction alternatives is 
done. Because of this, the proposed system han-
dles concatenation and splitting of words in ex-
actly the same manner as it handles 
transformations of words to other words. 
 Once the sets of all possible alternatives are com-
puted for each word form in the query, a modified 
Viterbi search (in which the transition probabilities 
are computed using bigram and unigram query-log 
statistics and output probabilities are replaced with 
inverse distances between words) is employed to 
find the best possible alternative string to the input 
query under the following constraint: no two adja-
cent in-vocabulary words are allowed to change 
simultaneously. This constraint prevents changes 
such as log wood  dog food. An algorithmic con-
sequence of this constraint is that there is no need 
to search all the possible paths in the trellis, which 
makes the modified search procedure much faster, 
as described further. We assume that the list of 
alternatives for each word is randomly ordered but 
the input word is on the first position of the list 
when the word is in the trusted lexicon. In this 
case, the searched paths form what we call fringes. 
Figure 1 presents an example of a trellis in which 
w1, w2 and w3 are in-lexicon word forms. Observe 
that instead of computing the cost of k1k2 possible 
paths between the alternatives corresponding to w1 
and w2, we only need to compute the cost of k1+k2 
paths. 
31 =l  
42 =l  
20 =l  0s  britenetspear   inconcert 
 
1s  britneyspears  in concert 
 
2s  britney spears in concert 
 
3s  britney spears in concert 
11
2
1
1
1
1ka
a
a
w

2
2
2
2
1
2
2ka
a
a
w

3
3
2
3
1
3
3ka
a
a
w

4
4
2
4
1
4
4ka
a
a
w

5
5
2
5
1
5
5ka
a
a
w

6
6
2
6
1
6
6ka
a
a
w

7
7
2
7
1
7
7ka
a
a
w

sto
p w
or
d
un
kno
wn
 
wo
rd
 
Figure 1. Example of trellis of the modified Viterbi search 
 Because we use word-bigram statistics, stop 
words such as prepositions and conjunctions may 
interfere negatively with the best path search. For 
example, in correcting a query such as platunum 
and rigs, the language model based on word bi-
grams would not provide a good context for the 
word form rigs. 
 To avoid this type of problems, stop words and 
their most likely misspelling are given a special 
treatment. The search is done by first ignoring 
them, as in Figure 1, where w4 is presumed to be 
such a word. Once a best path is found by ignoring 
stop words, the best alternatives for the skipped 
stop words (or their misspellings) are computed in 
a second Viterbi search with fringes in which the 
extremities are fixed, as presented in Figure 2. 
 
1
1
2
1
1
1
1ka
a
a
w

2
2
2
2
1
2
2ka
a
a
w

3
3
2
3
1
3
3ka
a
a
w

4
4
2
4
1
4
4ka
a
a
w

5
5
2
5
1
5
5ka
a
a
w

6
6
2
6
1
6
6ka
a
a
w

7
7
2
7
1
7
7ka
a
a
w

sto
p w
or
d
 
Figure 2. Modified Viterbi search ? stop-word treatment 
 The approach of search with fringes coupled with 
an iterative correction process is both very effi-
cient and very effective. In each iteration, the 
search space is much reduced. Changes such as log 
wood  dog food are avoided because they can not 
be made in one iteration and there are no interme-
diate corrections conditionally more probable than 
the left-hand-side query (log wood) and less prob-
able than the right-hand-side query (dog food). 
 An iterative process is prone to other types of 
problems. Short queries can be iteratively trans-
formed into other un-related queries; therefore, 
changing such queries is restricted additionally in 
our system. Another restriction we imposed is to 
not allow changes of in-lexicon words in the first 
iteration, so that easy-to-fix unknown-word errors 
are handled before any word substitution error. 
6 Evaluation 
For this work, we are concerned primarily with 
recall because providing good suggestions for mis-
spelled queries can be viewed as more important 
than abstaining to provide alternative query sug-
gestions for valid queries as long as these sugges-
tions are reasonable (for example, suggesting 
cowboy ropes for cowboy robes may not have ma-
jor cost to a user). A real system would have a 
component that decides whether to surface a spell-
ing suggestion based on where we want to be on 
the ROC curve, thus negotiating between precision 
and recall. 
 One problem with evaluating a spell checker de-
signed to correct search queries is that evaluation 
data is hard to get. Even if the system were used 
by a search engine and click-through information 
were available, such information would provide 
only a crude measure of precision and would not 
allow us to measure recall, by capturing only cases 
in which the corrections proposed by that particu-
lar speller are clicked on by the users. 
 We performed two different evaluations of the 
proposed system.4 The first evaluation was done 
on a test set comprising 1044 unique randomly 
sampled queries from a daily query log, which 
were annotated by two annotators. Their inter-
agreement rate was 91.3%. 864 of these queries 
were considered valid by both annotators; for the 
other 180, the annotators provided spelling correc-
tions. The overall agreement of our system with 
the annotators was 81.8%. The system suggested 
131 alternative queries for the valid set, counted as 
false positives, and 156 alternative queries for the 
misspelled set. Table 2 shows the accuracy ob-
tained by the proposed system and results from an 
ablation study where we disabled various compo-
nents of the system, to measure their influence on 
performance. 
                                                          
4
 The test data sets can be downloaded from 
http://research.microsoft.com/~silviu/Work 
  All queries Valid Misspelled 
Nr. queries 1044 864 180 
Full system 81.8 84.8 67.2 
No lexicon 70.3 72.2 61.1 
No query log 77.0 82.1 52.8 
All edits equal 80.4 83.3 66.1 
Unigrams only 54.7 57.4 41.7 
1 iteration only 80.9 88.0 47.2 
2 iterations only 81.3 84.4 66.7 
No fringes 80.6 83.3 67.2 
Table 2. Accuracy of various instantiations of the system 
 By completely removing the trusted lexicon, the 
accuracy of the system on misspelled queries 
(61.1%) was higher than in the case of only using 
a trusted lexicon and no query log data (52.8%). It 
can also be observed that the language model built 
using query logs is by far more important than the 
channel model employed: using a poorer character 
error model by setting all edit weights equal did 
not have a major impact on performance (66.1% 
recall), while using a poorer language model that 
only employs unigram statistics from the query 
logs crippled the system (41.7% recall). Another 
interesting aspect is related to the number of itera-
tions. Because the first iteration is more conserva-
tive than the following iterations, using only one 
iteration led to fewer false positives but also to a 
much lower recall (47.2%). Two iterations were 
sufficient to correct most of the misspelled queries 
that the full system could correct. While fringes 
did not have a major impact on recall, they helped 
avoid false positives (and had a major impact on 
speed). 
81.2
81.681.880.7
69.468.9
67.2
66.1
65
70
75
80
85
1 month 2 months 3 months 4 months
All queries
Mispelled queries
 
Figure 3. Accuracy and recall as functions of the number of  
 monthly query logs used to train the language model 
 Figure 3 shows the performance of the full system 
as a function of the number of monthly query logs 
employed. While both the total accuracy and the 
recall increased when using 2 months of data in-
stead of 1 month, by using more query log data (3 
and 4 month), the recall (or accuracy on mis-
spelled queries) still improves but at the expense 
of having more false positives for valid queries, 
which leads to an overall slightly smaller accuracy.  
 A post-analysis of the results showed that the sys-
tem suggested in many cases reasonable correc-
tions but different from the gold standard ones. 
Many false positives could be considered reason-
able suggestions, although it is not clear whether 
they would have been helpful to the users (e.g. 
2002 kawasaki ninja zx6e  2002 kawasaki ninja 
zx6r was counted as an error, although the sugges-
tion represents a more popular motorcycle model). 
In the case of misspelled queries in which the 
user?s intent was not clear, the suggestion made by 
the system could be considered valid despite the 
fact that it disagreed with the annotators? choice 
(e.g. gogle  google instead of the gold standard 
correction goggle). 
 To address the problems generated by the fact that 
the annotators could only guess the user intent, we 
performed a second evaluation, on a set of queries 
randomly extracted from query log data, by sam-
pling pairs of successive queries ),( 21 qq  sent by 
the same users in which the queries differ from 
one another by an un-weighted edit distance of at 
most 1+(len( 1q )+len( 2q ))/10 (i.e. allow a point 
change for every 5 letters). We then presented the 
list to human annotators who had the option to re-
ject a pair, choose one of the queries as a valid cor-
rection of the other, or propose a correction for 
both when none of them were valid but the in-
tended valid query was easy to guess from the se-
quence, as in example 3 below: 
(audio flie, audio file)  audio file 
(bueavista, buena vista)  buena vista 
(carrabean nooms, carrabean rooms)  caribbean rooms 
 Table 3 shows the performance obtained by dif-
ferent instantiations of the system on this set.  
 
Full system 73.1 
No lexicon 59.2 
No query log 44.9 
All edits equal 69.9 
Unigrams only 43.0 
1 iteration only 45.5 
2 iterations only 68.2 
No fringes 71.0 
Table 3. Accuracy of the proposed system on a set which  
     contains misspelled queries that the users had reformulated 
 The main system disagreed 99 times with the gold 
standard, in 80 of these cases suggesting a differ-
ent correction. 40 of the corrections were not ap-
propriate (e.g. porat was corrected by our system 
to pirate instead of port in chinese porat also 
called xiamen), 15 were functionally equivalent 
corrections given our target search engine (e.g. 
audio flie  audio files instead of audio file), 17 
were different valid suggestions (e.g. bellsouth 
lphone isting  bellsouth phone listings instead of 
bellsouth telephone listing), while 8 represented 
gold standard errors (e.g. the speller correctly sug-
gested brandy sniffters  brandy snifters instead 
of brandy sniffers). Out of 19 cases in which the 
system did not make a suggestion, 13 were genu-
ine errors (e.g. paul waskiewiscz with the correct 
spelling paul waskiewicz), 4 were cases in which 
the original input was correct, although different 
from the user?s intent (e.g. cooed instead of coed) 
and 2 were gold standard errors (e.g. commandos 3 
walkthrough had the wrong correction commando 
3 walkthrough, as this query refers to a popular 
videogame called ?commandos 3?). 
 
Differences Gold std errors Format  Diff. valid Real Errors 
80+19 8+2 15+0 17+4 40+13 
 The above table shows a synthesis of this error 
analysis on the second evaluation set. The first 
number in each column refers to a precision error 
(i.e. the speller suggested something different than 
the gold standard), while the second refers to a 
recall error (i.e. no suggestion). 
 As a result of this error analysis, we could argua-
bly consider that while the agreement with the 
gold standard experiments are useful for measur-
ing the relative importance of components, they do 
not give us an absolute measure of  system useful-
ness/accuracy. 
 
Agreement Correctness Precision Recall 
73.1 85.5 88.4 85.4 
 In the above table, we consider correctness as the 
relative number of times the suggestion made by 
the speller was correct or reasonable; precision 
measures the number of correct suggestions in the 
total number of spelling suggestions made by the 
system; recall is computed as the relative number 
of correct/reasonable suggestions made when such 
suggestions were needed.  
 As an additional verification and to confirm the 
difficulty of the test queries, we sent a set of them 
to Google and observed that Google speller?s 
agreement with the gold standard was slightly 
lower than our system?s agreement. 
7 Conclusion 
To our knowledge, this paper is the first to show a 
successful attempt of using the collective knowl-
edge stored in search query logs for the spelling 
correction task. We presented a technique to mine 
this extremely informative but very noisy resource 
that actually exploits the errors made by people as 
a way to do effective query spelling correction. A 
direction that we plan to investigate is the adapta-
tion of such a technique to the general purpose 
spelling correction, by using statistics from both 
query-logs and large office document collections. 
Acknowledgements 
We wish to thank Robert Ragno and Robert Roun-
thwaite for helpful comments and discussions.  
References 
Brill, E. and R. Moore. 2000. An improved error model for 
noisy channel spelling correction. In Proceedings of the ACL 
2000, pages 286-293. 
Cherkassky, V., N. Vassilas, G.L. Brodt, R.A. Wagner, and 
M.J. Fisher. 1974. The string to string correction problem. In 
Journal of ACM, 21(1):168-178. 
Damerau, F.J. 1964. A technique for computer detection and 
correction of spelling errors. In Communications of ACM, 
7(3):171-176. 
Garside, R., G. Leech and G. Sampson. 1987. Computational 
analysis of English: a corpus-based approach, Longman. 
Golding, A.R. 1995. A Bayesian hybrid method for context-
sensitive spelling correction. In Proceedings of the Work-
shop on Very Large Corpora, pages 39-53. 
Golding, A.R. and D. Roth. 1996. Applying winnow to con-
text-sensitive spelling correction. In Proceedings of ICML 
1996, pages 182-190. 
Heidorn, G.E., K. Jensen, L.A. Miller, R.J. Byrd and M.S. 
Chodorow. 1982. The EPISTLE text-critiquing system. In 
IBM Systems Journal, 21(3):305-326. 
Jurafsky, D. and J.H. Martin. 2000. Speech and language 
processing. Prentice-Hall. 
Kernighan, M., K. Church, and W. Gale. 1990. A spelling 
correction program based on a noisy channel model. In Pro-
ceedings of COLING 1990. 
Kukich, K. 1992. Techniques for automatically correcting 
words in a text. In Computing Surveys, 24(4):377-439. 
Mays, E., F.J. Damerau and R.L. Mercer. 1991. Context-
based spelling correction. In Information Processing and 
Management, 27(5):517-522. 
Mangu, L. and E. Brill. 1997. Automatic rule acquisition for 
spelling correction. In Proceedings of the ICML 1997, pages 
734-741. 
McIlroy, M.D. 1982. Development of a spelling list. In J-
IEEE-TRANS-COMM, 30(1);91-99. 
Peterson, J.L. 1980. Computer programs for spelling correc-
tion: an experiment in program design. Springer-Verlag. 
Rieseman, E.M. and A.R. Hanson. 1974. A contextual post-
processing system for error correction using binary n-grams. 
In IEEE Transactions on Computers, 23(5):480-493. 
Toutanova, K. and R. C. Moore. 2002. Pronunciation Model-
ing for Improved Spelling Correction. In Proceedings of the 
ACL 2002.pages 141-151. 
Wittgenstein, L. 1968. Philosophical Investigations. Basil 
Blackwell, Oxford. 
 	

 	
			ffAugmented Mixture Models for Lexical Disambiguation
Silviu Cucerzan and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{silviu,yarowsky}@cs.jhu.edu
Abstract
This paper investigates several augmented mixture
models that are competitive alternatives to standard
Bayesian models and prove to be very suitable to
word sense disambiguation and related classifica-
tion tasks. We present a new classification correc-
tion technique that successfully addresses the prob-
lem of under-estimation of infrequent classes in the
training data. We show that the mixture models are
boosting-friendly and that both Adaboost and our
original correction technique can improve the re-
sults of the raw model significantly, achieving state-
of-the-art performance on several standard test sets
in four languages. With substantially different out-
put to Na?ve Bayes and other statistical methods, the
investigated models are also shown to be effective
participants in classifier combination.
1 Introduction
The focus tasks of this paper are two re-
lated problems in lexical ambiguity resolution:
Word Sense Disambiguation (WSD) and Context-
Sensitive Spelling Correction (CSSC).
Word Sense Disambiguation has a long history as
a computational task (Kelly and Stone, 1975), and
the field has recently supported large-scale interna-
tional system evaluation exercises in multiple lan-
guages (SENSEVAL-1, Kilgarriff and Palmer (2000),
and SENSEVAL-2, Edmonds and Cotton (2001)).
General purpose Spelling Correction is also a
long-standing task (e.g. McIlroy, 1982), tradi-
tionally focusing on resolving typographical errors
such as transposition and deletion to find the clos-
est ?valid? word (in a dictionary or a morpholog-
ical variant), typically ignoring context. Yet Ku-
kich (1992) observed that about 25-50% of the
spelling errors found in modern documents are ei-
ther context-inappropriate misuses or substitutions
of valid words (such as principal and principle)
which are not detected by traditional spelling cor-
rectors. Previous work has addressed the problem
of CSSC from a machine learning perspective, in-
cluding Bayesian and Decision List models (Gold-
ing, 1995), Winnow (Golding and Roth, 1996) and
Transformation-Based Learning (Mangu and Brill,
1997).
Generally, both tasks involve the selection be-
tween a relatively small set of alternatives per key-
word (e.g. sense id?s such as church/BUILDING
and church/INSTITUTION or commonly confused
spellings such as quiet and quite), and are dependent
on local and long-distance collocational and syntac-
tic patterns to resolve between the set of alterna-
tives. Thus both tasks can share a common feature
space, data representation and algorithm infrastruc-
ture. We present a framework of doing so, while in-
vestigating the use of mixture models in conjunction
with a new error-correction technique as competi-
tive alternatives to Bayesian models. While several
authors have observed the fundamental similarities
between CSSC and WSD (e.g. Berleant, 1995 and
Roth, 1998), to our knowledge no previous com-
parative empirical study has tackled these two prob-
lems in a single unified framework.
2 Problem Formulation. Feature Space
The problem of lexical disambiguation can be mod-
eled as a classification task, in which each in-
stance of the word to be disambiguated (target word,
henceforth), identified by its context, has to be la-
beled with one of the established sense labels
 
	
	
.
1 The approaches we investigate
are statistical methods 
 ffBootstrapping a Multilingual Part-of-speech Tagger
in One Person-day
Silviu Cucerzan and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218 USA
{silviu,yarowsky}@cs.jhu.edu
Abstract
This paper presents a method for bootstrapping a
fine-grained, broad-coverage part-of-speech (POS)
tagger in a new language using only one person-
day of data acquisition effort. It requires only three
resources, which are currently readily available in
60-100 world languages: (1) an online or hard-copy
pocket-sized bilingual dictionary, (2) a basic library
reference grammar, and (3) access to an existing
monolingual text corpus in the language. The al-
gorithm begins by inducing initial lexical POS dis-
tributions from English translations in a bilingual
dictionary without POS tags. It handles irregular,
regular and semi-regular morphology through a ro-
bust generative model using weighted Levenshtein
alignments. Unsupervised induction of grammatical
gender is performed via global modeling of context-
window feature agreement. Using a combination of
these and other evidence sources, interactive train-
ing of context and lexical prior models are accom-
plished for fine-grained POS tag spaces. Experi-
ments show high accuracy, fine-grained tag resolu-
tion with minimal new human effort.
1 Introduction
Previous work in minimally supervised language
learning has defined minimal using several different
criteria. Some have assumed only partially tagged
training corpora (Merialdo, 1994), while others
have begin with small tagged seed wordlists (such
as Collins and Singer (1999) and Cucerzan and
Yarowsky (1999) for named-entity tagging). Oth-
ers have exploited the automatic transfer of some
already existing annotated resource in a different
medium or language (such as the translingual pro-
jection of part-of-speech tags, syntactic bracket-
ing and inflectional morphology in Yarowsky et al
(2001), requiring no direct supervision in the for-
eign language). Ngai and Yarowsky (2000) ob-
served that an often more practical measure of the
degree of supervision is not simply the quantity of
annotated words, but the total weighted human la-
bor and resource costs of different modes of su-
pervision (allowing manual rule writing to be com-
pared directly with active learning on a common
cost-performance learning curve).
In this paper we observe that another useful mea-
sure of (minimal) supervision is the additional cost
of obtaining a desired functionality from existing
commonly available knowledge sources. In particu-
lar, we note that for a remarkably wide range of lan-
guages, academic libraries, many booksellers and
websites offer a foundation of linguistic wisdom in
reference grammars and dictionaries. Thus starting
from this baseline, what is the marginal cost of dis-
tilling from and augmenting this existing knowledge
to achieve a desired new task functionality?
2 Inducing POS Tag Candidates from
Unlabeled Bilingual Dictionaries
A substantial percentage of foreign language dic-
tionaries that are available on line or in smaller pa-
perback format are simple bilingual word or phrase
translation lists which fail to specify part of speech.1
Thus one component question of this work is how
can one extract preliminary part-of-speech distribu-
tions from untagged monolingual translation lists.
Figure 1 illustrates such a bilingual dictionary, also
specifying the true part of speech for each possible
translation, which we do not assume to be generally
available.
One approach is to take an unweighted mixture
of the prior part-of-speech distributions for the En-
glish words 

given in the translation list (TL) as
illustrated in Figure 2. These probabilities may be
estimated from a large and preferably balanced, cor-
pus. In this work, we used statistics from the Brown
and WSJ corpora combined.
1In this section, we will use the term POS tag to denote
only the main part-of-speech tags (noun, verb, adjective, ad-
verb, preposition, etc.) and not the fine-grained tags (such as
Noun-Genitive-fem-plur-def).
True
Romanian POS English translation list
mandat N warrant; proxy; mandate;
money order;
power of attorney
manechin N model, dummy
manifesta V arise, express itself, show
manual Adj manual;
N manual; textbook;
handbook
mare Adj large; big; great; tall;
old; important;
N sea
maro Adj brown, chestnut
Figure 1: A sample Romanian-English dictionary.
The POS tags are used only for evaluation and are
not available in many bilingual dictionaries.
MANDAT  Warrant
Proxy
Mandate
.55  .00    .45
.66   .34    .00
.80  .20    .00
.67   .18   .15
N AV
N AV
e i jP(Pos  | e  )iFW P(Pos  | FW)j
(via English treebank)dictionary
bilingual
via
Figure 2: Inducing a preliminary POS distribution
for the Romanian word mandat via a simple English
translation list.
However, when a translation candidate is phrasal
(e.g. mandat  money order), one can model the
more general probability of the foreign word?s part
of speech tag (

) given the part of speech sequence
of the English phrasal translation (





).
For example, one could model P(T

money or-
der) via P(T





 and P(T

manifest itself) via
P(T





. However, because English words
often have multiple parts of speech (e.g. order may
be a verb), one may weight phrasal POS sequence
probabilities (making an independence assumption)
as:
 

	
  
 





   

	
   


 





   

	
   


 





   

	
   


 





   

	
   



And in general:
 







 








 







   





   






where  





 is estimated from the dictionary
as above. Without an independence assumption:
 







 
 







   












There are two major options via which one can
estimate  







. The first is to assume
that the part-of-speech usage of phrasal (English)
translations is generally consistent across dictionar-
ies (e.g.  







 remains high regardless
of publisher or language). Hence one could use
any foreign-English bilingual dictionary that also
includes the true foreign word part of speech in ad-
dition to its translations to train these probabilities.
Alternately, one could do a first-pass assignment
of foreign-word part of speech based on only sin-
gle word translations as in Figure 2, and use this to
train  







 for those foreign words hav-
ing both phrasal and single-word definitions (such
as mandat). The advantage of this approach is that it
may benefit dictionaries with different phrasal trans-
lation styles from the training dictionary (e.g. use
or omission of the word ?to? in verb definitions).
However, given the assumption of relatively consis-
tent dictionary formatting styles (which was unfor-
tunately not the case for Kurdish), we evaluated this
work based on supervised phrasal training from a
single independent third language dictionary.
Table 1 measures the POS induction performance
on three languages, where the true POS tags were
given in the dictionary (as in Figure 1), but ignored
except for evaluation. The accuracy values in this
table are based on exact matches between a word?s
dictionary-provided POS and the most probable tag
in its induced distribution.
For our target application of part-of-speech tag-
ging, what matters is to have a robust tag probabil-
ity distribution that includes the true candidate with
sufficiently large probability to seed further train-
ing. By setting this baseline threshold to 0.1 and
deleting lower ranked candidates, up to 98% of the
true POS were found to be above this threshold and
hence were considered in future training.
The Mean Probability of Truth, as shown in Ta-
ble 1, is another measure of the quality of the POS
predictions made by the algorithm, representing the
probability mass associated with the true POS tag
averaged over all words.
In some cases the algorithm could not predict a
POS tag, primarily due to English translations for
which no POS distribution was known (often an ob-
scure word, proper name or OCR error). This oc-
Target Training Accuracy Correct POS Coverage Mean Probability
Language Dictionary Exact POS Over Threshold of Truth
Romanian Spanish - English 92.9 97.8 98 .91
Kurdish Spanish - English 76.8 93.1 95 .82
Spanish Romanian - English 83.3 94.9 97 .86
Table 1: Performance of inducing candidate part-of-speech distributions derived solely from untagged En-
glish translation lists. Results are measured by type (all dictionary entries are weighted equally).
casional omission is measured by the coverage col-
umn.
Most of the observed errors are due to differences
in phrasal definitional conventions in the training
and testing dictionaries, long phrasal idioms, single-
word definitions with ambiguous English parts-of-
speech and OCR errors. The Kurdish dictionary was
particularly hindered by frequent long phrasal trans-
lations which often included an explanation or def-
inition in their translation. Because all dictionary
entries are equally weighted, errors on rare words
such as mythological characters or kinship terms
can substantially downgrade performance. But for
the purposes of providing seed POS distributions to
context-sensitive taggers, performance is quite ade-
quate for this follow-on task.
3 Inducing Morphological Analyses
There has been extensive previous work in the
supervised and minimally supervised induction of
both affix paradigms (e.g. Goldsmith, 2000; Snover
and Brent, 2001) and diverse models of regular and
irregular concatenative and non-concatenative mor-
phology (e.g. Schone and Jurafsky, 2000; van den
Bosch and Daelemans, 1999; Yarowsky and Wicen-
towski, 2000). While such approaches are impor-
tant from the perspective of learning theory or broad
coverage handling of irregular forms, another pos-
sible paradigm for minimal supervision is to begin
with whatever knowledge can be efficiently manu-
ally entered from the grammar book in several hours
work.
We defined such grammar-based ?supervision? as
entry of regular inflectional affix changes and their
associated part of speech in standardized ordering of
fine-grained attributes, as in Table 2 for Spanish and
Romanian. The full tables have approximately 200
lines each and required roughly 1.5-2 person-hours
for entry.
Given a dictionary marked with core parts of
speech, it is trivial to generate hypothesized in-
flected forms following the regular paradigms, as
shown in the left size of Figure 3. However, due
to irregularities and semi-regularities such as stem-
Root Inflected
Affix Affix Part-of-speech Tag
Spanish:
o$ o$ Adj-masc-sing
o$ os$ Adj-masc-plur
o$ a$ Adj-fem-sing
o$ as$ Adj-fem-plur
e$ e$ Adj-masc,fem-sing
e$ es$ Adj-masc,fem-plur
ar$ o$ Verb-Indic_Pres-p1-sing
ar$ as$ Verb-Indic_Pres-p2-sing
ar$ a$ Verb-Indic_Pres-p3-sing
ar$ amos$ Verb-Indic_Pres-p1-plur
ar$ ?is$ Verb-Indic_Pres-p2-plur
ar$ an$ Verb-Indic_Pres-p3-plur
Romanian:
a?$ e$ Noun-Nomin-p3-fem-plur-indef
e$ i$ Noun-Nomin-p3-fem-plur-indef
ea$ ele$ Noun-Nomin-p3-fem-plur-indef
i$ ile$ Noun-Nomin-p3-fem-plur-indef
a$ ale$ Noun-Nomin-p3-fem-plur-indef
$ $ Adj-masc,neut-sing
$ a?$ Adj-fem-sing
$ i$ Adj-masc,neut,fem-plur
$ e$ Adj-fem,neut-plur
ru$ ra$ Adj-fem-sing
ru$ ri$ Adj-masc,neut,fem-plur
ru$ re$ Adj-fem-plur
... ... ...
e$ $ Verb-Indic_Pres-p1-sing
e$ i$ Verb-Indic_Pres-p2-sing
e$ e$ Verb-Indic_Pres-p3-sing
e$ em$ Verb-Indic_Pres-p1-plur
e$ et?i$ Verb-Indic_Pres-p2-plur
e$ $ Verb-Indic_Pres-p3-plur
Table 2: Sample extracted regular inflectional
paradigms (suffix context is marked by $).
changes, such generation will clearly have substan-
tial inaccuracies and overgenerations.
However, through weighted-Levenshtein-based
iterative alignment models, such as described in
Yarowsky and Wicentowski (2000), one can per-
form a probabilistic string match from all lexical to-
kens actually observed in a monolingual corpus, as
z->cdestrozan destroc?
destroz?
V-pres-3pl
V-pret-1sg
V-subj-3pl destrozen
destrocen
destrozan
destrozar/V
z->c
destruo
destru?
destruen
V-pres-1sg
V-pres-1sg
V-pret-1sg
destrue destru?
destruyo
destruye
destruyen
destruir/V
V-pres-3sg
->y
->y?
?
 
->y
V-pres-3pl
V-pret-3pl
doler/V
dormir/V
V-pres-1sg
dormenV-pres-3pl
dormo
dolen
doli?
o->ue
o-
>u
e
doli?
duelen
duermen
duermo
dormi?
dorm?an
dorm?an
durmi?
V-pret-3pl
V-imprf-3pl
o->
ue
o->u
Observed
Rootword
Dictionary Corpus
Regular
Words
Inflection
Generation
?
Figure 3: Inflectional analysis induction via
weighted string alignment to noisy generations from
dictionary roots under regular paradigms
in the right side of Figure 32.
For example, when looking for a potential anal-
ysis path for the Spanish irregular inflection de-
strocen, the closest string match is the regular hy-
pothesis destrozar/V  destrozen/V-pres_subj-3pl.
Likewise, the closest string match for destruyen is
destruir/V  destruen/V-pres_indic-3pl. The dif-
ferences between these regular hypotheses and ob-
served inflected forms are the relatively productive
stem changes  and , neither of which was
listed in the inflectional supervision table, and yet
they were correctly handled. Note that a traditional
 POSsuffix) model would fail to handle this case
given that the common inflection suffix -en corre-
sponds to two different parts of speech here (present
indicative or subjunctive depending on -ir or -ar
paradigm).
Also note that the irregular stem change pro-
cesses such as dormirduermen have a correct
best-fit analysis, despite the absence of any internal
stem change exemplars (e.g. oue) in the human-
generated inflectional supervision table.
For further robustness, the consensus model of
 

  is estimated as a weighted mixture of
the part-of-speech tags of the most closely aligned
2For processing efficiency, one additional constraint is that
potential hypothesizedobserved string pair candidates must
exactly match in both initial consonant cluster and suffix of the
generated hypothesis.
pseudo-regular generated inflections.
The inflections of closed-class words (such as
pronouns, determiners and auxiliary verbs) are not
well handled by this generative-alignment model,
both due to their often very high irregularity (e.g.
the Spanish verb ser (to be)) and/or their typ-
ical shortness (e.g. the pronominal inflections
of mi, tu, su). Thus as one final amount of
supervision, lists of closed-class words, paired
with their inflections and fine-grained part-of-
speech tags were entered manually from the gram-
mar book (e.g. aquellas#(aquel)Adj_Dem-
fem-plur-p3). This final source of supervision
utilized an average of 400 lines and 3 person-hours
per language.
4 POS Model Induction
The non-traditional supervision methodology in
Sections 2 and 3 yields a noisy but broad-coverage
candidate space of parts of speech with little human
effort.
We then perform a noise-robust combination of
model estimation and re-estimation techniques for
the syntagmatic trigram models  



 


and lexical priors  



 using the word co-
occurrence information from a raw corpus.
 A suffix-based part-of-speech probability
model  

suffix

 using hierarchically
smoothed tries is trained on the raw initial
tag distributions, yielding coverage to unseen
words and smoothing of low-confidence initial
tag assignments.
 Paradigmatic cross-context tag modeling is
performed as in Cucerzan and Yarowsky
(2000) when sufficiently large unannotated
corpora are available.
 Sub-part-of-speech contextual agreement for
features such as gender is performed as de-
scribed in Section 4.1.
 The part-of-speech tag sequence models
 



 

 utilize a weighted backoff
between fine-grained and coarse-grained tags.
 Both the tag-sequence and lexical prior models
are iteratively retrained using these additional
evidence sources and first-pass probability dis-
tributions.
The success of this model is based on the as-
sumption that (a) words of the same part of speech
tend to have similar tag sequence behavior, and (b)
there are sufficient instances of each POS tag la-
beled by either the morphology models or closed-
class entries described in Section 3. One example
where these assumptions do not hold is for the Ro-
manian word a, which has 5 possible POS tags, in-
cluding Infinitive_Marker (corresponding to
the English word to). But because the Infini-
tive_Marker tag has no other word instances in
Romanian, no other filial supervision exists to re-
solve the ambiguity of a if no context-sensitive tag-
ging is provided (such as the preference for a to
be labeled Infinitive_Markerwhen followed
by a Verb-Infinitive). Thus one avenue of
potential improvement to these models would be
to include limited tagged contexts for ambiguous
small class (or singleton class) words, although such
supervision is less readily extractable from gram-
mar books by non-native speakers, and was not em-
ployed here.
4.1 Contextual-agreement models for
part-of-speech subtags
Traditional part-of-speech models assume a strict
Markovian sequential dependency. However, Adj-
Noun, Det-Noun and Noun-Verb agreement at the
subtag-level (e.g. for person, number, case and gen-
der) often do not require direct adjacency, and are
based on the selective matching of isolated subfea-
tures. This is particularly important for grammatical
gender, where the lack of gender features projected
from English rootwords in a bilingual dictionary (as
in Section 2) require contextual agreement to assign
gender to many inflected and root forms.
However, given the assumptions of minimal su-
pervision, it is not reasonable to require a parser or
dependency model to identify non-adjacent agree-
ing pairs explicitly. Rather, we utilize a much more
general tendency for words exhibiting a property
such as grammatical gender to co-occur in a rela-
tively narrow window with other words of the same
gender (etc.) with a probability greater than chance.
Empirically, we observe this in Figures 4-5, which
show the gender-agreement ratio between a target
noun/adjective and other gender marked words ap-
pearing in context at relative position . Adjec-
tives in Romanian exhibit a stronger agreement ten-
dency with words to their left (5/1 ratio), while for
nouns the agreement ratio is quite closely balanced
between -1 (primarily determiners) and +1 (primar-
ily adjectives), although weaker (2.4/1 ratio), per-
haps due to a greater relative tendency for nouns to
juxtapose directly with other independent clauses of
different gender. Also, both parts of speech con-
0
1
2
3
4
5
6
-10 -9 -8 -7 -6 -5 -4 -3 -2 -1
Relative Position
0
Ag
re
em
en
t/N
on
-A
gr
ee
m
en
t R
at
io
1 2 3 4 5 6 7 8 9 10
Adjectives
0
1
2
2.5
-10 -9 -8 -7 -6 -5 -4 -3 -2 -1
Nouns
Ag
ree
me
nt/
No
n-A
gre
em
en
t R
atio
0
Relative Position
1 2 3 4 5 6 7 8 9 10
Figure 4: Ratio of the frequency that a gender-
marked adjective (above) or noun (below) agrees
in gender with another noun/adjective/determiner at
relative position i over the frequency of gender dis-
agreement at that relative position.
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1 2 3 4
to
ke
ns
 w
ith
in
 c
on
te
xt
 w
in
do
w
Pr
ob
ab
ilit
y 
of
 e
xis
te
nc
e 
of
  g
en
de
r-m
ar
ke
d
Context Width
5 6 7 8 9 10
Figure 5: The probability that at least one gender-
marked word will occur within a window of 
words relative to another gender marked word (of
any part of speech).
verge on the agreement ratio expected by chance
(0.82) relatively quickly. Thus while any individ-
ual context may suggest incorrect gender based on
agreement, if one aggregates over all occurrences of
a word in a corpus, a consensus gender preference
emerges, with the true gender agreement signal ex-
ceeding nearby spurious gender noise.
Formally, we can model this window-weighted
global feature consensus as:
 


 



	




 





The  window-size parameter was selected
prior to the studies shown in Figures 4-5, but is sup-
ported by them. Beyond this window the agree-
ment/disagreement ratio approaches chance, but
with a smaller window the probability of finding any
gender-marked word in the window drops below the
80% coverage observed for , trading lower cov-
erage for increased accuracy.
If one makes the assumption that the overwhelm-
ing majority of nouns have a single grammatical
gender independent of context, we perform smooth-
ing to force nouns with sufficient global context fre-
quency towards their single most likely gender.
Finally, the trie-based suffix model noted in Sec-
tion 3 can be utilized here to further generalize gen-
der affixal tendencies for use in smoothing poorly
represented single words. Through this approach
we successfully discover a wide space of low-
entropy gender affix tendencies, including the com-
mon -a, -dad and -ci?n feminine affixes in Span-
ish, without any human or dictionary supervision
of nominal gender. But even those words with-
out gender-distinguishing affixes (e.g. parte, cabal)
can be successfully learned via global context max-
imization.
5 Evaluation of the Full Part-of-speech
Tagger
One problem with minimally supervised learning of
foreign languages is that annotated evaluation data
are often not available for the features being in-
duced, or are otherwise difficult to obtain. Thus we
have used for initial test languages two languages
familiar to the authors (Romanian and Spanish) for
which sufficient evaluation resources could be ob-
tained. However, the monolingual corpora utilized
for bootstrapping were quite small (123 thousand
words of the book 1984 for Romanian and 3.2 mil-
lion words of newswire for Spanish), which are eas-
ily comparable to the sizes that can be accessed on-
line for 60-100 world languages. The seed dictio-
naries were located online (for Spanish - 42k en-
tries) and via OCR (for Romanian - 7k entries), and
small grammar references were obtained at a local
bookstore. 1000 words of test data were annotated
with a standardized, finely detailed part-of-speech
tag inventory including the full complex distinctions
for gender, person, number, case, detailed tense and
nominal definiteness (an inventory of 259 and 230
fine-grained tags were used for Spanish and Roma-
nian respectively).
The minimal supervision in this study consisted
of an average total of 4 person-hours per language
for manually entering the inflectional paradigms
and associated parts of speech from a grammar as
in Section 3, and an additional average of 3 person-
hours per language for dictionary extraction and en-
try parsing. OCR itself on our high-speed 2-sided
scanner with OmniPage Pro took under 30 min-
utes). As would be expected given that data en-
try was done by computer scientists which were
not native speakers of the test languages, significant
analysis errors or gaps were introduced when rather
blindly transferring from the reference grammar.
Thus to test the relative contributions of limited na-
tive speaker help when available, for roughly 4 addi-
tional total person hours in a second test condition
for Romanian a native speaker corrected and aug-
mented gaps in the patterns previously entered from
the grammar book, focusing almost exclusively on
the complex inflections of closed-class words.
A summary of the results for these three super-
vision modes is given in Table 3. Performance is
broken down by fine-grained part of speech. Exact-
match accuracy is measured over both the full fine-
grained (up to 5-feature) part-of-speech space, as
well as the 12-class core POS tag (noun and proper
noun, pronoun, verb, adjective, adverb, numeral,
determiner, conjunction, preposition, interjection,
particle, punctuation). The feature of grammatical
gender was specifically isolated because it is rarely
salient for cross-language applications such as ma-
chine translation (where grammatical gender rarely
transfers), and because its induction algorithm in
Section 4.1 depends heavily on the size of the mono-
lingual corpus (which is small in these experiments,
suggesting size-dependent potential for significant
further improvement here).
Finally, a post-hoc analysis of the system vs. test
data discrepancies showed that a significant number
were simply arbitrary differences in annotation con-
vention between the grammar-book analyses and
the test data tagging policy. For example, one such
?error?/discrepancy is the rather arbitrary distinc-
tion of whether the Romanian word oricare (mean-
ing any) should be considered an adjective (as listed
in a standard bilingual dictionary) or a determiner.
Another difference is whether proper-name citations
of common nouns (e.g. Casa Blanca) should be an-
notated for gender/number etc. or not.
Yet regardless of exactly how many system-test
discrepancies are just policy differences rather than
errors, even the raw accuracy here is very promising
given the very fined-grained part-of-speech inven-
tory and small monolingual data size used for boot-
strapping. And ultimately the performance is quite
Spanish Romanian
NNS NNS NNS-8h
8h 8h NS-4h
All words
core-tag 93.1 86.3 89.2
exact-match 86.5 68.6 75.5
exact w/o gender 87.0 76.7 83.0
Nouns
core-tag 90.3 97.4 97.4
*number 100.0 97.4 98.9
*gender 100.0 54.9 64.7
*definiteness ? 96.6 93.7
*case ? 97.4 97.4
Verbs
core-tag 94.7 87.9 89.5
*tense 93.0 92.6 93.2
*number 100.0 91.5 91.2
*person 97.2 92.6 93.2
Adjectives
core-tag 79.7 78.6 81.5
*gender 100.0 81.3 82.2
*number 100.0 98.3 98.3
Table 3: Performance of POS tagger induction
based on 1 person-day of supervision, no tagged
training corpora and a fine-grained (250 tags)
tagset. NNS and NN refer to non-native-speaker and
native-speaker effort.
remarkable given that it is the result of less than 1
total person day of data collection and supervision,
in contrast to the thousands of hours and $100,000-
$1,000,000 spent on some annotated training data
in a much more limited tagset inventories. Thus
in terms of cost-benefit analysis, the supervision
paradigm and associated bootstrapping models pre-
sented here offer quite a good value of new func-
tionality per labor invested.
6 Conclusion
This paper has presented an alternative to tradi-
tional corpus annotation-based supervision of part-
of-speech taggers. Given that even obscure lan-
guages have reference grammars and dictionaries
available in large bookstores, libraries or even on-
line, the focus of this work is on using human su-
pervision for efficient structured entry of this seed
knowledge (in the form of regular and semi-regular
inflectional paradigms and often irregular closed-
class part-of-speech entries). Minimally supervised
bootstrapping procedures then used corpus-derived
distributional data to induce lexical tag probabilities
from dictionaries, irregular morphological analyses
via weighted Levenshtein-based alignment models,
tag sequence probability induction and grammati-
cal gender agreement modeling. Experiments show
high accuracy coarse and fine-grained ( 250 tag)
part-of-speech analyses using only one person day
of new human supervision based on readily avail-
able linguistic resources.
Acknowledgements
This work was partially supported by NSF grant
IIS-9985033 and ONR/MURI contract N00014-01-
1-0685.
References
Baum, L. 1972. An inequality and associated maximiza-
tion technique in statistical estimation of probabilistic
functions of a Markov process. Inequalities, 3:1?8.
Collins, M., and Y. Singer, 1999 Unsupervised models
for named entity classification. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC 1999,
pp. 100-110.
Cucerzan, S., and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining morpho-
logical and contextual evidence. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC 1999,
pp. 90-99.
Cucerzan, S., and D. Yarowsky, 2000. Language in-
dependent minimally supervised induction of lexical
probabilities. In Proceedings of ACL 2000, pp. 270-
277.
Goldsmith, J. A., 2000 Unsupervised learning of the
morphology of a natural language. Computational
Linguistics 27(2):153?198.
Merialdo, B., 1994. Tagging English text with a prob-
abilistic model. Computational Linguistics 20:155?
171.
Ngai, G., and D. Yarowsky, 2000. Inducing multilin-
gual POS taggers and NP bracketers via robust projec-
tion across aligned corpora. In Proceedings of NAACL
2000, pp. 200-207.
Schone, P., and D. Jurafsky, 2000. Knowledge-free in-
duction of morphology using latent semantic analysis.
In Proceedings of CoNLL 2000.
Snover, M. G., and M. R. Brent, 2001. A Bayesian model
for morpheme and paradigm identification. In Pro-
ceedings of ACL 2001, pp. 482-490.
Van den Bosch, A., and W. Daelemans, 1999. Memory-
based morphological analysis. In Proceedings of ACL
1999, pp.285-292
Yarowsky, D., G. Ngai, and R. Wicentowski, 2001. In-
ducing Multilingual Text Analysis Tools via Robust
Projection across Aligned Corpora. In Proceedings of
HLT 2001, pp. 161-168.
Yarowsky, D., and R. Wicentowski, 2000. Minimally su-
pervised morphological analysis by multimodal align-
ment. In Proceedings of ACL 2000, pp. 207-216.
Language Independent NER using a Unified Model of Internal and
Contextual Evidence
Silviu Cucerzan and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{silviu,yarowsky}@cs.jhu.edu
Abstract
This paper investigates the use of a language inde-
pendent model for named entity recognition based
on iterative learning in a co-training fashion, using
word-internal and contextual information as inde-
pendent evidence sources. Its bootstrapping pro-
cess begins with only seed entities and seed con-
texts extracted from the provided annotated corpus.
F-measure exceeds 77 in Spanish and 72 in Dutch.
1. Introduction
Our aim has been to build a maximally language-
independent system for named-entity recognition
using minimal supervision or knowledge of the
source language. The core model utilized, ex-
tended and evaluated here is based on Cucerzan and
Yarowsky (1999). It assumes that only an entity ex-
emplar list is provided as a bootstrapping seed set.
For the particular task of CoNLL-2002, the seed
entities are extracted from the provided annotated
corpus. As a consequence, the seed examples may
be ambiguous and the system must therefore han-
dle seeds with probability distribution over entity
classes rather than unambiguous seeds. Another
consequence is that this approach of extracting only
the entity seeds from the annotated text does not use
the full potential of the training data, ignoring con-
textual information. For example, Bosnia appears
labeled 9 times as LOC and 5 times as ORG and
the only information that would be used is that the
word Bosnia denotes a location 64% of the time,
and an organization 36% of the time, but not in
which contexts is labeled one way or the other. In
order to correct this problem, an improved system
also uses context seeds if available (for this particu-
lar task, they are extracted from the annotated cor-
pus). Because the representations of entity candi-
dates and contexts are identical, this modification
imposes only minor changes in algorithm and code.
Because the core model has been presented in de-
tail in Cucerzan and Yarowsky (1999), this paper
focuses primarily on the modifications of the algo-
rithm and its adaptation to the current task. The ma-
jor modifications besides the seed handling include
a different method of smoothing the distributions
along the paths in the tries, a new ?soft? discourse
segmentation method, and use of a different label-
ing methodology, as required by the current task i.e.
no overlapping entities are allowed (for example,
the correct labeling of colegio San Juan Bosco de
M?rida is considered to be ORG(colegio San Juan
Bosco) de LOC(M?rida) rather than ORG(colegio
PER(San Juan Bosco) de LOC(M?rida))).
2. Entity-Internal Information
Two types of entity-internal evidence are used in a
unified framework. The first consists of the pre-
fixes and suffixes of candidate entities. For exam-
ple, in Spanish, names ending in -ez (e.g. Alvarez
and Guti?rrez) are often surnames; names ending in
-ia are often locations (e.g. Austria, Australia, and
Italia). Likewise, common beginnings and endings
of multiword entities (e.g. Asociaci?n de la Prensa
de Madrid and Asociaci?n para el Desarrollo Rural
Jerez-Sierra Suroeste, which are both organizations)
are good indicators for entity type.
3. Contextual Information
An entity?s left and right context provides an essen-
tially independent evidence source for model boot-
strapping. This information is also important for en-
tities that do not have a previously seen word struc-
ture, are of foreign origin, or polysemous. Rather
than using word bigrams or trigrams, the system
handles the context in the same way it handles the
entities, allowing for variable-length contexts. The
advantages of this unified approach are presented in
the next paragraph.
4. A Unified Structure for both Internal and
Contextual Information
Character-based tries provide an effective, efficient
and flexible data structure for storing both con-
textual and morphological patterns and statistics.
... organizada por la Concejal?a de Cultura , tienen un ...
PREFIX RIGHT CONTEXTLEFT CONTEXT
SUFFIX
Figure 1: An example of entity candidate and context and
the way the information is introduced in the four tries (arrows
indicate the direction letters are considered)
They are very compact representations and support
a natural hierarchical smoothing procedure for dis-
tributional class statistics. In our implementation,
each terminal or branching node contains a prob-
ability distribution which encodes the conditional
probability of entity classes given the sistring cor-
responding to the path from the root to that node.
Each such distribution also has two standard classes,
named ?questionable? (unassigned probability mass
in terms of entity classes, to be motivated below)
and ?non-entity? (common words).
Two tries (denoted PT and ST) are used for in-
ternal representation of the entity candidates in pre-
fix, respectively suffix form, respectively. Other two
tries are used for left (LCT) and right (RCT) con-
text. Right contexts are introduced in RCT by con-
sidering their component letters from left to right,
left contexts are introduced in LCT using the re-
versed order of letters, from right to left (Figure 1).
In this way, the system handles variable length con-
texts and it attempts to match in each instance the
longest known context (as longer contexts are more
reliable than short contexts, and also the longer con-
text statistics incorporate the shorter context statis-
tics through smoothing along the paths in the tries).
The tries are linked together into two bipartite
structures, PT with LCT, and ST with RCT, by at-
taching to each node a list of links to the entity can-
didates or contexts with, respectively in which the
sistring corresponding to that node has been seen in
the text (Figure 2).
5. Unassigned Probability Mass
When faced with a highly skewed observed class
distribution for which there is little confidence due
to small sample size, a typical response is to back-
off or smooth to the more general class distribution.
Unfortunately, this representation makes problem-
atic the distinction between a back-off conditional
distribution and one based on a large sample (and
hence estimated with confidence). We address this
problem by explicitly representing the uncertainty
as a class, called "questionable". Probability mass
continues to be distributed among the primary en-
tity classes proportional to the observed distribu-
tion in the data, but with a total sum that reflects
ST
a
i
r
t
s
u
A
bA
... ...
z
... ...
RCT
#
...
,
h
i
#
C
h
i
r
a
#
H
o
l
a
n
rz
o
p
a
t
i
a
#
...
c
#
...
d
a
#
.
.
.
......
...
...
...
...
Figure 2: An example of links between the Suffix Trie and the
Right Context Trie for the entity candidate Austria and some of
its right contexts as observed in the corpus (< , Holanda >,
< , hizo >, < a Chirac >)
the confidence in the distribution and is equal to
 
	
.
Incremental learning essentially becomes the pro-
cess of gradually shifting probability mass from
questionable to one of the primary classes.
6. Smoothing
The probability of an entity candidate or context as
being or indicating a certain type of entity is com-
puted along the path from the root to the node in
the trie structure described above. In this way, ef-
fective smoothing can be realized for rare entities
or contexts. A smoothing formula taking advantage
of the distributional representation of uncertainty is
presented below.
For a sistring Proceedings of the Eighteenth Conference on Computational Language Learning, pages 109?118,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Temporal Scoping of Relational Facts based on Wikipedia Data
Avirup Sil
?
Computer and Information Sciences
Temple University
Philadelphia, PA 19122
avi@temple.edu
Silviu Cucerzan
Microsoft Research
One Microsoft Way
Redmond, WA 98052
silviu@microsoft.com
Abstract
Most previous work in information
extraction from text has focused on
named-entity recognition, entity linking,
and relation extraction. Less attention
has been paid given to extracting the
temporal scope for relations between
named entities; for example, the relation
president-Of(John F. Kennedy, USA)
is true only in the time-frame (January
20, 1961 - November 22, 1963). In this
paper we present a system for temporal
scoping of relational facts, which is
trained on distant supervision based on the
largest semi-structured resource available:
Wikipedia. The system employs language
models consisting of patterns automat-
ically bootstrapped from Wikipedia
sentences that contain the main entity of
a page and slot-fillers extracted from the
corresponding infoboxes. This proposed
system achieves state-of-the-art results
on 6 out of 7 relations on the benchmark
Text Analysis Conference 2013 dataset
for temporal slot filling (TSF), and out-
performs the next best system in the TAC
2013 evaluation by more than 10 points.
1 Introduction
Previous work on relation extraction (Agichtein
and Gravano, 2000; Etzioni et al., 2004) by sys-
tems such as NELL (Carlson et al., 2010), Know-
ItAll (Etzioni et al., 2004) and YAGO (Suchanek
et al., 2007) have targeted the extraction of en-
tity tuples, such as president-Of(George W.
Bush, USA), in order to build large knowl-
edge bases of facts. These systems assume
that relational facts are time-invariant. However,
this assumption is not always true, for example
?
This research was carried out during an internship at
Microsoft Research.
president-Of(George W. Bush, USA) holds
within the time-frame (2001-2009) only. In this
paper, we focus on the relatively less explored
problem of attaching temporal scope to relation
between entities. The Text Analysis Conference
(TAC) introduced temporal slot filling (TSF) as
one of the knowledge base population (KBP) tasks
in 2013 (Dang and Surdeanu, 2013). The in-
put to a TAC-TSF system is a binary relation e.g.
per:spouse(Brad Pitt, Jennifer Aniston) and a
document assumed to contain supporting evidence
for the relation. The required output is a 4-tuple
timestamp [T1, T2, T3, T4], where T1 and T2
are normalized dates that provide a range for the
start date of the relation, and T3 and T4 provide
the range for the end of the relationship. Sys-
tems must also output the offsets of the text men-
tions that support the temporal information ex-
tracted. For example, from a text such as ?Pitt
married Jennifer Aniston on July 29, 2000 [...] the
couple divorced five years later in 2005.?, a sys-
tem must extract the normalized timestamp [2000-
07-29, 2000-07-29, 2005-01-01, 2005-12-31], to-
gether with the entity and date offsets that support
the timestamp.
In this paper, we describe TSRF, a system for
temporal scoping of relational facts. For ev-
ery relation type, TSRF uses distant supervision
from Wikipedia infobox tuples to learn a language
model consisting of patterns of entity types, cate-
gories, and word n-grams. Then it uses this trained
relation-specific language model to extract the top
k sentences that support the given relation between
the query entity and the slot filler. In a second
stage, TSRF performs timestamp classification by
employing models which learn ?Start?, ?End? and
?In? predictors of entities in a relationship; it com-
putes the best 4-tuple timestamp [T1, T2, T3, T4]
based on the confidence values associated to the
top sentences extracted. Following the TAC-TSF
task for 2013, TSRF is trained and evaluated for
seven relation types, as shown in Table 1.
109
per:spouse
per:title
per:employee or member of
org:top employees/members
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
Table 1: Types of relations in the TAC-TSF.
The remainder of the paper is organized as fol-
lows: The next section describes related work.
Section 3 introduces the TAC-TSF input and out-
put formats. Section 4 discusses the main chal-
lenges, and Section 5 details our method for tem-
poral scoping of relations. Section 6 describes our
experiments and results, and it is followed by con-
cluding remarks.
2 Related Work
To our knowledge, there are only a small num-
ber of systems that have tackled the temporal
scoping of relations task. YAGO (Wang et al.,
2010) extracts temporal facts using regular expres-
sions from Wikipedia infoboxes, while PRAVDA
(Wang et al., 2011) uses a combination of textual
patterns and graph-based re-ranking techniques to
extract facts and their temporal scopes simultane-
ously. Both systems augment an existing KB with
temporal facts similarly to the CoTS system by
Talukdar et al. (2012a; 2012b). However, their
underlying techniques are not applicable to arbi-
trary text. In contrast, TSRF automatically boot-
straps patterns to learn relation-specific language
models, which can be used then for processing
any text. CoTS, a recent system that is part of
CMU?s NELL (Carlson et al., 2010) project, per-
forms temporal scoping of relational facts by using
manually edited temporal order constraints. While
manual ordering is appealing and can lead to high
accuracy, it is impractical from a scalability per-
spective. Moreover, the main goal of CoTS is to
predict temporal ordering of relations rather than
to scope temporally individual facts. Conversely,
our system automatically extracts text patterns,
and then uses them to perform temporal classi-
fication based on gradient boosted decision trees
(Friedman, 2001).
The TempEval task (Pustejovsky and Verhagen,
2009) focused mainly on temporal event order-
ing. Systems such as (Chambers et al., 2007) and
(Bethard and Martin, 2007) have been successful
Col.1: TEMP72211 Col.7: 1492
Col.2: per:spouse Col.8: 1311
Col.3: Brad Pitt Col.9: 1.0
Col.4: AFP ENG 20081208.0592 Col.10: E0566375
Col.5: Jennifer Aniston Col.11: E0082980
Col.6: 1098
Table 2: Input to a TSF System.
in extracting temporally related events. Sil et al.
(2011a) automatically extract STRIPS represen-
tations (Fikes and Nilsson, 1971) from web text,
which are defined as states of the world before and
after an event takes place. However, all these ef-
forts focus on temporal ordering of either events or
states of the world and do not extract timestamps
for events. By contrast, the proposed system ex-
tracts temporal expressions and also produces an
ordering of the timestamps of relational facts be-
tween entities.
The current state-of-the-art systems for TSF
have been the RPI-Blender system by Artiles et
al. (2011) and the UNED system by Garrido et
al. (2011; 2012). These systems obtained the
top scores in the 2011 TAC TSF evaluation by
outperforming the other participants such as the
Stanford Distant Supervision system (Surdeanu
et al., 2011). Similar to our work, these sys-
tems use distant supervision to assign temporal la-
bels to relations extracted from text. While we
employ Wikipedia infoboxes in conjunction with
Wikipedia text, the RPI-Blender and UNED sys-
tems use tuples from structured repositories like
Freebase. There are major differences in terms of
learning strategies of these systems: the UNED
system uses a rich graph-based document-level
representation to generate novel features whereas
RPI-Blender uses an ensemble of classifiers com-
bining flat features based on surface text and de-
pendency paths with tree kernels. Our system em-
ploys language models based on Wikipedia that
are annotated automatically with entity tags in a
boosted-trees learning framework. A less impor-
tant difference between TSRF and RPI-Blender is
that the latter makes use of an additional tempo-
ral label (Start-And-End) for facts within a time
range; TSRF employs Start, End, and In labels.
3 The Temporal Slot Filling Task
3.1 Input
The input format for a TSF system as instantiated
for the relation per:spouse(Brad Pitt, Jennifer
110
Aniston) is shown in Table 2. The field Column 1
contains a unique query ID for the relation. Col-
umn 2 is the name of the relationship, which also
encodes the type of the target entity. Column 3
contains the name of the query entity, i.e., the sub-
ject of the relation. Column 4 contains a valid doc-
ument ID and Column 5 indicates the slot-filler en-
tity. Columns 6 through 8 are offsets of the slot-
filler, query entity and the relationship justification
in the given text. Column 9 contains a confidence
score set to 1 to indicate that the relation is cor-
rect. Columns 10 and 11 contain the IDs in the
KBP knowledge base of the entity and filler, re-
spectively. All of the above are provided by TAC.
For the query in this example, a TSF system has to
scope temporally the per:spouse relation be-
tween Brad Pitt and Jennifer Aniston.
3.2 Output
Similar to the regular slot filling task in TAC, the
TSF output includes the offsets for at least one
entity mention and up to two temporal mentions
used for the extraction and normalization of
hypothesized answer. For instance, assume that a
system extracts the relative timestamp ?Monday?
and normalizes it to ?2010-10-04? for the relation
org:top employee(Twitter, Williams) using
the document date from the following document:
<DOCID> AFP ENG 20101004.0053.LDC2010T13 </DOCID>
<DATETIME> 2010-10-04 </DATETIME>
<HEADLINE>
Twitter co-founder steps down as CEO
</HEADLINE>
<TEXT>
<P>
Twitter co-founder Evan Williams announced on Monday
that he was stepping down as chief executive [...]
The system must report the offsets for both
?Monday? in the text body and ?2010-10-04? in
the DATETIME block for the justification.
The TAC-TSF task uses the following represen-
tation for the temporal information extracted: For
each relation provided in the input, TSF systems
must produce a 4-tuple of dates: [T1, T2, T3, T4],
which indicates that the relation is true for a pe-
riod beginning at some point in time between T1
and T2 and ending at some time between T3 and
T4. By convention, a hyphen in one of the po-
sitions implies a lack of a constraint. Thus, [-,
20120101, 20120101, -] implies that the relation
was true starting on or before January 1, 2012 and
ending on or after January 1, 2012. As discussed
in the TAC 2011 pilot study by Ji et al. (2011),
there are situations that cannot be covered by this
representation, such as recurring events, for ex-
ample repeated marriages between two persons.
However, the most common situations for the re-
lations covered in this task are captured correctly
by this 4-tuple representation.
4 Challenges
We discuss here some of the main challenges en-
countered in building a temporal scoping system.
4.1 Lack of Annotated Data
Annotation of data for this task is expensive, as
the human annotators must have extensive back-
ground knowledge and need to analyze the evi-
dence in text and reliable knowledge resources. As
per (Ji et al., 2013), a large team of human an-
notators were able to generate only 1,172 training
instances for 8 slots for KBP 2011. The authors
of the study concluded that such amount of data
is not enough for training a supervised temporal
scoping system. They also noted that only 32% of
employee Of queries were found to have poten-
tial temporal arguments, and only one third of the
queries could have reliable start or end dates.
4.2 Date Normalization
Sometimes temporal knowledge is not stated ex-
plicitly in terms of dates or timestamps. For exam-
ple, from the text ?they got married on Valentine?s
Day? a system can extract Valentine?s Day as the
surface form of the start of the per:spouse re-
lation. However, for a temporal scoping system it
needs to normalize the temporal string to the date
of February 14 and the year to which the document
refers to explicitly in text or implicitly, such as the
year in which the document was published.
4.3 Lexico-Syntactic Variety
A relation can be specified in text by employing
numerous syntactic and lexical constructions; e.g.
for the per:spouse relation the patterns ?got
married on [DATE]? and ?vowed to spend eternity
on [DATE]? have the same meaning. Addition-
ally, entities can appear mentioned in text in vari-
ous forms, different from the canonical form given
as input. For instance, Figure 1 shows an example
in which the input entity Bernardo Hees, which is
not in Wikipedia, is mentioned three times, with
two of the mentions using a shorter form (the last
name of the person).
111
org:top_members_employees    America Latina Logistica / NIL    Bernardo Hees / NIL 
 
<HEADLINE> Burger King buyer names future CEO </HEADLINE> 
<DATELINE> NEW YORK 2010-09-09 13:00:29 UTC </DATELINE> 
<TEXT> 
<P> The investment firm buying Burger King has named Bernardo Hees, a Latin 
American railroad executive, to be CEO of the company after it completes its 
$3.26 billion buyout of the fast-food chain. </P> 
<P> 3G Capital is naming Hees to replace John Chidsey, who will become co-
chairman after the deal closes. </P> 
<P> Hees was most recently CEO of America Latina Logistica, Latin America's 
largest railroad company. Alexandre Behring, managing partner at 3G Capital, was 
also a prior CEO of the railroad. </P> 
<P> 3G Capital is expected to begin its effort to acquire the outstanding shares 
of Burger King for $24 per share by Sept. 17. </P> 
</TEXT> 
Figure 1: Example data point from the TAC TSF 2013 training set, with the annotations hypothesized
by our system. The entity mentions identified by the entity linking (EL) component are shown in bold
blue; those that were linked to Wikipedia are also underlined. The highlighting (blue and green) is used
to show the mentions in the coreference chains identified for the two input entities, ?America Latina
Logistica? and ?Bernardo Hees?.
4.4 Inferred Meaning
A temporal scoping system also needs to learn the
inter-dependence of relations, and how one event
affects another. For instance, in our automatically
generated training data, we learn that a death
event specified by n-grams like ?was assassinated?
affects the per:title relation, and it indicates
that the relationship ended at that point. In Fig-
ure 1, while the CEO relationships for Bernardo
Hees with America Latina Logistica and Burger
King are indicated by clear patterns (?was most re-
cently CEO of? and ?to be CEO of?), the temporal
stamping is difficult to achieve in both cases, as
there is no standard normalization for ?recently?
in the former, and it is relative to the completion
of the buyout event in the latter.
4.5 Pattern Trustworthiness
A temporal scoping system should also be able
to model the trustworthiness of text patterns, and
even the evolution of patterns that indicate a rela-
tionship over time. For example, in current news,
the birth of a child does not imply that a couple
is married, although it does carry a strong signal
about the marriage relationship.
5 Learning to Attach Temporal Scope
5.1 Automatically Generating Training Data
As outlined in Section 4, one of the biggest chal-
lenges of a temporal scoping system is the lack
of annotated data to create a strong information
extraction system. Previous work on relation ex-
traction such as (Mintz et al., 2009) has shown
that distant supervision can be highly effective in
building a classifier for this purpose. Similar to
supervised classification techniques, some advan-
tages of using distant supervision are:
? It allows building classifiers with a large number
of features;
? The supervision is provided intrinsically by the
detailed user-contributed knowledge;
? There is no need to expand patterns iteratively.
Mintz et al. also point out that similar to unsuper-
vised systems, distant supervision also allows:
? Using large amounts of unlabeled data such as
the Web and social media;
? Employing techniques that are not sensitive to
the genre of training data.
We follow the same premise as (Cucerzan, 2007;
Weld et al., 2009) that the richness of the
Wikipedia collection, whether semantic, lexical,
syntactic, or structural, is a key enabler in re-
defining the state-of-the-art for many NLP and
IR task. Our target is to use distant supervision
from Wikipedia data to build an automatic tempo-
ral scoping system. However, for most relations,
we find that Wikipedia does not indicate specific
start or end dates in a structured form. In addition
to this, we need our system to be able to predict
whether two entities are currently in a relation-
ship or not based on the document date as well.
112
Hence, in our first step, we build an automatic sys-
tem which takes as input a binary relation between
two entities e.g. per:spouse(Brad Pitt, Jennifer
Aniston) and a number of documents. The system
needs to extract highly ranked/relevant sentences,
which indicate that the two entities are in the tar-
geted relationship. The next component takes as
input the top k sentences generated in the previous
step and extracts temporal labels for the input rela-
tion. Note that our target is to develop algorithms
that are not relation-specific but rather can work
well for a multitude of relations. We elaborate on
these two system components further.
5.1.1 Using Wikipedia as a Resource for
Distant Supervision
Wikipedia is the largest freely available encyclo-
pedic collection, which is built and organized as
a user-contributed knowledge base (KB) of enti-
ties. The current version of the English Wikipedia
contains information about 4.2 million entities.
In addition to the plain text about these entities,
Wikipedia also contains structured components.
One of these is the infobox. Infoboxes contain in-
formation about a large number of relations for the
target entity of the Wikipedia page, e.g. names of
spouses, birth and death dates, residence etc.. Sim-
ilar to structured databases, the infoboxes contain
the most important/useful relations in which enti-
ties take part, while the text of Wikipedia pages
contains mentions and descriptions of these rela-
tions. Because of this, Wikipedia can be seen as a
knowledge repository that contains parallel struc-
tured and unstructured information about entities,
and therefore, can be employed more easily than
Freebase or other structured databases for building
a relation extraction system. Figure 2 shows how
sentences from Wikipedia can be used to train a
system for the temporal slot filling task.
5.1.2 Extracting Relevant Sentences
For every relation, we extract slot-filler names
from infoboxes of each Wikipedia article. We
also leverage Wikipedia?s rich interlinking model
to automatically retrieve labeled entity mentions
in text. Because the format of the text values pro-
vided by different users for the infobox attributes
can vary greatly, we rely on regular expressions to
extract slot-filler names from the infoboxes. For
every relation targeted, we build a large set of reg-
ular expressions to extract entity names and filter
out noise e.g. html tags, redundant text etc..
To extract all occurrences of named-entities in
the Wikipedia text, we relabel each Wikipedia ar-
ticle with Wikipedia interlinks by employing the
entity linking (EL) system by Cucerzan (2012),
which obtained the top scores for the EL task in
successive TAC evaluations. This implementa-
tion takes into account and preserves the inter-
links created by the Wikipedia contributors, and
extracts all other entity mentions and links them to
Wikipedia pages if possible or hypothesizes coref-
erence chains for the mentions of entities that are
not in Wikipedia. The latter are extremely impor-
tant when the slot-filler for a relation is an entity
that does not have a Wikipedia page, as often is
the case with spouses or other family members of
famous people (as shown in Figure 1 for the slot-
filler Bernardo Hees).
As stated in Section 4, temporal information
in text is specified in various forms. To resolve
temporal mentions, we use the Stanford SUTime
(Chang and Manning, 2012) temporal tagger.
The system exhibits strong performance outper-
forming state-of-the-art systems like HeidelTime
(Str?otgen and Gertz, 2010) on the TempEval-2
Task A (Verhagen et al., 2010) in English. SU-
Time is a rule-based temporal tagger that employs
regular expression. Its input is English text in to-
kenized format; its output contains annotations in
the form of TIMEX3 tags. TIMEX3 is a part of
the TimeML annotation language as introduced by
(Pustejovsky et al., 2003) and is used to markup
date and time, events, and their temporal rela-
tions in text. When processing Web text, we of-
ten encounter date expressions that contain a rel-
ative time e.g. ?last Thursday?. To resolve them
to actual dates/time is a non-trivial task. However,
the heuristic of employing the document?s publi-
cation date as the reference works very well in
practice e.g. for a document published on 2011-
07-05, SUTime resolves ?last Thursday? to 2011-
06-30. It provides temporal tags in the following
labels: Time, Duration, Set and Interval. For our
experiments we used Time and Duration.
After running the Stanford SUTime, which au-
tomatically converts date expressions to their nor-
malized form, we collect sets of contiguous sen-
tences from the page that contain one mention of
the targeted entity and one mention of the slot-
filler, as extracted by the entity linking system. We
then build a large language model by bootstrap-
ping textual patterns supporting the relations, sim-
113
ilar to (Agichtein and Gravano, 2000). The general
intuition is that a set of sentences that mention the
two entities are likely to state something about re-
lationships in which they are.
For assigning sentences a relevance score with
respect to a targeted relation, we represent the sen-
tences in an input document (i.e., Wikipedia page)
as d dimensional feature vectors, which incorpo-
rate statistics about how relevant sentences are
to the relation between a query entity q and the
slot filler z. For example, for the per:spouse
relation, one binary feature is ?does the input
sentence contain the n-gram ?QUERY ENTITY
got married??. Note that the various surface
forms/mentions of q and z are resolved to their
canonical target at this stage.
We were able to extract 61,872 tuples of query
entity and slot filler relations from Wikipedia
for the per:spouse relation. Figure 2 shows
how we extract relevant sentences using slot-filler
names from Wikipedia. Consider the following
text (already processed by our EL system and
Stanford SUTime) taken from the Wikipedia page
of Tom Cruise:
On [November 18, 2006|
2006?11?18
],
[Holmes|
Katie Holmes
] and [Cruise|
Tom Cruise
]
were married in [Bracciano|
Bracciano
] . . .
On [June 29, 2012|
2012?06?29
],
[Holmes|
Katie Holmes
] filed for divorce
from [Cruise|
Tom Cruise
] after five and a half
years of marriage.
Considering Tom Cruise as the query entity and
his wife Katie Holmes as the slot filler for the
per:spouse relation, we normalize the above
text to the following form to extract features:
On DATE, SLOT FILLER and
QUERY ENTITY were married in
LOCATION . . .
On DATE, SLOT FILLER filed for divorce
from QUERY ENTITY after five and a half
years of marriage.
Our language model consists of n-grams (n ? 5)
like ?SLOT FILLER and QUERY ENTITY were
married?, ?SLOT FILLER filed for divorce from?
which provides clues for the marriage relation.
These n-grams are then used as features with
an implementation of a gradient boosted decision
trees classifier similar to that described by (Fried-
man, 2001; Burges, 2010). We also use features
provided by the EL system which are based on en-
tity types and categories. We call this ?relation-
ship? classifier RELCL. The output of this step is
In April 2005, Cruise began dating actress KatieHolmes. On April 27 that year, Cruise and Holmes ?dubbed "TomKat" by the media ? made their firstpublic appearance together in Rome. On October 6,2005, Cruise and Holmes announced they wereexpecting a child, and their daughter, Suri, was born inApril 2006. On November 18, 2006, Holmes and Cruisewere married in Bracciano, Italy, in a Scientologyceremony attended by many Hollywood stars. Therehas been widespread speculation that the marriagewas arranged by the Church of Scientology. On June 29,2012, it was announced that Holmes had filed fordivorce from Cruise after five and a half years ofmarriage. On July 9, 2012, it was announced that thecouple had signed a divorce settlement worked out bytheir lawyers. STARTOfmarriage
ENDOfmarriage
Spouse: Katie Holmes
Figure 2: Example of relevant sentences extracted
by using query entity and slot-filler names from
Wikipedia for the per:spouse relation.
a ranked list of sentences which indicate whether
there exists a relationship between the query entity
and the slot filler.
5.1.3 Learning Algorithm
Our objective is to rank the sentences in a docu-
ment based on the premise that entities q and z
are in the targeted relation r. We tackle this rank-
ing task by using gradient boosted decision trees
(GBDT) to learn temporal scope for entity rela-
tions. Previous work such as Sil et al. (2011a;
2011b) used SVMs for ranking event precondi-
tions and (Cucerzan, 2012) and (Zhou et al., 2010)
employed GBDT for ranking entities. GBDT can
achieve high accuracy as they can easily combine
features of different scale and missing values. In
our experiments, GBDT outperforms both SVMs
and MaxEnt models.
We employ the stochastic version of GBDT
similar to (Friedman, 2001; Burges, 2010). Ba-
sically, the model performs a numerical optimiza-
tion in the function space by computing a function
approximation in a sequence of steps. By build-
ing a smaller decision tree at each step, the model
computes residuals obtained in the previous step.
Note that in the stochastic variant of GBDT, for
computing the loss function, the model absorbs
several samples instead of using the whole train-
ing data. The parameters for our GBDT model
were tuned on a development set sampled from
our Wikipedia dump independent from the train-
ing set. These parameters include the number of
regression trees and the shrinkage factor.
114
Figure 3: Architecture of the proposed sys-
tem. Every input document is processed by the
(Cucerzan, 2012) entity linking system and the
Stanford SUTime system. Temporal information
is then extracted automatically using RELCL and
DATECL.
5.1.4 Gathering Relevant Sentences
On the unseen test data, we apply our trained
model and obtain a score for each new sentence s
that contains mentions of entities q and z that are
in a targeted relationship by turning s into a feature
vector as shown previously. Among all sentences
that contain mentions of q and z, we choose the
top k with the highest score. The value of k was
tuned based on the performance of TSRF on our
development set.
5.1.5 Extracting Timestamps
To predict timestamps for each relation, we build
another classifier, DATECL similar to that de-
scribed in the previous section, by using language
models for ?Start?, ?End? and ?In? predictors of
relationship. The ?Start? model predicts T1, T2;
?End? predicts T3, T4 and ?In? predicts T2, T3.
Raw Trigger Features: Similar to previous
work by (Sil et al., 2010) on using discriminative
words as features, each of these models compose
of ?Trigger Words? that indicate when a relation-
ship begins or ends. In the current implemen-
tation, these triggers are chosen manually from
the language model automatically bootstrapped
from Wikipedia. Future directions include how
to automatically learn these triggers. For ex-
ample, for the per:spouse relation, the trig-
gers for ?Start? contain n-grams such as ?mar-
ried since DATE? and ?married SLOT FILLER
on?; the ?End? model contains n-grams such as
?estranged husband QUERY ENTITY?, ?split in
DATE?; the ?In? model contains ?happily mar-
ried?, ?QUERY ENTITY with his wife? etc.. For
an input sentence with query entity q and slot-
filler z, a first class of raw trigger features con-
sists of cosine-similarity(Text(q, z), Triggers(r))
where r ? Start, End, In. Here, Text(q, z) in-
dicates the full sentence as context. We also
employ another feature that computes cosine-
similarity(Context(q, z), Triggers(r)), which con-
structs a mini-sentence Context(q, z) from the
original by choosing windows of three words be-
fore and after q and z, and ignoring duplicates.
External Event Triggers: Our system also
considers the presence of other events as triggers
e.g. a ?death? event signaled by ?SLOT FILLER
died? might imply that a relationship ended on that
timestamp. Similarly, a ?birth? event can imply
that an entity started living in a particular location
e.g. the per:born-In(Obama, Honolulu)
relation from the sentence ?President Obama was
born in Honolulu in 1961? indicates that T1 =
1961-01-01 and T2 = 1961-12-31 for the rela-
tion per:cities of residence(Obama,
Honolulu).
At each step, TSRF extracts the top timestamps
for predicting ?Start?, ?End? and ?In? based on
the confidence values of DATECL. Similar to pre-
vious work by (Artiles et al., 2011), we aggregate
and update the extracted timestamps using the fol-
lowing heuristics:
Step 1: Initialize T= [-?, +?, -?,+ ?]
Step 2: Iterate through the classified timestamps
Step 3: For a new T
?
aggregate :
T&&T
?
= [max(t
1
, t
?
1
),min(t
2
, t
?
2
),
max(t
3
, t
?
3
),min(t
4
, t
?
4
)]
Update only if: t
1
? t
2
; t
3
? t
4
; t
1
? t
4
This novel two-step classification strategy re-
moves noise introduced by distant supervision
training and decides if the extracted (entity, filler,
timestamp) tuples belong to the relation under
consideration or not. For example, for the
per:spouse relation between the entities Brad
Pitt and Jennifer Aniston, TSRF extracts sentences
like ?..On November 22, 2001, Pitt made a guest
appearance in the television series Friends, play-
ing a man with a grudge against Rachel Green,
played by Jennifer Aniston..? and ?Pitt met Jen-
nifer Aniston in 1998 and married her in a private
wedding ceremony in Malibu on July 29, 2000..?.
Note that both sentences contain the query entity
and the slot filler. The system automatically re-
jects the extraction of temporal information from
115
S1 S2 S3 S4 S5 S6 S7 ALL StDev
Baseline 24.70 17.40 15.18 17.83 14.75 21.08 23.20 19.10 3.60
TSRF 31.94 36.06 32.85 40.12 33.04 31.85 27.35 33.15 3.66
RPI-Blender 31.19 13.07 14.93 26.71 29.04 17.24 34.68 23.42 7.98
UNED 26.20 6.88 8.16 15.24 14.47 14.41 19.34 14.79 6.07
CMU-NELL 19.95 7.46 8.47 16.52 13.43 5.65 11.95 11.53 4.77
Abby-Compreno 0.0 2.42 8.56 0.0 13.50 7.91 0.0 5.14 4.99
LDC 69.87 60.22 58.26 72.27 81.10 54.07 91.18 68.84 12.32
Table 3: Results for the TAC-TSF 2013 test set, overall and for individual slots. The slots notation is: S1:
org:top members employees, S2: per:city of residence, S3: per:country of residence, S4: per:employee
or member of, S5: per:spouse, S6: per:statesorprovince of residence, S7: per:title. The score for the
output created by the LDC experts is also shown.
the former even though the sentence contains men-
tions of both entities. This is because the language
model for the marriage relation does not match
well this candidate sentence, which is actually fo-
cussing on the two entities being in the different
relation of co-acting/appearing in the same mo-
tion picture. The latter sentence is determined as
matching the language model for the marriage re-
lation, and TSRF extracts the temporal scope July
29, 2000 and attaches the START label to it. Most
previous systems do not perform this noise re-
moval step, which is a critical component in our
distant supervision approach.
6 Experiments
For evaluation, we train our system on the infobox
tuples and sentences extracted from the Wikipedia
dump of May 2013. We set aside a portion of the
dump as our development data. We chose to use
the top-relevant n-grams based on the performance
on the development data as features. We employ
then the TAC evaluation data, which is publicly
available through LDC.
We utilize the evaluation metric developed for
TAC (Dang and Surdeanu, 2013). In order for a
temporal constraint (T1-T4) to be valid, the doc-
ument must justify both the query relation (which
is similar to the regular English slot filling task)
and the temporal constraint. Since the time in-
formation provided in text may be approximate,
the TAC metric measures the similarity of each
constraint in the key and system response. For-
mally, if the date in the gold standard is k
i
, while
the date hypothesized by the system is r
i
, and
d
i
= |k
i
? r
i
| is their difference measured in
years, then the score for the set of temporal con-
straints on a slot is computed as:
Score(slot) =
1
4
4
?
i=1
c
c + d
i
TAC sets the constant c to one year, so that pre-
dictions that differ from the gold standard by one
year get 50% credit. The absence of a constraint
in T1 or T3 is treated as a value of?? and the ab-
sence of a constraint in T2 or T4 is treated as +?,
which lead to zero-value terms in the scoring sum.
Therefore, the overall achievable score has a range
between 0 and 1.
We compare TSRF against four other TSF sys-
tems: (i) RPI-Blender (Artiles et al., 2011), (ii)
CMU-NELL (Talukdar et al. (2012a; 2012b)),
(iii) UNED (Garrido et al. (2011; 2012)) and (iv)
Abby-Compreno (Kozlova et al., 2012). Most of
these systems employ distant supervision strate-
gies too. RPI-Blender and UNED obtained the top
scores in the 2011 TAC TSF pilot evaluation, and
thus, could be considered as the state-of-the-art at
the time.
We also compare our system with a reasonable
baseline similar to (Ji et al., 2011). This baseline
makes the simple assumption that the correspond-
ing relation is valid at the document date. That
means that it creates a ?within? tuple as follows:
< ??, doc date, doc date, +? >. Hence, this
baseline system for a particular relation always
predicts T2 = T3 = the date of the document.
Table 3 lists the results obtained by our system
on the TAC test set of 201 queries, overall and for
each individual slot, in conjunction with the re-
sults of the other systems evaluated and the output
generated by the LDC human experts. Only two
out of the five systems evaluated, TSRF and RPI-
Blender, are able to beat the ?within? baseline.
TSRF achieves approximately 48% of human
performance (LDC) and outperforms all other sys-
116
TSF Accuracy SF F1 SF Prec SF Recall
LDC 68.8 83.1 97.3 72.5
TSRF 33.1 77.3 96.8 64.4
RPI-Blender 23.4 51.8 69.2 41.4
UNED 14.8 46.6 69.9 35.0
CMU-NELL 11.5 32.2 38.5 27.6
Abby-Compreno 5.1 18.5 53.6 11.2
Table 4: Extraction accuracy for slot-filler men-
tions. TSRF clearly outperforms all systems and
comes close to human performance (LDC).
tems in overall score, as well as for all individ-
ual relations with the exception of per:title,
for which RPI-Blender obtains a better score. In
fact, TSRF outperforms the next best systems
by 10 and 19 points. These two systems ob-
tained the top score in TAC 2011, and outper-
formed other systems such as Stanford (Surdeanu
et al., 2011). TSRF also outperforms CMU-
NELL which employs a very large KB of re-
lational facts already extracted from the Web
and makes use of the Google N-gram corpus
(http://books.google.com/ngrams).
We believe that this large performance differ-
ence is due in part to the fact that TSRF uses a
language model to clean up the noise introduced
by distant supervision before the actual temporal
classification step. Also, the learning algorithm
employed, GBDT, is highly effective in using the
extracted n-grams as features to decide whether
the extracted (entity, filler, time) tuples belong to
the relation under consideration or not. Finally,
Table 4 shows another reason that gives TSRF an
edge in obtaining the best score. The employed EL
component (Cucerzan, 2012) is a state-of-the-art
system for extracting and linking entities, and re-
solving coreference chains. By using this system,
we have been able to extract slot-filler mentions
with a precision of 96.8% at 66.4% recall, which
is substantially higher than the extraction results
of all other systems. Encouragingly, the perfor-
mance of this component also comes close to that
of the LDC annotators, which obtained a precision
of 97.3% at 72.5% recall.
It is also important to note that our system ex-
hibits a balanced performance on the relations
on which it was tested. As shown in column
StDev in Table 3, this system achieves the low-
est standard deviation in the performance across
the relations tested. It is interesting to note also
that TSRF achieves the best performance on the
employee of (S4) and city of residence
(S2) relations even though the system develop-
ment was done on the spouse relation (S1) as an
encouraging sign that our distant supervision al-
gorithm can be transferred successfully across re-
lations for domain-specific temporal scoping.
7 Conclusion and Future Work
The paper described an automatic temporal scop-
ing system that requires no manual labeling ef-
fort. The system uses distant supervision from
Wikipedia to obtain a large training set of tuples
for training. It uses a novel two-step classifica-
tion to remove the noise introduced by the dis-
tant supervision training. The same algorithm
was employed for multiple relations and exhibited
similarly high accuracy. Experimentally, the sys-
tem outperforms by a large margin several other
systems that address this relatively less explored
problem. Future directions of development in-
clude extracting joint slot filler names and tem-
poral information, and leveraging the changes ob-
served over time in Wikipedia for a query entity
and a slot filler in a target relation.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Procs. of the Fifth ACM International Conference
on Digital Libraries.
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne
Tamang, and Heng Ji. 2011. CUNY BLENDER
TACKBP2011 Temporal Slot Filling System De-
scription. In TAC.
Steven Bethard and James H Martin. 2007. Cu-tmp:
Temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132.
Chris Burges. 2010. From ranknet to lambdarank to
lambdamart: An overview. Learning, 11:23?581.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstration
Sessions, pages 173?176.
Angel X Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In LREC, pages 3735?3740.
117
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708?716.
Silviu Cucerzan. 2012. The MSR System for Entity
Linking at TAC 2012. In TAC.
Hoa Trang Dang and Mihai Surdeanu. 2013. Task
description for knowledge-base population at TAC
2013. In TAC.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,
A. Popescu, T. Shaked, S. Soderland, D. Weld, and
A. Yates. 2004. Web-Scale Information Extraction
in KnowItAll. In WWW, New York City, New York.
R. Fikes and N. Nilsson. 1971. STRIPS: A new
approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2(3/4):189?
208.
Jerome H Friedman. 2001. Greedy function approx-
imation: a gradient boosting machine. Annals of
Statistics, pages 1189?1232.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Pe-
nas, Alvaro Rodrigo, and Damiano Spina. 2011. A
distant supervised learning system for the tac-kbp
slot filling and temporal slot filling tasks. In TAC.
Guillermo Garrido, Anselmo Penas, Bernardo Ca-
baleiro, and Alvaro Rodrigo. 2012. Temporally an-
chored relation extraction. In ACL.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In TAC.
Heng Ji, Taylor Cassidy, Qi Li, and Suzanne Tamang.
2013. Tackling representation, annotation and clas-
sification challenges for temporal knowledge base
population. Knowledge and Information Systems,
pages 1?36.
Ekaterina Kozlova, Manicheva Maria, Petrova Elena,
and Tatiana Popova. 2012. The compreno semantic
model as an integral framework for a multilingual
lexical database. In 3rd Workshop on Cognitive As-
pects of the Lexicon (CogALex-III).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL, pages 1003?
1011.
James Pustejovsky and Marc Verhagen. 2009.
Semeval-2010 task 13: evaluating events, time ex-
pressions, and temporal relations (tempeval-2). In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions,
pages 112?116.
James Pustejovsky, Jos?e M Castano, Robert Ingria,
Roser Sauri, Robert J Gaizauskas, Andrea Set-
zer, Graham Katz, and Dragomir R Radev. 2003.
Timeml: Robust specification of event and tempo-
ral expressions in text. New directions in question
answering, 3:28?34.
Avirup Sil and Alexander Yates. 2011a. Extracting
STRIPS representations of actions and events. In
RANLP.
Avirup Sil and Alexander Yates. 2011b. Machine
Reading between the Lines: A Simple Evaluation
Framework for Extracted Knowledge Bases. In
Workshop on Information Extraction and Knowl-
edge Acquisition (IEKA).
Avirup Sil, Fei Huang, and Alexander Yates. 2010.
Extracting action and event semantics fromweb text.
In AAAI Fall Symposium on Common-Sense Knowl-
edge (CSK).
Jannik Str?otgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normaliza-
tion of temporal expressions. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 321?324.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X Chang, Valentin I Spitkovsky, and
Christopher D Manning. 2011. Stanfords distantly-
supervised slot-filling system. In TAC.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012a. Acquiring temporal constraints
between relations. In CIKM.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012b. Coupled temporal scoping of rela-
tional facts. In WSDM.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely yago: harvest-
ing, querying, and visualizing temporal knowledge
from wikipedia. In Proceedings of the 13th Interna-
tional Conference on Extending Database Technol-
ogy, pages 697?700. ACM.
YafangWang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from tex-
tual web sources by constrained label propagation.
In CIKM, pages 837?846.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using Wikipedia to Bootstrap Open Information Ex-
traction. In ACM SIGMOD Record.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In COLING, pages 1335?
1343.
118
