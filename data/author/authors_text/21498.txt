Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 157?162,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
FLOW: A First-Language-Oriented Writing Assistant System 
 
 
Mei-Hua Chen*, Shih-Ting Huang+, Hung-Ting Hsieh*, Ting-Hui Kao+, Jason S. Chang+ 
  * Institute of Information Systems and Applications 
+ Department of Computer Science 
National Tsing Hua University  
HsinChu, Taiwan, R.O.C. 30013 
{chen.meihua,koromiko1104,vincent732,maxis1718,jason.jschang}@gmail.com 
 
 
Abstract 
Writing in English might be one of the most 
difficult tasks for EFL (English as a Foreign 
Language) learners. This paper presents 
FLOW, a writing assistance system. It is built 
based on first-language-oriented input function 
and context sensitive approach, aiming at 
providing immediate and appropriate 
suggestions including translations, paraphrases, 
and n-grams during composing and revising 
processes. FLOW is expected to help EFL 
writers achieve their writing flow without being 
interrupted by their insufficient lexical 
knowledge.  
 
1. Introduction 
Writing in a second language (L2) is a challenging 
and complex process for foreign language learners. 
Insufficient lexical knowledge and limited 
exposure to English might interrupt their writing 
flow (Silva, 1993). Numerous writing instructions 
have been proposed (Kroll, 1990) as well as 
writing handbooks have been available for 
learners. Studies have revealed that during the 
writing process, EFL learners show the inclination 
to rely on their native languages (Wolfersberger, 
2003) to prevent a breakdown in the writing 
process (Arndt, 1987; Cumming, 1989). However, 
existing writing courses and instruction materials, 
almost second-language-oriented, seem unable to 
directly assist EFL writers while writing. 
This paper presents FLOW1 (Figure 1), an 
interactive system for assisting EFL writers in 
                                                          
1 FLOW: http:// flowacldemo.appspot.com 
composing and revising writing. Different from 
existing tools, its context-sensitive and first-
language-oriented features enable EFL writers to 
concentrate on their ideas and thoughts without 
being hampered by the limited lexical resources. 
Based on the studies that first language use can 
positively affect second language composing, 
FLOW attempts to meet such needs. Given any L1 
input, FLOW displays appropriate suggestions 
including translation, paraphrases, and n-grams 
during composing and revising processes. We use 
the following example sentences to illustrate these 
two functionalities.  
Consider the sentence ?We propose a method 
to?. During the composing stage, suppose a writer 
is unsure of the phrase ?solve the problem?, he 
could write ??????, a corresponding word in 
his native language, like ?We propose a method to 
?????. The writer?s input in the writing area 
of FLOW actively triggers a set of translation 
suggestions such as ?solve the problem? and 
?tackle the problem? for him/her to complete the 
sentence.  
In the revising stage, the writer intends to 
improve or correct the content. He/She is likely to 
change the sentence illustrated above into ?We try 
all means to solve the problem.? He would select 
the phrase ?propose a method? in the original 
sentence and input a L1 phrase ????, which 
specifies the meaning he prefers. The L1 input 
triggers a set of context-aware suggestions 
corresponding to the translations such as ?try our 
best? and ?do our best? rather than ?try your best? 
and ?do your best?. The system is able to do that 
mainly by taking a context-sensitive approach. 
FLOW then inserts the phrase the writer selects 
into the sentence. 
157
   
Figure 1. Screenshot of FLOW 
 
In this paper, we propose a context-sensitive 
disambiguation model which aims to automatically 
choose the appropriate phrases in different contexts 
when performing n-gram prediction, paraphrase 
suggestion and translation tasks. As described in 
(Carpuat and Wu, 2007), the disambiguation model 
plays an important role in the machine translation 
task. Similar to their work, we further integrate the 
multi-word phrasal lexical disambiguation model 
to the n-gram prediction model, paraphrase model 
and translation model of our system. With the 
phrasal disambiguation model, the output of the 
system is sensitive to the context the writer is 
working on. The context-sensitive feature helps 
writers find the appropriate phrase while 
composing and revising. 
This paper is organized as follows. We review 
the related work in the next section. In Section 3, 
we brief our system and method. Section 4 reports 
the evaluation results. We conclude this paper and 
point out future directions to research in Section 5. 
 
2. Related Work 
2.1 Sub-sentential paraphrases  
A variety of data-driven paraphrase extraction 
techniques have been proposed in the literature.  
One of the most popular methods leveraging 
bilingual parallel corpora is proposed by Bannard 
and Callison-Burch (2005). They identify 
paraphrases using a phrase in another language as a 
pivot. Using bilingual parallel corpora for 
paraphrasing demonstrates the strength of semantic 
equivalence. Another line of research further 
considers context information to improve the 
performance. Instead of addressing the issue of 
local paraphrase acquisition, Max (2009) utilizes 
the source and target contexts to extract sub-
sentential paraphrases by using pivot SMT 
systems. 
 
2.2 N-gram suggestions  
After a survey of several existing writing tools, we 
focus on reviewing two systems closely related to 
our study.  
PENS (Liu et al 2000), a machine-aided English 
writing system, provides translations of the 
corresponding English words or phrases for 
writers? reference. Different from PENS, FLOW 
further suggests paraphrases to help writers revise 
their writing tasks. While revising, writers would 
alter the use of language to express their thoughts. 
The suggestions of paraphrases could meet their 
need, and they can reproduce their thoughts more 
fluently.  
Another tool, TransType (Foster, 2002), a text 
editor, provides translators with appropriate 
translation suggestions utilizing trigram language 
model. The differences between our system and 
TransType lie in the purpose and the input. FLOW 
aims to assist EFL writers whereas TransType is a 
tool for skilled translators. On the other hand, in 
TransType, the human translator types translation 
of a given source text, whereas in FLOW the input, 
158
either a word or a phrase, could be source or target 
languages.  
 
2.3 Multi-word phrasal lexical disambiguation 
In the study more closely related to our work, 
Carpuat and Wu (2007) propose a novel method to 
train a phrasal lexical disambiguation model to 
benefit translation candidates selection in machine 
translation. They find a way to integrate the state-
of-the-art Word Sense Disambiguation (WSD) 
model into phrase-based statistical machine 
translation. Instead of using predefined senses 
drawn from manually constructed sense 
inventories, their model directly disambiguates 
between all phrasal translation candidates seen 
during SMT training. In this paper, we also use the 
phrasal lexical disambiguation model; however, 
apart from using disambiguation model to help 
machine translation, we extend the disambiguation 
model. With the help of the phrasal lexical 
disambiguation model, we build three models: a 
context-sensitive n-gram prediction model, a 
paraphrase suggestion model, and a translation 
model which are introduced in the following 
sections. 
 
3. Overview of FLOW  
The FLOW system helps language learners in two 
ways: predicting n-grams in the composing stage 
and suggesting paraphrases in the revising stage 
(Figure 2). 
3.1  System architecture 
Composing Stage 
During the composing process, a user inputs S.  
FLOW first determines if the last few words of S is 
a L1 input. If not, FLOW takes the last k words to 
predict the best matching following n-grams. 
Otherwise, the system uses the last k words as the 
query to predict the corresponding n-gram 
translation. With a set of prediction (either 
translations or n-grams), the user could choose an 
appropriate suggestion to complete the sentence in 
the writing area. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
NO  
Writing process  
Input K 
K consists of  first language 
First-Language-Oriented N-gram 
Prediction 
User interface 
Context-Sensitive N-gram Prediction 
YES  
Revising process 
Get word sequence L and R 
surrounding user selected text K 
Foreign Language F 
is input 
Ontext-Sensitive Paraphrase 
Suggestion  
First-Language-Oriented Paraphrase 
Suggestion 
User interface 
Input S 
NO  YES  
159
Figure 2. Overall Architecture of FLOW in writing and 
revising processes 
 
 
Revising Stage 
In the revising stage, given an input I and the user 
selected words K, FLOW obtains the word 
sequences L and R surrounding K as reference for 
prediction. Next, the system suggests sub-
sentential paraphrases for K based on the 
information of L and R. The system then searches 
and ranks the translations. 
 
3.2  N-gram prediction 
In the n-gram prediction task, our model takes the 
last k words with m 2 English words and n foreign 
language words, {e1, e2, ?em, f1, f2 ?fn}, of the 
source sentences S as the input. The output would 
be a set of n-gram predictions. These n-grams can 
be concatenated to the end of the user-composed 
sentence fluently. 
 
Context-Sensitive N-gram Prediction (CS-NP) 
The CS-NP model is triggered to predict a 
following n-gram when a user composes sentences 
consisted of only English words with no foreign 
language words, namely, n is equal to 0.  The goal 
of the CS-NP model is to find the English phrase e 
that maximizes the language model probability of 
the word sequence, {e1, e2, ?em, e}: 
 
? ? argmax
?,????
???|??, ??, ? ??? 
???|??, ??, ? ??? ? ?
???, ??, ? ??, ??
????, ??, ? ???   
 
Translation-based N-gram Prediction (TB-NP) 
When a user types a set of L1 expression f = { f1, f2 
?fn }, following the English sentences S, the 
FLOW system will predict the possible translations 
of f. A simple way to predict the translations is to 
find the bilingual phrase alignments T(f) using the 
method proposed by (Och and Ney, 2003). 
However, the T(f) is ambiguous in different 
contexts. Thus, we use the context {e1, e2, ?em} 
proceeding f to fix the prediction of the translation. 
Predicting the translation e can be treated as a sub-
sentential translation task: 
                                                          
2 In this paper, m = 5. 
 
? ? argmax?????? ???|???, ??, ? ???,  
 
where we use the user-composed context {e1, e2, 
?em} to disambiguate the translation of f. 
Although there exist more sophisticated models 
which could make a better prediction, a simple 
na?ve-Bayes model is shown to be accurate and 
efficient in the lexical disambiguation task 
according to (Yarowsky and Florian, 2002).  
Therefore, in this paper, a na?ve-Bayes model is 
used to disambiguate the translation of f. In 
addition to the context-word feature, we also use 
the context-syntax feature, namely surrounding 
POS tag Pos, to constrain the syntactic structure of 
the prediction. The TB-NP model could be 
represented in the following equation: 
 
?? ? argmax
?
?????1, ?2,? ??, 1?, ???,2??,? 
 
??? ? ???, ??, ? ???  
According to the Bayes theorem, 
 
?????1, ?2, ???, 1?, ???,2??
??????|?? ? ?????????
????????
 
The probabilities can be estimated using a parallel 
corpus, which is also used to obtain bilingual 
phrase alignment. 
 
3.3  Paraphrase Suggestion 
Unlike the N-gram prediction, in the paraphrase 
suggestion task, the user selects k words, {e1, e2, 
?ek}, which he/she wants to paraphrase. The 
model takes the m words {r1, r2, ?rm} and n words 
{l1, l2, ?ln} in the right and left side of the user- 
selected k words respectively. The system also 
accepts an additional foreign language input, {f1,f2, 
?fl}, which helps limit the meaning of suggested 
paraphrases to what the user really wants. The 
output would be a set of paraphrase suggestions 
that the user-selected phrases can be replaced by 
those paraphrases precisely. 
 
Context-Sensitive Paraphrase Suggestion (CS-
PS) 
The CS-PS model first finds a set of local 
paraphrases P of the input phrase K using the 
160
pivot-based method proposed by Bannard and 
Callison-Burch (2005). Although the pivot-based 
method has been proved efficient and effective in 
finding local paraphrases, the local paraphrase 
suggestions may not fit different contexts. Similar 
to the previous n-gram prediction task, we use the 
na?ve-Bayes approach to disambiguate these local 
paraphrases. The task is to find the best e such that 
e with the highest probability for the given context 
R and L. We further require paraphrases to have 
similar syntactic structures to the user-selected 
phrase in terms of POS tags, Pos. 
 
?? ? argmax
???
???|?1, ?2,? ??, 1?, 2?,? ???,??? 
 
Translation-based Paraphrase Suggestion (TB-
PS) 
After the user selects a phrase for paraphrasing, 
with a L1 phrase F as an additional input, the 
suggestion problem will be: 
 
?? ? argmax
????????????????
???|??, ??, ? ??, ??, ??, ? ??, ???? 
 
The TB-PS model disambiguates paraphrases from 
the translations of F instead of paraphrases P. 
 
4. Experimental Results 
In this section, we describe the experimental 
setting and the preliminary results. Instead of 
training a whole machine translation using toolkits 
such as Moses (Koehn et. al, 2007), we used only 
bilingual phrase alignment as translations to 
prevent from the noise produced by the machine 
translation decoder. Word alignments were 
produced using Giza++ toolkit (Och and Ney, 
2003), over a set of 2,220,570 Chinese-English 
sentence pairs in Hong Kong Parallel Text 
(LDC2004T08) with sentences segmented using 
the CKIP Chinese word segmentation system (Ma 
and Chen, 2003). In training the phrasal lexical 
disambiguation model, we used the English part of 
Hong Kong Parallel Text as our training data.  
To assess the effectiveness of FLOW, we selected 
10 Chinese sentences and asked two students to 
translate the Chinese sentences to English 
sentences using FLOW. We kept track of the 
sentences the two students entered. Table 1 shows 
the selected results. 
 
 
 
 
Model Results 
TB-PS ????, the price of rice... 
 in short 
 all in all 
 in a nutshell 
 in a word 
 to sum up 
CS-PS She looks forward to coming 
 look forward to 
 looked forward to 
 is looking forward to 
forward to
 expect 
CS-PS there is no doubt that ? 
 there is no question 
it is beyond doubt 
 I have no doubt 
beyond doubt
 it is true 
CS-NP We put forward ? 
 the proposal 
additional
 our opinion 
the motion
 the bill 
TB-NP ...on ways to identify tackle ?? 
 money laundering 
 money 
 his 
 forum entitled 
 money laundry 
Table 1. The preliminary results of FLOW 
 
Both of the paraphrase models CS-PS and TB-PS 
perform quite well in assisting the user in the 
writing task. However, there are still some 
problems such as the redundancy suggestions, e.g., 
?look forward to? and ?looked forward to?. 
Besides, although we used the POS tags as 
features, the syntactic structures of the suggestions 
are still not consistent to an input or selected 
phrases. The CS-NP and the TB-NP model also 
perform a good task. However, the suggested 
phrases are usually too short to be a semantic unit. 
The disambiguation model tends to produce shorter 
phrases because they have more common context 
features.  
 
161
5. Conclusion and Future Work  
In this paper, we presented FLOW, an interactive 
writing assistance system, aimed at helping EFL 
writers compose and revise without interrupting 
their writing flow. First-language-oriented and 
context-sensitive features are two main 
contributions in this work. Based on the studies on 
second language writing that EFL writers tend to 
use their native language to produce texts and then 
translate into English, the first-language-oriented 
function provides writers with appropriate 
translation suggestions. On the other hand, due to 
the fact that selection of words or phrases is 
sensitive to syntax and context, our system 
provides suggestions depending on the contexts. 
Both functions are expected to improve EFL 
writers? writing performance. 
In future work, we will conduct experiments to 
gain a deeper understanding of EFL writers? 
writing improvement with the help of FLOW, such 
as integrating FLOW into the writing courses to 
observe the quality and quantity of students? 
writing performance. Many other avenues exist for 
future research and improvement of our system. 
For example, we are interested in integrating the 
error detection and correction functions into 
FLOW to actively help EFL writers achieve better 
writing success and further motivate EFL writers 
to write with confidence. 
 
References  
Valerie Arndt. 1987. Six writers in search of texts: A 
protocol based study of L1 and L2 writing. ELT 
Journal, 41, 257-267. 
Colin Bannard and Chris Callison-Burch. 2005. 
Paraphrasing with bilingual parallel corpora. In 
Proceedings of ACL, pp. 597-604. 
Marine Carpuat and Dekai Wu. 2007. Improving 
Statistical Machine Translation using Word Sense 
Disambiguation. In Proceedings of EMNLP-CoNLL, 
pp 61?72. 
Alister Cumming. 1989. Writing expertise and second 
language proficiency. Language Learning, 39, 81-
141. 
George Foster, Philippe Langlais, and Guy Lapalme. 
2002. Transtype: Text prediction for translators. In 
Proceedings of ACL Demonstrations, pp. 93-94. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan,Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constrantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of ACL Demonstration Session, pp. 
177?180.  
Barbara Kroll. 1990. Second Language Writing: 
Research Insights for the Classroom. Cambridge 
University Press, Cambridge. 
Aur?elien Max. 2009. Sub-sentential Paraphrasing by 
Contextual Pivot Translation. In Proceedings of the 
2009 Workshop on Applied Textual Inference, ACL-
IJCNLP, pp 18-26. 
Tony Silva.  1993. Toward an Understanding of the 
Distinct Nature of L2 Writing: The ESL Research 
and Its Implications. TESOL Quarterly 27(4): 657?
77. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. In 
Proceedings of ACL, pp 529-536. 
Mark Wolfersberger. 2003. L1 to L2 writing process 
and strategy transfer: a look at lower proficiency 
writers. TESL-EJ: Teaching English as a Second or 
Foreign Language, 7(2), A6 1-15. 
 
162
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 34?37,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
GLANCE Visualizes Lexical Phenomena for Language Learning  
 
Mei-Hua Chen*, Shih-Ting Huang+, Ting-Hui Kao+, Sun-Wen Chiu+, Tzu-His Yen+ 
* Department of Foreign Languages and Literature, Hua Fan University, Taipei, Taiwan, 
R.O.C. 22301 
+ Department of Computer Science, National Tsing Hua University, HsinChu, Taiwan, 
R.O.C. 30013 
{chen.meihua,koromiko1104,maxis1718,chiuhsunwen, joseph.yen}@gmail.com 
 
 
Abstract 
Facilitating vocabulary knowledge is a 
challenging aspect for language learners. 
Although current corpus-based reference 
tools provide authentic contextual clues, the 
plain text format is not conducive to fully 
illustrating some lexical phenomena. Thus, 
this paper proposes GLANCE 1 , a text 
visualization tool, to present a large amount 
of lexical phenomena using charts and graphs, 
aimed at helping language learners 
understand a word quickly and intuitively. To 
evaluate the effectiveness of the system, we 
designed interfaces to allow comparison 
between text and graphics presentation, and 
conducted a preliminary user study with ESL 
students. The results show that the visualized 
display is of greater benefit to the 
understanding of word characteristics than 
textual display. 
1 Introduction 
Vocabulary is a challenging aspect for language 
learners to master. Extended word knowledge, 
such as word polarity and position, is not widely 
available in traditional dictionaries. Thus, for 
most language learners, it is very difficult to 
have a good command of such lexical phenome-
na.  
Current linguistics software programs use 
large corpus data to advance language learning. 
The use of corpora exposes learners to authentic 
contextual clues and lets them discover patterns 
or collocations of words from contextual clues 
(Partington, 1998). However, a huge amount of 
data can be overwhelming and time-consuming 
(Yeh et al., 2007) for language learners to induce 
rules or patterns. On the other hand, some lexical 
phenomena seem unable to be comprehended 
                                                 
1 http://glance-it.herokuapp.com/ 
fast and directly in plain text format (Koo, 2006). 
For example, in the British National Corpus 
(2007), ?however? seems more negative than 
?but?. Also, compared with ?but?, ?however? 
appears more frequently at the beginning of a 
sentence. 
With this in mind, we proposed GLANCE1, a 
text visualization tool, which presents corpus 
data using charts and graphs to help language 
learners understand the lexical phenomena of a 
word quickly and intuitively. In this paper, we 
focused on five types of lexical phenomena: po-
larity, position, POS, form and discipline, which 
will be detailed in the Section 3. Given a single 
query word, the GLANCE system shows graph-
ical representations of its lexical phenomena se-
quentially within a single web page.  
Additionally we believe that the use of 
graphics also facilitates the understanding of the 
differences between two words. Taking this into 
consideration, we introduce a comparison mode 
to help learners differentiate two words at a 
glance. Allowing two word input, GLANCE 
draws the individual representative graphs for 
both words and presents these graphs in a two-
column view. The display of parallel graphs de-
picts the distinctions between the two words 
clearly. 
 
2 Related Work 
Corpus-based language learning has widened the 
perspectives in second and foreign language edu-
cation, such as vocabulary learning (Wood, 
2001). In past decades, various corpus-based ref-
erence tools have been developed. For example, 
WordSmith (Scott, 2000), Compleat Lexical Tu-
tor (Cobb, 2007), GRASP (Huang et al., 2011), 
PREFER (Chen et al, 2012). 
Recently, some interactive visualization tools 
have been developed for the purpose of illustrat-
ing various linguistic phenomena. Three exam-
34
ples are Word Tree, a visual concordance (Wat-
tenberg and  i gas, 2008), WORDGRAPH, a 
visual tool for context-sensitive word choice 
(Riehmann et al., 2012) and Visual Thesaurus, a 
3D interactive reference tool (ThinkMap Inc., 
2005). 
 
3 Design of the GLANCE System 
The GLANCE system consists of several com-
ponents of corpus data visualization. We design 
and implement these visualization modules sepa-
rately to ensure all graphs are simple and clear 
enough for users to capture and understand the 
lexical phenomena quickly. 
In this paper, we use the d3.js (Data-Driven 
Documents) (Bostock et al., 2011) to visualize 
the data. The d3.js enables direct inspection and 
manipulation of a standard document object 
model (DOM) so that we are able to transform 
numeric data into various types of graphs when 
fitting these data to other visualization tools. In 
this section, we describe the ways we extract the 
data from the corpus and how we translate these 
data into informative graphs. 
 
3.1 Data Preprocessing 
We use the well-formed corpus, the BNC, to ex-
tract the data. In order to obtain the Part-of-
speech tags for each text, we use the GENIA 
tagger (Tsuruoka et al., 2005) to analyze the sen-
tences of the BNC and build a list of <POS-tag, 
frequency> pairs for each word in the BNC. Also 
the BNC contains the classification code as-
signed to the text in a genre-based analysis car-
ried out at Lancaster University by Lee (2001). 
For each word, the classification codes are ag-
gregated to a list of <code, frequency> pairs.  
  
3.2 Visualization of Lexical Phenomena  
Polarity 
A word may carry different sentiment polarities 
(i.e., positive, negative and objective). To help 
users quickly determine the proper sentiment 
polarity of a word, we introduce the sentiment 
polarity information of SentiWordNet 
(Baccianella et al., 2010) into our system. For 
each synset of a word, GLANCE displays the 
polarity in a bar with three different colors. The 
individual length of the three parts in the bar cor-
responds to the polarity scores of a synset (Fig-
ure 1). 
 
 
Figure 1. Representation of sentiment polarity  
 
Position 
The word position in a sentence is also an im-
portant lexical phenomenon. By calculating the 
word position in each sentence, we then obtain 
the location distribution. GLANCE visualizes the 
distribution information of a word using a bar 
chart. Figure 2 shows a plot of distribution of 
word position on the x-axis against the word fre-
quency on the y-axis. 
 
 
Figure 2. Distribution of word position 
 
Part Of Speech (POS) 
A lexical item may have more than one part of 
speech. Knowing the distribution of POS helps 
users quickly understand the general usage of a 
word.  
GLANCE displays a pie chart for each word 
to differentiate between its parts of speech. We 
use the maximum likelihood probability of a 
POS tag for a word as the arc length of the pie 
chart (Figure 3). 
 
 
35
Figure 3. POS representation 
 
Form 
The levels of formality of written and spoken 
language are different, which also confuse lan-
guage learners. Pie charts are used to illustrate 
the proportion of written and spoken English of 
individual words as shown in Figure 4. 
We derive the frequencies of both forms from 
the BNC classification code for each word. The 
arc length of each sector is proportional to the 
maximum likelihood probability of forms. 
 
 
Figure 4. Form representation 
 
Discipline 
Similar to language form, the discipline infor-
mation (e.g., newspaper or fiction) was gathered 
from the BNC classification code. The relations 
of the disciplines of a word are presented using a 
sunburst graph, a radial space-filling tree layout 
implemented with prefuse (Heer et al., 2005). In 
the sunburst graph (Figure 5.), each level corre-
sponds to the relation of the disciplines of a cer-
tain word. The farther the level is away from the 
center, the more specific the discipline is. Each 
level is given equal width, but the circular angle 
swept out by a discipline corresponds to the fre-
quency of the disciplines. 
 
  
Figure 5. Discipline relations 
 
4 Results 
4.1 Experimental Setting 
We performed a preliminary user study to assess 
the efficiency of our system in assisting language 
learners in grasping lexical phenomena. To ex-
amine the effectiveness of visualization, we built 
a textual interface for comparison with the 
graphical interface. 
Ten pre-intermediate ESL college students 
participated in the study. A total of six pairs of 
similar words were listed on the worksheet. After 
being introduced to GLANCE, all students were 
randomly divided into two groups. One group 
was required to consult the first three pairs using 
the graphical interface and the second three pairs 
the textual interface, and vice versa. The partici-
pants were allowed a maximum of one minute 
per pair, which meets the goal of this study of 
quickly glancing at the graphics and grasping the 
concepts of words. Then a test sheet containing 
the same six similar word pairs was used to ex-
amine the extent of students? word understanding. 
Note that during the test, no tool supports were 
provided. The student scored one point if he gave 
the correct answers to each question. In other 
words he would be awarded 6 points (the highest 
number of points) if he provided all the correct 
answers. They also completed a questionnaire, 
described below, evaluating the system. 
 
4.2 Experimental Results 
To determine the effectiveness of visualization of 
lexical phenomena, the students? average scores 
were used as performance indicators. Students 
achieved the average score 61.9 and 45.0 out of 
100.00 after consulting the graphic interface and 
textual interface respectively. Overall, the visual-
ized display of word characteristics outper-
formed the textual version. 
The questionnaire revealed that all the partici-
pants showed a positive attitude to visualized 
word information. Further analyses showed that 
all ten participants appreciated the position dis-
play and nine of them the polarity and form dis-
plays. In short, the graphical display of lexical 
phenomena in GLANCE results in faster assimi-
lation and understanding of word information. 
Moreover, the participants suggested several in-
teresting aspects for improving the GLANCE 
system. For example, they preferred bilingual 
environment, further information concerning an-
tonyms, more example sentences, and increased 
36
detail in the sunburst representation of disci-
plines. 
 
5 Conclusion and Future Work 
In this paper, we proposed GLANCE, a text vis-
ualization tool, which provides graphical display 
of corpus data. Our goal is to assist language 
learners in glancing at the graphics and grasping 
the lexical knowledge quickly and intuitively. To 
evaluate the efficiency and effectiveness of 
GLANCE, we conducted a preliminary user 
study with ten non-native ESL learners. The re-
sults revealed that visualization format outper-
formed plain text format. 
Many avenues exist for future research and 
improvement. We attempt to expand the single 
word to phrase level. For example, the colloca-
tion behaviors are expected to be deduced and 
displayed. Moreover, we are interested in sup-
porting more lexical phenomena, such as hypo-
nyms, to provide learners with more lexical rela-
tions of the word with other words. 
 
Reference 
Baccianella, S., Esuli, A., & Sebastiani, F. (2010, 
May). SentiWordNet 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion Min-
ing. In LREC (Vol. 10, pp. 2200-2204). 
Bostock, M., Ogievetsky, V., & Heer, J. (2011). D? 
data-driven documents. Visualization and Comput-
er Graphics, IEEE Transactions on, 17(12), 2301-
2309. 
Chen, M. H., Huang, S. T., Huang, C. C., Liou, H. C., 
& Chang, J. S. (2012, June). PREFER: using a 
graph-based approach to generate paraphrases for 
language learning. In Proceedings of the Seventh 
Workshop on Building Educational Applications 
Using NLP (pp. 80-85). Association for Computa-
tional Linguistics. 
Cobb, T. (2007). The compleat lexical tutor. Retrieved 
September, 22, 2009. 
Heer, J., Card, S. K., & Landay, J. A. (2005, April). 
Prefuse: a toolkit for interactive information visual-
ization. In Proceedings of the SIGCHI conference 
on Human factors in computing systems (pp. 421-
430). ACM. 
Huang, C. C., Chen, M. H., Huang, S. T., Liou, H. C., 
& Chang, J. S. (2011, June). GRASP: grammar-
and syntax-based pattern-finder in CALL. 
In Proceedings of the 6th Workshop on Innovative 
Use of NLP for Building Educational Applications 
(pp. 96-104). Association for Computational Lin-
guistics. 
Kyosung Koo (2006). Effects of using corpora and 
online reference tools on foreign language writing: 
a study of Korean learners of English as a second 
language. PhD. dissertation, University of Iowa. 
Lee, D. Y. (2001). Genres, registers, text types, do-
mains and styles: Clarifying the concepts and 
nevigating a path through the BNC jungle. 
Partington, A. (1998). Patterns and meanings: Using 
corpora for English language research and teach-
ing (Vol. 2). John Benjamins Publishing. 
Riehmann, P., Gruendl, H., Froehlich, B., Potthast, M., 
Trenkmann, M., & Stein, B. (2011, March). The 
NETSPEAK WORDGRAPH: Visualizing key-
words in context. In Pacific Visualization Sympo-
sium (PacificVis), 2011 IEEE (pp. 123-130). IEEE. 
Scott, M. (2004). WordSmith tools version 4. 
The British National Corpus, version 3 (BNC XML 
Edition). 2007. Distributed by Oxford University 
Computing Services on behalf of the BNC Consor-
tium. URL: http://www.natcorp.ox.ac.uk/ 
ThinkMap Inc. (2005). Thinkmap Visual Thesaurus. 
Available from http:// www.visualthesaurus.com 
Tsuruoka, Y., Tateishi, Y., Kim, J. D., Ohta, T., 
McNaught, J., Ananiadou, S., & Tsujii, J. I. (2005). 
Developing a robust part-of-speech tagger for bio-
medical text. In Advances in informatics (pp. 382-
392). Springer Berlin Heidelberg. 
Wattenberg, M., & Vi?gas, F. B. (2008). The word 
tree, an interactive visual concord-
ance. Visualization and Computer Graphics, IEEE 
Transactions on, 14(6), 1221-1228. 
Wood, J. (2001). Can software support children?s 
vocabulary development.Language Learning & 
Technology, 5(1), 166-201. 
Yeh, Y., Liou, H. C., & Li, Y. H. (2007). Online syn-
onym materials and concordancing for EFL college 
writing. Computer Assisted Language Learning, 
20(2), 131-152. 
 
37
