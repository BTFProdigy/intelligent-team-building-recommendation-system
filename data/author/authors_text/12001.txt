Proceedings of NAACL HLT 2009: Short Papers, pages 165?168,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Search Result Re-ranking by Feedback Control Adjustment for
Time-sensitive Query
Ruiqiang Zhang? and Yi Chang? and Zhaohui Zheng?
Donald Metzler? and Jian-yun Nie?
?Yahoo! Labs, 701 First Avenue, Sunnyvale, CA94089
?University of Montreal, Montreal, Quebec,H3C 3J7, Canada
?{ruiqiang,yichang,zhaohui,metzler}@yahoo-inc.com
?nie@iro.umontreal.ca
Abstract
We propose a new method to rank a special
category of time-sensitive queries that are year
qualified. The method adjusts the retrieval
scores of a base ranking function according
to time-stamps of web documents so that the
freshest documents are ranked higher. Our
method, which is based on feedback control
theory, uses ranking errors to adjust the search
engine behavior. For this purpose, we use
a simple but effective method to extract year
qualified queries by mining query logs and a
time-stamp recognition method that considers
titles and urls of web documents. Our method
was tested on a commercial search engine. The
experiments show that our approach can sig-
nificantly improve relevance ranking for year
qualified queries even if all the existing meth-
ods for comparison failed.
1 Introduction
Relevance ranking plays a crucial role in search
engines. There are many proposed machine learn-
ing based ranking algorithms such as language
modeling-based methods (Zhai and Lafferty, 2004),
RankSVM (Joachims, 2002), RankBoost (Freund et al,
1998) and GBrank (Zheng et al, 2007). The input to
these algorithms is a set of feature vectors extracted from
queries and documents. The goal is to find the parameter
setting that optimizes some relevance metric given
training data. While these machine learning algorithms
can improve average relevance, they may be ineffctive
for certain special cases. Time-sensitive queries are one
such special case that machine-learned ranking functions
may have a hard time learning, due to the small number
of such queries.
Consider the query ?sigir? (the name of a conference),
which is time sensitive. Table 1 shows two example
search result pages for the query, SERP1 and SERP2. The
query: sigir
SERP1 url1: http://www.sigir.org
url2: http://www.sigir2008.org
url3: http://www.sigir2004.org
url4: http://www.sigir2009.org
url5: http://www.sigir2009.org/schedule
SERP2 url1: http://www.sigir.org
url2: http://www.sigir2009.org
url3: http://www.sigir2009.org/schedule
url4: http://www.sigir2008.org
url5: http://www.sigir2004.org
Table 1: Two contrived search engine result pages
ranking of SERP2 is clearly better than that of SERP1 be-
cause the most recent event, ?sigir2009?, is ranked higher
than other years.
Time is an important dimension of relevance in web
search, since users tend to prefer recent documents to old
documents. At the time of this writing (February 2009),
none of the major commercial search engines ranked the
homepage for SIGIR 2009 higher than previous SIGIR
homepages for the query ?sigir?. One possible reason for
this is that ranking algorithms are typically based on an-
chor text features, hyperlink induced features, and click-
through rate features. However, these features tend to fa-
vor old pages more than recent ones. For example, ?si-
gir2008? has more links and clicks than ?sigir2009? be-
cause ?sigir2008? has existed longer time and therefore
has been visited more. It is less likely that newer web
pages from ?sigir2009? can be ranked higher using fea-
tures that implicitly favor old pages.
However, the fundamental problem is that current ap-
proaches have focused on improving general ranking al-
gorithms. Methods for improving ranking of specific
types of query like temporal queries are often overlooked.
Aiming to improve ranking results, some methods of
re-ranking search results are proposed, such as the work
by (Agichtein et al, 2006) and (Teevan et al, 2005).
165
Search Engine
Detector 
Controller
error R(q, yn)R(q, yo)
_
+
Figure 1: Feedback control for search engine
These work uses user search behavior information or per-
sonalization information as features that are integrated
into an enhanced ranking model. We propose a novel
method of re-ranking search results. This new method
is based on feedback control theory, as illustrated in 1.
We make a Detector to monitor search engine (SE) out-
put and compare it with the input, which is the desired
search engine ranking. If an error is found, we design
the controller that uses the error to adjust the search en-
gine output, such that the search engine output tracks the
input. We will detail the algorithm in Section 4.1.
Our method was applied to a special class of time-
sensitive query, year qualified queries (YQQs). For this
category, we found users either attached a year with the
query explicitly, like ?sigir 2009?, or used the query only
without a year attached,like ?sigir?. We call the former
explicit YQQs, and the latter implicit YQQs. Using query
log analysis, we found these types of queries made up
about 10% of the total query volume. We focus exclu-
sively on implicit YQQs by translating the user?s im-
plicit intention as the most recent year. Explicit YQQs
are less interesting, because the user?s temporal inten-
tion is clearly specified in the query. Therefore, rank-
ing for these types of queries is relatively straightfor-
ward. Throughout the remainder of this paper, we use
the ?YQQ? to refer to implicit YQQs, unless otherwise
stated.
2 Adaptive score adjustment
Our proposed re-ranking model is shown in Eq. 1, as be-
low.
F(q, d) =
{
R(q, d) if q < YQQ
R(q, d) + Q(q, d) otherwise
Q(q, d) =
{ (e(do, dn) + k)e??(q) if y(d) = yn
0 otherwise
e(do, dn) = R(q, do) ? R(q, dn)
(1)
This work assumes that a base ranking function is used
to rank documents with respect to an incoming query. We
denote this base ranking function as R(q, d). This ranking
function is conditioned on a query q and a document d. It
is assumed to model the relevance between q and d.
Our proposed method is flexible for all YQQ queries.
Suppose the current base ranking function gives the re-
sults as SERP1 of Table 1. To correct the ranking, we
propose making an adjustment to R(q, d).
In Eq. 1, F(q, d) is the final ranking function. If the
query is not an YQQ, the base ranking function is used.
Otherwise, we propose an adjustment function, Q(q, d) ,
to adjust the base ranking function. Q(q, d) is controlled
by the ranking error, e(do, dn), signifying the base func-
tion ranking error if the newest web page dn is ranked
lower than the oldest web page do. y(d) is the year that
the event described by d has occurred or will occur. If
yo and yn indicate the oldest year and the newest year,
then y(do) = yo, y(dn) = yn. R(q, do) and R(q, dn) are the
base ranking function scores for the oldest and the newest
documents.
k is a small shift value for direction control. When
k < 0, the newest document is adjusted slightly under the
old one. Otherwise, it is adjusted slightly over the old
one. Experiments show k > 0 gave better results. The
value of k is determined in training.
?(q) is the confidence score of a YQQ query, mean-
ing the likelihood of a query to be YQQ. The confidence
score is bigger if a query is more likely to be YQQ. More
details are given in next section. ? is a weighting param-
eter for adjusting ?(q).
The exp function e??(q) is a weighting to control boost-
ing value. A higher value, confidence ?, a larger boosting
value, Q(q, d).
Our method can be understood by feedback control
theory, as illustrated in Fig. 1. The ideal input is R(q, yo)
representing the desired ranking score for the newest
Web page, R(q, yn). But the search engine real output
is R(q, yn). Because search engine is a dynamic system,
its ranking is changing over time. This results in ranking
errors, e(do, dn) = R(q, do) ? R(q, dn). The function of
?Controller? is to design a function to adjust the search
engine ranking so that the error approximates to zero,
e(do, dn) = 0. For this work, ?Controller? is Q(q, d).
?Detector? is a document year-stamp recognizer, which
will be described more in the next section. ?Detector?
is used to detect the newest Web pages and their ranking
scores. Fig. 1 is an ideal implementation of our methods.
We cannot carry out real-time experiments in this work.
Therefore, the calculation of ranking errors was made in
offline training.
3 YQQ detection and year-stamp
recognition
To implement Eq. 1, we need to find YQQ queries and to
identify the year-stamp of web documents.
Our YQQ detection method is simple, efficient, and
relies only on having access to a query log with frequency
information. First, we extracted all explicit YQQs from
166
query log. Then, we removed all the years from explicit
YQQs. Thus, implicit YQQs are obtained from explicit
YQQs. The implicit YQQs are saved in a dictionary. In
online test, we match input queries with each of implicit
YQQs in the dictionary. If an exact match is found, we
regard the input query as YQQ, and apply Eq. 1 to re-rank
search results.
After analyzing samples of the extracted YQQs, we
group them into three classes. One is recurring-event
query, like ?sigir?, ?us open tennis?; the second is news-
worthy query, like ?steve ballmer?, ?china foreign re-
serves?; And the class not belong to any of the above
two, like ?christmas?, ?youtube?. We found our proposed
methods were the most effective for the first category. In
Eq. 1, we can use confidence ?(q) to distinguish the three
categories and their change of ranking as shown in Eq.1,
that is defined as below.
?(q) =
?
y w(q, y)
#(q) +?y w(q, y)
(2)
where w(q, y) = #(q.y)+#(y.q). #(q.y) denotes the num-
ber of times that the base query q is post-qualified with
the year y in the query log. Similarly, #(y.q) is the num-
ber of times that q is pre-qualified with the year y. This
weight measures how likely q is to be qualified with y,
which forms the basis of our mining and analysis. #(q) is
the counts of independent query, without associating with
any other terms.
We also need to know the year-stamp y(d) for each
web document so that the ranking score of a document
is updated if y(d) = yn is satisfied. We can do this
from a few sources such as title, url, anchar text, and
extract date from documents that is possible for many
news pages. For example, from url of the web page,
?www.sigir2009.org?, we detect its year-stamp is 2009.
We have also tried to use some machine generated
dates. However, in the end we found such dates are in-
accurate and cannot be trusted. For example, discovery
time is the time when the document was found by the
crawler. But a web document may exist several years be-
fore a crawler found it. We show the worse effect of using
discovery time in the experiments.
4 Experiments
We will describe the implementation methods and experi-
mental results in this section. Our methods include offline
dictionary building and online test. In offline training, our
first step is to mine YQQs. A commercial search engine
company provided us with six months of query logs. We
extracted a list of YQQs using Section 3?s method. For
each of the YQQs, we run the search engine and output
the top N results. For each document, we used the method
described in Section 3 to recognize the year-stamp and
find the oldest and the newest page. If there are multiple
urls with the same yearstamp, we choose the first oldest
and the first most recent. Next,we calculated the boost-
ing value according to Eq. 1. Each query has a boosting
value. For online test, a user?s query is matched with each
of the YQQs in the dictionary. If an exact match is found,
the boosting value will be added to the base ranking score
iff the document has the newest yearstamp.
For evaluating our methods, we randomly extracted
600 YQQs from the dictionary. We extracted the top-5
search results for each of queries using the base ranking
function and the proposed ranking function. We asked
human editors to judge all the scraped results. We used
five judgment grades: Perfect, Excellent, Good, Fair,
and Bad. Editors were instructed to consider temporal
issues when judging. For example, sigir2004 is given
a worse grade than sigir2009. To avoid bias, we ad-
vised editors to retain relevance as their primary judg-
ment criteria. Our evaluation metric is relative change
in DCG, %?dcg = DCGproposed?DCGbaselineDCGbaseline , where DCG is
the traditional Discounted Cumulative Gain (Jarvelin and
Kekalainen, 2002).
4.1 Effect of the proposed boosting method
Our experimental results are shown in Table 2, where we
compared our work with the existing methods. While we
cannot apply (Li and Croft, 2003)?s approach directly be-
cause first, our search engine is not based on language
modeling; second, it is impossible to obtain exact times-
tamp for web pages as (Li and Croft, 2003) did in the
track evaluation. However, we tried to simulate (Li and
Croft, 2003)?s approach in web search by using the linear
integration method exactly as the same as(Li and Croft,
2003) by adding a time-based function with our base
ranking function. For the timestamp, we used discovery
time in the time-based function. The parameters (?, ?)
have the exact same meaning as in (Li and Croft, 2003)
but were tuned according to our base ranking function.
With regards to the approach by (Diaz and Jones, 2004),
we ranked the web pages in decreasing order of discov-
ery time. Our own approaches were tested under options
with and without using adaptation. For no adaption, we
let the e of Eq.1 equal to 0, meaning no score difference
between the oldest document and the newest document
was captured, but a constant value was used. It is equiv-
alent to an open loop in Fig.1. For adaption, we used the
ranking errors to adjust the base ranking. In the Table we
used multiple ks to show the effect of changing k. Using
different k can have a big impact on the performance. The
best value we found was k = 0.3. In this experiment, we
let ?(q) = 0 so that the result responds to k only.
Our approach is significantly better than the existing
methods. Both of the two existing methods produced
worse results than the baseline, which shows the ap-
167
Li & Croft (?, ?)=(0.2,2.0) -0.5
(?, ?)=(0.2,4.0) -1.2
Diaz & Jones -4.5?
No adaptation (e = 0, k=0.3 1.2
open loop) k=0.4 0.8
Adaptation (closed loop) k=0.3 6.6?
k=0.4 6.2?
Table 2: %?dcg of proposed method comparing with
existing methods.A sign ??? indicates statistical signifi-
cance (p-value<0.05)
? 0 0.2 0.4 0.6 0.8 1.0
%?dcg 6.6? 7.8? 8.4? 4.5 2.1 -0.2?
Table 3: Effect of confidence as changing ?.
proaches may be inappropriate for Web search. Not sur-
prisingly, using adaption achieved much better results
than without using adaption. Thus, these experiments
prove the effectiveness of our proposed methods.
Another important parameter in the Eq.1 is the confi-
dence score ?(q), which indicates the confidence of query
to be YQQ. In Eq. 1, ? is used to adjusting ?(q). We
observed dcg gain for each different ?. The results are
shown in Table 3. The value of ? needs to be tuned for
different base ranking functions. A higher ? can hurt per-
formance. In our experiments, the best value of 0.4 gave a
8.4% statistically significant gain in DCG. The ? = 0 set-
ting means we turn off confidence, which results in lower
performance. Thus, using YQQ confidence is effective.
5 Discussions and conclusions
In this paper, we proposed a novel approach to solve
YQQ ranking problem, which is a problem that seems
to plague most major commercial search engines. Our
approach for handling YQQs does not involve any query
expansion that adds a year to the query. Instead, keeping
the user?s query intact, we re-rank search results by ad-
justing the base ranking function. Our work assumes the
intent of YQQs is to find documents about the most recent
year. For this reason, we use YQQ confidence to measure
the probability of this intent. As our results showed, our
proposed method is highly effective. A real example is
given in Fig. 2 to show the significant improvement by
our method.
Our adaptive methods are not limited to YQQs only.
We believe this framework can be applied to any category
of queries once a query classification and a score detector
have been implemented.
Figure 2: Ranking improvement for query ICML by our
method: before re-rank(left) and after(right)
References
Eugene Agichtein, Eric Brill, and Susan Dumais. 2006.
Improving web search ranking by incorporating user
behavior information. In SIGIR ?06, pages 19?26.
Fernando Diaz and Rosie Jones. 2004. Using temporal
profiles of queries for precision prediction. In Proc.
27th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, pages 18?24, New
York, NY, USA. ACM.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In ICML ?98: Proceedings
of the Fifteenth International Conference on Machine
Learning, pages 170?178.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 133?
142.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based
language models. In Proc. 12th Intl. Conf. on Infor-
mation and Knowledge Management, pages 469?475,
New York, NY, USA. ACM.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005.
Personalizing search via automated analysis of inter-
ests and activities. In SIGIR ?05, pages 449?456.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Trans. Inf. Syst., 22(2):179?
214.
Zhaohui Zheng, Keke Chen, Gordon Sun, and Hongyuan
Zha. 2007. A regression framework for learning rank-
ing functions using relative relevance judgments. In
SIGIR ?07, pages 287?294.
168
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 646?655,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Structured Event Retrieval over Microblog Archives
Donald Metzler, Congxing Cai, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
Abstract
Microblog streams often contain a consider-
able amount of information about local, re-
gional, national, and global events. Most ex-
isting microblog search capabilities are fo-
cused on recent happenings and do not provide
the ability to search and explore past events.
This paper proposes the problem of structured
retrieval of historical event information over
microblog archives. Rather than retrieving in-
dividual microblog messages in response to an
event query, we propose retrieving a ranked
list of historical event summaries by distill-
ing high quality event representations using
a novel temporal query expansion technique.
The results of an exploratory study carried
out over a large archive of Twitter messages
demonstrates both the value of the microblog
event retrieval task and the effectiveness of our
proposed search methodologies.
1 Introduction
Real-time user generated content is one of the key
driving forces behind the growing popularity of so-
cial media-centric communication. The ability to in-
stantly share, often from your mobile phone, your
thoughts (via Twitter), your photos (via Facebook),
your location (via Foursquare), and a variety of other
information is changing the way that information is
created, communicated, and consumed.
There has been a substantial amount of research
effort devoted to user generated content-related
search tasks, including blog search, forum search,
and community-based question answering. How-
ever, there has been relatively little research on mi-
croblog search. Microblog services, such as Tumblr
and Twitter, provide users with the ability to broad-
cast short messages in real-time. This is in contrast
to traditional blogs that typically have considerably
more content that is updated less frequently. By
their very nature, microblog streams often contain
a considerable amount of information about local,
regional, national, and global news and events. A
recent study found that over 85% of trending topics
on Twitter are news-related (Kwak et al, 2010). An-
other recent study by Teevan et al that investigated
the differences between microblog and Web search
reported similar findings (Teevan et al, 2011). The
study also found that microblog search queries are
used to find information related to news and events,
while Web search queries are more navigational in
nature and used to find a variety of information on a
specific topic.
It is likely that microblogs have not received much
attention because, unlike blog search, there is no
well-defined microblog search task. Existing mi-
croblog search services, such as those offered by
Twitter and Google, only provide the ability to re-
trieve individual microblog posts in response to a
query. Unfortunately, this task has limited utility
since very few real information needs can be satis-
fied by a single short piece of text (e.g., the max-
imum length of a message on Twitter is 140 char-
acters). Hence, novel search tasks defined over mi-
croblog streams that go beyond ?message retrieval?
have the potential to add substantial value to users.
Given the somewhat limited utility of microblog
message search and the preponderance of news and
event-related material posted on microblogs, this pa-
646
July 16 2010 at 17 UTC, for 11 hours
Summary tweets:
i. Ok a 3.6 ?rocks? nothing. But boarding a plane
there now, Woodward ho! RT @todayshow: 3.6 mag-
nitude #earthquake rocks Washington DC area.
ii. RT @fredthompson: 3.6-magnitude earthquake hit
DC. President Obama said it was due to 8 years of
Bush failing to regulate plate tectonic ...
iii. 3.6-magnitude earthquake wakes Md. residents:
Temblor centered in Gaithersburg felt by as many as
3 million people... http://bit.ly/9iMLEk
Figure 1: Example structured event representation re-
trieved for the query ?earthquake?.
per proposes a novel search task that we call mi-
croblog event retrieval. Given a query that describes
an event, such as earthquake, terrorist bombing, or
bieber concert, the goal of the task is to retrieve a
ranked list of structured event representations, such
as the one shown in Figure 1, from a large archive of
historical microblog posts.
In this work, structured representations come in
the form of a list of timespans during which an in-
stance of the event occurred and was actively dis-
cussed within the microblog stream. Additionally,
for each timespan, a small set of relevant messages
are retrieved for the purpose of providing a high-
level summary of the event that occurred during the
timespan. This task leverages the large amount of
real-time, often first-hand information found in mi-
croblog archives to deliver a novel form of user gen-
erated content-based search results to users. Unlike
news search, which finds professionally written ar-
ticles on a news-related topic, and general-purpose
Web search, which is likely to find a large amount
of unrelated information, this task is designed to re-
trieve highly relevant news and event-related infor-
mation viewed through the lens of users who ex-
perienced or discussed the event while it happened
(or during its aftermath). Such search functional-
ity would not only be useful for everyday end-users,
but also social scientists, historians, journalists, and
emergency planners.
This paper has three primary contributions. First,
we introduce the microblog event retrieval task,
which retrieves a ranked list of structured event rep-
resentations in response to an event query. By going
beyond individual microblog message retrieval, the
task adds value to microblog archives and provides
users with the ability to find information that was
disseminated in real-time about past events, which
is not possible with news and Web search engines.
Second, we propose an unsupervised methodology
for distilling high quality event representations using
a novel temporal query expansion technique. The
technique synthesizes ideas from pseudo-relevance
feedback, term burstiness, and temporal aspects
of microblog streams. Third, we perform an ex-
ploratory evaluation of 50 event queries over a cor-
pus of 46 million Twitter messages. The results of
our evaluation demonstrate both the value of the mi-
croblog event retrieval task itself and the effective-
ness of our proposed search methodologies, which
show improvements of up to 42% compared to a
baseline approach.
2 Related Work
There are several directions of microblog research
that are related to our proposed work. First, there is
a growing body of literature that has focused on the
topical content of microblog posts. This research
has focused on microblog topic models (Hong and
Davison, 2010), event and topic detection and track-
ing (Sankaranarayanan et al, 2009; Cataldi et al,
2010; Petrovic? et al, 2010; Lin et al, 2010), predict-
ing flu outbreaks using keyword tracking (Culotta,
2010), and using microblog streams as a source
of features for improving recency ranking in Web
search (Dong et al, 2010). Most of these approaches
analyze content as it arrives in the system. While
tracking a small number of topics or keywords is fea-
sible using online algorithms, the general problem
of topic detection and tracking (Allan et al, 1998) is
considerably more challenging given the large num-
ber of topics being discussed at any one point. Our
work differs in that it does not attempt to track or
model topics as they arrive in the system. Instead,
given an event query, our system retrospectively an-
alyzes the corpus of microblog messages for the pur-
pose of retrieving structured event representations.
There is no shortage of previous work on using
pseudo-relevance feedback approaches for query ex-
pansion. Relevant research includes classical vector-
space approaches (Rocchio, 1971), language mod-
647
eling approaches (Lavrenko and Croft, 2001; Zhai
and Lafferty, 2001; Li and Croft, 2003), among oth-
ers (Metzler and Croft, 2007; Cao et al, 2008; Lv
and Zhai, 2010). The novel aspect of our proposed
temporal query expansion approach is the fact that
expansion is done over a temporal stream of very
short, noisy messages.
There has also been recent work on summarizing
sets of microblog posts (Sharifi et al, 2010). We
chose to make use of a simple approach in favor of
a more sophisticated one because summarization is
only a minor aspect of our proposed framework.
Finally, there are two previous studies that are
the most relevant to our work. First, Massoudi et
al. propose a retrieval model that uses query ex-
pansion and microblog quality indicators to retrieve
individual microblog messages (Massoudi et al,
2011). Their proposed query expansion approach
differs from ours in the sense that we utilize times-
pans from the (possibly distant) past when generat-
ing expanded queries and focus on event retrieval,
rather than individual message retrieval. The other
research that is closely related to ours is the work
done by Chieu and Lee (Chieu and Lee, 2004). The
authors propose an approach for automatically con-
structing timelines from news articles in response
to a query. The novelty of our proposed work de-
rives from our novel temporal query expansion ap-
proach, and the fact that our work focuses on mi-
croblog streams which are fundamentally different
in nature from news articles.
3 Microblog Event Retrieval
The primary goal of this paper is to introduce a new
microblog search paradigm that goes beyond retriev-
ing messages individually. We propose a novel task
called microblog event retrieval, which is defined as
follows. Given a query that specifies an event, re-
trieve a set of relevant structured event representa-
tions from a large archive of microblog messages.
This definition is purposefully general to allow for a
broad interpretation of the task.
There is nothing in our proposed retrieval frame-
work that precludes it from producing reasonable re-
sults for any type of query, not just those related to
events. However, we chose to primarily focus on
events in this paper because previous studies have
shown that a majority of trending topics within mi-
croblog streams are about news and events (Kwak et
al., 2010). The information found in microblogs is
difficult to find anywhere else, including news and
Web archives, thereby making it a valuable resource
for a wide variety of users.
3.1 Overview of Framework
Our microblog event retrieval framework takes a
query as input and returns a ranked list of struc-
tured event representations. To accomplish this, the
framework breaks the work into two steps ? times-
pan retrieval and summarization. The timespan re-
trieval step identifies the timespans when the event
happened, while the summarization step retrieves
a small set of microblog messages for each times-
pan that are meant to act as a summary. Figure 1
shows an example result that is returned in response
to the query ?earthquake?. The result consists of a
start time that indicates when the event began be-
ing discussed, a duration that specifies how long the
event was discussed, and a small number of mes-
sages posted during the time interval that are meant
to summarize what happened. This example corre-
sponds to an earthquake that struck the metropoli-
tan District of Colombia area in the United States.
The earthquake was heavily discussed for nearly 11
hours, because it hit a densely populated area that
does not typically experience earthquakes.
3.2 Temporal Query Expansion
We assume that queries issued to our retrieval frame-
work are simple keyword queries that consist of a
small number of terms. This sparse representation
of the user?s information need makes finding rel-
evant messages challenging, since microblog mes-
sages that are highly related to the query might not
contain any of the query keywords. It is common for
microblog messages about a given topic to express
the topic in a different, possibly shortened or slang,
manner. For example, rather than writing ?earth-
quake?, users may instead use the word ?quake? or
simply include a hashtag such as ?#eq? in their mes-
sage. It is impractical to manually identify the full
set of related keywords and folksonomy tags (i.e.,
hashtags) for each query. In information retrieval,
this is known as the vocabulary mismatch problem.
To address this problem, we propose a novel unsu-
648
pervised temporal query expansion technique. The
approach is unsupervised in the sense that it makes
use of a pseudo-relevance feedback-like mechanism
when extracting expansion terms. Traditional query
expansion approaches typically find terms that com-
monly co-occur with the query terms in documents
(or passages). However, such approaches are not
suitable for expanding queries in the microblog set-
ting since microblog messages are very short, yield-
ing unreliable co-occurrence information. Further-
more, microblog messages have an important tem-
poral dimension that should be considered when
they are being used to generate expansion terms.
Our proposed approach generates expansion
terms based on the temporal co-occurrence of terms.
Given keyword query q, we first automatically re-
trieve a set of N timespans for which the query key-
words were most heavily discussed. To do so, we
rank timespans according to the proportion of mes-
sages posted during the timespan that contain one
or more of the query keywords. This is a simple,
but highly reliable way of identifying timespans dur-
ing which a specific topic is being heavily discussed.
These timespans are then considered to be pseudo-
relevant. In our experiments, the microblog stream
is divided into hours, with each hour corresponding
to an atomic timespan. Although it is possible to
define timespans in many different ways, we found
that this was a suitable level of granularity for most
events that was neither overly broad nor overly spe-
cific.
For each pseudo-relevant timespan, a burstiness
score is computed for all of the terms that occur in
messages posted during the timespan. The bursti-
ness score is meant to quantify how trending a term
is during the timespan. Thus, if the query is be-
ing heavily discussed during the timespan and some
term is also trending during the timespan, then the
term may be related to the query. For each of the top
N time intervals, the burstiness score of each term
is computed as follows:
burstiness(w, TSi) =
P (w|TSi)
P (w)
(1)
which is the ratio of the term?s likelihood of occur-
ring within timespan TSi versus the likelihood of
the term occurring during any timespan. Hence, if
a term that generally infrequently occurs within the
message stream suddenly occurs many times within
a single time interval, then the term will be assigned
a high burstiness score. This weighting is similar in
nature to that proposed by Ponte for query expansion
within the language modeling framework for infor-
mation retrieval (Ponte, 1998). The following prob-
ability estimates are used for the expressions within
the burstiness score:
P (w|TSi) =
tfw,TSi + ?
tfw
N
|TSi|+ ?
, P (w) =
tfw +K
N +K|V |
where tfw,TSi is the number of occurrences of w in
timespan TSi, tfw is the number of occurrences of
w in the entire microblog archive, |TSi| is the num-
ber of terms in timespan TSi, N is the total number
of terms in the microblog archive, V is the vocabu-
lary size, and ? and K are smoothing parameters.
While it is common practice to smooth P (w|TSi)
using Dirichlet (or Bayesian) smoothing (Zhai and
Lafferty, 2004), it is less common to smooth the gen-
eral English language model P (w). However, we
found that this was necessary since term distribu-
tions in microblog services exhibit unique character-
istics. By smoothing P (w), we dampen the effect of
overweighting very rare terms. In our experiments,
we set the value of ? to 500 and K to 10 after some
preliminary exploration. We found that the overall
system effectiveness is generally insensitive to the
choice of smoothing parameters.
The final step of the query expansion process
involves aggregating the burstiness scores across
all pseudo-relevant timespans to generate an over-
all score for each term. To do so, we compute
the geometric mean of the burstiness scores across
the pseudo-relevant timespans. Preliminary experi-
ments showed that the arithmetic mean was suscep-
tible to overweighting terms that had a very large
burstiness score in a single timespan. By utiliz-
ing the geometric average instead, we ensure that
the highest weighted terms are those that have large
weights in a large number of the timespans, thereby
eliminating spurious terms. Seo and Croft (2010)
observed similar results with traditional pseudo-
relevance feedback techniques.
The k highest weighted terms are then used as
expansion terms. Using this approach, terms that
commonly trend during the same timespans that
649
the query terms commonly occur (i.e., the pseudo-
relevant timespans) are assigned high weights.
Hence, the approach is capable of capturing sim-
ple temporal dependencies between terms and query
keywords, which is not possible with traditional ap-
proaches.
3.3 Timespan Ranking
The end result of the query expansion process just
described is an expanded query q? that consists of a
set of k terms and their respective weights (denoted
as ?w). Our framework uses the expanded query q?
to retrieve relevant timespans. We hypothesize that
using the expanded version of the query for timespan
retrieval will yield significantly better results than
using the keyword version.
To retrieve timespans, we first identify the 1000
highest scoring timespans (with respect to q?). We
then merge contiguous timespans into a single,
longer timespan, where the score of the merged
timespan is the maximum score of its component
timespans. The final ranked list consists of the
merged timespans. Therefore, although our times-
pans are defined as hour intervals, it is possible for
our system to return longer (merged) timespans.
We now describe two scoring functions that can
be used to compute the relevance of a timespan with
respect to an expanded query representation.
3.3.1 Coverage Scoring Function
The coverage scoring function measures rele-
vance as the (weighted) number of expansion terms
that are covered within the timespan. This measure
assumes that the expanded query is a faithful repre-
sentation of the information need and that the more
times the highly weighted expansion terms occur,
the more relevant the timespan is. Using this defi-
nition, the coverage score of a time interval is com-
puted as:
s(q?, TS) =
?
w?q?
?w ? tfw,TS
where tfwi,TS is the term frequency of wi in times-
pan TS and ?w is the expansion weight of term w.
3.3.2 Burstiness Scoring Function
Since multiple events may occur at the same time,
microblog streams can easily be dominated by the
larger of two events. However, less popular events
may also exhibit burstiness at the same time. There-
fore, another measure of relevance is the burstiness
of the event signature during the timespan. If all
of the expansion terms exhibit burstiness during the
time interval, it strongly suggests the timespan may
be relevant to the query.
Therefore, to measure the relevance of the times-
pan, we first compute the burstiness scores for all of
the terms within the time interval. This yields a vec-
tor ?TS of burstiness scores. The cosine similarity
measure is used to compute the similarity between
the query burstiness scores and the timespan bursti-
ness scores. Hence, the burstiness scoring function
is computed as:
s(q?, TS) = cos(?q? , ?TS)
3.4 Timespan Summarization
The final step of the retrieval process is to produce
a short query-biased summary for each retrieved
time interval. The primary purpose for generating
this type of summary is to provide the user with
a quick overview of what happened during the re-
trieved timespans.
We utilize a simple, straightforward approach that
generates unexpectedly useful summaries. Given a
timespan, we use a relatively simple information re-
trieval model to retrieve a small set of microblog
messages posted during the timespan that are the
most relevant to the expanded representation of the
original query. These messages are then used as a
short summary of the timespan.
This is accomplished by scoring a microblog mes-
sageM with respect to an expanded query represen-
tation q? using a weighted variant of the query like-
lihood scoring function (Ponte and Croft, 1998):
s(q?,M) =
?
w?q?
?w ? logP (w|M)
where ?w is the burstiness score for expansion term
w and P (w|M) is a Dirichlet smoothed language
modeling estimate for term w in message M . This
scoring function is also equivalent to the cross en-
tropy and KL-divergence scoring functions (Lafferty
and Zhai, 2001).
650
Category Events
Business layoffs, bankruptcy, acquisition,
merger, hostile takeover
Celebrity wedding, divorce
Crime shooting, robbery, assassination,
court decision, school shooting
Death death, suicide, drowned
Energy blackout, brownout
Entertainment awards, championship game,
world record
Health recall, pandemic, disease, flu,
poisoning
Natural Disaster hurricane, tornado, earthquake,
flood, tsunami, wildfire, fire
Politics election, riots, protests
Terrorism hostage, explosion, terrorism,
bombing, terrorist attack, suicide
bombing, hijacked
Transportation plane crash, traffic jam, sinks,
pileup, road rage, train crash, de-
railed, capsizes
Table 1: The 50 event types used as queries during our
evaluation, divided into categories.
4 Experiments
This section describes our empirical evaluation of
the proposed microblog event retrieval task.
4.1 Microblog Corpus
Our microblog message archive consists of data
that we collected from Twitter using their Stream-
ing API. The API delivers a continuous 1% ran-
dom sample of public Twitter messages (also called
?tweets?). Our evaluation makes use of data col-
lected between July 16, 2010 and Jan 1st, 2011. Af-
ter eliminating all non-English tweets, our corpus
consists of 46,611,766 English tweets, which corre-
sponds to roughly 10,000 tweets per hour. Although
this only represents a 1% sample of all tweets, we
believe that the corpus is sizable enough to demon-
strate the utility of our proposed approach.
4.2 Event Queries
To evaluate our system, we prepared a list of 50
event types that fall into 11 different categories.
The event types and their corresponding categories
are listed in Table 1. The different event types
can have substantially different characteristics, such
as the frequency of occurrence, geographic or de-
mographic interest, popularity, etc. For example,
there are more weddings than earthquakes. Pub-
lic events, such as federal elections involve people
across the country. However, a car pileup typically
only attracts local attention. Moreover, microblog-
gers show different amounts of interest to each type
of event. For example, Twitter users are more likely
to tweet about politics than a business acquisition.
4.3 Methodology
To evaluate the quality of a particular configuration
of our framework, we run the microblog event re-
trieval task for the 50 different event type queries de-
scribed in the previous section. For each query, the
top 10 timespans retrieved are manually judged to
be relevant or non-relevant. If the summary returned
clearly indicated a real event instance occurred, then
the timespan was marked as relevant. The primary
metric of interest is precision at 10.
In addition to the temporal query expansion
approach (denoted TQE), we also ran exper-
iments using relevance-based language models,
which is a state-of-the-art query expansion ap-
proach (Lavrenko and Croft, 2001). We ran two
variants of relevance-based language models. In
the first, query expansion was done using the Twit-
ter corpus itself (denoted TwitterRM). This allows
us to compare the effectiveness of the TQE ap-
proach against a more traditional query expansion
approach. In the other variant, query expansion was
done using the English Gigaword corpus (denoted
NewsRM), which is a rich source of event informa-
tion created by traditional news media.
For all three query expansion approaches (TQE,
TwitterRM, and NewsRM), the two scoring func-
tions, burstiness and coverage, are used to rank
timespans. Hence, we evaluate six specific instances
of our framework. As a baseline, we use a sim-
ple (unexpanded) keyword retrieval approach that
scores timespans according to the relative frequency
of event keywords that occur during the timespan.
4.4 Timespan Retrieval Results
Before delving into the details of our quantitative
evaluation of effectiveness, we provide an illustra-
tive example of the type of results our system is ca-
pable of producing. Table 2 shows the top four re-
651
July 16 2010 at 17 UTC, for 11 hours
Ok a 3.6 ?rocks? nothing. But boarding a plane there
now, Woodward ho! RT @todayshow: 3.6 magnitude
#earthquake rocks Washington DC area.
September 28 2010 at 11 UTC, for 6 hours
RT @Quakeprediction: 2.6 earthquake
(possible foreshock) hits E of Los Ange-
les; http://earthquake.usgs.gov/
earthquakes/recenteqscanv/Fau ...
September 04 2010 at 01 UTC, for 3 hours
7.0 quake strikes New Zealand - A 7.0-magnitude
earthquake has struck near New Zealand?s second
largest city. Reside... http://ht.ly/18R2rw
October 27 2010 at 01 UTC, for 5 hours
RT @SURFER Magazine: Tsunami Strikes
Mentawais: Wave Spawned By A 7.5-Magnitude
Earthquake Off West Coast Of Indonesia
http://bit.ly/8Z9Lbv
Table 2: Top four timespans (with a single summary
tweet) retrieved for the query ?earthquake?.
sults retrieved using temporal query expansion with
the burstiness scoring function for the query ?earth-
quake?. Only a single summary tweet is displayed
for each timespan due to space restrictions. As we
can see from the tweets, all of the results are rele-
vant to the query, in that they all correspond to times
when an earthquake happened and was actively dis-
cussed on Twitter. Different from Web and news
search results, these types of ranked lists provide a
clear temporal picture of relevant events that were
actively discussed on Twitter.
The results of our microblog retrieval task are
shown in Table 3. The table reports the per-category
and overall precision at 10 for the baseline, and
the six configurations of our proposed framework.
Bolded values represent the best result per category.
As the results show, using temporal query expan-
sion with burstiness ranking yields a mean preci-
sion at 10 of 61%, making it the best overall sys-
tem configuration. The approach is 41.9% better
than the baseline, which is statistically significant
according to a one-sided paired t-test at the p < 0.01
level. Interestingly, the relevance model-based ex-
pansion techniques exhibit even worse performance,
on average, than our simple keyword baseline. For
example, the news-based expansion approach was
11.6% worse using the coverage scoring function
and 18.6% worse using the burstiness scoring func-
tion compared to the baseline. All of the traditional
query expansion results are statistically significantly
worse than the temporal query expansion-based ap-
proaches. Hence, the results suggest that capturing
temporal dependencies between terms yields bet-
ter expanded representations than simply capturing
term co-occurrences, as is done in traditional query
expansion approaches.
The results also indicate the burstiness scoring
function outperforms the coverage scoring function
for temporal query expansion. An analysis of the
results revealed that in many cases the timespans
returned using the coverage scoring function had a
small number of frequent terms that matched the ex-
panded query. This happened less often with the
burstiness scoring function, which is based on the
cosine similarity between the query and timespan?s
burstiness scores. The combination of burstiness
weighting and l2 normalization (when computing
the cosine similarity) appears to yield a more robust
scoring function.
4.5 Event Popularity Effects
It is also interesting to note that the retrieval perfor-
mance varies substantially across the different event
type categories. For example, the performance on
queries about ?natural disasters? and ?politics? is
consistently strong. Similar performance can also
be achieved for popular events related to celebri-
ties. However, energy-related event queries, such as
?blackout?, achieves very poor effectiveness. This
observation seems to suggest that the more popu-
lar an event is, the better the retrieval performance
that can be achieved. This is a reasonable hypothe-
sis since the more people tweet about the event, the
easier it is to identify the trend from the background.
To better understand this phenomenon, we com-
pute the correlation between timespan retrieval pre-
cision and event (query) popularity, where popular-
ity is measured according to:
Popularity(q) =
1
N
N?
i=1
burstiness(q, TSi),
where q is the event query, burstiness(q, TSi) is
the burstiness score of the event during timespan
652
Event Category Baseline
NewsRM TwitterRM TQE
burst cover burst cover burst cover
Business 0.50 0.46 0.30 0.70 0.18 0.74 0.64
Celebrity 0.75 0.30 0.40 0.50 0.60 0.80 0.45
Crime 0.44 0.28 0.54 0.22 0.32 0.46 0.28
Death 0.43 0.20 0.33 0.30 0.30 0.47 0.47
Energy 0.05 0.10 0.05 0.20 0.05 0.15 0.00
Entertainment 0.47 0.53 0.67 0.30 0.53 0.70 0.70
Health 0.48 0.28 0.36 0.44 0.16 0.60 0.60
Nat. Disaster 0.50 0.53 0.59 0.66 0.46 0.87 0.66
Politics 0.67 0.70 0.53 0.63 0.30 0.87 0.60
Terrorism 0.41 0.44 0.39 0.39 0.17 0.69 0.51
Transportation 0.21 0.08 0.08 0.08 0.10 0.31 0.19
All 0.43 0.35 0.38 0.40 0.26 0.61 0.47
Table 3: Per-category and overall (All) precision at 10 for the keyword only approach (Baseline), traditional newswire
expansion (NewsRM), traditional pseudo relevance feedback using the Twitter corpus (TwitterRM), and tempo-
ral query expansion (TQE). For the expansion-based approaches, results for the burstiness scoring (burst) and the
coverage-based scoring (cover) are given. Bold values indicate the best result per category.
Correlation
Baseline 0.63 (p < 0.01)
NewsRM 0.53 (p < 0.01)
TwitterRM 0.61 (p < 0.01)
TQE 0.50 (p < 0.01)
Table 4: Spearman rank correlation between event re-
trieval precisions and event popularity. All methods use
the burstiness scoring function.
TSi, as defined in Equation 1, and the sum goes over
the topN timespans retrieved for the event using our
proposed retrieval approach.
Using this measure, we find that Twitter users are
more interested in events related to entertainment
and politics, and less interested in events related to
energy or transportation. Also, we notice that Twit-
ter users actively discuss dramatic crisis-related top-
ics, including natural disasters (e.g., earthquakes,
hurricanes, tornado, etc.) and terrorist attacks.
Table 4 shows the correlations between effec-
tiveness and event popularity across different ap-
proaches. The correlations indicate a strong cor-
relation with event popularity for the keyword ap-
proach. This is expected, since the approach is based
on the number of times the keywords are mentioned
within the timespan. The correlations are signif-
icantly reduced by incorporating query expansion
terms. The configurations that use temporal query
expansion tend to have lower correlation than the
other approaches. Although the correlation is still
significant, the lower correlation suggests that tem-
poral query expansion approaches are more robust to
popularity effects than simple keywords approaches.
Additional work is necessary to better understand
the role of popularity in retrieval tasks like this.
5 Conclusions
In this paper, we proposed a novel microblog search
task called microblog event retrieval. Unlike previ-
ous microblog search tasks that retrieve individual
microblog messages, our task involves the retrieval
of structured event representations during which an
event occurs and is discussed within the microblog
community. In this way, users are presented with a
ranked list or timeline of event instances in response
to a query.
To tackle the microblog search task, we proposed
a novel timespan retrieval framework that first con-
structs an expanded representation of the incoming
query, performs timespan retrieval, and then pro-
duces a short summary of the timespan. Our experi-
mental evaluation, carried out over a corpus of over
46 million microblog messages collected from Twit-
ter, showed that microblog event retrieval is a feasi-
ble, challenging task, and that our proposed times-
pan retrieval framework is both robust and effective.
653
References
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic De-
tection and Tracking Pilot Study. In In Proceedings of
the DARPA Broadcast News Transcription and Under-
standing Workshop, pages 194?218.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen
Robertson. 2008. Selecting good expansion terms for
pseudo-relevance feedback. In Proc. 31st Ann. Intl.
ACM SIGIR Conf. on Research and Development in In-
formation Retrieval, SIGIR ?08, pages 243?250, New
York, NY, USA. ACM.
Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceedings
of the Tenth International Workshop on Multimedia
Data Mining, MDMKDD ?10, pages 4:1?4:10, New
York, NY, USA. ACM.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432, New York, NY,
USA. ACM.
Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing twitter messages. In 1st Work-
shop on Social Media Analytics (SOMA?10), July.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th international conference on World
wide web, WWW ?10, pages 331?340, New York, NY,
USA. ACM.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in twitter. In 1st Workshop on
Social Media Analytics (SOMA?10), July.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for informa-
tion retrieval. In Proc. 24th Ann. Intl. ACM SIGIR
Conf. on Research and Development in Information
Retrieval, pages 111?119.
V. Lavrenko and W. B. Croft. 2001. Relevance-based
language models. In Proc. 24th Ann. Intl. ACM SI-
GIR Conf. on Research and Development in Informa-
tion Retrieval, pages 120?127.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based lan-
guage models. In Proc. 12th Intl. Conf. on Information
and Knowledge Management, CIKM ?03, pages 469?
475, New York, NY, USA. ACM.
Cindy Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han.
2010. Pet: a statistical model for popular events track-
ing in social communities. In Proc. 16th Ann. Intl.
ACM SIGKDD Conf. on Knowledge Discovery and
Data Mining, KDD ?10, pages 929?938, New York,
NY, USA. ACM.
Yuanhua Lv and ChengXiang Zhai. 2010. Positional rel-
evance model for pseudo-relevance feedback. In Proc.
33rd Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, SIGIR ?10, pages
579?586, New York, NY, USA. ACM.
Kamran Massoudi, Manos Tsagkias, Maarten de Rijke,
and Wouter Weerkamp. 2011. Incorporating query ex-
pansion and quality indicators in searching microblog
posts. In Proc. 33rd European Conf. on Information
Retrieval, page To appear.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In Proc.
30th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, SIGIR ?07, pages
311?318, New York, NY, USA. ACM.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 181?189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J. Ponte and W. Bruce Croft. 1998. A language modeling
approach to information retrieval. In Proc. 21st Ann.
Intl. ACM SIGIR Conf. on Research and Development
in Information Retrieval, pages 275?281.
Jay Ponte. 1998. A Language Modeling Approach to In-
formation Retrieval. Ph.D. thesis, University of Mas-
sachusetts, Amherst, MA.
J. J. Rocchio, 1971. Relevance Feedback in Information
Retrieval, pages 313?323. Prentice-Hall.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings of
the 17th ACM SIGSPATIAL International Conference
on Advances in Geographic Information Systems, GIS
?09, pages 42?51, New York, NY, USA. ACM.
Jangwon Seo and W. Bruce Croft. 2010. Geometric rep-
resentations for multiple documents. In Proceeding of
the 33rd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?10, pages 251?258, New York, NY, USA. ACM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010. Experiments in microblog summariza-
tion. Social Computing / IEEE International Confer-
654
ence on Privacy, Security, Risk and Trust, 2010 IEEE
International Conference on, 0:49?56.
Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-
ris. 2011. #twittersearch: A comparison of microblog
search and web search. In WSDM 2011: Fourth Inter-
national Conference on Web Search and Data Mining,
Feb.
ChengXiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proc. 10th Intl. Conf. on Informa-
tion and Knowledge Management, pages 403?410.
C. Zhai and J. Lafferty. 2004. A study of smoothing
methods for language models applied to information
retrieval. ACM Trans. Inf. Syst., 22(2):179?214.
655
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 546?551,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques
Donald Metzler
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
metzler@isi.edu
Eduard Hovy
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
hovy@isi.edu
Chunliang Zhang
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
czheng@isi.edu
Abstract
Paraphrase generation is an important task
that has received a great deal of interest re-
cently. Proposed data-driven solutions to the
problem have ranged from simple approaches
that make minimal use of NLP tools to more
complex approaches that rely on numerous
language-dependent resources. Despite all of
the attention, there have been very few direct
empirical evaluations comparing the merits of
the different approaches. This paper empiri-
cally examines the tradeoffs between simple
and sophisticated paraphrase harvesting ap-
proaches to help shed light on their strengths
and weaknesses. Our evaluation reveals that
very simple approaches fare surprisingly well
and have a number of distinct advantages, in-
cluding strong precision, good coverage, and
low redundancy.
1 Introduction
A popular idiom states that ?variety is the spice of
life?. As with life, variety also adds spice and appeal
to language. Paraphrases make it possible to express
the same meaning in an almost unbounded number
of ways. While variety prevents language from be-
ing overly rigid and boring, it also makes it difficult
to algorithmically determine if two phrases or sen-
tences express the same meaning. In an attempt to
address this problem, a great deal of recent research
has focused on identifying, generating, and harvest-
ing phrase- and sentence-level paraphrases (Barzi-
lay and McKeown, 2001; Bhagat and Ravichan-
dran, 2008; Barzilay and Lee, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch, 2008; Lin
and Pantel, 2001; Pang et al, 2003; Pasca and Di-
enes, 2005)
Many data-driven approaches to the paraphrase
problem have been proposed. The approaches vastly
differ in their complexity and the amount of NLP re-
sources that they rely on. At one end of the spec-
trum are approaches that generate paraphrases from
a large monolingual corpus and minimally rely on
NLP tools. Such approaches typically make use
of statistical co-occurrences, which act as a rather
crude proxy for semantics. At the other end of
the spectrum are more complex approaches that re-
quire access to bilingual parallel corpora and may
also rely on part-of-speech (POS) taggers, chunkers,
parsers, and statistical machine translation tools.
Constructing large comparable and bilingual cor-
pora is expensive and, in some cases, impossible.
Despite all of the previous research, there have
not been any evaluations comparing the quality of
simple and sophisticated data-driven approaches for
generating paraphrases. Evaluation is not only im-
portant from a practical perspective, but also from
a methodological standpoint, as well, since it is of-
ten more fruitful to devote attention to building upon
the current state-of-the-art as opposed to improv-
ing upon less effective approaches. Although the
more sophisticated approaches have garnered con-
siderably more attention from researchers, from a
practical perspective, simplicity, quality, and flexi-
bility are the most important properties. But are sim-
ple methods adequate enough for the task?
The primary goal of this paper is to take a small
step towards addressing the lack of comparative
evaluations. To achieve this goal, we empirically
546
evaluate three previously proposed paraphrase gen-
eration techniques, which range from very simple
approaches that make use of little-to-no NLP or
language-dependent resources to more sophisticated
ones that heavily rely on such resources. Our eval-
uation helps develop a better understanding of the
strengths and weaknesses of each type of approach.
The evaluation also brings to light additional proper-
ties, including the number of redundant paraphrases
generated, that future approaches and evaluations
may want to consider more carefully.
2 Related Work
Instead of exhaustively covering the entire spectrum
of previously proposed paraphrasing techniques, our
evaluation focuses on two families of data-driven ap-
proaches that are widely studied and used. More
comprehensive surveys of data-driven paraphrasing
techniques can be found in Androutsopoulos and
Malakasiotis (2010) and Madnani and Dorr (2010).
The first family of approaches that we consider
harvests paraphrases from monolingual corpora us-
ing distributional similarity. The DIRT algorithm,
proposed by Lin and Pantel (2001), uses parse tree
paths as contexts for computing distributional sim-
ilarity. In this way, two phrases were considered
similar if they occurred in similar contexts within
many sentences. Although parse tree paths serve as
rich representations, they are costly to construct and
yield sparse representations. The approach proposed
by Pasca and Dienes (2005) avoided the costs asso-
ciated with parsing by using n-gram contexts. Given
the simplicity of the approach, the authors were able
to harvest paraphrases from a very large collection
of news articles. Bhagat and Ravichandran (2008)
proposed a similar approach that used noun phrase
chunks as contexts and locality sensitive hashing
to reduce the dimensionality of the context vectors.
Despite their simplicity, such techniques are suscep-
tible to a number of issues stemming from the distri-
butional assumption. For example, such approaches
have a propensity to assign large scores to antonyms
and other semantically irrelevant phrases.
The second line of research uses comparable or
bilingual corpora as the ?pivot? that binds para-
phrases together (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Pang et al,
2003). Amongst the most effective recent work,
Bannard and Callison-Burch (2005) show how dif-
ferent English translations of the same entry in a
statistically-derived translation table can be viewed
as paraphrases. The recent work by Zhao et al
(Zhao et al, 2009) uses a generalization of DIRT-
style patterns to generate paraphrases from a bilin-
gual parallel corpus. The primary drawback of these
type of approaches is that they require a consider-
able amount of resource engineering that may not be
available for all languages, domains, or applications.
3 Experimental Evaluation
The goal of our experimental evaluation is to ana-
lyze the effectiveness of a variety of paraphrase gen-
eration techniques, ranging from simple to sophis-
ticated. Our evaluation focuses on generating para-
phrases for verb phrases, which tend to exhibit more
variation than other types of phrases. Furthermore,
our interest in paraphrase generation was initially
inspired by challenges encountered during research
related to machine reading (Barker et al, 2007). In-
formation extraction systems, which are key compo-
nent of machine reading systems, can use paraphrase
technology to automatically expand seed sets of re-
lation triggers, which are commonly verb phrases.
3.1 Systems
Our evaluation compares the effectiveness of the
following paraphrase harvesting approaches:
PD: The basic distributional similarity-inspired
approach proposed by Pasca and Dienes (2005)
that uses variable-length n-gram contexts and
overlap-based scoring. The context of a phrase
is defined as the concatenation of the n-grams
immediately to the left and right of the phrase. We
set the minimum length of an n-gram context to be
2 and the maximum length to be 3. The maximum
length of a phrase is set to 5.
BR: The distributional similarity approach proposed
by Bhagat and Ravichandran (2008) that uses noun
phrase chunks as contexts and locality sensitive
hashing to reduce the dimensionality of the contex-
tual vectors.
547
BCB-S: An extension of the Bannard Callison-
Burch (Bannard and Callison-Burch, 2005)
approach that constrains the paraphrases to have the
same syntactic type as the original phrase (Callison-
Burch, 2008). We constrained all paraphrases to be
verb phrases.
We chose these three particular systems because
they span the spectrum of paraphrase approaches, in
that the PD approach is simple and does not rely on
any NLP resources while the BCB-S approach is so-
phisticated and makes heavy use of NLP resources.
For the two distributional similarity approaches
(PD and BR), paraphrases were harvested from the
English Gigaword Fourth Edition corpus and scored
using the cosine similarity between PMI weighted
contextual vectors. For the BCB-S approach, we
made use of a publicly available implementation1.
3.2 Evaluation Methodology
We randomly sampled 50 verb phrases from 1000
news articles about terrorism and another 50 verb
phrases from 500 news articles about American
football. Individual occurrences of verb phrases
were sampled, which means that more common verb
phrases were more likely to be selected and that a
given phrase could be selected multiple times. This
sampling strategy was used to evaluate the systems
across a realistic sample of phrases. To obtain a
richer class of phrases beyond basic verb groups, we
defined verb phrases to be contiguous sequences of
tokens that matched the following POS tag pattern:
(TO | IN | RB | MD | VB)+.
Following the methodology used in previous
paraphrase evaluations (Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Kok and Brock-
ett, 2010), we presented annotators with two sen-
tences. The first sentence was randomly selected
from amongst all of the sentences in the evaluation
corpus that contain the original phrase. The second
sentence was the same as the first, except the orig-
inal phrase is replaced with the system generated
paraphrase. Annotators were given the following
options, which were adopted from those described
by Kok and Brockett (2010), for each sentence pair:
0) Different meaning; 1) Same meaning; revised is
1Available at http://www.cs.jhu.edu/?ccb/.
grammatically incorrect; and 2) Same meaning; re-
vised is grammatically correct. Table 1 shows three
example sentence pairs and their corresponding an-
notations according to the guidelines just described.
Amazon?s Mechanical Turk service was used to
collect crowdsourced annotations. For each para-
phrase system, we retrieve (up to) 10 paraphrases
for each phrase in the evaluation set. This yields
a total of 6,465 unique (phrase, paraphrase) pairs
after pooling results from all systems. Each Me-
chanical Turk HIT consisted of 12 sentence pairs.
To ensure high quality annotations and help iden-
tify spammers, 2 of the 12 sentence pairs per HIT
were actually ?hidden tests? for which the correct
answer was known by us. We automatically rejected
any HITs where the worker failed either of these hid-
den tests. We also rejected all work from annotators
who failed at least 25% of their hidden tests. We
collected a total of 51,680 annotations. We rejected
65% of the annotations based on the hidden test fil-
tering just described, leaving 18,150 annotations for
our evaluation. Each sentence pair received a mini-
mum of 1, a median of 3, and maximum of 6 anno-
tations. The raw agreement of the annotators (after
filtering) was 77% and the Fleiss? Kappa was 0.43,
which signifies moderate agreement (Fleiss, 1971;
Landis and Koch, 1977).
The systems were evaluated in terms of coverage
and expected precision at k. Coverage is defined
as the percentage of phrases for which the system
returned at least one paraphrase. Expected precision
at k is the expected number of correct paraphrases
amongst the top k returned, and is computed as:
E[p@k] =
1
k
k?
i=1
pi
where pi is the proportion of positive annotations
for item i. When computing the mean expected
precision over a set of input phrases, only those
phrases that generate one or more paraphrases is
considered in the mean. Hence, if precision were
to be averaged over all 100 phrases, then systems
with poor coverage would perform significantly
worse. Thus, one should take a holistic view of the
results, rather than focus on coverage or precision
in isolation, but consider them, and their respective
tradeoffs, together.
548
Sentence Pair Annotation
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was officially announced in Aden.
0
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was cancelled in Aden.
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem to
protest against the killing of Sharif.
1
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem in
protest of against the killing of Sharif.
It says that foreign companies have no greater right to compensation ? establishing debts at a
1/1 ratio of the dollar to the peso ? than Argentine citizens do.
2
It says that foreign companies have no greater right to compensation ? setting debts at a 1/1
ratio of the dollar to the peso ? than Argentine citizens do.
Table 1: Example annotated sentence pairs. In each pair, the first sentence is the original and the second has a system-
generated paraphrase filled in (denoted by the bold text).
Method C
Lenient Strict
P1 P5 P10 P1 P5 P10
PD 86 .48 .42 .36 .25 .22 .19
BR 84 .83 .65 .52 .16 .17 .15
BCB-S 62 .63 .45 .34 .22 .17 .13
Table 2: Coverage (C) and expected precision at k (Pk)
under lenient and strict evaluation criteria.
Two binarized evaluation criteria are reported.
The lenient criterion allows for grammatical er-
rors in the paraphrased sentence, while the strict
criterion does not.
3.3 Basic Results
Table 2 summarizes the results of our evaluation.
For this evaluation, all 100 verb phrases were run
through each system. The paraphrases returned by
the systems were then ranked (ordered) in descend-
ing order of their score, thus placing the highest
scoring item at rank 1. Bolded values represent the
best result for a given metric.
As expected, the results show that the systems
perform significantly worse under the strict evalu-
ation criteria, which requires the paraphrased sen-
tences to be grammatically correct. None of the ap-
proaches tested used any information from the eval-
uation sentences (other than the fact a verb phrase
was to be filled in). Recent work showed that us-
ing language models and/or syntactic clues from the
evaluation sentence can improve the grammatical-
ity of the paraphrased sentences (Callison-Burch,
Method
Lenient Strict
P1 P5 P10 P1 P5 P10
PD .26 .22 .20 .19 .16 .15
BR .05 .10 .11 .04 .05 .05
BCB-S .24 .25 .20 .17 .14 .10
Table 3: Expected precision at k (Pk) when considering
redundancy under lenient and strict evaluation criteria.
2008). Such approaches could likely be used to im-
prove the quality of all of the approaches under the
strict evaluation criteria.
In terms of coverage, the distributional similarity
approaches performed the best. In another set of ex-
periments, we used the PD method to harvest para-
phrases from a large Web corpus, and found that the
coverage was 98%. Achieving similar coverage with
resource-dependent approaches would likely require
more human and machine effort.
3.4 Redundancy
After manually inspecting the results returned by the
various paraphrase systems, we noticed that some
approaches returned highly redundant paraphrases
that were of limited practical use. For example,
for the phrase ?were losing?, the BR system re-
turned ?are losing?, ?have been losing?, ?have lost?,
?lose?, ?might lose?, ?had lost?, ?stand to lose?,
?who have lost? and ?would lose? within the top 10
paraphrases. All of these are simple variants that
contain different forms of the verb ?lose?. Under
the lenient evaluation criterion almost all of these
paraphrases would be marked as correct, since the
549
same verb is being returned with some grammati-
cal modifications. While highly redundant output
of this form may be useful for some tasks, for oth-
ers (such as information extraction) is it more useful
to identify paraphrases that contain a diverse, non-
redundant set of verbs.
Therefore, we carried out another evaluation
aimed at penalizing highly redundant outputs. For
each approach, we manually identified all of the
paraphrases that contained the same verb as the
main verb in the original phrase. During evalua-
tion, these ?redundant? paraphrases were regarded
as non-related.
The results from this experiment are provided in
Table 3. The results are dramatically different com-
pared to those in Table 2, suggesting that evaluations
that do not consider this type of redundancy may
over-estimate actual system quality. The percent-
age of results marked as redundant for the BCB-S,
BR, and PD approaches were 22.6%, 52.5%, and
22.9%, respectively. Thus, the BR system, which
appeared to have excellent (lenient) precision in our
initial evaluation, returns a very large number of re-
dundant paraphrases. This remarkably reduces the
lenient P1 from 0.83 in our initial evaluation to just
0.05 in our redundancy-based evaluation. The BCB-
S and PD approaches return a comparable number of
redundant results. As with our previous evaluation,
the BCB-S approach tends to perform better under
the lenient evaluation, while PD is better under the
strict evaluation. Estimated 95% confidence inter-
vals show all differences between BCB-S and PD
are statistically significant, except for lenient P10.
Of course, existing paraphrasing approaches do
not explicitly account for redundancy, and hence this
evaluation is not completely fair. However, these
findings suggest that redundancy may be an impor-
tant issue to consider when developing and evalu-
ating data-driven paraphrase approaches. There are
likely other characteristics, beyond redundancy, that
may also be important for developing robust, effec-
tive paraphrasing techniques. Exploring the space
of such characteristics in a task-dependent manner
is an important direction of future work.
3.5 Discussion
In all of our evaluations, we found that the simple
approaches are surprisingly effective in terms of pre-
cision, coverage, and redundancy, making them a
reasonable choice for an ?out of the box? approach
for this particular task. However, additional task-
dependent comparative evaluations are necessary to
develop even deeper insights into the pros and cons
of the different types of approaches.
From a high level perspective, it is also important
to note that the precision of these widely used, com-
monly studied paraphrase generation approaches is
still extremely poor. After accounting for redun-
dancy, the best approaches achieve a precision at 1
of less than 20% using the strict criteria and less than
26% when using the lenient criteria. This suggests
that there is still substantial work left to be done be-
fore the output of these systems can reliably be used
to support other tasks.
4 Conclusions and Future Work
This paper examined the tradeoffs between simple
paraphrasing approaches that do not make use of any
NLP resources and more sophisticated approaches
that use a variety of such resources. Our evaluation
demonstrated that simple harvesting approaches fare
well against more sophisticated approaches, achiev-
ing state-of-the-art precision, good coverage, and
relatively low redundancy.
In the future, we would like to see more em-
pirical evaluations and detailed studies comparing
the practical merits of various paraphrase genera-
tion techniques. As Madnani and Dorr (Madnani
and Dorr, 2010) suggested, it would be beneficial
to the research community to develop a standard,
shared evaluation that would act to catalyze further
advances and encourage more meaningful compara-
tive evaluations of such approaches moving forward.
Acknowledgments
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C-3705. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. We would
also like to thank the anonymous reviewers for their
valuable feedback and the Mechanical Turk workers
for their efforts.
550
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ?05, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw, James
Fan, Noah Friedland, Michael Glass, Jerry Hobbs,
Eduard Hovy, David Israel, Doo Soon Kim, Rutu
Mulkar-Mehta, Sourabh Patwardhan, Bruce Porter,
Dan Tecuci, and Peter Yeh. 2007. Learning by read-
ing: a prototype system, performance baseline and
lessons learned. In Proceedings of the 22nd national
conference on Artificial intelligence - Volume 1, pages
280?286. AAAI Press.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 16?
23, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ?01, pages 50?57,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674?
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?08, pages
196?205, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Joseph L. Fleiss. 1971. Measuring Nominal Scale
Agreement Among Many Raters. Psychological Bul-
letin, 76(5):378?382.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 145?153, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343?360, December.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36:341?387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: ex-
tracting paraphrases and generating new sentences.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1, NAACL ?03, pages 102?109, Morristown,
NJ, USA. Association for Computational Linguistics.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119?130. Springer Berlin / Heidelberg.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(Special Issue 04):503?526.
551
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 20?29,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Contextual Bearing on Linguistic Variation in Social Media
Stephan Gouws?, Donald Metzler, Congxing Cai and Eduard Hovy
{gouws, metzler, ccai, hovy}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
Microtexts, like SMS messages, Twitter posts,
and Facebook status updates, are a popular
medium for real-time communication. In this
paper, we investigate the writing conventions
that different groups of users use to express
themselves in microtexts. Our empirical study
investigates properties of lexical transforma-
tions as observed within Twitter microtexts.
The study reveals that different populations of
users exhibit different amounts of shortened
English terms and different shortening styles.
The results reveal valuable insights into how
human language technologies can be effec-
tively applied to microtexts.
1 Introduction
Microtexts, like SMS messages, Twitter posts, and
Facebook status updates, are becoming a popular
medium for real-time communication in the modern
digital age. The ubiquitous nature of mobile phones,
tablets, and other Internet-enabled consumer devices
provide users with the ability to express what is
on their mind nearly anywhere and at just about
any time. Since such texts have the potential to
provide unique perspectives on human experiences,
they have recently become the focus of many studies
within the natural language processing and informa-
tion retrieval research communities.
The informal nature of microtexts allows users
to invent ad hoc writing conventions that suit their
?This work was done while the first author was a visiting stu-
dent at ISI from the MIH Media Lab at Stellenbosch University,
South Africa. Correspondence may alternatively be directed to
stephan@ml.sun.ac.za.
particular needs. These needs strongly depend on
various user contexts, such as their age, geographic
location, how they want to be outwardly perceived,
and so on. Hence, social factors influence the way
that users express themselves in microtexts and other
forms of media.
In addition to social influences, there are also us-
ability and interface issues that may affect the way a
user communicates using microtexts. For example,
the Twitter microblog service imposes an explicit
message length limit of 140 characters. Users of
such services also often send messages using mobile
devices. There may be high input costs associated
with using mobile phone keypads, thus directly im-
pacting the nature of how users express themselves.
In this paper, we look specifically at understand-
ing the writing conventions that different groups
of users use to express themselves. This is ac-
complished by carrying out a novel empirical in-
vestigation of the lexical transformation character-
istics observed within Twitter microtexts. Our em-
pirical evaluation includes: (i) an analysis of how
frequently different user populations apply lexical
transformations, and (ii) a study of the types of
transformations commonly employed by different
populations of users. We investigate several ways of
defining user populations (e.g., based on the Twitter
client, time zone, etc.). Our results suggest that not
all microtexts are created equal, and that certain pop-
ulations of users are much more likely to use certain
types of lexical transformations than others.
This paper has two primary contributions. First,
we present a novel methodology for contextualized
analysis of lexical transformations found within mi-
20
crotexts. The methodology leverages recent ad-
vances in automated techniques for cleaning noisy
text. This approach enables us to study the fre-
quency and types of transformations that are com-
mon within different user populations and user con-
texts. Second, we present results from an empirical
evaluation over microtexts collected from the Twit-
ter microblog service. Our empirical analysis re-
veals that within Twitter microtexts, different user
populations and user contexts give rise to different
forms of expression, by way of different styles of
lexical transformations.
The remainder of this paper is laid out as follows.
Section 2 describes related work, while Section 3
motivates our investigation. Our multi-pronged
methodology for analyzing lexical transformations
is described in Section 4. Section 5 describes our
experimental results. Finally, Section 6 concludes
the paper and describes possible directions for fu-
ture work.
2 Related Work
Although our work is primarily focused on analyz-
ing the lexical variation in language found in on-
line social media, our analysis methodology makes
strong use of techniques for normalizing ?noisy text?
such as SMS-messages and Twitter messages into
standard English.
Normalizing text can traditionally be approached
using three well-known NLP metaphors, namely
that of spell-checking, machine translation (MT) and
automatic speech recognition (ASR) (Kobus et al,
2008).
In the spell-checking approach, corrections from
?noisy? words to ?clean? words proceed on a word-
by-word basis. Choudhury (2007) implements
the noisy channel model (Shannon and Weaver,
1948) using a hidden Markov model to handle both
graphemic and phonemic variations, and Cook and
Stevenson (2009) improve on this model by adapt-
ing the channel noise according to several predefined
word formations such as stylistic variation, word
clipping, etc. However, spelling correction is tra-
ditionally conducted in media with relatively high
percentages of well-formed text where one can per-
form word boundary detection and thus tokenization
to a high degree of accuracy. The main drawback is
the strong confidence this approach places on word
boundaries (Beaufort et al, 2010), since detecting
word boundaries in noisy text is not a trivial prob-
lem.
In the machine translation approach (Bangalore
et al, 2002; Aw et al, 2006), normalizing noisy
text is considered as a translation task from a source
language (the noisy text) to a target language (the
cleansed text). Since noisy- and clean text typically
vary wildly, it satisfies the notion of translating be-
tween two languages. However, since these trans-
formations can be highly creative, they usually need
a wide context (more than one word) to be resolved
adequately. Kobus (2008) also points out that de-
spite the fairly good results achieved with this sys-
tem, such a purely phrase-based translation model
cannot adequately handle the wide level of lexical
creativity found in these media.
Finally, the ASR approach is based on the ob-
servation that many noisy word forms in SMSes
or other noisy text are based on phonetic plays of
the clean word. This approach starts by convert-
ing the input message into a phone lattice, which
is converted to a word lattice using a phoneme-
grapheme dictionary. Finally the word lattice is de-
coded by applying a language model to the word lat-
tice and using a best-path algorithm to recover the
most likely original word sequence. This approach
has the advantage of being able to handle badly seg-
mented word boundaries efficiently, however it pre-
vents the next normalization steps from knowing
what graphemes were in the initial sequence (Kobus
et al, 2008).
What fundamentally separates the noisy text
cleansing task from the spell-checking problem is
that most often lexical ill-formedness in these me-
dia is intentional. Han (2011) proposes that this
might be in an attempt to save characters in length-
constrained media (such as Twitter or SMS), for
social identity (conversing in the dialect of a spe-
cific group), or due to convention of the medium.
Emotional context is typically expressed with re-
peat characters such as ?I am sooooooo tired? or
excessive punctuation. At times, however, out-
of-vocabulary tokens (spelling errors) might result
purely as the result of cognitive oversight.
Cook and Stevenson (2009) are one of the first to
explicitly analyze the types of transformations found
21
in short message domains. They identify: 1) stylis-
tic variation (better?betta), 2) subsequence abbre-
viation (doing?dng), 3) clipping of the letter ?g?
(talking?talkin), 4) clipping of ?h? (hello?ello),
and 5) general syllable clipping (anyway?neway),
to be the most frequent transformations. Cook and
Stevenson then incorporate these transformations
into their model. The idea is that such an unsuper-
vised approach based on the linguistic properties of
creative word forms has the potential to be adapted
for normalization in other similar genres without the
cost of developing a large training corpus. Most im-
portantly, they find that many creative texting forms
are the result of a small number of specific word for-
mation processes.
Han (2011) performs a simple analysis on the out-
of-vocabulary words found in Twitter, and find that
the majority of ill-formed words in Twitter can be
attributed to instances where letters are missing or
where there are extraneous letters, but the lexical
correspondence to the target word is trivially acces-
sible. They find that most ill-formed words are based
on morphophonemic variations.
3 Motivation
All of the previous work described in Section 2 ei-
ther
i) only focus on recovering the most likely ?stan-
dard English? form of a message, disregarding
the stylistic structure of the original noisy text,
or
ii) considers the structure of the noisy text found
in a medium as a whole, only as a first step
(the means) to identify common types of noisy
transformations which can subsequently be ac-
counted for (or ?corrected?) to produce normal-
ized messages (the desired end result).
However, based on the fact that language is highly
contextual, we ask the question: What influence
does the context in which a message is produced
have on the resulting observed surface structure and
style of the message?
In general, since some topics are for instance
more formal or informal than others, vocabulary and
linguistic style often changes based on the topic that
is being discussed. Moreover, in social media one
can identify several other types of context. Specif-
ically in Twitter, one might consider a user?s geo-
graphical location, the client from which a user is
broadcasting her message, how long she has been
using the Twitter service, and so forth.
The intuition is that the unconstrained nature of
these media afford users the ability to invent writing
conventions to suit their needs. Since users? needs
depend on their circumstances, and hence their con-
text, we hypothesize that the observed writing sys-
tems might be influenced by some elements of their
context. For instance, phonemic writing systems
might be related to a user?s dialect which is re-
lated to a user?s geographical location. Furthermore,
highly compressed writing conventions (throwing
away vowels, using prefixes of words, etc.) might
result from the relatively high input cost associ-
ated with using unwieldy keypads on some mobile
clients, etc.
The present work is focused on looking at these
stylistic elements of messages found in social media,
by analyzing the types of stylistic variation at the
lexical level, across these contextual dimensions.
4 Method
In the following discussion we make a distinc-
tion between within-tweet context and the general
message-context in which a message is created.
Within-tweet context is the linguistic context (the
other terms) that envelopes a term in a Twitter mes-
sage. The general context of a Twitter message is the
observable elements of the environment in which it
was conceived. For the current study, we record
1. the user?s location, and
2. the client from which the message was sent,
We follow a two-pronged analytic approach:
Firstly, we conduct a na??ve, context-free analysis
(at the linguistic level) of all words not commonly
found in standard, everyday English. This analy-
sis purely looks at the terminology that are found
on Twitter, and does not attempt to normalize these
messages in any way. Therefore, different surface
forms of the same word, such as ?today?, ?2day?,
?2d4y?, are all considered distinct terms. We then
analyse the terminology over different contextual di-
mensions such as client and location.
22
Secondly, we perform a more in-depth and con-
textual analysis (at the word level) by first normaliz-
ing the potentially noisy message to recover the most
likely surface form of the message and recording the
types of changes that were made, and then analyz-
ing these types of changes across different general
contextual dimensions (client and location).
As noted in Section 2, text message normalization
is not a trivial process. As shown by Han (2011),
most transformations from in-vocabulary words to
out-of-vocabulary words can be attributed to a single
letter that is changed, removed, or added. Further-
more, they note that most ill-formed words are re-
lated to some morphophonemic variation. We there-
fore implemented a text cleanser based on the de-
sign of Contractor (2010) using pre-processing tech-
niques discussed in (Kaufmann and Kalita, 2010).
It works as follows: For each input message, we
replace @-usernames with ?*USR*? and urls with
?*URL*?. Hash tags can either be part of the sen-
tence (?just got a #droid today?) or be peripheral to
the sentence (?what a loooong day! #wasted?). Fol-
lowing Kaufmann (2010) we remove hashtags at the
end of messages when they are preceded by typical
end-of-sentence punctuation marks. Hash tags in the
middle of messages are retained, and the hash sign
removed.
Next we tokenize this preprocessed message us-
ing the NLTK tokenizer (Loper and Bird, 2002). As
noted earlier, standard NLP tools do not perform
well on noisy text out-of-the-box. Based on inspec-
tion of incorrectly tokenized output, we therefore in-
clude a post-tokenization phase where we split all
tokens that include a punctuation symbol into the in-
dividual one or two alphanumeric tokens (on either
side of the punctuation symbol), and the punctuation
symbol1. This heuristic catches most cases of run-on
sentences.
Given a set of input tokens, we process these one
by one, by comparing each token to the words in
the lexicon L and constructing a confusion network
CN. Each in-vocabulary term, punctuation token or
other valid-but-not-in-vocabulary term is added to
CN with probability 1.0 as shown in Algorithm 1.
1This is easily accomplished using a regular expression
group-substitution of the form (\w*)([P])(\w*)?[\1,
\2, \3], where \w represents the set of alphanumeric char-
acters, and P is the set of all punctuation marks [.,;?". . .]
Character Transliteration candidates
1 ?1?, ?l?, ?one?
2 ?2?, ?to?, ?too?, ?two?
3 ?3?, ?e?, ?three?
4 ?4?, ?a?, ?for?, ?four?
5 ?5?, ?s?, ?five?
6 ?6?, ?b?, ?six?
7 ?7?, ?t?, ?seven?
8 ?8?, ?ate?, ?eight?
9 ?9?, ?g?, ?nine?
0 ?0?, ?o?, ?zero?
?@? ?@?, ?at?
?&? ?&?, ?and?
Table 1: Transliteration lookup table.
valid tok(wi) checks for ?*USR*?, ?*URL*?, or
any token longer than 1 character with no alphabet-
ical characters. This heuristic retains tokens such as
?9-11?, ?12:44?, etc.
At this stage, all out-of-vocabulary (OOV) terms
represent the terms that we are uncertain about, and
hence candidate terms for cleansing. First, for each
OOV term, we enumerate each possibly ambiguous
character into all its possible interpretations with the
transliteration table shown in Table 1. This expands,
for example, ?t0day?? [?t0day?, ?today?], and also
?2day?? [?2day?, ?twoday?, ?today?], etc.
Each transliterated candidate word in each con-
fusion set produced this way is then scored with
the original word and ranked using the heuristic
function (sim()) described in (Contractor et al,
2010)2. We also evaluated a purely phonetic edit-
distance similarity function, based on the Double
Metaphone algorithm (Philips, 2000), but found the
string-similarity-based function to give more reli-
able results.
Each confusion set produced this way (see Al-
gorithm 2) is joined to its previous set to form a
growing confusion lattice. Finally this lattice is de-
coded by converting it into the probabilistic finite-
state grammar format, and by using the SRI-LM
toolkit?s (Stolcke, 2002) lattice-tool com-
mand to find the best path through the lattice by
2The longest common subsequence between the two words,
normalized by the edit distances between their consonant skele-
tons.
23
Transformation Type Rel %
single char (?see?? ?c?) 29.1%
suffix (?why?? ?y?) 18.8%
drop vowels (?be?? ?b?) 16.4%
prefix (?tomorrow?? ?tom?) 9.0%
you to u (?you?? ?u?) 8.3%
drop last char (?running?? ?runnin?) 7.0%
repeat letter (?so?? ?soooo?) 5.5%
contraction (?you will?? ?you?ll?) 5.0%
th to d (?this?? ?dis?) 1.0%
Table 2: Most frequently observed types of transforma-
tions with an example in parentheses. Rel % shows the
relative percentage of the top-10 transformations which
were identified (excluding unidentified transformations)
to belong to a specific class.
making use of a language model to promote fluid-
ity in the text, and trained as follows:
We generated a corpus containing roughly 10M
tokens of clean English tweets. We used a simple
heuristic for selecting clean tweets: For each tweet
we computed if #(OOV )#(IV )+1 < ?, where ? = 0.5
was found to give good results. On this corpus
we trained a trigram language model, using Good-
Turing smoothing. Next, a subset of the LA Times
containing 30M words was used to train a ?general
English? language model in the same way. These
two models were combined3 in the ratio 0.7 to 0.3.
The result of the decoding process is the hypoth-
esized clean tokens of the original sentence. When-
ever the cleanser makes a substitution, it is recorded
for further analysis. Upon closer inspection, it was
found that most transformation types can be recog-
nized by using a fairly simple post-processing step.
Table 2 lists the most frequent types of transforma-
tions. While these transformations do not have per-
fect coverage, they account for over 90% of the (cor-
rect) transformations produced by the cleanser. The
rules fail to cover relatively infrequent edge cases,
such as ?l8r ? later?, ?cuz ? because?, ?dha ?
the?, and ?yep? yes? 4.
3Using the -mix-lm and -lambda and -mix-lambda2
options to the SRI-LM toolkit?s ngram module.
4To our surprise these ?typical texting forms? disappeared
into the long tail in our data set.
Original Cleansed
Swet baby jeebus, some-
one PLEASE WINE ME!
sweet baby jesus , some-
one please wine me !
2 years with Katie today! two years with katie to-
day!
k,hope nobody was
hurt.gud mornin jare
okay , hope nobody was
hurt . good morning jamie
When u a bum but think u
da best person on da court
you doodooforthebooboo
when you a bum but think
you the best person on the
court you dorothy
NYC premiere 2morrow. nice premiere tomorrow .
Table 3: Examples of original and automatically cleansed
versions of Twitter messages.
Algorithm 1 Main cleanser algorithm pseudo code.
The decode() command converts the confusion
network (CN) into PFSG format and decodes it us-
ing the lattice-tool of the SRI-LM toolkit.
Require: Lexicon L, Punctuation set P
function CLEANSE MAIN(Min)
for wi ?Min do
if wi ? L ? P or valid tok(wi) then
Add (1.0, wi) to CNout . Probability 1.0
else
Add conf set(wi) to CNout
end if
end for
return decode(CNout)
end function
Table 3 illustrates some example corrections
made by the cleanser. As the results show, the
cleanser is able to correct many of the more com-
mon types of transformations, but can fail when it
encounters infrequent or out-of-vocabulary terms.
5 Evaluation
This section describes our empirical evaluation and
analysis of how users in different contexts express
themselves differently using microtexts. We focus
specifically on the types of lexical transformations
that are commonly applied globally, within popula-
tions of users, and in a contextualized manner.
24
Algorithm 2 Algorithm pseudo code for generating
confusion set CS. L[wi] is the lexicon partitioning
function for word wi.
Require: Lexicon L, confusion set CS, implemented as
top-K heap containing (si, wi), indexed on si
function CONF SET(wi)
W? translits(wi)
for wj ?W do
for wk ? L[wj ] do
sk ? sim(wj , wk)
if sk > min(CS) then
Add (sk, wk) to CS
end if
end for
end for
return CS
end function
5.1 Out-of-Vocabulary Analysis
We begin by analyzing the types of terms that are
common in microtexts but not typically used in
proper, everyday English texts (such as newspapers).
We refer to such terms as being out-of-vocabulary,
since they are not part of the common written En-
glish lexicon. The goal of this analysis is to un-
derstand how different contexts affect the number
of out-of-vocabulary terms found in microtexts. We
hypothesize that certain contextual factors may in-
fluence a user?s ability (or interest) to formulate
clean microtexts that only contain common English
terms.
We ran our analysis over a collection of one mil-
lion Twitter messages collected using the Twitter
streaming API during 2010. Tweets gathered from
the Twitter API are tagged with a language identifier
that indicates the language a user has chosen for his
or her account. However, we found that many tweets
purported to be English were in fact not. Hence,
we ran all of the tweets gathered through a simple
English language classifier that was trained using a
small set of manually labeled tweets, uses character
trigrams and average word length as features, and
achieves an accuracy of around 93%. The every-
day written English lexicon, which we treat as the
?gold standard? lexicon, was distilled from the same
collection of LA Times news articles described in
Section 4. This yielded a comprehensive lexicon of
approximately half a million terms.
Timezone % In-Vocabulary
Australia 86%
UK 85%
US (Atlantic) 84%
Hong Kong 83%
US (Pacific) 81%
Hawaii 81%
Overall 81%
Table 4: Percentage of in-vocabulary found in large En-
glish lexicon for different geographic locations.
For each tweet, the tokenized terms were looked
up in the LA Times lexicon to determine if the
term was out-of-vocabulary or not. Not surprisingly,
the most frequent out-of-vocabulary terms identi-
fied are Twitter usernames, URLs, hasthags, and RT
(the terminology for a re-broadcast, or re-tweeted,
message). These tokens alone account for approx-
imately half of all out-of-vocabulary tokens. The
most frequent out-of-vocabulary terms include ?lol?,
?haha?, ?gonna?, ?lmao?, ?wanna?, ?omg?, ?gotta?.
Numerous expletives also appear amongst the most
common out-of-vocabulary terms, since such terms
never appear in the LA Times. Out of vocabulary
terms make up 19% of all terms in our data set.
In the remainder of this section, we examine
the out-of-vocabulary properties of different popu-
lations of users based on their geographic location
and their client (e.g., Web-based or mobile phone-
based).
5.1.1 Geographic Locations
To analyze the out-of-vocabulary properties of
users in different geographic locations, we extracted
the time zone information from each Tweet in our
data set. Although Twitter allows users to specify
their location, many users leave this field blank, use
informal terminology (?lower east side?), or fabri-
cate non-existent locations (e.g., ?wherever i want
to be?). Therefore, we use the user?s time zone as
a proxy for their actual location, in hopes that users
have less incentive to provide incorrect information.
For the Twitter messages associated with a given
time zone, we computed the percentage of tokens
found within our LA Times-based lexicon. The re-
sults from this analysis are provided in Table 4. It is
25
Client % In-Vocabulary
Facebook 88%
Twitter for iPhone 84%
Twitter for Blackberry 83%
Web 82%
UberTwitter 78%
Snaptu 73%
Overall 81%
Table 5: Percentage of in-vocabulary found in large En-
glish lexicon for different Twitter clients.
important to note that these results were computed
over hundreds of thousands of tokens, and hence
the variance of our estimates is very small. This
means that the differences observed here are statis-
tically meaningful, even though the absolute differ-
ences tend to be somewhat small.
These results indicate that microtexts composed
by users in different geographic locations exhibit
different amounts of out-of-vocabulary terms. Users
in Australia, the United Kingdom, Hong Kong, and
the East Coast of the United States (e.g., New York
City) include fewer out-of-vocabulary terms in their
Tweets than average. However, users from the West
Coast of the United States (e.g., Los Angeles, CA)
and Hawaii are on-par with the overall average, but
include 5% more out-of-vocabulary terms than the
Australian users.
As expected, the locations with fewer-than-
average in-vocabulary tokens are associated with
non-English speaking countries, despite the output
from the classifier.
5.1.2 Twitter Clients
In a similar experiment, we also investigated the
frequency of out-of-vocabulary terms conditioned
on the Twitter client (or ?source?) used to compose
the message. Example Twitter clients include the
Web-based client at www.twitter.com, official
Twitter clients for specific mobile platforms (e.g.,
iPhone, Android, etc.), and third-party clients. Each
client has its own characteristics, target user base,
and features.
In Table 5, we show the percentage of in-
vocabulary terms for a sample of the most widely
used Twitter clients. Unlike the geographic location-
based analysis, which showed only minor differ-
ences amongst the user populations, we see much
more dramatic differences here. Some clients, such
as Facebook, which provides a way of cross-posting
status updates between the two services, has the
largest percentage of in-vocabulary terms of the ma-
jor clients in our data.
One interesting, but unexpected, finding is that the
mobile phone (i.e., iPhone and Blackberry) clients
have fewer out-of-vocabulary terms, on average,
than the Web-based client. This suggests that ei-
ther the users of the clients are less likely to misspell
words or use slang terminology or that the clients
may have better or more intuitive spell checking ca-
pabilities. A more thorough analysis is necessary to
better understand the root cause of this phenomenon.
At the other end of the spectrum are the UberTwit-
ter and Snaptu clients, which exhibit a substantially
larger number of out-of-vocabulary terms. These
clients are also typically used on mobile devices. As
with our previous analysis, it is difficult to pinpoint
the exact cause of such behavior, but we hypothe-
size that it is a function of user demographics and
difficulties associated with inputting text on mobile
devices.
5.2 Contextual Analysis
In this section, we test the hypothesis that different
user populations make use of different types of lex-
ical transformations. To achieve this goal, we make
use of our noisy text cleanser. For each Twitter mes-
sage run through the cleanser, we record the origi-
nal and cleaned version of each term. For all of the
terms that the cleanser corrects, we automatically
identify which (if any) of the transformation rules
listed in Table 2 explain the transformation between
the original and clean version of the term. We use
this output to analyze the distribution of transforma-
tions observed across different user populations.
We begin by analyzing the types of transforma-
tions observed across Twitter clients. Figure 1 plots
the (normalized) distribution of lexical transforma-
tions observed for the Web, Twitter for Blackberry,
Twitter for iPhone, and UberTwitter clients, grouped
by the transformations. We also group the trans-
formations by the individual clients in Figure 2 for
more direct comparison.
The results show that Web users tend to use more
26
Figure 1: Proportion of transformations observed across
Twitter clients, grouped by transformation type.
contractions than Blackberry and UberTwitter users.
We relate this result to the differences in typing on
a virtual compared to a multi-touch keypad. It was
surprising to see that iPhone users tended to use con-
siderably more contractions than the other mobile
device clients, which we relate to its word-prediction
functionality. Another interesting result is the fact
that Web users often drop vowels to shorten terms
more than their mobile client counterparts. Instead,
mobile users often use suffix-style transformations
more, which is often more aggressive than the drop-
ping vowels transformation, and possibly a result of
the pervasiveness of mobile phones: Large popu-
lations of people?s first interaction with technology
these days are through a mobile phone, a device
where strict length limits are imposed on texting,
and which hence enforce habits of aggressive lex-
ical compression, which might transfer directly to
their use of PCs. Finally, we observe that mobile de-
vice users replace ?you? with ?u? substantially more
than users of the Web client.
We also performed the same analysis across time
zones/locations. The results are presented in Fig-
ure 3 by transformation-type, and again grouped by
location for direct comparison in Figure 4. We ob-
serve, perhaps not surprisingly, that the East Coast
US, West Coast US, and Hawaii are the most similar
with respect to the types of transformations that they
Figure 2: Proportion of transformations observed across
Twitter clients, grouped by client.
commonly use. However, the most interesting find-
ing here is that British users tend to utilize a notice-
ably different set of transformations than American
users in the Pacific time zones. For example, British
users are much more likely to use contractions and
suffixes, but far less likely to drop the last letter of
a word, drop all of the vowels in a word, use prefix-
style transformations, or to repeat a given letter mul-
tiple times. In a certain sense, this suggests that
British users tend to write more proper, less informal
English and make use of strikingly different styles
for shortening words compared to American users.
This might be related to the differences in dialects
between the two regions manifesting itself during a
process of phonetic transliteration when composing
the messages: Inhabitants of the south-west regions
in the US are known for pronouncing for instance
running as runnin?, which manifests as dropping the
last letter, and so forth.
Therefore, when taken with our out-of-vocabulary
analysis, our experimental evaluation shows clear
evidence that different populations of users express
themselves differently online and use different types
of lexical transformations depending on their con-
text. It is our hope that the outcome of this study
will spark further investigation into these types of
issues and ultimately lead to effective contextually-
aware natural language processing and information
retrieval approaches that can adapt to a wide range
of user contexts.
27
Figure 3: Proportion of transformations observed across
geographic locations, grouped by transformation type.
6 Conclusions and Future Work
This paper investigated the writing conventions that
different groups of users use to express themselves
in microtexts. We analyzed characteristics of terms
that are commonly found in English Twitter mes-
sages but are never seen within a large collection
of LA Times news articles. The results showed
that a very small number of terms account for a
large proportion of the out-of-vocabulary terms. The
same analysis revealed that different populations of
users exhibit different propensities to use out-of-
vocabulary terms. For example, it was found that
British users tend to use fewer out-of-vocabulary
terms compared to users within the United States.
We also carried out a contextualized analysis that
leveraged a state-of-the-art noisy text cleanser. By
analyzing the most common types of lexical trans-
formations, it was observed that the types of trans-
formations used varied across Twitter clients (e.g.,
Web-based clients vs. mobile phone-based clients)
and geographic location. This evidence supported
our hypothesis that the measurable contextual indi-
cators surrounding messages in social media play an
important role in determining how messages in these
media vary at the surface (lexical) level from what
might be considered standard English.
The outcome of our empirical evaluation and
subsequent analysis suggests that human language
Figure 4: Proportion of transformations observed across
geographic locations, grouped by location.
technologies (especially natural language process-
ing techniques that rely on well-formed inputs) are
likely to be highly susceptible to failure as the result
of lexical transformations across nearly all popula-
tions and contexts. However, certain simple rules
can be used to clean up a large number of out-of-
vocabulary tokens. Unfortunately, such rules would
not be able to properly correct the long tail of
the out-of-vocabulary distribution. In such cases,
more sophisticated approaches, such as the noisy
text cleanser used in this work, are necessary to
combat the noise. Interestingly, most of the lexical
transformations observed affect non-content words,
which means that most information retrieval tech-
niques will be unaffected by such transformations.
As part of future work, we are generally interested
in developing population and/or context-aware lan-
guage processing and understanding techniques on
top of microtexts. We are also interested in ana-
lyzing different user contexts, such as those based
on age and gender and to empirically quantify the
effect of noise on actual natural language process-
ing and information retrieval tasks, such as part of
speech tagging, parsing, summarization, etc.
Acknowledgments
We would like to thank the anonymous reviewers for
their insightful comments. Stephan Gouws would
like to thank MIH Holdings Ltd. for financial sup-
port during the course of this work.
28
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A Phrase-
based Statistical Model for SMS Text Normalization.
In Proceedings of the COLING/ACL Main Conference
Poster Sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping Bilingual Data Using Consensus Transla-
tion for a Multilingual Instant Messaging System. In
Proceedings of the 19th International Conference on
Computational Linguistics-Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
R. Beaufort, S. Roekhaut, L.A. Cougnon, and C. Fa-
iron. 2010. A Hybrid Rule/Model-based Finite-State
Framework for Normalizing SMS Messages. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 770?779. Asso-
ciation for Computational Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and Modeling of the
Structure of Texting Language. International Journal
on Document Analysis and Recognition, 10(3):157?
174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised Cleansing of Noisy Text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An Unsupervised
Model for Text Message Normalization. In Proceed-
ings of the Workshop on Computational Approaches
to Linguistic Creativity, pages 71?78. Association for
Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normaliz-
ing SMS: Are Two Metaphors Better Than One? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics-Volume 1, pages 63?70. Association for
Computational Linguistics.
L. Philips. 2000. The Double Metaphone Search Algo-
rithm. CC Plus Plus Users Journal, 18(6):38?43.
C.E. Shannon and W. Weaver. 1948. The Mathemati-
cal Theory of Communication. Bell System Technical
Journal, 27:623?656.
A. Stolcke. 2002. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901?904.
29
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 82?90,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Mining of Lexical Variants from Noisy Text
Stephan Gouws?, Dirk Hovy and Donald Metzler
stephan@ml.sun.ac.za, {dirkh, metzler}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
The amount of data produced in user-
generated content continues to grow at a stag-
gering rate. However, the text found in these
media can deviate wildly from the standard
rules of orthography, syntax and even seman-
tics and present significant problems to down-
stream applications which make use of this
noisy data. In this paper we present a novel
unsupervised method for extracting domain-
specific lexical variants given a large volume
of text. We demonstrate the utility of this
method by applying it to normalize text mes-
sages found in the online social media service,
Twitter, into their most likely standard English
versions. Our method yields a 20% reduction
in word error rate over an existing state-of-the-
art approach.
1 Introduction
The amount of data produced in user-generated con-
tent, e.g. in online social media, and from machine-
generated sources such as optical character recog-
nition (OCR) and automatic speech recognition
(ASR), surpasses that found in more traditional me-
dia by orders of magnitude and continues to grow
at a staggering rate. However, the text found in
these media can deviate wildly from the standard
rules of orthography, syntax and even semantics and
present significant problems to downstream applica-
tions which make use of this ?noisy? data. In social
?This work was done while the first author was a visiting
student at ISI from the MIH Media Lab at Stellenbosch Univer-
sity, South Africa.
media this noise might result from the need for so-
cial identity, simple spelling errors due to high in-
put cost associated with the device (e.g. typing on
a mobile phone), space constraints imposed by the
specific medium or even a user?s location (Gouws et
al., 2011). In machine-generated texts, noise might
result from imperfect inputs, imperfect conversion
algorithms, or various degrees of each.
Recently, several works have looked at the pro-
cess of normalizing these ?noisy? types of text into
more standard English, or in other words, to convert
the various forms of idiosyncratic spelling and writ-
ing errors found in these media into what would nor-
mally be considered standard English orthography.
Many of these works rely on supervised methods
which share the common burden of requiring train-
ing data in the form of noisy input and clean output
pairs. The problem with developing large amounts
of annotated training data is that it is costly and re-
quires annotators with sufficient expertise. However,
the volume of data that is available in these media
makes this a suitable domain for applying semi- and
even fully unsupervised methods.
One interesting observation is that these noisy
out-of-vocabulary (OOV) words are typically
formed through some semi-deterministic process
which doesn?t render them completely indiscernible
at a lexical level from the original words they are
meant to represent. We therefore refer to these OOV
tokens as lexical variants of the clean in-vocabulary
(IV) tokens they are derived from. For instance,
in social media ?2morrow? ?2morow? and ?2mrw?
still share at least some lexical resemblance with
?tomorrow?, due to the fact that it is mainly the
82
Figure 1: A plot of the OOV distribution found in Twit-
ter. Also indicated is the potential for using (OOV,most-
likely-IV) training pairs found on this curve for either
exception dictionary entries (the most frequent pairs),
or for learning lexical transformations (the long tail).
The threshold between the two (vertical bar) is domain-
specific.
result of a phonetic transliteration procedure. Also,
?computer? and ?conpu7er? share strong lexical
overlap, and might be the result of noise in the OCR
process.
As with many aspects of NLP, the distribution of
these OOV tokens resemble a power law distribution
(see Figure 1 for the OOV distribution in Twitter).
Thus, some words are commonly converted to some
OOV representation (e.g. domain-specific abbrevia-
tions in social media, or words which are commonly
incorrectly detected in OCR) and these account for
most of the errors, with the rest making up the long
tail. If one could somehow automatically extract a
list of all the domain-specific OOV tokens found in
a collection of texts, along with the most likely clean
word (or words) each represents, then this could play
a key role in for instance normalizing individual
messages. Very frequent (noisy, clean) pairs at the
head of the distribution could be used for extracting
common domain-specific abbreviations, and word-
pairs in the long tail may be used as input to learn-
ing algorithms for automatically learning the types
of transformations found in these media, as shown
in Figure 1.
For example, taking Twitter as our target domain,
examples for learning common exception pairs may
include ?gf ???girlfriend?. For learning types of lex-
ical transformations, one might learn from ?think-
ing???thinkin? and ?walking???walkin? that ?ng?
could go to ?n? (known as ?g-clipping?).
In this paper we present a novel unsupervised
method for extracting an approximation to such a
domain-specific list of (noisy, clean) pairs, given
only a large volume of representative text. We fur-
thermore demonstrate the utility of this method by
applying it to normalize text messages found in the
online social media service, Twitter, into their most
likely standard English versions.
The primary contributions of this paper are:
? We present an unsupervised method that mines
(noisy, clean) pairs and requires only large
amounts of domain-specific noisy data
? We demonstrate the utility of this method by in-
corporating it into a standard method for noisy
text normalization, which results in a signifi-
cant reduction in the word error rate compared
to the original method.
2 Training Pair Mining
Given a large corpus of noisy text, our challenge is to
automatically mine pairs of domain-specific lexical
variants that can be used as training data for a va-
riety of natural language processing tasks. The key
challenge is how to develop an effective approach
that is both domain-specific and robust to noisy cor-
pora. Our proposed approach requires nothing more
than a large ?common English? corpus (e.g., a large
newswire corpus) and a large corpus of domain text
(e.g., a large corpus of Twitter data, a query log,
OCR output, etc.). Using these two sources of ev-
idence, the approach mines domain-specific lexical
variants in a fully unsupervised manner.
Before describing the details of our approach, we
first describe the characteristics that we would like
the mined lexical variants to have. First, the variants
should be semantically related to each other. Pairs
of words that are lexically similar, but semantically
unrelated are not of particular interest since such
pairs can be found using basic edit distance-based
approaches. Second, the variants should be domain-
specific. Variants that capture common English lexi-
cal variations (e.g., ?running? and ?run?) can be cap-
tured using standard normalization procedures, such
83
Figure 2: Flow chart illustrating our procedure for mining
pairs of lexical variants.
as stemming. Instead, we are interested in identify-
ing domain-specific variations (e.g., ?u? and ?you?
in the SMS and Twitter domains) that cannot eas-
ily be handled by existing approaches. Finally, the
variants should be lexically similar, by definition.
Hence, ideal variants will be domain-specific, lex-
ically similar, and semantically related.
To mine such variants we synthesize ideas from
natural language processing and large-scale text
mining to derive a novel mining procedure. Our pro-
cedure can be divided into three atomic steps. First
we identify semantically similar pairs, then we filter
out common English variants, and finally we rescore
the resulting list based on lexical similarity (see Fig-
ure 2). The remainder of this section describes the
complete details of each of these steps.
2.1 Identifying Semantically Similar Pairs
The first step of our mining procedure harvests se-
mantically similar pairs of terms from both the com-
mon English corpus and the domain corpus. There
are many different ways to measure semantic relat-
edness. In this work, we use distributional similar-
ity as our measure of semantic similarity. However,
since we are taking a fully unsupervised approach,
we do not know a priori which pairs of terms may
be related to each other. Hence, we must compute
the semantic similarity between all possible pairs of
terms within the lexicon. To solve this computa-
tionally challenging task, we use a large-scale all-
pairs distributional similarity approach similar to the
one originally proposed by Pasca and Dienes (Pasca
and Dienes, 2005). Our implementation, which
makes use of Hadoop?s MapReduce distributed pro-
gramming paradigm, can efficiently compute all-
pairs distributional similarity over very large corpora
(e.g., the Twitter pairs we use later were mined from
a corpus of half a billion Twitter messages).
Using a similar strategy as Pasca and Dienes, we
define term contexts as the bigrams that appear to
the left and to the right of a given word (Pasca and
Dienes, 2005). Following standard practice, the con-
textual vectors are weighted according to pointwise
mutual information and the similarity between the
vectors is computed using the cosine similarity met-
ric (Lin and Pantel, 2001; Bhagat and Ravichandran,
2008). It is important to note that there are many
other possible ways to compute distributional and
semantic similarity, and that just about any approach
can be used within our framework. The approach
used here was chosen because we had an existing
implementation. Indeed, other approaches may be
more apt for other data sets and tasks.
This approach is applied to both the common En-
glish corpus and the domain corpus. This yields two
sets of semantically (distributionally) similar word
pairs that will ultimately be used to distill unsuper-
vised lexical variants.
2.2 Filtering Common English Variants
Given these two sets of semantically similar word
pairs, the next step in our procedure is designed to
identify the domain-specific pairs by filtering out the
common English variants. The procedure that we
follow is very simple, yet highly effective. Given
the semantically similar word pairs harvested from
the domain corpus, we eliminate all of the pairs that
are also found in the semantically similar common
English pairs.
Any type of ?common English? corpus can be
used for this purpose, depending on the task. How-
ever, we found that a large corpus of newswire ar-
ticles tends to work well. Most of the semanti-
cally similar word pairs harvested from such a cor-
pus are common lexical variants and synonyms. By
eliminating these common variants from the har-
vested domain corpus pairs, we are left with only
the domain-specific semantically similar word pairs.
2.3 Lexical Similarity-Based Re-ordering
The first step of our mining procedure identified
semantically similar term pairs using distributional
similarity, while the second identified those that
were domain-specific by filtering out common En-
glish variants. The third, and final, step of our pro-
cedure re-orders the output of the second step to ac-
count for lexical similarity.
For each word pair (from the second step of our
procedure), we compute two scores: 1) a seman-
84
tic similarity score, and 2) a lexical similarity score.
The final score of the pair is then simply the prod-
uct of the two scores. In this work, we use the
cosine similarity score as our semantic similarity
score, since it is already computed during the first
step of our procedure.
In the social media domain, as in the mobile tex-
ting domain, compressed writing schemes typically
involve deleting characters or replacing one or more
characters with some other characters. For example,
users might delete vowels (?tomorrow???tmrrw?),
or replace ?ph? with its phonetic equivalent ?f ?,
as in ?phone???fone?. We make use of a subse-
quence similarity function (Lodhi et al, 2002) which
can still capture the structural overlap (in the form
of string subsequences) between the remaining un-
changed letters in the noisy word and the original
clean word from which it was derived. In this work
we use a subsequence length of 2, but as with the
other steps in our procedure, this one is purpose-
fully defined in a general way. Any semantic sim-
ilarity score, lexical similarity score, and combina-
tion function can be used in practice.
The output of the entire procedure is a scored list
of word pairs that are semantically related, domain-
specific, and lexically similar, thereby exhibiting the
characteristics that we initially defined as important.
We treat these (scored) pairs as pseudo training data
that has been derived in a fully unsupervised manner.
We anticipate that these pairs will serve as powerful
training data for a variety of tasks, such as noisy text
normalization, which we will return to in Section 3.
2.4 Example and Error Analysis
As an illustrative example of this procedure in prac-
tice, Table 1 shows the actual output of our system
for each step of the mining procedure. To generate
this example, we used a corpus of 2GB of English
news articles as our ?common English? corpus and
a corpus of approximately 500 million Twitter mes-
sages as our domain corpus. In this way, our goal
is to identify Twitter-specific lexical variants, which
we will use in the next section to normalize noisy
Twitter messages.
Column (A) of the table shows that our distribu-
tional similarity approach is capable of identifying
a variety of semantically similar terms in the Twit-
ter corpus. However, the list contains a large num-
Rank Precision
P@50 0.90
P@100 0.88
Table 2: Precision at 50 and 100 of the induced exception
dictionary.
ber of common English variants that are not spe-
cific to Twitter. Column (B) shows the outcome of
eliminating all of the pairs that were found in the
newswire corpus. Many of the common pairs have
been eliminated and the list now contains mostly
Twitter-specific variants. Finally, Column (C) shows
the result of re-ordering the domain-specific pairs to
account for lexical similarity.
In our specific case, the output of step 1 yielded
a list of roughly 3.3M potential word variants. Fil-
tering out common English variants reduced this to
about 314K pairs. In order to estimate the quality of
the list we computed the precision at 50 and at 100
for which the results are shown in Table 2. Further-
more, we find that up to position 500 the pairs are
still of reasonable quality. Thereafter, the number of
errors start to increase noticeably. In particular, we
find that the most common types of errors are
1. Number-related: e.g. ?30? and ?30pm? (due to
incorrect tokenization), or ?5800? and ?5530?;
2. Lemma-related: e.g. ?incorrect? and ?incor-
rectly?; and
3. Negations: e.g. ?could? and ?couldnt?.
Performance can thus be improved by making
use of better tokenization, lemmatizing words, fil-
tering out common negations and filtering out pairs
of numbers.
Still, the resulting pairs satisfy all of our de-
sired qualities rather well, and hence we hypothesize
would serve as useful training data for a number of
different Twitter-related natural language processing
tasks. Indeed, we will now describe one such possi-
ble application and empirically validate the utility of
the automatically mined pairs.
85
(A) (B) (C)
i? you u? you ur? your
my? the seeking? seeks wit? with
u? you 2? to to? too
is? was lost? won goin? going
a? the q? que kno? know
i? we f*ck? hell about? bout
my? your feat? ft wat? what
and? but bday? birthday jus? just
seeking? seeks ff? followfriday talkin? talking
me? you yang? yg gettin? getting
2? to wit? with doin? doing
am? was a? my so? soo
are? were are? r you? your
lost? won amazing? awesome dnt? dont
he? she til? till bday? birthday
q? que fav? favorite nothin? nothing
it? that mostly? partly people? ppl
f*ck? hell northbound? southbound lil? little
can? could hung? toned sayin? saying
im? its love? miss so? sooo
Table 1: Column (A) shows the highest weighted distributionally similar terms harvested from a large Twitter corpus.
Column (B) shows which pairs from (A) remain after filtering out distributionally similar word pairs mined from a
large news corpus. Column (C) shows the effect of reordering the pairs from (B) using a string similarity kernel.
3 Deriving A Common Exception
Dictionary for Text Normalization as a
Use Case for Mining Lexical Variants
As discussed in Section 1, these training pairs may
aid methods which attempt to normalize noisy text
by translating from the ill-formed text into stan-
dard English. Since the OOV distribution in noisy
text mostly resemble a power law distribution (see
Figure 1), one may use the highest scoring train-
ing pairs to induce ?exception dictionaries? (lists of
(noisy word)?(most likely clean word)) of the most
common domain-specific abbreviations found in the
text.
We will demonstrate the utility of our derived
pairs in one specific use case, namely inducing a
domain-specific exception dictionary to augment a
vanilla normalization method. We leave the sec-
ond proposed use-case, namely using pairs in the
long tail for learning transformation rules, for future
work.
We evaluate the first use case in Section 4.
3.1 Baseline Normalization Method
We make use of a competitive heuristic text nor-
malization method over Twitter data as a baseline,
and compare its accuracy to an augmented method
which makes use of an automatically induced excep-
tion dictionary (using the method described in Sec-
tion 2) as a first step, before resorting to the same
baseline method as a ?back-off? for words not found
in the dictionary.
As we point out in Section 5, there are various
metaphors within which the noisy text normalization
problem has been approached. In general, however,
the problem of noisy text normalization may be ap-
proached by using a three step process (Gouws et al,
2011):
1. In the out-of-vocabulary (OOV) detection
step, we detect unknown words which are can-
didates for normalization
2. In the candidate selection step, we find the
weighted lists of most likely candidates (from
a list of in-vocabulary (IV) words) for the OOV
words and group them into a confusion set. The
86
confusion sets are then appended to one another
to create a confusion- network or lattice
3. Finally, in the decoding step, we use a lan-
guage model to rescore the confusion network,
and then find the most likely posterior path
(Viterbi path) through this network.
The words at each node in the resulting posterior
Viterbi path represents the words of the hypothe-
sized original clean sentence.
In this work, we reimplement the method de-
scribed in Contractor (2010) as our baseline method.
We next describe the details of this method in the
context of the framework presented above. See
(Gouws et al, 2011) for more details.
OOV DETECTION is a crucial part of the nor-
malizaton process, since false-positives will result
in undesirable attempts to ?correct? IV words, hence
bringing down the method?s accuracy. We imple-
ment OOV detection as a simple lexicon-lookup pro-
cedure, with heuristics for handling specific out-of-
vocabulary-but-valid tokens such as hash tags and
@usernames.
CANDIDATE SELECTION involves comparing
an unknown OOV word to a list of words which
are deemed in-vocabulary, and producing a top-K
ranked list with candidate words and their estimated
probabilities of relevance as output. This process re-
quires a function with which to compute the simi-
larity or alternatively, distance, between two words.
More traditional string-similarity functions like the
simple Lehvenshtein string edit distance do not fare
too well in this domain.
We implement the IBM-similarity (Contractor et
al., 2010) which employs a slightly more advanced
similarity function. It finds the length of the longest
common subsequence (LCS) between two strings s1
and s2, normalized by the edit distance (ED) be-
tween the consonants in each string (referred to as
the ?consonant skeleton? (CS)), thus
sim(s1, s2) =
LCS(s1, s2)
ED(CS(s1),CS(s2))
Finally, the DECODING step takes an input word
lattice (lattice of concatenated, weighted confusion
sets), and produces a new lattice by incorporating
the probabilities from an n-gram language model
with the prior probabilities in the lattice to produce a
reranked posterior lattice. The most likely (Viterbi)
path through this lattice represents the decoded clean
output. We use SRI-LM (Stolcke, 2002) for this.
3.2 Augmenting the Baseline: Our Method
In order to demonstrate the utility of the mined lex-
ical variant pairs, we first construct a (noisy, clean)
lookup table from the mined pairs. We (arbitrarily)
use the 50 mined pairs with the highest overall com-
bined score (see Section 2.3) for the exception dic-
tionary. For each pair, we map the OOV term (noisy
and typically shorter) to the IV term (clean and usu-
ally longer). The exception lookup list is then used
to augment the baseline method (see Section 3.1) in
the following way: When the method encounters a
new word, it first checks to see if the word is in the
exception dictionary. If it is, we normalize to the
value in the dictionary. If it is not, we pass the ill-
formed word to the baseline method to proceed as
normal.
4 Evaluation
4.1 Dataset
We make use of the Twitter dataset discussed in
Han (2011). It consists of a random sampling of 549
English tweets, annotated by three independent an-
notators. All OOV words were pre-identified and the
annotators were requested to determine the standard
form (gold standard) for each ill-formed word.
4.2 Evaluation Metrics
In this study, we are interested in measuring the
quality of our mined training pairs by evaluating its
utility on an external task: Using the training pairs
to induce a (noisy?clean) exception dictionary to
augment the working of a standard noisy text nor-
malization system. Hence, our focus is entirely on
the accuracy of the candidate selection procedure as
defined in Section 3.1. We compute this accuracy
in terms of the word error rate (WER), defined as
the number of token substitutions, insertions or dele-
tions one has to make to turn the system output into
the gold standard, normalized by the total number of
tokens in the output. In order to remove the possi-
ble bias introduced by our very basic OOV-detection
87
Method WER % Change
Naive baseline 10.7% ?
IBM-baseline 7.8% ?27.1%
Our method 5.6% ?47.7%
Table 3: Word error rate (WER, lower is better) results
of our method against a naive baseline and the much
stronger IBM-baseline (Contractor et al, 2010). We also
show the relative change in WER for our method and the
IBM-baseline compared to the naive baseline.
mechanism, we evaluate the output of all systems
only on the oracle pairs. Oracle pairs are defined as
the (input,system-output,gold) pairs where input and
gold do not match. In other words, we remove the
possible confounding impact of imperfect OOV de-
tection on the accuracy of the normalization process
by assuming a perfect OOV-detection step.
4.3 Discussion of Results
The results of our experiments are displayed in Ta-
ble 3. It is important to note that the focus is not
on achieving the best WER compared to other sys-
tems (although we achieve very competitive scores),
but to evaluate the added utility of integrating an
exception dictionary which is based purely on the
mined (noisy, clean) pairs with an already competi-
tive baseline method.
The ?naive baseline? shows the results if we make
no changes to the input tokens for all oracle pairs.
Therefore it reflects the total level of errors that are
present in the corpus.
The IBM-method is seen to reduce the amount of
errors by a substantial 27.1%. However, the aug-
mented method results in a further 20.6% reduction
in errors, for a total reduction of 47.7% of all er-
rors in the dataset, compared to the IBM-baseline?s
27.1%.
Since we replace matches in the dictionary indis-
criminately, and since the dictionary comprise those
pairs that typically occur most frequently in the cor-
pus from which they were mined, it is important to
note that if these pairs are of poor quality, then their
sheer frequency will drive the overall system accu-
racy down. Therefore, the accuracy of these pairs
are strongly reflected in the WER performance of
the augmented method.
Noisy Clean % Oracle Pairs
u you 8.7
n and 1.4
ppl people 1
da the 1
w with 0.7
cuz because 0.5
y why 0.5
yu you 0.5
lil little 0.5
dat that 0.5
wat what 0.4
tha the 0.4
kno know 0.4
r are 0.4
Table 4: Error analysis for all (noisy, clean) normaliza-
tions missed by the vanilla IBM-baseline method, but in-
cluded in the top-50 pairs used for constructing the ex-
ception dictionary. We also show the percentage of all
oracle pairs that are corrected by including each pair in
an exception dictionary.
Table 4 shows the errors missed by the IBM-
baseline, but contained in the mined exception dic-
tionary. We also show each pair?s frequency of oc-
currence in the oracle pairs (hence its contribution
towards lowering WER).
5 Related work
To the best of our knowledge, we are the first to ad-
dress the problem of mining pairs of lexical variants
from noisy text in an unsupervised and purely sta-
tistical manner that does not require aligned noisy
and clean messages. To obtain aligned clean and
noisy text without annotated data implies the use
of some normalizing method first. Yvon (2010)
presents one such approach, where they generate ex-
ception dictionaries from their finite-state system?s
normalized output. However, their method is still
trained on annotated training pairs, and hence su-
pervised. A related direction is ?transliteration min-
ing? (Jiampojamarn et al, 2010) which aims to au-
tomatically obtain bilingual lists of names written in
different scripts. They also employ string-similarity
measures to find similar string pairs written in differ-
ent scripts. However, their input data is constrained
88
to Wikipedia articles written in different languages,
whereas we impose no constrains on our input data,
and merely require a large collection thereof.
Noisy text normalization, on the other hand, has
recently received a lot of focus. Most works con-
strue the problem in the metaphors of either ma-
chine translation (MT) (Bangalore et al, 2002;
Aw et al, 2006; Kaufmann and Kalita, 2010),
spelling correction (Choudhury et al, 2007; Cook
and Stevenson, 2009), or automated speech recog-
nition (ASR) (Kobus et al, 2008). For our evalua-
tion, we developed an implementation of Contrac-
tor (2010) which works on the same general ap-
proach as Han (2011).
6 Conclusions and Future Work
The ability to automatically extract lexical variants
from large noisy corpora has many practical appli-
cations, including noisy text normalization, query
spelling suggestion, fixing OCR errors, and so on.
This paper developed a novel methodology for au-
tomatically mining such pairs from a large domain-
specific corpus. The approach makes use of distri-
butional similarity for measuring semantic similar-
ity, a novel approach for filtering common English
pairs by comparing against pairs mined from a large
news corpus, and a substring similarity measure for
re-ordering the pairs according to their lexical simi-
larity.
To demonstrate the utility of the method, we used
automatically mined pairs to construct an unsuper-
vised exception dictionary, that was used in con-
junction with a string similarity measure, to form
a highly effective hybrid noisy text normalization
technique. By exploiting the properties of the power
law distribution, the exception dictionary can suc-
cessfully correct a large number of cases, while the
heuristic string similarity-based approach handled
many of the less common test cases from the tail of
the distribution. The hybrid approach showed sub-
stantial reductions in WER (around 20%) versus the
string similarity approach, hence validating our pro-
posed approach.
For future work we are interested in exploiting the
(noisy, clean) pairs contained in the long tail as input
to learning algorithms for acquiring domain-specific
lexical transformations.
Acknowledgments
Stephan Gouws would like to thank MIH Holdings
Ltd. for financial support during the course of this
work.
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A phrase-
based statistical model for SMS text normalization. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002.
Bootstrapping bilingual data using consensus transla-
tion for a multilingual instant messaging system. In
Proceedings of the 19th International Conference on
Computational Linguistics Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674?
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and modeling of the
structure of texting language. International Journal on
Document Analysis and Recognition, 10(3):157?174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised cleansing of noisy text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An unsupervised model
for text message normalization. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, pages 71?78. Association for Computa-
tional Linguistics.
S. Gouws, D. Metzler, C. Cai, and E. Hovy. 2011. Con-
textual Bearing on Linguistic Variation in Social Me-
dia. In Proceedings of the ACL-11 Workshop on Lan-
guage in Social Media. Association for Computational
Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
S. Jiampojamarn, K. Dwyer, S. Bergsma, A. Bhargava,
Q. Dou, M.Y. Kim, and G. Kondrak. 2010. Translit-
eration generation and mining with limited training
89
resources. In Proceedings of the 2010 Named Enti-
ties Workshop, pages 39?47. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages. In International Conference
on Natural Language Processing, Kharagpur, India.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normal-
izing SMS: are two metaphors better than one? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343?360, December.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. The Journal of Machine Learning Research,
2:419?444.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119?130. Springer Berlin / Heidelberg.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, volume 2,
pages 901?904. Citeseer.
F. Yvon. 2010. Rewriting the orthography of sms mes-
sages. Journal of Natural Language Engineering,
16(02):133?159.
90
