Proceedings of NAACL HLT 2009: Short Papers, pages 273?276,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Chinese Abbreviation Generation Using Conditional Random
Field
Dong Yang, Yi-cheng Pan, and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552 Japan
{raymond,thomas,furui}@furui.cs.titech.ac.jp
Abstract
This paper presents a new method for au-
tomatically generating abbreviations for Chi-
nese organization names. Abbreviations are
commonly used in spoken Chinese, especially
for organization names. The generation of
Chinese abbreviation is much more complex
than English abbreviations, most of which are
acronyms and truncations. The abbreviation
generation process is formulated as a character
tagging problem and the conditional random
field (CRF) is used as the tagging model. A
carefully selected group of features is used in
the CRF model. After generating a list of ab-
breviation candidates using the CRF, a length
model is incorporated to re-rank the candi-
dates. Finally the full-name and abbreviation
co-occurrence information from a web search
engine is utilized to further improve the per-
formance. We achieved top-10 coverage of
88.3% by the proposed method.
1 Introduction
Long named entities are frequently abbreviated in
oral Chinese language for efficiency and simplic-
ity. Therefore, abbreviation modeling is an impor-
tant building component for many systems that ac-
cept spoken input, such as directory assistance and
voice search systems.
While English abbreviations are usually formed
as acronyms, Chinese abbreviations are much more
complex, as shown in Figure 1. Most of the Chi-
nese abbreviations are formed by selecting several
characters from full-names, which are not necessar-
ily the first character of each word. Usually the orig-
inal character order in the full-name is preserved in
???? ?? T s i n g h u a U n i v e r s i t y
??????? ?? C h i n a c e n t r a l t e l e v i s i o n
F u l l ? n a m e  a b b r e v i a t i o n  E n g l i s h  e x p l a n a t i o n
?????? ?? ???? P e k i n g  U n i v e r s i t y  N o . 3  h o s p i t a l
Figure 1: Chinese abbreviation examples
the abbreviation. However, re-ordering of charac-
ters as shown in the third example in Figure 1 where
characters ?n? and ??? are swapped in the abbre-
viation, also happens.
There has been a considerable amount of research
on extracting full-name and abbreviation pairs in
the same document for obtaining abbreviations (Li
and Yarowsky, 2008; Sun et al, 2006; Fu et al,
2006). However, generation of abbreviations given
a full-name is still a non-trivial problem. Chang
and Lai (Chang and Lai, 2004) have proposed using
a hidden Markov model to generate abbreviations
from full-names. However, their method assumes
that there is no word-to-null mapping, which means
that every word in the full-name has to contribute at
least one character to the abbreviation. This assump-
tion does not hold for organizations? names which
have many word skips in the abbreviation genera-
tion.
The CRF was first introduced to natural language
processing (NLP) by (Lafferty et al, 2001) and has
been widely used in word segmentation, part-of-
speech (POS) tagging, and some other NLP tasks.
In this paper, we convert the Chinese abbreviation
generation process to a CRF tagging problem. The
key problem here is how to find a group of discrim-
273
inant and robust features. After using the CRF, we
get a list of abbreviation candidates with associate
probability scores. We also use the prior condi-
tional probability of the length of the abbreviations
given the length of the full-names to complement the
CRF probability scores. Such global information is
hard to include in the CRF model. In addition, we
apply the full-name and abbreviation candidate co-
occurrence statistics obtained on the web to increase
the correctness of the abbreviation candidates.
2 Chinese Abbreviation Introduction
Chinese abbreviations are generated by three meth-
ods (Lee, 2005): reduction, elimination, and gener-
alization.
Both in the reduction and elimination methods,
characters are selected from the full-name, and the
order of the characters is sometime changed. Note
that this paper does not cover the case when the or-
der is changed. The elimination means that one or
more words in the full-name are ignored completely,
while the reduction requires that at least one char-
acter is selected from each word. All the three ex-
amples in Figure 1 are produced by the elimination,
where at least one word is skipped.
Generalization, which is used to abbreviate a list
of similar terms, is usually composed of the number
of terms and a shared character across the terms. A
example is ?n? (three forces) for ?????
?? (land force, sea force, air force). This is the
most difficult scenario for the abbreviations and is
not considered in this paper.
3 CRF Model for Abbreviation Modeling
3.1 CRF model
A CRF is an undirected graphical model and assigns
the following probability to a label sequence L =
l1l2 . . . lT , given an input sequence C = c1c2 . . . cT ,
P (L|C) = 1Z(C)exp(
T?
t=1
?
k
?kfk(lt, lt?1, C, t))
(1)
Here, fk is the feature function for the k-th fea-
ture, ?k is the parameter which controls the weight
of the k-th feature in the model, and Z(C) is the nor-
malization term that makes the summation of the
probability of all label sequences to 1. CRF training
is usually performed through the typical L-BFGS al-
gorithm (Wallach, 2002) and decoding is performed
by Viterbi algorithm (Viterbi, 1967). In this paper,
we use an open source toolkit ?crf++?.
3.2 Abbreviation modeling as a tagging
problem
In order to use the CRF method in abbreviation gen-
eration, the abbreviation generation problem was
converted to a tagging problem. The character is
used as a tagging unit and each character in a full-
name is tagged by a binary variable with the values
of either Y or N: Y stands for a character used in the
abbreviation and N means not. An example is given
in Figure 2.
??????? ??
?/ N ?/ N ?/ N ?/ Y ?/ N ?/ Y ?/ N
Figure 2: Abbreviation in the CRF tagging format
3.3 Feature selection for the CRF
In the CRF method, feature function describes
a co-occurrence relation, and it is defined as
fk(lt, lt?1, C, t) (Eq. 1). fk is usually a binary func-
tion, and takes the value 1 when both observation ct
and transition lt?1 ? lt are observed. In our ab-
breviation generation model, we use the following
features:
1. Current character The character itself is the
most important feature for abbreviation as it will be
either retained or discarded. For example, ??? (bu-
reau) and ??? (institue), indicating a government
department, are very common characters used in ab-
breviations. When they appear in full-names, they
are likely to be kept in abbreviations.
2. Current word In the full name of ??I??
??? (China Agricultural university), the word ??
I? (China) is usually ignored in the abbreviation,
but the word ???? (agriculture) is usually kept.
The length (the number of characters) is also an im-
portant feature of the current word.
3. Position of the current character in the cur-
rent word Previous work (Chang and Lai, 2004)
showed that the first character of a word has high
possibility to form part of the abbreviation and this
is also true for the last character of a three-character
word.
4. Combination of feature 2. and 3. above
Combination of the features 2 and 3 is expected to
improve the performance, since the position infor-
274
mation affects the abbreviation along with the cur-
rent word. For example, ending character in ????
(university) and that in ???? (research institute)
have very different possibilities to be selected for ab-
breviations.
Besides the features above, we have examined
context information (previous word, previous char-
acter, next character, etc.) and other local features
like the length of the word, but these features did
not improve the performance. The reason may be
due to the sparseness of the training data.
4 Improvement by a Length Model and a
Web Search Engine
4.1 Length model
There is a strong correlation between the length of
organizations? full-names and their abbreviations.
We use the length modeling based on discrete prob-
ability of P (M |L), in which the variables M and
L are lengths of abbreviations and full-names, re-
spectively. Since it is difficult to incorporate length
information into the CRF model explicitly, we use
P (M |L) to rescore the output of the CRF.
In order to use the length information, we model
the abbreviation process with two steps:
? 1st step: evaluate the length in abbreviation ac-
cording to the length model P (M |L);
? 2nd step: choose the abbreviation, given the
length and full-name.
We assume the following approximation:
P (A|F ) ? P (M |L) ? P (A|M,F ) (2)
in which variable A is the abbreviation and F is the
full-name; P (M |L) is the length model, and the sec-
ond probability can be calculated according to the
Bayesian rule:
P (A|M,F ) = P (A,M |F )P (M |F )
= P (A,M |F )?
length(A?)=M P (A?,M |F )
(3)
It is obvious that P (A,M |F ) = P (A|F ) (as A
contains the information M implicitly) and P (A|F )
can be obtained from the output of the CRF.
4.2 Web search engine
Co-occurrence of a full-name and an abbreviation
candidate can be a clue of the correctness of the ab-
breviation. We use the ?abbreviation candidate?+
?full-name? as queries and input them to the most
popular Chinese search engine (www.baidu.com),
and then we use the number of hits as the metric
to perform re-ranking. The hits is theoretically re-
lated to the number of pages which contain both the
full-name and abbreviation. The bigger the value of
hits, the higher probability that the abbreviation is
correct.
We then simply multiply the previous probability
score, obtained from Eq. 2, by the number of hits
and re-rank the top-30 candidates accordingly.
There are some other ways to use information re-
trieval methods (Mandala et al, 2000). Our method
has an advantage that the access load to the web
search engine is relatively small.
5 Experiment
5.1 Data introduction
The corpus we use in this paper comes from two
sources: one is the book ?modern Chinese abbre-
viation dictionary? (Yuan and Ruan, 2002) and the
other is the wikipedia. Altogether we collected 1945
pairs of organization full-names and their abbrevia-
tions.
The data is randomly divided into two parts, a
training set with 1298 pairs and a test set with 647
pairs. Table 1 shows the length mapping statistics
of the training set. It can be seen that the average
length of full-names is about 7.29. We know that for
a full-name with length N, the number of abbrevia-
tion candidates is about 2N ? 2?N (exclude length
of 0, 1, and N) and we can conclude that the average
number of candidates for organization names in this
corpus is more than 100.
5.2 Results
The abbreviation method described is part of a
project to develop a voice-based search application.
For our name abbreviation system we plan to add 10
abbreviation candidates for each organization name
into the vocabulary of our voice search application,
hence here we consider top-10 coverage.
275
length of length of abbreviation
full-name 2 3 4 5 >5 sum
4 107 1 0 0 0 108
5 89 140 0 0 0 229
6 96 45 46 0 0 187
7 60 189 49 16 0 314
8 48 29 60 3 6 146
9 10 47 35 12 2 106
10 18 11 29 8 6 73
others 21 43 38 17 14 133
average length of the full-name 7.27
average length of the abbreviation 3.01
Table 1: Length statistics on the training set
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 72?75,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Combining a Two-step Conditional Random Field Model and a Joint
Source Channel Model for Machine Transliteration
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi
Masanobu Nakamura and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Techonology
{raymond,dixonp,thomas,oonishi,masa,furui}@furui.cs.titech.ac.jp
Abstract
This paper describes our system for
?NEWS 2009 Machine Transliteration
Shared Task? (NEWS 2009). We only par-
ticipated in the standard run, which is a
direct orthographical mapping (DOP) be-
tween two languages without using any
intermediate phonemic mapping. We
propose a new two-step conditional ran-
dom field (CRF) model for DOP machine
transliteration, in which the first CRF seg-
ments a source word into chunks and the
second CRF maps the chunks to a word
in the target language. The two-step CRF
model obtains a slightly lower top-1 ac-
curacy when compared to a state-of-the-
art n-gram joint source-channel model.
The combination of the CRF model with
the joint source-channel leads to improve-
ments in all the tasks. The official re-
sult of our system in the NEWS 2009
shared task confirms the effectiveness of
our system; where we achieved 0.627 top-
1 accuracy for Japanese transliterated to
Japanese Kanji(JJ), 0.713 for English-to-
Chinese(E2C) and 0.510 for English-to-
Japanese Katakana(E2J) .
1 Introduction
With the increasing demand for machine transla-
tion, the out-of-vocabulary (OOV) problem caused
by named entities is becoming more serious.
The translation of named entities from an alpha-
betic language (like English, French and Spanish)
to a non-alphabetic language (like Chinese and
Japanese) is usually performed through transliter-
ation, which tries to preserve the pronunciation in
the source language.
For example, in Japanese, foreign words im-
ported from other languages are usually written
H a r r i n g t o n ? ? ? ? ? English-to-Japanese
T i m o t h y ??? English-to-Chinese
Source Name       Target Name          Note
ti mo   xi                     Chinese Romanized writing
ha  ri n   to  n Japanese Romanized writing
Figure 1: Transliteration examples
in a special syllabary called Katakana; in Chi-
nese, foreign words accepted to Chinese are al-
ways written by Chinese characters; examples are
given in Figure 1.
An intuitive transliteration method is to first
convert a source word into phonemes, then find the
corresponding phonemes in the target language,
and finally convert to the target language?s writ-
ing system (Knight and Graehl, 1998; Oh et al,
2006). One major limitation of this method is that
the named entities are usually OOVs with diverse
origins and this makes the grapheme-to-phoneme
conversion very difficult.
DOP is gaining more attention in the transliter-
ation research community which is also the stan-
dard evaluation of NEWS 2009.
The source channel and joint source-channel
models (Li et al, 2004) have been proposed for
DOP, which try to model P (T |S) and P (T, S) re-
spectively, where T and S denotes the words in
the target and source languages. (Ekbal et al,
2006) modified the joint source-channel model to
incorporate different context information into the
model for the Indian languages. Here we propose
a two-step CRF model for transliteration, and the
idea is to make use of the discriminative ability of
CRF. For example, in E2C transliteration, the first
step is to segment an English name into alphabet
chunks and after this step the number of Chinese
characters is decided. The second step is to per-
form a context-dependent mapping from each En-
glish chunk into one Chinese character. Figure 1
shows that this method is applicable to many other
72
transliteration tasks including E2C and E2J.
Our CRF method and the n-gram joint source-
channel model use different information in pre-
dicting the corresponding Chinese characters and
therefore in combination better results are ex-
pected. We interpolate the two models linearly
and use this as our final system for NEWS 2009.
The rest of the paper is organized as follows: Sec-
tion 2 introduces our system in detail including the
alignment and decoding modules, Section 3 ex-
plains our experiments and finally Section 4 de-
scribes conclusions and future work.
2 System Description
Our system starts from a joint source channel
alignment to train the CRF segmenter. The CRF
is used to re-segment and align the training data,
and from this alignment we create a Weighted Fi-
nite State Transducer (WFST) based n-gram joint
source-channel decoder and a CRF E2C converter.
The following subsections explain the structure of
our system shown in Figure 2.
N-gram joint source-channel Alignment
CRF segmenter
N-gram WFST decoder CRF E2C converter
Each pair in the training corpus
New Alignment
N-gram WFST decoder
CRF E2C converter
Linear combination
Each source name in the test corpus
CRF segmenter
Tr
ai
ni
ng
Te
st
in
g
Output
Figure 2: System structure
2.1 Theoretical background
2.1.1 Joint source channel model
The source channel model represents the condi-
tional probability of target names given a source
name P (T |S). The joint source channel model
calculates how the source words and target names
are generated simultaneously (Li et al, 2004):
P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)
= P (< s, t >1, < s, t >2, ..., < s, t >k)
=
K?
k=1
P (< s, t >k | < s, t >k?11 ) (1)
where, S = (s1, s2, ..., sk) and T =
(t1, t2, ..., tk).
2.1.2 CRF
A CRF (Lafferty et al, 2001) is an undirected
graphical model which assigns a probability to a
label sequence L = l1l2 . . . lT , given an input se-
quence C = c1c2 . . . cT ,
P (L|C) = 1
Z(C)
exp(
T?
t=1
?
k
?kfk(lt, lt?1, C, t))
(2)
For the kth feature, fk denotes the feature function
and ?k is the parameter which controls the weight-
ing. Z(C) is a normalization term that ensure the
distribution sums to one. CRF training is usually
performed through the L-BFGS algorithm (Wal-
lach, 2002) and decoding is performed by Viterbi
algorithm (Viterbi, 1967). In this paper, we use an
open source toolkit ?crf++?1.
2.2 N-gram joint source-channel alignment
To calculate the probability in Equation 1, the
training corpus needs to be aligned first. We use
the Expectation-Maximization(EM) algorithm to
optimize the alignment A between the source S
and target T pairs, that is:
A? = arg max
A
P (S, T,A) (3)
The procedure is summarized as follows:
1. Initialize a random alignment
2. E-step: update n-gram probability
3. M-step: apply the n-gram model to realign
each entry in corpus
4. Go to step 2 until the alignment converges
2.3 CRF alignment & segmentation
The performance of EM algorithm is often af-
fected by the initialization. Fortunately, we can
correct mis-alignments by using the discriminative
ability of the CRF. The alignment problem is con-
verted into a tagging problem that doesn?t require
the use of the target words at all. Figure 3 is an
example of a segmentation and alignment, where
the labels B and N indicate whether the character
is in the starting position of the chunk or not.
In the CRF method the feature function de-
scribes a co-occurrence relation, and it is formally
1crfpp.sourceforge.net
73
T i m o t h y ???
T/B i/N m/B o/N t/B h/N y/N
Ti/? mo/? thy/?
Figure 3: An example of the CRF segmenter for-
mat and E2C converter
defined as fk(lt, lt?1, C, t) (Eq. 2). fk is usually a
binary function, and takes the value 1 when both
observation ct and transition lt?1 ? lt are ob-
served. In our segmentation tool, we use the fol-
lowing features
? 1. Unigram features: C?2, C?1, C0, C1, C2
? 2. Bigram features:C?1C0, C0C1
Here, C0 is the current character, C?1 and C1 de-
note the previous and next characters and C?2 and
C2 are the characters two positions to the left and
right of C0.
In the alignment process, we use the CRF seg-
menter to split each English word into chunks.
Sometimes a problem occurs in which the num-
ber of chunks in the segmented output will not be
equal to the number of Chinese characters. In such
cases our solution is to choose from the n-best list
the top scoring segmentation which contains the
correct number of chunks.
In the testing process, we use the segmenter in
the similar way, but only take top-1 output seg-
mented English chunks for use in the following
CRF E2C conversion.
2.4 CRF E2C converter
Similar to the CRF segmenter, the CRF E2C con-
verter has the format shown in Figure 3. For this
CRF, we use the following features:
? 1. Unigram features: C?1, C0, C1
? 2. Bigram features:C?1C0, C0C1
where C represents the English chunks and the
subscript notation is the same as the CRF seg-
menter.
2.5 N-gram WFST decoder for joint source
channel model
Our decoding approach makes use of WFSTs to
represent the models and simplify the develop-
ment by utilizing standard operations such as com-
position and shortest path algorithms.
After the alignments are generated, the first
step is to build a corpus to train the translit-
eration WFST. Each aligned word is converted
to a sequence of transliteration alignment pairs
?s, t?1 , ?s, t?2 , ... ?s, t?k, where each s can be a
chunk of one or more characters and t is assumed
to be a single character. Each of the pairs is
treated as a word and the entire set of alignments is
used to train an n-gram language model. In these
evaluations we used the MITLM toolkit (Hsu and
Glass, 2008) to build a trigram model with modi-
fied Kneser-Ney smoothing.
We then use the procedure described in (Caseiro
et al, 2002) and convert the n-gram to a weighted
acceptor representation where each input label be-
longs to the set of transliteration alignment pairs.
Next the pairs labels are broken down into the in-
put and output parts and the acceptor is converted
to a transducer M . To allow transliteration from a
sequence of individual characters, a second WFST
T is constructed. T has a single state and for each
s a path is added to allow a mapping from the
string of individual characters.
To perform the actual transliteration, the input
word is converted to an acceptor I which has one
arc for each of the characters in the word. I is
then combined with T and M according to O =
I ?T ?M where ? denotes the composition opera-
tor. The n?best paths are extracted from O by pro-
jecting the output, removing the epsilon labels and
applying the n-shortest paths algorithm with de-
terminization from the OpenFst Toolkit(Allauzen
et al, 2007).
2.6 Linear combination
We notice that there is a significant difference be-
tween the correct answers of the n-gram WFST
and CRF decoders. The reason may be due to
the different information utilized in the two de-
coding methods. Since their performance levels
are similar, the overall performance is expected
to be improved by the combination. From the
CRF we compute the probability PCRF (T |S) and
from the list of scores output from the n-gram de-
coder we calculate the conditional probability of
Pn?gram(T |S). These are used in our combina-
tion method according to:
P (T |S) = ?PCRF (T |S)+(1??)Pn?gram(T |S)
(4)
where ? denotes the interpolation weight (0.3 in
this paper).
74
3 Experiments
We use the training and development sets of
NEWS 2009 data in our experiments as detailed
in Table 12. There are several measure metrics in
the shared task and due to limited space in this pa-
per we provide the results for top-1 accuracy.
Task Training data size Test data size
E2C 31961 2896
E2J 23808 1509
Table 1: Corpus introduction
n-gram+CRF
Task Alignment interpolation
WFST CRF
E2C 70.3 67.3 71.5
E2J 44.9 44.8 46.7
Table 2: Top-1 accuracies(%)
The results are listed in Table 2. For E2C
task the top-1 accuracy of the joint source-channel
model is 70.3% and 67.3% for the two-step CRF
model. After combining the two results together
the top-1 accuracy increases to 71.5% correspond-
ing to a 1.2% absolute improvement over the state-
of-the-art joint source-channel model. Similarly,
we get 1.8% absolute improvement for E2J task.
4 Conclusions and future work
In this paper we have presented our new hybrid
method for machine transliteration which com-
bines a new two-step CRF model with a state-of-
the-art joint source-channel model. In compari-
son to the joint source-channel model the combi-
nation approach achieved 1.2% and 1.8% absolute
improvements for E2C and E2J task respectively.
In the first step of the CRF method we only
use the top-1 segmentation, which may propagate
transliteration errors to the following step. In fu-
ture work we would like to optimize the 2-step
CRF jointly. Currently, we are also investigating
minimum classification error (MCE) discriminant
training as a method to further improve the joint
source channel model.
2For the JJ task the submitted results
are only based on the joint source
channel model. Unfortunately, we were
unable to submit a combination result
because the training time for the CRF
was too long.
Acknowledgments
The corpora used in this paper are from ?NEWS
2009 Machine Transliteration Shared Task? (Li et
al., 2004; CJK, website)
References
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration, 1998 Association for Computa-
tional Linguistics.
Li Haizhou, Zhang Min and Su Jian. 2004. A joint
source-channel model for machine transliteration,
2004 Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics.
Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration, Proceedings of the COL-
ING/ACL, pages 191-198.
Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models , Journal of Artificial Intelligence Re-
search, 27, pages 119-151.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data., Proceedings of International Confer-
ence on Machine Learning, 2001, pages 282-289.
Hanna Wallach 2002. Efficient Training of Condi-
tional Random Fields. M. Thesis, University of Ed-
inburgh, 2002.
Andrew J. Viterbi 1967. Error Bounds for Convolu-
tional Codes and an Asymptotically Optimum De-
coding Algorithm. IEEE Transactions on Informa-
tion Theory, Volume IT-13, 1967,pages 260-269.
Bo-June Hsu and James Glass 2008. Iterative Lan-
guage Model Estimation: Efficient Data Structure
& Algorithms. Proceedings Interspeech, pages 841-
844.
Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira
and Ceu Viana 2002. Grapheme-to-phone using
finite state transducers. Proceedings 2002 IEEE
Workshop on Speech Synthesis.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut and Mehryar Mohri 2002. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. Proceedings of the Ninth Interna-
tional Conference on Implementation and Applica-
tion of Automata, (CIAA 2007), pages 11-23.
http://www.cjk.org
75
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 755?763,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Discriminative Lexicon Adaptation for Improved Character Accuracy ?
A New Direction in Chinese Language Modeling
Yi-cheng Pan
Speech Processing Labratory
National Taiwan University
Taipei, Taiwan 10617
thomashughPan@gmail.com
Lin-shan Lee
Speech Processing Labratory
National Taiwan University
Taipei, Taiwan 10617
lsl@speech.ee.ntu.edu.tw
Sadaoki Furui
Furui Labratory
Tokyo Institute of Technology
Tokyo 152-8552 Japan
furui@furui.cs.titech.ac.jp
Abstract
While OOV is always a problem for most
languages in ASR, in the Chinese case the
problem can be avoided by utilizing char-
acter n-grams and moderate performances
can be obtained. However, character n-
gram has its own limitation and proper
addition of new words can increase the
ASR performance. Here we propose a dis-
criminative lexicon adaptation approach for
improved character accuracy, which not
only adds new words but also deletes some
words from the current lexicon. Different
from other lexicon adaptation approaches,
we consider the acoustic features and make
our lexicon adaptation criterion consistent
with that in the decoding process. The pro-
posed approach not only improves the ASR
character accuracy but also significantly
enhances the performance of a character-
based spoken document retrieval system.
1 Introduction
Generally, an automatic speech recognition (ASR)
system requires a lexicon. The lexicon defines the
possible set of output words and also the building
units in the language model (LM). Lexical words
offer local constraints to combine phonemes into
short chunks while the language model combines
phonemes into longer chunks by more global con-
straints. However, it?s almost impossible to include
all words into a lexicon both due to the technical
difficulty and also the fact that new words are cre-
ated continuously. The missed out words will never
be recognized, which is the well-known OOV prob-
lem. Using graphemes for OOV handling is pro-
posed in English (Bisani and Ney, 2005). Although
this sacrifices some of the lexical constraints and in-
troduces a further difficulty to combine graphemes
back into words, it is compensated by its ability for
5.8K characters 61.5K full lexicon
bigram 63.55% 73.8%
trigram 74.27% 79.28%
Table 1: Character recognition accuracy under dif-
ferent lexicons and the order of language model.
open vocabulary ASR. Morphs are another possi-
bility, which are longer than graphemes but shorter
than words, in other western languages (Hirsima?ki
et al, 2005).
Chinese language, on the other hand, is quite
different from western languages. There are no
blanks between words and the definition for words
is vague. Since almost all characters in Chinese
have their own meanings and words are composed
of the characters, there is an obvious solution for
the OOV problem: simply using all characters as
the lexicon. In Table 1 we see the differences in
character recognition accuracy by using only 5.8K
characters and a full set of 61.5K lexicon. The train-
ing set and testing set are the same as those that
will be introduced in Section 4.1. It is clear that
characters alone can provide moderate recognition
accuracies while augmenting new words signifi-
cantly improves the performance. If the words?
semantic functionality can be abandoned, which
definitely can not be replaced by characters, we can
treat words as a means to enhance character recog-
nition accuracy. Such arguments stand at least for
Chinese ASR since they evaluate on character error
rate and do not add explicit blanks between words.
Here we formulate a lexicon adaptation problem
and try to discriminatively find out not only OOV
words beneficial for ASR but also those existing
words that can be deleted.
Unlike previous lexicon adaptation or construc-
tion approaches (Chien, 1997; Fung, 1998; Deligne
and Sagisaka, 2000; Saon and Padmanabhan, 2001;
Gao et al, 2002; Federico and Bertoldi, 2004), we
755
consider the acoustic signals and also the whole
speech decoding structure. We propose to use
a simple approximation for the character poste-
rior probabilities (PPs), which combines acoustic
model and language model scores after decoding.
Based on the character PPs, we adapt the current
lexicon. The language model is then re-trained ac-
cording the new lexicon. Such procedure can be
iterated until convergence.
Characters, are not only the output units in Chi-
nese ASR but also have their roles in spoken docu-
ment retrieval (SDR). It has been shown that char-
acters are good indexing units. Generally, char-
acters can at least help OOV query handling; in
the subword-based confusion network (S-CN) pro-
posed by Pan et al (2007), characters are even
better than words for in-vocabulary (IV) queries.
In addition to evaluating the proposed approach on
ASR performance, we investigate its helpfulness
when integrated with an S-CN framework.
2 Related Work
Previous works for lexicon adaptation were focused
on OOV rate reduction. Given an adaptation cor-
pus, the standard way is to first identify OOVwords.
These OOV words are selected into the current lex-
icon based on the criterion of frequency or recency
(Federico and Bertoldi, 2004). The language model
is also re-estimated according to the new corpus
and new derived words.
For Chinese, it is more difficult to follow the
same approach since OOV words are not readily
identifiable. Several methods have been proposed
to extract OOV words from the new corpus based
on different statistics, which include associate norm
and context dependency (Chien, 1997), mutual in-
formation (Gao et al, 2002), morphological and
statistical rules (Chen and Ma, 2002), and strength
and spread measure (Fung, 1998). The used statis-
tics generally help find sequences of characters
that are consistent to the general concept of words.
However, if we focus on ASR performance, the
constraint of the extracted character strings to be
word-like is unnecessary.
Yang et al (1998) proposed a way to select new
character strings based on average character per-
plexity reduction. The word-like constraint is not
required and they show a significant improvement
on character-based perplexity. Similar ideas were
found to use mutual probability as an effective mea-
sure to combine two existing lexicon words into a
new word (Saon and Padmanabhan, 2001). Though
proposed for English, this method is effective for
Chinese ASR (Chen et al, 2004). Gao et al (2002)
combined an information gain-like metric and the
perplexity reduction criterion for lexicon word se-
lection. The application is on Chinese pinyin-to-
character conversion, which has very good correla-
tion with the underlying language model perplexity.
The above works actually are all focused on the
text level and only consider perplexity effect. How-
ever, as pointed by Rosenfeld (2000), lower per-
plexity does not always imply lower ASR error rate.
Here we try to face the lexicon adaptation problem
from another aspect and take the acoustic signals
involved in the decoding procedure into account.
3 Proposed Approach
3.1 Overall Pictureord
Chara
cter-b
ased  
Confu
sion 
Autom
atic 
Speec
h Rec
ogniti
on
(ASR
)
Chara
cter-b
ased 
Confu
sion N
etwor
k
(CCN
) cons
tructio
n
word lattice
s
Netw
ork (C
CN)
Adap
tation Corpu
s
Lexic
on Ad
aptati
on
for Im
prove
d 
Chara
cter A
ccurac
y
Add/D
elete w
ords
Lexic
on (Lex i)
Langu
age 
Mode
l(LM
i)
y
(LAIC
A)
Word
 
Segm
entati
on
LM Traini
ng 
(Lex i)
Mode
l (LM
i)
Manu
al 
Trans
criptio
n
Segm
entati
on 
and L
M Tra
ining
g
Corpo
ra 
Figure 1: The flow chart of the proposed approach.
We show the complete flow chart in Figure 1. At
the beginning we are given an adaptation spoken
corpus and manual transcriptions. Based on a base-
line lexicon (Lex0) and a language model (LM0)
we perform ASR on the adaptation corpus and con-
struct corresponding word lattices. We then build
character-based confusion networks (CCNs) (Fu
et al, 2006; Qian et al, 2008). On the CCNs we
perform the proposed algorithm to add and delete
words into/from the current lexicon. The LM train-
ing corpora joined with the adaptation corpus is
then segmented using Lex1 and the language model
is in turn re-trained, which gives LM1. This pro-
cedure can be iterated to give Lexi and LMi until
convergence.
3.2 Character Posterior Probability and
Character-based Confusion Network
(CCN)
Consider a word W as shown in Figure 2 with
characters {c1c2c3} corresponding to the edge e
starting at time ? and ending at time t in a word
lattice. During decoding the boundaries between c1
756
Figure 2: An edge e of word W composed of char-
acters c1c2c3 starting at time ? and ending at time
t.
and c2, and c2 and c3 are recorded respectively as t1
and t2. The posterior probability (PP) of the edge e
given the acoustic features A, P (e|A), is (Wessel
et al, 2001):
P (e|A) =
?(?) ? P (xt? |W ) ? PLM (W ) ? ?(t)
?start
,
(1)
where ?(?) and ?(t) denote the forward and back-
ward probability masses accumulated up to time ?
and t obtained by the standard forward-backward
algorithm, P (xt? |W ) is the acoustic likelihood
function, PLM (W ) the language model score, and
?start the sum of all path scores in the lattice. Equa-
tion (1) can be extended to the PP of a character of
W , say c1 with edge e1:
P (e1|A) =
?(?) ? P (xt1? |c1) ? PLM (c1) ? ?(t1)
?start
.
(2)
Here we need two new probabilities, PLM (c1)
and ?(t1). Since neither is easy to estimate, we
make some approximations. First, we assume
PLM (c1) ? PLM (W ). Of course this is not true,
the actual relation being PLM (c1) ? PLM (W ),
since the set of events having c1 given its his-
tory includes a set of events having W given the
same history. We used the above approximation
for easier implementation. Second, we assume
that after c1 there is only one path from t1 to
t: through c2 and c3. This is more reasonable
since we restrain the hypotheses space to be in-
side the word lattice, and pruned paths are sim-
ply neglected. With this approximation we have
?(t1) = P (xtt1 |c2c3) ? ?(t). Substituting these
two approximate values for PLM (c1) and ?(t1) in
Equation (2), the result turns out to be very sim-
ple: P (e1|A) ? P (e|A). With similar assump-
tions for the character edges e2 and e3, we have
P (e2|A) ? P (e3|A) ? P (e|A). Similar results
were obtained by Yao et al (2008) from a different
point of view.
The result that P (ei|A) ? P (e|A) seems to
diverge from the intuition: approximating an
n-segment word by splitting the probability of
the entire edge over the segments ? P (ei|A) ?
n
?
P (e|A). The basic meaning of Equation (1) is
to calculate the ratio of the paths going through a
specific edge divided by the total paths while each
path is weighted properly. Of course the paths go-
ing through a sub-edge ei should be definitely more
than the paths through the corresponding full-edge
e. As a result, P (ei|A) should usually be greater
than P (e|A), as implied by the intuition. However,
the inter-connectivity between all sub-edges and
the proper weights of them are not easy to be han-
dled well. Here we constrain the inter-connectivity
of sub-edges to be only inside its own word edge
and also simplify the calculation of the weights
of paths. This offers a tractable solution and the
performance is quite acceptable.
After we obtain the PPs for each character arc
in the lattice, such as P (ei|A) as mentioned above,
we can perform the same clustering method pro-
posed by Mangu et al (2000) to convert the word
lattice to a strict linear sequence of clusters, each
consisting of a set of alternatives of character hy-
potheses, or a character-based confusion network
(CCN) (Fu et al, 2006; Qian et al, 2008). In CCN
we collect the PPs for all character arc c with begin-
ning time ? and end time t as P ([c; ?, t]|A) (based
on the above mentioned approximation):
P ([c; ?, t]|A) =
?
H = w1 . . . wN ? lattice :
?i ? {1 . . . N} :
wi contains [c; ?, t]
P (H)P (A|H)
?
pathH? ? lattice
P (H ?)P (A|H ?)
,
(3)
whereH stands for a path in the word lattice. P (H)
is the language model score of H (after proper scal-
ing) and P (A|H) is the acoustic model score. CCN
was known to be very helpful in reducing character
error rate (CER) since it minimizes the expected
CER (Fu et al, 2006; Qian et al, 2008). Given
a CCN, we simply choose the characters with the
highest PP from each cluster as the recognition
results.
3.3 Lexicon Adaptation with Improved
Character Accuracy (LAICA)
In Figure 3 we show a piece of a character-based
confusion network (CCN) aligned with the corre-
sponding manual transcription characters. Such
alignment can be implemented by an efficient dy-
namic programming method. The CCN is com-
posed of several strict linear ordering clusters of
757
R m-1
R m
Refe
renc
e 
Char
acter
s ?
R m+1
R m+2
R m+3
n ||
o ||
p ||
q ||
r ||
?
Char
acter
-bas
ed 
Con
fusio
n Ne
twor
k 
(CC
N)
?
?
n
s
t
u
?
?
??
??.C
align
(m)
C alig
n(m+
2)
C alig
n(m+
3)
o
q
?
??..
?
?
?
?
C alig
n(m-
1)
C alig
n(m+
1)a
lign(
m+2
)
p
R m: 
char
acter
 vari
able
 at th
e mt
h po
sitio
n in 
the r
efere
nce 
char
acter
s
m
p
C alig
n(m)
: a c
luste
r of 
CCN
 alig
ned 
with
 the 
mth c
hara
cter 
in th
e ref
eren
ce
n~u:
 sym
bols
 for 
Chin
ese c
hara
cters
Figure 3: A character-based confusion network
(CCN) and corresponding reference manual tran-
scription characters.
character alternatives. In the figure, Calign(m)
is a specific cluster aligned with the mth char-
acter in the reference, which contains characters
{s . . . o . . .} (The alphabets n, o . . . u are symbols
for specific Chinese characters) . The characters in
each cluster of CCN are well sorted according to
the PP, and in each cluster a special null character
 with its PP being equal to 1 minus the summation
of PPs for all character hypotheses in that cluster.
The clusters with  ranked first are neglected in the
alignment.
After the alignment, there are only three pos-
sibilities corresponding to each reference charac-
ter. (1) The reference character is ranked first in
the corresponding cluster (Rm?1 and the cluster
Calign(m?1)). In this case the reference charac-
ter can be correctly recognized. (2) The refer-
ence character is included in the corresponding
cluster but not ranked first. ([Rm . . . Rm+2] and
{Calign(m), . . . , Calign(m+2)}) (3) The reference
character is not included in the corresponding clus-
ter (Rm+3 and Calign(m+3)). For cases (2) and (3),
the reference character will be incorrectly recog-
nized.
The basic idea of the proposed lexicon adapta-
tion with an improved character accuracy (LAICA)
approach is to enhance the PPs of those incorrectly
recognized characters by adding new words and
deleting existing words in the lexicon. Here we
only focus on those characters of case (2) men-
tioned above. This is primarily motivated by the
minimum classification error (MCE) discriminative
training approach proposed by Juang et al (1997),
where a sigmoid function was used to suppress the
impacts of those perfectly and very poorly recog-
nized training samples. In our approach, the case
(1) is the perfect case and case (3) is the very poor
one. Another motivation is that for characters in
case (1), since they are already correctly recognized
we do not try to enhance their PPs.
The procedure of LAICA then becomes simple.
Among the aligned reference characters and clus-
ters of CCN, case (1) and (3) are anchors. The
reference characters between two anchors then be-
come our focus segment and their PPs should be en-
hanced. By investigating Equation (3), to enhance
the PP of a specific character we can adjust the
language model (P (H)), and the acoustic model
(P (A|H)), or we can simply modify the lexicon
(the constraint under summation). We should add
new words to cover the characters of case (2) to
enlarge the numerator of Equation (3) and at the
same time delete some existing words to suppress
the denominator.
In Figure 3, reference characters
[RmRm+1Rm+2=opq] and the clusters
{Calign(m), . . . , Calign(m+2)} show an exam-
ple of our focus segment. For each such segment,
we at most add one new word and delete an
existing word. From the string [opq] we choose
the longest OOV part from it as a new word.
To select a word to be deleted, we choose the
longest in-vocabulary (IV) part from the top
ranked competitors of [opq], which are then [stu]
in clusters {Calign(m), . . . , Calign(m+2)}. This is
also motivated by MCE that we only suppress the
strongest competitors? probabilities. Note that we
do not delete single-characters in the procedure.
The ?at most one? constraint here is motivated
by previous language model adaptation works (Fed-
erico, 1999) which usually try to introduce new ev-
idences in the adaptation corpus but with the least
modification of the original model. Of course the
modification of language models led by the addi-
tion and deletion of words is hard to quantify and
we choose to add and delete as fewer words as pos-
sible, which is just a simple heuristic. On the other
hand, adding fewer words means that longer words
are added. It has been shown that longer words are
more helpful for ASR (Gao et al, 2004; Saon and
Padmanabhan, 2001).
The proposed LAICA approach can be regarded
as a discriminative one since it not only considers
the reference characters but also those wrongly rec-
ognized characters. This can be beneficial since it
reduces potential ambiguities existing in the lexi-
con.
758
The Expectation-Maximization algorithm
1. Bootstrap initial word segmentation by
maximum-matching algorithm
(Wong and Chan, 1996)
2. Estimate unigram LM
3. Expectation: Re-segment according
to the unigram LM
4. Maximization: Estimate the n-gram LM
5. Expectation: Re-segment according to
the n-gram LM
6. Go to step 4 until convergence
Table 2: EM algorithm for word segmentation and
LM estimation
3.4 Word Segmentation and Language
Model Training
If we regard the word segmentation process as a
hidden variable, then we can apply EM algorithm
(Dempster et al, 1977) to train the underlying n-
gram language model. The procedure is described
in Table 2. In the algorithm we can see two ex-
pectation phases. This is natural since at the be-
ginning the bootstrap segmentation can not give
reliable statistics for higher order n-gram and we
choose to only use the unigram marginal probabili-
ties. The procedure was well established by Hwang
et al (2006).
Actually, the EM algorithm proposed here is sim-
ilar to the n-multigram model training procedure
proposed by Deligne and Sagisaka (2000). The role
of multigrams can be regarded as the words here,
except that multigrams begin from scratch while
here we have an initial lexicon and use maximum-
matching algorithm to offer an acceptable initial
unigram probability distributions. If the initial lex-
icon is not available, the procedure proposed by
Deligne and Sagisaka (2000) is preferred.
4 Experimental Results
4.1 Baseline Lexicon, Corpora and Language
Models
The baseline lexicon was automatically constructed
from a 300 MB Chinese news text corpus ranging
from 1997 to 1999 using the widely applied PAT-
tree-based word extraction method (Chien, 1997).
It includes 61521 words in total, of which 5856
are single-characters. The key principles of the
PAT-tree-based approach to extract a sequence of
characters as a word are: (1) high enough frequency
count; (2) high enough mutual information between
component characters; (3) large enough number of
context variations on both sides; (4) not dominated
by the most frequent context among all context
variations. In general the words extracted have high
frequencies and clear boundaries, thus very often
they have good semantic meanings. Since all the
above statistics of all possible character sequences
in a raw corpus are combinatorially too many, we
need an efficient data structure such as the PAT-tree
to record and access all such information.
With the baseline lexicon, we performed the EM
algorithm as in Table 2 to train the trigram LM.
Here we used a 313 MB LM training corpus, which
contains text news articles in 2000 and 2001. Note
that in the following Sections, the pronunciations
of the added words were automatically labeled by
exhaustively generating all possible pronunciations
from all component characters? canonical pronun-
ciations.
4.2 ASR Character Accuracy Results
A set of broadcast news corpus collected from a
Chinese radio station from January to September,
2001 was used as the speech corpus. It contained
10K utterances. We separated these utterances into
two parts randomly: 5K as the adaptation corpus
and 5K as the testing set. We show the ASR char-
acter accuracy results after lexicon adaptation by
the proposed approach in Table 3.
LAICA-1 LAICA-2
A D A+D A D A+D
Baseline +1743 -1679 +1743 +409 -112 +314
-1679 -88
79.28 80.48 79.31 80.98 80.58 79.33 81.21
Table 3: ASR character accuracies for the baseline
and the proposed LAICA approach. Two iterations
are performed, each with three versions. A: only
add new words, D: only delete words and A+D: si-
multaneously add and delete words. + and - means
the number of words added and deleted, respec-
tively.
For the proposed LAICA approach, we show
the results for one (LAICA-1) and two (LAICA-
2) iterations respectively, each of which has three
different versions: (A) only add new words into
the current lexicon, (D) only delete words, (A+D)
simultaneously add and delete words. The num-
ber of added or deleted words are also included in
Table 3.
There are some interesting observations. First,
we see that deletion of current words brought much
759
less benefits than adding new words. We try to give
some explanations. Deleting existing words in the
lexicon actually is a passive assistance for recog-
nizing reference characters correctly. Of course
we eliminate some strong competitive characters
in this way but we can not guarantee that refer-
ence characters will then have high enough PP
to be ranked first in its own cluster. Adding new
words into the lexicon, on the other hand, offers
explicit reinforcement in PP of the reference char-
acters. Such reinforcement offers the main positive
boosting for the PP of reference characters. These
boosted characters are under some specific con-
texts which normally correspond to OOV words
and sometimes in-vocabulary (IV) words that are
hard to be recognized.
From the model training aspect, adding new
words gives the maximum-likelihood flavor while
deleting existing words provides discriminant abil-
ity. It has been shown that discriminative train-
ing does not necessarily outperform maximum-
likelihood training until we have enough training
data (Ng and Jordan, 2001). So it is possible that
discriminatively trained model performs worse than
that trained by maximum likelihood. In our case,
adding and deleting words seem to compliment
each other well. This is an encouraging result.
Another good property is that the proposed ap-
proach converged quickly. The number of words to
be added or deleted dropped significantly in the sec-
ond iteration, compared to the first one. Generally
the fewer words to be changed the fewer recogni-
tion improvement can be expected. Actually we
have tried the third iteration and simply obtained
dozens of words to be added and no words to be
deleted, which resulted in negligible changes in
ASR recognition accuracy.
4.3 Comparison with other Lexicon
Adaptation Methods
In this section we compare our method with two
other traditionally used approaches: one is the PAT-
tree-based as introduced in Section 4.1 and the
other is based on mutual probability (Saon and Pad-
manabhan, 2001), which is the geometrical average
of the direct and reverse bigram:
PM (wi, wj) =
?
Pf (wj |wi)Pr(wi|wj),
where the direct (Pf (?) and reverse bigram (Pr(?))
can be estimated as:
Pf (wj |wi) =
P (Wt+1 = wj ,Wt = wi)
P (Wt = wi)
,
Pr(wj |wi) =
P (Wt+1 = wj ,Wt = wi)
P (Wt+1 = wj)
.
PM (wi, wj) is used as a measure about whether to
combine wi and wj as a new word. By properly
setting a threshold, we may iteratively combine
existing characters and/or words to produce the re-
quired number of new words. For both the PAT-tree-
and mutual-information-based approaches, we use
the manual transcriptions of the development 5K
utterances to collect the required statistics and we
extract 2159 and 2078 words respectively to match
the number of added words by the proposed LAICA
approach after 2 iterations (without word deletion).
The language model is also re-trained as described
in Section 3.4. The results are shown in Table 4,
where we also include the results of our approach
with 2 iterations and adding words only for refer-
ence.
PAT-
tree
Mutual
Probability LAICA-2(A)
Character
Accuracy 79.33 80.11 80.58
Table 4: ASR character accuracies on the lexicon
adapted by different approaches.
From the results we observe that the PAT-tree-
based approach did not give satisfying improve-
ments while the mutual probability-based one
worked well. This may be due to the sparse adap-
tation data, which includes only 81K characters.
PAT-tree-based approach relies on the frequency
count, and some terms which occur only once in
the adaptation data will not be extracted. Mutual
probability-based approach, on the other hand, con-
siders two simple criterion: the components of a
new word occur often together and rarely in con-
junction with other words (Saon and Padmanabhan,
2001). Compared with the proposed approach, both
PAT-tree and mutual probability do not consider the
decoding structure.
Some new words are clearly good for human
sense and definitely convey novel semantic infor-
mation, but they can be useless for speech recogni-
tion. That is, character n-gram may handle these
words equally well due to the low ambiguities with
other words. The proposed LAICA approach tries
to focus on those new words which can not be han-
dled well by simple character n-grams. Moreover,
the two methods discussed here do not offer pos-
sible ways to delete current words, which can be
considered as a further advantage of the proposed
LAICA approach.
760
4.4 Application: Character-based Spoken
Document Indexing and Retrieval
Pan et al (2007) recently proposed a new Subword-
based Confusion Network (S-CN) indexing struc-
ture for SDR, which significantly outperforms
word-based methods for IV or OOV queries. Here
we apply S-CN structure to investigate the effec-
tiveness of improved character accuracy for SDR.
Here we choose characters as the subword units,
and then the S-CN structure is exactly the same as
CCN, which was introduced in Section 3.2.
For the SDR back-end corpus, the same 5K test
utterances as used for the ASR experiment in Sec-
tion 4.2 were used. The previously mentioned lexi-
con adaptation approaches and corresponding lan-
guage models were used in the same speech recog-
nizer for the spoken document indexing. We auto-
matically choose 139 words and terms as queries
according to the frequency (at least six times in the
5K utterances). The SDR performance is evaluated
by mean average precision (MAP) calculated by
the trec eval1 package. The results are shown
in Table 5.
Character
Accuracy MAP
Baseline 79.28 0.8145
PAT-tree 79.33 0.8203
Mutual
Probability 80.11 0.8378
LAICA-2(A+D) 81.21 0.8628
Table 5: ASR character accuracies and SDR MAP
performances under S-CN structure.
From the results, we see that generally the
increasing of character recognition accuracy im-
proves the SDR MAP performance. This seems
trivial but we have to note the relative improve-
ments. Actually the transformation ratios from the
relative increased character accuracy to the relative
increased MAP for the three lexicon adaptation ap-
proaches are different. A key factor making the
proposed LAICA approach advantageous is that
we try to extensively raise the incorrectly recog-
nized character posterior probabilities, by means
of adding effective OOV words and deleting am-
biguous words. Actually S-CN is relying on the
character posterior probability for indexing, which
is consistent with our criterion and makes our ap-
proach beneficial. The degree of the raise of char-
acter posterior probabilities can be visualized more
clearly in the following experiment.
1http://trec.nist.gov/
4.5 Further Investigation: the Improved
Rank in Character-based Confusion
Networks
In this experiment, we have the same setup as in
Section 4.2. After decoding, we have character-
based confusion networks (CCNs) for each test
utterance. Rather than taking the top ranked char-
acters in each cluster as the recognition result, we
investigate the ranks of the reference characters in
these clusters. This can be achieved by the same
alignment as we did in Section 3.3. The results are
shown in Table 6.
# of ranked
reference
characters
Average
Rank
baseline 70993 1.92
PAT-tree 71038 1.89
Mutual
Probability
71054 1.81
LAICA-2(A+D) 71083 1.67
Table 6: Average ranks of reference characters in
the confusion networks constructed by different
lexicons and corresponding language models
In Table 6 we only evaluate ranks on those ref-
erence characters that can be found in its corre-
sponding confusion network cluster (case (1) and
(2) as described in Section 3.3). The number of
those evaluated reference characters depends on
the actual CCN and is also included in the results.
Generally, over 93% of reference characters are in-
cluded (the total number is 75541). Such ranks are
critical for lattice-based spoken document indexing
approaches such as S-CN since they directly affect
retrieval precision. The advantage of the proposed
LAICA approach is clear. The results here provide
a more objective point of view since SDR evalua-
tion is inevitably effected by the selected queries.
5 Conclusion and Future Work
Characters together is an interesting and distinct
language unit for Chinese. They can be simultane-
ously viewed as words and subwords, which offer
a special means for OOV handling. While relying
only on characters gives moderate performances in
ASR, properly augmenting new words significantly
increases the accuracy. An interesting question
would then be how to choose words to augment.
Here we formulate the problem as an adaptation
one and try to find the best way to alter the current
761
lexicon for improved character accuracy.
This is a new perspective for lexicon adaptation.
Instead of identifying OOV words from adaptation
corpus to reduce OOV rate, we try to pick out word
fragments hidden in the adaptation corpus that help
ASR. Furthermore, we delete some existing words
which may result in ambiguities. Since we directly
match our criterion with that in decoding, the pro-
posed approach is expected to have more consistent
improvements than perplexity based criterions.
Characters also play an important role in spoken
document retrieval. This extends the applicability
of the proposed approach and we found that the
S-CN structure proposed by Pan et al for spoken
document indexing fitted well with the proposed
LAICA approach.
However, there still remain lots to be improved.
For example, considering Equation 3, the language
model score and the summation constraint are not
independent. After we alter the lexicon, the LM is
different accordingly and there is no guarantee that
the obtained posterior probabilities for those incor-
rectly recognized characters would be increased.
We increased the path alternatives for those refer-
ence characters but this can not guarantee to in-
crease total path probability mass. This can be
amended by involving the discriminative language
model adaptation in the iteration, which results in
a unified language model and lexicon adaptation
framework. This can be our future work. Moreover,
the same procedure can be used in the construction.
That is, beginning with only characters in the lexi-
con and using the training data to alter the current
lexicon in each iteration. This is also an interesting
direction.
References
Maximilian Bisani and Hermann Ney. 2005. Open vo-
cabulary speech recognition with flat hybrid models.
In Interspeech, pages 725?728.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown
word extraction for chinese documents. In COLING,
pages 169?175.
Berlin Chen, Jen-Wei Kuo, and Wen-Hung Tsai. 2004.
Lightly supervised and data-driven approaches to
mandarin broadcast news transcription. In ICASSP,
pages 777?780.
Lee-Feng Chien. 1997. Pat-tree-based keyword ex-
traction for Chinese information retrieval. In SIGIR,
pages 50?58.
Sabine Deligne and Yoshinori Sagisaka. 2000. Sta-
tistical language modeling with a class-based n-
multigram model. Comp. Speech and Lang.,
14(3):261?279.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistics Soci-
ety, 39(1):1?38.
Marcello Federico and Nicola Bertoldi. 2004. Broad-
cast news LM adaptation over time. Comp. Speech
Lang., 18:417?435.
Marcello Federico. 1999. Efficient language model
adaptation through MDI estimation. In Intersspech,
pages 1583?1586.
Yi-Sheng Fu, Yi-Cheng Pan, and Lin-Shan Lee.
2006. Improved large vocabulary continuous Chi-
nese speech recognition by character-based consen-
sus networks. In ISCSLP, pages 422?434.
Pascale Fung. 1998. Extracting key terms from chi-
nese and japanese texts. Computer Processing of
Oriental Languages, 12(1):99?121.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statis-
tical language modeling for Chinese. ACM Trans-
action on Asian Language Information Processing,
1(1):3?33.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2004. Chinese word segmentation: A prag-
matic approach. In MSR-TR-2004-123.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen.
2005. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Comp. Speech Lang.
Mei-Yuh Hwang, Xin Lei, Wen Wang, and Takahiro
Shinozaki. 2006. Investigation on mandarin
broadcast news speech recognition. In Interspeech-
ICSLP, pages 1233?1236.
Bing-Hwang Juang, Wu Chou, and Chin-Hui Lee.
1997. Minimum classification error rate methods for
speech recognition. IEEE Trans. Speech Audio Pro-
cess., 5(3):257?265.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word er-
ror minimization and other applications of confusion
networks. Comp. Speech Lang., 14(2):373?400.
Andrew Y. Ng and Michael I. Jordan. 2001. On
discriminative vs. generative classifiers: A compar-
ison of logistic regression and naive bayes. In Ad-
vances in Neural Information Processing Systems
(14), pages 841?848.
762
Yi-Cheng Pan, Hung-Lin Chang, and Lin-Shan Lee.
2007. Analytical comparison between position spe-
cific posterior lattices and confusion networks based
on words and subword units for spoken document
indexing. In ASRU.
Yao Qian, Frank K. Soong, and Tan Lee. 2008. Tone-
enhanced generalized character posterior probabil-
ity (GCPP) for Cantonese LVCSR. Comp. Speech
Lang., 22(4):360?373.
Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here?
Proceeding of IEEE, 88(8):1270?1278.
George Saon and Mukund Padmanabhan. 2001. Data-
driven approach to designing compound words for
continuous speech recognition. IEEE Trans. Speech
and Audio Process., 9(4):327?332, May.
Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and Her-
mann Ney. 2001. Confidence measures for large
vocabulary continuous speech recognition. IEEE
Trans. Speech Audio Process., 9(3):288?298, Mar.
Pak-kwong Wong and Chorkin Chan. 1996. Chinese
word segmentation based on maximum matching
and word binding force. In Proceedings of the 16th
International Conference on Computational Linguis-
tic, pages 200?203.
Kae-Cherng Yang, Tai-Hsuan Ho, Lee-Feng Chien, and
Lin-Shan Lee. 1998. Statistics-based segment pat-
tern lexicon: A new direction for chinese language
modeling. In ICASSP, pages 169?172.
763
