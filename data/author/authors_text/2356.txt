Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 787?794, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Comparing and Combining Finite-State and Context-Free Parsers
Kristy Hollingshead and Seeger Fisher and Brian Roark
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton, Oregon, 97006
{hollingk,fishers,roark}@cslu.ogi.edu
Abstract
In this paper, we look at comparing high-
accuracy context-free parsers with high-
accuracy finite-state (shallow) parsers on
several shallow parsing tasks. We
show that previously reported compar-
isons greatly under-estimated the perfor-
mance of context-free parsers for these
tasks. We also demonstrate that context-
free parsers can train effectively on rel-
atively little training data, and are more
robust to domain shift for shallow pars-
ing tasks than has been previously re-
ported. Finally, we establish that combin-
ing the output of context-free and finite-
state parsers gives much higher results
than the previous-best published results,
on several common tasks. While the
efficiency benefit of finite-state models
is inarguable, the results presented here
show that the corresponding cost in accu-
racy is higher than previously thought.
1 Introduction
Finite-state parsing (also called chunking or shallow
parsing) has typically been motivated as a fast first-
pass for ? or approximation to ? more expensive
context-free parsing (Abney, 1991; Ramshaw and
Marcus, 1995; Abney, 1996). For many very-large-
scale natural language processing tasks (e.g. open-
domain question answering from the web), context-
free parsing may be too expensive, whereas finite-
state parsing is many orders of magnitude faster and
can also provide very useful syntactic annotations
for large amounts of text. For this reason, finite-state
parsing (hereafter referred to as shallow parsing) has
received increasing attention in recent years.
In addition to the clear efficiency benefit of
shallow parsing, Li and Roth (2001) have further
claimed both an accuracy and a robustness benefit
versus context-free parsing. The output of a context-
free parser, such as that of Collins (1997) or Char-
niak (2000), can be transformed into a sequence of
shallow constituents for comparison with the output
of a shallow parser. Li and Roth demonstrated that
their shallow parser, trained to label shallow con-
stituents along the lines of the well-known CoNLL-
2000 task (Sang and Buchholz, 2000), outperformed
the Collins parser in correctly identifying these con-
stituents in the Penn Wall Street Journal (WSJ) Tree-
bank (Marcus et al, 1993). They argued that their
superior performance was due to optimizing directly
for the local sequence labeling objective, rather than
for obtaining a hierarchical analysis over the entire
string. They further showed that their shallow parser
trained on the Penn WSJ Treebank did a far better
job of annotating out-of-domain sentences (e.g. con-
versational speech) than the Collins parser.
This paper re-examines the comparison of shal-
low parsers with context-free parsers, beginning
with a critical examination of how their outputs
are compared. We demonstrate that changes to the
conversion routine, which take into account differ-
ences between the original treebank trees and the
trees output by context-free parsers, eliminate the
previously-reported accuracy differences. Second,
we show that a convention that is widely accepted
for evaluation of context-free parses ? ignoring
punctuation when setting the span of a constituent ?
results in improved shallow parsing performance by
certain context-free parsers across a variety of shal-
low parsing tasks. We also demonstrate that context-
free parsers perform competitively when applied to
out-of-domain data. Finally, we show that large im-
provements can be obtained in several shallow pars-
ing tasks by using simple strategies to incorporate
context-free parser output into shallow parsing mod-
els. Our results demonstrate that a rich context-free
787
parsing model is, time permitting, worth applying,
even if only shallow parsing output is needed. In
addition, our best results, which greatly improve on
the previous-best published results on several tasks,
shed light on how much accuracy is sacrificed in
shallow parsing to get finite-state efficiency.
2 Evaluating Heterogeneous Parser Output
Two commonly reported shallow parsing tasks are
Noun-Phrase (NP) Chunking (Ramshaw and Mar-
cus, 1995) and the CoNLL-2000 Chunking task
(Sang and Buchholz, 2000), which extends the NP-
Chunking task to recognition of 11 phrase types1
annotated in the Penn Treebank. Reference shal-
low parses for this latter task were derived from
treebank trees via a conversion script known as
chunklink2. We follow Li and Roth (2001) in
using chunklink to also convert trees output by a
context-free parser into a flat representation of shal-
low constituents. Figure 1(a) shows a Penn Tree-
bank tree and Figure 1(c) its corresponding shallow
parse constituents, according to the CoNLL-2000
guidelines. Note that consecutive verb phrase (VP)
nodes result in a single VP shallow constituent.
Just as the original treebank trees are converted
for training shallow parsers, they are also typ-
ically modified for training context-free parsers.
This modification includes removal of empty nodes
(nodes tagged with ?-NONE-? in the treebank), and
removal of function tags on non-terminals; e.g., NP-
SBJ (subject NP) and NP-TMP (temporal NP) are
both mapped to NP. The output of the context-free
parser is, of course, in the same format as the train-
ing input, so empty nodes and function tags are not
present. This type of modified tree is what is shown
in Figure 1(b); note that the original treebank tree,
shown in Figure 1(a), had an empty subject NP in
the embedded clause which has been removed for
the modified tree.
To compare the output of their shallow parser with
the output of the well-known Collins (1997) parser,
Li and Roth applied the chunklink conversion
script to extract the shallow constituents from the
output of the Collins parser on WSJ section 00. Un-
1These include: ADJP, ADVP, CONJP, INTJ, LST, NP, PP,
PRT, SBAR, UCP and VP. Anything not in one of these base
phrases is designated as ?outside?.
2Downloaded from http://ilk.kub.nl/?sabine/chunklink/.
(a) S


H
HH
NP-SBJ-1
They
VP
 HH
are VP
 HH
starting S


HH
H
NP-SBJ
-NONE-
*-1
VP


H
HH
to VP


HH
H
buy NP
 HH
growth stocks
(b) S


HH
H
NP
They
VP


HH
H
are VP


HH
H
starting S
VP


HH
H
to VP


H
HH
buy NP
 HH
growth stocks
(c) [NP They] [VP are starting to buy] [NP growth stocks]
Figure 1: (a) Penn WSJ treebank tree, (b) modified treebank
tree, and (c) CoNLL-2000 style shallow bracketing, all of the
same string.
fortunately, the script was built to be applied to the
original treebank trees, complete with empty nodes,
which are not present in the output of the Collins
parser, or any well-known context-free parser. The
chunklink script searches for empty nodes in the
parse tree to perform some of its operations. In par-
ticular, any S node that contains an empty subject
NP and a VP is reduced to just a VP node, and
then combined with any immediately-preceding VP
nodes to create a single VP constituent. If the S
node does not contain an empty subject NP, as in
Figure 1(b), the chunklink script creates two VP
constituents: [VP are starting] [VP to buy], which
in this case results in a bracketing error. However,
it is a simple matter to insert an empty subject NP
into unary S?VP productions so that these nodes
are processed correctly by the script.
Various conventions have become standard in
evaluating parser output over the past decade. Per-
haps the most widely accepted convention is that
of ignoring punctuation for the purposes of assign-
ing constituent span, under the perspective that, fun-
788
Phrase Evaluation Scenario
System Type (a) (b) (c)
?Modified? All 98.37 99.72 99.72
Truth VP 92.14 98.70 98.70
Li and Roth All 94.64 - -
(2001) VP 95.28 - -
Collins (1997) All 92.16 93.42 94.28
VP 88.15 94.31 94.42
Charniak All 93.88 95.15 95.32
(2000) VP 88.92 95.11 95.19
Table 1: F-measure shallow bracketing accuracy under three
different evaluation scenarios: (a) baseline, used in Li and Roth
(2001), with original chunklink script converting treebank
trees and context-free parser output; (b) same as (a), except that
empty subject NPs are inserted into every unary S?VP produc-
tion; and (c) same as (b), except that punctuation is ignored for
setting constituent span. Results for Li and Roth are reported
from their paper. The Collins parser is provided with part-of-
speech tags output by the Brill tagger (Brill, 1995).
damentally, constituents are groupings of words.
Interestingly, this convention was not followed in
the CoNLL-2000 task (Sang and Buchholz, 2000),
which as we will see has a variable effect on context-
free parsers, presumably depending on the degree to
which punctuation is moved in training.
2.1 Evaluation Analysis
To determine the effects of the conversion routine
and different evaluation conventions, we compare
the performance of several different models on one
of the tasks presented in Li and Roth (2001). For
this task, which we label the Li & Roth task, sec-
tions 2-21 of the Penn WSJ Treebank are used as
training data, section 24 is held out, and section 00
is for evaluation.
For all trials in this paper, we report F-measure
labeled bracketing accuracy, which is the harmonic
mean of the labeled precision (P ) and labeled recall
(R), as they are defined in the widely used PARSE-
VAL metrics; i.e. the F-measure accuracy is 2PRP+R .
Table 1 shows baseline results for the Li and
Roth3 shallow parser, two well-known, high-
accuracy context-free parsers, and the reference
(true) parses after being modified as described
3We were unable to obtain the exact model used in Li and
Roth (2001), and so we use their reported results here. Note
that they used reference part-of-speech (POS) tags for their re-
sults on this task. All other results reported in this paper, unless
otherwise noted, were obtained using Brill-tagger POS tags.
above (by removing empty nodes and function
tags). Evaluation scenario (a) in Table 1 corre-
sponds to what was used in Li and Roth (2001) fol-
lowing CoNLL-2000 guidelines, with the original
chunklink script used to transform the context-
free parser output into shallow constituents. We
can see from the performance of the modified truth
in this scenario that there are serious problems
with this conversion, due to the way in which
it handles unary S?VP productions. If we de-
terministically insert empty subject NP nodes for
all such unary productions prior to the use of the
chunklink script, which we do in evaluation sce-
nario (b) of Table 1, this repairs the bulk of the
errors. Some small number of errors remain, due
largely to the fact that if the S node has been an-
notated with a function tag (e.g. S-PRP, S-PRD, S-
CLR), then chunklink will not perform its re-
duction operation on that node. However, for our
purposes, this insertion repair sufficiently corrects
the error to perform meaningful comparisons. Fi-
nally, evaluation scenario (c) follows the context-
free parsing evaluation convention of ignoring punc-
tuation when assigning constituent span. This af-
fects some parsers more than others, depending on
how the parser treats punctuation internally; for
example, Bikel (2004) documents that the Collins
parser raises punctuation nodes within the parse
tree. Since ignoring punctuation cannot hurt perfor-
mance, only improve it, even the smallest of these
differences are statistically significant.
Note that after inserting empty nodes and ignor-
ing punctuation, the accuracy advantage of Li and
Roth over Collins is reduced to a dead heat. Of
the two parsers we evaluated, the Charniak (2000)
parser gave the best performance, which is consis-
tent with its higher reported performance on the
context-free parsing task versus other context-free
parsers. Collins (2000) reported a reranking model
that improved his parser output to roughly the same
level of accuracy as Charniak (2000), and Charniak
and Johnson (2005) report an improvement using
reranking over Charniak (2000). For the purposes
of this paper, we needed an available parser that
was (a) trainable on different subsets of the data to
be applied to various tasks; and (b) capable of pro-
ducing n-best candidates, for potential combination
with a shallow parser. Both the Bikel (2004) imple-
789
System NP-Chunking CoNLL-2000 Li & Roth task
SPRep averaged perceptron 94.21 93.54 95.12
Kudo and Matsumoto (2001) 94.22 93.91 -
Sha and Pereira (2003) CRF 94.38 - -
Voted perceptron 94.09 - -
Zhang et al (2002) - 94.17 -
Li and Roth (2001) - 93.02 94.64
Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task (Ramshaw and Marcus, 1995); the CoNLL-2000
Chunking task (Sang and Buchholz, 2000); and the Li & Roth task (Li and Roth, 2001), which is the same as CoNLL-2000 but
with more training data and a different test section. The results reported in this table include the best published results on each of
these tasks.
mentation of the Collins parser and the n-best ver-
sion of the Charniak (2000) parser, documented in
Charniak and Johnson (2005), fit the requirements.
Since we observed higher accuracy from the Char-
niak parser, from this point forward we report just
Charniak parser results4.
2.2 Shallow Parser
In addition to the trainable n-best context-free parser
from Charniak (2000), we needed a trainable shal-
low parser to apply to the variety of tasks we were
interested in investigating. To this end, we repli-
cated the NP-chunker described in Sha and Pereira
(2003) and trained it as either an NP-chunker or with
the tagset extended to classify all 11 phrase types
included in the CoNLL-2000 task (Sang and Buch-
holz, 2000). Our shallow parser uses exactly the fea-
ture set delineated by Sha and Pereira, and performs
the decoding process using a Viterbi search with a
second-order Markov assumption as they described.
These features include unigram and bigram words
up to two positions to either side of the current word;
unigram, bigram, and trigram part-of-speech (POS)
tags up to two positions to either side of the current
word; and unigram, bigram, and trigram shallow
constituent tags. We use the averaged perceptron al-
gorithm, as presented in Collins (2002), to train the
parser. See (Sha and Pereira, 2003) for more details
on this approach.
To demonstrate the competitiveness of our base-
line shallow parser, which we label the SPRep av-
eraged perceptron, Table 2 shows results on three
different shallow parsing tasks. The NP-Chunking
4The parser is available for research purposes at
ftp://ftp.cs.brown.edu/pub/nlparser/ and can be run in n-
best mode. The one-best performance of the parser is the same
as what was presented in Charniak (2000).
task, originally introduced in Ramshaw and Marcus
(1995) and also described in (Collins, 2002; Sha and
Pereira, 2003), brackets just base NP constituents5.
The CoNLL-2000 task, introduced as a shared task
at the CoNLL workshop in 2000 (Sang and Buch-
holz, 2000), extends the NP-Chunking task to label
11 different base phrase constituents. For both of
these tasks, the training set was sections 15-18 of
the Penn WSJ Treebank and the test set was section
20. We follow Collins (2002) and Sha and Pereira
(2003) in using section 21 as a heldout set. The third
task, introduced by Li and Roth (2001), performs the
same labeling as in the CoNLL-2000 task, but with
more training data and different testing sets: training
was WSJ sections 2-21 and test was section 00. We
used section 24 as a heldout set; this section is often
used as heldout for training context-free parsers.
Training and testing data for the CoNLL-2000
task is available online6. For the heldout sets for
each of these tasks, as well as for all data sets
needed for the Li & Roth task, reference shallow
parses were generated using the chunklink script
on the original treebank trees. All data was tagged
with the Brill POS tagger (Brill, 1995) after the
chunklink conversion. We verified that using
this method on the original treebank trees in sections
15-18 and 20 generated data that is identical to the
CoNLL-2000 data sets online. Replacing the POS
tags in the input text with Brill POS tags before the
5We follow Sha and Pereira (2003) in deriving the NP con-
stituents from the CoNLL-2000 data sets, by replacing all non-
NP shallow tags with the ?outside? (?O?) tag. They mention
that the resulting shallow parse tags are somewhat different than
those used by Ramshaw and Marcus (1995), but that they found
no significant accuracy differences in training on either set.
6Downloaded from the CoNLL-2000 Shared Task website
http://www.cnts.ua.ac.be/conll2000/chunking/.
790
chunklink conversion results in slightly different
shallow parses.
From Table 2 we can see that our shallow parser
is competitive on all three tasks7. Sha and Pereira
(2003) noted that the difference between their per-
ceptron and CRF results was not significant, and
our performance falls between the two, thus repli-
cating their result within noise. Our performance
falls 0.6 percentage points below the best published
result on the CoNLL-2000 task, and 0.5 percentage
points above the performance by Li and Roth (2001)
on their task. Overall, ours is a competitive approach
for shallow parsing.
3 Experimental Results
3.1 Comparing Finite-State and
Context-Free Parsers
The first two rows of Table 3 present a comparison
between the SPRep shallow parser and the Charniak
(2000) context-free parser detailed in Charniak and
Johnson (2005). We can see that the performance
of the two models is virtually indistinguishable for
all three of these tasks, with or without ignoring of
punctuation. As mentioned earlier, we used the ver-
sion of this parser with improved n-best extraction,
as documented in Charniak and Johnson (2005), al-
though without the reranking of the candidates that
they also report in that paper. For these trials, we
used just the one-best output of that model, which is
the same as in Charniak (2000).
Note that the standard training set for context-free
parsing (sections 2-21) is only used for the Li &
Roth task; for the other two tasks, both the SPRep
and the Charniak parsers were trained on sections
15-18, with section 21 as heldout. This demonstrates
that the context-free parser, even when trained on a
small fraction of the total treebank, is able to learn a
competitive model for this task.
3.2 Combining Finite-State and
Context-Free Parsers
It is likely true that a context-free parser which has
been optimized for global parse accuracy will, on
occasion, lose some shallow parse accuracy to sat-
isfy global structure constraints that do not constrain
7Sha and Pereira (2003) reported the Kudo and Matsumoto
(2001) performance on the NP-Chunking task to be 94.39 and
to be the best reported result on this task. In the cited paper,
however, the result is as reported in our table.
a shallow parser. However, it is also likely true
that these longer distance constraints will on occa-
sion enable the context-free parser to better identify
the shallow constituent structure. In other words,
despite having very similar performance, our shal-
low parser and the Charniak context-free parser are
likely making complementary predictions about the
shallow structure that can be exploited for further
improvements. In this section, we explore two sim-
ple methods for combining the system outputs.
The first combination of the system outputs,
which we call unweighted intersection, is the sim-
plest kind of ?rovered? system, which restricts the
set of shallow parse candidates to the intersection
of the sets output by each system, but does not
combine the scores. Since the Viterbi search of
the SPRep model provides a score for all possi-
ble shallow parses, the intersection of the two sets
is simply the set of shallow-parse sequences in the
50-best candidates output by the Charniak parser.
We then use the SPRep perceptron-model scores to
choose from among just these candidates. We con-
verted the 50-best lists returned by the Charniak
parser into k-best lists of shallow parses by using
chunklink to convert each candidate context-free
parse into a shallow parse. Many of the context-free
parses map to the same shallow parse, so the size of
this list is typically much less than 50, with an aver-
age of around 7. Each of the unique shallow-parse
candidates is given a score by the SPRep percep-
tron, and the best-scoring candidate is selected. Ef-
fectively, we used the Charniak parser?s k-best shal-
low parses to limit the search space for our shallow
parser.
The second combination of the system outputs,
which we call weighted intersection, extends the un-
weighted intersection by including the scores from
the Charniak parser, which are log probabilities.
The score for a shallow parse output by the Char-
niak parser is the log of the sum of the probabili-
ties of all context-free parses mapping to that shal-
low parse. We normalize across all candidates for
a given string, hence these are conditional log prob-
abilities. We multiply these conditional log proba-
bilities by a scaling factor ? before adding them to
the SPRep perceptron score for a particular candi-
date. Again, the best-scoring candidate using this
composite score is selected from among the shallow
791
NP-Chunking CoNLL-2000 Li & Roth task
Punctuation Punctuation Punctuation
System Leave Ignore Leave Ignore Leave Ignore
SPRep averaged perceptron 94.21 94.25 93.54 93.70 95.12 95.27
Charniak (2000) 94.17 94.20 93.77 93.92 95.15 95.32
Unweighted intersection 95.13 95.16 94.52 94.64 95.77 95.92
Weighted intersection 95.57 95.58 95.03 95.16 96.20 96.33
Table 3: F-measure shallow bracketing accuracy on three shallow parsing tasks, for the SPRep perceptron shallow parser, the
Charniak (2000) context-free parser, and for systems combining the SPRep and Charniak system outputs.
parse candidates output by the Charniak parser. We
used the heldout data to empirically estimate an op-
timal scaling factor for the Charniak scores, which
is 15 for all trials reported here. This factor com-
pensates for differences in the dynamic range of the
scores of the two parsers.
Both of these intersections are done at test-time,
i.e. the models are trained independently. To remain
consistent with task-specific training and testing sec-
tion conventions, the individual models were always
trained on the appropriate sections for the given task,
i.e. WSJ sections 15-18 for NP-Chunking and the
CoNLL-2000 tasks, and sections 2-21 for the Li &
Roth task.
Results from these methods of combination are
shown in the bottom two rows of Table 3. Even
the simple unweighted intersection gives quite large
improvements over each of the independent systems
for all three tasks. All of these improvements are
significant at p < 0.001 using the Matched Pair
Sentence Segment test (Gillick and Cox, 1989). The
weighted intersection gives further improvements
over the unweighted intersection for all tasks, and
this improvement is also significant at p < 0.001,
using the same test.
3.3 Robustness to Domain Shift
Our final shallow parsing task was also proposed in
Li and Roth (2001). The purpose of this task was
to examine the degradation in performance when
parsers, trained on one relatively clean domain such
as WSJ, are tested on another, mismatched domain
such as Switchboard. The systems that are reported
in this section are trained on sections 2-21 of the
WSJ Treebank, with section 24 as heldout, and
tested on section 4 of the Switchboard Treebank.
Note that the systems used here are exactly the ones
presented for the original Li & Roth task, in Sec-
Punctuation
System Leave Ignore
Li & Roth (reference tags) 88.47 -
SPRep avg perceptron
Reference tags 91.37 91.86
Brill tags 87.94 88.42
Charniak (2000) 87.94 88.44
Unweighted intersection 88.66 89.16
Weighted intersection 89.22 89.69
Table 4: Shallow bracketing accuracy of several different sys-
tems, trained on sections 2-21 of WSJ Treebank and applied
to section 4 of the Switchboard Treebank. Li and Roth (2001)
results are as reported in their paper, with reference POS tags
rather than Brill-tagger POS tags.
tions 3.1 and 3.2; only the test set has changed, train-
ing and heldout sets remain exactly the same, as do
the mixing parameters for the weighted intersection.
In the trials reported in Li and Roth (2001), both of
the evaluated systems were provided with reference
POS tags from the Switchboard Treebank. In the
current results, we show our SPRep averaged per-
ceptron system provided both with reference POS
tags for comparison with the Li and Roth results,
and provided with Brill-tagger POS tags for com-
parison with other systems. Table 4 shows our re-
sults for this task. Whereas Li and Roth reported
a more marked degradation in performance when
using a context-free parser as compared to a shal-
low parser, we again show virtually indistinguish-
able performance between our SPRep shallow parser
and the Charniak context-free parser. Again, using a
weighted combined model gave us large improve-
ments over each independent model, even in this
mismatched domain.
3.4 Reranked n-best List
Just prior to the publication of this paper, we were
able to obtain the trained reranker from Charniak
792
WSJ Sect. 00 SWBD Sect. 4
Punctuation Punctuation
System Leave Ignore Leave Ignore
SPRep 95.12 95.27 87.94 88.43
C & J one-best 95.15 95.32 87.94 88.44
(2005) reranked 95.81 96.04 88.64 89.17
Weighted intersection 96.32 96.47 89.32 89.80
Table 5: F-measure shallow bracketing accuracy when trained
on WSJ sections 2-21 and applied to either WSJ section 00 or
SWBD section 4. Systems include our shallow parser (SPRep);
the Charniak and Johnson (2005) system (C & J), both initial
one-best and reranked-best; and the weighted intersection be-
tween the reranked 50-best list and the SPRep system.
and Johnson (2005), which allows a comparison of
the shallow parsing gains that they obtain from that
system with those documented here. The reranker is
a discriminatively trained Maximum Entropy model
with an F-measure parsing accuracy objective. It
uses a large number of features, and is applied to the
50-best output from the generative Charniak parsing
model. The reranking model was trained on sections
2-21, with section 24 used as heldout. This allows us
to compare its shallow parsing accuracy with other
systems on the tasks that use this training setup: the
Li & Roth task (testing on WSJ section 00) and the
domain shift task (testing on Switchboard section
4). Table 5 shows two new trials making use of this
reranking model.
The Charniak and Johnson (2005) system out-
put (denoted C & J in the table) before rerank-
ing (denoted one-best) is identical to the Charniak
(2000) results that have been reported in the other
tables. After reranking (denoted reranked), the per-
formance improves by roughly 0.7 percentage points
for both tasks, nearly reaching the performance
that we obtained with weighted intersection of the
SPRep model and the n-best list before reranking.
Weighted intersection between the reranked list and
the shallow parser as described earlier, with a newly
estimated scaling factor (?=30), provides a roughly
0.5 percentage point increase over the result ob-
tained by the reranker. The difference between the
Charniak output before and after reranking is statis-
tically significant at p < 0.001, as is the difference
between the reranked output and the weighted inter-
section, using the same test reported earlier.
3.5 Discussion
While it may be seen to be overkill to apply a
context-free parser for these shallow parsing tasks,
we feel that these results are very interesting for
a couple of reasons. First, they go some way to-
ward correcting the misperception that context-free
parsers are less applicable in real-world scenarios
than finite-state sequence models. Finite-state mod-
els are undeniably more efficient; however, it is
important to have a clear idea of how much ac-
curacy is being sacrificed to reach that efficiency.
Any given application will need to examine the ef-
ficiency/accuracy trade-off with different objectives
for optimality. For those willing to trade efficiency
for accuracy, it is worthwhile knowing that it is pos-
sible to do much better on these tasks than what has
been reported in the past.
4 Conclusion and Future Work
In summary, we have demonstrated in this paper that
there is no accuracy or robustness benefit to shal-
low parsing with finite-state models over using high-
accuracy context-free models. Even more, there is a
large benefit to be had in combining the output of
high-accuracy context-free parsers with the output
of shallow parsers. We have demonstrated a large
improvement over the previous-best reported re-
sults on several tasks, including the well-known NP-
Chunking and CoNLL-2000 shallow parsing tasks.
Part of the misperception of the relative benefits
of finite-state and context-free models is due to dif-
ficulty evaluating across these differing annotation
styles. Mapping from context-free parser output
to the shallow constituents defined in the CoNLL-
2000 task depends on many construction-specific
operations that have unfairly penalized context-free
parsers in previous comparisons.
While the results of combining system outputs
show one benefit of combining systems, as presented
in this paper, they hardly exhaust the possibilities
of exploiting the differences between these models.
Making use of the scores for the shallow parses out-
put by the Charniak parser is a demonstrably ef-
fective way to improve performance. Yet there are
other possible features explicit in the context-free
parse candidates, such as head-to-head dependen-
cies, which might be exploited to further improve
performance. We intend to explore including fea-
tures from the context-free parser output in our per-
ceptron model to improve shallow parsing accuracy.
Another possibility is to look at improving
793
context-free parsing accuracy. Within a multi-pass
parsing strategy, the high-accuracy shallow parses
that result from system combination could be used
to restrict the search within yet another pass of a
context-free parser. That parser could then search
for the best global analysis from within just the
space of parses consistent with the provided shallow
parse. Also, features of the sort used in our shallow
parser could be included in a reranker, such as that
in Charniak and Johnson (2005), with a context-free
parsing accuracy objective.
A third possibility is to optimize the definition of
the shallow-parse phrase types themselves, for use
in other applications. The composition of the set of
phrase types put forth by Sang and Buchholz (2000)
may not be optimal for certain applications. One
such application is discourse parsing, which relies
on accurate detection of clausal boundaries. Shal-
low parsing could provide reliable information on
the location of these boundaries, but the current set
of phrase types may be too general for such use. For
example, consider infinitival verb phrases, which of-
ten indicate the start of a clause whereas other types
of verb phrases do not. Unfortunately, with only one
VP category in the CoNLL-2000 set of phrase types,
this distinction is lost. Expanding the defined set of
phrase types could benefit many applications.
Future work will also include continued explo-
ration of possible features that can be of use for ei-
ther shallow parsing models or context-free parsing
models. In addition, we intend to investigate ways
in which to encode approximations to context-free
parser derived features that can be used within finite-
state models, thus perhaps preserving finite-state ef-
ficiency while capturing at least some of the accu-
racy gain that was observed in this paper.
Acknowledgments
We would like to thank Eugene Charniak and Mark
Johnson for help with the parser and reranker doc-
umented in their paper. The first author of this pa-
per was supported under an NSF Graduate Research
Fellowship. In addition, this research was supported
in part by NSF Grant #IIS-0447214. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
Steven Abney. 1991. Parsing by chunks. In Robert Berwick,
Steven Abney, and Carol Tenny, editors, Principle-Based
Parsing. Kluwer Academic Publishers, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cascades.
Natural Language Engineering, 2(4):337?344.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of NAACL, pages
132?139.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th Annual
Meeting of ACL, pages 16?23.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of the 17th ICML Confer-
ence.
Michael Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the Conference on
EMNLP, pages 1?8.
L. Gillick and S. Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
ICASSP, pages 532?535.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support
vector machines. In Proceedings of the 2nd Annual Meeting
of NAACL.
Xin Li and Dan Roth. 2001. Exploring evidence for shallow
parsing. In Proceedings of the 5th Conference on CoNLL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceedings of
the 3rd Workshop on Very Large Corpora, pages 82?94.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proceed-
ings of the 4th Conference on CoNLL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the HLT-NAACL
Annual Meeting.
Tong Zhang, Fred Damerau, and David Johnson. 2002. Text
chunking based on a generalization of Winnow. Journal of
Machine Learning Research, 2:615?637.
794
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488?495,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The utility of parse-derived features for automatic discourse segmentation
Seeger Fisher and Brian Roark
Center for Spoken Language Understanding, OGI School of Science & Engineering
Oregon Health & Science University, Beaverton, Oregon, 97006 USA
{fishers,roark}@cslu.ogi.edu
Abstract
We investigate different feature sets for
performing automatic sentence-level dis-
course segmentation within a general ma-
chine learning approach, including features
derived from either finite-state or context-
free annotations. We achieve the best re-
ported performance on this task, and demon-
strate that our SPADE-inspired context-free
features are critical to achieving this level of
accuracy. This counters recent results sug-
gesting that purely finite-state approaches
can perform competitively.
1 Introduction
Discourse structure annotations have been demon-
strated to be of high utility for a number of NLP
applications, including automatic text summariza-
tion (Marcu, 1998; Marcu, 1999; Cristea et al,
2005), sentence compression (Sporleder and Lap-
ata, 2005), natural language generation (Prasad et
al., 2005) and question answering (Verberne et al,
2006). These annotations include sentence segmen-
tation into discourse units along with the linking
of discourse units, both within and across sentence
boundaries, into a labeled hierarchical structure. For
example, the tree in Figure 1 shows a sentence-level
discourse tree for the string ?Prices have dropped but
remain quite high, according to CEO Smith,? which
has three discourse segments, each labeled with ei-
ther ?Nucleus? or ?Satellite? depending on how cen-
tral the segment is to the coherence of the text.
There are a number of corpora annotated with
discourse structure, including the well-known RST
Treebank (Carlson et al, 2002); the Discourse
GraphBank (Wolf and Gibson, 2005); and the Penn
Discourse Treebank (Miltsakaki et al, 2004). While
the annotation approaches differ across these cor-
pora, the requirement of sentence segmentation into
Root





H
H
H
H
H
Nucleus


HH
HH
Nucleus


PP
PP
Prices have dropped
Satellite


PP
PP
but remain quite high
Satellite



PP
PP
P
according to CEO Smith
Figure 1: Example Nucleus/Satellite labeled sentence-level
discourse tree.
sub-sentential discourse units is shared across all ap-
proaches. These resources have facilitated research
into stochastic models and algorithms for automatic
discourse structure annotation in recent years.
Using the RST Treebank as training and evalua-
tion data, Soricut and Marcu (2003) demonstrated
that their automatic sentence-level discourse pars-
ing system could achieve near-human levels of ac-
curacy, if it was provided with manual segmenta-
tions and manual parse trees. Manual segmenta-
tion was primarily responsible for this performance
boost over their fully automatic system, thus mak-
ing the case that automatic discourse segmentation is
the primary impediment to high accuracy automatic
sentence-level discourse structure annotation. Their
models and algorithm ? subsequently packaged to-
gether into the publicly available SPADE discourse
parser1 ? make use of the output of the Charniak
(2000) parser to derive syntactic indicator features
for segmentation and discourse parsing.
Sporleder and Lapata (2005) also used the RST
Treebank as training data for data-driven discourse
parsing algorithms, though their focus, in contrast
to Soricut and Marcu (2003), was to avoid context-
free parsing and rely exclusively on features in their
model that could be derived via finite-state chunkers
and taggers. The annotations that they derive are dis-
1
http://www.isi.edu/publications/licensed-sw/spade/
488
course ?chunks?, i.e., sentence-level segmentation
and non-hierarchical nucleus/span labeling of seg-
ments. They demonstrate that their models achieve
comparable results to SPADE without the use of any
context-free features. Once again, segmentation is
the part of the process where the automatic algo-
rithms most seriously underperform.
In this paper we take up the question posed by
the results of Sporleder and Lapata (2005): how
much, if any, accuracy reduction should we expect
if we choose to use only finite-state derived fea-
tures, rather than those derived from full context-
free parses? If little accuracy is lost, as their re-
sults suggest, then it would make sense to avoid rel-
atively expensive context-free parsing, particularly
if the amount of text to be processed is large or if
there are real-time processing constraints on the sys-
tem. If, however, the accuracy loss is substantial,
one might choose to avoid context-free parsing only
in the most time-constrained scenarios.
While Sporleder and Lapata (2005) demonstrated
that their finite-state system could perform as well as
the SPADE system, which uses context-free parse
trees, this does not directly answer the question of
the utility of context-free derived features for this
task. SPADE makes use of a particular kind of fea-
ture from the parse trees, and does not train a gen-
eral classifier making use of other features beyond
the parse-derived indicator features. As we shall
show, its performance is not the highest that can be
achieved via context-free parser derived features.
In this paper, we train a classifier using a gen-
eral machine learning approach and a range of finite-
state and context-free derived features. We investi-
gate the impact on discourse segmentation perfor-
mance when one feature set is used versus another,
in such a way establishing the utility of features de-
rived from context-free parses. In the course of so
doing, we achieve the best reported performance on
this task, an absolute F-score improvement of 5.0%
over SPADE, which represents a more than 34% rel-
ative error rate reduction.
By focusing on segmentation, we provide an ap-
proach that is generally applicable to all of the
various annotation approaches, given the similari-
ties between the various sentence-level segmenta-
tion guidelines. Given that segmentation has been
shown to be a primary impediment to high accu-
racy sentence-level discourse structure annotation,
this represents a large step forward in our ability to
automatically parse the discourse structure of text,
whatever annotation approach we choose.
2 Methods
2.1 Data
For our experiments we use the Rhetorical Structure
Theory Discourse Treebank (Carlson et al, 2002),
which we will denote RST-DT, a corpus annotated
with discourse segmentation and relations according
to Rhetorical Structure Theory (Mann and Thomp-
son, 1988). The RST-DT consists of 385 docu-
ments from the Wall Street Journal, about 176,000
words, which overlaps with the Penn Wall St. Jour-
nal (WSJ) Treebank (Marcus et al, 1993).
The segmentation of sentences in the RST-DT
is into clause-like units, known as elementary dis-
course units, or edus. We will use the two terms
?edu? and ?segment? interchangeably throughout the
rest of the paper. Human agreement for this segmen-
tation task is quite high, with agreement between
two annotators at an F-score of 98.3 for unlabeled
segmentation (Soricut and Marcu, 2003).
The RST-DT corpus annotates edu breaks, which
typically include sentence boundaries, but sentence
boundaries are not explicitly annotated in the corpus.
To perform sentence-level processing and evalua-
tion, we aligned the RST-DT documents to the same
documents in the Penn WSJ Treebank, and used the
sentence boundaries from that corpus.2 An addi-
tional benefit of this alignment is that the Penn WSJ
Treebank tokenization is then available for parsing
purposes. Simple minimum edit distance alignment
effectively allowed for differences in punctuation
representation (e.g., double quotes) and tokenization
when deriving the optimal alignment.
The RST-DT corpus is partitioned into a train-
ing set of 347 documents and a test set of 38 doc-
uments. This test set consists of 991 sentences with
2,346 segments. For training purposes, we created
a held-out development set by selecting every tenth
sentence of the training set. This development set
was used for feature development and for selecting
the number of iterations used when training models.
2.2 Evaluation
Previous research into RST-DT segmentation and
parsing has focused on subsets of the 991 sentence
test set during evaluation. Soricut and Marcu (2003)
2A small number of document final parentheticals are in the
RST-DT and not in the Penn WSJ Treebank, which our align-
ment approach takes into account.
489
omitted sentences that were not exactly spanned by
a subtree of the treebank, so that they could fo-
cus on sentence-level discourse parsing. By our
count, this eliminates 40 of the 991 sentences in the
test set from consideration. Sporleder and Lapata
(2005) went further and established a smaller sub-
set of 608 sentences, which omitted sentences with
only one segment, i.e., sentences which themselves
are atomic edus.
Since the primary focus of this paper is on seg-
mentation, there is no strong reason to omit any sen-
tences from the test set, hence our results will eval-
uate on all 991 test sentences, with two exceptions.
First, in Section 2.3, we compare SPADE results un-
der our configuration with results from Sporleder
and Lapata (2005) in order to establish compara-
bility, and this is done on their 608 sentence sub-
set. Second, in Section 3.2, we investigate feed-
ing our segmentation into the SPADE system, in or-
der to evaluate the impact of segmentation improve-
ments on their sentence-level discourse parsing per-
formance. For those trials, the 951 sentence subset
from Soricut and Marcu (2003) is used. All other
trials use the full 991 sentence test set.
Segmentation evaluation is done with precision,
recall and F1-score of segmentation boundaries.
Given a word string w1 . . . wk, we can index word
boundaries from 0 to k, so that each word wi falls
between boundaries i?1 and i. For sentence-based
segmentation, indices 0 and k, representing the be-
ginning and end of the string, are known to be seg-
ment boundaries. Hence Soricut and Marcu (2003)
evaluate with respect to sentence internal segmenta-
tion boundaries, i.e., with indices j such that 0<j<k
for a sentence of length k. Let g be the number
of sentence-internal segmentation boundaries in the
gold standard, t the number of sentence-internal seg-
mentation boundaries in the system output, and m
the number of correct sentence-internal segmenta-
tion boundaries in the system output. Then
P = mt R =
m
g and F1 =
2PR
P+R =
2m
g+t
In Sporleder and Lapata (2005), they were pri-
marily interested in labeled segmentation, where the
segment initial boundary was labeled with the seg-
ment type. In such a scenario, the boundary at in-
dex 0 is no longer known, hence their evaluation in-
cluded those boundaries, even when reporting un-
labeled results. Thus, in section 2.3, for compar-
ison with reported results in Sporleder and Lapata
(2005), our F1-score is defined accordingly, i.e., seg-
Segmentation system F1
Sporleder and Lapata best (reported) 88.40
SPADE
Sporleder and Lapata configuration (reported): 87.06
current configuration: 91.04
Table 1: Segmentation results on the Sporleder and Lapata
(2005) data set, with accuracy defined to include sentence initial
segmentation boundaries.
mentation boundaries j such that 0 ? j < k.
In addition, we will report unlabeled bracketing
precision, recall and F1-score, as defined in the
PARSEVAL metrics (Black et al, 1991) and eval-
uated via the widely used evalb package. We also
use evalb when reporting labeled and unlabeled dis-
course parsing results in Section 3.2.
2.3 Baseline SPADE setup
The publicly available SPADE package, which en-
codes the approach in Soricut and Marcu (2003),
is taken as the baseline for this paper. We made
several modifications to the script from the default,
which account for better baseline performance than
is achieved with the default configuration. First, we
modified the script to take given parse trees as input,
rather than running the Charniak parser itself. This
allowed us to make two modifications that improved
performance: turning off tokenization in the Char-
niak parser, and reranking. The default script that
comes with SPADE does not turn off tokenization
inside of the parser, which leads to degraded perfor-
mance when the input has already been tokenized in
the Penn Treebank style. Secondly, Charniak and
Johnson (2005) showed how reranking of the 50-
best output of the Charniak (2000) parser gives sub-
stantial improvements in parsing accuracy. These
two modifications to the Charniak parsing output
used by the SPADE system lead to improvements
in its performance compared to previously reported
results.
Table 1 compares segmentation results of three
systems on the Sporleder and Lapata (2005) 608
sentence subset of the evaluation data: (1) their best
reported system; (2) the SPADE system results re-
ported in that paper; and (3) the SPADE system re-
sults with our current configuration. The evaluation
uses the unlabeled F1 measure as defined in that pa-
per, which counts sentence initial boundaries in the
scoring, as discussed in the previous section. As can
be seen from these results, our improved configu-
ration of SPADE gives us large improvements over
the previously reported SPADE performance on this
subset. As a result, we feel that we can use SPADE
490
as a very strong baseline for evaluation on the entire
test set.
Additionally, we modified the SPADE script to al-
low us to provide our segmentations to the full dis-
course parsing that it performs, in order to evalu-
ate the improvements to discourse parsing yielded
by any improvements to segmentation.
2.4 Segmentation classifier
For this paper, we trained a binary classifier, which
was applied independently at each word wi in the
string w1 . . . wk, to decide whether that word is the
last in a segment. Note that wk is the last word in
the string, and is hence ignored. We used a log-
linear model with no Markov dependency between
adjacent tags,3 and trained the parameters of the
model with the perceptron algorithm, with averag-
ing to control for over-training (Collins, 2002).
Let C={E, I} be the set of classes: seg-
mentation boundary (E) or non-boundary (I). Let
f(c, i, w1 . . . wk) be a function that takes as in-
put a class value c, a word index i and the word
string w1 . . . wk and returns a d-dimensional vector
of feature values for that word index in that string
with that class. For example, one feature might be
(c = E,wi = the), which returns the value 1 when
c = E and the current word is ?the?, and returns
0 otherwise. Given a d-dimensional parameter vec-
tor ?, the output of the classifier is that class which
maximizes the dot product between the feature and
parameter vectors:
c?(i, w1 . . . wk) = argmax
c?C
? ? f(c, i, w1 . . . wk) (1)
In training, the weights in ? are initialized to 0.
For m epochs (passes over the training data), for
each word in the training data (except sentence final
words), the model is updated. Let i be the current
word position in string w1 . . . wk and suppose that
there have been j?1 previous updates to the model
parameters. Let c?i be the true class label, and let c?i
be shorthand for c?(i, w1 . . . wk) in equation 1. Then
the parameter vector ?j at step j is updated as fol-
lows:
?j = ?j?1 ? f(c?, i, w1 . . . wk) + f(c?, i, w1 . . . wk) (2)
As stated in Section 2.1, we reserved every tenth
sentence as held-out data. After each pass over the
training data, we evaluated the system performance
3Because of the sparsity of boundary tags, Markov depen-
dencies between tags buy no additional system accuracy.
on this held-out data, and chose the model that op-
timized accuracy on that set. The averaged percep-
tron was used on held-out and evaluation sets. See
Collins (2002) for more details on this approach.
2.5 Features
To tease apart the utility of finite-state derived fea-
tures and context-free derived features, we consider
three feature sets: (1) basic finite-state features; (2)
context-free features; and (3) finite-state approxima-
tion to context-free features. Note that every feature
must include exactly one class label c in order to
discriminate between classes in equation 1. Hence
when presenting features, it can be assumed that the
class label is part of the feature, even if it is not ex-
plicitly mentioned.
The three feature sets are not completely disjoint.
We include simple position-based features in every
system, defined as follows. Because edus are typi-
cally multi-word strings, it is less likely for a word
near the beginning or end of a sentence to be at an
edu boundary. Thus it is reasonable to expect the
position within a sentence of a token to be a helpful
feature. We created 101 indicator features, repre-
senting percentages from 0 to 100. For a string of
length k, at position i, we round i/k to two decimal
places and provide a value of 1 for the corresponding
quantized position feature and 0 for the other posi-
tion features.
2.5.1 Basic finite-state features
Our baseline finite-state feature set includes simple
tagger derived features, as well as features based on
position in the string and n-grams4. We annotate
tag sequences onto the word sequence via a compet-
itive discriminatively trained tagger (Hollingshead
et al, 2005), trained for each of two kinds of tag
sequences: part-of-speech (POS) tags and shallow
parse tags. The shallow parse tags define non-
hierarchical base constituents (?chunks?), as defined
for the CoNLL-2000 shared task (Tjong Kim Sang
and Buchholz, 2000). These can either be used
as tag or chunk sequences. For example, the tree
in Figure 2 represents a shallow (non-hierarchical)
parse tree, with four base constituents. Each base
constituent X begins with a word labeled with BX ,
which signifies that this word begins the constituent.
All other words within a constituent X are labeled
4We tried using a list of 311 cue phrases from Knott (1996)
to define features, but did not derive any system improvement
through this list, presumably because our simple n-gram fea-
tures already capture many such lexical cues.
491
ROOT





 
 
 
@
@
@
PP
PP
PP
PP
P
NP
 HH
BNP
DT
the
INP
NN
broker
VP
 HH
BVP
MD
will
IVP
VBD
sell
NP
 HH
BNP
DT
the
INP
NNS
stocks
NP
BNP
NN
tomorrow
Figure 2: Tree representation of shallow parses, with B(egin)
and I(nside) tags
IX , and words outside of any base constituent are la-
beled O. In such a way, each word is labeled with
both a POS-tag and a B/I/O tag.
For our three sequences (lexical, POS-tag and
shallow tag), we define n-gram features surround-
ing the potential discourse boundary. If the current
word is wi, the hypothesized boundary will occur
between wi and wi+1. For this boundary position,
the 6-gram including the three words before and the
three words after the boundary is included as a fea-
ture; additionally, all n-grams for n < 6 such that
either wi or wi+1 (or both) is in the n-gram are in-
cluded as features. In other words, all n-grams in a
six word window of boundary position i are included
as features, except those that include neither wi nor
wi+1 in the n-gram. The identical feature templates
are used with POS-tag and shallow tag sequences as
well, to define tag n-gram features.
This feature set is very close to that used in
Sporleder and Lapata (2005), but not identical.
Their n-gram feature definitions were different
(though similar), and they made use of cue phrases
from Knott (1996). In addition, they used a rule-
based clauser that we did not. Despite such differ-
ences, this feature set is quite close to what is de-
scribed in that paper.
2.5.2 Context-free features
To describe our context-free features, we first
present how SPADE made use of context-free parse
trees within their segmentation algorithm, since this
forms the basis of our features. The SPADE features
are based on productions extracted from full syntac-
tic parses of the given sentence. The primary feature
for a discourse boundary after word wi is based on
the lowest constituent in the tree that spans words
wm . . . wn such that m ? i < n. For example, in
the parse tree schematic in Figure 3, the constituent
labeled with A is the lowest constituent in the tree
whose span crosses the potential discourse bound-
ary after wi. The primary feature is the production
A




 
 
 
@
@
@
PP
PP
PP
PP
B1 . . . Bj?1


H
H
C1 . . . Cn
H
. . . Ti
wi
Bj . . . Bm
Figure 3: Parse tree schematic for describing context-free seg-
mentation features
that expands this constituent in the tree, with the
proposed segmentation boundary marked, which in
this case is: A ? B1 . . . Bj?1||Bj . . . Bm, where
|| denotes the segmentation boundary. In SPADE,
the production is lexicalized by the head words of
each constituent, which are determined using stan-
dard head-percolation techniques. This feature is
used to predict a boundary as follows: if the relative
frequency estimate of a boundary given the produc-
tion feature in the corpus is greater than 0.5, then a
boundary is predicted; otherwise not. If the produc-
tion has not been observed frequently enough, the
lexicalization is removed and the relative frequency
of a boundary given the unlexicalized production is
used for prediction. If the observations of the unlex-
icalized production are also too sparse, then only the
children adjacent to the boundary are maintained in
the feature, e.g., A ? ?Bj?1||Bj? where ? repre-
sents zero or more categories. Further smoothing is
used when even this is unobserved.
We use these features as the starting point for our
context-free feature set: the lexicalized production
A ? B1 . . . Bj?1||Bj . . . Bm, as defined above for
SPADE, is a feature in our model, as is the unlexi-
calized version of the production. As with the other
features that we have described, this feature is used
as an indicator feature in the classifier applied at the
word wi preceding the hypothesized boundary. In
addition to these full production features, we use the
production with only children adjacent to the bound-
ary, denoted by A ? ?Bj?1||Bj?. This production
is used in four ways: fully lexicalized; unlexicalized;
only category Bj?1 lexicalized; and only category
Bj lexicalized. We also use A ? ?Bj?2Bj?1||?
and A ? ?||BjBj+1? features, both unlexicalized
and with the boundary-adjacent category lexical-
ized. If there is no category Bj?2 or Bj+1, they are
replaced with ?N/A?.
In addition to these features, we fire the same fea-
tures for all productions on the path from A down
492
Segment Boundary accuracy Bracketing accuracy
Segmentation system Recall Precision F1 Recall Precision F1
SPADE 85.4 85.5 85.5 77.7 77.9 77.8
Classifier: Basic finite-state 81.5 83.3 82.4 73.6 74.5 74.0
Classifier: Full finite-state 84.1 87.9 86.0 78.0 80.0 79.0
Classifier: Context-free 84.7 91.1 87.8 80.3 83.7 82.0
Classifier: All features 89.7 91.3 90.5 84.9 85.8 85.3
Table 2: Segmentation results on all 991 sentences in the RST-DT test set. Segment boundary accuracy is for sentence internal
boundaries only, following Soricut and Marcu (2003). Bracketing accuracy is for unlabeled flat bracketing of the same segments.
While boundary accuracy correctly depicts segmentation results, the harsher flat bracketing metric better predicts discourse parsing
performance.
to the word wi. For these productions, the seg-
mentation boundary || will occur after all children
in the production, e.g., Bj?1 ? C1 . . . Cn||, which
is then used in both lexicalized and unlexicalized
forms. For the feature with only categories adja-
cent to the boundary, we again use ?N/A? to denote
the fact that no category occurs to the right of the
boundary: Bj?1 ? ?Cn||N/A. Once again, these
are lexicalized as described above.
2.5.3 Finite-state approximation features
An approximation to our context-free features can
be made by using the shallow parse tree, as shown
in Figure 2, in lieu of the full hierarchical parse
tree. For example, if the current word was ?sell?
in the tree in Figure 2, the primary feature would
be ROOT ? NP VP||NP NP, and it would have an
unlexicalized version and three lexicalized versions:
the category immediately prior to the boundary lex-
icalized; the category immediately after the bound-
ary lexicalized; and both lexicalized. For lexicaliza-
tion, we choose the final word in the constituent as
the lexical head for the constituent. This is a rea-
sonable first approximation, because such typically
left-headed categories as PP and VP lose their argu-
ments in the shallow parse.
As a practical matter, we limit the number of cat-
egories in the flat production to 8 to the left and 8 to
the right of the boundary. In a manner similar to the
n-gram features that we defined in Section 2.5.1, we
allow all combinations with less than 8 contiguous
categories on each side, provided that at least one
of the adjacent categories is included in the feature.
Each feature has an unlexicalized and three lexical-
ized versions, as described above.
3 Experiments
We performed a number of experiments to deter-
mine the relative utility of features derived from
full context-free syntactic parses and those derived
solely from shallow finite-state tagging. Our pri-
mary concern is with intra-sentential discourse seg-
mentation, but we are also interested in how much
the improved segmentation helps discourse parsing.
The syntactic parser we use for all context-free
syntactic parses used in either SPADE or our clas-
sifier is the Charniak parser with reranking, as de-
scribed in Charniak and Johnson (2005). The Char-
niak parser and reranker were trained on the sections
of the Penn Treebank not included in the RST-DT
test set.
All statistical significance testing is done via the
stratified shuffling test (Yeh, 2000).
3.1 Segmentation
Table 2 presents segmentation results for SPADE
and four versions of our classifier. The ?Basic finite-
state? system uses only finite-state sequence fea-
tures as defined in Section 2.5.1, while the ?Full
finite-state? also includes the finite-state approxima-
tion features from Section 2.5.3. The ?Context-free?
system uses the SPADE-inspired features detailed in
Section 2.5.2, but none of the features from Sections
2.5.1 or 2.5.3. Finally, the ?All features? section in-
cludes features from all three sections.5
Note that the full finite-state system is consider-
ably better than the basic finite-state system, demon-
strating the utility of these approximations of the
SPADE-like context-free features. The performance
of the resulting ?Full? finite-state system is not sta-
tistically significantly different from that of SPADE
(p=0.193), despite no reliance on features derived
from context-free parses.
The context-free features, however, even without
any of the finite-state sequence features (even lex-
ical n-grams) outperforms the best finite-state sys-
tem by almost two percent absolute, and the sys-
tem with all features improves on the best finite-state
system by over four percent absolute. The system
5In the ?All features? condition, the finite-state approxima-
tion features defined in Section 2.5.3 only include a maximum
of 3 children to the left and right of the boundary, versus a max-
imum of 8 for the ?Full finite-state? system. This was found to
be optimal on the development set.
493
Segmentation Unlabeled Nuc/Sat
SPADE 76.9 70.2
Classifier: Full finite state 78.1 71.1
Classifier: All features 83.5 76.1
Table 3: Discourse parsing results on the 951 sentence Sori-
cut and Marcu (2003) evaluation set, using SPADE for parsing,
and various methods for segmentation. Scores are unlabeled
and labeled (Nucleus/Satellite) bracketing accuracy (F1). The
first line shows SPADE performing both segmentation and dis-
course parsing. The other two lines show SPADE performing
discourse parsing with segmentations produced by our classi-
fier using different combinations of features.
with all features is statistically significantly better
than both SPADE and the ?Full finite-state? classi-
fier system, at p < 0.001. This large improvement
demonstrates that the context-free features can pro-
vide a very large system improvement.
3.2 Discourse parsing
It has been shown that accurate discourse segmen-
tation within a sentence greatly improves the over-
all parsing accuracy to near human levels (Sori-
cut and Marcu, 2003). Given our improved seg-
mentation results presented in the previous section,
improvements would be expected in full sentence-
level discourse parsing. To achieve this, we modi-
fied the SPADE script to accept our segmentations
when building the fully hierarchical discourse tree.
The results for three systems are presented in Ta-
ble 3: SPADE, our ?Full finite-state? system, and
our system with all features. Results for unlabeled
bracketing are presented, along with results for la-
beled bracketing, where the label is either Nucleus
or Satellite, depending upon whether or not the node
is more central (Nucleus) to the coherence of the text
than its sibling(s) (Satellite). This label set has been
shown to be of particular utility for indicating which
segments are more important to include in an auto-
matically created summary or compressed sentence
(Sporleder and Lapata, 2005; Marcu, 1998; Marcu,
1999; Cristea et al, 2005).
Once again, the finite-state system does not
perform statistically significantly different from
SPADE on either labeled or unlabeled discourse
parsing. Using all features, however, results in
greater than 5% absolute accuracy improvement
over both of these systems, which is significant, in
all cases, at p < 0.001.
4 Discussion and future directions
Our results show that context-free parse derived fea-
tures are critical for achieving the highest level of
accuracy in sentence-level discourse segmentation.
Given that edus are by definition clause-like units,
it is not surprising that accurate full syntactic parse
trees provide highly relevant information unavail-
able from finite-state approaches. Adding context-
free features to our best finite-state feature model
reduces error in segmentation by 32.1%, an in-
crease in absolute F-score of 4.5%. These increases
are against a finite-state segmentation model that is
powerful enough to be statistically indistinguishable
from SPADE.
Our experiments also confirm that increased seg-
mentation accuracy yields significantly better dis-
course parsing accuracy, as previously shown to be
the case when providing reference segmentations to
a parser (Soricut and Marcu, 2003). The segmen-
tation reduction in error of 34.5% propagates to a
28.6% reduction in error for unlabeled discourse
parse trees, and a 19.8% reduction in error for trees
labeled with Nucleus and Satellite.
We have several key directions in which to con-
tinue this work. First, given that a general ma-
chine learning approach allowed us to improve upon
SPADE?s segmentation performance, we also be-
lieve that it will prove useful for improving full
discourse parsing, both at the sentence level and
beyond. For efficient inter-sentential discourse
parsing, we see the need for an additional level
of segmentation at the paragraph level. Whereas
most sentences correspond to a well-formed subtree,
Sporleder and Lascarides (2004) report that over
20% of the paragraph boundaries in the RST-DT do
not correspond to a well-formed subtree in the hu-
man annotated discourse parse for that document.
Therefore, to perform accurate and efficient pars-
ing of the RST-DT at the paragraph level, the text
should be segmented into paragraph-like segments
that conform to the human-annotated subtree bound-
aries, just as sentences are segmented into edus.
We also intend to begin work on the other dis-
course annotated corpora. While most work on tex-
tual discourse parsing has made use of the RST-DT
corpus, the Discourse GraphBank corpus provides a
competing annotation that is not constrained to tree
structures (Wolf and Gibson, 2005). Once accurate
levels of segmentation and parsing for both corpora
are attained, it will be possible to perform extrinsic
evaluations to determine their relative utility for dif-
ferent NLP tasks. Recent work has shown promis-
ing preliminary results for recognizing and labeling
relations of GraphBank structures (Wellner et al,
2006), in the case that the algorithm is provided with
494
manually segmented sentences. Sentence-level seg-
mentation in the GraphBank is very similar to that in
the RST-DT, so our segmentation approach should
work well for Discourse GraphBank style parsing.
The Penn Discourse Treebank (Miltsakaki et al,
2004), or PDTB, uses a relatively flat annotation of
discourse structure, in contrast to the hierarchical
structures found in the other two corpora. It contains
annotations for discourse connectives and their argu-
ments, where an argument can be as small as a nom-
inalization or as large as several sentences. This ap-
proach obviates the need to create a set of discourse
relations, but sentence internal segmentation is still
a necessary step. Though segmentation in the PDTB
tends to larger units than edus, our approach to seg-
mentation should be straightforwardly applicable to
their segmentation style.
Acknowledgments
Thanks to Caroline Sporleder and Mirella Lapata for
their test data and helpful comments. Thanks also to
Radu Soricut for helpful input. This research was
supported in part by NSF Grant #IIS-0447214. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grish-
man, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M.P. Marcus, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quantita-
tively comparing the syntactic coverage of english gram-
mars. In DARPA Speech and Natural Language Workshop,
pages 306?311.
L. Carlson, D. Marcu, and M.E. Okurowski. 2002. RST dis-
course treebank. Linguistic Data Consortium, Catalog #
LDC2002T07. ISBN LDC2002T07.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proceedings of
the 43rd Annual Meeting of ACL, pages 173?180.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics,
pages 132?139.
M.J. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages
1?8.
D. Cristea, O. Postolache, and I. Pistol. 2005. Summarisation
through discourse structure. In 6th International Conference
on Computational Linguistics and Intelligent Text Process-
ing (CICLing).
K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing
and combining finite-state and context-free parsers. In Pro-
ceedings of HLT-EMNLP, pages 787?794.
A. Knott. 1996. A Data-Driven Methodology for Motivating
a Set of Coherence Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of Edinburgh.
W.C. Mann and S.A. Thompson. 1988. Rhetorical structure
theory: Toward a functional theory of text organization. Text,
8(3):243?281.
D. Marcu. 1998. Improving summarization through rhetorical
parsing tuning. In The 6th Workshop on Very Large Corpora.
D. Marcu. 1999. Discourse trees are good indicators of im-
portance in text. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization, pages 123?136.
MIT Press, Cambridge, MA.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004. The
Penn Discourse TreeBank. In Proceedings of the Language
Resources and Evaluation Conference.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and
B. Webber. 2005. The Penn Discourse TreeBank as a re-
source for natural language generation. In Proceedings of
the Corpus Linguistics Workshop on Using Corpora for Nat-
ural Language Generation.
R. Soricut and D. Marcu. 2003. Sentence level discourse pars-
ing using syntactic and lexical information. In Human Lan-
guage Technology Conference of the North American Asso-
ciation for Computational Linguistics (HLT-NAACL).
C. Sporleder and M. Lapata. 2005. Discourse chunking and its
application to sentence compression. In Human Language
Technology Conference and the Conference on Empirical
Methods in Natural Language Processing (HLT-EMNLP),
pages 257?264.
C. Sporleder and A. Lascarides. 2004. Combining hierarchi-
cal clustering and machine learning to predict high-level dis-
course structure. In Proceedings of the International Confer-
ence in Computational Linguistics (COLING), pages 43?49.
E.F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings of
CoNLL, pages 127?132.
S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen. 2006.
Discourse-based answering of why-questions. Traitement
Automatique des Langues (TAL).
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and
R. Sauri. 2006. Classification of discourse coherence re-
lations: An exploratory study using multiple knowledge
sources. In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue.
F. Wolf and E. Gibson. 2005. Representing discourse coher-
ence: A corpus-based analysis. Computational Linguistics,
31(2):249?288.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of the 18th Inter-
national COLING, pages 947?953.
495
