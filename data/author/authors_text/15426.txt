Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1384?1393,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Improved Transliteration Mining Using Graph Reinforcement 
Ali El-Kahky1, Kareem Darwish1, Ahmed Saad Aldein2, Mohamed Abd El-Wahab3, 
Ahmed Hefny2, Waleed Ammar4 
 
1 Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar 
2 Computer Engineering Department, Cairo University, Cairo, Egypt 
3 Microsoft Research, Microsoft, Cairo, Egypt 
4 Microsoft Research, Microsoft, Redmond, WA, US 
{aelkahky,kdarwish}@qf.org.qa1, asaadaldien@hotmail.com2, 
ahmed.s.hefny@gmail.com2, t-momah@microsoft.com3, 
i-waamma@microsoft.com4 
  
 
 
Abstract 
Mining of transliterations from comparable 
or parallel text can enhance natural 
language processing applications such as 
machine translation and cross language 
information retrieval. This paper presents 
an enhanced transliteration mining 
technique that uses a generative graph 
reinforcement model to infer mappings 
between source and target character 
sequences. An initial set of mappings are 
learned through automatic alignment of 
transliteration pairs at character sequence 
level. Then, these mappings are modeled 
using a bipartite graph. A graph 
reinforcement algorithm is then used to 
enrich the graph by inferring additional 
mappings. During graph reinforcement, 
appropriate link reweighting is used to 
promote good mappings and to demote bad 
ones. The enhanced transliteration mining 
technique is tested in the context of mining 
transliterations from parallel Wikipedia 
titles in 4 alphabet-based languages pairs, 
namely English-Arabic, English-Russian, 
English-Hindi, and English-Tamil. The 
improvements in F1-measure over the 
baseline system were 18.7, 1.0, 4.5, and 
32.5 basis points for the four language 
pairs respectively. The results herein 
outperform the best reported results in the 
literature by 2.6, 4.8, 0.8, and 4.1 basis 
points for the four language pairs 
respectively. 
Introduction 
Transliteration Mining (TM) is the process of 
finding transliterated word pairs in parallel or 
comparable corpora. TM has many potential 
applications such as mining training data for 
transliteration, improving lexical coverage for 
machine translation, and cross language retrieval 
via translation resource expansion. TM has been 
gaining some attention lately with a shared task in 
the ACL 2010 NEWS workshop (Kumaran, et al 
2010). 
One popular statistical TM approach is performed 
in two stages. First, a generative model is trained 
by performing automatic character level alignment 
of parallel transliterated word pairs to find 
character segment mappings between source and 
target languages. Second, given comparable or 
parallel text, the trained generative model is used 
to generate possible transliterations of a word in 
the source language while constraining the 
transliterations to words that exist in the target 
language. 
However, two problems arise in this approach: 
1. Many possible character sequence mappings 
between source and target languages may not be 
observed in training data, particularly when limited 
training data is available ? hurting recall. 
2. Conditional probability estimates of obtained 
mappings may be inaccurate, because some 
mappings and some character sequences may not 
1384
appear a sufficient number of times in training to 
properly estimate their probabilities ? hurting 
precision. 
In this paper we focus on overcoming these two 
problems to improve overall TM. To address the 
first problem, we modeled the automatically 
obtained character sequence mappings (from 
alignment) as a bipartite graph and then we 
performed graph reinforcement to enrich the graph 
and predict possible mappings that were not 
directly obtained from training data. The example 
in Figure 1 motivates graph reinforcement. In the 
example, the Arabic letter ??? (pronounced as 
?qa?) was not aligned to the English letter ?c? in 
training data. Such a mapping seems probable 
given that another Arabic letter, ??? (pronounced 
as ?ka?), maps to two English letters, ?q? and ?k?, 
to which ??? also maps. In this case, there are 
multiple paths that would lead to a mapping 
between the Arabic letter ??? and the English letter 
?c?, namely ? ? q ? ? ? c and ? ? k ? ? ? 
c. By using multiple paths as sources of evidence, 
we can infer the new mapping and estimate its 
probability.   
Another method for overcoming the missing 
mappings problem entails assigning small 
smoothing probabilities to unseen mappings. 
However, from looking at the graph, it is evident 
that some mappings could be inferred and should 
be assigned probabilities that are higher than a 
small smoothing probability. 
The second problem has to do primarily with some 
characters in one language, typically vowels, 
mapping to many character sequences in the other 
language, with some of these mappings assuming 
very high probabilities (due to limited training 
data). To overcome this problem, we used link 
reweighting in graph reinforcement to scale down 
the likelihood of mappings to target character 
sequences in proportion to how many source 
sequences map to them. 
We tested the proposed method using the ACL 
2010 NEWS workshop data for English-Arabic, 
English-Russian, English-Hindi, and English-
Tamil (Kumaran et al, 2010). For each language 
pair, the standard ACL 2010 NEWS workshop data 
contained a base set of 1,000 transliteration pairs 
for training, and set of 1,000 parallel Wikipedia 
titles for testing. 
The contributions of the paper are: 
1. Employing graph reinforcement to improve the 
coverage of automatically aligned data ? as they 
apply to transliteration mining. This positively 
affects recall. 
2. Applying link reweighting to overcome 
situations where certain tokens ? character 
sequences in the case of transliteration ? tend to 
have many mappings, which are often erroneous. 
This positively affects precision. 
The rest of the paper is organized as follows: 
Section 2 surveys prior work on transliteration 
mining; Section 3 describes the baseline TM 
approach and reports on its effectiveness; Section 4 
describes the proposed graph reinforcement along 
with link reweighting and reports on the observed 
improvements; and Section 5 concludes the paper. 
 
Figure 1:  Example mappings seen in training 
 
Background 
Much work has been done on TM for different 
language pairs such as English-Chinese (Kuo et al, 
2006; Kuo et al, 2007; Kuo et al, 2008; Jin et al 
2008;), English-Tamil (Saravanan and Kumaran, 
2008; Udupa and Khapra, 2010), English-Korean 
(Oh and Isahara, 2006; Oh and Choi, 2006), 
English-Japanese (Qu et al, 2000; Brill et al, 
2001; Oh and Isahara, 2006), English-Hindi (Fei et 
al., 2003; Mahesh and Sinha, 2009), and English-
Russian (Klementiev and Roth, 2006). 
TM typically involves two main tasks, namely: 
finding character mappings between two 
languages, and given the mappings ascertaining 
whether two words are transliterations or not. 
When training with a limited number of 
transliteration pairs, two additional problems 
appear: many possible character sequence 
mappings between source and target languages 
may not be observed in training data, and 
conditional probability estimates of obtained 
1385
mappings may be inaccurate. These two problems 
affect recall and precision respectively. 
1.1 Finding Character Mappings 
To find character sequence mappings between two 
languages, the most common approach entails 
using automatic letter alignment of transliteration 
pairs. Akin to phrasal alignment in machine 
translation, character sequence alignment is treated 
as a word alignment problem between parallel 
sentences, where transliteration pairs are treated as 
if they are parallel sentences and the characters 
from which they are composed are treated as if 
they are words. Automatic alignment can be 
performed using different algorithms such as the 
EM algorithm (Kuo et al, 2008; Lee and Chang, 
2003) or HMM based alignment (Udupa et al, 
2009a; Udupa et al, 2009b). In this paper, we use 
automatic character alignment between 
transliteration pairs using an HMM aligner. 
Another method is to use automatic speech 
recognition confusion tables to extract phonetically 
equivalent character sequences to discover 
monolingual and cross lingual pronunciation 
variations (Kuo and Yang, 2005). Alternatively, 
letters can be mapped into a common character set 
using a predefined transliteration scheme (Oh and 
Choi, 2006). 
1.2 Transliteration Mining 
For the problem of ascertaining if two words can 
be transliterations of each other, a common 
approach involves using a generative model that 
attempts to generate all possible transliterations of 
a source word, given the character mappings 
between two languages, and restricting the output 
to words in the target language (Fei et al, 2003; 
Lee and Chang, 2003, Udupa et al, 2009a). This is 
similar to the baseline approach that we used in 
this paper. Noeman and Madkour (2010) 
implemented this technique using a finite state 
automaton by generating all possible 
transliterations along with weighted edit distance 
and then filtered them using appropriate thresholds 
and target language words. They reported the best 
TM results between English and Arabic with F1-
measure of 0.915 on the ACL-2010 NEWS 
workshop standard TM dataset. A related 
alternative is to use back-transliteration to 
determine if one sequence could have been 
generated by successively mapping character 
sequences from one language into another (Brill et 
al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 
2006). 
Udupa and Khapra (2010) proposed a method in 
which transliteration candidates are mapped into a 
?low-dimensional common representation space?. 
Then, similarity between the resultant feature 
vectors for both candidates can be computed. 
Udupa and Kumar (2010) suggested that mapping 
to a common space can be performed using context 
sensitive hashing. They applied their technique to 
find variant spellings of names. 
Jiampojamarn et al (2010) used classification to 
determine if a source language word and target 
language word are valid transliterations. They used 
a variety of features including edit distance 
between an English token and the Romanized 
versions of the foreign token, forward and 
backward transliteration probabilities, and 
character n-gram similarity. They reported the best 
results for Russian, Tamil, and Hindi with F1-
measure of 0.875, 0.924, and 0.914 respectively on 
the ACL-2010 NEWS workshop standard TM 
datasets. 
1.3 Training with Limited Training Data 
When only limited training data is available to 
train a character mapping model, the resultant 
mappings are typically incomplete (due to 
sparseness in the training data). Further, resultant 
mappings may not be observed a sufficient of 
times and hence their mapping probabilities may 
be inaccurate. 
Different methods were proposed to solve these 
two problems. These methods focused on making 
training data less sparse by performing some kind 
of letter conflation. Oh and Choi (2006) used a 
SOUNDEX like scheme. SOUNDEX is used to 
convert English words into a simplified phonetic 
representation, in which vowels are removed and 
phonetically similar characters are conflated. A 
variant of SOUNDEX along with iterative training 
was proposed by Darwish (2010). Darwish (2010) 
reported significant improvements in TM recall at 
the cost of limited drop in precision. Another 
method involved expanding character sequence 
maps by automatically mining transliteration pairs 
and then aligning these pairs to generate an 
expanded set of character sequence maps (Fei et 
al., 2003). In this work we proposed graph 
1386
reinforcement with link reweighting to address this 
problem. Graph reinforcement was used in the 
context of different problems such as mining 
paraphrases (Zhao et al, 2008; Kok and Brockett, 
2010; Bannard and  Callison-Burch 2005) and 
named entity translation extraction (You et al, 
2010). 
Baseline Transliteration Mining 
1.4 Description of Baseline System 
The basic TM setup that we employed in this 
work used a generative transliteration model, 
which was trained on a set of transliteration pairs. 
The training involved automatically aligning 
character sequences. The alignment was performed 
using a Bayesian learner that was trained on word 
dependent transition models for HMM based word 
alignment (He, 2007). Alignment produced 
mappings of source character sequences to target 
character sequences along with the probability of 
source given target and vice versa. Source 
character sequences were restricted to be 1 to 3 
characters long. 
For all the work reported herein, given an 
English-foreign language transliteration candidate 
pair, English was treated as the target language and 
the foreign language as the source.  Given a 
foreign source language word sequence   
  and an 
English target word sequence   
 ,      
   could 
be a potential transliteration of      
 .  An 
example of word sequences pair is the Tamil-
English pair:  (??????? ???? ??????, 
Haile Selassie I of Ethiopia), where ????????? 
could be transliteration for any or none of the 
English words {?Haile?, ?Selassie?, ?I?, ?of?, 
?Ethiopia?}.  The pseudo code below describes 
how transliteration mining generates candidates. 
Basically, given a source language word, all 
possible segmentations, where each segment has a 
maximum length of 3 characters, are produced 
along with their associated mappings into the 
target language. Given all mapping combinations, 
combinations producing valid target words are 
retained and sorted according to the product of 
their mapping probabilities. If the product of the 
mapping probabilities for the top combination is 
above a certain threshold, then it is chosen as the 
transliteration candidate. Otherwise, no candidate 
is chosen. To illustrate how TM works, consider 
the following example: Given the Arabic word 
????, all possible segmentations are (? ? ?) and (??). 
Given the target words {the, best, man} and the 
possible mappings for the segments and their 
probabilities: 
?? = {(m, 0.7), (me, 0.25), (ma, 0.05)} 
? = {n, 0.7), (nu, 0.2), (an, 0.1)} 
?? = {(men, 0.4), (man, 0.3), (mn, 0.3)} 
The only combinations leading valid target 
words would be: 
(??) ? {(man: 0.3)} 
( ?? ? )? ? {(m,an: 0.07), (ma, n: 0.035)} 
Consequently, the algorithm would produce the 
tuple with the highest probability: (?? , man, 0.3). 
As the pseudo code suggests, the actual 
implementation is optimized via: incremental left 
to right processing of source words; the use of a 
Patricia trie to prune mapping combinations that 
don?t lead to valid words; and the use of a priority 
queue to insure that the best candidate is always at 
the top of the queue. 
1.5 Smoothing and Thresholding  
We implemented the baseline system with and 
without assigning small smoothing probabilities 
for unseen source character to target character 
mappings. Subsequent to training, the smoothing 
probability was selected as the smallest observed 
mapping probability in training.   
We used a threshold on the minimum acceptable 
transliteration score to filter out unreliable 
transliterations. We couldn?t fix a minimum score 
for reliable transliterations to a uniform value for 
all words, because this would have caused the 
model to filter out long transliterations. Thus, we 
tied the threshold to the length of transliterated 
words. We assumed a threshold d for single 
character mappings and the transliteration 
threshold for a target word of length l was 
computed as    . We selected d by sorting the 
mapping probabilities, removing the lowest 10% of 
mapping probabilities (which we assumed to be 
outliers), and then selecting the smallest observed 
probability to be the character threshold d. The 
choice of removing the lowest ranking 10% of 
mapping probabilities was based on intuition, 
because we did not have a validation set. The 
threshold was then applied to filter out 
transliteration with                         . 
1387
1.6 Effectiveness of Baseline System 
To test the effectiveness of the baseline system, we 
used the standard TM training and test datasets 
from the ACL-2010 NEWS workshop shared task. 
The datasets are henceforth collectively referred to 
as the NEWS dataset. The dataset included 4 
alphabet-based language pairs, namely English-
Arabic, English-Russian, English-Hindi, and 
English-Tamil. For each pair, a dataset included a 
list of 1,000 parallel transliteration word pairs to 
train a transliteration model, and a list of 1,000 
parallel word sequences to test TM. The parallel 
sequences in the test sets were extracted titles from 
Wikipedia article for which cross language links 
exist between both languages. 
We preprocessed the different languages as 
follows: 
? Russian: characters were case-folded 
? Arabic: the different forms of alef (alef, alef 
maad, alef with hamza on top, and alef with 
hamza below it) were normalized to alef, ya 
and alef maqsoura were normalized to ya, and 
ta marbouta was mapped to ha. 
? English: letters were case-folded and the 
following letter conflations were performed: 
?, ? ? z  ?, ?, ?, ?, ?, ?, ?, ? ? a 
?, ?, ? ? e  ?, ?, ? ?c 
? ?l  ?, ?, ?, ? ? i 
?, ?, ?, ? ? o ?, ?, ? ? n 
?, ?, ?, ? ? s ? ? r 
? ? y  ?, ?, ?, ? ? u 
? Tamil and Hindi: no preprocessing was 
performed.  
 
English/ P R F 
Arabic 0.988 0.983 0.583 0.603 0.733 0.748 
Russian 0.975 0.967 0.831 0.862 0.897 0.912 
Hindi 0.986 0.981 0.693 0.796 0.814 0.879 
Tamil 0.984 0.981 0.274 0.460 0.429 0.626 
 
Table 1:  Baseline results for all language pairs.  
Results with smoothing are shaded. 
 
Table 1 reports the precision, recall, and F1-
measure results for using the baseline system in 
TM between English and each of the 4 other 
languages in the NEWS dataset with and without 
smoothing.  As is apparent in the results, without 
smoothing, precision is consistently high for all 
languages, but recall is generally poor, particularly 
for Tamil. When smoothing is applied, we 
observed a slight drop in precision for Arabic, 
Hindi, and Tamil and a significant drop of 5.6 
1: Input:  Mappings, set of source given target mappings with associated Prob.  
2: Input:  SourceWord ( 
 
  
1
 
), Source language word 
3: Input:  TargetWords, Patricia trie containing all target language words ( 
1
?
) 
4: Data Structures:  DFS, Priority queue to store candidate transliterations pair ordered by their transliteration 
score ? Each candidate transliteration tuple = (SourceFragment, TargetTransliteration, TransliterationScore). 
5: StartSymbol = (??, ??, 1.0) 
6: DFS={StartSymbol}  
7: While(DFS is not empty) 
8:  SourceFragment= DFS.Top().SourceFragment 
9:  TargetFragment= DFS.Top().TargetTransliteration 
10:  FragmentProb=DFS.Top().TransliterationScore 
11:  If (SourceWord == SourceFragment ) 
12:   If(FragmentScore > Threshold) 
13:    Return (SourceWord, TargetTransliteration, TransliterationScore) 
14:   Else 
15:    Return Null 
16:  DFS.RemoveTop() 
17:  For SubFragmentLength=1 to 3 
18:   SourceSubString= SubString( SourceWord, SourceFragment.Length , SubFragmentLength) 
19:   Foreach mapping in Mappings[SourceSubString]  
20:    If( (TargetFragment + mapping)  is a sub-string in TargetWords) 
21:     DFS.Add(SourceFragment + SourceSubString, Mapping.Score * FragmentScore) 
22:  DFS.Remove(SourceFragment) 
23: End While 
24: Return Null 
Figure 2:  Pseudo code for transliteration mining 
 
1388
basis points for Russian. However, the application 
of smoothing increased recall dramatically for all 
languages, particularly Tamil. For the remainder of 
the paper, the results with smoothing are used as 
the baseline results. 
Background 
1.7 Description of Graph Reinforcement 
In graph reinforcement, the mappings deduced 
from the alignment process were represented using 
a bipartite graph G = (S, T, M), where S was the 
set of source language character sequences, T was 
the set of target language character sequences, and 
M was the set of mappings (links or edges) 
between S and T. The score of each mapping 
m(v1|v2), where m(v1|v2) ? M, was initially set to 
the conditional probability of target given source 
p(v1|v2). Graph reinforcement was performed by 
traversing the graph from S ? T ? S ? T in 
order to deduce new mappings. Given a source 
sequence s' ? S and a target sequence t' ??T, the 
deduced mapping probabilities were computed as 
follows (Eq.1):  
?(  |  )    ? (  ?(  | )?( | )?( |  ))
        
 
where the term (  ?(  | )?( | )?( |  )) 
computed the probability that a mapping is not 
correct. Hence, the probability of an inferred 
mapping would be boosted if it was obtained from 
multiple paths. Given the example in Figure 1, 
m(c|?) would be computed as follows:  
  (  ?( |?)?(?| )?( |?))  
(  ?( |?)?(?| )?( |?)) 
We were able to apply reinforcement iteratively on 
all mappings from S to T to deduce previously 
unseen mappings (graph edges) and to update the 
probabilities of existing mappings. 
1.8 Link Reweighting  
The basic graph reinforcement algorithm is prone 
to producing irrelevant mappings by using 
character sequences with many different possible 
mappings as a bridge. Vowels were the most 
obvious examples of such character sequences. For 
example, automatic alignment produced 26 Hindi 
character sequences that map to the English letter 
?a?, most of which were erroneous such as the 
mapping between ?a? and ??? (pronounced va). 
Graph reinforcement resulted in many more such 
mappings. After successive iterations, such 
character sequences would cause the graph to be 
fully connected and eventually the link weights 
will tend to be uniform in their values. To illustrate 
this effect, we experimented with basic graph 
reinforcement on the NEWS dataset. The figures of 
merit were precision, recall, and F1-measure. 
Figures 3, 4, 5, and 6 show reinforcement results 
for Arabic, Russian, Hindi, and Tamil respectively. 
The figures show that: recall increased quickly and 
nearly saturated after several iterations; precision 
continued to drop with more iterations; and F1-
measure peaked after a few iterations and began to 
drop afterwards. This behavior was undesirable 
because overall F1-measure values did not 
converge with iterations, necessitating the need to 
find clear stopping conditions. 
To avoid this effect and to improve precision, we 
applied link reweighting after each iteration. Link 
reweighting had the effect of decreasing the 
weights of target character sequences that have 
many source character sequences mapping to them 
and hence reducing the effect of incorrectly 
inducing mappings. Link reweighting was 
performed as follows (Eq. 2): 
  ? ( | )   
 ( | )
?  (  | )    
 
Where si ? S is a source character sequence that 
maps to t. So in the case of ?a? mapping to the ??? 
character in Hindi, the link weight from ?a? to ??? 
is divided by the sum of link weights from ?a? to 
all 26 characters to which ?a? maps. 
We performed multiple experiments on the NEWS 
dataset to test the effect of graph reinforcement 
with link reweighting with varying number of 
reinforcement iterations. Figures 7, 8, 9, and 10 
compare baseline results with smoothing to results 
with graph reinforcement at different iterations. 
As can be seen in the figures, the F1-measure 
values stabilized as we performed multiple graph 
reinforcement iterations. Except for Russian, the 
results across different languages behaved in a 
similar manner. 
For Russian, graph reinforcement marginally 
affected TM F1-measure, as precision and recall 
1389
marginally changed. The net improvement was 1.1 
basis points. English and Russian do not share the 
same alphabet, and the number of initial mappings 
was bigger compared to the other language pairs.  
Careful inspection of the English-Russian test set, 
with the help of a Russian speaker, suggests that:  
1) the test set reference contained many false 
negatives;  
2) Russian names often have multiple phonetic 
forms (or spellings) in Russian with a single 
standard transliteration in English. For example, 
the Russian name ?Olga? is often written and 
pronounced as ?Ola? and ?Olga? in Russian; and  
3) certain English phones do not exist in Russian, 
leading to inconsistent character mappings in 
Russian.  For example, the English phone for ?g?, 
as in ?George?, does not exist in Russian. 
 
For the other languages, graph reinforcement 
yielded steadily improving recall and consequently 
steadily improving F1-measure. Most 
improvements were achieved within the first 5 
iterations, and improvements beyond 10 iterations 
were generally small (less than 0.5 basis points in 
F1-measure). After 15 iterations, the improvements 
in overall F1-measure above the baseline with 
smoothing were 19.3, 5.3, and 32.8 basis points for 
Arabic, Tamil, and Hindi respectively. The F1-
measure values seemed to stabilize with successive 
iterations. The least improvements were observed 
for Hindi. This could be attributed to the fact that 
Hindi spelling is largely phonetic, making letters in 
words pronounceable in only one way. This fact 
makes transliteration between Hindi and English 
easier than Arabic and Tamil. In the case of Tamil, 
the phonetics of letters change depending on the 
position of letters in words. As for Arabic, multiple 
letters sequences in English can map to single 
letters in Arabic and vice versa. Also, Arabic has 
diacritics which are typically omitted, but 
commonly transliterate to English vowels. Thus, 
the greater the difference in phonetics between two 
languages and the greater the phonetic complexity 
of either, the more TM can gain from the proposed 
technique. 
1.9 When Graph Reinforcement Worked  
An example where reinforcement worked entails 
the English-Arabic transliteration pair (Seljuq, 
?????). In the baseline runs with 1,000 training 
examples, both were not mapped to each other 
because there were no mappings between the letter 
?q? and the Arabic letter sequence ???? 
(pronounced as ?qah?). The only mappings that 
were available for ?q? were ???? (pronounced as 
?kah?), ??? (pronounced as ?q?), and ??? 
(pronounced as ?k?) with probabilities 54.0, 0.10, 
and 5452 respectively. Intuitively, the third 
mapping is more likely than the second. After 3 
graph reinforcement iterations, the top 5 mappings 
for ?q? were ??? (pronounced as ?q?), ???? 
(pronounced as ?qah?), ???? (pronounced as 
?kah?), ??? (pronounced as ?k?), and ????? 
(pronounced as ?alq?) with mapping probabilities 
of 0.22, 0.19, 0.15, 0.05, and 0.05 respectively. In 
this case, graph reinforcement was able to find the 
missing mapping and properly reorder the 
mappings.  Performing 10 iterations with link 
reweighting for Arabic led to 17 false positives. 
Upon examining them, we found that: 9 were 
actually correct, but erroneously labeled as false in 
the test set; 6 were phonetically similar like ????????? 
(pronounced espanya) and ?Spain? and ????????????? 
(pronounced alteknologya) and ?technology?; and 
the remaining 2 were actually wrong, which were 
??????? (pronounced beatchi) and ?medici? and 
? ????? (pronounced sidi) and ?taya?. This seems to 
indicate that graph reinforcement generally 
introduced more proper mappings than improper 
ones. 
1.10 Comparing to the State-of-the-Art  
Table 2 compares the best reported results in ACL-
2010 NEWS TM shared task for Arabic (Noeman 
and Madkour, 2010) and for the other languages 
(Jiampojamarn et al 2010) and the results obtained 
by the proposed technique using 10 iterations, with 
link reweighting. The comparison shows that the 
proposed algorithm yielded better results than the 
best reported results in the literature by 2.6, 4.8, 
0.8 and 4.1 F1-measure points in Arabic, Russian, 
Hindi and Tamil respectively. For Arabic, the 
improvement over the previously reported result 
was due to improvement in precision, while for the 
other languages the improvements were due to 
improvements in both recall and precision. 
 
 
1390
  
Figure 3: Graph reinforcement w/o link reweighting 
for Arabic 
 
Figure 4: Graph reinforcement w/o link reweighting 
for Russian 
 
Figure 5: Graph reinforcement w/o link reweighting 
for Hindi 
 
Figure 6: Graph reinforcement w/o link reweighting 
for Tamil 
 
Figure 7:  Graph reinforcement results for Arabic 
 
Figure 8: Graph reinforcement results for Russian 
 
Figure 9:  Graph reinforcement results for Hindi 
 
Figure 10:  Graph reinforcement results for Tamil 
 
0.400
0.500
0.600
0.700
0.800
0.900
1.000
b
a
s
e
l
i
n
e 1 2 3 4 5 6 7 8 9
1
0
Iterations
F
R
P
0.400
0.500
0.600
0.700
0.800
0.900
1.000
b
a
s
e
l
i
n
e 1 2 3 4 5 6 7 8 9
1
0
Iterations
F
R
P
0.400
0.500
0.600
0.700
0.800
0.900
1.000
b
a
s
e
l
i
n
e 1 2 3 4 5 6 7 8 9
1
0
Iterations
F
R
P
0.400
0.500
0.600
0.700
0.800
0.900
1.000
b
a
s
e
l
i
n
e 1 2 3 4 5 6 7 8 9
1
0
Iterations
F
R
P
0.82
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
F
R
P
baseline
F = 0.748
R = 0.603
P = 0.983
0.82
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
F
R
P
baseline
F = 0.912
R = 0.862
P = 0.967
0.82
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
F
R
P
baseline
F = 0.879
R = 0.796
P = 0.981
0.82
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
F
R
P
baseline
F = 0.626
R = 0.460
P = 0.981
1391
 Shared Task Proposed Algorithm 
English/ P R F P R F 
Arabic 0.887 0.945 0.915 0.979 0.905 0.941 
Russian 0.880 0.869 0.875 0.921 0.925 0.923 
Hindi 0.954 0.895 0.924 0.972 0.895 0.932 
Tamil 0.923 0.906 0.914 0.964 0.945 0.955 
Table 2: Best results obtained in ACL-2010 NEWS TM 
shared task compared to graph reinforcement with link 
reweighting after 10 iterations 
Conclusion 
In this paper, we presented a graph reinforcement 
algorithm with link reweighting to improve 
transliteration mining recall and precision by 
systematically inferring mappings that were unseen 
in training. We used the improved technique to 
extract transliteration pairs from parallel Wikipedia 
titles. The proposed technique solves two problems 
in transliteration mining, namely: some mappings 
may not be seen in training data ? hurting recall, 
and certain mappings may not be seen a sufficient 
number of times to appropriate estimate mapping 
probabilities ? hurting precision. The results 
showed that graph reinforcement yielded improved 
transliteration mining from parallel Wikipedia 
titles for all four languages on which the technique 
was tested. 
Generally iterative graph reinforcement was able to 
induce unseen mappings in training data ? 
improving recall. Link reweighting favored 
precision over recall counterbalancing the effect of 
graph reinforcement. The proposed system 
outperformed the best reported results in the 
literature for the ACL-2010 NEWS workshop 
shared task for Arabic, Russian, Hindi and Tamil.  
To extend the work, we would like to try 
transliteration mining from large comparable texts. 
The test parts of the NEWS dataset only contained 
short parallel fragments. For future work, graph 
reinforcement could be extended to MT to improve 
the coverage of aligned phrase tables. In doing so, 
it is reasonable to assume that there are multiple 
ways of expressing a singular concept and hence 
multiple translations are possible. Using graph 
reinforcement can help discover such translation 
though they may never be seen in training data. 
Using link reweighting in graph reinforcement can 
help demote unlikely translations while promoting 
likely ones. This could help clean MT phrase 
tables. Further, when dealing with transliteration, 
graph reinforcement can help find phonetic 
variations within a single language, which can 
have interesting applications in spelling correction 
and information retrieval. Applying the same to 
machine translation phrase tables can help identify 
paraphrases automatically. 
References  
Colin Bannard, Chris Callison-Burch. 2005. 
Paraphrasing with Bilingual Parallel Corpora. ACL-
2005, pages 597?604. 
Slaven Bilac, Hozumi Tanaka. 2005. Extracting 
transliteration pairs from comparable corpora. NLP-
2005. 
Eric Brill, Gary Kacmarcik, Chris Brockett. 2001. 
Automatically harvesting Katakana-English term 
pairs from search engine query logs. NLPRS 2001, 
pages 393?399. 
Kareem Darwish. 2010. Transliteration Mining with 
Phonetic Conflation and Iterative Training. ACL 
NEWS workshop 2010. 
Huang Fei, Stephan Vogel, and Alex Waibel. 2003. 
Extracting Named Entity Translingual Equivalence 
with Limited Resources. TALIP, 2(2):124?129. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. ACL-07 2nd SMT 
workshop. 
Sittichai Jiampojamarn, Kenneth Dwyer, Shane 
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young 
Kim and Grzegorz Kondrak. 2010. Transliteration 
Generation and Mining with Limited Training 
Resources. ACL NEWS workshop 2010. 
Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, Jong-
Hyeok Lee. 2008. Automatic Extraction of English-
Chinese Transliteration Pairs using Dynamic 
Window and Tokenizer. Sixth SIGHAN Workshop 
on Chinese Language Processing, 2008. 
Alexandre Klementiev and Dan Roth. 2006. Named 
Entity Transliteration and Discovery from 
Multilingual Comparable Corpora. HLT Conf. of the 
North American Chapter of the ACL, pages 82?88. 
Stanley Kok, Chris Brockett.. 2010. Hitting the Right 
Paraphrases in Good Time. Human Language 
Technologies: The 2010 Annual Conference of the 
North American Chapter of the ACL, June 2010 
A. Kumaran, Mitesh M. Khapra, Haizhou Li. 2010. 
Report of NEWS 2010 Transliteration Mining Shared 
Task. Proceedings of the 2010 Named Entities 
1392
Workshop, ACL 2010, pages 21?28, Uppsala, 
Sweden, 16 July 2010. 
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. 2006. 
Learning Transliteration Lexicons from the Web. 
COLING-ACL2006, Sydney, Australia, 1129 ? 1136. 
Jin-shea Kuo, Haizhou Li, Ying-kuei Yang. 2007. A 
phonetic similarity model for automatic extraction of 
transliteration pairs. TALIP, 2007 
Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin. 2008. 
Mining Transliterations from Web Query Results: An 
Incremental Approach. Sixth SIGHAN Workshop on 
Chinese Language Processing, 2008. 
Jin-shea Kuo, Ying-kuei Yang. 2005. Incorporating 
Pronunciation Variation into Extraction of 
Transliterated-term Pairs from Web Corpora. Journal 
of Chinese Language and Computing, 15 (1): (33-
44). 
Chun-Jen Lee, Jason S. Chang. 2003. Acquisition of 
English-Chinese transliterated word pairs from 
parallel-aligned texts using a statistical machine 
transliteration model. Workshop on Building and 
Using Parallel Texts, HLT-NAACL-2003, 2003. 
Sara Noeman and Amgad Madkour. 2010. Language 
Independent Transliteration Mining System Using 
Finite State Automata Framework. ACL NEWS 
workshop 2010. 
R. Mahesh, K. Sinha. 2009. Automated Mining Of 
Names Using Parallel Hindi-English Corpus. 7th 
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, pages 48?54, 2009. 
Jong-Hoon Oh, Key-Sun Choi. 2006. Recognizing 
transliteration equivalents for enriching domain 
specific thesauri. 3rd Intl. WordNet Conf. (GWC-
06), pages 231?237, 2006. 
Jong-Hoon Oh, Hitoshi Isahara. 2006. Mining the Web 
for Transliteration Lexicons: Joint-Validation 
Approach. pp.254-261, 2006 IEEE/WIC/ACM Intl. 
Conf. on Web Intelligence (WI'06), 2006. 
Yan Qu, Gregory Grefenstette, David A. Evans. 2003. 
Automatic transliteration for Japanese-to-English text 
retrieval. SIGIR 2003:353-360 
Robert Russell. 1918. Specifications of Letters. US 
patent number 1,261,167. 
K Saravanan, A Kumaran. 2008. Some Experiments in 
Mining Named Entity Transliteration Pairs from 
Comparable Corpora. The 2nd Intl. Workshop on 
Cross Lingual Information Access: Addressing the 
Need of Multilingual Societies, 2008. 
Raghavendra Udupa, K. Saravanan, Anton Bakalov, 
Abhijit Bhole. 2009a. "They Are Out There, If You 
Know Where to Look": Mining Transliterations of 
OOV Query Terms for Cross-Language Information 
Retrieval. ECIR-2009, Toulouse, France, 2009. 
Raghavendra Udupa, K. Saravanan, A. Kumaran, and 
Jagadeesh Jagarlamudi. 2009b. MINT: A Method for 
Effective and Scalable Mining of Named Entity 
Transliterations from Large Comparable Corpora. 
EACL 2009. 
Raghavendra Udupa and Mitesh Khapra. 2010a. 
Transliteration Equivalence using Canonical 
Correlation Analysis. ECIR-2010, 2010. 
Raghavendra Udupa, Shaishav Kumar. 2010b. Hashing-
based Approaches to Spelling Correction of Personal 
Names. EMNLP 2010. 
Gae-won You, Seung-won Hwang, Young-In Song, 
Long Jiang, Zaiqing Nie. 2010. Mining Name 
Translations from Entity Graph Mapping. 
Proceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
430?439. 
Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li. 2008. 
Pivot Approach for Extracting Paraphrase Patterns 
from Bilingual Corpora. Proceedings of ACL-08: 
HLT, pages 780?788. 
1393
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 66?70,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transliteration by Sequence Labeling with Lattice Encodings and Reranking
Waleed Ammar Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{wammar,cdyer,nasmith}@cs.cmu.edu
Abstract
We consider the task of generating transliter-
ated word forms. To allow for a wide range of
interacting features, we use a conditional ran-
dom field (CRF) sequence labeling model. We
then present two innovations: a training objec-
tive that optimizes toward any of a set of possi-
ble correct labels (since more than one translit-
eration is often possible for a particular in-
put), and a k-best reranking stage to incorpo-
rate nonlocal features. This paper presents re-
sults on the Arabic-English transliteration task
of the NEWS 2012 workshop.
1 Introduction
Transliteration is the transformation of a piece of
text from one language?s writing system into an-
other. Since the transformation is mostly explained
as local substitutions, deletions, and insertions, we
treat word transliteration as a sequence labeling
problem (Ganesh et al, 2008; Reddy and Waxmon-
sky, 2009), using linear-chain conditional random
fields as our model (Lafferty et al, 2001; Sha and
Pereira, 2003). We tailor this model to the transliter-
ation task in several ways.
First, for the Arabic-English task, each Arabic in-
put is paired with multiple valid English transliter-
ation outputs, any of which is judged to be correct.
To effectively exploit these multiple references dur-
ing learning, we use a training objective in which
the model may favor some correct transliterations
over the others. Computationally efficient inference
is achieved by encoding the references in a lattice.
Second, inference for our first-order sequence la-
beling model requires a runtime that is quadratic in
the number of labels. Since our labels are character
n-grams in the target language, we must cope with
thousands of labels. To make the most of each in-
ference call during training, we apply a mini-batch
training algorithm which converges quickly.
Finally, we wish to consider some global features
that would render exact inference intractable. We
therefore use a reranking model (Collins, 2000).
We demonstrate the performance benefits of these
modifications on the Arabic-English transliteration
task, using the open-source library cdec (Dyer et
al., 2010)1 for learning and prediction.
2 Problem Description
In the NEWS 2012 workshop, the task is to gener-
ate a list of ten transliterations in a specified target
language for each named entity (in a known source
language) in the test set. A training set is provided
for each language pair. An entry in the training set
comprises a named entity in the source language and
one or more transliterations in the target language.
Zhang et al (2012) provides a detailed description
of the shared task.
3 Approach
3.1 Character Alignment
In order to extract source-target character map-
pings, we use m2m-aligner (Jiampojamarn et al,
2007),2 which implements a forward-backward al-
gorithm to sum over probabilities of possible charac-
ter sequence mappings, and uses Expectation Max-
imization to learn mapping probabilities. We allow
source characters to be deleted, but not target char-
acters. Parameters -maxX and -maxY are tuned on
a devevelopment set.
Our running example is the Arabic name EAdl
(in Buckwalter?s ASCII-based encoding of Arabic)
with two English transliterations: ADEL and ?ADIL.
The character alignment for the two pairs is shown
in Fig. 1.
1http://www.cdec-decoder.org
2http://code.google.com/p/m2m-aligner
66
AD
E
L
E
A
d
l
?
?
?
?
A
D
I
L
E
A
d
l
?
?
?
?
'
Arabic English Arabic English
Figure 1: Character alignment for transliterating EAdl to
ADEL and ?ADIL.
3.2 Sequence Labeling Scheme and Notation
We frame transliteration as a sequence labeling
problem. However, transliteration is not a one-to-
one process, meaning that a na??ve application of
one-label-per-token sequence models would be un-
likely to perform well. Previous work has taken
two different approaches. Reddy and Waxmonsky
(2009) first segment the input character sequence,
then use the segments to construct a transliteration
in the target language. Since segmentation errors
will compound to produce transliteration errors, we
avoid this. Ganesh et al (2008) do not require a seg-
mentation step, but their model does not allow for
many-to-one and many-to-many character mappings
which are often necessary.
Our approach overcomes both these shortcom-
ings: we have neither an explicit segmentation step,
nor do we forbid many-to-many mappings. In our
model, each character xi in the source-language in-
put x = ?x1, x2, . . . , xn? is assigned a label yi.
However, a label yi is a sequence of one or more
target-language characters, a special marker indi-
cating a deletion (), or a special marker indicat-
ing involvement in a many-to-one mapping (?), that
is, yi ? ?+ ? {, ?}, where ? is the target lan-
guage alphabet.3 When an input x has multiple al-
ternative reference transliterations, we denote the set
Y?(x) = {y1,y2, . . . ,yK}.
We map the many-to-many alignments produced
by m2m-aligner to one label for each input char-
acter, using the scheme in Table 1. Note that zero-
to-one alignments are not allowed.
The two reference label sequences for our running
example, which are constructed from the alignments
in Fig. 1 are:
3For an input type x, we only consider labels that were ac-
tually observed in the training data, which means the label set
is finite.
Type Alignment Labels
1:0 xi :  yi = 
1:1 xi : tj yi = tj
1:many xi : tj . . . tk yi = tj . . . tk
many:1 xi . . . xp : tj yp = tj
yi = ? ? ? = yp?1 = ?
many:many xi . . . xp : tj . . . tk yp = tj . . . tk
yi = ? ? ? = yp?1 = ?
Table 1: Transforming alignments to sequence labels.
x y1 y2
E ? ?
A A A
d DE DI
l L L
Of key importance in our model is defining, for
each source character, the set of labels that can be
considered for it. For each source character, we add
all labels consistent with character alignments to the
lexicon.
3.3 Model
Our model for mapping from inputs to outputs is
a conditional random field (Lafferty et al, 2001),
which defines the conditional probability of every
possible sequence labeling y of a sequence x with
the parametric form:
p?(y | x) ? exp
?|x|
i=1 ? ? f(x, yi, yi?1) (1)
where f is a vector of real-valued feature functions.
3.4 Features
The feature functions used are instantiated by apply-
ing templates shown in Table 2 to each position i in
the input string x.
3.5 Parameter Learning
Given a training dataset of pairs {?xj ,yj?}
`
j=1 (note
that each y is derived from the max-scoring char-
acter alignment), a CRF is trained to maximize the
regularized conditional log-likelihood:
max
?
L{1,...,`}(?) ,
?`
j=1 log p?(yj | xj) ? C||?||
2
2
(2)
The regularization strength hyperparameter is tuned
on development data. On account of the large data
sizes and large label sets in several language pairs
67
Feature Template Description
U1:yi-xi,
U2:yi-xi?1-xi,
U3:yi-xi-xi+1, moving window of unigram,
U4:yi-xi?2-xi?1-xi, bigram and trigram context
U5:yi-xi?1-xi-xi+1,
U6:yi-xi-xi+1-xi+2
U7:yi, B1:yi-yi?1 label unigrams and bigrams
U8:|yi| label size (in characters)
Table 2: Feature templates for features extracted from
transliteration hypotheses. The SMALLCAPS prefixes
prevent accidental feature collisions.
(Table 3), batch optimization with L-BFGS is in-
feasible. Therefore, we use a variant of the mini-
batch L-BFGS learning approach proposed by Le
et al (2011). This algorithm uses a series of ran-
domly chosen mini-batches B(1),B(2), . . ., each a
subset of {1, . . . , `}, to produce a series of weights
?(1),?(2), . . . by running N iterations of L-BFGS
on each mini-batch to compute the following:
max?(i) LB(i)(?
(i)) ? T??(i) ? ?(i?1)?22 (3)
The T parameter controls how far from the previ-
ous weights the optimizer can move in any particu-
lar mini-batch4. We use mini-batch sizes of 5, and
start training with a small value of T and increase it
as we process more iterations. This is equivalent to
reducing the step-size with the number of iterations
in conventional stochastic learning algorithms.
Language Pair Unique Labels
Arabic-English 1,240
Chinese-English 2,985
Thai-English 1,771
English-Chinese 1,321
English-Japanese Kanji 4,572
Table 3: Size of the label set in some language pairs.
3.6 Using Multiple Reference Transliterations
In some language pairs, NEWS-2012 provides mul-
tiple reference transliterations in the training set. In
this section, we discuss two possibilities for using
these multiple references to train our transliteration
4When T = 0, our learning algorithm is identical to the L-
BFGS mini-batch algorithm of Le et al (2011); however, we
find that more rapid convergence is possible when T > 0.
'
A
DI
L
DE
A
?
Figure 2: Lattice encoding two transliterations of EAdl:
ADEL and ?ADIL.
model. The first possibility is to create multiple in-
dependent training inputs for each input x, one for
each correct transliteration in Y?(x). Using this ap-
proach, with K different transliterations, the CRF
training objective will attempt to assign probability
1
K to each correct transliteration, and 0 to all others
(modulo regularization).
Alternatively, we can train the model to maximize
the marginal probability assigned by the model to
the set of correct labels Y? = {y1, . . . ,yK}. That
is, we assume a set of training data {(xj ,Y?j )}
`
j=1
and replace the standard CRF objective with the fol-
lowing (Dyer, 2009):5
max?
?`
j=1 log
?
y?Y?j
p?(y | xj) ? C||?||22 (4)
This learning objective has more flexibility. It can
maximize the likelihood of the training data by giv-
ing uniform probability to each reference transliter-
ation for a given x, but it does not have to. In effect,
we do not care how probability mass is distributed
among the correct labels. Our hope is that if some
transliterations are difficult to model?perhaps be-
cause they are incorrect?the model will be able to
disregard them.
To calculate the marginal probability for each xj ,
we represent Y?(x) as a label lattice, which is sup-
ported as label reference format in cdec. A fur-
ther computational advantage is that each x in the
training data is now only a single training instance
meaning that fewer forward-backward evaluations
are necessary. The lattice encoding of both translit-
erations of our running example is shown in Fig. 2.
3.7 Reranking
CRFs require feature functions to be ?local? to
cliques in the underlying graphical model. One way
to incorporate global features is to first decode the
5Unlike the standard CRF objective in eq. 2, the marginal
probability objective is non-convex, meaning that we are only
guaranteed to converge to a local optimum in training.
68
k-best transliterations using the CRF, then rerank
based on global features combined with the CRF?s
conditional probability of each candidate. We ex-
periment with three non-local features:
Character language model: an estimate of
pcharLM (y) according to a trigram character lan-
guage model (LM). While a bigram LM can be fac-
tored into local features in a first order CRF, higher
n-gram orders require a higher-order CRF.
Class language model: an estimate of pclassLM (y),
similar to the character LM, but collapses characters
which have a similar phonetic function into one class
(vowels, consonants, and hyphens/spaces). Due to
the reduced number of types in this model, we can
train a 5-gram LM.
Transliteration length: an estimate of plen(|y| |
|x|) assuming a multinomial distribution with pa-
rameters estimated using transliteration pairs of the
training set.
The probabilistic model for each of the global
features is trained using training data provided for
the shared task. The reranking score is a linear
combination of log pcrf (y | x), log pcharLM (y),
log pclassLM (y) and log plen(|y| | |x|). Linear co-
efficients are optimized using simulated annealing,
optimizing accuracy of the 1-best transliteration in a
development set. k-best lists are extracted from the
CRF trellis using the lazy enumeration algorithm of
Huang and Chiang (2005).
4 Experiments
We tested on the NEWS 2012 Arabic-English
dataset. The train, development, and test sets con-
sist of 27,177, 1,292, and 1,296 source named enti-
ties, respectively, with an average 9.6 references per
name in each case.
Table 4 summarizes our results using the ACC
score (Zhang et al, 2012) (i.e., word accuracy in
top-1). ?Basic CRF? is the model with mini-batch
learning and represents multiple reference translit-
erations as independent training examples. We man-
ually tuned the number of training examples and
LBFGS iterations per mini-batch to five and eight,
respectively. ?CRF w/lattice? compactly represents
the multiple references in a lattice, as detailed in
?3.6. We consider reranking using each of the three
global features along with the CRF, as well as the
Model Ar-En
Basic CRF 23.5
CRF w/lattice 37.0
CRF w/lattice; rerank pcrf , pcharLM 40.7
CRF w/lattice; rerank pcrf , pclassLM 38.4
CRF w/lattice; rerank pcrf , plen 37.3
CRF w/lattice, rerank all four 42.8
Table 4: Model performance, measured in word accuracy
in top-1 (ACC, %).
full set of four features.
Maximizing the marginal conditional likelihood
of the set of alternative transliterations (rather than
maximizing each alternative independently) shows
a dramatic improvement in transliteration accuracy
for Arabic-English. Moreover, in Arabic-English
the basic CRF model converges in 120K mini-batch
iterations, which is, approximately, seven times the
number of iterations needed for convergence with
lattice-encoded labels. A model converges when its
ACC score on the development set ceases to improve
in 800 mini-batch iterations. Results also show that
reranking a k-best list of only five transliterations
with any of the global features improves accuracy.
Using all the features together to rerank the k-best
list gives further improvements.
5 Conclusion
We built a CRF transliteration model that allows
for many-to-many character mappings. We address
limitations of CRFs using mini-batch learning and
reranking techniques. We also show how to relax
the learning objective when the training set contains
multiple references, resulting in faster convergence
and improved transliteration accuracy.
We suspect that including features of higher-order
n-gram labels would help improve transliteration ac-
curacy further, but it makes inference intractable due
to the large set of labels. In future work, coarse
transformations of label n-grams might address this
problem.
Acknowledgments
This research was supported in part by the U.S. Army
Research Laboratory and the U.S. Army Research Office
under contract/grant number W911NF-10-1-0533. We
thank anonymous reviewers for the valuable comments.
69
References
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
S. Ganesh, S. Harsha, P. Pingali, and V. Varma. 2008.
Statistical transliteration for cross language informa-
tion retrieval using HMM alignment and CRF. In
Proc. of the 2nd Workshop On Cross Lingual Infor-
mation Access.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
In Proc. of the 9th International Workshop on Parsing
Technologies.
S. Jiampojamarn, G. Kondrak, and T. Sherif. 2007. Ap-
plying many-to-many alignments and hidden Markov
models to letter-to-phoneme conversion. In Proc. of
NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
Q. V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow,
and A. Y. Ng. 2011. On optimization methods for
deep learning. In Proc. of ICML.
S. Reddy and S. Waxmonsky. 2009. Substring-based
transliteration with conditional random fields. In Proc.
of the Named Entities Workshop.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of NAACL-HLT.
M. Zhang, H. Li, M. Liu, and A. Kumaran. 2012.
Whitepaper of NEWS 2012 shared task on machine
transliteration.
70
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2014
Austin Matthews Waleed Ammar Archna Bhatia Weston Feely
Greg Hanneman Eva Schlinger Swabha Swayamdipta Yulia Tsvetkov
Alon Lavie Chris Dyer
?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?
Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submitted
to the 2014 WMT shared translation task.
We participated in two language pairs,
German?English and Hindi?English. Our
innovations include: a label coarsening
scheme for syntactic tree-to-tree transla-
tion, a host of new discriminative features,
several modules to create ?synthetic trans-
lation options? that can generalize beyond
what is directly observed in the training
data, and a method of combining the out-
put of multiple word aligners to uncover
extra phrase pairs and grammar rules.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute partici-
pated in two language pairs for the 2014 Workshop
on Machine Translation shared translation task:
German?English and Hindi?English. Our systems
showcase our multi-phase approach to translation,
in which synthetic translation options supple-
ment the default translation rule inventory that is
extracted from word-aligned training data.
In the German?English system, we used our
compound splitter (Dyer, 2009) to reduce data
sparsity, and we allowed the translator to back
off to translating lemmas when it detected case-
inflected OOVs. We also demonstrate our group?s
syntactic system with coarsened nonterminal types
(Hanneman and Lavie, 2011) as a contrastive
German?English submission.
In both the German?English and Hindi?English
systems, we used an array of supplemental ideas to
enhance translation quality, ranging from lemma-
tization and synthesis of inflected phrase pairs to
novel reordering and rule preference features.
2 Core System Components
The decoder infrastructure we used was cdec
(Dyer et al., 2010). For our primary systems,
all data was tokenized using cdec?s tokenization
tool. Only the constrained data resources pro-
vided for the shared task were used for training
both the translation and language models. Word
alignments were generated using both FastAlign
(Dyer et al., 2013) and GIZA++ (Och and Ney,
2003). All our language models were estimated
using KenLM (Heafield, 2011). Translation model
parameters were chosen using MIRA (Eidelman,
2012) to optimize BLEU (Papineni et al., 2002)
on a held-out development set.
Our data was filtered using qe-clean
(Denkowski et al., 2012), with a cutoff of
two standard deviations from the mean. All
data was left in fully cased form, save the first
letter of each segment, which was changed to
whichever form the first token more commonly
used throughout the data. As such, words like The
were lowercased at the beginning of segments,
while words like Obama remained capitalized.
Our primary German?English and Hindi?
English systems were Hiero-based (Chiang,
2007), while our contrastive German?English sys-
tem used cdec?s tree-to-tree SCFG formalism.
Before submitting, we ran cdec?s implementa-
tion of MBR on 500-best lists from each of our
systems. For both language pairs, we used the
Nelder?Mead method to optimize the MBR pa-
rameters. In the German?English system, we ran
MBR on 500 hypotheses, combining the output of
the Hiero and tree-to-tree systems.
The remainder of the paper will focus on our
primary innovations in the two language pairs.
142
3 Common System Improvements
A number of our techniques were used for both our
German?English and Hindi?English primary sub-
missions. These techniques each fall into one of
three categories: those that create translation rules,
those involving language models, or those that add
translation features. A comparison of these tech-
niques and their performance across the two lan-
guage pairs can be found in Section 6.
3.1 Rule-Centric Enhancements
While many of our methods of enhancing the
translation model with extra rules are language-
specific, three were shared between language
pairs.
First, we added sentence-boundary tokens <s>
and </s> to the beginning and end of each line in
the data, on both the source and target sides.
Second, we aligned all of our training data us-
ing both FastAlign and GIZA++ and simply con-
catenated two copies of the training corpus, one
aligned with each aligner, and extracted rules from
the resulting double corpus.
Third, we hand-wrote a list of rules that trans-
form numbers, dates, times, and currencies into
well-formed English equivalents, handling differ-
ences such as the month and day reversal in dates
or conversion from 24-hour time to 12-hour time.
3.2 Employed Language Models
Each of our primary systems uses a total of three
language models.
The first is a traditional 4-gram model gen-
erated by interoplating LMs built from each of
the available monolingual corpora. Interpolation
weights were calculated used the SRILM toolkit
(Stolcke, 2002) and 1000 dev sentences from the
Hindi?English system.
The second is a model trained on word clus-
ters instead of surface forms. For this we mapped
the LM vocabulary into 600 clusters based on the
algorithm of Brown et al. (1992) and then con-
structed a 7-gram LM over the resulting clusters,
allowing us to capture more context than our tra-
ditional surface-form language model.
The third is a bigram model over the source side
of each language?s respective bitext. However, at
run time this LM operates on the target-side out-
put of the translator, just like the other two. The
intuition here is that if a source-side LM likes our
output, then we are probably passing through more
than we ought to.
Both source and target surface-form LM used
modified Kneser-Ney smoothing (Kneser and Ney,
1995), while the model over Brown clusters
(Brown et al., 1992) used subtract-0.5 smoothing.
3.3 New Translation Features
In addition to the standard array of features, we
added four new indicator feature templates, lead-
ing to a total of nearly 150,000 total features.
The first set consists of target-side n-gram fea-
tures. For each bigram of Brown clusters in the
output string generated by our translator, we fire
an indicator feature. For example, if we have the
sentence, Nato will ihren Einfluss im Osten st?arken
translating as NATO intends to strengthen its influ-
ence in the East, we will fire an indicator features
NGF C367 C128=1, NGF C128 C31=1, etc.
The second set is source-language n-gram fea-
tures. Similar to the previous feature set, we fire
an indicator feature for each ngram of Brown clus-
ters in the output. Here, however, we use n = 1,
and we use the map of source language words to
Brown clusters, rather than the target language?s,
despite the fact that this is examining target lan-
guage output. The intuition here is to allow this
feature to penalize passthroughs differently de-
pending on their source language Brown cluster.
For example, passing through the German word
zeitung (?newspaper?) is probably a bad idea, but
passing through the German word Obama proba-
bly should not be punished as severely.
The third type of feature is source path features.
We can imagine translation as a two-step process
in which we first permute the source words into
some order, then translate them phrase by phrase.
This set of features examines that intermediate
string in which the source words have been per-
muted. Again, we fire an indicator feature for each
bigram in this intermediate string, this time using
surface lexical forms directly instead of first map-
ping them to Brown clusters.
Lastly, we create a new type of rule shape fea-
ture. Traditionally, rule shape features have indi-
cated, for each rule, the sequence of terminal and
non-terminal items on the right-hand side. For ex-
ample, the rule [X] ? der [X] :: the [X] might
have an indicator feature Shape TN TN, where
T represents a terminal and N represents a non-
terminal. One can also imagine lexicalizing such
rules by replacing each T with its surface form.
We believe such features would be too sparse, so
instead of replacing each terminal by its surface
form, we instead replace it with its Brown cluster,
143
creating a feature like Shape C37 N C271 N.
4 Hindi?English Specific Improvements
In addition to the enhancements common to the
two primary systems, our Hindi?English system
includes improved data cleaning of development
data, a sophisticated linguistically-informed tok-
enization scheme, a transliteration module, a syn-
thetic phrase generator that improves handling of
function words, and a synthetic phrase generator
that leverages source-side paraphrases. We will
discuss each of these five in turn.
4.1 Development Data Cleaning
Due to a scarcity of clean development data, we
augmented the 520 segments provided with 480
segments randomly drawn from the training data
to form our development set, and drew another
random 1000 segments to serve as a dev test set.
After observing large discrepencies between the
types of segments in our development data and the
well-formed news domain sentences we expected
to be tested on, we made the decision to prune our
tuning set by removing any segment that did not
appear to be a full sentence on both the Hindi and
English sides. While this reduced our tuning set
from 1000 segments back down to 572 segments,
we believe it to be the single largest contributor to
our success on the Hindi?English translation task.
4.2 Nominal Normalization
Another facet of our system was normalization of
Hindi nominals. The Hindi nominal system shows
much more morphological variation than English.
There are two genders (masculine and feminine)
and at least six noun stem endings in pronuncia-
tion and 10 in writing.
The pronominal system also is much richer than
English with many variants depending on whether
pronouns appear with case markers or other post-
positions.
Before normalizing the nouns and pronouns, we
first split these case markers / postpositions from
the nouns / pronouns to result in two words in-
stead of the original combined form. If the case
marker was n (ne), the ergative case marker in
Hindi, we deleted it as it did not have any trans-
lation in English. All the other postpositions were
left intact while splitting from and normalizing the
nouns and pronouns.
These changes in stem forms contribute to the
sparsity in data; hence, to reduce this sparsity, we
construct for each input segment an input lattice
that allows the decoder to use the split or original
forms of all nouns or pronouns, as well as allowing
it to keep or delete the case marker ne.
4.3 Transliteration
We used the 12,000 Hindi?English transliteration
pairs from the ACL 2012 NEWS workshop on
transliteration to train a linear-chained CRF tag-
ger
1
that labels each character in the Hindi token
with a sequence of zero or more English characters
(Ammar et al., 2012). At decoding, unseen Hindi
tokens are fed to the transliterator, which produces
the 100 most probable transliterations. We add
a synthetic translation option for each candidate
transliteration.
In addition to this sophisticated transliteration
scheme, we also employ a rule-based translitera-
tor that specifically targets acronyms. In Hindi,
many acronyms are spelled out phonetically, such
as NSA being rendered as enese (en.es.e). We
detected such words in the input segments and
generated synthetic translation options both with
and without periods (e.g. N.S.A. and NSA).
4.4 Synthetic Handling of Function Words
In different language pairs, individual source
words may have many different possible trans-
lations, e.g., when the target language word has
many different morphological inflections or is sur-
rounded by different function words that have no
direct counterpart in the source language. There-
fore, when very large quantities of parallel data
are not available, we can expect our phrasal inven-
tory to be incomplete. Synthetic translation option
generation seeks to fill these gaps using secondary
generation processes that exploit existing phrase
pairs to produce plausible phrase translation alter-
natives that are not directly extractable from the
training data (Tsvetkov et al., 2013; Chahuneau et
al., 2013).
To generate synthetic phrases, we first remove
function words from the source and target sides
of existing non-gappy phrase pairs. We manually
constructed English and Hindi lists of common
function words, including articles, auxiliaries, pro-
nouns, and adpositions. We then employ the
SRILM hidden-ngram utility (Stolcke, 2002) to re-
store missing function words according to an n-
gram language model probability, and add the re-
sulting synthetic phrases to our phrase table.
1
https://github.com/wammar/transliterator
144
4.5 Paraphrase-Based Synthetic Phrases
We used a graph-based method to obtain transla-
tion distributions for source phrases that are not
present in the phrase table extracted from the par-
allel corpus. Monolingual data is used to construct
separate similarity graphs over phrases (word se-
quences or n-grams), using distributional features
extracted from the corpora. The source similar-
ity graph consists of phrase nodes representing se-
quences of words in the source language. In our
instance, we restricted the phrases to bigrams, and
the bigrams come from both the phrase table (the
labeled phrases) and from the evaluation set but
not present in the phrase table (unlabeled phrases).
The labels for these source phrases, namely the
target phrasal inventory, can also be represented
in a graph form, where the distributional features
can also be computed from the target monolingual
data. Translation information is then propagated
from the labeled phrases to the unlabeled phrases
in the source graph, proportional to how similar
the phrases are to each other on the source side,
as well as how similar the translation candidates
are to each other on the target side. The newly
acquired translation distributions for the unlabeled
phrases are written out to a secondary phrase table.
For more information, see Saluja et al. (2014).
5 German?English Specific
Improvements
Our German?English system also had its own
suite of tricks, including the use of ?pseudo-
references? and special handling of morphologi-
cally inflected OOVs.
5.1 Pseudo-References
The development sets provided have only a sin-
gle reference, which is known to be sub-optimal
for tuning of discriminative models. As such,
we use the output of one or more of last year?s
top performing systems as pseudo-references dur-
ing tuning. We experimented with using just one
pseudo-reference, taken from last year?s Spanish?
English winner (Durrani et al., 2013), and with
using four pseudo-references, including the out-
put of last year?s winning Czech?English, French?
English, and Russian?English systems (Pino et al.,
2013).
5.2 Morphological OOVs
Examination of the output of our baseline sys-
tems lead us to conclude that the majority of our
system?s OOVs were due to morphologically in-
flected nouns in the input data, particularly those
in genitive case. As such, for each OOV in the
input, we attempt to remove the German genitive
case marker -s or -es. We then run the resulting
form f through our baseline translator to obtain a
translation e of the lemma. Finally, we add two
translation rules to our translation table: f ? e,
and f ? e?s.
6 Results
As we added each feature to our systems, we
first ran a one-off experiment comparing our base-
line system with and without each individual fea-
ture. The results of that set of experiments are
shown in Table 1 for Hindi?English and Table 2
for German?English. Features marked with a *
were not included in our final system submission.
The most surprising result is the strength of
our Hindi?English baseline system. With no extra
bells or whistles, it is already half a BLEU point
ahead of the second best system submitted to this
shared task. We believe this is due to our filter-
ing of the tuning set, which allowed our system to
generate translations more similar in length to the
final test set.
Another interesting result is that only one fea-
ture set, namely our rule shape features based on
Brown clusters, helped on the test set in both lan-
guage pairs. No feature hurt the BLEU score on
the test set in both language pairs, meaning the
majority of features helped in one language and
hurt in the other.
If we compare results on the tuning sets, how-
ever, some clearer patterns arise. Brown cluster
language models, n-gram features, and our new
rule shape features all helped.
Furthermore, there were a few features, such as
the Brown cluster language model and tuning to
Meteor (Denkowski and Lavie, 2011), that helped
substantially in one language pair while just barely
hurting the other. In particular, the fact that tuning
to Meteor instead of BLEU can actually help both
BLEU and Meteor scores was rather unexpected.
7 German?English Syntax System
In addition to our primary German?English sys-
tem, we also submitted a contrastive German?
English system showcasing our group?s tree-to-
tree syntax-based translation formalism.
145
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 15.7 25.3 68.0 11.4 22.9 70.3
*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3
Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0
Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2
Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3
Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8
*Source LM 14.2 25.1 72.1 11.3 23.0 72.3
N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2
Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5
Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4
Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6
Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3
CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1
Acronym Translit. 15.8 25.8 68.8 12.4 23.4 70.2
Synth. Func. Words 15.7 25.3 67.8 11.4 22.8 70.4
Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9
Final Submission 16.7
Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?
English system. Each line is the baseline plus that one feature, non-cumulatively. Lines marked with a *
were not included in our final WMT submission.
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 25.3 30.4 52.6 26.2 31.3 53.6
*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4
Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3
Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6
Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4
Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2
*Source LM 25.8 30.6 52.4 26.4 31.5 53.4
N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0
Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4
Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3
Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2
One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3
*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1
OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3
Final Submission 27.1
Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero
German?English system. Each line is the baseline plus that one feature, non-cumulatively.
Dev (2013) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 20.98 29.81 58.47 18.65 28.72 61.80
+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16
+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87
+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51
+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18
+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73
Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?English
system. The system in the bottom line was submitted to WMT as a contrastive entry.
7.1 Basic System Construction
Since all training data for the tree-to-tree system
must be parsed in addition to being word-aligned,
we prepared separate copies of the training, tun-
ing, and testing data that are more suitable for in-
put into constituency parsing. Importantly, we left
the data in its original mixed-case format. We used
the Stanford tokenizer to replicate Penn Treebank
tokenization on the English side. On the German
side, we developed new in-house normalization
and tokenization script.
We filtered tokenized training sentences by sen-
146
tence length, token length, and sentence length ra-
tio. The final corpus for parsing and word align-
ment contained 3,897,805 lines, or approximately
86 percent of the total training resources released
under the WMT constrained track. Word align-
ment was carried out using FastAlign (Dyer et
al., 2013), while for parsing we used the Berke-
ley parser (Petrov et al., 2006).
Given the parsed and aligned corpus, we ex-
tracted synchronous context-free grammar rules
using the method of Hanneman et al. (2011).
In addition to aligning subtrees that natively ex-
ist in the input trees, our grammar extractor also
introduces ?virtual nodes.? These are new and
possibly overlapping constituents that subdivide
regions of flat structure by combining two adja-
cent sibling nodes into a single nonterminal for
the purposes of rule extraction. Virtual nodes
are similar in spirit to the ?A+B? extended cate-
gories of SAMT (Zollmann and Venugopal, 2006),
and their nonterminal labels are constructed in the
same way, but with the added restriction that they
do not violate any existing syntactic structure in
the parse tree.
7.2 Improvements
Nonterminals in our tree-to-tree grammar are
made up of pairs of symbols: one from the source
side and one from the target side. With virtual
nodes included, this led to an initial German?
English grammar containing 153,219 distinct non-
terminals ? a far larger set than is used in SAMT,
tree-to-string, string-to-tree, or Hiero systems. To
combat the sparsity introduce by this large nonter-
minal set, we coarsened the label set with an ag-
glomerative label-clustering technique(Hanneman
and Lavie, 2011; Hanneman and Lavie, 2013).
The stopping point was somewhat arbitrarily cho-
sen to be a grammar of 916 labels.
Table 3 shows a significant improvement in
translation quality due to coarsening the label set:
approximately +1.8 BLEU, +0.6 Meteor, and ?1.6
TER on our dev test set, newtest2012.
2
In the MERT runs, however, we noticed that the
length of the MT output can be highly variable,
ranging on the tuning set from a low of 92.8% of
the reference length to a high of 99.1% in another.
We were able to limit this instability by tuning to
Meteor instead of BLEU. Aside from a modest
2
We follow the advice of Clark et al. (2011) and eval-
uate our tree-to-tree experiments over multiple independent
MERT runs. All scores in Table 3 are averages of two or
three runs, depending on the row.
score improvement, we note that the variability in
length ratio is reduced from 6.3% to 2.8%.
Specific difficulties of the German?English lan-
guage pair led to three additional system compo-
nents to try to combat them.
First, we introduced a second language model
trained on Brown clusters instead of surface forms.
Next we attempted to overcome the sparsity
of German input by making use of cdec?s lattice
input functionality introduce compound-split ver-
sions of dev and test sentences.
Finally, we attempted to improve our grammar?s
coverage of new German words by introducing
synthetic rules for otherwise out-of-vocabulary
items. Each token in a test sentence that the gram-
mar cannot translate generates a synthetic rule al-
lowing the token to be translated as itself. The left-
hand-side label is decided heuristically: a (coars-
ened) ?noun? label if the German OOV starts with
a capital letter, a ?number? label if the OOV con-
tains only digits and select punctuation characters,
an ?adjective? label if the OOV otherwise starts
with a lowercase letter or a number, or a ?symbol?
label for anything left over.
The effect of all three of these improvements
combined is shown in the fourth row of Table 3.
By default our previous experiments were per-
formed with a span limit of 12 tokens. Increasing
this limit to 15 has a mixed effect on metric scores,
as shown in the fifth row of Table 3. Since two out
of three metrics report improvement, we left the
longer span limit in effect in our final system.
Our final improvement was to augment our tun-
ing set with the same set of pseudo-references
as our Hiero systems. We found that using one
pseudo-reference versus four pseudo-references
had negligible effect on the (single-reference) tun-
ing scores, but four produced a better improve-
ment on the test set.
The best MERT run of this final system (bottom
line of Table 3) was submitted to the WMT 2014
evaluation as a contrastive entry.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
147
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine transla-
tion systems for european language pairs.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 406?414. Association for Computational Lin-
guistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-
pert, Federico Flego, and William Byrne. 2013.
The university of cambridge russian-english system
at wmt13.
148
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, Maryland, June.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
149
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 80?86,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The CMU Submission for the Shared Task on Language Identification in
Code-Switched Data
Chu-Cheng Lin Waleed Ammar Lori Levin Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{chuchenl,wammar,lsl,cdyer}@cs.cmu.edu
Abstract
We describe the CMU submission for
the 2014 shared task on language iden-
tification in code-switched data. We
participated in all four language pairs:
Spanish?English, Mandarin?English,
Nepali?English, and Modern Standard
Arabic?Arabic dialects. After describing
our CRF-based baseline system, we
discuss three extensions for learning from
unlabeled data: semi-supervised learning,
word embeddings, and word lists.
1 Introduction
Code switching (CS) occurs when a multilingual
speaker uses more than one language in the same
conversation or discourse. Automatic idenefica-
tion of the points at which code switching occurs
is important for two reasons: (1) to help sociolin-
guists analyze the frequency, circumstances and
motivations related to code switching (Gumperz,
1982), and (2) to automatically determine which
language-specific NLP models to use for analyz-
ing segments of text or speech.
CS is pervasive in social media due to its in-
formal nature (Lui and Baldwin, 2014). The first
workshop on computational approaches to code
switching in EMNLP 2014 organized a shared task
(Solorio et al., 2014) on identifying code switch-
ing, providing training data of multilingual tweets
with token-level language-ID annotations. See
?2 for a detailed description of the shared task.
This short paper documents our submission in the
shared task.
We note that constructing a CS data set that is
annotated at the token level requires remarkable
manual effort. However, collecting raw tweets is
easy and fast. We propose leveraging both labeled
and unlabeled data in a unified framework; condi-
tional random field autoencoders (Ammar et al.,
2014). The CRF autoencoder framework consists
of an encoding model and a reconstruction model.
The encoding model is a linear-chain conditional
random field (CRF) (Lafferty et al., 2001) which
generates a sequence of labels, conditional on a
token sequence. Importantly, the parameters of
the encoding model can be interpreted in the same
way a CRF model would. This is in contrary to
generative model parameters which explain both
the observation sequence and the label sequence.
The reconstruction model, on the other hand, inde-
pendently generates the tokens conditional on the
corresponding labels. Both labeled and unlabeled
data can be efficiently used to fit parameters of this
model, minimizing regularized log loss. See ?4.1
for more details.
After modeling unlabeled token sequences, we
explore two other ways of leveraging unlabeled
data: word embeddings and word lists. The word
embeddings we use capture monolingual distribu-
tional similarities and therefore may be indicative
of a language (see ?4.2). A word list, on the other
hand, is a collection of words which have been
manually or automatically constructed and share
some property (see ?4.3). For example, we extract
the set of surface forms in monolingual corpora.
In ?5, we describe the experiments and discuss
results. According to the results, modeling unla-
beled data using CRF autoencoders did not im-
prove prediction accuracy. Nevertheless, more ex-
periments need to be run before we can conclude
this setting. On the positive side, word embed-
dings and word lists have been shown to improve
CS prediction accuracy, provided they have decent
coverage of tokens in the test set.
2 Task Description
The shared task training data consists of code?
switched tweets with token-level annotations.
The data is organized in four language pairs:
English?Spanish (En-Es), English?Nepali (En-
80
Ne), Mandarin?English (Zh-En) and Modern
Standard Arabic?Arabic dialects (MSA-ARZ).
Table 1 shows the size of the data sets provided
for the shared task in each language pair.
For each tweet in the data set, the user ID, tweet
ID, and a list of tokens? start offset and end offset
are provided. Each token is annotated with one
of the following labels: lang1, lang2, ne (i.e.,
named entities), mixed (i.e., mixed parts of lang1
and lang2), ambiguous (i.e., cannot be identified
given context), and other.
Two test sets were used to evaluate each sub-
mission for the shared task in each language pair.
The first test set consists of Tweets, similar to the
training set. The second test set consists of token
sequences from a surprise genre. Since partici-
pants were not given the test sets, we only report
results on a Twitter test set (a subset of the data
provided for shared task participants). Statistics
of our train/test data splits are given in Table 5.
lang. pair split tweets tokens users
En?Ne all 9, 993 146, 053 18
train 7, 504 109, 040 12
test 2, 489 37, 013 6
En?Es all 11, 400 140, 738 9
train 7, 399 101, 451 6
test 4, 001 39, 287 3
Zh?En all 994 17, 408 995
train 662 11, 677 663
test 332 5, 731 332
MSA?ARZ all 5, 862 119, 775 7
train 4, 800 95, 352 6
test 1, 062 24, 423 1
Table 1: Total number of tweets, tokens, and Twit-
ter user IDs for each language pair. For each lan-
guage pair, the first line represents all data pro-
vided to shared task participants. The second and
third lines represent our train/test data split for the
experiments reported in this paper. Since Twit-
ter users are allowed to delete their tweets, the
number of tweets and tokens reported in the third
and fourth columns may be less than the number
of tweets and tokens originally annotated by the
shared task organizers.
3 Baseline System
We model token-level language ID as a sequence
of labels using a linear-chain conditional ran-
dom field (CRF) (Lafferty et al., 2001) described
in ?3.1 with the features in ?3.2.
3.1 Model
A linear-chain CRF models the conditional proba-
bility of a label sequence y given a token sequence
x and given extra context ?, as follows:
p(y | x,?) =
exp?
>
?
|x|
i=1
f(x, y
i
, y
i?1
,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i
, y
?
i?1
,?)
where ? is a vector of feature weights, and f is
a vector of local feature functions. We use ? to
explicitly represent context information necessary
to compute the feature functions described below.
In a linear-chain structure, y
i
only depends on
observed variables x,? and the neighboring labels
y
i?1
and y
i+1
. Therefore, we can use dynamic
programming to do inference in run time that is
quadratic in the number of unique labels and lin-
ear in the sequence length. We use L-BFGS to
learn the feature weights ?, maximizing the L
2
-
regularized log-likelihood of labeled examples L:
``
supervised
(?) =
c
L
2
||?||
2
2
+
?
?x,y??L
log p(y | x,?)
After training the model, we use again use dy-
namic programming to find the most likely label
sequence, for each token sequence in the test set.
3.2 Features
We use the following features in the baseline sys-
tem:
? character n-grams (loweredcased tri- and quad-
grams)
? prefixes and suffixes of lengths 1, 2, 3 and 4
? unicode page of the first character
1
? case (first-character-uppercased vs. all-
characters-uppercased vs. all-characters-
alphanumeric)
? tweet-level language ID predictions from two
off-the-shelf language identifiers: cld2
2
and
ldig
3
1
http://www.unicode.org/charts/
2
https://code.google.com/p/cld2/
3
https://github.com/shuyo/ldig
81
encod
ing 
recon
struc
tion 
x 
yi-1 yi yi+1 
xi-1 xi xi+1 ? ? ? 
? 
Figure 1: A diagram of the CRF autoencoder
4 Using Unlabeled Data
In ?3, we learn the parameters of the CRF model
parameters in a standard fully supervised fashion,
using labeled examples in the training set. Here,
we attempt to use unlabeled examples to improve
our system?s performance in three ways: model-
ing unlabeled token sequences in the CRF autoen-
coder framework, word embeddings, and word
lists.
4.1 CRF Autoencoders
A CRF autoencoder (Ammar et al., 2014) consists
of an input layer, an output layer, and a hidden
layer. Both input and output layer represent the
observed token sequence. The hidden layer rep-
resents the label sequence. Fig. 1 illustrates the
model dependencies for sequence labeling prob-
lems with a first-order Markov assumption. Con-
ditional on an observation sequence x and side in-
formation ?, a traditional linear-chain CRF model
is used to generate the label sequence y. The
model then generates x? which represents a recon-
struction of the original observation sequence. El-
ements of this reconstruction (i.e., x?
i
) are then in-
dependently generated conditional on the corre-
sponding label y
i
using simple categorical distri-
butions.
The parametric form of the model is given by:
p(y, x? | x,?) =
|x|
?
i=1
?
x?
i
|y
i
?
exp?
>
?
|x|
i=1
f(x, y
i?1
, y
i
, i,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i?1
, y
?
i
, i,?)
where ? is a vector of CRF feature weights, f is a
vector of local feature functions (we use the same
features described in ?3.2), and ?
x?
i
|y
i
are categor-
ical distribution parameters of the reconstruction
model representing p(x?
i
| y
i
).
We can think of a label sequence as a low-
cardinality lossy compression of the correspond-
ing token sequence. CRF autoencoders explic-
itly model this intuition by creating an information
bottleneck where label sequences are required to
regenerate the same token sequence despite their
limited capacity. Therefore, when only unlabeled
examples U are available, we train CRF autoen-
coders by maximizing the regularized likelihood
of generating reconstructions x?, conditional on x,
marginalizing values of label sequences y:
``
unsupervised
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)
where R
Dirichlet
is a regularizer based on a vari-
ational approximation of a symmetric Dirichlet
prior with concentration parameter ? for the re-
construction parameters ?.
Having access to labeled examples, it is easy to
modify this objective to learn from both labeled
and unlabeled examples as follows:
``
semi
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
c
unlabeled
?
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)+
c
labeled
?
?
?x,y??L
log p(y | x)
We use block coordinate descent to optimize
this objective. First, we use c
em
iterations of
the expectation maximization algorithm to opti-
mize the ?-block while the ?-block is fixed, then
we optimize the ?-block with c
lbfgs
iterations of
L-BFGS (Liu et al., 1989) while the ?-block is
fixed.
4
4.2 Unsupervised Word Embeddings
For many NLP tasks, using unsupervised
word representations as features improves
accuracy (Turian et al., 2010). We use
word2vec (Mikolov et al., 2013) to train
100?dimensional word embeddings from a
large Twitter corpus of about 20 million tweets
extracted from the live stream, in multiple lan-
guages. We define an additional feature function
4
An open source efficient c++ imple-
mentation of our method can be found at
https://github.com/ldmt-muri/alignment-with-openfst
82
in the CRF autoencoder model ?4.1 for each of
the 100 dimensions, conjoined with the label y
i
.
The feature value is the corresponding dimension
for x
i
. A binary feature indicating the absence of
word embeddings is fired for out-of-vocabulary
words (i.e., words for which we do not have word
embeddings). The token-level coverage of the
word embeddings for each of the languages or
dialects used in the training data is reported in
Table 2.
4.3 Word List Features
While some words are ambiguous, many words
frequently occur in only one of the two lan-
guages being considered. An easy way to iden-
tify the label of such unambiguous words is to
check whether they belong to the vocabulary of
either language. Moreover, named entity recog-
nizers typically rely on gazetteers of named enti-
ties to improve their performance. We generalize
the notion of using monolingual vocabularies and
gazetteers of named entities to general word lists.
Using K word lists {l
1
, . . . , l
K
}, when a token x
i
is labeled with y
i
, we fire a binary feature that con-
joins ?y
i
, ?(x
i
? l
1
), . . . , ?(x
i
? l
K
)?, where ? is
an indicator boolean function. We use the follow-
ing word lists:
? Hindi and Nepali Wikipedia article titles
? multilingual named entities from the JRC
dataset
5
and CoNLL 2003 shared task
? word types in monolingual corpora in MSA,
ARZ, En and Es.
? set difference between the following pairs of
word lists: MSA-ARZ, ARZ-MSA, En-Es, Es-
En.
Transliteration from Devanagari The Nepali?
English tweets in the dataset are romanized. This
renders our Nepali word lists, which are based
on the Devanagari script, useless. Therefore, we
transliterate the Hindi and Nepali named entities
lists using a deterministic phonetic mapping. We
romanize the Devanagari words using the IAST
scheme.
6
We then drop all accent marks on the
characters to make them fit into the 7?bit ASCII
range.
5
http://datahub.io/dataset/jrc-names
6
http://en.wikipedia.org/wiki/
International_Alphabet_of_Sanskrit_
Transliteration
embeddings word lists
language coverage coverage
ARZ 30.7% 68.8%
En 73.5% 55.7%
MSA 26.6% 76.8%
Ne 14.5% 77.0%
Es 62.9% 78.0%
Zh 16.0% 0.7%
Table 2: The type-level coverage of annotated data
according to word embeddings (second column)
and according to word lists (third column), per lan-
guage.
5 Experiments
We compare the performance of five models for
each language pair, which correspond to the five
lines in Table 3. The first model, ?CRF? is the
baseline model described in ?3. The second ?CRF
+ U
test
? and the third ?CRF + U
all
? are CRF au-
toencoder models (see ?4.1) with two sets of un-
labeled data: (1) U
test
which only includes the test
set,
7
and (2) U
all
which includes the test set as well
as all tweets by the set of users who contributed
any tweets in L. The fourth model ?CRF + U
all
+
emb.? is a CRF autoencoder which uses word em-
bedding features (see ?4.2), as well as the features
described in ?3.2. Finally, the fifth model ?CRF +
U
all
+ emb. + lists? further adds word list features
(see ?4.3). In all but the ?CRF? model, we adopt a
transductive learning setup.
Since the CRF baseline is used as the encoding
part of the CRF autoencoder model, we use the
supervisedly-trained CRF parameters to initialize
the CRF autoencoder models. The categorical dis-
tributions of the reconstruction model are initial-
ized with discrete uniforms. We set the weight
of the labeled data log-likelihood c
labeled
= 0.5,
the weight of the unlabeled data log-likelihood
c
unlabeled
= 0.5, the L
2
regularization strength
c
L
2
= 0.3, the concentration parameter of the
Dirichlet prior ? = 0.1, the number of L-BFGS
iterations c
LBFGS
= 4, and the number of EM iter-
ations c
EM
= 4.
8
We stop training after 50 itera-
tions of block coordinate descent.
7
U
test
is potentially useful when the test set belongs to a
different domain than the labeled examples, which is often
referred to as ?domain adaptation?. However we were unable
to test this hypothesis since all the CS annotations we had
access to are from Twitter.
8
Hyper-parameters c
L
2
and ? were tuned using cross-
validation. The remaining hyper-parameters were not tuned.
83
config En?Ne MSA?ARZ En?Es Zh?En
CRF 95.2% 80.5% 94.6% 94.9%
+T
test
95.2% 80.6% 94.6% 94.9%
+T
all
95.2% 80.7% 94.6% 94.9%
+emb. 95.3% 81.3% 95.1% 95.0%
+lists 97.0% 81.2% 96.7% 95.3%
Table 3: Token level accuracy results for each of
the four language pairs.
label predicted predicted
MSA ARZ
true MSA 93.9% 5.3%
true ARZ 32.1% 65.2%
Table 4: Confusion between MSA and ARZ in the
Baseline configuration.
Results. The CRF baseline results are reported
in the first line in Table 3. For three language
pairs, the overall token-level accuracy ranges be-
tween 94.6% and 95.2%. In the fourth language
pair, MSA-ARZ, the baseline accuracy is 80.5%
which indicates the relative difficulty of this task.
The second and third lines in Table 3 show the
results when we use CRF autoencoders with the
unlabeled test set (U
test
), and with all unlabeled
tweets (U
all
), respectively. While semi-supervised
learning did not hurt accuracy on any of the lan-
guages, it only resulted in a tiny increase in accu-
racy for the Arabic dialects task.
The fourth line in Table 3 extends the CRF au-
toencoder model (third line) by adding unsuper-
vised word embedding features. This results in
an improvement of 0.6% for MSA-ARZ, 0.5% for
En-Es, 0.1% for En-Ne and Zh-En.
The fifth line builds on the fourth line by adding
word list features. This results in an improvement
of 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En,
and degradation of 0.1% in MSA-ARZ.
Analysis and Discussion The baseline perfor-
mance in the MSA-ARZ task is considerably
lower than those of the other tasks. Table 4 illus-
trates how the baseline model confuses lang1 and
lang2 in the MSA-ARZ task. While the baseline
system correctly labels 93.9% of MSA tokens, it
only correctly labels 65.2% of ARZ tokens.
Although the reported semi-supervised results
did not significantly improve on the CRF baseline,
more work needs to be done in order to conclude
these results:
lang. pair |U
test
| |U
all
| |L|
En?Ne 2489 6230 7504
MSA?ARZ 1062 2520 4800
Zh?En 332 332 663
En?Es 4001 7177 7399
Table 5: Number of tweets inL, U
test
andU
all
used
for semi-supervised learning of CRF autoencoders
models.
? Use an out-of-domain test set where some adap-
tation to the test set is more promising.
? Vary the number of labeled examples |L| and
the number of unlabeled examples |U|. Table 5
gives the number of labeled and unlabeled ex-
amples used for training the model. It is pos-
sible that semi-supervised learning would have
been more useful with a smaller |L| and a larger
|U|.
? Tune c
labeled
and c
unlabeled
.
? Split the parameters ? into two subsets: ?
labeled
and ?
unlabeled
; where ?
labeled
are the parameters
which have a non-zero value for any input x in
L and ?
unlabeled
are the remaining parameters in
? which only have non-zero values with unla-
beled examples but not with the labeled exam-
ples.
? Use a richer reconstruction model.
? Reconstruct a transformation of the token se-
quences instead of their surface forms.
? Train a token-level language ID model trained
on a large number of languages, as opposed to
disambiguating only two languages at a time.
Word embeddings improve the results for all
language pairs, but the largest improvement is in
MSA-ARZ and En-Es. Looking into the word em-
beddings coverage of those languages (i.e., MSA,
ARZ, Es, En in Table 2), we find that they are bet-
ter covered than the other languages (Ne, Zh). We
conclude that further improvements on En-Ne and
Zh-En may be expected if they are better repre-
sented in the corpus used to learn word embed-
dings.
As for the word lists, the largest improvement
we get is the romanized word lists of Nepali,
which have a 77.0% coverage and improve the
accuracy by 1.7%. This shows that our translit-
erated word lists not only cover a lot of tokens,
and are also useful for language ID. The Spanish
84
Config lang1 lang2 ne
+lists 84.1% 76.5% 73.7%
-lists 84.2% 77.1% 71.5%
Table 6: F?Measures of two Arabic configura-
tions. lang1 is MSA. lang2 is ARZ.
word lists also have a wide coverage, improving
the overall accuracy by 1.6%. The overall accu-
racy of the Arabic dialects slightly degrades with
the addition of the word lists. Closer inspection
in table 6 reveals that it improves the F?Measure
of the named entities at the expense of both MSA
(lang1) and ARZ (lang2).
6 Related Work
Previous work on identifying languages in a mul-
tilingual document includes (Singh and Gorla,
2007; King and Abney, 2013; Lui et al., 2014).
Their goal is generally more about identifying the
languages that appear in the document than intra?
sentential CS points.
Previous work on computational models of
code?switching include formalism (Joshi, 1982)
and language models that encode syntactic con-
straints from theories of code?switching, such as
(Li and Fung, 2013; Li and Fung, 2014). These
require the existence of a parser for the languages
under consideration. Other work on prediction
of code?switching points, such as (Elfardy et al.,
2013; Nguyen and Dogruoz, 2013) and ours, do
not depend upon such NLP infrastructure. Both of
the aforementioned use basic character?level fea-
tures and dictionaries on sequence models.
7 Conclusion
We have shown that a simple CRF baseline with
a handful of feature templates obtains strong re-
sults for this task. We discussed three methods
to improve over the supervised baseline using un-
labeled data: (1) modeling unlabeleld data using
CRF autoencoders, (2) using pre-trained word em-
beddings, and (3) using word list features.
We show that adding word embedding features
and word lists features is useful when they have
good coverage of words in a data set. While mod-
est improvements are observed due to modeling
unlabeled data with CRF autoenocders, we iden-
tified possible directions to gain further improve-
ments.
While bilingual disambiguation was a good first
step for identifying code switching, we suggest a
reformulation of the task such that each label can
take on one of many languages.
Acknowledgments
We thank Brendan O?Connor who helped assem-
ble the Twitter dataset. We also thank the work-
shop organizers for their hard work, and the re-
viewers for their comments. This work was
sponsored by the U.S. Army Research Labora-
tory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533. The
statements made herein are solely the responsibil-
ity of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.
Conditional random field autoencoders for unsuper-
vised structured prediction. In Proc. of NIPS.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
John J. Gumperz. 1982. Discourse Strategies. Studies
in Interactional Sociolinguistics. Cambridge Univer-
sity Press.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of
the 9th Conference on Computational Linguistics -
Volume 1, COLING ?82, pages 145?150, Czechoslo-
vakia. Academia Praha.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML.
Ying Li and Pascale Fung. 2013. Improved mixed lan-
guage speech recognition using asymmetric acous-
tic model and language model with code-switch in-
version constraints. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2013 IEEE International
Conference on, pages 7368?7372, May.
Ying Li and Pascale Fung. 2014. Code switch lan-
guage modeling with functional head constraint. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages
4913?4917, May.
85
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.
Marco Lui, Han Jey Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation of Computational Linguistics, 2:27?40.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Dong Nguyen and Seza A. Dogruoz. 2013. Word level
language identification in online multilingual com-
munication. Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 857?862. Association for Computational Lin-
guistics.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identi-
fication of languages and encodings in a multilingual
document. In Building and Exploring Web Corpora
(WAC3-2007): Proceedings of the 3rd Web as Cor-
pus Workshop, Incorporating Cleaneval, volume 4,
page 95.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
86
