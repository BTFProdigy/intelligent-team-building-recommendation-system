Statistical Machine Translation Models for Personalized Search
Rohini U ?
AOL India R& D
Bangalore, India
Rohini.uppuluri@corp.aol.com
Vamshi Ambati
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, USA
vamshi@cs.cmu.edu
Vasudeva Varma
LTRC, IIIT Hyd
Hyderabad, India
vv@iiit.ac.in
Abstract
Web search personalization has been well
studied in the recent few years. Relevance
feedback has been used in various ways to
improve relevance of search results. In this
paper, we propose a novel usage of rele-
vance feedback to effectively model the pro-
cess of query formulation and better char-
acterize how a user relates his query to the
document that he intends to retrieve using
a noisy channel model. We model a user
profile as the probabilities of translation of
query to document in this noisy channel us-
ing the relevance feedback obtained from the
user. The user profile thus learnt is applied
in a re-ranking phase to rescore the search
results retrieved using an underlying search
engine. We evaluate our approach by con-
ducting experiments using relevance feed-
back data collected from users using a pop-
ular search engine. The results have shown
improvement over baseline, proving that our
approach can be applied to personalization
of web search. The experiments have also
resulted in some valuable observations that
learning these user profiles using snippets
surrounding the results for a query gives bet-
ter performance than learning from entire
document collection.
1 Introduction
Most existing text retrieval systems, including the
web search engines, suffer from the problem of ?one
?This work was done when the first and second authors were
at IIIT Hyderabad, India.
size fits all?: the decision of which documents to re-
trieve is made based only on the query posed, with-
out consideration of a particular user?s preferences
and search context. When a query (e.g. ?jaguar?) is
ambiguous, the search results are inevitably mixed
in content (e.g. containing documents on the jaguar
cat and on the jaguar car), which is certainly non-
optimal for a given user, who is burdened by having
to sift through the mixed results. In order to opti-
mize retrieval accuracy, we clearly need to model the
user appropriately and personalize search according
to each individual user. The major goal of person-
alized search is to accurately model a user?s infor-
mation need and store it in the user profile and then
re-rank the results to suit to the user?s interests using
the user profile. However, understanding a user?s in-
formation need is, unfortunately, a very difficult task
partly because it is difficult to model the search pro-
cess which is a cognitive process and partly because
it is difficult to characterize a user and his prefer-
ences and goals. Indeed, this has been recognized as
a major challenge in information retrieval research
(et. al, 2003).
In order to address the problem of personalization
one needs to clearly understand the actual process of
search. First the user has an information need that
he would like to fulfill. He is the only entity in the
process that knows the exact information he needs
and also has a vague notion of the document that
can full fill his specific information need. A query
based search engine is at his disposal for identifying
this particular document or set of documents from
among a vast repository of them. He then formu-
lates a query that he thinks is congruent to the doc-
ument he imagines to fulfill his need and poses it to
the search engine. The search engine now returns
521
a list of results that it calculates as relevant accord-
ing to its ranking algorithm. Every user is different
and has a different information need, perhaps over-
lapping sometimes. The way a user conceives an
ideal document that fulfills his need also varies. It is
our hypothesis that if one can learn the variations of
each user in this direction, effective personalization
can be done.
Most approaches to personalization have tried
to model the user?s interests by requesting explicit
feedback from the user during the search process
and observing these relevance judgments to model
the user?s interests. This is called relevance feed-
back, and personalization techniques using it have
been proven to be quite effective for improving re-
trieval accuracy (Salton and Buckley, 1990; Roc-
chio, 1971). These approaches to personalization
have considered, user profile to be a collection of
words, ontology, a matrix etc.
We use relevance feedback for personalization in
our approach. However we propose a novel usage of
relevance feedback to effectively model the process
of query formulation and better characterize how a
user relates his query to the document that he in-
tends to retrieve as discussed in the web search pro-
cess above. A user profile learnt from the relevance
feedback that captures the query generation process
is used as a guide to understand user?s interests over
time and personalize his web search results.
Interestingly, a new paradigm has been proposed
for retrieval rooted from statistical language mod-
eling recently that views the query generation pro-
cess through a Noisy channel model (Berger and
Lafferty, 1999) . It was assumed that the docu-
ment and query are from different languages and
the query generation process was viewed as a trans-
lation from the document language which is more
verbose to the language of the query which is more
compact and brief. The noisy channel model pro-
posed by Berger and Lafferty (Berger and Lafferty,
1999) inherently captures the dependencies between
the query and document words by learning a trans-
lation model between them. As we intend to achieve
personalized search by personalizing the query for-
mulation process, we also perceive the user profile
learning through a Noisy Channel Model. In the
model, when a user has an information need, he also
has an ideal document in mind that fulfills his need.
The user tries to in a way translate the notion of
the ideal document into a query that is more com-
pact but congruent to the document. He then poses
this query to the search engine and retrieves the re-
sults. By observing this above process over time,
we can capture how the user is generating a query
from his ideal document. By learning this model of a
user, we can predict which document best describes
his information need for the query he poses. This
is the motive of personalization. In our approach,
we learn a user model which is probabilistic model
for the noisy channel using statistical translation ap-
proaches and from the past queries and their corre-
sponding relevant documents provided as feedback
by the user.
The rest of the paper is organized as follows.
We first describe the related work on personalized
search then we provide the background and the
framework that our approach is based upon. we
discuss the modeling of a user profile as a transla-
tion model. after which we describe applying it to
personalized search. we describe our experimental
results followed by conclusions with directions to
some future work.
2 Related Work
There has been a growing literature available with
regard to personalization of search results. In this
section, we briefly overview some of the available
literature.
(Pretschner and Gauch, 1999) used ontology to
model users interests, which are studied from users
browsed web pages. (Speretta and Gauch, 2004)
used users search history to construct user profiles.
(Liu et al, 2002) performed personalized web search
by mapping a query to a set of categories using a
user profile and a general profile learned from the
user?s search history and a category hierarchy re-
spectively. (Hatano and Yoshikawa., 2004) consid-
ered the unseen factors of the relationship between
the web users behaviors and information needs and
constructs user profiles through a memory-based
collaborative filtering approach.
To our knowledge, there has been a very little
work has been done that explicitly uses language
models to personalization of search results. (Croft
et al, 2001) discuss about relevance feedback and
522
query expansion using language modeling. (Shen et
al., 2005) use language modeling for short term per-
sonalization by expanding queries.
Earlier approaches to personalization have con-
sidered, user profile to be a collection of words, on-
tology, language model etc. We perceive the user
profile learning through a Noisy Channel Model. In
the model, when a user has an information need, he
also has a vague notion of what is the ideal document
that he would like to retrieve. The user then creates
a compact query that he thinks would retrieve the
document. He then poses the query to the search en-
gine. By observing this above process over time, we
learn a user profile as the probabilities of translation
for the noisy channel that converts his document to
the query. We then use this profile in re-ranking the
results of a search engine to provide personalized re-
sults.
3 Background
In this section, we describe the statistical language
modeling and the translation model framework for
information retrieval that form a basis for our re-
search.
The basic approach for language modeling for IR
was proposed by Ponte and Croft (Ponte and Croft,
1998). It assumes that the user has a reasonable idea
of the terms that are likely to appear in the ideal doc-
ument that can satisfy his/her information need, and
that the query terms the user chooses can distinguish
the ideal document from the rest of the collection.
The query is thus generated as the piece of text rep-
resentative of the ideal document. The task of the
system is then to estimate, for each of the documents
in the collection, which is most likely to be the ideal
document.
argmax
D
P (D|Q) = argmax
D
P (Q|D)P (D)
where Q is a query and D is a document. The prior
probability P (D) is usually assumed to be uniform
and a language model P (Q|D) is estimated for ev-
ery document. In other words, they estimate a prob-
ability distribution over words for each document
and calculate the probability that the query is a sam-
ple from that distribution. Documents are ranked
according to this probability. The basic model has
been extended in a variety of ways. Modeling doc-
uments as in terms of a noisy channel model by
Berger & Lafferty (Berger and Lafferty, 1999), mix-
ture of topics, and phrases are considered (Song and
Croft., 1999), (Lavrenko and Croft, 2001) explicitly
models relevance, and a risk minimization frame-
work based on Bayesian decision theory has been
developed (Zhai and Lafferty, 2001).
The noisy channel by Berger and Lafferty (Berger
and Lafferty, 1999) view a query as a distilla-
tion or translation from a document describing the
query generation process in terms of a noisy channel
model. In formulating a query to a retrieval system,
a user begins with an information need. This infor-
mation need is then represented as a fragment of an
?ideal document?, a portion of the type of document
that the user hopes to receive from the system. The
user then translates or ?distills? this ideal document
fragment into a succinct query, selecting key terms
and replacing some terms with related terms.
To determine the relevance of a document to a
query, their model estimates the probability that the
query would have been generated as a translation
of that document. Documents are then ranked ac-
cording to these probabilities. More specifically,
the mapping from a document term w to a query
term qi is achieved by estimating translation mod-
els P (q|w). Using translation models, the retrieval
model becomes
P (Q|D) =
?
qi?Q
?P (qi|GE)+(1??)
?
w?D
P (qi|w)P (w|D)
where P (qi|GE) is the smoothed or general
probability obtained from a large general corpus.
P (qi|w) is an entry in the translation model. It repre-
sents the probability of generation of the query word
qi for a word w in the document. P (w|D) is the
probability of the word w in the document and ? is
a weighting parameter which lies between 0 and 1.
4 User Profile as a Translation Model
We perceive the user profile learning as learning
the channel probabilities of a Noisy Channel Model
that generates the query from the document. In the
model, when a user has an information need, he also
has a vague notion of what is the ideal document
that he would like to retrieve. The user then creates
523
a compact query that he thinks would retrieve the
document. He then poses the query to the search en-
gine. By observing this above process over time, we
can learn how the user is generating a query from
his notion of an ideal document. By learning this,
we can predict which document best describes his
information need. The learnt model, called a user
profile, is thus capable of personalizing results for
that particular user. Hence, the user profile here is
a translation model learnt from explicit feedback of
the user using statistical translation approaches. Ex-
plicit feedback consists of the past queries and their
corresponding relevant documents provided as feed-
back by the user. A translation model is a proba-
bilistic model consisting of the triples, the source
word, the target word and the probability of trans-
lation. The translation model here is between doc-
ument words and queries words. Therefore the user
profile as a translation model in our approach will
consist of triples of a document word, a query word
and the probability of the document word generating
the query word.
5 Personalized Search
In this section, we describe how we perform person-
alized search using the proposed translation model
based user profile. First, a user profile is learnt using
the translation model process then the re-ranking is
done using the learnt user profile.
5.1 Learning user profile
In our approach, a user profile consists of a statisti-
cal translation model. A translation model is a prob-
abilistic model consisting of the triples, the source
word, the target word and the probability of trans-
lation. Our user profiles consists of the following
triples, a document word, a query word and the prob-
ability of the document word generating the query
word.
Consider a user u, let { {Qi, Di}, i = 1, 2, ..., N}
represent the past history of the user u. where Qi
is the query and Di is the concatenation of all the
relevant documents for the query Qi and let Di =
{w1, w2, ..., wn} be the words in it. The user profile
learnt from the past history of user consists of the
following triples of the form (q, wi, p(q|wi)) where
q is a word in the query Qi and wi is a word in the
document Di.
Translation model is typically learnt from paral-
lel texts i.e a set of translation pairs consisting of
source and target language sentences. In learning
the user profile, we first extract parallel texts from
the past history of the user and then learn the trans-
lation model which is essentially the user profile. In
the subsections below, we describe the process in de-
tail.
5.1.1 Extracting Parallel Texts
By viewing documents as samples of a verbose
language and the queries as samples of a concise
language, we can treat each document-query pair as
a translation pair, i.e. a pair of texts written in the
verbose language and the concise language respec-
tively. The extracted parallel texts consists of pairs
of the form {Qi, Drel} where Drel is the concatena-
tion of contexts extracted from all relevant document
for the query Qi.
We believe that short snippets extracted in the
context of the query would be better candidates for
Drel than using the whole document. This is be-
cause there can be a lot of noisy terms which need
not right in the context of the query. We believe a
short snippet usually N (we considered 15) words
to the left and right of the query words, similar to a
short snippet displayed by search engines can bet-
ter capture the context of the query. In deed we
experimented with different context sizes for Drel.
The first is using the whole document i.e., consider-
ing the query and concatenation of all the relevant
documents as a pair in the parallel texts extracted
which is called Ddocuments The second is using just
a short text snippet from the document in the con-
text of query instead of the whole document which
is called Dsnippets Details are described in the ex-
periments section.
5.1.2 Learning Translation Model
According to the standard statistical translation
model (Brown et al, 1993), we can find the optimal
model M? by maximizing the probability of gener-
ating queries from documents or
M? = argmax
M
N?
i=1
P (Qi|Di,M)
524
qw dw P(qw|dw,u)
journal kdd 0.0176
journal conference 0.0123
journal journal 0.0176
journal sigkdd 0.0088
journal discovery 0.0211
journal mining 0.0017
journal acm 0.0088
music music 0.0375
music purchase 0.0090
music mp3 0.0090
music listen 0.0180
music mp3.com 0.0450
music free 0.0008
Table 1: Sample user profile
To find the optimal word translation probabilities
P (qw|dw,M?), we can use the EM algorithm. The
details of the algorithm can be found in the literature
for statistical translation models, such as (Brown et
al., 1993).
IBM Model1 (Brown et al, 1993) is a simplistic
model which takes no account of the subtler aspects
of language translation including the way word or-
der tends to differ across languages. Similar to ear-
lier work (Berger and Lafferty, 1999), we use IBM
Model1 because we believe it is more suited for IR
because the subtler aspects of language used for ma-
chine translation can be ignored for IR. GIZA++
(Och and Ney, 2003), an open source tool which im-
plements the IBM Models which we have used in
our work for computing the translation probabilities.
A sample user profile learned is shown in Table 1.
5.2 Re-ranking
Re-ranking is a phase in personalized search where
the set of documents matching the query retrieved
by a general search engine are re-scored using the
user profile and then re-ranked in descending order
of rank of the document. We follow a similar ap-
proach in our work.
Let D be set of all the documents returned by the
search engine. The rank of each document D re-
turned for a query Q for user u is computing using
his user profile as shown in Equation 1.
P (Q|D,u) =
?
qi?Q
?P (qi|GE)+(1??)
?
w?D
P (qi|w, u)P (w|D)
(1)
where P (qi|GE) is the smoothed or general
probability obtained from a large general corpus.
P (qi|w, u) is an entry in the translation model of the
user. It represents the probability of generation of
the query word qi for a word w in the document.
P (w|D) is the probability of the word w in the doc-
ument and ? is a weighting parameter which lies be-
tween 0 and 1.
6 Experiments
We performed experiments evaluating our approach
on data set consisting of 7 users. Each user submit-
ted a number of queries to a search engine (Google).
For each query, the user examined the top 10 docu-
ments and identified the set of relevant documents.
Table 2 gives the statistics of the data sets. There is
no repetition of query for any user though repetition
of some words in the query exists (see Table 2). The
document collection consists of top 20 documents
from google which is actually the set of documents
seen by the user while accessing the relevance of the
documents. In all, the total size of the document
collection was 3,469 documents. We did not include
documents of type doc and pdf files.
To evaluate our approach, we use the 10-fold
cross-validation strategy (Mitchell, 1997). We di-
vide the data of each user into 10 sets each hav-
ing (approximately) equal number of search queries
(For example, for user1 had 37 queries in total, we
divided this into 10 sets with 4 queries each approx-
imately). Learning of user profile is done 10 times,
each time leaving out one of the sets from training,
but using only the omitted subset for testing. Per-
formance is computed in the testing phase for each
time and average of the 10 times is taken. In the
testing phase, we take each query and re rank the
results using the proposed approach using his pro-
file learned from nine other sets. For measuring
performance for each query, we compute Precision
@10 (P@10), a widely used metric for evaluating
personalized search algorithms. It is defined as the
proportion of relevant documents among the top 10
results for the given ranking of documents. P@10
is computed by comparing with the relevant docu-
ments present in the data. All the values presented
in the tables are average values which are averaged
over all queries for each user, unless otherwise spec-
ified. We used Lucene1, an open source search en-
gine as the general search engine to first retrieve a
1http://lucene.apache.org
525
User No. Q % of Unique Total Rel Avg. Rel
words in Q
1 37 89 236 6.378
2 50 68.42 178 3.56
3 61 82.63 298 4.885
4 26 86.95 101 3.884
5 33 80.76 134 4.06
6 29 78.08 98 3.379
7 29 88.31 115 3.965
Table 2: Statistics of the data set of 7 users
set of results matching the query.
6.0.1 Comparison with Contextless Ranking
We test the effectiveness of our user profile by
comparing with a contextless ranking algorithm. We
used a generative language modeling for IR as the
context less ranking algorithm (Query Likelihood
model (Ponte and Croft, 1998; Song and Croft.,
1999)). This is actually the simplest version of the
model described in Equation 1. Each word w can be
translated only as itself that is the translation proba-
bilities (see Equation 1) are ?diagonal?.
P (qi|w, u) =
{
1 if q = w
0 Otherwise
This serves as a good baseline for us to see how
well the translation model actually captured the user
information. For fair testing similar to our approach,
for each query, we first retrieve results matching
a query using a general search engine (Lucene).
Then we rank the results using the formula shown
in Equation 2.
P (Q|D) =
?
qi?Q
?P (qi|GE)+(1??)P (qi|D) (2)
We used IBM Model1 for learning the translation
model (i.e., the user profile). The general English
probabilities are computed from all the documents in
the lucene?s index. Similar to earlier works (Berger
and Lafferty, 1999), we simply set the value of ? to
be 0.05. The values reported are P@10 values aver-
age over all 10 sets and the queries for the respec-
tive user. Table 3 clearly shows the improvement
brought in by the user profile.
6.0.2 Experiments with Different Models
We performed an experiment to see if different
training models for learning the user profile affected
Set Contextless Proposed
User1 0.1433 0.1421
User2 0.1426 0.2445
User3 0.1016 0.1216
User4 0.0557 0.1541
User5 0.1877 0.3933
User6 0.1566 0.3941
User7 0.1 0.1833
Avg 0.1268 0.2332
Table 3: Precision @10 results for 7 users
Training Model Document Test Snippet Test
IBM Model1
Document Train 0.2062 0.2028
Snippet Train 0.2333 0.2488
GIZA++
Document Train 0.1799 0.1834
Snippet Train 0.2075 0.2034
Table 4: Summary of Comparison of different Mod-
els and Contexts for learning user profile
the performance. We experimented with two mod-
els. The first is a basic model and used in ear-
lier work, IBM Model1. The second is using the
GIZA++ default parameters. We observed that user
profile learned using IBM Model1 outperformed
that using GIZA++ default parameters. We believe
this is because, IBM Model1 is more suited for IR
because the subtler aspects of language used for ma-
chine translation (which are used in GIZA++ default
parameters) can be ignored for IR. We obtained an
average P@10 value of 0.2333 for IBM Model1 and
0.2075 for GIZA++.
6.0.3 Snippet Vs Document
In extracting parallel texts consists of pairs of the
form {Qi, Drel} where Drel is the concatenation of
contexts extracted from all relevant document for
the queryQi we experimented with different context
sizes for Drel.
We believe that a short snippet extracted in the
context of the query would be better candidate for
Drel than using the whole document. This is be-
cause there can be a lot of noisy terms which need
not useful in the context of the query. We believe
a short snippet usually N (we considered 15) words
to the left and right of the query words, similar to a
short snippet displayed by search engines can better
526
Figure 1: Comparison of Snippet Vs Document
Training using IBM Model1 for training.
IBM Model1 : I - Document Training and Document Testing,
IBM Model1 : II - Document Training and Snippet Testing,
IBM Model1 : III - Snippet Training and Document Testing,
IBM Model1 : IV - Snippet Training and Snippet Testing
capture the context of the query.
We experimented with two context sizes. The
first is using the whole document i.e., considering
the query and concatenation of all the relevant doc-
uments as a pair in the parallel texts extracted which
is calledDdocuments. The second is using just a short
text snippet from the document in the context of
query instead of the whole document which is called
Dsnippets. The user profile learning from pairs of
parallel texts {Q,Ddocuments} is called Document
Train. The user profile learning from pairs of paral-
lel texts {Q,Dsnippets} is called Snippet Train. The
user profiles are trained using both IBM Model1 and
GIZA++ and comparison of the two is shown in Ta-
ble 4.
We also experimented with the size of the context
used for testing. Using the document for re-ranking
as shown in Equation 1 (called Document Test) 2 and
using just a short snippet extracted from the docu-
ment for testing (called Snippet Test). Table 4 shows
the average P@10 over the 10 sets and all queries
and users.
We observed that, not only did the model used
for training affected P@10, but also the data used in
training and testing, whether it was a snippet or doc-
ument, showed a large variation in the performance.
Training using IBM Model1 using the snippet and
2It is to be noted that Snippet Train and Document Test and
training using IBM Model1 is the default configuration used for
all the reported results unless explicitly specified.
Figure 2: Comparison of Snippet Vs Document
Training using GIZA++ Default parameters for
training.
GIZA++:I - Document Training and Document Testing,
GIZA++:II - Document Training and Snippet Testing,
GIZA++:III - Snippet Training and Document Testing,
GIZA++:IV - Snippet Training and Snippet Testing
testing using snippet achieved the best results. This
is in agreement with the discussion that the snip-
pet surrounding the query captures the context of
the query better than a document which may con-
tain many words that could possibly be unrelated to
the query, therefore diluting the strength of the mod-
els learnt. The detailed results for all the users are
shown in Figure 1 and Figure 2.
7 Conclusions and Future Work
Relevance feedback from the user has been used in
various ways to improve the relevance of the re-
sults for the user. In this paper we have proposed
a novel usage of relevance feedback to effectively
model the process of query formulation and better
characterize how a user relates his query to the doc-
ument that he intends to retrieve. We applied a noisy
channel model approach for the query and the doc-
uments in a retrieval process. The user profile was
modeled using the relevance feedback obtained from
the user as the probabilities of translation of query
to document in this noisy channel. The user pro-
file thus learnt was applied in a re-ranking phase to
rescore the search results retrieved using general in-
formation retrieval models. We evaluate the usage of
our approach by conducting experiments using rele-
vance feedback data collected from users of a popu-
lar search engine. Our experiments have resulted in
527
some valuable observations that learning these user
profiles using snippets surrounding the results for
a query show better performance than when learn-
ing from entire documents. In this paper, we have
only evaluated explicit relevance feedback gathered
from a user and performed our experiments. As part
of future work, we would like to evaluate our ap-
proach on implicit feedback gathered probably as
click-through data in a search engine, or on the client
side using customized browsers.
References
Adam Berger and John D. Lafferty. 1999. Information
retrieval as statistical translation. In Research and De-
velopment in Information Retrieval, pages 222?229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
W. Bruce Croft, Stephen Cronen-Townsend, and Victor
Larvrenko. 2001. Relevance feedback and person-
alization: A language modeling perspective. In DE-
LOS Workshop: Personalisation and Recommender
Systems in Digital Libraries.
Jamie Allan et. al. 2003. Challenges in information re-
trieval language modeling. In SIGIR Forum, volume
37 Number 1.
K. Sugiyama K. Hatano and M. Yoshikawa. 2004. Adap-
tive web search based on user profile constructed with-
out any effort from users. In Proceedings of WWW
2004, page 675 684.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance-
based language models. In Research and Development
in Information Retrieval, pages 120?127.
F. Liu, C. Yu, and W. Meng. 2002. Personalized web
search by mapping user queries to categories. In Pro-
ceedings of the eleventh international conference on
Information and knowledge management, ACM Press,
pages 558?565.
Tom Mitchell. 1997. Machine Learning. McGrawHill.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information retrieval. In
Research and Development in Information Retrieval,
pages 275?281.
A. Pretschner and S. Gauch. 1999. Ontology based per-
sonalized search. In ICTAI., pages 391?398.
J. J. Rocchio. 1971. Relevance feedback in information
retrieval, the smart retrieval system. Experiments in
Automatic Document Processing, pages 313?323.
G. Salton and C. Buckley. 1990. Improving retrieval per-
formance by relevance feedback. Journal of the Amer-
ican Society of Information Science, 41:288?297.
Xuehua Shen, Bin Tan, and Chengxiang Zhai. 2005. Im-
plicit user modeling for personalized search. In Pro-
ceedings of CIKM 2005.
F. Song and W. B. Croft. 1999. A general language
model for information retrieval. In Proceedings on
the 22nd annual international ACM SIGIR conference,
page 279280.
Micro Speretta and Susan Gauch. 2004. Personalizing
search based on user search histories. In Thirteenth
International Conference on Information and Knowl-
edge Management (CIKM 2004).
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to ad
hoc information retrieval. In Proceedings of ACM SI-
GIR?01, pages 334?342.
528
Proceedings of the Third Workshop on Statistical Machine Translation, pages 163?166,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Statistical Transfer Systems for French?English
and German?English Machine Translation
Greg Hanneman and Edmund Huber and Abhaya Agarwal and Vamshi Ambati
and Alok Parlikar and Erik Peterson and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, ehuber, abhayaa, vamshi, aup, eepeter, alavie}@cs.cmu.edu
Abstract
We apply the Stat-XFER statistical transfer
machine translation framework to the task of
translating from French and German into En-
glish. We introduce statistical methods within
our framework that allow for the principled
extraction of syntax-based transfer rules from
parallel corpora given word alignments and
constituency parses. Performance is evaluated
on test sets from the 2007 WMT shared task.
1 Introduction
The Carnegie Mellon University statistical trans-
fer (Stat-XFER) framework is a general search-
based and syntax-driven framework for develop-
ing MT systems under a variety of data condi-
tions (Lavie, 2008). At its core is a transfer en-
gine using two language-pair-dependent resources:
a grammar of weighted synchronous context-free
rules (possibly augmented with unification-style fea-
ture constraints), and a probabilistic bilingual lexi-
con of syntax-based word- and phrase-level transla-
tions. The Stat-XFER framework has been used to
develop research MT systems for a number of lan-
guage pairs, including Chinese?English, Hebrew?
English, Urdu?English, and Hindi?English.
In this paper, we describe our use of the frame-
work to create new French?English and German?
English MT systems for the 2008 Workshop on Sta-
tistical Machine Translation shared translation task.
We first describe the acquisition and processing of
resources for each language pair and the roles of
those resources within the Stat-XFER system (Sec-
tion 2); we then report results on common test sets
(Section 3) and share some early analysis and future
directions (Section 4).
2 System Description
Building a new machine translation system under
the Stat-XFER framework involves constructing a
bilingual translation lexicon and a transfer gram-
mar. Over the past six months, we have developed
new methods for extracting syntax-based translation
lexicons and transfer rules fully automatically from
parsed and word-aligned parallel corpora. These
new methods are described in detail by Lavie et
al. (2008). Below, we detail the statistical meth-
ods by which these resources were extracted for our
French?English and German?English systems.
2.1 Lexicon
The bilingual lexicon is automatically extracted
from automatically parsed and word-aligned paral-
lel corpora. To obtain high-quality statistical word
alignments, we run GIZA++ (Och and Ney, 2003)
in both the source-to-target and target-to-source di-
rections, then combine the resulting alignments with
the Sym2 symmetric alignment heuristic of Ortiz-
Mart??nez et al (2005)1. From this data, we extract a
lexicon of both word-to-word and syntactic phrase-
to-phrase translation equivalents.
The word-level correspondences are extracted di-
rectly from the word alignments: parts of speech for
these lexical entries are obtained from the preter-
1We use Sym2 over more well-known heuristics such as
?grow-diag-final? because Sym2 has been shown to give the
best results for the node-alignment subtask that is part of our
processing chain.
163
ws cs wt ct r
paru V appeared V 0.2054
paru V seemed V 0.1429
paru V found V 0.0893
paru V published V 0.0804
paru V felt V 0.0714
.
.
.
.
.
.
.
.
.
paru V already ADV 0.0089
paru V appear V 0.0089
paru V authoritative ADJ 0.0089
Table 1: Part of the lexical entry distribution for the
French (source) word paru.
minal nodes of parse trees of the source and target
sentences. If parsers are unavailable for either lan-
guage, we have also experimented with determin-
ing parts of speech with independent taggers such
as TreeTagger (Schmid, 1995). Alternatively, parts
of speech may be projected through the word align-
ments from one language to the other if the infor-
mation is available on at least one side. Syntactic
phrase-level correspondences are extracted from the
parallel data by applying the PFA node alignment
algorithm described by Lavie et al (2008). The
yields of the aligned parse tree nodes are extracted
as constituent-level translation equivalents.
Each entry in the lexicon is assigned a rule score,
r, based on its source-side part of speech cs, source-
side text ws, target-side part of speech ct, and target-
side text wt. The score is a maximum-likelihood es-
timate of the distribution of target-language transla-
tion and source- and target-language parts of speech,
given the source word or phrase.
r = p(wt, ct, cs |ws) (1)
? #(wt, ct, ws, cs)#(ws) + 1
(2)
We employ add-one smoothing in the denominator
of Equation 2 to counteract overestimation in the
case that #(ws) is small. Rule scores provide a way
to promote the more likely translation alternatives
while still retaining a high degree of diversity in the
lexicon. Table 1 shows part of the lexical distribu-
tion for the French (source) word paru.
The result of the statistical word alignment pro-
cess and lexical extraction is a bilingual lexicon con-
taining 1,064,755 entries for French?English and
1,111,510 entries for German?English. Sample lex-
ical entries are shown in Figure 1.
2.2 Grammar
Transfer grammars for our earlier statistical transfer
systems were manually created by in-house experts
of the languages involved and were therefore small.
The Stat-XFER framework has since been extended
with procedures for automatic grammar acquisition
from a parallel corpus, given constituency parses for
source or target data or both. Our French and Ger-
man systems used the context-free grammar rule ex-
traction process described by Lavie et al (2008).
For French, we used 300,000 parallel sentences from
the Europarl training data parsed on the English side
with the Stanford parser (Klein and Manning, 2003)
and on the French side with the Xerox XIP parser
(A??t-Mokhtar et al, 2001). For German, we used
300,000 Europarl sentence pairs parsed with the En-
glish and German versions of the Stanford parser2.
The set of rules extracted from the parsed corpora
was filtered down after scoring to improve system
performance and run time. The final French rule set
was comprised of the 1500 most frequently occur-
ring rules. For German, rules that occurred less than
twice were filtered out, leaving a total of 16,469. In
each system, rule scores were again calculated by
Equation 2, with ws and wt representing the full
right-hand sides of the source and target grammar
rules.
A secondary version of our French system used a
word-level lexicon extracted from the intersection,
rather than the symmetricization, of the GIZA++
alignments in each direction; we hypothesize that
this tends to improve precision at the expense of re-
call. The word-level lexicon was supplemented with
syntax-based phrase-level entries obtained from the
PFA node alignment algorithm. The grammar
contained the 700 highest-frequency and the 500
highest-scoring rules extracted from the parallel
parsed corpus. This version had a total lexicon size
of 2,023,531 entries and a total grammar of 1034
rules after duplicates were removed. Figure 2 shows
2Due to a combination of time constraints and paucity of
computational resources, only a portion of the Europarl parallel
corpus was utilized, and none of the supplementary news com-
mentary training data was integrated.
164
)(
{VS,248840}
V::V |: ["paru"] ?> ["appeared"]
  (*score* 0.205357142857143)
)
  (*score* 0.763636363636364)
{NP,2000012}
NP::NP |: ["ein" "Beispiel"] ?> ["an" "example"]
(
Figure 1: Sample lexical entries for French and German.
sample grammar rules automatically learned by the
process described above.
2.3 Transfer Engine
The Stat-XFER transfer engine runs in a two-stage
process, first applying the grammar and lexicon
to an input sentence, then running a decoder over
the resulting lattice of scored translation pieces.
Scores for each translation piece are based on a
log-linear combination of several features: language
model probability, rule scores, source-given-target
and target-given-source lexical probabilities, parse
fragmentation, and length. For more details, see
Lavie (2008). The use of a German transfer gram-
mar an order of magnitude larger than the corre-
sponding French grammar was made possible due to
a recent optimization made in the engine. When en-
abled, it constrains the search of translation hypothe-
ses to the space of hypotheses whose structure satis-
fies the consituent structure of a source-side parse.
3 Evaluation
We trained our model parameters on a subset of
the provided ?dev2006? development set, optimiz-
ing for case-insensitive IBM-style BLEU (Papineni
et al, 2002) with several iterations of minimum error
rate training on n-best lists. In each iteration?s list,
we also included the lists from previous iterations in
order to maintain a diversity of hypothesis types and
scores. The provided ?test2007? and ?nc-test2007?
data sets, identical with the test data from the 2007
Workshop on Statistical Machine Translation shared
task, were used as internal development tests.
Tables 2, 3, and 4 report scores on these data sets
for our primary French, secondary French, and Ger-
man systems. We report case-insensitive scores for
version 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-style
BLEU (Papineni et al, 2002), and version 5 of TER
(Snover et al, 2006).
Data Set METEOR BLEU TER
dev2006 0.5332 0.2063 64.81
test2007 0.5358 0.2078 64.75
nc-test2007 0.5369 0.1719 69.83
Table 2: Results for the primary French?English system
on provided development and development test sets.
Data Set METEOR BLEU TER
dev2006 0.5330 0.2086 65.02
test2007 0.5386 0.2129 64.29
nc-test2007 0.5311 0.1680 70.90
Table 3: Results for the secondary French?English sys-
tem on provided development and development test sets.
4 Analysis and Conclusions
From the development test results in Section 3, we
note that the Stat-XFER systems? performance cur-
rently lags behind the state-of-the-art scores on the
2007 test data3. This may be in part due to the low
volume of training data used for rule learning. A key
research question in our approach is how to distin-
guish low-frequency correct and useful transfer rules
from ?noisy? rules that are due to parser errors and
incorrect word alignments. We believe that learning
rules from more data will help alleviate this prob-
lem by proportionally increasing the counts of good
rules compared to incorrect ones. We also plan to
study methods for more effective rule set pruning,
regardless of the volume of training data used.
The difference in metric scores between in-
domain and out-of-domain data is partly due to ef-
fects of reference length on the metrics used. De-
tailed output from METEOR and BLEU shows that
the reference translations for the test2007 set are
about 94% as long as the primary French?English
3Top scores on the 2007 test data are approximately 0.60
METEOR, 0.33 BLEU, and 57.6 TER. See Callison-Burch et
al. (2007) for full results.
165
(
  (*score* 0.866050808314088
)
{PP,1627955}
PP:PP [PRE "d?" "autres" N] ?> [PRE "other" N]
  (X1::Y1)
  (X4::Y3)
)
(
{PP,3000085}
PP:ADVP ["vor" CARD "Monaten"] ?> [NUM "months" "ago"]
  (*score* 0.9375)
  (X2::Y1)
)
Figure 2: Sample grammar rules for French and German.
Data Set METEOR BLEU TER
dev2006 0.4967 0.1794 68.68
test2007 0.5052 0.1878 67.94
nc-test2007 0.4939 0.1347 74.38
Table 4: Results for the German?English system on pro-
vided development and development test sets.
system?s translations. On this set, our system has
approximately balanced precision (0.62) and recall
(0.66). However, the nc-test2007 references are only
84% as long as our output, a situation that hurts our
system?s precision (0.57) but boosts its recall (0.68).
METEOR, as a metric that favors recall, shows a
negligible increase in score between these two test
sets, while BLEU and TER report significant relative
drops of 17.3% and 7.8%. This behavior appears to
be consistent on the test2007 and nc-test2007 data
sets across systems (Callison-Burch et al, 2007).
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank the members of the Parsing and Semantics
group at Xerox Research Centre Europe for assisting
in parsing the French data using their XIP parser.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, China, October.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed paral-
lel corpora. In Proceedings of the Second Work-
shop on Syntax and Structure in Statistical Transla-
tion, Columbus, OH, June. To appear.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: A toolkit to train
phrase-based models for statistical machine transla-
tion. In Proceedings of the 10th Machine Translation
Summit, pages 141?148, Phuket, Thailand, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
166
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 87?95,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Syntax-driven Learning of Sub-sentential Translation Equivalents and
Translation Rules from Parsed Parallel Corpora
Alon Lavie
alavie@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Alok Parlikar
aup@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Vamshi Ambati
vambati@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
We describe a multi-step process for automati-
cally learning reliable sub-sentential syntactic
phrases that are translation equivalents of each
other and syntactic translation rules between
two languages. The input to the process is a
corpus of parallel sentences, word-aligned and
annotated with phrase-structure parse trees.
We first apply a newly developed algorithm
for aligning parse-tree nodes between the two
parallel trees. Next, we extract all aligned
sub-sentential syntactic constituents from the
parallel sentences, and create a syntax-based
phrase-table. Finally, we treat the node align-
ments as tree decomposition points and extract
from the corpus all possible synchronous par-
allel tree fragments. These are then converted
into synchronous context-free rules. We de-
scribe the approach and analyze its application
to Chinese-English parallel data.
1 Introduction
Phrase-based Statistical MT (PB-SMT) (Koehn et
al., 2003) has become the predominant approach to
Machine Translation in recent years. PB-SMT re-
quires broad-coverage databases of phrase-to-phrase
translation equivalents. These are commonly ac-
quired from large volumes of automatically word-
aligned sentence-parallel text corpora. Accurate
identification of sub-sentential translation equiva-
lents, however, is a critical process in all data-driven
MT approaches, including a variety of data-driven
syntax-based approaches that have been developed
in recent years. (Chiang, 2005) (Imamura et al,
2004) (Galley et al, 2004).
In this paper, we describe a multi-step process for
automatically learning reliable sub-sentential syn-
tactic phrases that are translation equivalents of each
other and syntactic translation rules between two
languages. The input to the process is a corpus of
parallel sentences, word-aligned and annotated with
phrase-structure parse trees for both languages. Our
method consists of three steps. In the first step,
we apply a newly developed algorithm for aligning
parse-tree nodes between the two parallel trees. In
the second step, we extract all aligned sub-sentential
syntactic constituents from the parallel sentences,
and create a syntax-based phrase-table. Our syn-
tactic phrases come with constituent ?labels? which
can guide their syntactic function during decoding.
In the final step, we treat the node alignments as
tree decomposition points and extract from the cor-
pus all possible synchronous parallel tree fragments.
These are then converted into synchronous context-
free rules. Our methods do not depend on any spe-
cific properties of the underlying phrase-structure
representations or the parsers used, and were de-
signed to be applicable even when these represen-
tations are quite different for the two languages.
The approach described is used to acquire the re-
sources for a statistical syntax-based MT approach
that we have developed (Stat-XFER), briefly de-
scribed below. The resulting resources can, how-
ever, be used in any syntax-based data-driven MT
approach other than our own. The focus of this pa-
per is on our syntax-driven process for extracting
phrases and rules from data. We describe the ap-
proach and analyze its effectiveness when applied to
large-volumes of Chinese-English parallel data.
87
1.1 The Stat-XFER MT Framework
Stat-XFER is a search-based syntax-driven frame-
work for building MT systems. The underlying for-
malism is based on synchronous context-free gram-
mars. The synchronous rules can optionally be aug-
mented by unification-style feature constraints. The
synchronous grammars can be acquired automati-
cally from data, but also manually developed by ex-
perts. A simple example transfer-rule (for Chinese-
to-English) can be seen below:
{NP,1062753}
NP::NP [DNP NP] -> [NP PP]
(
(*score* 0.946640316205534)
(X2::Y1)
(X1::Y2)
)
Each rule has a unique identifier followed by a
synchronous rule for both source and target sides.
The alignment of source-to-target constituents is ex-
plicitly represented using ?X? indices for the source
side, and ?Y? indices for the target side. Rules can
also have lexical items on either side, in which case
no alignment information is required for these ele-
ments. Feature constraints can optionally be speci-
fied for both source and target elements of the rule.
We do not address the learning of feature constraints
in the work described here, and concentrate only
on the acquisition of the synchronous CFG rules.
The rules can be modeled statistically and assigned
scores, which can then be used as decoding features.
The Stat-XFER framework also includes a fully-
implemented transfer engine that applies the trans-
fer grammar to a source-language input sentence at
runtime, and produces collections of scored word
and phrase-level translations according to the gram-
mar. These are collected into a lattice data-structure.
Scores are based on a log-linear combination of sev-
eral features, and a beam-search controls the un-
derlying parsing and transfer process. A second-
stage monotonic decoder is responsible for combin-
ing translation fragments into complete translation
hypotheses (Lavie, 2008)
2 PFA Algorithm for Node Aligment
2.1 Objectives of the Algorithm
Our objective of the first stage of our approach is to
detect sub-sentential constituent correspondences in
parallel sentences, based on phrase-structure parses
for the two corresponding sentences. Given a pair
of parallel sentences and their corresponding parse
trees, our goal is to find pairings of nodes in the
source and target trees whose yields are translation
equivalents of each other. Our current approach only
considers complete constituents and their contigious
yields, and will therefore not align discontiguous
phrases or partial constituents. Similar to phrase ex-
traction methods in PB-SMT, we rely on word-level
alignments (derived manually or automatically) as
indicators for translation equivalence. The assump-
tion applied is that if two words are aligned with
each other, they carry the same meaning and can be
treated as translation equivalents. Constituents are
treated as compositional units of meaning and trans-
lation equivalence.
2.2 Related Work
Aligning nodes in parallel trees has been in-
vestigated by a number of previous researchers.
(Samuelsson and Volk, 2007) describe a process for
manual alignment of nodes in parallel trees. This
approach is well suited for generating reliable par-
allel treebanks, but is impractical for accumulating
resources from large parallel data. (Tinsley et al,
2007) use statistical lexicons derived from automatic
statistical word alignment for aligning nodes in par-
allel trees. In our approach, we use the word align-
ment information directly, which we believe may be
more reliable than the statistical lexicon. (Groves et
al., 2004) propose a method of aligning nodes be-
tween parallel trees automatically, based on word
alignments. In addition to the word alignment in-
formation, their approach uses the constituent labels
of nodes in the trees, and the general structure of the
tree. Our approach is more general in the sense that
we only consider the word alignments, thereby mak-
ing the approach applicable to any parser or phrase-
structure representation, even ones that are quite dif-
ferent for the two languages involved.
88
2.3 Unaligned Words and Contiguity
Word-level alignment of phrase-level translation
equivalents often leaves some words unaligned. For
example, some languages have articles, while oth-
ers do not. It is thus reasonable to expect that con-
stituent pairs in parallel trees that are good transla-
tion equivalents of each other may contain some un-
aligned words. Our PFA node-alignment algorithm
allows for such constituents to be matched.
Different languages have different word orders. In
English, an adjective always comes before a noun,
while in French, in most cases, the adjective fol-
lows its noun. Our node alignment algorithm allows
aligning of constituents regardless of the word order
expressed by the linear precedence relation of their
sub-constituents. As long as one piece of contiguous
text dominated by a node covers the same word-level
alignments as the yield of a node in the parallel tree,
the two nodes can be aligned.
2.4 Wellformedness constraints
Given a pair of word-aligned sentences and their
corresponding parse trees S and T , represented as
sets of constituent nodes, our PFA node alignment
algorithm produces a collection of aligned node-
pairs (Si, Tj). The underlying assumptions of com-
positionality in meaning and word-level alignments
being indicative of translation equivalence lead di-
rectly to the following node alignment wellformed-
ness criteria:
1. If a node Si is linked to a node Tj , then any
node within the subtree of node Si can only be
linked to nodes within the subtree of node Tj .
2. If a node Si is linked to a node Tj , then any
node that dominates the node Si can only be
linked to nodes that dominate the node Tj .
3. If a node Si is linked to a node Tj , then the
word alignments of the yields of the two con-
stituents must satisfy the following:
(a) Every word in the yield of the node Si
must be aligned to one or more words in
the yield of the node Tj , or it should be
unaligned.
(b) Every word in the yield of the node Tj
must be aligned to one or more words in
the yield of the node Si, or it should be
unaligned.
(c) There should be at least one alignment be-
tween the yields of nodes Si and Tj . Thus,
the words in the yields can not all be un-
aligned.
2.5 Arithmetic Representation
Our PFA algorithm uses a arithmetic mapping that
elegently carries over the constraints characterized
by the wellformedness constraints elaborated above.
This mapping is designed to ensure that each aligned
word, which carries a distinct ?piece of meaning?
can be uniquely identified, and also inherently re-
flects the compositional properties of constituent
translation equivalence. This is accomplished by
assigning numerical values to the nodes of the two
parse trees being aligned, in a bottom-up fashion,
starting from the leaf nodes of the trees. Leaf nodes
that correspond to words that are aligned are each
assigned a unique prime number. Unaligned leaf
nodes are assigned a value of ?1?. Constituent nodes
in the parse trees are then assigned a value that is
the product of all its sub-constituent nodes. Because
of the arithmetic property that any composite num-
ber can be uniquely factored into primes, it should
be evident that the value of every constituent node
uniquely identifies the aligned words that are cov-
ered by its yield. Consequently, by assigning the
same prime values to the aligned words of both trees,
retrieving aligned constituent nodes is as simple as
finding the set of nodes in the two trees that carry the
same numerical value. Note that by assigning values
of ?1? to unaligned words, these unaligned words
do not influence the numerical values assigned to
constituent nodes, thus reflecting their treatment as
?don?t cares? with respect to the translation equiva-
lence of constituent nodes.
2.6 Description of the PFA Algorithm
The PFA algorithm uses the concept of ?composite
meaning as prime factorization?, and hence the name
(Prime Factorization and Alignments). The algo-
rithm assigns values to the leaf nodes, propogates
the values up the tree, and then compares the node
values across the trees to align the nodes. As de-
scribed above, leaf nodes which have word align-
ments are assigned unique prime numbers, and the
89
Figure 1: Node-Aligned Parallel Sentences
same prime is assigned to the corresponding aligned
words in the parallel sentences. Leaf nodes corre-
sponding to unaligned words are assigned the value
?1?. The treatment of ?one-to-many? word align-
ments is a special case. Such alignments are con-
sidered to carry the same meaning, and should thus
be assigned the same value. To accomplish this, if a
single word is aligned to multiple words in the other
language, we assign the same prime number to all
words on the ?multiple? side, and assign the product
of these to the single word equivalent.
Another special case is when the parse trees con-
tain unary productions. In this case, the values of
both nodes involved in this production are the same.
Our node alignment algorithm breaks this ?tie? by
selecting the node that is ?lower? in the tree (the
daughter node of the unary production). A simi-
lar situation with two nodes being assigned identical
values can arise when one or more unaligned words
are attached directly to the parent node. Here too,
our algorithm aligns the ?lower? node and leaves
the ?higher? node unaligned. These decisions reflect
our desire to be conservative with respect to such
ambiguous cases, and their implications on the no-
tion of translational equivalence. This also provides
some robustness against noisy alignments.
It is straightfoward to verify that the PFA algo-
rithm satisfies the wellformedness constraints de-
scribed above. Also, since multiplication is com-
mutative, the algorithm is not effected by differing
word orders within parallel constituent structures.
The PFA algorithm run on a sample Chinese-
English parallel sentence is shown in Figure 1. The
value of each node as shown as a part of its label.
The aligned nodes are marked by shapes. A triangle
aligns to a triangle, and squares to squares.
3 Syntax-based Sub-sentential Phrase
Extraction
The alignment of nodes as described in the previous
section allows us to build a comprehensive syntax-
based phrase-to-phrase translation lexicon from a
parallel corpus. To build a syntax-based ?phrase
table?, we simply extract all aligned constituent
nodes along with their yields and enter them into
a database, while accumulating frequency counts.
In addition to the source-to-target phrase corre-
spondences, we record the constituent labels of the
aligned constituent nodes on both the source and tar-
get sides (which may be different). These labels
?connect? the phrases with synatactic transfer rules
during decoding. The set of phrases extracted from
the example sentence in Figure 1 is shown in Fig-
ure 2.
90
Figure 2: Phrases extracted from Aligned Nodes
The process of building syntax-based ?phrase ta-
bles? from large corpora of sentence-parallel data is
quite similar to the corresponding process in phrase-
based SMT systems. Our phrase correspondences,
however, only reflect contiguous and complete con-
stituent correspondences. We also note that the ex-
tracted phrase tables in both approaches can be mod-
eled statistically in similar ways. Similar to common
practice in PB-SMT, we currently use the frequency
counts of the phrases to calculate relative likelihood
estimates and use these as features in our Stat-XFER
decoder.
4 Evaluation of the PFA algorithm
The accuracy of our node alignment algorithm de-
pends on both the quality of the word alignments
as well as the accuracy of the parse trees. We per-
formed several experiments to assess the effects of
these underlying resources on the accuracy of our
approach. The most accurate condition is when the
parallel sentences are manually word-aligned, and
when verified correct parse trees are available for
both source and target sentences. Performance is
expected to degrade when word alignments are pro-
duced using automatic methods, and when correct
parse trees are replaced with automatic parser out-
put. In these experiments, we used a manually word-
aligned parallel Chinese-English TreeBank consist-
ing of 3342 parallel sentences.
4.1 Manual Constituent Node Alignments
We first investigated the accuracy of our approach
under the most accurate condition. We sampled 30
sentences from the Chinese-English treebank cor-
pus. A bilingual expert from our group then man-
ually aligned the nodes in these trees. These node
Precision Recall F-1 F-0.5
0.8129 0.7325 0.7705 0.7841
Table 1: Accuracy of PFA Node Alignments against
Manual Node Alignments
alignments were then used as a ?gold standard?. We
then used the accurate parse trees and the manually
created word alignments for these sentence pairs,
and ran the PFA node algorithm, and compared the
resulting node alignments with the gold standard
alignments. The Precision, Recall, F-1 and F-0.5 re-
sults are reported in Table 1.
We manually inspected cases where there was a
mismatch between the manual and automatic node
alignments, and found several trends. Many of
the alignment differences were the result of one-to-
many or many-to-many word alignemnts. For ex-
ample, in some cases a verb in Chinese was word-
aligned to an auxiliary and a head verb on the en-
glish side (e.g. have and put). The PFA algorithm
in this case node-aligns the VP that governs the Chi-
nese verb to the VP that contains both auxiliary and
head verbs on the English side. The gold standard
human alignments, however, in some cases, aligned
the VP of the Chinese verb to the English VP that
governs just the main verb. Other mismatches were
attributed to errors or inconsistencies in the manual
word alignment and to the treatment of traces and
fillers in the parse trees.
4.2 Effect of Using Automatic Word
Alignments
We next tested how sensitive the PFA algorithm is
to errors in automatic word alignment. We use the
entire 3342 sentences in the parallel treebank for
this experiment. We first ran the algorithm with
the correct parse trees and manual word-alignments
as input. We use the resulting node alignments
as the gold standard in this case. We then used
GIZA++ to get bidirectional word alignments, and
combined them using various strategies. In this sce-
nario, the trees are high-quality (from the treebank),
but the alignments are noisy. The results obtained
are shown in Table 2. Unsurprisingly, the ?Union?
combination method has the best precision but worst
recall, while the ?Intersection? combination method
has the best recall but worst precision. The four
91
Comb Method Prec Rec F-1 F-0.5
Intersection 0.6382 0.5395 0.5846 0.6014
Union 0.8114 0.2915 0.4288 0.5087
Sym1 0.7142 0.4534 0.5546 0.5992
Sym2 0.7135 0.4631 0.5616 0.6045
Grow-Diag-Final 0.7777 0.3462 0.4790 0.5493
Grw-Diag-Fin-And 0.6988 0.4700 0.5619 0.6011
Table 2: Manual Trees, Automatic Node Alignments
other methods for combining word alignments fall
in between. Three of the four (all except ?grow-
diag-final?) behave quite similarly. We generally be-
lieve that precision is somewhat more important than
recall for this task, and have thus used the ?sym2?
method (Ortiz-Mart??nez et al, 2005) (which has the
best F-0.5 score) for our translation experiments.
4.3 Effect of Using Automatic Parses
We evaluated the effect of parsing errors (as re-
flected in automatically derived parse trees) on the
quality of the node alignments. We parsed the tree-
bank corpus on both English and Chinese using the
Stanford parser, and extracted phrases using manual
word alignments. Compared to the phrases extracted
from the manual trees, we obtained a precision of
0.8749, and a recall of 0.7227, that is, an F-0.5 mea-
sure of 0.8174. We then evaluated the most ?noisy?
condition that involves both automatic word align-
ments and automatic parse trees. We evaluated the
phrase extraction with different Viterbi combination
strategies. The ?sym2? combination gave the best
results, with a precision of 0.6251, recall of 0.3566,
thus an F-0.5 measure of 0.4996.
5 Synchronous Tree Fragment and CFG
Rule Extraction
5.1 Related Work
Syntax-based reordering rules can be used as a pre-
processing step for PB-SMT (and other approaches),
to decrease the word-order and syntactic distor-
tion between the source and target languages (Xia
and McCord, 2004). A variety of hierarchical and
syntax-based models, which are applied during de-
coding, have also been developed. Many of these
approaches involve automatic learning and extrac-
tion of the underlying syntax-based rules from data.
The underlying formalisms used has been quite
broad and include simple formalisms such as ITGs
(Wu, 1997), hierarchical synchronous rules (Chiang,
2005), string to tree models by (Galley et al, 2004)
and (Galley et al, 2006), synchronous CFG models
such (Xia and McCord, 2004) (Yamada and Knight,
2001), synchronous Lexical Functional Grammar
inspired approaches (Probst et al, 2002) and others.
Most of the previous approaches for acquiring
syntactic transfer or reordering rules from paral-
lel corpora use syntactic information from only one
side of the parallel corpus, typically the target side.
(Hearne and Way, 2003) describes an approach that
uses syntactic information from the source side to
derive reordering subtrees, which can then be used
within a ?data-oriented translation? (DOT) MT sys-
tem, similar in framework to (Poutsma, 2000). Our
work is different from the above in that we use syn-
tactic trees for both source and target sides to infer
constituent node alignments, from which we then
learn synchronous trees and rules. Our process of
extraction of rules as synchronous trees and then
converting them to synchronous CFG rules is most
similar to that of (Galley et al, 2004).
5.2 Synchronous Tree Fragment Pair
Extraction
The main concept underlying our syntactic rule ex-
traction process is that we treat the node alignments
discovered by the PFA algorithm (described in pre-
vious sections) as synchronous tree decomposition
points. This reflects the fact that these nodes denote
points in the synchronous parse trees where transla-
tion correspondences can be put together composi-
tionally. Using the aligned nodes as decomposition
points, we break apart the synchronous trees into
collections of minimal synchronous tree fragments.
Finally, the synchronous fragments are also con-
verted into synchronous context-free rules. These
are then collected into a database of synchronous
rules.
The input to our rule extraction process consists of
the parallel parse trees along with their node align-
ment information. The constituent nodes in the par-
allel trees that were aligned by the PFA node align-
ment algorithm are treated as tree decomposition
points. At each such decomposition point, spliting
the two parallel trees results in two partial trees or
tree fragments. One synchronous pair consists of
92
the subtrees that are headed by the aligned nodes
where the decomposition took place. Since the sub-
trees are rooted at aligned nodes, their yields are
translation equivalents of each other. The other syn-
chronous tree fragment pair consists of the remain-
ing portions of the trees. The translation equivalence
of the complete tree (or subtree) prior to decomposi-
tion implies that these tree fragments (which exclude
the detached subtrees) also correspond to translation
equivalents. The tree fragments that are obtained by
decomposing the synchronous trees in this fashion
are similar to the Synchronous Tree Insertion Gram-
mar of (Shieber and Schabes, 1990).
We developed a tree traversal algorithm that de-
composes parallel trees into all minimal tree frag-
ments. Given two synchronous trees and their node
alignment decomposition information, our tree frag-
ment extraction algorithm operates by an ?in-order?
traversal of the trees top down, starting from the root
nodes. The traversal can be guided by either the
source or target parse tree. Each node in the tree
that is marked as an aligned node triggers a decom-
position. The subtree that is rooted at this node is
removed from the currently traversed tree. A copy
of the removed subtree is then recursively processed
for top-down decomposition. If the current tree node
being explored is not an aligned node (and thus is not
a decomposition point), the traversal continues down
the tree, possibly all the way to the leaves of the tree.
Decomposition is performed on the corresponding
parallel tree at the same time. We apply this pro-
cess on all the aligned constituent nodes (decompo-
sition points) to obtain all possible decomposed syn-
chronous tree fragment pairs from the original par-
allel parse trees. This results in a collection of all
minimal synchronous subtree fragments. These syn-
chronous subtree fragments are minimal in the sense
that they do not contain any internal aligned nodes.
Another property of the synchronous subtree frag-
ments is that their frontier nodes are either aligned
nodes from the original tree or leaf nodes (corre-
sponding to lexical items). Figure 3 shows some
sample tree fragment pairs that were obtained from
the example discussed earlier in Figure 1.
5.3 Synchronous Transfer Rule Creation
In the last step, we convert the synchronous tree
fragment pairs obtained as described above into syn-
Figure 3: Tree Fragment Pairs Extracted from Aligned
Nodes
chronous context-free rules. This creates rules in a
format that is compatible with the Stat-XFER for-
malism that was described in Section 1. Our system
currently does not use the internal tree structure in-
formation that is contained in the synchronous tree
fragments. Therefore, only the syntactic category la-
bels of the roots of the tree fragments, and the nodes
on the fragment frontier are relevant to decoding.
This in essense corresponds to a ?flattening? of the
synchronous tree fragment into a synchronous con-
text free style rule.
The flattening of the tree fragments is accom-
plished by an ?in-order? traversal on each of the tree
fragments to produce a string representation. Fron-
tier nodes in the fragment are either labeled con-
stituent nodes or leaf nodes of the original parse tree.
These form the right-hand sides of the flattened rule.
The positions of the constituent nodes in the output
string are numbered to keep track of alignment of the
nodes, which is often non-monotonic due to reorder-
ing between the source and target languages. Finally
the root constituent label of the source tree fragment
becomes the source-side parent category of the rule,
while the root label of the target tree fragment be-
comes the target side parent category.
Accurate automatic transfer rule learning re-
quires accurate word alignments and parse struc-
tures. Thus, to favor high precision (at the expense
of some loss of recall), in our work to date on Chi-
nese and other languages, while we extract syntactic
phrases from all available parallel data, we extract
93
rules only from manually word-aligned parsed par-
allel data. To compensate for the limited amount of
data, we generalize the rules as much as possible.
Elements in the rules that originate from leaf nodes
in the parse trees are generalized to their part-of-
speech categories, if the corresponding words were
one-to-one aligned in the parallel sentences. Un-
aligned words and words that are part of one-to-
many alignments are not generalized to the POS
level and remain lexicalized in the final rule.
The phrase table extracted from the corpus and the
rules are scored together to ensure that they are con-
sistent when used in our translation system. For all
Stat-XFER experiments to date, we have used just
the source side conditionig with a constant smooth-
ing factor for robustness to noise.
6 Extraction Applied to Chinese-English
Parallel Data
We used the pipeline of PFA node alignment fol-
lowed by rule extraction to build resources for a
Stat-XFER Chinese-to-English MT system. The
syntax-based phrase table was constructed from
two large parallel corpora released by LDC for the
DARPA/GALE program. The parallel sentences for
both English and Chinese were parsed using the
Stanford parser. The first corpus consists of about
1.2 million sentence pairs. Our extraction process
applied to this corpus resulted in a syntax-based
phrase table of about 9.2 million entries. The other
data source used was a parallel corpus of about 2.6
million sentences, but many of its entries were from
a Chinese-English lexicon. From this corpus, we ex-
tracted 8.75 million phrases.
Rule learning was performed on a 10K-sentence
parallel corpus that was manually word-aligned, re-
leased by LDC for the DARPA/GALE program.
This manually word-aligned corpus includes the par-
allel Chinese-English treebank of 3,343 sentence
pairs. The treebank sentences come with verified
correct parse trees for English and Chinese. The rest
of the 10K corpus was parsed by the Stanford parser.
The complete 10K parallel corpus was node aligned
and rules were extracted as described in Section 5.
Figure 3 shows two synchronous tree fragments that
were extracted from the example node-aligned sen-
tence pair in Figure 1. After generalization and flat-
Figure 4: Rules Extracted from Tree Pairs
Table 3: Statistics for Chinese-English Rules
tening, we obtain rules such as those shown in Fig-
ure 4. The above process resulted in a collection
of almost 100K rules. Some statistics on this rule
set are shown in Table 3. Analysis of this rule set
indicates that only about 4% of these rules were ob-
served more than once in the data. These include
the most general and useful rules for mapping Chi-
nese syntactic structures to their corresponding En-
glish structures. Most of the ?singleton? rules are
highly lexicalized. A large portion of the singleton
rules are noisy rules, but many of them are good and
useful rules. Experiments indicate that removing all
singleton rules hurts translation performance.
7 Conclusions
The process described in this paper provides a fully
automated solution for extracting large collection
of reliable syntax-based phrase tables and syntac-
tic synchronous transfer rules from large volumes
of parsed parallel corpora. In conjunction with the
Stat-XFER syntax-based framework, this provides a
fully automated solution for building syntax-based
MT systems. The current performance of this ap-
proach still lags behind state-of-the-art phrase-based
systems when trained on the same parallel data but is
showing encouraging improvements. Furthermore,
the resources extracted by our process can be used
by various other syntax-based MT approaches.
94
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Su-
san Dumais; Daniel Marcu and Salim Roukos, editors,
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL ?06:
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 961?968, Morristown, NJ, USA.
Association for Computational Linguistics.
Declan Groves, Mary Hearne, and Andy Way. 2004. Ro-
bust sub-sentential alignment of phrase-structure trees.
In COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 1072,
Morristown, NJ, USA. Association for Computational
Linguistics.
M. Hearne and A. Way. 2003. Seeing the wood for the
trees: Data-oriented translation.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical mod-
els. In COLING ?04: Proceedings of the 20th in-
ternational conference on Computational Linguistics,
page 99, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Alon Lavie. 2008. A general search-based syntax-driven
framework for machine translation. In Invited paper in
Proceedings of CICLing-2008, pages 362?375. Com-
putational Linguistics and Intelligent Text Processing,
LNCS 4919,Springer.
D. Ortiz-Mart??nez, I. Garc??a-Varea, and F. Casacuberta.
2005. Thot: a toolkit to train phrase-based statisti-
cal translation models. In Tenth Machine Translation
Summit. AAMT, Phuket, Thailand, September.
Arjen Poutsma. 2000. Data-oriented translation. In Pro-
ceedings of the 18th conference on Computational lin-
guistics, pages 635?641, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,
and Jaime Carbonell. 2002. Mt for minority lan-
guages usingelicitation-based learning of syntactic-
transfer rules. Machine Translation, 17(4):245?270.
Yvonne Samuelsson and Martin Volk. 2007. Alignment
tools for Parallel Treebanks. In Proceedings of the
GLDV Fruhjahrstagung.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proceedings of the 13th
Conference on Computational Linguistics, pages 253?
258, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
John Tinsley, Mary Hearne, and Andy Way. 2007. Ex-
ploiting Parallel Treebanks to Improve Phrase-Based
Statistical Machine Translation. In Proceedings of
the Sixth International Workshop on Treebanks and
Linguistic Theories (TLT-07), pages 175?187, Bergen,
Norway.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical machine translation system with automatically
learned rewrite patterns. In COLING ?04: Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, page 508, Morristown, NJ, USA.
Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL ?01: Proceedings
of the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
95
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 140?144,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
An Improved Statistical Transfer System for French?English
Machine Translation
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark, Alok Parlikar, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,vamshi,jhclark,aup,alavie}@cs.cmu.edu
Abstract
This paper presents the Carnegie Mellon
University statistical transfer MT system
submitted to the 2009 WMT shared task
in French-to-English translation. We de-
scribe a syntax-based approach that incor-
porates both syntactic and non-syntactic
phrase pairs in addition to a syntactic
grammar. After reporting development
test results, we conduct a preliminary anal-
ysis of the coverage and effectiveness of
the system?s components.
1 Introduction
The statistical transfer machine translation group
at Carnegie Mellon University has been devel-
oping a hybrid approach combining a traditional
rule-based MT system and its linguistically ex-
pressive formalism with more modern techniques
of statistical data processing and search-based de-
coding. The Stat-XFER framework (Lavie, 2008)
provides a general environment for building new
MT systems of this kind. For a given language
pair or data condition, the framework depends on
two main resources extracted from parallel data: a
probabilistic bilingual lexicon, and a grammar of
probabilistic synchronous context-free grammar
rules. Additional monolingual data, in the form of
an n-gram language model in the target language,
is also used. The statistical transfer framework op-
erates in two stages. First, the lexicon and gram-
mar are applied to synchronously parse and trans-
late an input sentence; all reordering is applied
during this stage, driven by the syntactic grammar.
Second, a monotonic decoder runs over the lat-
tice of scored translation pieces produced during
parsing and assembles the highest-scoring overall
translation according to a log-linear feature model.
Since our submission to last year?s Workshop
on Machine Translation shared translation task
(Hanneman et al, 2008), we have made numerous
improvements and extensions to our resource ex-
traction and processing methods, resulting in sig-
nificantly improved translation scores. In Section
2 of this paper, we trace our current methods for
data resource management for the Stat-XFER sub-
mission to the 2009 WMT shared French?English
translation task. Section 3 explains our tuning pro-
cedure, and Section 4 gives our experimental re-
sults on various development sets and offers some
preliminary analysis.
2 System Construction
Because of the additional data resources provided
for the 2009 French?English task, our system this
year is trained on nearly eight times as much
data as last year?s. We used three officially pro-
vided data sets to make up a parallel corpus for
system training: version 4 of the Europarl cor-
pus (1.43 million sentence pairs), the News Com-
mentary corpus (0.06 million sentence pairs), and
the pre-release version of the new Giga-FrEn cor-
pus (8.60 million sentence pairs)1. The combined
corpus of 10.09 million sentence pairs was pre-
processed to remove blank lines, sentences of 80
words or more, and sentence pairs where the ra-
tio between the number of English and French
words was larger than 5 to 1 in either direction.
These steps removed approximately 3% of the cor-
pus. Given the filtered corpus, our data prepara-
tion pipeline proceeded according to the descrip-
tions below.
1Because of data processing time, we were unable to use
the larger verions 1 or 2 of Giga-FrEn released later in the
evaluation period.
140
2.1 Parsing and Word Alignment
We parsed both sides of our parallel corpus with
independent automatic constituency parsers. We
used the Berkeley parser (Petrov and Klein, 2007)
for both English and French, although we obtained
better results for French by tokenizing the data
with our own script as a preprocessing step and
not allowing the parser to change it. There were
approximately 220,000 English sentences that did
not return a parse, which further reduced the size
of our training corpus by 2%.
After parsing, we re-extracted the leaf nodes
of the parse trees and statistically word-aligned
the corpus using a multi-threaded implementa-
tion (Gao and Vogel, 2008) of the GIZA++ pro-
gram (Och and Ney, 2003). Unidirectional align-
ments were symmetrized with the ?grow-diag-
final? heuristic (Koehn et al, 2005).
2.2 Phrase Extraction and Combination
Phrase extraction for last year?s statistical transfer
system used automatically generated parse trees
on both sides of the corpus as absolute constraints:
a syntactic phrase pair was extracted from a given
sentence only when a contiguous sequence of En-
glish words exactly made up a syntactic con-
stituent in the English parse tree and could also
be traced though symmetric word alignments to a
constituent in the French parse tree. While this
?tree-to-tree? extraction method is precise, it suf-
fers from low recall and results in a low-coverage
syntactic phrase table. Our 2009 system uses an
extended ?tree-to-tree-string? extraction process
(Ambati and Lavie, 2008) in which, if no suit-
able equivalent is found in the French parse tree
for an English node, a copy of the English node is
projected into the French tree, where it spans the
French words aligned to the yield of the English
node. This method can result in a 50% increase
in the number of extracted syntactic phrase pairs.
Each extracted phrase pair retains a syntactic cat-
egory label; in our current system, the node label
in the English parse tree is used as the category for
both sides of the bilingual phrase pair, although we
subsequently map the full set of labels used by the
Berkeley parser down to a more general set of 19
syntactic categories.
We also ran ?standard? phrase extraction on the
same corpus using Steps 4 and 5 of the Moses sta-
tistical machine translation training script (Koehn
et al, 2007). The two types of phrases were then
merged in a syntax-prioritized combination that
removes all Moses-extracted phrase pairs that have
source sides already covered by the tree-to-tree-
string syntactic phrase extraction. The syntax pri-
oritization has the advantage of still including a se-
lection of non-syntactic phrases while producing a
much smaller phrase table than a direct combina-
tion of all phrase pairs of both types. Previous ex-
periments we conducted indicated that this comes
with only a minor drop in automatic metric scores.
In our current submission, we modify the proce-
dure slightly by removing singleton phrase pairs
from the syntactic table before the combination
with Moses phrases. The coverage of the com-
bined table is not affected ? our syntactic phrase
extraction algorithm produces a subset of the non-
syntactic phrase pairs extracted from Moses, up to
phrase length constraints ? but the removal al-
lows Moses-extracted versions of some phrases to
survive syntax prioritization. In effect, we are lim-
iting the set of category-labeled syntactic transla-
tions we trust to those that have been seen more
than once in our training data. For a given syn-
tactic phrase pair, we also remove all but the most
frequent syntactic category label for the pair; this
removes a small number of entries from our lexi-
con in order to limit label ambiguity, but does not
affect coverage.
From our training data, we extracted 27.6 mil-
lion unique syntactic phrase pairs after single-
ton removal, reducing this set to 27.0 million en-
tries after filtering for category label ambiguity.
Some 488.7 million unique phrase pairs extracted
from Moses were reduced to 424.0 million after
syntax prioritization. (The remaining 64.7 mil-
lion phrase pairs had source sides already covered
by the 27.0 million syntactically extracted phrase
pairs, so they were thrown out.) This means non-
syntactic phrases outnumber syntactic phrases by
nearly 16 to 1. However, when filtering the phrase
table to a particular development or test set, we
find the syntactic phrases play a larger role, as this
ratio drops to approximately 3 to 1.
Sample phrase pairs from our system are shown
in Figure 1. Each pair includes two rule scores,
which we calculate from the source-side syntac-
tic category (cs), source-side text (ws), target-side
category (ct), and target-side text (wt). In the
case of Moses-extracted phrase pairs, we use the
?dummy? syntactic category PHR. Rule score rt|s
is a maximum likelihood estimate of the distri-
141
cs ct ws wt rt|s rs|t
ADJ ADJ espagnols Spanish 0.8278 0.1141
N N repre?sentants officials 0.0653 0.1919
NP NP repre?sentants de la Commission Commission officials 0.0312 0.0345
PHR PHR haute importance a` very important to 0.0357 0.0008
PHR PHR est charge? de has responsibility for 0.0094 0.0760
Figure 1: Sample lexical entries, including non-syntactic phrases, with rule scores (Equations 1 and 2).
bution of target-language translations and source-
and target-language syntactic categories given the
source string (Equation 1). The rs|t score is simi-
lar, but calculated in the reverse direction to give a
source-given-target probability (Equation 2).
rt|s =
#(wt, ct, ws, cs)
#(ws) + 1
(1)
rs|t =
#(wt, ct, ws, cs)
#(wt) + 1
(2)
Add-one smoothing in the denominators counter-
acts overestimation of the rule scores of lexical en-
tries with very infrequent source or target sides.
2.3 Syntactic Grammar
Syntactic phrase extraction specifies a node-to-
node alignment across parallel parse trees. If these
aligned nodes are used as decomposition points,
a set of synchronous context-free rules that pro-
duced the trees can be collected. This is our pro-
cess of syntactic grammar extraction (Lavie et al,
2008). For our 2009 WMT submission, we ex-
tracted 11.0 million unique grammar rules, 9.1
million of which were singletons, from our paral-
lel parsed corpus. These rules operate on our syn-
tactically extracted phrase pairs, which have cat-
egory labels, but they may also be partially lexi-
calized with explicit source or target word strings.
Each extracted grammar rule is scored according
to Equations 1 and 2, where now the right-hand
sides of the rule are used as ws and wt.
As yet, we have made only minimal use of the
Stat-XFER framework?s grammar capabilities, es-
pecially for large-scale MT systems. For the cur-
rent submission, the syntactic grammar consisted
of 26 manually chosen high-frequency grammar
rules that carry out some reordering between En-
glish and French. Since rules for high-level re-
ordering (near the top of the parse tree) are un-
likely to be useful unless a large amount of parse
structure can first be built, we concentrate our
rules on low-level reorderings taking place within
or around small constituents. Our focus for this
selection is the well-known repositioning of adjec-
tives and adjective phrases when translating from
French to English, such as from le Parlement eu-
rope?en to the European Parliament or from l? in-
tervention forte et substantielle to the strong and
substantial intervention. Our grammar thus con-
sists of 23 rules for building noun phrases, two
rules for building adjective phrases, and one rule
for building verb phrases.
2.4 English Language Model
We built a suffix-array language model (Zhang and
Vogel, 2006) on approximately 700 million words
of monolingual data: the unfiltered English side of
our parallel training corpus, plus the 438 million
words of English monolingual news data provided
for the WMT 2009 shared task. With the relatively
large amount of data available, we made the some-
what unusual decision of building our language
model (and all other data resources for our system)
in mixed case, which adds approximately 12.3%
to our vocabulary size. This saves us the need to
build and run a recaser as a postprocessing step
on our output. Our mixed-case decision may also
be validated by preliminary test set results, which
show that our submission has the smallest drop in
BLEU score (0.0074) between uncased and cased
evaluation of any system in the French?English
translation task.
3 System Tuning
Stat-XFER uses a log-linear combination of seven
features in its scoring of translation fragments:
language model probability, source-given-target
and target-given-source rule probabilities, source-
given-target and target-given-source lexical prob-
abilities, a length score, and a fragmentation score
based on the number of parsed translation frag-
ments that make up the output sentence. We tune
the weights for these features with several rounds
of minimum error rate training, optimizing to-
142
Primary Contrastive
Data Set METEOR BLEU TER METEOR BLEU TER
news-dev2009a-425 0.5437 0.2299 60.45 ? ? ?
news-dev2009a-600 ? ? ? 0.5134 0.2055 63.46
news-dev2009b 0.5263 0.2073 61.96 0.5303 0.2104 61.74
nc-test2007 0.6194 0.3282 51.17 0.6195 0.3226 51.49
Figure 2: Primary and contrastive system results on tuning and development test sets.
wards the BLEU metric. For each tuning itera-
tion, we save the n-best lists output by the sys-
tem from previous iterations and concatenate them
onto the current n-best list in order to present the
optimizer with a larger variety of translation out-
puts and score values.
From the provided ?news-dev2009a? develop-
ment set we create two tuning sets: one using the
first 600 sentences of the data, and a second using
the remaining 425 sentences. We tuned our sys-
tem separately on each set, saving the additional
?news-dev2009b? set as a final development test to
choose our primary and contrastive submissions2.
At run time, our full system takes on average be-
tween four and seven seconds to translate each in-
put sentence, depending on the size of the final
bilingual lexicon.
4 Evaluation and Analysis
Figure 2 shows the results of our primary and con-
trastive systems on four data sets. First, we report
final (tuned) performance on our two tuning sets
? the last 425 sentences of news-dev2009a for the
primary system, and the first 600 sentences of the
same set for the contrastive. We also include our
development test (news-dev2009b) and, for addi-
tional comparison, the ?nc-test2007? news com-
mentary test set from the 2007 WMT shared task.
For each, we give case-insensitive scores on ver-
sion 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-
style BLEU (Papineni et al, 2002), and version 5
of TER (Snover et al, 2006).
From these results, we highlight two interest-
ing areas of analysis. First, the low tuning and
development test set scores bring up questions
about system coverage, given that the news do-
main was not strongly represented in our system?s
2Due to a data processing error, the choice of the primary
submission was based on incorrectly computed scores. In
fact, the contrastive system has better performance on our de-
velopment test set.
training data. We indeed find a significantly larger
proportion of out-of-vocabulary (OOV) words in
news-domain sets: the news-dev2009b set is trans-
lated by our primary submission with 402 of 6263
word types (6.42%) or 601 of 27,821 word tokens
(2.16%) unknown. The same system running on
the 2007 WMT ?test2007? set of Europarl-derived
data records an OOV rate of only 87 of 7514
word types (1.16%) or 105 of 63,741 word tokens
(0.16%).
Second, we turn our attention to the usefulness
of the syntactic grammar. Though small, we find
it to be both beneficial and precise. In the 1026-
sentence news-dev2009b set, for example, we find
351 rule applications ? the vast majority of them
(337) building noun phrases. The three most fre-
quently occurring rules are those for reordering the
sequence [DET N ADJ] to [DET ADJ N] (52 oc-
currences), the sequence [N ADJ] to [ADJ N] (51
occurrences), and the sequence [N1 de N2] to [N2
N1] (45 occurrences). We checked precision by
manually reviewing the 52 rule applications in the
first 150 sentences of news-dev2009b. There, 41
of the occurrences (79%) were judged to be cor-
rect and beneficial to translation output. Of the
remainder, seven were judged incorrect or detri-
mental and four were judged either neutral or of
unclear benefit.
We expect to continue to analyze the output and
effectiveness of our system in the coming months.
In particular, we would like to learn more about
the usefulness of our 26-rule grammar with the
view of using significantly larger grammars in fu-
ture versions of our system.
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank Yahoo! for the use of theM45 research com-
puting cluster, where we ran the parsing stage of
our data processing.
143
References
Vamshi Ambati and Alon Lavie. 2008. Improving
syntax driven translation models by re-structuring
divergent and non-isomorphic parse tree structures.
In Proceedings of the Eighth Conference of the As-
sociation for Machine Translation in the Americas,
pages 235?244, Waikiki, HI, October.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems
for French?English and German?English machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 163?166,
Columbus, OH, June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of IWSLT 2005, Pittsburgh, PA, Oc-
tober.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL 2007 Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Carnegie Mellon University, Pittsburgh, PA, Decem-
ber.
144
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 58?61,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Proactive Learning for Building Machine Translation Systems for Minority
Languages
Vamshi Ambati
vamshi@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Jaime Carbonell
jgc@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
Building machine translation (MT) for many
minority languages in the world is a serious
challenge. For many minor languages there is
little machine readable text, few knowledge-
able linguists, and little money available for
MT development. For these reasons, it be-
comes very important for an MT system to
make best use of its resources, both labeled
and unlabeled, in building a quality system.
In this paper we argue that traditional active
learning setup may not be the right fit for seek-
ing annotations required for building a Syn-
tax Based MT system for minority languages.
We posit that a relatively new variant of active
learning, Proactive Learning, is more suitable
for this task.
1 Introduction
Speakers of minority languages could benefit from
fluent machine translation (MT) between their native
tongue and the dominant language of their region.
But scarcity in capital and know-how has largely
restricted machine translation to the dominant lan-
guages of first world nations. To lower the barriers
surrounding MT system creation, we must reduce
the time and resources needed to develop MT for
new language pairs. Syntax based MT has proven
to be a good choice for minority language scenario
(Lavie et al, 2003). While the amount of paral-
lel data required to build such systems is orders of
magnitude smaller than corresponding phrase based
statistical systems (Koehn et al, 2003), the variety
of linguistic annotation required is greater. Syntax
based MT systems require lexicons that provide cov-
erage for the target translations, synchronous gram-
mar rules that define the divergences in word-order
across the language-pair. In case of minority lan-
guages one can only expect to find meagre amount
of such data, if any. Building such resources effec-
tively, within a constrained budget, and deploying an
MT system is the need of the day.
We first consider ?Active Learning? (AL) as a
framework for building annotated data for the task
of MT. However, AL relies on unrealistic assump-
tions related to the annotation tasks. For instance,
AL assumes there is a unique omniscient oracle. In
MT, it is possible and more general to have multiple
sources of information with differing reliabilities or
areas of expertise. A literate bilingual speaker with
no extra training can produce translations for word,
phrase or sentences and even align them. But it re-
quires a trained linguist to produce syntactic parse
trees. AL also assumes that the single oracle is per-
fect, always providing a correct answer when re-
quested. In reality, an oracle (human or machine)
may be incorrect (fallible) with a probability that
should be a function of the difficulty of the question.
There is also no notion of cost associated with the
annotation task, that varies across the input space.
But in MT, it is easy to see that length of a sentence
and cost of translation are superlinear. Also not all
annotation tasks for MT have the same level of dif-
ficulty or cost. For example, it is relatively cheap
to ask a bilingual speaker whether a word, phrase
or sentence was correctly translated by the system,
but a bit more expensive to ask for a correction. As-
sumptions like these render active learning unsuit-
58
able for our task at hand which is building an MT
system for languages with limited resources. We
make the case for ?Proactive Learning? (Donmez
and Carbonell, 2008) as a solution for this scenario.
In the rest of the paper, we discuss syntax based
MT approach in Section 2. In Section 3 we first
discuss active learning approaches for MT and de-
tail the characteristics of MT for minority languages
problem that render traditional active learning un-
suitable for practical purposes. In Section 4 we dis-
cuss proactive learning as a potential solution for the
current problem. We conclude with some challenges
that still remain in applying proactive learning for
MT.
2 Syntax Based Machine Translation
In recent years, corpus based approaches to ma-
chine translation have become predominant, with
Phrase Based Statistical Machine Translation (PB-
SMT) (Koehn et al, 2003) being the most ac-
tively progressing area. Recent research in syn-
tax based machine translation (Yamada and Knight,
2001; Chiang, 2005) incorporates syntactic informa-
tion to ameliorate the reordering problem faced by
PB-SMT approaches. While traditional approaches
to syntax based MT were dependent on availabil-
ity of manual grammar, more recent approaches op-
erate within the resources of PB-SMT and induce
hierarchical or linguistic grammars from existing
phrasal units, to provide better generality and struc-
ture for reordering (Yamada and Knight, 2001; Chi-
ang, 2005; Wu, 1997).
2.1 Resources for Syntax MT
Syntax based approaches to MT seek to leverage the
structure of natural language to automatically induce
MT systems. Depending upon the MT system and
the paradigm, the resource requirements may vary
and could also include modules such as morpholog-
ical analyzers, sense disambiguation modules, gen-
erators etc. A detailed discussion of the comprehen-
sive pipeline, may be out of the scope of this pa-
per, more so because such resources can not be ex-
pected in a low-resource language scenario. We only
focus on the quintessential set of modules for MT
pipeline - data acquisition, word-alignment, syntac-
tic analysis etc. The resources can broadly be cat-
egorized as ?monolingual? vs ?bilingual? depending
upon whether it requires knowledge in one language
or both languages for annotation. A sample of the
different kinds of data and annotation that is ex-
pected by an MT system is shown below. Each of
the additional information can be seen as extra an-
notations for the ?Source? sentence. The language
of target in the example is ?Hindi?.
? Source: John ate an apple
? Target: John ne ek seb khaya
? Alignment: (1,1),(2,5),(3,3),(4,4)
? SourceParse: (S (NP (NNP John)) (VP (VBD
ate) (NP (DT an) (NN apple))))
? Lexicon: (seb? apple),(ate? khaya)
? Grammar: VP: V NP? NP V
3 Active Learning for MT
Modern syntax based MT rides on the success of
both Statistical Machine Translation and Statistical
Parsing. Active learning has been applied to Statis-
tical Parsing (Hwa, 2004; Baldridge and Osborne,
2003) to improve sample selection for manual anno-
tation. In case of MT, active learning has remained
largely unexplored. Some attempts include training
multiple statistical MT systems on varying amounts
of data, and exploring a committee based selection
for re-ranking the data to be translated and included
for re-training. But this does not apply to training in
a low-resource scenario where data is scarce.
In the rest of the section we discuss the different
scenarios that arise in gathering of annotation for
MT under a traditional ?active learning? setup and
discuss the characteristics of the task that render it
difficult.
3.1 Multiple Oracles
For each of the sub-tasks of annotation, in reality
we have multiple sources of information or multi-
ple oracles. We can elicit translations for building
a parallel corpus from bilingual speakers who speak
both the languages with certain accuracy or from a
linguist who is well educated in the formal sense
of the languages. With the success of collabora-
tive sites like Amazon?s ?Mechanical Turk? 1, one
1http://www.mturk.com/
59
can provide the task of annotation to multiple ora-
cles on the internet (Snow et al, 2008). The task
of word alignment can be posed in a similar fash-
ion too. More interestingly, there are statistical tools
like GIZA 2 that take as input un-annotated paral-
lel data and propose automatic correspondences be-
tween words in the language-pair, giving scope to
?machine oracles?.
3.2 Varying Quality and Reliability
Oracles also vary on the correctness of the answers
they provide (quality) as well as their availability
(robustness) to answer. One typical distinction is
?human oracles? vs ?machine oracles?. Human or-
acle produce higher quality annotations when com-
pared to a machine oracle. We would prefer a tree
bank of parse trees that were manually created over
automatically generated tree banks. Similar is the
case with word-alignment and other tasks of trans-
lation. Some oracles are ?reluctant? to produce an
output, for example parsers tend to break on really
long sentences, but when they produce an output
we can associate some confidence with it about the
quality. One can expect a human oracle to produce
parse trees for long sentences, but the quality could
be questionable.
3.3 Non-uniform costs
Each of the annotation tasks has a non-uniform cost
associated with it, the distribution of which is de-
pendent upon the difficulty over the input space.
Clearly, length of the sentence is a good indicator of
the cost. It takes much longer to translate a sentence
of 100 words than to translate one with 10 words. It
takes at least twice as long to create word-alignment
correspondences for a sentence-pair with 40 tokens
than a pair with 20 tokens. Similarly, a human takes
much longer to manually create parse tree for a long
sentence than a short sentence.
It is also the case that not all oracles have the
same non-uniform cost distribution over the input
space. Some oracles are more expensive than the
others. For example a practicing linguist?s time is
perhaps costlier than that of an undergraduate who
is a bilingual speaker. As noticed above, this may
reflect upon the quality of annotation for the task,
2http://www.fjoch.com/GIZA++.html
but sometimes a tradeoff to make is cost vs qual-
ity. We can not afford to introduce a grammar rule
of low-quality into the system, but can possibly do
away with an incorrect word-correspondence link.
4 Proactive Learning
Proactive learning (Donmez and Carbonell, 2008) is
a generalization of active learning designed to re-
lax unrealistic assumptions and thereby reach prac-
tical applications. Active learning seeks to select the
most informative unlabeled instances and ask an om-
niscient oracle for their labels, so as to retrain the
learning algorithm maximizing accuracy. However,
the oracle is assumed to be infallible (never wrong),
indefatigable (always answers), individual (only one
oracle), and insensitive to costs (always free or al-
ways charges the same). Proactive learning relaxes
all these four assumptions, relying on a decision-
theoretic approach to jointly select the optimal or-
acle and instance, by casting the problem as a utility
optimization problem subject to a budget constraint.
maximize E[V (S)] subject to B
maxS?ULE[V (S)]? ?(
?
k
tk ? Ck)s.t
?
k
tk ? Ck = B
The above equation can be interpreted as maximiz-
ing the expected value of labeling the input set S
under the budget constraint B. The subscript k de-
notes the oracle from which the answer was elicited
under a cost function C. A greedy approximation of
the above results in the equation 1, where Ek[V (x)]
is the expected value of information of the example
x corresponding to oracle k. One can design inter-
esting functions that calculate V (x) in case of MT.
For example, selecting short sentences with an unre-
solved linguistic issue could maximize the utility of
the data at a low cost.
(x?, k?) = argmaxx?UEk[V (x)] subject to B (1)
We now turn to how proactive learning framework
helps solve the issues raised for active learning in
MT in section 3. We can address the issue of multi-
ple oracles where one oracle is fallible or reluctant to
answer, by factoring into Equation 2 its probability
60
function for returning an answer. The score returned
by such a factoring can be called the utility associ-
ated with that input for a particular oracle. We call
this U(x, k). A similar factorization can be done in
order to address the issue of oracles that are fallible.
U(x, k) = P (ans|x, k) ? V (x)? Ck
(x?, k?) = argmaxx?U U(x, k)
Since we do not have the P (ans/x, k) distribu-
tion information for each oracle, proactive learning
proposes to discover this in a discovery phase under
some allocated budget Bd. Once we have an esti-
mate from the discovery phase, the rest of the label-
ing proceeds according to the optimization function.
For more details of the algorithms refer (Donmez
and Carbonell, 2008). Finally, we can also relax the
assumption of uniform cost per annotation, but re-
placing the Ck term in the above equations with a
Cnon?unifk function denoting the non-uniform cost
function associated with the oracle.
5 Future Challenges
While proactive learning is a good framework for
building MT systems for minority languages, there
are however a few issues that still remain that need
careful attention.
Joint Utility: In a complex system like MT where
different models combine forces to produce the
translation we have a situation where we need to op-
timize not only for an input and the oracle, but also
the kind of annotation we would like to elicit. For
example given a particular translation model, we do
not know if the most optimal thing at a given point is
to seek more word-alignment annotation from a par-
ticular ?alignment oracle? or seek parse annotation
from a ?parsing oracle?.
Machine oracles vs Human oracles: The assump-
tion with an oracle is that the knowledge and exper-
tise of the oracle does not change over the course of
annotation. We do not assume that the oracle learns
over time and hence the speed of annotation or per-
haps the accuracy of annotation increases. This is
however very common with ?machine oracles?. For
example, an oracle that suggests automatic align-
ment of data using statistical concordances may ini-
tially be unreliable due to the less amount of data it is
trained on, but as it receives more data, the estimates
get better and so the system gets more reliable.
Evaluation: Performance of underlying system is
typically done by well understood metrics like pre-
cision/recall. However, evaluation of MT output
is quite subjective and automatic evaluation met-
rics may be too coarse to distinguish the nuances
of translation. This becomes quite important in
an online active learning setup, where we add an-
notated data incrementally, and the immediately
trained translation models are not sufficient to make
a difference in the scores of the evaluation metric.
References
Jason Baldridge and Miles Osborne. 2003. Active learn-
ing for hpsg parse selection. In Proc. of the HLT-
NAACL 2003, pages 17?24, Morristown, NJ, USA.
Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. 43rd ACL,
pages 263?270, Morristown, NJ, USA. Association for
Computational Linguistics.
Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In CIKM ?08, pages 619?628, New
York, NY, USA. ACM.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Comput. Linguist., 30(3):253?276.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the HLT/NAACL, Edomonton, Canada.
Alon Lavie, Stephan Vogel, Lori Levin, Erik Peter-
son, Katharina Probst, Ariadna Font Llitjo?s, Rachel
Reynolds, Jaime Carbonell, and Richard Cohen. 2003.
Experiments with a hindi-to-english transfer-based mt
system under a miserly data scenario. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 2(2):143?163.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP 2008, pages 254?
263, Honolulu, Hawaii, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL ?01,
pages 523?530, Morristown, NJ, USA. Association for
Computational Linguistics.
61
Proceedings of the ACL 2010 Conference Short Papers, pages 365?370,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Active Learning-Based Elicitation for Semi-Supervised Word Alignment
Vamshi Ambati, Stephan Vogel and Jaime Carbonell
{vamshi,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Semi-supervised word alignment aims to
improve the accuracy of automatic word
alignment by incorporating full or par-
tial manual alignments. Motivated by
standard active learning query sampling
frameworks like uncertainty-, margin- and
query-by-committee sampling we propose
multiple query strategies for the alignment
link selection task. Our experiments show
that by active selection of uncertain and
informative links, we reduce the overall
manual effort involved in elicitation of
alignment link data for training a semi-
supervised word aligner.
1 Introduction
Corpus-based approaches to machine translation
have become predominant, with phrase-based sta-
tistical machine translation (PB-SMT) (Koehn et
al., 2003) being the most actively progressing area.
The success of statistical approaches to MT can
be attributed to the IBM models (Brown et al,
1993) that characterize word-level alignments in
parallel corpora. Parameters of these alignment
models are learnt in an unsupervised manner us-
ing the EM algorithm over sentence-level aligned
parallel corpora. While the ease of automati-
cally aligning sentences at the word-level with
tools like GIZA++ (Och and Ney, 2003) has en-
abled fast development of SMT systems for vari-
ous language pairs, the quality of alignment is typ-
ically quite low for language pairs like Chinese-
English, Arabic-English that diverge from the in-
dependence assumptions made by the generative
models. Increased parallel data enables better es-
timation of the model parameters, but a large num-
ber of language pairs still lack such resources.
Two directions of research have been pursued
for improving generative word alignment. The
first is to relax or update the independence as-
sumptions based on more information, usually
syntactic, from the language pairs (Cherry and
Lin, 2006; Fraser and Marcu, 2007a). The sec-
ond is to use extra annotation, typically word-level
human alignment for some sentence pairs, in con-
junction with the parallel data to learn alignment
in a semi-supervised manner. Our research is in
the direction of the latter, and aims to reduce the
effort involved in hand-generation of word align-
ments by using active learning strategies for care-
ful selection of word pairs to seek alignment.
Active learning for MT has not yet been ex-
plored to its full potential. Much of the litera-
ture has explored one task ? selecting sentences
to translate and add to the training corpus (Haf-
fari and Sarkar, 2009). In this paper we explore
active learning for word alignment, where the in-
put to the active learner is a sentence pair (S, T )
and the annotation elicited from human is a set of
links {aij , ?si ? S, tj ? T}. Unlike previous ap-
proaches, our work does not require elicitation of
full alignment for the sentence pair, which could
be effort-intensive. We propose active learning
query strategies to selectively elicit partial align-
ment information. Experiments in Section 5 show
that our selection strategies reduce alignment error
rates significantly over baseline.
2 Related Work
Researchers have begun to explore models that
use both labeled and unlabeled data to build
word-alignment models for MT. Fraser and Marcu
(2006) pose the problem of alignment as a search
problem in log-linear space with features com-
ing from the IBM alignment models. The log-
365
linear model is trained on available labeled data
to improve performance. They propose a semi-
supervised training algorithm which alternates be-
tween discriminative error training on the la-
beled data to learn the weighting parameters and
maximum-likelihood EM training on unlabeled
data to estimate the parameters. Callison-Burch
et al (2004) also improve alignment by interpolat-
ing human alignments with automatic alignments.
They observe that while working with such data
sets, alignments of higher quality should be given
a much higher weight than the lower-quality align-
ments. Wu et al (2006) learn separate models
from labeled and unlabeled data using the standard
EM algorithm. The two models are then interpo-
lated to use as a learner in the semi-supervised
algorithm to improve word alignment. To our
knowledge, there is no prior work that has looked
at reducing human effort by selective elicitation of
partial word alignment using active learning tech-
niques.
3 Active Learning for Word Alignment
Active learning attempts to optimize performance
by selecting the most informative instances to la-
bel where ?informativeness? is defined as maximal
expected improvement in accuracy. The objective
is to select optimal instance for an external expert
to label and then run the learning method on the
newly-labeled and previously-labeled instances to
minimize prediction or translation error, repeat-
ing until either the maximal number of external
queries is reached or a desired accuracy level is
achieved. Several studies (Tong and Koller, 2002;
Nguyen and Smeulders, 2004; Donmez and Car-
bonell, 2008) show that active learning greatly
helps to reduce the labeling effort in various clas-
sification tasks.
3.1 Active Learning Setup
We discuss our active learning setup for word
alignment in Algorithm 1. We start with an un-
labeled dataset U = {(Sk, Tk)}, indexed by k,
and a seed pool of partial alignment links A0 =
{akij , ?si ? Sk, tj ? Tk}. This is usually an empty
set at iteration t = 0. We iterate for T itera-
tions. We take a pool-based active learning strat-
egy, where we have access to all the automatically
aligned links and we can score the links based
on our active learning query strategy. The query
strategy uses the automatically trained alignment
model Mt from current iteration t for scoring the
links. Re-training and re-tuning an SMT system
for each link at a time is computationally infeasi-
ble. We therefore perform batch learning by se-
lecting a set of N links scored high by our query
strategy. We seek manual corrections for the se-
lected links and add the alignment data to the
current labeled data set. The word-level aligned
labeled data is provided to our semi-supervised
word alignment algorithm for training an align-
ment model Mt+1 over U .
Algorithm 1 AL FOR WORD ALIGNMENT
1: Unlabeled Data Set: U = {(Sk, Tk)}
2: Manual Alignment Set : A0 = {akij ,?si ?
Sk, tj ? Tk}
3: Train Semi-supervised Word Alignment using
(U , A0)?M0
4: N : batch size
5: for t = 0 to T do
6: Lt = LinkSelection(U ,At,Mt,N )
7: Request Human Alignment for Lt
8: At+1 = At + Lt
9: Re-train Semi-Supervised Word Align-
ment on (U,At+1)?Mt+1
10: end for
We can iteratively perform the algorithm for a
defined number of iterations T or until a certain
desired performance is reached, which is mea-
sured by alignment error rate (AER) (Fraser and
Marcu, 2007b) in the case of word alignment. In
a more typical scenario, since reducing human ef-
fort or cost of elicitation is the objective, we iterate
until the available budget is exhausted.
3.2 Semi-Supervised Word Alignment
We use an extended version of MGIZA++ (Gao
and Vogel, 2008) to perform the constrained semi-
supervised word alignment. Manual alignments
are incorporated in the EM training phase of these
models as constraints that restrict the summation
over all possible alignment paths. Typically in the
EM procedure for IBM models, the training pro-
cedure requires for each source sentence position,
the summation over all positions in the target sen-
tence. The manual alignments allow for one-to-
many alignments and many-to-many alignments
in both directions. For each position i in the source
sentence, there can be more than one manually
aligned target word. The restricted training will
allow only those paths, which are consistent with
366
the manual alignments. Therefore, the restriction
of the alignment paths reduces to restricting the
summation in EM.
4 Query Strategies for Link Selection
We propose multiple query selection strategies for
our active learning setup. The scoring criteria is
designed to select alignment links across sentence
pairs that are highly uncertain under current au-
tomatic translation models. These links are diffi-
cult to align correctly by automatic alignment and
will cause incorrect phrase pairs to be extracted in
the translation model, in turn hurting the transla-
tion quality of the SMT system. Manual correc-
tion of such links produces the maximal benefit to
the model. We would ideally like to elicit the least
number of manual corrections possible in order to
reduce the cost of data acquisition. In this section
we discuss our link selection strategies based on
the standard active learning paradigm of ?uncer-
tainty sampling?(Lewis and Catlett, 1994). We use
the automatically trained translation model ?t for
scoring each link for uncertainty, which consists of
bidirectional translation lexicon tables computed
from the bidirectional alignments.
4.1 Uncertainty Sampling: Bidirectional
Alignment Scores
The automatic Viterbi alignment produced by
the alignment models is used to obtain transla-
tion lexicons. These lexicons capture the condi-
tional distributions of source-given-target P (s/t)
and target-given-source P (t/s) probabilities at the
word level where si ? S and tj ? T . We de-
fine certainty of a link as the harmonic mean of the
bidirectional probabilities. The selection strategy
selects the least scoring links according to the for-
mula below which corresponds to links with max-
imum uncertainty:
Score(aij/s
I
1, t1
J) =
2 ? P (tj/si) ? P (si/tj)
P (tj/si) + P (si/tj)
(1)
4.2 Confidence Sampling: Posterior
Alignment probabilities
Confidence estimation for MT output is an in-
teresting area with meaningful initial exploration
(Blatz et al, 2004; Ueffing and Ney, 2007). Given
a sentence pair (sI1, t
J
1 ) and its word alignment,
we compute two confidence metrics at alignment
link level ? based on the posterior link probability
as seen in Equation 5. We select the alignment
links that the initial word aligner is least confi-
dent according to our metric and seek manual cor-
rection of the links. We use t2s to denote com-
putation using higher order (IBM4) target-given-
source models and s2t to denote source-given-
target models. Targeting some of the uncertain
parts of word alignment has already been shown
to improve translation quality in SMT (Huang,
2009). We use confidence metrics as an active
learning sampling strategy to obtain most informa-
tive links. We also experimented with other con-
fidence metrics as discussed in (Ueffing and Ney,
2007), especially the IBM 1 model score metric,
but it did not show significant improvement in this
task.
Pt2s(aij , t
J
1 /s
I
1) =
pt2s(tj/si,aij?A)
?M
i pt2s(tj/si)
(2)
Ps2t(aij , s
I
1/t
J
1 ) =
ps2t(si/tj ,aij?A)
?N
i ps2t(si/tj)
(3)
Conf1(aij/S, T ) =
2?Pt2s?Ps2t
Pt2s+Ps2t
(4)
(5)
4.3 Query by Committee
The generative alignments produced differ based
on the choice of direction of the language pair. We
useAs2t to denote alignment in the source to target
direction and At2s to denote the target to source
direction. We consider these alignments to be two
experts that have two different views of the align-
ment process. We formulate our query strategy
to select links where the agreement differs across
these two alignments. In general query by com-
mittee is a standard sampling strategy in active
learning(Freund et al, 1997), where the commit-
tee consists of any number of experts, in this case
alignments, with varying opinions. We formulate
a query by committee sampling strategy for word
alignment as shown in Equation 6. In order to
break ties, we extend this approach to select the
link with higher average frequency of occurrence
of words involved in the link.
Score(aij) = ? (6)
where ? =
?
?
?
2 aij ? As2t ?At2s
1 aij ? As2t ?At2s
0 otherwise
4.4 Margin Sampling
The strategy for confidence based sampling only
considers information about the best scoring link
367
conf(aij/S, T ). However we could benefit from
information about the second best scoring link as
well. In typical multi-class classification prob-
lems, earlier work shows success using such a
?margin based? approach (Scheffer et al, 2001),
where the difference between the probabilities as-
signed by the underlying model to the first best
and second best labels is used as a sampling cri-
teria. We adapt such a margin-based approach to
link-selection using the Conf1 scoring function
discussed in the earlier sub-section. Our margin
technique is formulated below, where a?1ij and
a?2ij are potential first best and second best scor-
ing alignment links for a word at position i in the
source sentence S with translation T . The word
with minimum margin value is chosen for human
alignment. Intuitively such a word is a possible
candidate for mis-alignment due to the inherent
confusion in its target translation.
Margin(i) =
Conf1(a?1ij/S, T ) ?Conf1(a?2ij/S, T )
5 Experiments
5.1 Data Setup
Our aim in this paper is to show that active learn-
ing can help select the most informative alignment
links that have high uncertainty according to a
given automatically trained model. We also show
that fixing such alignments leads to the maximum
reduction of error in word alignment, as measured
by AER. We compare this with a baseline where
links are selected at random for manual correction.
To run our experiments iteratively, we automate
the setup by using a parallel corpus for which the
gold-standard human alignment is already avail-
able. We select the Chinese-English language pair,
where we have access to 21,863 sentence pairs
along with complete manual alignment.
5.2 Results
We first automatically align the Cn-En corpus us-
ing GIZA++ (Och and Ney, 2003). We then
use the learned model in running our link selec-
tion algorithm over the entire corpus to determine
the most uncertain links according to each active
learning strategy. The links are then looked up in
the gold-standard human alignment database and
corrected. In case a link is not present in the
gold-standard data, we introduce a NULL align-
ment, else we propose the alignment as given in
Figure 1: Performance of active sampling strate-
gies for link selection
the gold standard. We select the partial align-
ment as a set of alignment links and provide it to
our semi-supervised word aligner. We plot per-
formance curves as number of links used in each
iteration vs. the overall reduction of AER on the
corpus.
Query by committee performs worse than ran-
dom indicating that two alignments differing in
direction are not sufficient in deciding for uncer-
tainty. We will be exploring alternative formula-
tions to this strategy. We observe that confidence
based metrics perform significantly better than the
baseline. From the scatter plots in Figure 1 1 we
can say that using our best selection strategy one
achieves similar performance to the baseline, but
at a much lower cost of elicitation assuming cost
per link is uniform.
We also perform end-to-end machine transla-
tion experiments to show that our improvement
of alignment quality leads to an improvement of
translation scores. For this experiment, we train
a standard phrase-based SMT system (Koehn et
al., 2007) over the entire parallel corpus. We tune
on the MT-Eval 2004 dataset and test on a subset
of MT-Eval 2004 dataset consisting of 631 sen-
tences. We first obtain the baseline score where
no manual alignment was used. We also train a
configuration using gold standard manual align-
ment data for the parallel corpus. This is the max-
imum translation accuracy that we can achieve by
any link selection algorithm. We now take the
best link selection criteria, which is the confidence
1X axis has number of links elicited on a log-scale
368
System BLEU METEOR
Baseline 18.82 42.70
Human Alignment 19.96 44.22
Active Selection 20% 19.34 43.25
Table 1: Alignment and Translation Quality
based method and train a system by only selecting
20% of all the links. We observe that at this point
we have reduced the AER from 37.09 AER to
26.57 AER. The translation accuracy as measured
by BLEU (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007) also shows improve-
ment over baseline and approaches gold standard
quality. Therefore we achieve 45% of the possible
improvement by only using 20% elicitation effort.
5.3 Batch Selection
Re-training the word alignment models after elic-
iting every individual alignment link is infeasible.
In our data set of 21,863 sentences with 588,075
links, it would be computationally intensive to re-
train after eliciting even 100 links in a batch. We
therefore sample links as a discrete batch, and train
alignment models to report performance at fixed
points. Such a batch selection is only going to be
sub-optimal as the underlying model changes with
every alignment link and therefore becomes ?stale?
for future selections. We observe that in some sce-
narios while fixing one alignment link could po-
tentially fix all the mis-alignments in a sentence
pair, our batch selection mechanism still samples
from the rest of the links in the sentence pair. We
experimented with an exponential decay function
over the number of links previously selected, in
order to discourage repeated sampling from the
same sentence pair. We performed an experiment
by selecting one of our best performing selection
strategies (conf ) and ran it in both configurations
- one with the decay parameter (batchdecay) and
one without it (batch). As seen in Figure 2, the
decay function has an effect in the initial part of
the curve where sampling is sparse but the effect
gradually fades away as we observe more samples.
In the reported results we do not use batch decay,
but an optimal estimation of ?staleness? could lead
to better gains in batch link selection using active
learning.
Figure 2: Batch decay effects on Conf-posterior
sampling strategy
6 Conclusion and Future Work
Word-Alignment is a particularly challenging
problem and has been addressed in a completely
unsupervised manner thus far (Brown et al, 1993).
While generative alignment models have been suc-
cessful, lack of sufficient data, model assump-
tions and local optimum during training are well
known problems. Semi-supervised techniques use
partial manual alignment data to address some of
these issues. We have shown that active learning
strategies can reduce the effort involved in elicit-
ing human alignment data. The reduction in ef-
fort is due to careful selection of maximally un-
certain links that provide the most benefit to the
alignment model when used in a semi-supervised
training fashion. Experiments on Chinese-English
have shown considerable improvements. In future
we wish to work with word alignments for other
language pairs like Arabic and English. We have
tested out the feasibility of obtaining human word
alignment data using Amazon Mechanical Turk
and plan to obtain more data reduce the cost of
annotation.
Acknowledgments
This research was partially supported by DARPA
under grant NBCHC080097. Any opinions, find-
ings, and conclusions expressed in this paper are
those of the authors and do not necessarily reflect
the views of the DARPA. The first author would
like to thank Qin Gao for the semi-supervised
word alignment software and help with running
experiments.
369
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for machine
translation. In Proceedings of Coling 2004, pages 315?
321, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In ACL 2004, page
175, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 105?112, Morristown, NJ,
USA.
Pinar Donmez and Jaime G. Carbonell. 2008. Optimizing es-
timated loss reduction for active sampling in rank learning.
In ICML ?08: Proceedings of the 25th international con-
ference on Machine learning, pages 248?255, New York,
NY, USA. ACM.
Alexander Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In ACL-44: Pro-
ceedings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, pages 769?
776, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Alexander Fraser and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Proceedings
of the 2007 Joint Conference on EMNLP-CoNLL, pages
51?60.
Alexander Fraser and Daniel Marcu. 2007b. Measuring word
alignment quality for statistical machine translation. Com-
put. Linguist., 33(3):293?303.
Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine. Learning., 28(2-3):133?168.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Pro-
cessing, pages 49?57, Columbus, Ohio, June. Association
for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active learn-
ing for multilingual statistical machine translation. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 181?189, Suntec, Singapore, August. Association
for Computational Linguistics.
Fei Huang. 2009. Confidence measure for word alignment.
In Proceedings of the Joint ACL and IJCNLP, pages 932?
940, Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of the
HLT/NAACL, Edomonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL Demon-
stration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an auto-
matic metric for mt evaluation with high levels of corre-
lation with human judgments. In WMT 2007, pages 228?
231, Morristown, NJ, USA.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In In Proceed-
ings of the Eleventh International Conference on Machine
Learning, pages 148?156. Morgan Kaufmann.
Hieu T. Nguyen and Arnold Smeulders. 2004. Active learn-
ing using pre-clustering. In ICML.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In ACL 2002, pages 311?318, Mor-
ristown, NJ, USA.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel.
2001. Active hidden markov models for information ex-
traction. In IDA ?01: Proceedings of the 4th Interna-
tional Conference on Advances in Intelligent Data Anal-
ysis, pages 309?318, London, UK. Springer-Verlag.
Simon Tong and Daphne Koller. 2002. Support vector ma-
chine active learning with applications to text classifica-
tion. Journal of Machine Learning, pages 45?66.
Nicola Ueffing and Hermann Ney. 2007. Word-level con-
fidence estimation for machine translation. Comput. Lin-
guist., 33(1):9?40.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and unlabeled
data. In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 913?920, Morristown, NJ,
USA. Association for Computational Linguistics.
370
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 10?17,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Active Semi-Supervised Learning for Improving Word Alignment
Vamshi Ambati, Stephan Vogel and Jaime Carbonell
{vamshi,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Word alignment models form an important
part of building statistical machine transla-
tion systems. Semi-supervised word align-
ment aims to improve the accuracy of auto-
matic word alignment by incorporating full
or partial alignments acquired from humans.
Such dedicated elicitation effort is often ex-
pensive and depends on availability of bilin-
gual speakers for the language-pair. In this
paper we study active learning query strate-
gies to carefully identify highly uncertain or
most informative alignment links that are pro-
posed under an unsupervised word alignment
model. Manual correction of such informative
links can then be applied to create a labeled
dataset used by a semi-supervised word align-
ment model. Our experiments show that using
active learning leads to maximal reduction of
alignment error rates with reduced human ef-
fort.
1 Introduction
The success of statistical approaches to Machine
Translation (MT) can be attributed to the IBM mod-
els (Brown et al, 1993) that characterize word-
level alignments in parallel corpora. Parameters of
these alignment models are learnt in an unsupervised
manner using the EM algorithm over sentence-level
aligned parallel corpora. While the ease of auto-
matically aligning sentences at the word-level with
tools like GIZA++ (Och and Ney, 2003) has enabled
fast development of statistical machine translation
(SMT) systems for various language pairs, the qual-
ity of alignment is typically quite low for language
pairs that diverge from the independence assump-
tions made by the generative models. Also, an im-
mense amount of parallel data enables better estima-
tion of the model parameters, but a large number of
language pairs still lack parallel data.
Two directions of research have been pursued for
improving generative word alignment. The first is to
relax or update the independence assumptions based
on more information, usually syntactic, from the
language pairs (Cherry and Lin, 2006). The sec-
ond is to use extra annotation, typically word-level
human alignment for some sentence pairs, in con-
junction with the parallel data to learn alignment in
a semi-supervised manner. Our research is in the
direction of the latter, and aims to reduce the effort
involved in hand-generation of word alignments by
using active learning strategies for careful selection
of word pairs to seek alignment.
Active learning for MT has not yet been explored
to its full potential. Much of the literature has ex-
plored one task ? selecting sentences to translate
and add to the training corpus (Haffari et al, 2009).
In this paper we explore active learning for word
alignment, where the input to the active learner is
a sentence pair (sJ1 , t
I
1), present in two different lan-
guages S = {s?} and T = {t?}, and the annotation
elicited from human is a set of links {(j, i) : j =
0 ? ? ? J ; i = 0 ? ? ? I}. Unlike previous approaches,
our work does not require elicitation of full align-
ment for the sentence pair, which could be effort-
intensive. We use standard active learning query
strategies to selectively elicit partial alignment infor-
mation. This partial alignment information is then
fed into a semi-supervised word aligner which per-
10
forms an improved word alignment over the entire
parallel corpus.
Rest of the paper is organized as follows. We
present related work in Section 2. Section 3 gives
an overview of unsupervised word alignment mod-
els and its semi-supervised improvisation. Section 4
details our active learning framework with discus-
sion of the link selection strategies in Section 5. Ex-
periments in Section 6 have shown that our selection
strategies reduce alignment error rates significantly
over baseline. We conclude with discussion on fu-
ture work.
2 Related Work
Semi-supervised learning is a broader area of Ma-
chine Learning, focusing on improving the learn-
ing process by usage of unlabeled data in conjunc-
tion with labeled data (Chapelle et al, 2006). Many
semi-supervised learning algorithms use co-training
framework, which assumes that the dataset has mul-
tiple views, and training different classifiers on a
non-overlapping subset of these features provides
additional labeled data (Zhu, 2005). Active query
selection for training a semi-supervised learning al-
gorithm is an interesting method that has been ap-
plied to clustering problems. Tomanek and Hahn
(2009) applied active semi supervised learning to
the sequence-labeling problem. Tur et al (2005) de-
scribe active and semi-supervised learning methods
for reducing labeling effort for spoken language un-
derstanding. They train supervised classification al-
gorithms for the task of call classification and apply
it to a large unlabeled dataset to select the least con-
fident instances for human labeling.
Researchers have begun to explore semi-
supervised word alignment models that use both
labeled and unlabeled data. Fraser and Marcu
(2006) pose the problem of alignment as a search
problem in log-linear space with features coming
from the IBM alignment models. The log-linear
model is trained on the available labeled data
to improve performance. They propose a semi-
supervised training algorithm which alternates
between discriminative error training on the la-
beled data to learn the weighting parameters and
maximum-likelihood EM training on unlabeled
data to estimate the parameters. Callison-Burch et
al. (2004) also improve alignment by interpolating
human alignments with automatic alignments. They
observe that while working with such datasets,
alignments of higher quality should be given a much
higher weight than the lower-quality alignments.
Wu et al (2006) learn separate models from labeled
and unlabeled data using the standard EM algo-
rithm. The two models are then interpolated as a
learner in the semi-supervised AdaBoost algorithm
to improve word alignment.
Active learning has been applied to various fields
of Natural Language Processing like statistical pars-
ing, entity recognition among others (Hwa, 2004;
Tang et al, 2001; Shen et al, 2004). In case of
MT, the potential of active learning has remained
largely unexplored. For Statistical Machine Transla-
tion, application of active learning has been focused
on the task of selecting the most informative sen-
tences to train the model, in order to reduce cost
of data acquisition. Recent work in this area dis-
cussed multiple query selection strategies for a Sta-
tistical Phrase Based Translation system (Haffari et
al., 2009). Their framework requires source text to
be translated by the system and the translated data
is used in a self-training setting to train MT models.
To our knowledge, we are not aware of any work
that has looked at reducing human effort by selec-
tive elicitation of alignment information using active
learning techniques.
3 Word Alignment
3.1 IBM models
IBM models provide a generative framework for
performing word alignment of parallel corpus.
Given two strings from source and target languages
sJ1 = s1, ? ? ? , sj , ? ? ? sJ and t
I
1 = t1, ? ? ? , ti, ? ? ? tI ,
an alignment A is defined as a subset of the Carte-
sian product of the word indices as shown in Eq 1.
In IBM models, since alignment is treated as a func-
tion, all the source positions must be covered exactly
once (Brown et al, 1993).
A ? {(j, i) : j = 0 ? ? ? J ; i = 0 ? ? ? I} (1)
For the task of translation, we would ideally want
to model P (sI1|t
J
1 ), which is the probability of ob-
serving source sentence sI1 given target sentence t
J
1 .
This requires a lot of parallel corpus for estimation
11
and so it is then factored over the word alignment
A for the sentence pair, which is a hidden variable.
Word alignment is therefore a by-product in the pro-
cess of modeling translation. We can also represent
the same under some parameterization of ?, which
is the model we are interested to estimate.
P (sJ1 |t
I
1) =
?
aJ1
Pr(sJ1 , A|t
J
1 ) (2)
=
?
A
p?(s
J
1 , A|t
I
1) (3)
Given a parallel corpus U of sentence pairs
{(sk, tk) : k = 1, ? ? ? ,K} the parameters can be
estimated by maximizing the conditional likelihood
over the data. IBM models (Brown et al, 1993) from
1 to 5 are different ways of factoring the probability
model to estimate the parameter set ?. For example
in the simplest of the models, IBM model 1, only the
lexical translation probability is considered treating
each word being translated independent of the other
words.
?? = argmax
?
K?
k=1
?
A
p?(sk, A|tk) (4)
The parameters of the model above are estimated
as ??, using the EM algorithm. We can also extract
the Viterbi alignment ,A?, for all the sentence pairs,
which is the alignment with the highest probability
under the current model parameters ?:
A? = argmax
A
p??(s
J
1 , A|t
I
1) (5)
The alignment models are asymmetric and dif-
fer with the choice of translation direction. We can
therefore perform the above after switching the di-
rection of the language pair and obtain models and
Viterbi alignments for the corpus as represented be-
low:
?? = argmax
?
K?
k=1
?
a
p?(tk, a|sk) (6)
A? = argmax
A
p??(t
I
1, A|s
J
1 ) (7)
Given the Viterbi alignment for each sentence
pair in the parallel corpus, we can also compute the
word-level alignment probabilities using simple rel-
ative likelihood estimation for both the directions.
As we will discuss in Section 5, the alignments and
the computed lexicons form an important part of our
link selection strategies.
P (sj/ti) =
?
s count(ti, sj ; A?)?
s count(ti)
(8)
P (ti/sj) =
?
s count(ti, sj ; A?)?
s count(sj)
(9)
We perform all our experiments on a symmetrized
alignment that combines the bidirectional align-
ments using heuristics as discussed in (Koehn et al,
2007). We represent this alignment as A = {aij :
i = 0 ? ? ? J ? sJ1 ; j = 0 ? ? ? I ? t
I
1}.
3.2 Semi-Supervised Word Alignment
We use an extended version of MGIZA++ (Gao
and Vogel, 2008) to perform the constrained semi-
supervised word alignment. To get full benefit
from the manual alignments, MGIZA++ modifies all
alignment models used in the standard training pro-
cedure, i.e. the IBM1, HMM, IBM3 and IBM4 mod-
els. Manual alignments are incorporated in the EM
training phase of these models as constraints that
restrict the summation over all possible alignment
paths. Typically in the EM procedure for IBM mod-
els, the training procedure requires for each source
sentence position, the summation over all positions
in the target sentence. The manual alignments al-
low for one-to-many alignments and many-to-many
alignments in both directions. For each position i
in the source sentence, there can be more than one
manually aligned target word. The restricted train-
ing will allow only those paths, which are consistent
with the manual alignments. Therefore, the restric-
tion of the alignment paths reduces to restricting the
summation in EM.
4 Active Learning for Word Alignment
Active learning attempts to optimize performance
by selecting the most informative instances to la-
bel, where ?informativeness? is defined as maximal
expected improvement in accuracy. The objective
is to select optimal instance for an external expert
to label and then run the learning method on the
newly-labeled and previously-labeled instances to
minimize prediction or translation error, repeating
until either the maximal number of external queries
12
is reached or a desired accuracy level is achieved.
Several studies (Tong and Koller, 2002; Nguyen
and Smeulders, 2004; Donmez and Carbonell, 2008)
show that active learning greatly helps to reduce the
labeling effort in various classification tasks.
We discuss our active learning setup for word
alignment in Algorithm 1. We start with an un-
labeled dataset U = {(Sk, Tk)}, indexed by k,
and a seed pool of partial alignment links A0 =
{akij , ?si ? Sk, tj ? Tk}. Each a
k
ij represents an
alignment link from a sentence pair k that connects
source word si with tj .
This is usually an empty set at iteration t = 0. We
iterate for T iterations. We take a pool-based active
learning strategy, where we have access to all the au-
tomatically aligned links and we can score the links
based on our active learning query strategy. The
query strategy uses the automatically trained align-
ment model ?t from the current iteration t, for scor-
ing the links. Re-training and re-tuning an SMT sys-
tem for each link at a time is computationally infea-
sible. We therefore perform batch learning by se-
lecting a set of N links scored high by our query
strategy. We seek manual corrections for the se-
lected links and add the alignment data to the cur-
rent labeled dataset. The word-level aligned labeled
dataset is then provided to our semi-supervised word
alignment algorithm, which uses it to produces the
alignment model ?t+1 for U .
Algorithm 1 AL FOR WORD ALIGNMENT
1: Unlabeled Data Set: U = {(sk, tk)}
2: Manual Alignment Set : A0 = {akij ,?si ?
Sk, tj ? Tk}
3: Train Semi-supervised Word Alignment using
(U , A0)? ?0
4: N : batch size
5: for t = 0 to T do
6: Lt = LinkSelection(U ,At,?t,N )
7: Request Human Alignment for Lt
8: At+1 = At + Lt
9: Re-train Semi-Supervised Word Align-
ment on (U,At+1)? ?t+1
10: end for
We can iteratively perform the algorithm for a de-
fined number of iterations T or until a certain desired
performance is reached, which is measured by align-
ment error rate (AER) (Fraser and Marcu, 2007) in
the case of word alignment. In a more typical sce-
nario, since reducing human effort or cost of elici-
tation is the objective, we iterate until the available
budget is exhausted.
5 Query Strategies for Link Selection
We propose multiple query selection strategies for
our active learning setup. The scoring criteria is
designed to select alignment links across sentence
pairs that are highly uncertain under current au-
tomatic translation models. These links are diffi-
cult to align correctly by automatic alignment and
will cause incorrect phrase pairs to be extracted in
the translation model, in turn hurting the transla-
tion quality of the SMT system. Manual correction
of such links produces the maximal benefit to the
model. We would ideally like to elicit the least num-
ber of manual corrections possible in order to reduce
the cost of data acquisition. In this section we dis-
cuss our link selection strategies based on the stan-
dard active learning paradigm of ?uncertainty sam-
pling?(Lewis and Catlett, 1994). We use the au-
tomatically trained translation model ?t for scoring
each link for uncertainty. In particular ?t consists of
bidirectional lexicon tables computed from the bidi-
rectional alignments as discussed in Section 3.
5.1 Uncertainty based: Bidirectional
Alignment Scores
The automatic Viterbi alignment produced by the
alignment models is used to obtain translation lexi-
cons, as discussed in Section 3. These lexicons cap-
ture the conditional distributions of source-given-
target P (s/t) and target-given-source P (t/s) prob-
abilities at the word level where si ? S and tj ? T .
We define certainty of a link as the harmonic mean
of the bidirectional probabilities. The selection strat-
egy selects the least scoring links according to the
formula below which corresponds to links with max-
imum uncertainty:
Score(aij/sI1, t
J
1 ) =
2 ? P (tj/si) ? P (si/tj)
P (tj/si) + P (si/tj)
(10)
5.2 Confidence Based: Posterior Alignment
probabilities
Confidence estimation for MT output is an interest-
ing area with meaningful initial exploration (Blatz
13
et al, 2004; Ueffing and Ney, 2007). Given a sen-
tence pair (sI1, t
J
1 ) and its word alignment, we com-
pute two confidence metrics at alignment link level ?
based on the posterior link probability and a simple
IBM Model 1 as seen in Equation 13. We select the
alignment links that the initial word aligner is least
confident according to our metric and seek manual
correction of the links. We use t2s to denote com-
putation using higher order (IBM4) target-given-
source models and s2t to denote source-given-target
models. Targeting some of the uncertain parts of
word alignment has already been shown to improve
translation quality in SMT (Huang, 2009). In our
current work, we use confidence metrics as an ac-
tive learning sampling strategy to obtain most infor-
mative links. We also experiment with other con-
fidence metrics as discussed in (Ueffing and Ney,
2007), especially the IBM 1 model score metric
which showed some improvement as well.
Pt2s(aij , tJ1 /s
I
1) =
pt2s(tj/si, aij ? A)
?M
i pt2s(tj/si)
(11)
Ps2t(aij , sI1/t
J
1 ) =
ps2t(si/tj , aij ? A)
?N
i pt2s(tj/si)
(12)
Conf(aij/S, T ) =
2 ? Pt2s ? Ps2t
Pt2s + Ps2t
(13)
5.3 Agreement Based: Query by Committee
The generative alignments produced differ based on
the choice of direction of the language pair. We use
As2t to denote alignment in the source to target di-
rection and At2s to denote the target to source direc-
tion. We consider these alignments to be two experts
that have two different views of the alignment pro-
cess. We formulate our query strategy to select links,
where the agreement differs across these two align-
ments. In general query by committee is a standard
sampling strategy in active learning(Freund et al,
1997), where the committee consists of any number
of experts with varying opinions, in this case align-
ments in different directions. We formulate a query
by committee sampling strategy for word alignment
as shown in Equation 14. In order to break ties, we
extend this approach to select the link with higher
average frequency of occurrence of words involved
in the link.
Language Sentences Words
Src Tgt
Ch-En 21,863 424,683 524,882
Ar-En 29,876 630,101 821,938
Table 1: Corpus Statistics of Human Data
Alignment Automatic Links Manual Links
Ch-En 491,887 588,075
Ar-En 786,223 712,583
Table 2: Alignment Statistics of Human Data
Score(aij) = ? where (14)
? =
?
?
?
2 aij ? At2s ?At2s
1 aij ? At2s ?At2s
0 otherwise
6 Experiments
6.1 Data Analysis
To run our active learning and semi-supervised word
alignment experiments iteratively, we simulate the
setup by using a parallel corpus for which the
gold standard human alignment is already available.
We experiment with two language pairs - Chinese-
English and Arabic-English. Corpus-level statistics
for both language pairs can be seen in Table 1 and
their alignment link level statistics can be seen in
Table 2. Both datasets were released by LDC as part
of the GALE project.
Chinese-English dataset consists of 21,863 sen-
tence pairs with complete manual alignment. The
human alignment for this dataset is much denser
than the automatic word alignment. On an aver-
age each source word is linked to more than one
target word. Similarly, the Arabic-English dataset
consisting of 29,876 sentence pairs also has a denser
manual alignment. Automatic word alignment in
both cases was computed as a symmetrized version
of the bidirectional alignments obtained from using
GIZA++ (Och and Ney, 2003) in each direction sep-
arately.
6.2 Word Alignment Results
We first perform an unsupervised word alignment of
the parallel corpus. We then use the learned model
14
Figure 1: Chinese-English: Link Selection Results
in running our link selection algorithm over the en-
tire alignments to determine the most uncertain links
according to each active learning strategy. The links
are then looked up in the gold standard human align-
ment database and corrected. In scenarios where
an alignment link is not present in the gold stan-
dard data for the source word, we introduce a NULL
alignment constraint, else we select all the links as
given in the gold standard. The aim of our work is to
show that active learning can help in selecting infor-
mative alignment links, which if manually labeled
can reduce the overall alignment error rate of the
given corpus. We, therefore measure the reduction
of alignment error rate (AER) of a semi-supervised
word aligner that uses this extra information to align
the corpus. We plot performance curves for both
Chinese-English, Figure 1 and Arabic-English, Fig-
ure 2, with number of manual links elicited on x-axis
and AER on y-axis. In each iteration of the experi-
ment, we gradually increase the number of links se-
lected from gold standard and make them available
to the semi-supervised word aligner and measure the
overall reduction of AER on the corpus. We com-
pare our link selection strategies to a baseline ap-
proach, where links are selected at random for man-
ual correction.
All our approaches perform equally or better than
the baseline for both language pairs. Query by
committee (qbc) performs similar to the baseline in
Chinese-English and only slightly better for Arabic-
Figure 2: Arabic-English: Link Selection Results
English. This could be due to our committee con-
sisting of two alignments that differ only in direc-
tion and so are not sufficient in deciding for uncer-
tainty. We will be exploring alternative formulations
to this strategy. Confidence based and uncertainty
based metrics perform significantly better than the
baseline in both language pairs. We can interpret the
improvements in two ways. For the same number
of manual alignments elicited, our selection strate-
gies select links that provide higher reduction of er-
ror when compared to the baseline. An alternative
interpretation is that assuming a uniform cost per
link, our best selection strategy achieves similar per-
formance to the baseline, at a much lower cost of
elicitation.
6.3 Translation Results
We also perform end-to-end machine translation ex-
periments to show that our improvement of align-
ment quality leads to an improvement of translation
scores. For Chinese-English, we train a standard
phrase-based SMT system (Koehn et al, 2007) over
the available 21,863 sentences. We tune on the MT-
Eval 2004 dataset and test on a subset of MT-Eval
2005 dataset consisting of 631 sentences. The lan-
guage model we use is built using only the English
side of the parallel corpus. We understand that this
language model is not the optimal choice, but we
are interested in testing the word alignment accu-
racy, which primarily affects the translation model.
15
Cn-En BLEU METEOR
Baseline 18.82 42.70
Human Alignment 19.96 44.22
Active Selection 20% 19.34 43.25
Table 3: Effect of Alignment on Translation Quality
We first obtain the baseline score by training in an
unsupervised manner, where no manual alignment
is used. We also train a configuration, where we
substitute the final word alignment with gold stan-
dard manual alignment for the entire parallel corpus.
This is an upper bound on the translation accuracy
that can be achieved by any alignment link selec-
tion algorithm for this dataset. We now take our
best link selection criteria, which is the confidence
based method and re-train the MT system after elic-
iting manual information for only 20% of the align-
ment links. We observe that at this point we have
reduced the AER from 37.09 to 26.57. The trans-
lation accuracy reported in Table 3, as measured by
BLEU (Papineni et al, 2002) and METEOR (Lavie
and Agarwal, 2007), also shows significant improve-
ment and approaches the quality achieved using gold
standard data. We did not perform MT experiments
with Arabic-English dataset due to the incompatibil-
ity of tokenization schemes between the manually
aligned parallel corpora and publicly available eval-
uation sets.
7 Conclusion
Word-Alignment is a particularly challenging prob-
lem and has been addressed in a completely unsuper-
vised manner thus far (Brown et al, 1993). While
generative alignment models have been successful,
lack of sufficient data, model assumptions and lo-
cal optimum during training are well known prob-
lems. Semi-supervised techniques use partial man-
ual alignment data to address some of these issues.
We have shown that active learning strategies can
reduce the effort involved in eliciting human align-
ment data. The reduction in effort is due to care-
ful selection of maximally uncertain links that pro-
vide the most benefit to the alignment model when
used in a semi-supervised training fashion. Experi-
ments on Chinese-English have shown considerable
improvements.
8 Future Work
In future, we wish to work with word alignments for
other language pairs as well as study the effect of
manual alignments by varying the size of available
parallel data. We also plan to obtain alignments from
non-experts over online marketplaces like Amazon
Mechanical Turk to further reduce the cost of an-
notation. We will be experimenting with obtain-
ing full-alignment vs. partial alignment from non-
experts. Our hypothesis is that, humans are good
at performing tasks of smaller size and so we can
extract high quality alignments in the partial align-
ment case. Cost of link annotation in our current
work is assumed to be uniform, but this needs to
be revisited. We will also experiment with active
learning techniques for identifying sentence pairs
with very low alignment confidence, where obtain-
ing full-alignment is equivalent to obtaining multi-
ple partial alignments.
Acknowledgments
This research was partially supported by DARPA
under grant NBCHC080097. Any opinions, find-
ings, and conclusions expressed in this paper are
those of the authors and do not necessarily reflect the
views of the DARPA. The first author would like to
thank Qin Gao for the semi-supervised word align-
ment software and help with running experiments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of Coling 2004,
pages 315?321, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In ACL 2004, page
175, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
16
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 105?112, Morris-
town, NJ, USA.
Pinar Donmez and Jaime G. Carbonell. 2008. Opti-
mizing estimated loss reduction for active sampling in
rank learning. In ICML ?08: Proceedings of the 25th
international conference on Machine learning, pages
248?255, New York, NY, USA. ACM.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 769?776, Morristown, NJ, USA.
Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine. Learning., 28(2-
3):133?168.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In Proceedings of HLT NAACL
2009, pages 415?423, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the Joint ACL and IJCNLP,
pages 932?940, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Comput. Linguist., 30(3):253?276.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL Demonstration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT 2007,
pages 228?231, Morristown, NJ, USA.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 148?156. Morgan Kauf-
mann.
Hieu T. Nguyen and Arnold Smeulders. 2004. Active
learning using pre-clustering. In ICML.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL 2002, pages 311?
318, Morristown, NJ, USA.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In ACL ?04: Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589, Morristown, NJ,
USA. Association for Computational Linguistics.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
ACL ?02, pages 120?127, Morristown, NJ, USA.
Katrin Tomanek and Udo Hahn. 2009. Semi-supervised
active learning for sequence labeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1039?1047, Suntec, Singapore, August. Association
for Computational Linguistics.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. Journal of Machine Learning, pages 45?66.
Gokhan Tur, Dilek Hakkani-Tr, and Robert E. Schapire.
2005. Combining active and semi-supervised learning
for spoken language understanding. Speech Commu-
nication, 45(2):171 ? 186.
Nicola Ueffing and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
put. Linguist., 33(1):9?40.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boosting
statistical word alignment using labeled and unlabeled
data. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 913?920, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
X. Zhu. 2005. Semi-Supervised Learning Lit-
erature Survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
17
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 62?65,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Can Crowds Build Parallel Corpora for Machine Translation Systems?
Vamshi Ambati and Stephan Vogel
{vamshi,vogel}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Corpus based approaches to machine transla-
tion (MT) rely on the availability of parallel
corpora. In this paper we explore the effec-
tiveness of Mechanical Turk for creating par-
allel corpora. We explore the task of sen-
tence translation, both into and out of a lan-
guage. We also perform preliminary experi-
ments for the task of phrase translation, where
ambiguous phrases are provided to the turker
for translation in isolation and in the context
of the sentence it originated from.
1 Introduction
Large scale parallel data generation for new lan-
guage pairs requires intensive human effort and
availability of bilingual speakers. Only a few lan-
guages in the world enjoy sustained research inter-
est and continuous financial support for develop-
ment of automatic translation systems. For most
remaining languages there is very little interest or
funding available and limited or expensive access to
experts for data elicitation. Crowd-sourcing com-
pensates for the lack of experts with a large pool of
expert/non-expert crowd. However, crowd-sourcing
has thus far been explored in the context of elicit-
ing annotations for a supervised classification task,
typically monolingual in nature (Snow et al, 2008).
In this shared task we test the feasibility of eliciting
parallel data for Machine Translation (MT) using
Mechanical Turk (MTurk). MT poses an interesting
challenge as we require turkers to have understand-
ing/writing skills in both the languages. Our work is
similar to some recent work on crowd-sourcing and
machine translation (Ambati et al, 2010; Callison-
Burch, 2009), but focuses primarily on the setup and
design of translation tasks on MTurk with varying
granularity levels, both at sentence- and phrase-level
translation.
2 Language Landscape on MTurk
We first conduct a pilot study by posting 25 sen-
tences each from a variety of language pairs and
probing to see the reception on MTurk. Language-
pair selection was based on number of speakers in
the language and Internet presence of the popula-
tion. Languages like Spanish, Chinese, English,
Arabic are spoken by many and have a large pres-
ence of users on the Internet. Those like Urdu,
Tamil, Telugu although spoken by many are not well
represented on the Web. Languages like Swahili,
Zulu, Haiti are neither spoken by many nor have
a great presence on the Web. For this pilot study
we selected Spanish, Chinese, English, Urdu, Tel-
ugu, Hindi, Haitian Creole languages. We do not
select German, French and other language pairs as
they have already been explored by Callison-Burch
(2009). Our pilot study helped us calibrate the costs
for different language pairs as well as helped us se-
lect the languages to pursue further experiments. We
found that at lower pay rates like 1 cent, it is difficult
to find a sufficient number of translators to complete
the task. For example, we could not find turkers
to complete the translation from English to Haitian-
Creole even after a period of 10 days. Haitian creole
is spoken by a small population and it seems that
only a very small portion of that was on MTurk. For
a few other languages pairs, while we could find a
62
Pair Cost per sen Days
Spanish-Eng $0.01 1
Telugu-Eng $0.02 2
Eng-Creole $0.06 -
Urdu-Eng $0.03 1
Hindi-Eng $0.03 1
Chinese-Eng $0.02 1
Table 1: Cost vs. Completion for Language pairs
few turkers attempting the task, the price had to be
increased to attract any attention. Table 1 shows the
findings of our pilot study. We show the minimum
cost at which we could start getting turkers to pro-
vide translations and the number of days they took to
complete the task. MTurk has so far been a suppli-
ers? market, and translation of rare-languages shows
how a limited supply of turkers leads to a buyer?s
market; only fair.
3 Challenges for Crowd-Sourcing and
Machine Translation
We use MTurk for all our crowd-sourcing experi-
ments. In case of MT, a HIT on MTurk is one or
more sentences in the source language that need to
be translated to a target language. Making sure that
the workers understand the task is the first step to-
wards a successful elicitation using the crowd. We
provide detailed instructions on the HIT for both
completion of the task and its evaluation. Mechan-
ical turk also has a provision to seek annotations
from qualified workers, from a specific location with
a specific success rate in their past HITs. For all
our HITs we set the worker qualification threshold
to 90%. We use the terms HIT vs. task and turker
vs. translator interchangeably.
3.1 Quality Assurance
Quality assurance is a concern with an online crowd
where the expertise of the turkers is unknown. We
also notice from the datasets we receive that consis-
tently poor and noisy translators exist. Problems like
blank annotations, mis-spelling, copy-pasting of in-
put are prevalent, but easy to identify. Turkers who
do not understand the task but attempt it anyway are
the more difficult ones to identify, but this is to be
expected with non-experts. Redundancy of transla-
tions for the input and computing majority consen-
sus translation is agreed to be an effective solution to
identify and prune low quality translation. We dis-
cuss in following section computation of majority
vote using fuzzy matching.
For a language pair like Urdu-English, we noticed
a strange scenario, where the translations from two
turkers were significantly worse in quality, but con-
sistently matched each other, there by falsely boost-
ing the majority vote. We suspect this to be a case of
cheating, but this exposes a loop in majority voting
which needs to be addressed, perhaps by also using
gold standard data.
Turking Machines: We also have the problem
of machines posing as turkers ? ?Turking machine?
problem. With the availability of online translation
systems like Google translate, Yahoo translate (Ba-
belfish) and Babylon, translation tasks on MTurk
become easy targets to this problem. Turkers ei-
ther use automatic scripts to get/post data from au-
tomatic MT systems, or make slight modifications
to disguise the fact. This defeats the purpose of the
task, as the resulting corpus would then be biased to-
wards some existing automatic MT system. It is ex-
tremely important to keep gamers in check; not only
do they pollute the quality of the crowd data, but
their completion of a HIT means it becomes unavail-
able to genuine turkers who are willing to provide
valuable translations. We, therefore, collect transla-
tions from existing automatic MT services and use
them to match and block submissions from gamers.
We rely on some gold-standard to identify genuine
matches with automatic translation services.
3.2 Output Space and Fuzzy Matching
Due to the natural variability in style of turkers, there
could be multiple different, but perfectly valid trans-
lations for a given sentence. Therefore it is dif-
ficult to match translation outputs from two turk-
ers or even with gold standard data. We there-
fore need a fuzzy matching algorithm to account
for lexical choices, synonymy, word ordering and
morphological variations. This problem is similar
to the task of automatic translation output evalua-
tion and so we use METEOR (Lavie and Agarwal,
2007), an automatic MT evaluation metric for com-
paring two sentences. METEOR has an internal
aligner that matches words in the sentences given
63
and scores them separately based on whether the
match was supported by synonymy, exact match or
fuzzy match. The scores are then combined to pro-
vide a global matching score. If the score is above a
threshold ?, we treat the sentences to be equivalent
translations of the source sentence. We can set the
? parameter to different values, based on what is ac-
ceptable to the application. In our experiments, we
set ? = 0.7. We did not choose BLEU scoring met-
ric as it is strongly oriented towards exact matching
and high precision, than towards robust matching for
high recall.
4 Sentence Translation
The first task we setup on MTurk was to translate
full sentences from a source language into a tar-
get language. The population we were interested in
was native speakers of one of the languages. We
worked with four languages - English, Spanish, Tel-
ugu and Urdu. We chose 100 sentences for each
language-pair and requested three different transla-
tions for each sentence. The Spanish data was taken
from BTEC (Takezawa et al, 2002) corpus, consist-
ing of short sentences in the travel domain. Telugu
data was taken from the sports and politics section
of a regional newspaper. For Urdu, we used the
NIST-Urdu Evaluation 2008 data. We report results
in Table 2. Both Spanish and Urdu had gold stan-
dard translations, as they were taken from parallel
corpora created by language experts. As the data
sets are small, we chose to perform manual inspec-
tion rather than use automatic metrics like BLEU to
score match against gold-standard data.
4.1 Translating into English
The first batch of HITs were posted to collect trans-
lations into English. We noticed from manual in-
spection of the quality of translations that most of
our translators were non-native speakers of English.
This calls for adept and adequate methods for evalu-
ating the translation quality. For example more than
50% of the Spanish-English tasks were completed in
India, and in some cases a direct output of automatic
translation services.
4.2 Translating out of English
The second set of experiments were to test the ef-
fectiveness of translating out of English. The ideal
Language Pair Cost #Days #Turkers
Spanish-English $0.01 1 16
Telugu-English $0.02 4 12
Urdu-English $0.03 2 13
English-Spanish $0.01 1 19
English-Telugu $0.02 3 35
English-Urdu $0.03 2 21
Table 2: Sentence translation data
target population for this task were native speakers
of the target language who also understood English.
Most participant turkers who provided Urdu and Tel-
ugu translations, were from India and USA and were
non-native speakers of English. However, one prob-
lem with enabling this task was the writing system.
Most turkers do not have the tools to create content
in their native language. We used ?Google Translit-
erate? API 1 to enable production of non-English
content. This turned out to be an interesting HIT
for the turkers, as they were excited to create their
native language content. This is evident from the
increased number of participant turkers. Manual in-
spection of translations revealed that this direction
resulted in higher quality translations for both Urdu
and Telugu and slightly lower quality for Spanish.
5 Phrase Translation
Phrase translation is useful in reducing the cost
and effort of eliciting translations by focusing on
those parts of the sentence that are difficult to
translate. It fits well into the paradigm of crowd-
sourcing where small tasks can be provided to a lot
of translators. For this task, we were interested in
understanding how well non-experts translate sub-
sentential segments, and whether exposure to ?con-
text? was helpful. For this set of experiments we use
the Spanish-English language pair, where the turk-
ers were presented with Spanish phrases to trans-
late. The phrases were selected from the standard
phrase tables produced by statistical phrase-based
MT (Koehn et al, 2007), that was trained on the en-
tire 128K BTEC corpus for Spanish-English. We
computed an entropy score for each entry in the
phrase table under the translation probability distri-
butions in both directions and picked the set of 50
1http://www.google.com/transliterate/
64
Type %Agreement %Gold match
Out of Context 64% 32%
In Context 68% 33%
Table 3: Phrase Translation: Spanish-English
Length Count Example
1 2 cierras
2 11 vienes aqu
3 26 hay una en
4 8 a conocer su decisin
5 4 viene bien a esa hora
Table 4: Details of Spanish-English phrases used
most ambiguous phrases according to this metric.
Table 4 shows sample and the length distribution of
the phrases selected for this task.
5.1 In Context vs. Out of Context
We performed two kinds of experiments to study
phrase translation and role of context. In the first
case, the task was designed to be as simple as possi-
ble with each phrase to be translated as an individual
HIT. We provided a source phrase and request turk-
ers to translate a phrase under any hypothesized con-
text. For the second task, we gave a phrase associ-
ated with the sentence that it originated from and re-
quested the turkers to translate the phrase only in the
context of the sentence. For both cases, we analyzed
the data for inter-translator agreement;% of cases
where there was a consensus translation), and agree-
ment with the gold standard; % of times the trans-
lated phrase was present in the gold standard transla-
tion of the source sentence it came from. As shown
in Table 3, translating in-context produced a better
match with gold standard data and scored slightly
better on the inter-translator agreement. We think
that when translating out of context, most translators
choose as appropriate for a context in their mind and
so the inter-translator agreement could be lower, but
when translating within the context of a sentence,
they make translation choices to suit the sentence
which could lead to better agreement scores. In fu-
ture, we will extend these experiments to other lan-
guage pairs and choose phrases not by entropy met-
ric, but to study specific language phenomenon.
6 Conclusion
Our experiments helped us better understand the
formulation of translation tasks on MTurk and its
challenges. We experimented with both translating
into and out of English and use transliteration for
addressing the writing system issue. We also ex-
periment with in-context and out-of-context phrase
translation task. While working with non-expert
translators it is important to address quality concerns
alongside keeping in check any usage of automatic
translation services. At the end of the shared task we
have sampled the ?language landscape? on MTurk
and have a better understanding of what to expect
when building MT systems for different language
pairs.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the LREC 2010,
Malta, May.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP 2009, pages 286?295, Sin-
gapore, August. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL Demonstration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT 2007,
pages 228?231, Morristown, NJ, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP 2008, pages 254?
263, Honolulu, Hawaii, October.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Towards a broad-coverage bilingual corpus for speech
translation of travel conversation in the real world. In
Proceedings of LREC 2002, Las Palmas, Spain.
65
Active Learning with Multiple Annotations for Comparable Data
Classification Task
Vamshi Ambati, Sanjika Hewavitharana, Stephan Vogel and Jaime Carbonell
{vamshi,sanjika,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Supervised learning algorithms for identify-
ing comparable sentence pairs from a domi-
nantly non-parallel corpora require resources
for computing feature functions as well as
training the classifier. In this paper we pro-
pose active learning techniques for addressing
the problem of building comparable data for
low-resource languages. In particular we pro-
pose strategies to elicit two kinds of annota-
tions from comparable sentence pairs: class
label assignment and parallel segment extrac-
tion. We also propose an active learning strat-
egy for these two annotations that performs
significantly better than when sampling for ei-
ther of the annotations independently.
1 Introduction
The state-of-the-art Machine Translation (MT) sys-
tems are statistical, requiring large amounts of paral-
lel corpora. Such corpora needs to be carefully cre-
ated by language experts or speakers, which makes
building MT systems feasible only for those lan-
guage pairs with sufficient public interest or finan-
cial support. With the increasing rate of social media
creation and the quick growth of web media in lan-
guages other than English makes it relevant for lan-
guage research community to explore the feasibility
of Internet as a source for parallel data. (Resnik and
Smith, 2003) show that parallel corpora for a variety
of languages can be harvested on the Internet. It is to
be observed that a major portion of the multilingual
web documents are created independent of one an-
other and so are only mildly parallel at the document
level.
There are multiple challenges in building compa-
rable corpora for consumption by the MT systems.
The first challenge is to identify the parallelism be-
tween documents of different languages which has
been reliably done using cross lingual information
retrieval techniques. Once we have identified a sub-
set of documents that are potentially parallel, the
second challenge is to identify comparable sentence
pairs. This is an interesting challenge as the avail-
ability of completely parallel sentences on the inter-
net is quite low in most language-pairs, but one can
observe very few comparable sentences among com-
parable documents for a given language-pair. Our
work tries to address this problem by posing the
identification of comparable sentences from com-
parable data as a supervised classification problem.
Unlike earlier research (Munteanu and Marcu, 2005)
where the authors try to identify parallel sentences
among a pool of comparable documents, we try to
first identify comparable sentences in a pool with
dominantly non-parallel sentences. We then build
a supervised classifier that learns from user annota-
tions for comparable corpora identification. Train-
ing such a classifier requires reliably annotated data
that may be unavailable for low-resource language
pairs. Involving a human expert to perform such
annotations is expensive for low-resource languages
and so we propose active learning as a suitable tech-
nique to reduce the labeling effort.
There is yet one other issue that needs to be solved
in order for our classification based approach to
work for truly low-resource language pairs. As we
will describe later in the paper, our comparable sen-
tence classifier relies on the availability of an ini-
69
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 69?77,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
tial seed lexicon that can either be provided by a hu-
man or can be statistically trained from parallel cor-
pora (Och and Ney, 2003). Experiments show that a
broad coverage lexicon provides us with better cov-
erage for effective identification of comparable cor-
pora. However, availability of such a resource can
not be expected in very low-resource language pairs,
or even if present may not be of good quality. This
opens an interesting research question - Can we also
elicit such information effectively at low costs? We
propose active learning strategies for identifying the
most informative comparable sentence pairs which a
human can then extract parallel segments from.
While the first form of supervision provides us
with class labels that can be used for tuning the fea-
ture weights of our classifier, the second form of su-
pervision enables us to better estimate the feature
functions. For the comparable sentence classifier to
perform well, we show that both forms of supervi-
sion are needed and we introduce an active learning
protocol to combine the two forms of supervision
under a single joint active learning strategy.
The rest of the paper is organized as follows. In
Section 2 we survey earlier research as relevant to
the scope of the paper. In Section 3 we discuss the
supervised training setup for our classifier. In Sec-
tion 4 we discuss the application of active learning to
the classification task. Section 5 discusses the case
of active learning with two different annotations and
proposes an approach for combining them. Section 6
presents experimental results and the effectiveness
of the active learning strategies. We conclude with
further discussion and future work.
2 Related Work
There has been a lot of interest in using compara-
ble corpora for MT, primarily on extracting paral-
lel sentence pairs from comparable sources (Zhao
and Vogel, 2002; Fung and Yee, 1998). Some work
has gone beyond this focussing on extracting sub-
sentential fragments from noisier comparable data
(Munteanu and Marcu, 2006; Quirk et al, 2007).
The research conducted in this paper has two pri-
mary contributions and so we will discuss the related
work as relevant to each of them.
Our first contribution in this paper is the appli-
cation of active learning for acquiring comparable
data in the low-resource scenario, especially rele-
vant when working with low-resource languages.
There is some earlier work highlighting the need
for techniques to deal with low-resource scenar-
ios.(Munteanu and Marcu, 2005) propose bootstrap-
ping using an existing classifier for collecting new
data. However, this approach works when there is
a classifier of reasonable performance. In the ab-
sence of parallel corpora to train lexicons human
constructed dictionaries were used as an alternative
which may, however, not be available for a large
number of languages. Our proposal of active learn-
ing in this paper is suitable for highly impoverished
scenarios that require support from a human.
The second contribution of the paper is to ex-
tend the traditional active learning setup that is suit-
able for eliciting a single annotation. We highlight
the needs of the comparable corpora scenario where
we have two kinds of annotations - class label as-
signment and parallel segment extraction and pro-
pose strategies in active learning that involve multi-
ple annotations. A relevant setup is multitask learn-
ing (Caruana, 1997) which is increasingly becom-
ing popular in natural language processing for learn-
ing from multiple learning tasks. There has been
very less work in the area of multitask active learn-
ing. (Reichart et al, 2008) proposes an extension of
the single-sided active elicitation task to a multi-task
scenario, where data elicitation is performed for two
or more independent tasks at the same time. (Settles
et al, 2008) propose elicitation of annotations for
image segmentation under a multi-instance learning
framework.
Active learning with multiple annotations also has
similarities to the recent body of work in learn-
ing from instance feedback and feature feedback
(Melville et al, 2005). (Druck et al, 2009) pro-
pose active learning extensions to the gradient ap-
proach of learning from feature and instance feed-
back. However, in the comparable corpora problem
although the second annotation is geared towards
learning better features by enhancing the coverage
of the lexicon, the annotation itself is not on the fea-
tures but for extracting training data that is then used
to train the lexicon.
70
3 Supervised Comparable Sentence
Classification
In this section we discuss our supervised training
setup and the classification algorithm. Our classifier
tries to identify comparable sentences from among a
large pool of noisy comparable sentences. In this pa-
per we define comparable sentences as being trans-
lations that have around fifty percent or more trans-
lation equivalence. In future we will evaluate the ro-
bustness of the classifier by varying levels of noise
at the sentence level.
3.1 Training the Classifier
Following (Munteanu and Marcu, 2005), we use a
Maximum Entropy classifier to identify comparable
sentences. The classifier probability can be defined
as:
Pr(ci|S, T ) = 1Z(S, T )exp
?
?
n?
j=1
?jfij(ci, S, T )
?
?
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the fea-
ture functions and are estimated by optimizing on a
training data set. For the task of classifying a sen-
tence pair, there are two classes, c0 = comparable
and c1 = non parallel. A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are comparable.
To train the classifier we need comparable sen-
tence pairs and non-parallel sentence pairs. While
it is easy to find negative examples online, ac-
quiring comparable sentences is non-trivial and re-
quires human intervention. (Munteanu and Marcu,
2005) construct negative examples automatically
from positive examples by pairing all source sen-
tences with all target sentences. We, however, as-
sume the availability of both positive and negative
examples to train the classifier. We use the GIS
learning algorithm for tuning the model parameters.
3.2 Feature Computation
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a target
word t if p(s|t) > 0.5. Target word alignment is
computed similarly. Long contiguous sections of
aligned words indicate parallelism. We use the fol-
lowing features:
? Source and target sentence length ratio
? Source and target sentence length difference
? Lexical probability score, similar to IBM
model 1
? Number of aligned words
? Longest aligned word sequence
? Number of un-aligned words
Lexical probability score, and alignment features
generate two sets of features based on translation
lexica obtained by training in both directions. Fea-
tures are normalized with respect to the sentence
length.
Figure 1: Seed parallel corpora size vs. Classifier perfor-
mance in Urdu-English language pair
In our experiments we observe that the most in-
formative features are the ones involving the prob-
abilistic lexicon. However, the comparable corpora
obtained for training the classifier cannot be used for
automatically training a lexicon. We, therefore, re-
quire the availability of an initial seed parallel cor-
pus that can be used for computing the lexicon and
the associated feature functions. We notice that the
size of the seed corpus has a large influence on the
accuracy of the classifier. Figure 1 shows a plot with
71
the initial size of the corpus used to construct the
probabilistic lexicon on x-axis and its effect on the
accuracy of the classifier on y-axis. The sentences
were drawn randomly from a large pool of Urdu-
English parallel corpus and it is clear that a larger
pool of parallel sentences leads to a better lexicon
and an improved classifier.
4 Active Learning with Multiple
Annotations
4.1 Cost Motivation
Lack of existing annotated data requires reliable
human annotation that is expensive and effort-
intensive. We propose active learning for the prob-
lem of effectively acquiring multiple annotations
starting with unlabeled data. In active learning, the
learner has access to a large pool of unlabeled data
and sometimes a small portion of seed labeled data.
The objective of the active learner is then to se-
lect the most informative instances from the unla-
beled data and seek annotations from a human ex-
pert, which it then uses to retrain the underlying su-
pervised model for improving performance.
A meaningful setup to study multi annotation ac-
tive learning is to take into account the cost involved
for each of the annotations. In the case of compara-
ble corpora we have two annotation tasks, each with
cost modelsCost1 andCost2 respectively. The goal
of multi annotation active learning is to select the
optimal set of instances for each annotation so as to
maximize the benefit to the classifier. Unlike the tra-
ditional active learning, where we optimize the num-
ber of instances we label, here we optimize the se-
lection under a provided budget Bk per iteration of
the active learning algorithm.
4.2 Active Learning Setup
We now discuss our active learning framework for
building comparable corpora as shown in Algo-
rithm 1. We start with an unlabeled dataset U0 =
{xj =< sj , tj >} and a seed labeled dataset L0 =
{(< sj , tj >, ci)}, where c ? 0, 1 are class la-
bels with 0 being the non-parallel class and 1 being
the comparable data class. We also have T0 = {<
sk, tk >} which corresponds to parallel segments
or sentences identified from L0 that will be used in
training the probabilistic lexicon. Both T0 and L0
can be very small in size at the start of the active
learning loop. In our experiments, we tried with as
few as 50 to 100 sentences for each of the datasets.
We perform an iterative budget motivated active
learning loop for acquiring labeled data over k it-
erations. We start the active learning loop by first
training a lexicon with the available Tk and then us-
ing that we train the classifier over Lk. We, then
score all the sentences in the Uk using the model ?
and apply our selection strategy to retrieve the best
scoring instance or a small batch of instances. In the
simplest case we annotate this instance and add it
back to the tuning set Ck for re-training the classi-
fier. If the instance was a comparable sentence pair,
then we could also perform the second annotation
conditioned upon the availability of the budget. The
identified sub-segments (ssi , tti) are added back to
the training data Tk used for training the lexicon in
the subsequent iterations.
Algorithm 1 ACTIVE LEARNING SETUP
1: Given Unlabeled Comparable Corpus: U0
2: Given Seed Parallel Corpus: T0
3: Given Tuning Corpus: L0
4: for k = 0 to K do
5: Train Lexicon using Tk
6: ? = Tune Classifier using Ck
7: while Cost < Bk do
8: i = Query(Uk,Lk,Tk,?)
9: ci = Human Annotation-1 (si, ti)
10: (ssi ,tti) = Human Annotation-2 xi
11: Lk = Ck ? (si, ti, ci)
12: Tk = Tk ? (ssi, tti)
13: Uk = Uk - xi
14: Cost = Cost1 + Cost2
15: end while
16: end for
5 Sampling Strategies for Active Learning
5.1 Acquiring Training Data for Classifier
Our selection strategies for obtaining class labels for
training the classifier uses the model in its current
state to decide on the informative instances for the
next round of iterative training. We propose the fol-
lowing two sampling strategies for this task.
72
5.1.1 Certainty Sampling
This strategy selects instances where the current
model is highly confident. While this may seem
redundant at the outset, we argue that this crite-
ria can be a good sampling strategy when the clas-
sifier is weak or trained in an impoverished data
scenario. Certainty sampling strategy is a lot sim-
ilar to the idea of unsupervised approaches like
boosting or self-training. However, we make it a
semi-supervised approach by having a human in the
loop to provide affirmation for the selected instance.
Consider the following scenario. If we select an
instance that our current model prefers and obtain
a contradicting label from the human, then this in-
stance has a maximal impact on the decision bound-
ary of the classifier. On the other hand, if the label
is reaffirmed by a human, the overall variance re-
duces and in the process, it also helps in assigning
higher preference for the configuration of the deci-
sion boundary. (Melville et al, 2005) introduce a
certainty sampling strategy for the task of feature
labeling in a text categorization task. Inspired by
the same we borrow the name and also apply this
as an instance sampling approach. Given an in-
stance x and the classifier posterior distribution for
the classes as P (.), we select the most informative
instance as follows:
x? = argmaxxP (c = 1|x)
5.1.2 Margin-based Sampling
The certainty sampling strategy only considers the
instance that has the best score for the comparable
sentence class. However we could benefit from in-
formation about the second best class assigned to
the same instance. In the typical multi-class clas-
sification problems, earlier work shows success us-
ing such a ?margin based? approach (Scheffer et al,
2001), where the difference between the probabil-
ities assigned by the underlying model to the first
best and second best classes is used as the sampling
criteria.
Given a classifier with posterior distribution
over classes for an instance P (c = 1|x),
the margin based strategy is framed as x? =
argminxP (c1|x)? P (c2|x), where c1 is the best
prediction for the class and c2 is the second best
prediction under the model. It should be noted that
for binary classification tasks with two classes, the
margin sampling approach reduces to an uncertainty
sampling approach (Lewis and Catlett, 1994).
5.2 Acquiring Parallel Segments for Lexicon
Training
We now propose two sampling strategies for the sec-
ond annotation. Our goal is to select instances that
could potentially provide parallel segments for im-
proved lexical coverage and feature computation.
5.2.1 Diversity Sampling
We are interested in acquiring clean parallel seg-
ments for training a lexicon that can be used in fea-
ture computation. It is not clear how one could use a
comparable sentence pair to decide the potential for
extracting a parallel segment. However, it is highly
likely that if such a sentence pair has new cover-
age on the source side, then it increases the chances
of obtaining new coverage. We, therefore, propose
a diversity based sampling for extracting instances
that provide new vocabulary coverage . The scor-
ing function tc score(s) is defined below, where
V oc(s) is defined as the vocabulary of source sen-
tence s for an instance xi =< si, ti >, T is the set
of parallel sentences or segments extracted so far.
tc score(s) =
|T |?
s=1
sim(s, s?) ? 1|T | (1)
sim(s, s?) = |(V oc(s) ? V oc(s?)| (2)
5.2.2 Alignment Ratio
We also propose a strategy that provides direct in-
sight into the coverage of the underlying lexicon and
prefers a sentence pair that is more likely to be com-
parable. We call this alignment ratio and it can be
easily computed from the available set of features
discussed in Section 3 as below:
a score(s) = #unalignedwords#alignedwords (3)
s? = argmaxsa score(s) (4)
This strategy is quite similar to the diversity based
approach as both prefer selecting sentences that have
73
a potential to offer new vocabulary from the com-
parable sentence pair. However while the diver-
sity approach looks only at the source side coverage
and does not depend upon the underlying lexicon,
the alignment ratio utilizes the model for computing
coverage. It should also be noted that while we have
coverage for a word in the sentence pair, it may not
make it to the probabilistically trained and extracted
lexicon.
5.3 Combining Multiple Annotations
Finally, given two annotations and corresponding
sampling strategies, we try to jointly select the sen-
tence that is best suitable for obtaining both the an-
notations and is maximally beneficial to the classi-
fier. We select a single instance by combining the
scores from the different selection strategies as a
geometric mean. For instance, we consider a mar-
gin based sampling (margin) for the first annota-
tion and a diversity sampling (tc score) for the sec-
ond annotation, we can jointly select a sentence that
maximizes the combined score as shown below:
total score(s) = margin(s) ? tc score(s) (5)
s? = argmaxstotal score(s) (6)
6 Experiments and Results
6.1 Data
This research primarily focuses on identifying com-
parable sentences from a pool of dominantly non-
parallel sentences. To our knowledge, there is a
dearth of publicly available comparable corpora of
this nature. We, therefore, simulate a low-resource
scenario by using realistic assumptions of noise
and parallelism at both the corpus-level and the
sentence-level. In this section we discuss the pro-
cess and assumptions involved in the creation of our
datasets and try to mimic the properties of real-world
comparable corpora harvested from the web.
We first start with a sentence-aligned parallel cor-
pus available for the language pair. We then divide
the corpus into three parts. The first part is called
the ?sampling pool? and is set aside to use for draw-
ing sentences at random. The second part is used
to act as a non-parallel corpus. We achieve non-
parallelism by randomizing the mapping of the tar-
get sentences with the source sentences. This is a
slight variation of the strategy used in (Munteanu
and Marcu, 2005) for generating negative examples
for their classifier. The third part is used to synthe-
size a comparable corpus at the sentence-level. We
perform this by first selecting a parallel sentence-
pair and then padding either sides by a source and
target segment drawn independently from the sam-
pling pool. We control the length of the non-parallel
portion that is appended to be lesser than or equal
to the original length of the sentence. Therefore, the
resulting synthesized comparable sentence pairs are
guaranteed to contain at least 50% parallelism.
We use this dataset as the unlabeled pool from
which the active learner selects instances for label-
ing. Since the gold-standard labels for this corpus
are already available, which gives us better control
over automating the active learning process, which
typically requires a human in the loop. However,
our active learning strategies are in no way limited
by the simulated data setup and can generalize to the
real world scenario with an expert providing the la-
bels for each instance.
We perform our experiments with data from two
language pairs: Urdu-English and Spanish-English.
For Urdu-English, we use the parallel corpus NIST
2008 dataset released for the translation shared task.
We start with 50,000 parallel sentence corpus from
the released training data to create a corpus of
25,000 sentence pairs with 12,500 each of compa-
rable and non-parallel sentence pairs. Similarly, we
use 50,000 parallel sentences from the training data
released by the WMT 2008 datasets for Spanish-
English to create a corpus of 25,000 sentence pairs.
We also use two held-out data sets for training and
tuning the classifier, consisting of 1000 sentence
pairs (500 non-parallel and 500 comparable).
6.2 Results
We perform two kinds of evaluations: the first, to
show that our active learning strategies perform well
across language pairs and the second, to show that
multi annotation active learning leads to a good im-
provement in performance of the classifier.
6.2.1 How does the Active Learning perform?
In section 5, we proposed multiple active learn-
ing strategies for both eliciting both kinds of annota-
tions. A good active learning strategy should select
74
instances that contribute to the maximal improve-
ment of the classifier. The effectiveness of active
learning is typically tested by the number of queries
the learner asks and the resultant improvement in
the performance of the classifier. The classifier per-
formance in the comparable sentence classification
task can be computed as the F-score on the held out
dataset. For this work, we assume that both the an-
notations require the same effort level and so assign
uniform cost for eliciting each of them. Therefore
the number of queries is equivalent to the total cost
of supervision.
Figure 2: Active learning performance for the compara-
ble corpora classification in Urdu-English language-pair
Figure 3: Active learning performance for the compara-
ble corpora classification in Spanish-English language-
pair
Figure 2 shows our results for the Urdu-English
language pair, and Figure 3 plots the Spanish-
English results with the x-axis showing the total
number of queries posed to obtain annotations and
the y-axis shows the resultant improvement in accu-
racy of the classifier. In these experiments we do
not actively select for the second annotation but ac-
quire the parallel segment from the same sentence.
We compare this over a random baseline where the
sentence pair is selected at random and used for elic-
iting both annotations at the same time.
Firstly, we notice that both our active learn-
ing strategies: certainty sampling and margin-based
sampling perform better than the random baseline.
For the Urdu-English language pair we can see that
for the same effort expended (i.e 2000 queries) the
classifier has an increase in accuracy of 8 absolute
points. For Spanish-English language pair the ac-
curacy improvement is 6 points over random base-
line. Another observation from Figure 3 is that for
the classifier to reach an fixed accuracy of 68 points,
the random sampling method requires 2000 queries
while the from the active selection strategies require
significantly less effort of about 500 queries.
6.2.2 Performance of Joint Selection with
Multiple Annotations
We now evaluate our joint selection strategy that
tries to select the best possible instance for both
the annotations. Figure 4 shows our results for the
Urdu-English language pair, and Figure 5 plots the
Spanish-English results for active learning with mul-
tiple annotations. As before, the x-axis shows the
total number of queries posed, equivalent to the cu-
mulative effort for obtaining the annotations and the
y-axis shows the resultant improvement in accuracy
of the classifier.
We evaluate the multi annotation active learning
against two single-sided baselines where the sam-
pling focus is on selecting instances according to
strategies suitable for one annotation at a time. The
best performing active learning strategy for the class
label annotations is the certainty sampling (annot1)
and so for one single-sided baseline, we use this
baseline. We also obtain the second annotation for
the same instance. By doing so, we might be se-
lecting an instance that is sub-optimal for the sec-
ond annotation and therefore the resultant lexicon
may not maximally benefit from the instance. We
also observe, from our experiments, that the diver-
sity based sampling works well for the second anno-
75
tation and alignment ratio does not perform as well.
So, for the second single-sided baseline we use the
diversity based sampling strategy (annot2) and get
the first annotation for the same instance. Finally
we compare this with the joint selection approach
proposed earlier that combines both the annotation
strategies (annot1+annot2). In both the language
pairs we notice that joint selection for both anno-
tations performs better than the baselines.
Figure 4: Active learning with multiple annotations and
classification performance in Urdu-English
Figure 5: Active learning with multiple annotations and
classification performance in Spanish-English
7 Conclusion and Future Work
In this paper, we proposed active learning with mul-
tiple annotations for the challenge of building com-
parable corpora in low-resource scenarios. In par-
ticular, we identified two kinds of annotations: class
labels (for identifying comparable vs. non-parallel
data) and clean parallel segments within the com-
parable sentences. We implemented multiple inde-
pendent strategies for obtaining each of the abve in
a cost-effective manner. Our active learning experi-
ments in a simulated low-resource comparable cor-
pora scenario across two language pairs show signif-
icant results over strong baselines. Finally we also
proposed a joint selection strategy that selects a sin-
gle instance which is beneficial to both the annota-
tions. The results indicate an improvement over sin-
gle strategy baselines.
There are several interesting questions for future
work. Throughout the paper we assumed uniform
costs for both the annotations, which will need to
be verified with human subjects. We also hypoth-
esize that obtaining both annotations for the same
sentence may be cheaper than getting them from two
different sentences due to the overhead of context
switching. Another assumption is that of the exis-
tence of a single contiguous parallel segment in a
comparable sentence pair, which needs to be veri-
fied for corpora on the web.
Finally, active learning assumes availability of an
expert to answer the queries. Availability of an ex-
pert for low-resource languages and feasibility of
running large scale experiments is difficult. We,
therefore, have started working on crowdsourcing
these annotation tasks on Amazon Mechanical Turk
(MTurk) where it is easy to find people and quickly
run experiments with real people.
Acknowledgement
This material is based upon work supported in part
by the U. S. Army Research Laboratory and the U.
S. Army Research Office under grant W911NF-10-
1-0533, and in part by NSF under grant IIS 0916866.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Rich Caruana. 1997. Multitask learning. In Machine
Learning, pages 41?75.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active learning by labeling features. In Pro-
ceedings of Conference on Empirical Methods in Nat-
76
ural Language Processing (EMNLP 2009), pages 81?
90.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL 2010.
Pascale Fung and Lo Yen Yee. 1998. An IR approach for
translating new words from nonparallel, comparable
texts. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics, pages
414?420, Montreal, Canada.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 148?156. Morgan Kauf-
mann.
Prem Melville, Foster Provost, Maytal Saar-Tsechansky,
and Raymond Mooney. 2005. Economical active
feature-value acquisition through expected utility esti-
mation. In UBDM ?05: Proceedings of the 1st interna-
tional workshop on Utility-based data mining, pages
10?16, New York, NY, USA. ACM.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81?88, Sydney, Aus-
tralia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345?1359, October.
Chris Quirk, Raghavendra U. Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of the Machine Translation Summit XI, pages
377?384, Copenhagen, Denmark.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguis-
tic annotations. In Proceedings of ACL-08: HLT,
pages 861?869, Columbus, Ohio, June. Association
for Computational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as a
parallel corpus. Comput. Linguist., 29(3):349?380.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In IDA ?01: Proceedings of the 4th
International Conference on Advances in Intelligent
Data Analysis, pages 309?318, London, UK. Springer-
Verlag.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In In Advances in
Neural Information Processing Systems (NIPS, pages
1289?1296. MIT Press.
Bing Zhao and Stephan Vogel. 2002. Full-text story
alignment models for chinese-english bilingual news
corpora. In Proceedings of the ICSLP ?02, September.
77
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 386?392,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU Haitian Creole-English Translation System for WMT 2011
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi Ambati, Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sanjika,nbach,qing,vamshi,vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine
translation system submitted to the WMT11
Featured Translation Task, which involves
translating Haitian Creole SMS messages into
English. In our experiments we try to ad-
dress the issue of noise in the training data,
as well as the lack of parallel training data.
Spelling normalization is applied to reduce
out-of-vocabulary words in the corpus. Us-
ing Semantic Role Labeling rules we expand
the available training corpus. Additionally we
investigate extracting parallel sentences from
comparable data to enhance the available par-
allel data.
1 Introduction
In this paper we describe the CMU-SMT Haitian
Creole-English translation system that was built as
part of the Featured Translation Task of the WMT11.
The task involved translating text (SMS) messages
that were collected during the humanitarian opera-
tions in the aftermath of the earthquake in Haiti in
2010.
Due to the circumstances of this situation, the
SMS messages were often noisy, and contained in-
complete information. Additionally they sometimes
contained text from other languages (e.g. French).
As is typical in SMS messages, abbreviated text (as
well as misspelled words) were present. Further,
since the Haitian Creole orthography is not fully
standardized (Allen, 1998), the text inherently con-
tained several different spelling variants.
These messages were translated into English by
a group of volunteers during the disaster response.
The background and the details of this crowdsourc-
ing translation effort is discussed in Munro (2010).
Some translations contain additional annotations
which are not part of the original SMS, possibly
added by the translators to clarify certain issues with
the original message. Along with the noise, spelling
variants, and fragmented nature of the SMS mes-
sages, the annotations contribute to the overall diffi-
culty in building a machine translation system with
this type of data. We aim to address some of these
issues in out effort.
Another challenge with building a Haitian Creole-
English translation system is the lack of parallel
data. As Haitian Creole is a less commonly spo-
ken language, the available resources are limited.
Other than the manually translated SMS messages,
the available Haitian Creole-English parallel data
is about 2 million tokens, which is considerably
smaller than the parallel data available for the Stan-
dard Translation Task of the WMT11.
Lewis (2010) details the effort quickly put
forth by the Microsoft Translator team in building
a Haitian Creole-English translation system from
scratch, as part of the relief effort in Haiti. We took
a similar approach to this shared task: rapidly build-
ing a translation system to a new language pair uti-
lizing available resources. Within a short span (of
about one week), we built a baseline translation sys-
tem, identified the problems with the system, and
exploited several approaches to rectify them and im-
prove its overall performance. We addressed the is-
sues above (namely: noise in the data and sparsity of
parallel data) when building our translation system
for Haitian Creole-English task. We also normalized
386
different spelling variations to reduce the number of
out-of-vocabulary (OOV) tokens in the corpus. We
used Semantic Role Labeling to expand the available
training corpus. Additionally we exploited other re-
sources, such as comparable corpora, to extract par-
allel data to enhance the limited amount of available
parallel data.
The paper is organized as follows: Section 2
presents the baseline system used, along with a de-
scription of training and testing data used. Section 3
explains different preprocessing schemes that were
tested for SMS data, and their effect on the trans-
lation performance. Corpus expansion approach is
given in Section 4. Parallel data extraction from
comparable corpora is presented in section 5. We
present our concluding remarks in Section 6.
2 System Architecture
The WMT11 has provided a collection of Haitian
Creole-English parallel data from a variety of
sources, including data from CMU1. A summary
of the data is given in Table 1. The primary in-
domain data comprises the translated (noisy) SMS
messages. The additional data contains newswire
text, medical dialogs, the Bible, several bilingual
dictionaries, and parallel sentences from Wikipedia.
Corpus Sentences Tokens (HT/EN)
SMS messages 16,676 351K / 324K
Newswire text 13,517 336K / 292K
Medical dialog 1,619 10K / 10K
Dictionaries 42,178 97K / 92K
Other 41,872 939K / 865K
Wikipedia 8,476 77K / 90K
Total 124,338 1.81M / 1.67M
Table 1: Haitian Creole (HT) and English (EN) parallel
data provide by WMT11
We preprocessed the data by separating the punc-
tuations, and converting both sides into lower case.
SMS data was further processed to normalize quo-
tations and other punctuation marks, and to remove
all markups.
To build a baseline translation system we fol-
lowed the recommended steps: generate word align-
1www.speech.cs.cmu.edu/haitian/
ments using GIZA++ (Och and Ney, 2003) and
phrase extraction using Moses (Koehn et al, 2007).
We built a 4-gram language model with the SRI
LM toolkit (Stolcke, 2002) using English side of
the training corpus. Model parameters for the lan-
guage model, phrase table, and lexicalized reorder-
ing model were optimized via minimum error-rate
(MER) training (Och, 2003).
The SMS test sets were provided in two formats:
raw (r) and cleaned (cl), where the latter had been
manually cleaned. We used the SMS dev clean to op-
timize the decoder parameters and the SMS devtest
clean and SMS devtest raw as held-out evaluation sets.
Each set contains 900 sentences. A separate SMS
test, with 1274 sentences, was used as the unseen
test set in the final evaluation. For each experiment
we report the case-insensitive BLEU (Papineni et
al., 2002) score.
Using the available training data we built several
baseline systems: The first system (Parallel-OOD),
uses all the out-of-domain parallel data except the
Wikipedia sentences. The second system, in addi-
tion, includes Wikipedia data. The third system uses
all available parallel training data (including both the
out-of-domain data as well as in-domain SMS data).
We used the third system as the baseline for later
experiments.
dev (cl) devtest (cl) devtest (r)
Parallel-OOD 23.84 22.28 17.32
+Wikipedia 23.89 22.42 17.37
+SMS 32.28 33.49 29.95
Table 2: Translation results in BLEU for different corpora
Translation results for different test sets using the
three systems are presented in Table 2. No signifi-
cant difference in BLEU was observed with the ad-
dition of Wikipedia data. However, a significant
improvement in performance can be seen when in-
domain SMS data is added, despite the fact that this
is noisy data. Because of this, we paid special atten-
tion to clean the noisy SMS data.
3 Preprocessing of SMS Data
In this section we explain two approaches that we
explored to reduce the noise in the SMS data.
387
3.1 Lexicon-based Collapsing of OOV Words
We observed that a number of words in the raw SMS
data consisted of asterisks or special character sym-
bols. This seems to occur because either users had
to type with a phone-based keyboard or simply due
to processing errors in the pipeline. Our aim, there-
fore, was to collapse these incorrectly spelled words
to their closest vocabulary entires from the rest of
the data.
We first built a lexicon of words using the entire
data provided for the Featured Task. We then built
a second probabilistic lexicon by cross-referencing
SMS dev raw with the cleaned-up SMS dev clean.
The first resource can be treated as a dictionary
while the second is a look-up table. We processed
incoming text by first selecting all the words with
special characters in the text, and then computing
an edit distance with each of the words in the first
lexicon. We return the most frequent word that is
the closest match as a substitute. For all words that
don?t have a closest match, we looked them up in the
probabilistic dictionary and return a potential substi-
tution if it exists. As the probabilistic dictionary is
constructed using a very small amount of data, the
two-level lookup helps to place less trust in it and
use it only as a back-off option for a missing match
in the larger lexicon.
This approach only collapses words with special
characters to their closest in-vocabulary words. It
does not make a significant difference to the OOV
ratios, but reduces the number of tokens in the
dataset. Using this approach we were able to col-
lapse about 80% of the words with special characters
to existing vocabulary entries.
3.2 Spelling Normalization
One of the most problematic issues in Haitian Cre-
ole SMS translation system is misspelled words.
When training data contains misspelled words, the
translation system performance will be affected at
several levels, such as word alignment, phrase/rule
extractions, and tuning parameters (Bertoldi et al,
2010). Therefore, it is desirable to perform spelling
correction on the data. Spelling correction based
on the noisy channel model has been explored in
(Kernighan et al, 1990; Brill and Moore, 2000;
Toutanova and Moore, 2002). The model is gener-
ally presented in the following form:
p(c?|h) = argmax
?c
p(h|c)p(c) (1)
where h is the Haitian Creole word, and c is a pos-
sible correction. p(c) is a source model which is a
prior of word probabilities. p(h|c) is an error model
or noisy channel model that accounts for spelling
transformations on letter sequences.
Unfortunately, in the case of Haitian Creole SMS
we do not have sufficient data to estimate p(h|c)
and p(c). However, we can assume p(c|h) ? p(c)
and c is in the French vocabulary and is not an En-
glish word. The rationale for this, from linguistic
point of view, is that Haitian Creole developed from
the 18th century French. As a result, an important
part of the Haitian Creole lexicon is directly derived
from French. Furthermore, SMS messages some-
times were mixed with English words. Therefore,
we ignore c if it appears in an English dictionary.
Given h, how do we get a list of possible normal-
ization c and estimate p(c)? We use edit distance
of 1 between h and c. An edit can be a deletion,
transposition, substitution, or insertion. If a word
has l characters, there will be 66l+31 possible cor-
rections2. It may result in a large list. However,
we only keep possible normalizations which appear
in a French dictionary and do not appear in an En-
glish dictionary3. To approximate p(c), we use the
French parallel Giga training data from the Shared
Task of the WMT11. p(c) is estimated by MLE. Fi-
nally, our system chooses the French word with the
highest probability.
dev (cl) devtest (cl) test (cl)
Before 2.6 ; 16 2.7 ; 16 2.6 ; 16
After 2.2 ; 13.63 2.3 ; 13.95 2.2 ; 14.3
Table 3: Percentage of OOV tokens and types in test sets
before and after performing spelling normalization.
Table 3 shows that spelling normalization helps
to bring down the percentage of OOV tokens and
types by 0.4% and 2% respectively on the three test
2l deletions, l-1 transpositions, 32l substitutions, and 32(l+1)
insertions; Haitian Creole orthography has 32 forms.
3The English dictionary was created from the English Gigaword
corpus.
388
sets. Some examples of Haitian Creole words and
their French normalization are (tropikal:tropical),
(economiques:economique), (irjan:iran), (idanti-
fie:identifie).
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
S1 32.18 30.22 25.45
S2 28.9 31.06 27.69
Table 4: Translation results in BLEU with/without
spelling correction
Given the encouraging OOV reductions, we ap-
plied the spelling normalization for the full corpus,
and built new translation systems. Our baseline sys-
tem has no spelling correction (for the training cor-
pus or the test sets); in S1, the spelling corrections
is applied to all words; in S2, the spelling correc-
tion is only applied to Haitian Creole words that oc-
cur only once or twice in the data. In S1, 11.5% of
Haitian Creole words had been mapped to French,
including high frequency words. Meanwhile, 4.5%
Haitian Creole words on training data were mapped
to French words in S2. Table 4 presents a compar-
ison of translation performance of the baseline, S1
and S2 for the SMS test sets. Unfortunately, none of
systems with spelling normalization outperformed
the system trained on the original data. Restricting
the spelling correction only to infrequent words (S2)
performed better for the devtest sets, but not for the
dev set, although all the test sets come from the same
domain.
4 Corpus Expansion using Semantic Role
Labeling
To address the problem of limited resources, we
tried to expand the training corpus by applying the
corpus expansion method described in (Gao and Vo-
gel, 2011). First, we parsed and labeled the semantic
roles of the English side of the corpus, using the AS-
SERT labeler (Pradhan et al, 2004). Next, using the
word alignment models of the parallel corpus, we
extracted Semantic Role Label (SRL) substitution
rules. SRL rules consist of source and target phrases
that cover whole constituents of semantic roles, the
verb frames they belong to, and the role labels of
the constituents. The source and target phrases must
comply with the restrictions detailed in (Gao and Vo-
gel, 2011). Third, for each sentence, we replaced
one of embedded SRL substitution rules with equiv-
alent rules that have the same verb frame and the
same role label.
The original method includes an additional but
crucial step of filtering out the grammatically incor-
rect sentences using an SVM classifier, trained with
labeled samples. However, we were unable to find
Haitian Creole speakers who could manually label
training data for the filtering step. Therefore, we
were forced to skip this filtering step. We expanded
the full training corpus which contained 124K sen-
tence pairs, resulting in an expanded corpus with
505K sentences. The expanded corpus was force-
aligned using the word alignment models trained
on the original unexpanded corpus. A new trans-
lation system was built using the original plus the
expanded corpus. As seen in Table 5, we observed
a small improvement with the expanded corpus for
the raw devtest. This method did not improve per-
formance for the other two test sets.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Expanded 31.79 32.98 30.1
Table 5: Translation results in BLEU with/without corpus
expansion
A possible explanation for this, in addition to
the missing component of filtering, is the low qual-
ity of SRL parsing on the SMS corpus. We ob-
served a very small ratio of expansions in the
Haitian Creole-English data, when compared to the
Chinese-English experiment shown in (Gao and Vo-
gel, 2011). The latter used a high quality corpus for
the expansion and the expanded corpus was 20 times
larger than the original one. Due to the noisy nature
of the available parallel data, only 61K of the 124K
sentences were successfully parsed and SRL-labeled
by the labeler.
389
5 Extracting Parallel Data from
Comparable Data
As we only have a limited amount of parallel data,
we focused on automatically extracting additional
parallel data from other available resources, such as
comparable corpora. We were not able to find com-
parable news articles in Haitian Creole and English.
However, we found several hundred Haitian Creole
medical articles on the Web which were linked to
comparable English articles4. Although some of the
medical articles seemed to be direct translations of
each other, converting the original pdf formats into
text did not produce sentence aligned parallel arti-
cles. Rather, it produced sentence fragments (some-
times in different orders) due to the structural dif-
ferences in the article pair. Hence a parallel sen-
tence detection technique was necessary to process
the data. Because the SMS messages are related to
the disaster relief effort, which may include many
words in the medical domain, we believe the newly
extracted data may help improve translation perfor-
mance.
Following Munteanu and Marcu (2005), we used
a Maximum Entropy classifier to identify compara-
ble sentence. To avoid the problem of having dif-
ferent sentence orderings in the article pair, we take
every source-target sentence pair in the two articles,
and apply the classifier to detect if they are paral-
lel. The classifier approach is appealing to a low-
resource language such as Haitian Creole, because
the features for the classifier can be generated with
minimal translation resources (i.e. a translation lex-
icon).
5.1 Maximum Entropy Classifier
The classifier probability can be defined as:
Pr(ci|S, T ) =
exp
(?n
j=1 ?jfij(ci, S, T )
)
Z(S, T )
(2)
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the feature
functions and are estimated by optimizing on a train-
ing data set. For the task of classifying a sentence
pair, there are two classes, c0 = non ? parallel
4Two main sources were: www.rhin.org and www.nlm.nih.gov
and c1 = parallel . A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are parallel.
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a tar-
get word t if p(s|t) > 0.5. Target word align-
ment is computed similarly. We defined a feature set
which includes: length ratio and length difference
between source and target sentences, lexical proba-
bility scores similar to IBM model 1 (Brown et al,
1993), number of aligned/unaligned words and the
length of the longest aligned word sequence. Lexi-
cal probability score, and alignment features gener-
ate two sets of features based on translation lexica
obtained by training in both directions. Features are
normalized with respect to the sentence length.
5.2 Training and Testing the Classifier
To train the model we need training examples that
belong to each of the two classes: parallel and non-
parallel. Initially we used a subset of the available
parallel data as training examples for the classifier.
This data was primarily sourced from medical con-
versations and newswire text, whereas the compa-
rable data was found in medical articles. This mis-
match in domain resulted in poor classification per-
formance. Therefore we manually aligned a set of
250 Haitian Creole-English sentence pairs from the
medical articles and divided them in to a training set
(175 sentences) and a test set (100 sentences).
The parallel sentence pairs were directly used as
positive examples. In selecting negative examples,
we followed the same approach as in (Munteanu
and Marcu, 2005): pairing all source phrases with
all target phrases, but filter out the parallel pairs and
those that have high length difference or a low lex-
ical overlap, and then randomly select a subset of
phrase pairs as the negative training set. The test
set was generated in a similar manner. The model
parameters were estimated using the GIS algorithm.
We used the trained ME model to classify the sen-
tences in the test set into the two classes, and notice
how many instances are classified correctly.
Classification results are as given in Table 6. We
notice that even with a smaller training set, the clas-
sifier produces results with high precision. Using
390
Precision Recall F-1 Score
Training Set 93.90 77.00 84.61
Test Set 85.53 74.29 79.52
Table 6: Performance of the Classifier
the trained classifier, we processed 220 article pairs
which contained a total of 20K source sentences
and 18K target sentences. The classifier selected
about 10K sentences as parallel. From these, we se-
lected sentences where pr(c1|S, T ) > 0.7 for trans-
lation experiments. The extracted data expanded the
source vocabulary by about 5%.
We built a second translation system by combin-
ing the baseline parallel corpus and the extracted
corpus. Table 7 shows the translation results for this
system.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Extracted 32.29 33.29 29.89
Table 7: Translation results in BLEU with/without ex-
tracted data
The results indicate that there is no significant per-
formance difference in using the extracted data. This
may be due to the relatively small size of the com-
parable corpus we used when extract the data.
6 Conclusion
Building an MT system to translate Haitian Creole
SMS messages involved several challenges. There
was only a limited amount of parallel data to train
the models. The SMS messages tend to be quite
noisy. After building a baseline MT system, we
investigated several approaches to improve its per-
formance. In particular, we tried collapsing OOV
words using a lexicon generated with clean data, and
normalize different variations in spelling. However,
these methods did not results in improved translation
performance.
We tried to address the data sparseness problem
with two approaches: expanding the corpus using
SRL rules, and extracting parallel sentences from
a collection of comparable documents. Corpus ex-
pansion showed a small improvement for the raw
devtest. Both corpus expansion and parallel data
extraction did not have a positive impact on other
test sets. Both these methods have shown significant
performance improvement in the past in large data
scenarios (for Chinese-English and Arabic-English),
but failed to show improvements in the current low-
data scenario. Thus, we need further investigations
in handling noisy data, especially in low-resource
scenarios.
Acknowledgment
We thank Julianne Mentzer for assisting with editing
and proofreading the final version of the paper. We
also thank the anonymous reviewers for their valu-
able comments.
References
Jeff Allen. 1998. Lexical variation in haitian cre-
ole and orthographic issues for machine translation
(MT) and optical character recognition (OCR) appli-
cations. In Proceedings of the First Workshop on Em-
bedded Machine Translation systems of AMTA confer-
ence, Philadelphia, Pennsylvania, USA, October.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.
2010. Statistical machine translation of texts with mis-
spelled words. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, California, June.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics (ACL 2000), pages
286?293.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Portland,
Oregon, USA, June.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics - Volume 2,
COLING ?90, pages 205?210.
391
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
June.
William Lewis. 2010. Haitian Creole: How to build and
ship an mt engine from scratch in 4 days, 17 hours, &
30 minutes. In Proceedings of the 14th Annual confer-
ence of the European Association for Machine Trans-
lation (EAMT), Saint-Raphae?l, France, May.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collab-
orative Crowdsourcing for Translation, Denver, Col-
orado, USA, October-November.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL-2004).
Andreas Stolcke. 2002. An extensible language model-
ing toolkit. In Proc. of International Conference on
Spoken Language Processing, volume 2, pages 901?
904, Denver, CO, September.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2002).
392
