Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 829?839,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Not as Awful as it Seems: Explaining German Case through
Computational Experiments in Fluid Construction Grammar
Remi van Trijp
Sony Computer Science Laboratory Paris
6 Rue Amyot
75005 Paris (France)
remi@csl.sony.fr
Abstract
German case syncretism is often assumed
to be the accidental by-product of historical
development. This paper contradicts this
claim and argues that the evolution of Ger-
man case is driven by the need to optimize
the cognitive effort and memory required
for processing and interpretation. This hy-
pothesis is supported by a novel kind of
computational experiments that reconstruct
and compare attested variations of the Ger-
man definite article paradigm. The exper-
iments show how the intricate interaction
between those variations and the rest of the
German ?linguistic landscape? may direct
language change.
1 Introduction
In his 1880 essay, Mark Twain famously com-
plained that The awful German Language is the
most ?slipshod and systemless, and so slippery
and elusive to grasp? language of all. A brief
look at the literature on the German case system
seems to provide sufficient evidence for instantly
agreeing with the American author. But what if
the German case system were not the accidental
by-product of diachronic changes as is often as-
sumed? Are there linguistic forces that are not yet
fully appreciated in the field, but which may ex-
plain the German case paradigm?
This paper demonstrates that there indeed are
such forces through a case study on German def-
inite articles. The experiments ?reconstruct? deep
language processing models for different variants
of this paradigm, and show how the ?linguistic
landscape? of German has allowed its speakers to
reduce their definite article system without loss in
efficiency for processing and interpretation.
2 The Problem of German Case
German articles, adjectives and nouns are marked
for gender, number and case through morpholog-
ical inflection, as illustrated for definite articles in
Table 1.
Case SG-M SG-F SG-N PL
NOM der die das die
ACC den die das die
DAT dem der dem den
GEN des der des der
Table 1: German definite articles.
The system is notorious for its syncretism (i.e.
the same form can be mapped onto different func-
tions), a riddle that has fascinated many formal
and historical linguists looking for explanations.
2.1 Historical Linguistics
Studies in historical linguistics and grammatical-
ization often propose the following three forces to
explain syncretism (Heine and Kuteva, 2005, p.
148):
1. The formal distinction between case markers
is lost through phonological changes.
2. One case takes over the functional domain of
another case and replaces it.
3. A case marker disappears and its functions
are usurped by another marker.
Syncretism is thus considered as the accidental
by-product of such forces, and German case syn-
cretism is typically analyzed according to these
lines (Bar?dal, 2009; Baerman, 2009, p. 229).
However, these forces are not explanatory: they
only describe what has happened, but not why.
829
Another problem for the ?syncretism by acci-
dent? hypothesis is the fact that the collapsing of
case forms is not randomly distributed over the
whole paradigm as would be expected. Hawkins
(2004, p. 78) observes that instead there is a sys-
tematic tendency for ?lower? cells in the paradigm
(e.g. genitive; Table 1) to collapse before cells in
?higher? positions (e.g. nominative) do so.
2.2 Formal Linguistics
Many hidden effects of verbal linguistic theo-
ries can be uncovered through explicit formaliza-
tions. Unfortunately, formal linguists also typi-
cally distinguish between ?systematic? and ?non-
systematic? syncretism when analyzing German
case. For instance, in his review of a number of
studies on German (a.o. Bierwisch, 1967; Blevins,
1995; Wiese, 1996; Wunderlich, 1997), M?ller
(2002) concludes that none of these approaches
is able to rule out accidental syncretism.
There is however one major stone that has been
left unturned by formal linguists: processing.
Most formal theories, such as HPSG (Ginzburg
and Sag, 2000), assume a strict division between
?competence? and ?performance? and therefore
represent linguistic knowledge in a purely declar-
ative, process-independent way (Sag and Wasow,
2011). While such an approach may be desirable
from a ?mathematical? point of view, it puts the
burden of efficient processing on the shoulders
of computational linguists, who have to develop
more intelligent interpreters.
One example of the gap between description
and computational implementation is disjunctive
feature representation, which became popular in
feature-based grammar formalisms in the 1980s
(Karttunen, 1984). Disjunctions allow an elegant
notation for multiple feature values, as illustrated
in example 1 for the German definite article die,
which is either assigned nominative or accusative
case, and which is either feminine-singular or plu-
ral. The feature structure (adopted from Kart-
tunen, 1984, p. 30) represents disjunctions by en-
closing the alternatives in curly brackets ({ }).
(1)
?
?
?
?
?
?
?
?
?
AGREEMENT
?
????
????
[
GENDER f
NUM sg
]
[
NUM pl
]
?
????
????
CASE
{
nom acc
}
?
?
?
?
?
?
?
?
?
However, it is a well-established fact that dis-
junctions are computationally expensive, which
is illustrated in the top of Figure 1. This Fig-
ure shows the search tree of a small grammar
when parsing the utterance Die Kinder gaben der
Lehrerin die Zeichnung (?the children gave the
drawing to the (female) teacher?), which is un-
ambiguous to German speakers. As can be seen
in the Figure, the search tree has to explore sev-
eral branches before arriving at a valid solution.
Most of the splits are caused by disjunctions. For
example, when a determiner-noun construction
specifies that the case features of the definite ar-
ticle die (nominative or accusative) and the noun
Kinder (?children?; nominative, accusative or gen-
itive) have to unify, the search tree splits into two
hypotheses (a nominative and an accusative read-
ing) even though for native speakers of German,
the syntactic context unambiguously points to a
nominative reading (because it is the only noun
phrase that agrees with the main verb).
It should be no surprise, then, that a lot of work
has focused on processing disjunctions more ef-
ficiently (e.g. Carter, 1990; Ramsay, 1990). As
observed by Flickinger (2000), however, most of
these studies implicitly assume that the grammar
representation has to remain unchanged. He then
demonstrates through computational experiments
how a different representation can directly impact
efficiency, and argues that revisions of the gram-
mar for efficiency should be discussed more thor-
oughly in the literature.
The impact of representation on processing is
illustrated at the bottom of Figure 1, which shows
the performance of a grammar that uses the same
processing technique for handling the same utter-
ance, but a different representation than the dis-
junctive grammar. As can be seen, the alternative
grammar (whose technical details are disclosed
further below) is able to parse the German defi-
nite articles without tears, and the resulting search
tree arguably better reflects the actual processing
performed by native speakers of German.
2.3 Alternative Hypothesis
The effect of processing-friendly representations
on search suggests that answers for the unsolved
problems concerning case syncretism have to
be sought in performance. This paper there-
fore rejects the processing-independent approach
and explores the alternative hypothesis, following
830
(a) Search with disjunctive feature representation:
top
initial
structure top
application
process
queue
reset
sem syn
initial
* der-lex
(lex), die-
lex (lex),
die-lex
(lex),
gaben-lex
(lex),
zeichnung-
lex (lex)
determiner-
nominal-phrase-
cxn
(marked-phrasal)
lehrerin-
lex (lex)
determiner-nominal-
phrase-cxn
(marked-phrasal)
kinder-
lex
(lex)
determiner-nominal-
phrase-cxn
(marked-phrasal)
determiner-nominal-
phrase-cxn
(marked-phrasal)
determiner-
nominal-phrase-
cxn
(marked-phrasal)
kinder-
lex
(lex)
determiner-
nominal-phrase-
cxn
(marked-phrasal)
ditransitive-
cxn (arg)
determiner-nominal-phrase-cxn
(marked-phrasal)
+
determiner-
nominal-phrase-
cxn
(marked-phrasal)
lehrerin-
lex (lex)
determiner-nominal-
phrase-cxn
(marked-phrasal)
kinder-
lex
(lex)
determiner-nominal-
phrase-cxn
(marked-phrasal)
determiner-nominal-
phrase-cxn
(marked-phrasal)
determiner-
nominal-phrase-
cxn
(marked-phrasal)
kinder-
lex
(lex)
determiner-nominal-phrase-cxn
(marked-phrasal)
determiner-
nominal-phrase-
cxn
(marked-phrasal)
ditransitive-
cxn (arg)
determiner-nominal-phrase-cxn (marked-phrasal) kinder-lex (lex) lehrerin-lex (lex) zeichnung-lex (lex)(b) Search with feature matrices:
top
top
Parsing "die Kinder gaben der Lehrerin die Z ichnung ."
Applying construction set (8)  in direction 
Found a solution
initial
structure top
application
process
queue
applied
constructions
... and 1 more
resulting
structure
top
Meaning:
((teacher.f ?recipient-1) (unique-referent ?recipient-1) (drawing ?sem-role-3)  
(unique-referent ?sem-role-3) (children ?ref-2) (unique-referent ?ref-2)  
(gave ?ev-1 ?ref-2 ?sem-role-3 ?recipient-1))
reset
sem syn
initial * zeichnung-lex,  kinder-lex,  lehrerin-lex,  gaben-lex,  die-lex,  detnp-cxn,die-lex ,  detnp-cxn,  der-lex,  detnp-cxn
ditransitive-
cxn
detnp-cxn der-lex (t) die-lex (t) die-lex (t)
ditransitive-cxn detnp-cxn der-lex (t) detnp-cxn die-lex (t) detnp-cxn die-lex (t) gaben-lex (t)
lehrerin-lex (t) kinder-lex (t)
ditransitive-
unit-1
detnp-
unit-1
kinder-
1
die-1
detnp-
unit-2
zeichnung-
1
die-2
gaben-1
detnp-
unit-3
lehrerin-
1
der-1
sem syn ditransitive-
unit-1
detnp-
unit-3
der-1
lehrerin-
1
detnp-
unit-2
die-2
zeichnung-
1
detnp-
unit-1
die-1
kinder-
1
gaben-1
Figure 1: The representation of linguistic information has a direct impact on processing efficiency. The top
figure shows a search tree when parsing the unambiguous utterance Die Kinder gaben der Lehrerin die Zeich-
nung (?The children gave the drawing to the (female) teacher?) using disjunctive feature representation. The
bottom figure shows the search tree using distinctive feature matrices. Labels in the boxes show the names
of the applied constructions; boxes with a bold border are successful end nodes. Both grammars have been
implemented in Fluid Construction Grammar (FCG; Steels, 2011, 2012a) and are processed using a standard
depth-first search algorithm (Bleys et al 2011) and general unification (without optimization for particular
types or data structures; Steels and De Beule, 2006; De Beule, 2012). The utterance is assumed to be seg-
mented into words. Interested readers can explore the Figure through an interactive web demonstration at
http://www.fcg-net.org/demos/design-patterns/07-feature-matrices/.
Steels (2004, 2012b), that grammar evolves in or-
der to optimize communicative success by damp-
ening the search space in linguistic processing and
reducing the cognitive effort needed for interpre-
tation, while at the same time minimizing the re-
sources required for doing so. More specifically,
this paper explores the following claims:
1. The G rman definite article system can be
processed as efficiently as its Old High Ger-
man predecessor, which had less syncretism.
2. The presence of other grammatical structures
have made it possible to reduce the definite
article paradigm without increasing the cog-
nitive effort needed for disambiguating the
argument structures that underly German ut-
terances.
3. The decrease of cue-reliability of case for
disambiguation encourages the emergence of
competing systems (such as word order).
The hypothesis is substantiated through com-
putational experiments that reconstruct three dif-
ferent variants of the German definite article sys-
tem (the current system, its Old High German pre-
decessor, Wright, 1906; and the Texas German
dialect system, Boas, 2009a,b) and compare their
performance in terms of processing efficiency and
cognitive effort in interpretation.
3 Operationalizing German Case
An adequate operationalization of German case
requires a bidirectional grammar (for parsing and
production) and easy access to linguistic process-
831
ing data. All experiments reported in this paper
have therefore been implemented in Fluid Con-
struction Grammar (FCG; Steels, 2011, 2012a), a
unification-based grammar formalism that comes
equipped with an interactive web interface and
monitoring tools (Loetzsch, 2012). A second ad-
vantage of FCG is that it features strong bidirec-
tionality: the FCG-interpreter can achieve both
parsing and production using the same linguistic
inventory. Other feature structure platforms, such
as the lkb-system (Copestake, 2002), require a
separate parser and generator for formalizing bidi-
rectional grammars, which make them less suited
for substantiating the claims of this paper.
3.1 Distinctive Feature Matrix
German case has become the litmus test for
demonstrating how well a feature-based grammar
formalism copes with multifunctionality, espe-
cially since Ingria (1990) provocatively stated that
unification is not the best technique for handling
it. People have gone to great lengths to counter
Ingria?s claim, especially within the HPSG frame-
work (e.g. M?ller, 1999; Daniels, 2001; Sag,
2003), and various formalizations have been of-
fered for German case (Heinz and Matiasek,
1994; M?ller, 2001; Crysmann, 2005). However,
these proposals either do not succeed in avoiding
inefficient disjunctions or they require a complex
double type hierarchy (Crysmann, 2005).
The experiments in this paper use a more
straightforward solution, called a distinctive fea-
ture matrix, which is based on an idea that was
first explored by Ingria (1990) and of which a
variation has recently also been proposed for
Lexical Functional Grammar (Dalrymple et al
2009). Instead of treating case as a single-valued
feature, it can be represented as an array of fea-
tures, as shown for the definite article die (ignor-
ing the genitive case for the time being):
(2) die:
?
?
?
?CASE
?
?
?
nom ?nom
acc ?acc
dat ?
?
?
?
?
?
?
?
The case feature includes a paradigm of three
cases (nom, acc and dat), whose values can ei-
ther be ?+? or ???, or left unspecified through a
variable (indicated by a question mark). The two
variables ?nom and ?acc indicate that die can
potentially be assigned nominative or accusative
case, the value ??? for dative means that die can-
not be assigned dative case. We can do the same
for Kinder (?children?), which can be nominative
or accusative, but not dative:
(3) Kinder:
?
?
?
?CASE
?
?
?
nom ?nom
acc ?acc
dat ?
?
?
?
?
?
?
?
As demonstrated in Figure 1, disjunctive fea-
ture representation would cause a split in the
search tree when unifying die and Kinder. Us-
ing a feature matrix, however, the choice between
a nominative and accusative reading can simply
be postponed until enough information from the
rest of the utterance is available. Unifying die and
Kinder yields the following feature structure:
(4) die Kinder:
?
?
?
?CASE
?
?
?
nom ?nom
acc ?acc
dat ?
?
?
?
?
?
?
?
3.2 A Three-Dimensional Matrix
The German case paradigm is obviously more
complex than the examples shown so far. Let?s
consider Table 1 again, but this time we replace
every cell in the table by a variable. This leads to
the following feature matrix for the German defi-
nite articles:
Case SG-M SG-F SG-N PL
?NOM ?n-s-m ?n-s-f ?n-s-n ?n-pl
?ACC ?a-s-m ?a-s-f ?a-s-n ?a-pl
?DAT ?d-s-m ?d-s-f ?d-s-n ?d-pl
?GEN ?g-s-m ?g-s-f ?g-s-n ?g-pl
Table 2: A distinctive feature matrix for German case.
Each cell in this matrix represents a specific
feature bundle that collects the features case,
number, and person. For example, the variable
?n-s-m stands for nominative singular mascu-
line. Note that also the cases themselves have
their own variable (?nom, ?acc, ?dat and
?gen). This allows us to single out a specific di-
mension of the matrix for constructions that only
care about case distinctions, but abstract away
from gender or number. Each linguistic item fills
in as much information as possible in this case
matrix. For example, Table 3 shows how the def-
inite article die underspecifies its potential values
and rules out all other options through ???.
832
Case SG-M SG-F SG-N PL
?NOM ? ?n-s-f ? ?n-pl
?ACC ? ?a-s-f ? ?a-pl
? ? ? ? ?
? ? ? ? ?
Table 3: The feature matrix of die.
The feature matrix of Kinder (?children?),
which underspecifies for nominative, accusative
and genitive, is shown in Table 4. Notice, how-
ever, that the same variable names are used for
both the column that singles out the case dimen-
sion as for the column of the plural feature bun-
dles.
Case SG-M SG-F SG-N PL
?n-pl ? ? ? ?n-pl
?a-pl ? ? ? ?a-pl
? ? ? ? ?
?g-pl ? ? ? ?g-pl
Table 4: The feature matrix of Kinder (?children?).
Unification of die and Kinder can exploit these
variable ?equalities? for ruling out a singular value
of the definite article. Likewise, the matrix of die
rules out the genitive reading of Kinder, as illus-
trated in Table 5.
Case SG-M SG-F SG-N PL
?n-pl ? ? ? ?n-pl
?a-pl ? ? ? ?a-pl
? ? ? ? ?
? ? ? ? ?
Table 5: The feature matrix of die Kinder.
Argument structure constructions (Goldberg,
2006), such as the ditransitive, can then later as-
sign either nominative or accusative case. The
main advantage of feature matrices is that linguis-
tic search only has to commit to specific feature-
values once sufficient information is available, so
the search tree only splits when there is an actual
ambiguity. Moreover, they can be handled using
standard unification. Interested readers can con-
sult van Trijp (2011) for a thorough description of
the approach, as well as a discussion on how the
FCG implementation differs from Ingria (1990)
and Dalrymple et al(2009).
4 Experiments
This section describes the experimental set-up and
discusses the experimental results.
4.1 Three Paradigms
The experiments compare three different variants
of the German definite article paradigm.
Standard German. The Standard German
paradigm has been illustrated in Table 1 and its
operationalization has been shown in section 3.2.
The paradigm has been inherited without signifi-
cant changes from Middle High German (1050-
1350; Walshe, 1974) and features six different
forms.
Old High German. The Old High German
paradigm is the direct predecessor of the current
paradigm of definite articles. It contained at least
twelve distinct forms (depending on which varia-
tion is taken) that included gender distinctions in
plural (Wright, 1906, p. 67). It also included one
definite article that marked the now extinct instru-
mental case, which is ignored in this paper. The
variant of the Old High German paradigm that has
been implemented in the experiments is summa-
rized in Table 6.
Case Singular
M F N
NOM d?r diu daz?
ACC d?n die daz?
DAT d?mu d?ru d?mu
GEN d?s d?ra d?s
Plural
M F N
NOM die deo diu
ACC die deo diu
DAT de?m de?m de?m
GEN d?ro d?ro d?ro
Table 6: The Old High German definite article system.
Texas German. The third variant is an
American-German dialect called Texas German
(Boas, 2009a,b), which evolved a two-way case
distinction between nominative and oblique. This
type of case system, in which the accusative and
dative case have collapsed, is also a common
evolution in the Low German dialects (Shrier,
1965). The implemented paradigm of Texas
German is shown in Table 7.
833
Case SG-M SG-F SG-N PL
NOM der die das die
ACC/DAT den die den die
Table 7: The Texas German definite article system.
4.2 Production and Parsing Tasks
Each grammar is tested as to how efficiently it can
produce and parse utterances in terms of cognitive
effort and search (see section 4.3). There are three
basic types of utterances:
1. Ditransitive: NOM ? Verb ? DAT ? ACC
2. Transitive (a): NOM ? Verb ? ACC
3. Transitive (b): NOM ? Verb ? DAT
The argument roles are filled by noun phrases
whose head nouns always have a distinct form
for singular and plural (e.g. Mann vs. M?n-
ner; ?man? vs. ?men?), but that are unmarked for
case. The combinations of arguments is always
unique along the dimensions of number and gen-
der, which yields 216 unique utterance types for
the ditransitive as follows:
(5)
NOM.S.M V DAT.S.M ACC.S.M
NOM.S.M V DAT.S.F ACC.S.M
NOM.S.M V DAT.S.N ACC.S.M
NOM.S.M V DAT.PL.M ACC.S.M
etc.
In transitive utterances, there is an additional
distinction based on animacy for noun phrases in
the Object position of the utterance, which yields
72 types in the NOM-ACC configuration and 72
in the NOM-DAT configuration. Together, there
are 360 unique utterance types. As can be gleaned
from the utterance types, the genitive case is not
considered by the experiments, as the genitive is
not part of basic German argument structures and
it has almost disappeared in most dialects of Ger-
man (Shrier, 1965).
In production, the grammar is presented with a
meaning that needs to be verbalized into an utter-
ance. In parsing, the produced utterance has to be
analyzed back into a meaning. Every utterance is
processed using a full search, that is, all branches
and solutions are calculated.
The experiments exploit types because there
are three different language systems, hence it is
impossible to use a single, real corpus and its to-
ken frequencies. It would also be unwarranted to
use different corpora because corpus-specific bi-
ases would distort the comparative results. Sec-
ondly, as the experiments involve models of deep
language processing (as opposed to stochastic
models), the use of types instead of tokens is
justified in this phase of the research: the first
concern of precision-grammars is descriptive ade-
quacy, for which types are a more reliable source.
Obviously, the effect of token frequency needs to
be examined in future research.
4.3 Measuring Cognitive Effort
The experiments measure two kinds of cognitive
effort: syntactic search and semantic ambiguity.
Search. The search measure counts the number
of branches in the search process that reach an end
node, which can either be a possible solution or
a dead end (i.e. no constructions can be applied
anymore). Duplicate nodes (for instance, nodes
that use the same rules but in a different order)
are not counted. The search measure is then used
as a ?sanity check? to verify whether the three dif-
ferent paradigms can be processed with the same
efficiency in terms of search tree length, as hy-
pothesized by this paper. More specifically, the
following conditions have to be met:
1. In production, there should only be one
branch.
2. In parsing, search has to be equal to the se-
mantic effort.
The single branch constraint in production
checks whether the definite articles are suffi-
ciently distinct from one another. Since there is no
ambiguity about which argument plays which role
in the utterance, the grammar should only come
up with one solution. In parsing, the number of
branches has to correspond to ?real? semantic am-
biguities and not create additional search, as ar-
gued in section 2.2.
Semantic Ambiguity. Semantic ambiguity
equals the number of possible interpretations
of an utterance. For instance, the utterance
Der Hund bei?t den Mann ?the dog bites the
man? is unambiguous in Modern High German,
834
since der Hund can only be nominative singular-
masculine, and den Mann can only be accusative
masculine-singular. There is thus only one pos-
sible interpretation in which the dog is the biter
and the man is being bitten, illustrated as follows
using a logic-based meaning representation (also
see Steels, 2004, for this operationalization of
cognitive effort):
(6) Interpretation 1:
Der Hund den Mann.bei?t
bite(?ev)
biter(?ev, ?x)
bitten(?ev, ?y)
dog(?a) man(?b)
?a=?x
?b=?y
However, an utterance such as die Katze bei?t
die Frau ?the cat bites the woman? is ambiguous
because die has both a nominative and accusative
singular-feminine reading:
(7) a. Interpretation 1:
Die Katze die Frau.bei?t
bite(?ev)
biter(?ev, ?x)
bitten(?ev, ?y)
cat(?a) woman(?b)
?a=?x
?b=?y
b. Interpretation 2:
Die Katze die Frau.bei?t
bite(?ev)
biter(?ev, ?x)
bitten(?ev, ?y)
cat(?a) woman(?b)
?a=?y
?b=?x
Here, German speakers are likely to use word
order, intonation and world knowledge (i.e. cats
are more likely to bite a person than the other way
round) for disambiguating the utterance.
4.4 Experimental Parameters
The experiments (E1-E4) concern the cue-
reliability of the definite articles for disambiguat-
ing event structure. In all experiments, the differ-
ent grammars can exploit the case-number-gender
information of definite articles, and also the gen-
der and number specifications of nouns, and the
syntactic valence of verbs. For instance, the
noun form Frauen ?women? is specified as plural-
feminine, and verbs like helfen ?to help? are spec-
ified to take a dative object, whereas verbs like
finden ?to find? take an accusative object. In other
experiments, different combinations of grammat-
ical cues become available or not:
Cue E1 E2 E3 E4
SV-agreement + +
Selection restrictions + +
SV-agreement restricts the subject to singular
or plural nouns, and semantic selection restric-
tions can disambiguate utterances in which for ex-
ample the Agent-role has to be animate (e.g. in
perception verbs such as sehen ?to see?). All other
possible cues, such as word order, are ignored.
5 Results
5.1 Search
In all experiments, the constraints of the search
measure were satisfied: every grammar only re-
quired one branch per utterance in production,
and the number of branches in parsing never ex-
ceeded the number of possible interpretations. In
terms of search length, more syncretism therefore
does not automatically harm efficiency, provided
that the grammar uses an adequate representation.
Arguably, the smaller paradigms are even more
efficient because they require less unifications to
be performed.
5.2 Semantic Ambiguity
Now that it has been ascertained that more
syncretism does not harm processing efficiency,
we can compare cue-reliability of the different
paradigms for semantic interpretation.
Ambiguous Utterances. Figure 2 shows the
number of ambiguous utterances in parsing (in %)
per paradigm and per set-up. As can be seen,
the Old High German paradigm (black) is the
most reliable cue in Experiment 1 (E1; when SV-
agreement and selection restrictions are ignored)
with 35.56% of ambiguous utterances, as opposed
to 55.56% for Modern High German (grey) and
77.78% for Texas German (white).
When SV-agreement is taken into account (E2),
the difference between Old and Modern High
German becomes smaller, with both paradigms
offering a reliability of more than 70%, while
Texas German still faces more than 70% of am-
biguous utterances.
Ambiguity is even more reduced when using
semantic selection restrictions of the verb (set-up
835
E3). Here, the difference between Old and Mod-
ern High German becomes trivial with 4.44% and
6.94% of ambiguous utterances respectively. The
difference with Texas German remains apparent,
even though its ambiguity is cut by half.
In set-up E4 (case, SV-agreement and selection
restrictions), the Old and Modern High German
paradigms resolve almost all ambiguities, leaving
little difference between them. Using the Texas
German dialect, one utterance out of five remains
ambiguous and requires additional grammatical
cues or inferencing for semantic interpretation.
Number of possible interpretations. Semantic
ambiguity can also be measured by counting the
number of possible interpretations per utterance.
A non-ambiguous language would thus have 1
possible interpretation per utterance. The aver-
age number of interpretations per utterance (per
paradigm and per set-up) is shown in Table 8.
Paradigm E1 E2 E3 E4
Old High German 1.56 1.22 1.04 1.03
Modern High German 1.56 1.28 1.07 1.04
Texas German 2.84 2.39 1.36 1.22
Table 8: Average number of interpretations per utter-
ance type.
The Old High German paradigm has the least
semantic ambiguity throughout, except in Exper-
iment 1 (E1). Here, Modern High German has
the same average effort despite having more am-
biguous utterances. This means that the Old High
German paradigm provides a better coverage in
terms of construction types, but when ambiguity
occurs, more possible interpretations exist.
6 Discussion
The experiments compare how well three differ-
ent paradigms of definite articles perform if they
are inserted in the grammar of Modern High Ger-
man. The results show that, in isolation, Old High
German offers the best cue-reliability for retriev-
ing who?s doing what to whom in events. How-
ever, when other grammatical cues are taken into
account, it turns out that Modern High German
achieves similar results with respect to syntactic
search and semantic ambiguity, with a reduced
paradigm (using only six instead of twelve forms).
As for the Texas German dialect, which has
collapsed the accusative-dative distinction, the
amount of ambiguity remains more than 20% us-
ing all available cues. One verifiable predic-
tion of the experiments is therefore that this di-
alect should show an increase in alternative syn-
tactic restrictions (such as word order) in order
to make up for the lost case distinctions. Inter-
estingly, such alternatives have been attested in
Low German dialects that have evolved a simi-
lar two-way case system (Shrier, 1965). Modern
High German, on the other hand, has already re-
cruited word order for other purposes (such as in-
formation structure; Lenerz, 1977; Micelli, 2012),
which may explain why the current paradigm has
been able to survive since the Middle Ages.
Instead of an accidental by-product of phono-
logical and morphological changes, then, a new
picture emerges for explaining syncretism in
Modern High German definite articles: German
speakers have been able to reduce their case
paradigm without loss in processing and interpre-
tation efficiency. With cognitive effort as a selec-
tion criterion, subsequent generations of speakers
found no linguistic pressures for maintaining par-
ticular distinctions such as gender in plural arti-
cles. Especially forms whose acoustic distinctions
are harder to perceive are candidates for collapse
if they are no longer functional for processing or
interpretation. Other factors, such as frequency,
may accelerate this evolution, as also argued by
Bar?dal (2009). For instance, there may be less
benefits for upholding a case distinction for infre-
quent than for frequent forms.
If case syncretism is not randomly distributed
over a grammatical paradigm, but rather func-
tionally motivated, a new explanatory model is
needed. One candidate is evolutionary linguistics
(Steels, 2012b), a framework of cultural evolu-
tion in which populations of language users con-
stantly shape and reshape their language in re-
sponse to their communicative needs. The ex-
periments reported here suggest that this dynamic
shaping process is guided by the ?linguistic land-
scape? of a language. For instance, the pres-
ence of grammatical cues such as gender, num-
ber and SV-agreement may encourage paradigm
reduction. However, reduction may be the start
of a self-enforcing loop in which the decreasing
cue-reliability of a paradigm may pressure lan-
guage users into enforcing the alternatives to take
on even more of the cognitive load of processing.
The intricate interactions between grammati-
836
35.56	 ?
22.22	 ?
4.44	 ? 2.78	 ?
55.56	 ?
28.89	 ?
6.94	 ? 3.61	 ?
77.78	 ?
71.11	 ?
35.56	 ?
22.22	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
90	 ?
100	 ?
E1	 ? E2	 ? E3	 ? E4	 ?
%	 ?of	 ?ambiguous	 ?u?erances	 ?
Old	 ?High	 ?German	 ? Modern	 ?High	 ?German	 ? Texas	 ?German	 ?
Figure 2: This chart shows the number of ambiguous utterances per paradigm per E(xperimental set-up) in %.
cal systems also requires more sophisticated mea-
sures. A promising extension of this paper could
lie in an information-theoretic approach to lan-
guage (Hale, 2003; Jaeger and Tily, 2011), which
has recently explored a set of tools for assessing
linguistic complexity, processing effort and un-
certainty. Unfortunately, only little work has been
done on morphological paradigms so far (see e.g.
Ackerman et al 2011), and the approach is typi-
cally applied in stochastic or Probabilistic Context
Free Grammars, hence it remains unclear how the
assumptions of this field fit into models of deep
language processing.
7 Conclusions
More than 130 years after Mark Twain?s com-
plaints, it seems that the German language is not
that awful after all. Through a series of compu-
tational experiments, this paper has proposed a
different explanation for German case syncretism
that answers some of the unsolved riddles of pre-
vious studies. First, the experiments have shown
that an increase in syncretism does not necessar-
ily lead to an increase in the cognitive effort re-
quired for syntactic search, provided that the rep-
resentation of the grammar is processing-friendly.
Secondly, by comparing cue-reliability of differ-
ent paradigms for semantic disambiguation, the
experiments have demonstrated that Modern High
German achieves a similar performance as its Old
High German predecessor using only half of the
forms in its definite article paradigm.
Instead of a series of historical accidents, the
German case system thus underwent a systematic
and ?performance-driven [...] morphological re-
structuring? (Hawkins, 2004, p. 79), in which lin-
guistic pressures such as cognitive effort decided
on the maintenance or loss of certain distinctions.
The case study makes clear that formal and com-
putational models of deep language understand-
ing have to reconsider their strict division between
competence and performance if the goal is to ex-
plain individual language development. This pa-
per proposed that new tools and methodologies
should be sought in evolutionary linguistics.
Acknowledgements
This research has been conducted at the Sony
Computer Science Laboratory Paris. I would like
to thank Luc Steels, director of Sony CSL Paris
and the VUB AI-Lab of the University of Brus-
sels, for his support and feedback. I also thank
Hans Boas, J?hanna Bar?dal, Peter Hanappe,
Manfred Hild and the anonymous reviewers for
helping to improve this article. All errors remain
of course my own.
837
References
Farrell Ackerman, James P. Blevins, and Robert
Malouf. Parts and wholes: Implicative patterns
in inflectional paradigms. In J.P. Blevins and
J. Blevins, editors, Analogy in Grammar: Form
and Acquisition, pages 54?81. Oxford Univer-
sity Press, Oxford, 2011.
Matthew Baerman. Case syncretism. In An-
drej Malchukov and Andrew Spencer, editors,
The Oxford Handbook of Case, chapter 14,
pages 219?230. Oxford University Press, Ox-
ford, 2009.
J. Bar?dal. The development of case in germanic.
In J. Bar?dal and S. Chelliah, editors, The Role
of Semantics and Pragmatics in the Develop-
ment of Case, pages 123?159. John Benjamins,
Amsterdam, 2009.
Manfred Bierwisch. Syntactic features in
morphology: General problems of so-called
pronominal inflection in German. In To Hon-
our Roman Jakobson, pages 239?270. Mouton
De Gruyter, Berlin, 1967.
James Blevins. Syncretism and paradigmatic op-
position. Linguistics and Philosophy, 18:113?
152, 1995.
Joris Bleys, Kevin Stadler, and Joachim De Beule.
Search in linguistic processing. In Luc Steels,
editor, Design Patterns in Fluid Construction
Grammar. John Benjamins, Amsterdam, 2011.
Hans C. Boas. Case loss in Texas German: The
influence of semantic and pragmatic factors. In
J. Bar?dal and S. Chelliah, editors, The Role of
Semantics and Pragmatics in the Development
of Case, pages 347?373. John Benjamins, Am-
sterdam, 2009a.
Hans C. Boas. The Life and Death of Texas
German, volume 93 of Publication of the The
American Dialect Society. Duke University
Press, Durham, 2009b.
David Carter. Efficient disjunctive unification
for bottom-up parsing. In Proceedings of the
13th Conference on Computational Linguistics,
pages 70?75. ACL, 1990.
Ann Copestake. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stan-
ford, 2002.
Berthold Crysmann. Syncretism in german: A
unified approach to underspecification, indeter-
minacy, and likeness of case. In Stefan M?ller,
editor, Proceedings of the 12th International
Conference on Head-Driven Phrase Structure
Grammar, pages 91?107, Stanford, 2005. CSLI
Publications.
Mary Dalrymple, Tracy Holloway King, and
Louisa Sadler. Indeterminacy by underspecifi-
cation. Journal of Linguistics, 45:31?68, 2009.
Michael Daniels. On a type-based analysis of fea-
ture neutrality and the coordination of unlikes.
In Proceedings of the 8th International Confer-
ence on HPSG, pages 137?147, Stanford, 2001.
CSLI.
Joachim De Beule. A formal deconstruction of
Fluid Construction Grammar. In Luc Steels, ed-
itor, Computational Issues in Fluid Construc-
tion Grammar. Springer Verlag, Berlin, 2012.
Daniel P. Flickinger. On building a more efficient
grammar by exploiting types. Natural Lan-
guage Engineering, 6(1):15?28, 2000.
Jonathan Ginzburg and Ivan A. Sag. Interroga-
tive Investigations: the Form, the Meaning, and
Use of English Interrogatives. CSLI Publica-
tions, Stanford, 2000.
Adele E. Goldberg. Constructions At Work: The
Nature of Generalization in Language. Oxford
University Press, Oxford, 2006.
John T. Hale. The information conveyed by words
in sentences. Journal of Psycholinguistic Re-
search, 32(2):101?123, 2003.
John A. Hawkins. Efficiency and Complexity in
Grammars. Oxford University Press, Oxford,
2004.
Bernd Heine and Tania Kuteva. Language Con-
tact and Grammatical Change. Cambridge
University Press, Cambridge, 2005.
Wolfgang Heinz and Johannes Matiasek. Argu-
ment structure and case assignment in german.
In John Nerbonne, Klaus Netter, and Carl Pol-
lard, editors, German in Head-Driven Phrase
Structure Grammar, volume 46 of CSLI Lec-
ture Notes, pages 199?236. CSLI Publications,
Stanford, 1994.
R.J.P. Ingria. The limits of unification. In Pro-
ceedings of the 28th Annual Meeting of the
ACL, pages 194?204, 1990.
T. Florian Jaeger and Harry Tily. On language
?utility?: Processing complexity and commu-
838
nicative efficiency. WIREs: Cognitive Science,
2(3):323?335, 2011.
L. Karttunen. Features and values. In Proceedings
of the 10th International Conference on Com-
putational Linguistics, Stanford, 1984.
J?rgen Lenerz. Zur Abfolge nominaler
Satzglieder im Deutschen. Narr, T?bin-
gen, 1977.
Martin Loetzsch. Tools for grammar engineering.
In Luc Steels, editor, Computational Issues in
Fluid Construction Grammar. Springer Verlag,
Berlin, 2012.
Vanessa Micelli. Field topology and information
structure: A case study for German constituent
order. In Luc Steels, editor, Computational Is-
sues in Fluid Construction Grammar. Springer
Verlag, Berlin, 2012.
Gereon M?ller. Remarks on nominal inflection
in German. In Ingrid Kaufmann and Bar-
bara Stiebels, editors, More than Words: A
Festschrift for Dieter Wunderlich, pages 113?
145. Akademie Verlag, Berlin, 2002.
Stefan M?ller. An HPSG-analysis for free rela-
tive clauses in german. Grammars, 2(1):53?
105, 1999.
Stefan M?ller. Case in German ? towards and
HPSG analysis. In Tibor Kiss and Det-
mar Meurers, editors, Constraint-Based Ap-
proaches to Germanic Syntax. CSLI, Stanford,
2001.
Allan Ramsay. Disjunction without tears. Com-
putational Linguistics, 16(3):171?174, 1990.
Ivan A. Sag. Coordination and underspecifica-
tion. In Jongbok Kom and Stephen Wechsler,
editors, Proceedings of the Ninth International
Conference on HPSG, Stanford, 2003. CSLI.
Ivan A. Sag and Thomas Wasow. Performance-
compatible competence grammar. In Robert D.
Borsley and Kersti B?rjars, editors, Non-
Transformational Syntax: Formal and Explicit
Models of Grammar. Wiley-Blackwell, Ox-
ford, 2011.
Martha Shrier. Case systems in German dialects.
Language, 41(3):420?438, 1965.
Luc Steels. Constructivist development of
grounded construction grammars. In Walter
Daelemans, editor, Proceedings 42nd Annual
Meeting of the Association for Computational
Linguistics, pages 9?19, Barcelona, 2004.
Luc Steels, editor. Design Patterns in Fluid Con-
struction Grammar. John Benjamins, Amster-
dam, 2011.
Luc Steels, editor. Computational Issues in
Fluid Construction Grammar. Springer, Berlin,
2012a.
Luc Steels. Self-organization and selection in cul-
tural language evolution. In Luc Steels, editor,
Experiments in Cultural Language Evolution.
John Benjamins, Amsterdam, 2012b.
Luc Steels and Joachim De Beule. Unify and
merge in Fluid Construction Grammar. In
P. Vogt, Y. Sugita, E. Tuci, and C. Nehaniv,
editors, Symbol Grounding and Beyond., LNAI
4211, pages 197?223, Berlin, 2006. Springer.
Remi van Trijp. Feature matrices and agreement:
A case study for German case. In Luc Steels,
editor, Design Patterns in Fluid Construction
Grammar. John Benjamins, Amsterdam, 2011.
M. Walshe. A Middle High German Reader: With
Grammar, Notes and Glossary. Oxford Univer-
sity Press, Oxford, 1974.
Bernd Wiese. Iconicity and syncretism. on
pronominal inflection in Modern German. In
Robin Sckmann, editor, Theoretical Linguistics
and Grammatical Description, pages 323?344.
John Benjamins, Amsterdam, 1996.
Joseph Wright. An Old High German Primer.
Clarendon Press, Oxford, 2nd edition, 1906.
Dieter Wunderlich. Der unterspezifizierte Artikel.
In Karl Heinz Ramers D?rscheid and Monika
Schwarz, editors, Sprache im Fokus, pages 47?
55. Niemeyer, T?bingen, 1997.
839
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 63?68,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Fluid Construction Grammar:
The New Kid on the Block
Remi van Trijp1, Luc Steels1,2, Katrien Beuls3, Pieter Wellens3
1Sony Computer Science 2ICREA Institute for 3 VUB AI Lab
Laboratory Paris Evolutionary Biology (UPF-CSIC) Pleinlaan 2
6 Rue Amyot PRBB, Dr Aiguidar 88 1050 Brussels (Belgium)
75005 Paris (France) 08003 Barcelona (Spain) katrien|pieter@
remi@csl.sony.fr steels@ai.vub.ac.be ai.vub.ac.be
Abstract
Cognitive linguistics has reached a stage
of maturity where many researchers are
looking for an explicit formal grounding
of their work. Unfortunately, most current
models of deep language processing incor-
porate assumptions from generative gram-
mar that are at odds with the cognitive
movement in linguistics. This demonstra-
tion shows how Fluid Construction Gram-
mar (FCG), a fully operational and bidi-
rectional unification-based grammar for-
malism, caters for this increasing demand.
FCG features many of the tools that were
pioneered in computational linguistics in
the 70s-90s, but combines them in an inno-
vative way. This demonstration highlights
the main differences between FCG and re-
lated formalisms.
1 Introduction
The ?cognitive linguistics enterprise? (Evans
et al 2007) is a rapidly expanding research dis-
cipline that has so far avoided rigorous formal-
izations. This choice was wholly justified in the
70s-90s when the foundations of this scientific
movement were laid (Rosch, 1975; Lakoff, 1987;
Langacker, 1987), and it remained so during the
past two decades while the enterprise worked on
getting its facts straight through empirical stud-
ies in various subfields such as language acqui-
sition (Tomasello, 2003; Goldberg et al 2004;
Lieven, 2009), language change and grammati-
calization (Heine et al 1991; Bar?dal and Chel-
liah, 2009), and corpus research (Boas, 2003; Ste-
fanowitsch and Gries, 2003). However, with nu-
merous textbooks on the market (Lee, 2001; Croft
and Cruse, 2004; Evans and Green, 2006), cogni-
tive linguistics has by now established itself as a
serious branch in the study of language, and many
cognitive linguists are looking for ways of explic-
itly formalizing their work through computational
models (McClelland, 2009).
Unfortunately, it turns out to be very difficult
to adequately formalize a cognitive linguistic ap-
proach to grammar (or ?construction grammar?)
using the tools for precision-grammars developed
in the 70s-90s such as unification (Kay, 1979;
Carpenter, 1992), because these tools are typi-
cally incorporated in a generative grammar (such
as HPSG; Ginzburg and Sag, 2000) whose as-
sumptions are incompatible with the foundations
of construction grammar. First, cognitive linguis-
tics blurs the distinction between ?competence?
and ?performance?, which means giving up the
sharp distinction between declarative and proce-
dural representations. Next, construction gram-
marians argue for a usage-based approach (Lan-
gacker, 2000), so the constraints on features may
change and features may emerge or disappear
from a grammar at any given time.
This demonstration introduces Fluid Construc-
tion Grammar (FCG; Steels, 2011, 2012a), a
novel unification-based grammar formalism that
addresses these issues, and which is available as
open-source software at www.fcg-net.org.
After more than a decade of development, FCG
is now ready to handle sophisticated linguistic
issues. FCG revisits many of the technologies
developed by computational linguists and intro-
duces several key innovations that are of inter-
est to anyone working on deep language process-
ing. The demonstration illustrates these innova-
tions through FCG?s interactive web interface.
63
semantic 
pole
syntactic 
pole
transient structure
semantic 
pole
syntactic 
pole
construction
matching phase
first 
merging 
phase
second 
merging 
phase
semantic 
pole
syntactic 
pole
transient structure
semantic 
pole
syntactic 
pole
construction
second
merging
phase
first 
merging 
phase
matching phase
Figure 1: FCG allows the implementation of efficient and strongly reversible grammars. Left: In production,
conditional units of the semantic pole of a construction are matched against a transient structure, before additional
semantic constraints and the syntactic pole are merged with the structure. Right: In parsing, the same algorithm
applies but in the opposite direction.
2 Strong and Efficient Reversibility
Reversible or bidirectional grammar formalisms
can achieve both production and parsing (Strza-
lkowski, 1994). Several platforms, such as the
LKB (Copestake, 2002), already achieve bidirec-
tionality, but they do so through separate algo-
rithms for parsing and production (mainly for effi-
ciency reasons). One problem with this approach
is that there may be a loss of coherence in gram-
mar engineering. For instance, the LKB parser
can handle a wider variety of structures than its
generator.
FCG uses one core engine that handles both
parsing and production with a single linguistic
inventory (see Figure 1). When processing, the
FCG-system builds a transient structure that con-
tains all the information concerning the utterance
that the system has to parse or produce, divided
into a semantic and syntactic pole (both of whom
are feature structures). Grammar rules or ?con-
structions? are coupled feature structures as well
and thus contain a semantic and syntactic pole.
When applying constructions, the FCG-system
goes through three phases. In production, FCG
first matches all feature-value pairs of the seman-
tic pole of a construction with the semantic pole
of the transient structure, except fv-pairs that are
marked for being attributed by the construction
(De Beule and Steels, 2005). Matching is a more
strict form of unification that resembles a sub-
sumption test (see Steels and De Beule, 2006).
If matching is successful, all the marked fv-pairs
of the semantic pole are merged with the tran-
sient structure in a first merge phase, after which
the whole syntactic pole is merged in a second
phase. FCG-merge is equivalent to ?unification?
in other formalisms. The same three-phase algo-
rithm is applied in parsing as well, but this time in
the opposite direction: if the syntactic pole of the
construction matches with the transient structure,
the attributable syntactic fv-pairs and the seman-
tic pole are merged.
3 WYSIWYG Grammar Engineering
Most unification grammars use non-directional
linguistic representations that are designed to be
independent of any model of processing (Sag
and Wasow, 2011). Whereas this may be de-
sirable from a ?mathematical? point-of-view, it
puts the burden of efficient processing on the
shoulders of computational linguists, who have to
find a balance between faithfulness to the hand-
written theory and computational efficiency (Mel-
nik, 2005). For instance, there is no HPSG imple-
mentation, but rather several platforms that sup-
port the implementation of ?HPSG-like? gram-
mars: ALE (Carpenter and Penn, 1995), ALEP
(Schmidt et al 1996), CUF (D?rre and Dorna,
64
top
cxn-applied
top
nominal-adjectival-cxn
sem-subunits  
footprints  
args  
sem-cat  
nominal-adjectival-phrase-1
(word-ballon-1 
word-rouge-1)
(nominal-adjectival-cxn)
(red-ball-15 context-19)
((sem-function 
identifier))
word-
ballon-
1
word-
rouge-
1
word-le-1
sem syn
form  
syn-subunits  
syn-cat  
footprints  
nominal-adjectival-phrase-1
((meets 
word-ballon-1 
word-rouge-1))
(word-ballon-1 
word-rouge-1)
((number singular) 
(gender masculine) 
(syn-function nominal))
(nominal-adjectival-cxn)
word-
rouge-
1
word-
ballon-
1
word-le-1
Figure 2: FCG comes equipped with an interactive web interface for inspecting the linguistic inventory, con-
struction application and search. This Figure shows an example construction where two units are opened up for
closer inspection of their feature structures.
1993), LIGHT (Ciortuz, 2002), LKB (Copestake,
2002), ProFIT (Erbach, 1995), TDL (Krieger and
Sch?fer, 1994), TFS (Emele, 1994), and others
(see Bolc et al 1996, for a survey). Unfortu-
nately, the optimizations and technologies devel-
oped within these platforms are often considered
by theoretical linguists as engineering solutions
rather than scientific contributions.
FCG, on the other hand, adheres to the cogni-
tive linguistics assumption that linguistic perfor-
mance is equally important as linguistic compe-
tence, hence processing becomes a central notion
in the formalism. FCG representations therefore
offer a ?what you see is what you get? approach
to grammar engineering where the representations
have a direct impact on processing and vice versa.
For instance, a construction?s division between a
semantic and syntactic pole is informative with re-
spect to how the construction is applied.
Some grammarians may object that this design
choice forces linguists to worry about process-
ing, but that is entirely the point. It has already
been demonstrated in other unification-based for-
malisms that different grammar representations
have a significant impact on processing efficiency
(Flickinger, 2000). Moreover, FCG-style repre-
sentations can be directly implemented and tested
without having to compromise on either faithful-
ness to a theory or computational efficiency.
Since writing grammars is highly complex,
however, FCG also features a ?design level? on top
of its operational level (Steels, 2012b). On this
level, grammar engineers can use templates that
build detailed constructions. The demonstration
shows how to write a grammar in FCG, switch-
ing between its design level, its operational level
and its interactive web interface (see Figure 2).
The web interface allows FCG-users to inspect the
linguistic inventory, the search tree in processing,
and so on.
4 Robustness and Learning
Unification-based grammars have the reputation
of being brittle when it comes to processing nov-
elty or ungrammatical utterances (Tomuro, 1999).
Since cognitive linguistics adheres to a usage-
based view on language (Langacker, 2000), how-
ever, an adequate formalization must be robust
and open-ended.
A first requirement is that there can be differ-
ent degrees of ?entrenchment? in the grammar:
while some features might still be emergent, oth-
ers are already part of well-conventionalized lin-
guistic patterns. Moreover, new features and con-
structions may appear (or disappear) from a gram-
mar at any given time. These requirements are
hard to reconcile with the type hierarchy approach
of other formalisms, so FCG does not imple-
ment typed feature structures. The demonstra-
tion shows how FCG can nevertheless prevent
over-licensing of linguistic structures through its
matching phase and how it captures generaliza-
tions through its templates ? two benefits typically
associated with type hierarchies.
Secondly, FCG renders linguistic processing
fluid and robust through a meta-level architec-
ture, which consists of two layers of processing,
as shown in Figure 3 (Beuls et al 2012). There
is a routine layer in which constructional process-
ing takes place. At the same time, a meta-layer
65
!"!"
routine processing 
diagnostic 
problem repair 
diagnostic diagnostic diagnostic 
problem 
repair meta-layer processing 
Figure 3: There are two layers of processing in FCG. On the routine level, constructional processing takes place.
At the same time, a meta-layer of diagnostics and repairs try to detect and solve problems that occur in the routine
layer.
is active that runs diagnostics for detecting prob-
lems in routine processing, and repairs for solving
those problems. The demonstration shows how
the meta-layer is used for solving common prob-
lems such as missing lexical entries and coercion
(Steels and van Trijp, 2011), and how its archi-
tecture offers a uniform way of implementing the
various solutions for robustness already pioneered
in the aforementioned grammar platforms.
5 Efficiency
Unification is computationally expensive, and
many technical solutions have been proposed for
efficient processing of rich and expressive fea-
ture structures (Tomuro, 1999; Flickinger, 2000;
Callmeier, 2001). In FCG, however, research
on efficiency takes a different dimension because
performance is considered to be an integral part of
the linguistic theory that needs to be operational-
ized. The demonstration allows conference par-
ticipants to inspect the following research results
on the interplay between grammar and efficiency:
? In line with construction grammar, there is
no distinction between the lexicon and the
grammar. Based on language usage, the lin-
guistic inventory can nevertheless organize
itself in the form of dependency networks
that regulate which construction should be
considered when in processing (Wellens and
De Beule, 2010; Wellens, 2011).
? There is abundant psycholinguistic evidence
that language usage contains many ready-
made language structures. FCG incorporates
a chunking mechanism that is able to cre-
ate such canned phrases for faster processing
(Stadler, 2012).
? Morphological paradigms, such as the Ger-
man case system, can be represented in the
form of ?feature matrices?, which reduce
syntactic and semantic ambiguity and hence
speed up processing efficiency and reliability
(van Trijp, 2011).
? Many linguistic domains, such as spatial lan-
guage, are known for their high degree of
polysemy. By distinguishing between actual
and potential values, such polysemous struc-
tures can be processed smoothly (Spranger
and Loetzsch, 2011).
6 Conclusion
With many well-developed unification-based
grammar formalisms available to the community,
one might wonder whether any ?new kid on the
block? can still claim relevance today. With this
demonstration, we hope to show that Fluid Con-
struction Grammar allows grammar engineers to
unchart new territory, most notably in the relation
between linguistic competence and performance,
and in modeling usage-based approaches to lan-
guage.
66
References
Johanna Bar?dal and Shobhana Chelliah, edi-
tors. The Role of Semantic, Pragmatic and
Discourse Factors in the Development of Case.
John Benjamins, Amsterdam, 2009.
Katrien Beuls, Remi van Trijp, and Pieter
Wellens. Diagnostics and repairs in Fluid Con-
struction Grammar. In Luc Steels, editor, Com-
putational Issues in Fluid Construction Gram-
mar. Springer Verlag, Berlin, 2012.
Hans C. Boas. A Constructional Approach to Re-
sultatives. Stanford Monograph in Linguistics.
CSLI, Stanford, 2003.
Leonard Bolc, Krzysztof Czuba, Anna
Kups?c?, Malgorzata Marciniak, Agnieszka
Mykowiecka, and Adam Przepi?rkowski. A
survey of systems for implementing HPSG
grammars. Research Report 814 of IPI
PAN (Institute of Computer Science, Polish
Academy of Sciences), 1996.
Ulrich Callmeier. Efficient parsing with large-
scale unification grammars. Master?s thesis,
Universit?t des Saarlandes, 2001.
Bob Carpenter. The Logic of Typed Feature Struc-
tures. Cambridge UP, Cambridge, 1992.
Bob Carpenter and Gerald Penn. The Attribute
Logic Engine (Version 2.0.1). Pittsburgh, 1995.
Liviu Ciortuz. LIGHT ? a constraint language and
compiler system for typed-unification gram-
mars. In Proceedings of The 25th German Con-
ferences on Artificial Intelligence (KI 2002),
volume 2479 of LNAI, pages 3?17, Berlin,
2002. Springer-Verlag.
Ann Copestake. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stan-
ford, 2002.
William Croft and D. Alan Cruse. Cognitive Lin-
guistics. Cambridge Textbooks in Linguistics.
Cambridge University Press, Cambridge, 2004.
J. De Beule and L. Steels. Hierarchy in fluid con-
struction grammar. In U. Furbach, editor, Pro-
ceedings of the 28th Annual German Confer-
ence on Artificial Intelligence, volume 3698 of
Lecture Notes in Artificial Intelligence, pages
1?15, Berlin, Germany, 2005. Springer Verlag.
Jochen D?rre and Michael Dorna. CUF ? a
formalism for linguistic knowledge represen-
tation. In Jochen D?rre, editor, Computa-
tional Aspects of Constraint Based Linguistic
Descriptions, volume I, pages 1?22. DYANA-2
Project, Amsterdam, 1993.
Martin C. Emele. The typed feature structure rep-
resentation formalism. In Proceedings of the
International Workshop on Sharable Natural
Language Resources, Ikoma, Nara, 1994.
Gregor Erbach. ProFIT: Prolog with features,
inheritance and templates. In Proceedings of
EACL-95, 1995.
Vyvyan Evans and Melanie Green. Cognitive Lin-
guistics: An Introduction. Lawrence Erlbaum
Associates / Edinburgh University Press, Hills-
dale, NJ/Edinburgh, 2006.
Vyvyan Evans, Benjamin K. Bergen, and J?rg
Zinken. The cognitive linguistics enterprise:
An overview. In V. Evans, B.K. Bergen, and
J. Zinken, editors, The Cognitive Linguistics
Reader. Equinox Publishing, London, 2007.
Daniel P. Flickinger. On building a more efficient
grammar by exploiting types. Natural Lan-
guage Engineering, 6(1):15?28, 2000.
Jonathan Ginzburg and Ivan A. Sag. Interroga-
tive Investigations: the Form, the Meaning, and
Use of English Interrogatives. CSLI Publica-
tions, Stanford, 2000.
Adele E. Goldberg, Devin M. Casenhiser, and
Nitya Sethuraman. Learning argument struc-
ture generalizations. Cognitive Linguistics, 15
(3):289?316, 2004.
Bernd Heine, Ulrike Claudi, and Friederike H?n-
nemeyer. Grammaticalization: A Concep-
tual Framework. University of Chicago Press,
Chicago, 1991.
Martin Kay. Functional grammar. In Proceedings
of the Fifth Annual Meeting of the Berkeley Lin-
guistics Society, pages 142?158. Berkeley Lin-
guistics Society, 1979.
Hans-Ulrich Krieger and Ulrich Sch?fer. TDL ?
a type description language for HPSG. part 1:
Overview. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics, pages 893?899, Kyoto, 1994.
George Lakoff. Women, Fire, and Danger-
ous Things: What Categories Reveal about
the Mind. The University of Chicago Press,
Chicago, 1987.
67
Ronald W. Langacker. Foundations of Cognitive
Grammar: Theoretical Prerequisites. Stanford
University Press, Stanford, 1987.
Ronald W. Langacker. A dynamic usage-based
model. In Michael Barlow and Suzanne Kem-
mer, editors, Usage-Based Models of Lan-
guage, pages 1?63. Chicago University Press,
Chicago, 2000.
David Lee. Cognitive Linguistics: An Introduc-
tion. Oxford University Press, Oxford, 2001.
Elena Lieven. Developing constructions. Cogni-
tive Linguistics, 20(1):191?199, 2009.
James L. McClelland. The place of modeling in
cognitive science. Topics in Cognitive Science,
1:11?38, 2009.
Nurit Melnik. From ?hand-written? to computa-
tionally implemented HPSG theories. In Ste-
fan M?ller, editor, Proceedings of the HPSG05
Conference, Stanford, 2005. CSLI Publica-
tions.
Eleanor Rosch. Cognitive representations of se-
mantic categories. Journal of Experimental
Psychology: General, 104:192?233, 1975.
Ivan A. Sag and Thomas Wasow. Performance-
compatible competence grammar. In Robert D.
Borsley and Kersti B?rjars, editors, Non-
Transformational Syntax: Formal and Explicit
Models of Grammar, pages 359?377. Wiley-
Blackwell, 2011.
Paul Schmidt, Sibylle Rieder, Axel Theofilidis,
and Thierry Declerck. Lean formalisms, lin-
guistic theory, and applications. grammar de-
velopment in ALEP. In Proceedings of the
16th International Conference on Computa-
tional Linguistics (COLING-96), pages 286?
291, Copenhagen, 1996.
Michael Spranger and Martin Loetzsch. Syntac-
tic indeterminacy and semantic ambiguity: A
case study for German spatial phrases. In Luc
Steels, editor, Design Patterns in Fluid Con-
struction Grammar. John Benjamins, Amster-
dam, 2011.
Kevin Stadler. Chunking constructions. In
Luc Steels, editor, Computational Issues in
Fluid Construction Grammar. Springer Verlag,
Berlin, 2012.
Luc Steels, editor. Design Patterns in Fluid Con-
struction Grammar. John Benjamins, Amster-
dam, 2011.
Luc Steels, editor. Computational Issues in
Fluid Construction Grammar. Springer, Berlin,
2012a.
Luc Steels. Design methods for Fluid Construc-
tion Grammar. In Luc Steels, editor, Computa-
tional Issues in Fluid Construction Grammar.
Springer Verlag, Berlin, 2012b.
Luc Steels and Joachim De Beule. Unify and
merge in Fluid Construction Grammar. In
P. Vogt, Y. Sugita, E. Tuci, and C. Nehaniv,
editors, Symbol Grounding and Beyond., LNAI
4211, pages 197?223, Berlin, 2006. Springer.
Luc Steels and Remi van Trijp. How to make con-
struction grammars fluid and robust. In Luc
Steels, editor, Design Patterns in Fluid Con-
struction Grammar, pages 301?330. John Ben-
jamins, Amsterdam, 2011.
Anatol Stefanowitsch and Stefan Th. Gries. Col-
lostructions: Investigating the interaction of
words and constructions. International Journal
of Corpus Linguistics, 2(8):209?243, 2003.
Tomek Strzalkowski, editor. Reversible Grammar
in Natural Language Processing. Kluwer Aca-
demic Publishers, Boston, 1994.
Michael Tomasello. Constructing a Language. A
Usage Based Theory of Language Acquisition.
Harvard University Press, 2003.
Noriko Tomuro. Left-Corner Parsing Algorithm
for Unification Grammars. PhD thesis, DePaul
University, Chicago, 1999.
Remi van Trijp. Feature matrices and agree-
ment: A case study for German case. In Luc
Steels, editor, Design Patterns in Fluid Con-
struction Grammar, pages 205?236. John Ben-
jamins, Amsterdam, 2011.
Pieter Wellens. Organizing constructions in net-
works. In Luc Steels, editor, Design Patterns in
Fluid Construction Grammar. John Benjamins,
Amsterdam, 2011.
Pieter Wellens and Joachim De Beule. Prim-
ing through constructional dependencies: A
case study in Fluid Construction Grammar.
In A. Smith, M. Schouwstra, Bart de Boer,
and K. Smith, editors, The Evolution of Lan-
guage (EVOLANG8), pages 344?351, Singa-
pore, 2010. World Scientific.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 127?132,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fluid Construction Grammar for Historical and Evolutionary Linguistics
Pieter Wellens1, Remi van Trijp2, Katrien Beuls1, Luc Steels2,3
1VUB AI Lab 2Sony Computer Science 3 ICREA Institute for
Pleinlaan 2 Laboratory Paris Evolutionary Biology (UPF-CSIC)
1050 Brussels (Belgium) 6 Rue Amyot PRBB, Dr Aiguidar 88
pieter|katrien@ 75005 Paris (France) 08003 Barcelona (Spain)
ai.vub.ac.be remi@csl.sony.fr steels@ai.vub.ac.be
Abstract
Fluid Construction Grammar (FCG) is an
open-source computational grammar for-
malism that is becoming increasingly pop-
ular for studying the history and evolution
of language. This demonstration shows
how FCG can be used to operationalise the
cultural processes and cognitive mecha-
nisms that underly language evolution and
change.
1 Introduction
Historical linguistics has been radically trans-
formed over the past two decades by the ad-
vent of corpus-based approaches. Ever increas-
ing datasets, both in size and richness of anno-
tation, are becoming available (Yuri et al, 2012;
Davies, 2011), and linguists now have more pow-
erful tools at their disposal for uncovering which
changes have taken place. In this demonstration,
we present Fluid Construction Grammar (Steels,
2011, FCG), an open-source grammar formalism
that makes it possible to also address the question
of how these changes happened by uncovering the
cognitive mechanisms and cultural processes that
drive language evolution.
FCG combines the expressive power of fea-
ture structures and unification with the adaptiv-
ity and robustnes of machine learners. In sum,
FCG aims to be an open instrument for de-
veloping robust and open-ended models of lan-
guage processing that can be used for both pars-
ing and production. FCG can be downloaded at
http://www.fcg-net.org.
2 Design Philosophy
Fluid Construction Grammar is rooted in a
cognitive-functional approach to language, which
is quite different from a generative grammar such
as HPSG (Pollard and Sag, 1994). A genera-
tive grammar is a model of language competence
that licenses well-formed structures and rejects ill-
formed utterances. Such grammars often decide
on the well- or ill-formedness of utterances by us-
ing a strong type system that defines a set of fea-
tures and possible values for those features. The
burden of efficient and robust language process-
ing with a generative grammar largely rests on the
shoulders of the language processor.
A cognitive-functional grammar, on the other
hand, functions more like a transducer between
meaning and form. In parsing, such a grammar
tries to uncover as much meaning as possible from
a given utterance rather than deciding on its gram-
maticality. In the other direction, the grammar
tries to produce intelligible utterances, which are
well-formed as a side-effect if the grammar ad-
equately captures the conventions of a particular
language. A cognitive-functional grammar can
best be implemented without a strong type system
because the set of possible features and values for
them is assumed to be open-ended. Efficient and
robust language processing also becomes a joint
responsibility of the grammar and the linguistic
processor.
3 Reversible Language Processing
As a construction grammar, FCG represents all
linguistic knowledge as pairings of function and
form (called constructions). This means that any
linguistic item, be it a concrete lexical item (see
Figure 1) or a schematic construction, shares the
same fundamental representation in FCG.
Each construction consists of two poles (a se-
mantic/functional one and a syntactic/form one),
each represented as a feature structure. By using a
separate semantic and syntactic pole, FCG allows
the same construction to be efficiently parsed and
produced by the same processing engine by sim-
ply changing the direction of application.
127
reset
tag ?meaning-849 
footprints  
?top-unit-1611
(meaning
(==
(identify-person
?kim-1
 
?context-243
?person-119)
(bind
 
person
?person-119
 
[kim])))
(==0
 
kim-lex
 
lex)
footprints  
tag ?form-946 
?top-unit-1611
kim-lex (lex)
?top-unit-1611
(==0
 
kim-lex
 
lex)
(form
(
==
 
(string
 
?word-kim-1
 
"Kim")))
?top-unit-1611
sem syn
args  
sem-cat  
footprints  
?word-kim-1
? ?meaning-849
(?kim-1)
((sem-function
referring)
(sem-class
 
person))
(==1
 
kim-lex
 
lex)
footprints  
syn-cat  
?word-kim-1
? ?form-946
(==1
 
kim-lex
 
lex)
((lex-cat
proper-noun)
(syn-function
nominal))
Babel web interface http://localhost:8000/
1 of 1 12/6/12 11:08 PM
Figure 1: Lexical construction for the proper
noun ?Kim? as shown in the FCG web interface.
All constructions are mappings between semantic
(left) and syntactic feature structures (right).
FCG processing uses two different kinds of uni-
fication called match and merge. The match phase
is a conditional phase which checks for applicabil-
ity of the construction. The merge operation most
closely resembles classical (yet untyped) unifica-
tion. In production (i.e. going from meaning to
form), the processor will consider a construction?s
semantic pole as a set of conditions that need to be
satisfied, and the syntactic pole as additional infor-
mation that can be contributed by the construction.
In parsing (i.e. going from form to meaning), the
roles of the poles are reversed.
Since FCG pays a lot of attention to the inter-
action between linguistic knowledge and process-
ing, it makes it possible to investigate the conse-
quences of particular aspects of grammar with re-
gard to representation, production, parsing, learn-
ing and propagation (in a population of language
users). For example, a small case system may be
easier to represent and produce than a large sys-
tem, but it might also lead to increased ambigu-
ity in parsing and learning that the larger system
would avoid. Fluid Construction Grammar can
bring these differences to the surface for further
computational analysis.
It is exactly this ability to monitor the impact of
grammatical choices, that has sparked the interest
of an increasingly wide audience of historical and
evolutionary linguists. With FCG, different histor-
ical stages can be implemented (which addresses
questions about representation and processing) but
FCG also comes bundled with a reflective learn-
ing framework (Beuls et al, 2012) for learning the
key constructions of each stage. That same archi-
tecture has proven to be adequately powerful to
implement processes of grammaticalization so that
Linguistic system 1
Reconstruction
Individual Learning
Population 
Alignment
Grammaticalization
Linguistic system 2
Reconstruction
Individual Learning
Population 
Alignment
1.
2.
3.
1.
2.
3.
4.
Figure 2: Schematic overview of the experimental
methodology for historical and evolutionary lin-
guists. The example here shows only two linguis-
tic stages but there could be more.
actual linguistic change over time can be modeled
(van Trijp, 2010; Beuls and Steels, 2013; Wellens
and Loetzsch, 2012).
4 How to set up an evolutionary
linguistics experiment in FCG?
As the FCG processor can both produce and
parse utterances it is possible to instantiate not
one but a set or population of FCG processors
(or FCG agents) that can communicatively inter-
act with each other. Experiments in historical or
evolutionary linguistics make use of this multi-
agent approach where all agents engage in situated
pairwise interactions (language games) (Steels,
2012b).
In this systems demo we will focus on a re-
cent experiment in the emergence of grammatical
agreement (Beuls and Steels, 2013). The language
game consists of two agents in which one agent
(the speaker) has to describe one or more (max
three) objects in a scene to the other agent (the
hearer). Each object can be described by one or
more words. It follows that without any grammat-
ical marking it would be difficult (often impossi-
ble) for the hearer to figure out which words de-
scribe the same object and thus to arrive at a suc-
cessful interpretation. The hypothesis is that the
introduction of agreement markers helps solve this
ambiguity.
Next to setting up a language game script the
methodology consists of operationalizing the lin-
guistic strategies required for a population to boot-
strap and maintain a particular linguistic system (in
this case nominal agreement). Examples of lin-
128
!"!"
routine processing 
diagnostic 
problem repair 
diagnostic diagnostic diagnostic 
problem 
repair meta-layer processing 
Figure 3: Reflective meta-layer architecture oper-
ating as part of an FCG agent/processor.
guistic systems already investigated include Ger-
man case (van Trijp, 2012a; van Trijp, 2013),
the grammatical expression of space (Spranger
and Steels, 2012), the emergence of quantifiers
(Pauw and Hilferty, 2012) and the expression of
aspect in Russian (Gerasymova et al, 2012) [for
an overview see (Steels, 2011; Steels, 2012a)].
An experiment generally investigates multi-
ple linguistic systems of increasing complexity
where each system can, but need not, map to a
stage along an attested grammaticalization path-
way. Most often a stage is introduced in order
to gradually increase the complexity of the emer-
gent dynamics. In this demo we posit four sys-
tems/strategies, (1) a baseline purely lexical strat-
egy, (2) a strategy to bootstrap and align formal
(meaningless) agreement markers, (3) a strategy to
bootstrap and align meaningful agreement mark-
ers, and finally (4) a strategy that allows re-use
of existing lexical constructions as markers (gram-
maticalization).
Implementing and linking together all the com-
ponents involved in a single system is a highly
non-trivial undertaking and our methodology pre-
scribes the following four steps to undertake for
each system (see also Figure 2).
Reconstruction: A full operationalization of all
the constructions (lexical and grammatical)
involved in the chosen linguistic phenom-
ena. When multiple agents are initialized
with these constructions they should be able
to communicate successfully with each other.
This stage serves primarily to test and verify
intuitions about the different linguistic sys-
tems.
Individual Learning: Implementation of learn-
ing algorithms (or re-use of existing ones)
Figure 4: Meaningful marker strategy.
so that one agent can learn the constructions
based on the input of another agent. These
learning operations are generally divided into
diagnostics and repair strategies (see Fig-
ure 3). Diagnostics continually monitor FCG
processing for errors or inefficiencies and
generate problems if they are found. Repair
strategies then act on these problems by al-
tering the linguistic inventory (e.g. adding,
removing or changing constructions).
Population Alignment: There exists a large gap
between the cognitive machinary needed for
learning an existing linguistic system (step 2)
and bootstrapping, aligning and maintaining
a complete linguistic system from scratch. In
this step individual learning operators are ex-
tended with alignment strategies.
Grammaticalization: Moving from one linguis-
tic system to another is the final step of the
experiment. The challenge is to find and im-
plement the mechanisms that drive grammat-
icalization (Heine and Kuteva, 2007) in line
with observed grammaticalization pathways.
As an example we?ll give a short sketch of one
possible game as played in the meaningful marker
strategy as schematically shown in Figure 4. The
sketch shows a context of four objects (O1 to O4),
each described by three features. The speaker
chooses topic O1 + O2 which, given his vocab-
ulary (shown top right), results in uttering ?shuq-
fon sizhic zabu?. Words ?shuqfon? and ?sizhic?
both describe parts of O1 and ?zabu? of O2. In
order to explicitly communicate this linking the
speaker attaches the markers ?-ti? and ?-ta? so that
their meaning is compatible with the objects they
are linking as shown in the Figure. This allows
129
Figure 5: A network of constructions. Diamond shaped nodes represent lexical constructions, egg shaped
nodes represent grammatical constructions and rectangular nodes represent semantic categories. Arrows
can be read as ?primes?. For example the preposition between [BETWEEN.PREP] primes the category
LOCATIVE RELATION which in turn primes both the [LOCATIVE RELATION] and [SPATIAL PHRASE]
constructions. Both of these constructions also require a semantic category [REFERENT].
the hearer to arrive at a single non-ambiguous in-
terpretation. For more details we refer the reader
to (Beuls and Steels, 2013) and the web demo at
http://ai.vub.ac.be/materials/plos-agreement/.
5 Features of FCG
A number of key features of FCG have already
been introduced. Reversible bidirectional process-
ing, a single data representation for all linguistic
knowledge, a reflective meta-layer architecture for
learning and a multi-agent component for manag-
ing multiple interacting FCG instances. Other fea-
tures, some of which are unique to FCG, include,
but are not limited to:
Web interface: FCG comes with a rich
HTML/AJAX based web interface (Loet-
zsch, 2012) where it can show fine-grained
information to the user in a user-friendly
manner through the use of expandable
elements. See Figure 6.
Customizable processing: Linguistic process-
ing is implemented as a search process
(Bleys et al, 2011). The user has easy
access to the most important parameters
influencing this process. Examples of these
are the heuristics and the tests that determine
whether a node represents an acceptable
solution. FCG comes bundled with a library
of heuristics and goal tests and with a bit
of programming skills users can add new
primitives easily.
Customizable construction inventory: By de-
fault, FCG stores all constructions in one
large set. FCG however supplies a num-
ber of different taxonomies, both for concep-
tual and efficiency reasons. One popular op-
tion is to organize constructions in smaller
subsets (Beuls, 2011) like lexical, morpho-
logical, functional, etc. Another option is
to use networks (Wellens, 2011) that can
learn co-occurrence relations between con-
structions and ?prime? constructions when
they are likely to apply (see Figure 5).
Interfaces to external repositories: FCG
can connect to external repositories like
Framenet (Baker et al, 1998) and Wordnet
(Miller, 1995) to load thousands of lexical
entries (Micelli et al, 2009; Wellens and
Beule, 2010).
Robustness: FCG continues operation as far as
it can get even if some constructions do not
apply (Steels and van Trijp, 2011). Sup-
plied with appropriate diagnostics and repair
strategies FCG can even recover from errors
(van Trijp, 2012b).
Open source: Best of all, FCG is freely down-
loadable and open source (http://www.fcg-
net.org). It is written in Common Lisp
(CLOS) and compatible with most popu-
lar lisp implementations (SBCL, CCL, Lisp-
works, ...).
130
top
top
Parsing "block"
Applying construction set (70)  in direction 
Found a solution
initialstructure top
applicationprocess
appliedconstructions
resultingstructure
top
Meaning:
((apply-class ?ref-2 ?src-2 ?class-1) (bind object-class ?class-1 block))
sem syn
initial
top
top
cxn-applied
application result
status cxn-applied
sourcestructure top
appliedconstruction
resultingstructure top
resultingbindings ((?form-84 form ((string block-83 "block"))) (?block-unit-2 . block-83) (?top-39 . top))
added infirst merge block-83
added insecondmerge
block-83
cxn supplier :ordered-by-label
remaining labels (cat  gram)
remaining cxns (right-lex speaker-lex unique-lex hearer-lex)
block-morph (morph t)
sem syn
block-morph (morph t)
sem syn block-83 block-lex(lex t)
noun-
cat
(cat t)
noun-cat (cat t) block-lex (lex t) block-morph (morph t)
noun-unit-273
footprints  
meaning  
ref  
sem-cat  
block-83
(block-lex)
((bind object-class ?class-1 block))
?class-1
((sem-function ((value ?sem-function-value-4) (valence (identifier))))(class (object-class)))
sem syn noun-unit-273 block-83
expanded search tree node
expanded unit
Figure 6: An example of parsing the noun ?Block? as shown in the FCG web interface. Users can click
on nearly every element to show an expanded version.
The reader is encouraged to take a look at
http://www.fcg-net.org/projects/design-patterns-
in-fluid-construction-grammar for a selection of
demonstrations of Fluid Construction Grammar.
6 Conclusion
Fluid Construction Grammar is a mature technol-
ogy that can be used by computational linguists
to complement more traditional corpus-based ap-
proaches. FCG builds on many existing and
proven technologies and adds new innovations to
the mix resulting in a user friendly, yet powerful
and extensible framework for in-depth investiga-
tions in natural language phenomena.
Acknowledgments
The FCG formalism is being developed at the Ar-
tificial Intelligence Laboratory of the Vrije Uni-
versiteit Brussel and the Sony Computer Science
Laboratory in Paris. Pieter Wellens has been
supported by the ESF EuroUnderstanding project
DRUST funded by FWO and by the Vrije Uni-
versiteit Brussel. Katrien Beuls received fund-
ing from a strategic basic research grant from the
agency for Innovation by Science and Technol-
ogy (IWT). Remi van Trijp is funded by the Sony
Computer Science Laboratory Paris. We would
also like to thank Michael Spranger for his con-
tributions to the FCG formalism.
131
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Katrien Beuls and Luc Steels. 2013. Agent-based
models of strategies for the emergence and evo-
lution of grammatical agreement. PLoS ONE,
8(3):e58960, 03.
Katrien Beuls, Remi van Trijp, and Pieter Wellens.
2012. Diagnostics and repairs in Fluid Construc-
tion Grammar. In Luc Steels and Manfred Hild, ed-
itors, Language Grounding in Robots. Springer Ver-
lag, Berlin.
Katrien Beuls. 2011. Construction sets and unmarked
forms: A case study for Hungarian verbal agree-
ment. In Luc Steels, editor, Design Patterns in Fluid
Construction Grammar, pages 237?264. John Ben-
jamins, Amsterdam.
Joris Bleys, Kevin Stadler, and Joachim De Beule.
2011. Search in linguistic processing. In Luc Steels,
editor, Design Patterns in Fluid Construction Gram-
mar, pages 149?179. John Benjamins, Amsterdam.
Mark Davies. 2011. N-grams and word frequency
data from the corpus of historical american english
(coha).
Kateryna Gerasymova, Michael Spranger, and Katrien
Beuls. 2012. A language strategy for aspect: En-
coding aktionsarten through morphology. In Luc
Steels, editor, Experiments in Cultural Language
Evolution, pages 257 ? 276. John Benjamins.
Bernd Heine and Tania Kuteva. 2007. The Genesis
of Grammar: A Reconstruction. Oxford University
Press, October.
Martin Loetzsch. 2012. Tools for grammar engineer-
ing. In Luc Steels, editor, Computational Issues
in Fluid Construction Grammar. Springer Verlag,
Berlin.
V. Micelli, R. van Trijp, and J. De Beule. 2009. Fram-
ing fluid construction grammar. In N.A. Taatgen and
H. van Rijn, editors, the 31th Annual Conference
of the Cognitive Science Society, pages 3023?3027.
Cognitive Science Society.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38:39?41, November.
Simon Pauw and Joseph Hilferty. 2012. The emer-
gence of quantifiers. In Luc Steels, editor, Experi-
ments in Cultural Language Evolution, pages 277 ?
304. John Benjamins.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago.
Michael Spranger and Luc Steels. 2012. Emergent
functional grammar for space. In Luc Steels, editor,
Experiments in Cultural Language Evolution, pages
207 ? 232. John Benjamins, Amsterdam.
Luc Steels and Remi van Trijp. 2011. How to make
construction grammars fluid and robust. In Luc
Steels, editor, Design Patterns in Fluid Construction
Grammar, pages 301?330. John Benjamins, Ams-
terdam.
Luc Steels, editor. 2011. Design Patterns in Fluid
Construction Grammar. John Benjamins.
Luc Steels, editor. 2012a. Computational Issues in
Fluid Construction Grammar, volume 7249 of Lec-
ture Notes in Computer Science. Springer, Berlin.
Luc Steels, editor. 2012b. Experiments in Cultural
Language Evolution. John Benjamins, Amsterdam.
Remi van Trijp. 2010. Grammaticalization and seman-
tic maps: Evidence from artificial language evolu-
tion. Linguistic Discovery, 8:310?326.
Remi van Trijp. 2012a. Not as awful as it seems : Ex-
plaining german case through computational exper-
iments in fluid construction grammar. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 829?839.
Remi van Trijp. 2012b. A reflective architecture for
language processing and learning. In Luc Steels,
editor, Computational Issues in Fluid Construction
Grammar. Springer Verlag, Berlin.
Remi van Trijp. 2013. Linguistic assessment crite-
ria for explaining language change: A case study on
syncretism in German definite articles. Language
Dynamics and Change, 3(1).
Pieter Wellens and Joachim De Beule. 2010. Priming
through constructional dependencies: a case study
in fluid construction grammar. In The Evolution
of Language ( EVOLANG8), pages 344?351. World
Scientific.
Pieter Wellens and Martin Loetzsch. 2012. Multi-
dimensional meanings in lexicon formation. In Luc
Steels, editor, Experiments in Cultural Language
Evolution, pages 143?166. John Benjamins, Ams-
terdam.
Pieter Wellens. 2011. Organizing constructions in net-
works. In Luc Steels, editor, Design Patterns in
Fluid Construction Grammar, pages 181?201. John
Benjamins, Amsterdam.
Lin Yuri, Michel Jean-Baptiste, Lieberman Aiden Erez,
Orwant Jon, Brockman Will, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram
corpus. In ACL (System Demonstrations). The As-
sociation for Computer Linguistics.
132
