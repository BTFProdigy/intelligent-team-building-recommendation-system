Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 857?868,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Soft Dependency Constraints for Reordering
in Hierarchical Phrase-Based Translation
Yang Gao, Philipp Koehn and Alexandra Birch
School of Informatics
University of Edinburgh
Edinburgh, UK, EH8 9AB
yanggao1119@gmail.com, pkoehn@inf.ed.ac.uk, a.birch@ed.ac.uk
Abstract
Long-distance reordering remains one of the
biggest challenges facing machine translation.
We derive soft constraints from the source de-
pendency parsing to directly address the re-
ordering problem for the hierarchical phrase-
based model. Our approach significantly im-
proves Chinese?English machine translation
on a large-scale task by 0.84 BLEU points
on average. Moreover, when we switch the
tuning function from BLEU to the LRscore
which promotes reordering, we observe total
improvements of 1.21 BLEU, 1.30 LRscore
and 3.36 TER over the baseline. On aver-
age our approach improves reordering preci-
sion and recall by 6.9 and 0.3 absolute points,
respectively, and is found to be especially ef-
fective for long-distance reodering.
1 Introduction
Reordering, especially movement over longer dis-
tances, continues to be a hard problem in statistical
machine translation. It motivates much of the re-
cent work on tree-based translation models, such as
the hierarchical phrase-based model (Chiang, 2007)
which extends the phrase-based model (Koehn et al,
2003) by allowing the so-called hierarchical phrases
containing subphrases.
The hierarchical phrase-based model captures the
recursiveness of language without relying on syntac-
tic annotation, and promises better reordering than
the phrase-based model. However, Birch et al
(2009) find that although the hierarchical phrase-
based model outperforms the phrase-based model in
terms of medium-range reordering, it does equally
poorly in long-distance reordering due to constraints
to guarantee efficiency.
Syntax-based models that use phrase structure
constituent labels as non-terminals in their transfer
rules, exemplified by that of Galley et al (2004),
produce smarter and syntactically motivated re-
ordering. However, when working with off-the-shelf
tools for parsing and alignment, this approach may
impose harsh limits on rule extraction and requires
serious efforts of optimization (Wang et al, 2010).
An alternative approach is to augment the general
hierarchical phrase-based model with soft syntactic
constraints. Here, we derive three word-based, com-
plementary constraints from the source dependency
parsing, including:
? A dependency orientation feature, trained with
maximum entropy on the word-aligned par-
allel data, which directly models the head-
dependent orientation for source words;
? An integer-valued cohesion penalty that com-
plements the dependency orientation feature,
and fires when a word is not translated with its
head. It measures derivation well-formedness
and is used to indirectly help reordering;
? An auxiliary unaligned penalty feature that mit-
igates search error given the other two features.
We achieve significant improvements in terms of
the overall translation quality and reordering behav-
ior. To our knowledge we are the first to use the
source dependency parsing to target the reordering
problem for hierarchical phrase-based MT.
857
??   ?   ?  ??      ?    ??   ?   ??   ??    ??  .pobj dobj
prep
 Australia  is          with  North Korea    have       dipl. rels.  that      few           countries    one of    .     Aozhou    shi        yu     Beihan             you        bangjiao    de        shaoshu    guojia         zhiyi      .    
 Australia is one of the few countries that have diplomatic relations with North Korea. 
top
pobj
cpm nummodnn
attrpunct
rcmod
 
Figure 1: Example dependency parsing generated by the Stanford Parser. The Chinese source sentence and its English
translation come from (Chiang, 2007).
2 Three Soft Dependency Constraints
Our features are based on the source dependency
parsing, as shown in Figure 1. The basic unit of de-
pendency parsing is a triple consisting of the depen-
dent word, the head word and the dependency rela-
tion that connects them. For example, in Figure 1,
an arrow labelled prep goes from the word yu (En-
glish with) to the word you (English have), showing
that yu is a prepositional modifier of you.
We use the Stanford Parser1 to generate depen-
dency parsing, which automatically extracts de-
pendency relations from phrase structure parsing
(de Marneffe et al, 2006).
2.1 Dependency Orientation
Based on the assumption that constituents generally
move as a whole (Quirk et al, 2005), we decompose
the sentence reordering probability into the reorder-
ing probability for each aligned source word with re-
spect to its head, excluding the root word at the top
of the dependency hierarchy which does not have a
head word. Similarly, Hayashi et al (2010) also take
a word-based reordering approach for HPBMT, but
they model all possible pairwise orientation from
the source side as a general linear ordering prob-
lem (Tromble and Eisner, 2009).
To be more specific, we have a maximum entropy
orientation classifier that predicts the probability of
a source word being translated in a monotone or re-
versed manner with respect to its head. For example,
1http://nlp.stanford.edu/software/lex-parser.shtml
(a) (b)
pob poj pod pob poj pod
rob rob
roj roj
rod rod
roe roe
iheadidep idepihead
Figure 2: Word alignments to illustrate orientation clas-
sification. In (a), monotone (M); in (b), reversed (R).
given the alignment in Figure 2(a), with the align-
ment points (idep, jdep) for the source dependent
word and (ihead, jhead) for the source head word,
we define two orientation classes as:
c =
{
R if (jdep ? jhead)(idep ? ihead) < 0
M otherwise
(1)
When a source head or dependent word is aligned
to multiple target words, as shown in Figure 2(b),
we always take the first target word for orientation
classification.
The orientation classifier is trained on the large
word-aligned parallel corpus. Various features can
potentially be used, based on the source and target
context as well as syntactic and semantic analysis.
The orientation probability is evaluated in the fol-
lowing log-linear equation, where f is the source
context, d is the source dependency parsing, e? is
the target context produced so far, a? is the align-
ment produced so far and c is the orientation class:
858
Word p(M) p(R)
Aozhou 0.81 0.19
shi NA NA
yu 0.45 0.55
Beihan 0.88 0.12
you 0.12 0.88
bangjiao 0.83 0.17
de 0.58 0.42
shaoshu 0.30 0.70
guojia 0.19 0.81
zhiyi 0.85 0.15
. 1.00 0.00
Table 1: The dependency orientation probabilities for
words of the Figure 1 sentence, in both monotone and
reversed cases.
p(c|f, d, e?, a?) =
exp(
?N
n=1 ?nhn(f, d, e?, a?, c))?
c??{M,R} exp(
?N
n=1 ?nhn(f, d, e?, a?, c?))
(2)
Currently, we only use two kinds of features: (1)
the concatenation of the source dependent word with
the dependency relation and (2) the concatenation of
the source head word with the dependency relation.
So for the word yu (English with) in Figure 1, we
extract these features for orientation classification:
prep DEP yu and prep HEAD you.
We define the dependency orientation feature
score for a translation hypothesis as the sum of the
log orientation probabilities for each source word.
This score is used as one feature in the log-linear
formulation of the hierarchical phrase-based model.
Table 1 shows the dependency orientation proba-
bilities for all words in the Figure 1 sentence. Most
interestingly, the orientation probabilities for you
(English have) strongly support global reordering of
one of the few countries with the relative clause that
have diplomatic relations with North Korea. We find
that it is a general trend for long-distance reordering
to gain stronger support, since it is often correlated
with prominent reordering patterns (such as relative
clause and preposition) as well as lexical evidences
(such as ?... zhiyi? (English ?one of ...?)) for which
the reversed orientation takes up the majority of the
training cases.
Consider the following rules (both terminals and
nonterminals are coindexed):
X ? (yu1 Beihan2 you3 bangjiao4,
have3 dipl.4 rels.4 with1 North2 Korea2)
(3)
X ? (yu1 Beihan2 you3 bangjiao4,
with1 North2 Korea2 have3 dipl.4 rels.4)
(4)
According to Table 1, the hypothesis that applies
Rule 3 receives a probability of 0.55 for yu getting
reversed with its head you, as well as 0.88 and 0.83
for translating Beihan and bangjiao in a monotone
manner with respect to their heads. Rule 4 is associ-
ated with probabilities 0.45, 0.88 and 0.83 for mono-
tone translation of yu, Beihan and bangjiao. Thus
our dependency orientation feature is able to trace
the difference in ordering the PP with North Korea
(as underlined) and the VP have dipl. rels. down to
the orientation of the preposition yu (English with)
with respect to its head you (English have), and pro-
mote Rule 3 which has the right word order.
The word you (English have) cannot be scored
in Rules 3 or 4, since its head word zhiyi (English
one of) is not covered. In this case, we say that
the word you is unresolved. We carry an unre-
solved word along in the derivation process until we
reach a terminator hypothesis which translates the
head word. Then the resulting dependency orien-
tation score is added to the terminator hypothesis.
This means that the dependency orientation feature
is ?stateless?, i.e., hypotheses that cover the same
source span with the same orientation information
will receive the same feature score, regardless of the
derivation history. Therefore, Derivation 5 in the fol-
lowing will have the same dependency orientation
score as Derivation (Rule) 3, and Derivation 6 will
score the same as Derivation (Rule) 4.
5.1 X ? (yu1, with1)
5.2 X ? (Beihan1, North1 Korea1)
5.3 X ? (X1 X2, X1 X2)
5.4 X ? (X1 you2 bangjiao3,
have2 dipl.3 rels.3 X1)
(5)
859
6.1 X ? (Beihan1 you2, North1 Korea1 has2)
6.2 X ? (X1 bangjiao2, X1 dipl.2 rels.2)
6.3 X ? (yu1 X2, with1 X2)
(6)
2.2 Cohesion Penalty
When the dependency orientation for a word is
temporarily unavailable (?unresolved?), a cohesion
penalty fires. Cohesion penalty counts the total oc-
currences of unresolved words for a translation hy-
pothesis, which involve newly encountered unre-
solved words as well as old unresolved words car-
ried on from the derivation history. Therefore, the
cohesion penalty is ?stateful?, i.e., an unresolved
word is repeatedly penalized until it gets resolved.
Under this definition, the most cohesive derivation
translates the entire sentence with one rule, where
every word is locally resolved. The least cohe-
sive derivation translates each word individually and
glues word translations together. Consulting Fig-
ure 1, the cohesion penalty in Derivation 5 is 4, since
the word yu (English with) is unresolved twice (in
5.1 and 5.3), and both Beihan (English North Ko-
rea) and you (English have) are unresolved once (in
5.2 and 5.4, respectively); the cohesion penalty in
Derivation 6 is 5: 2 from Beihan (English North
Korea) (in 6.1 and 6.2) and 3 from you (English
have). As a result, Derivation 5 gets promoted,
which echoes with human intuition since Deriva-
tion 5 translates syntactic constituents. To sum
up, our cohesion penalty provides an integer-valued
measure of derivation well-formedness in the hierar-
chical phrase-based MT. Same as dependency orien-
tation, the cohesion penalty is not applicable to the
root word of the sentence.
We propose the cohesion penalty in order to fur-
ther improve reordering, especially in long-distance
cases, since a well-formed derivation at an earlier
stage makes it more likely to explore hierarchical
rules that perform more reliable reordering. In this
respect, the cohesion penalty can be seen as an aid
to the glue rule penalty and as an alternative to
constituency-based constraints.
Specifically, the glue rule penalty (Chiang, 2007)
promotes hierarchical rules. Hierarchical rules
whose lexical evidence helps resolve words locally
will also be favored by our cohesion penalty feature.
However, ignorant of the syntactic structure, the
glue rule penalty may penalize a reasonably cohe-
sive derivation such as Derivation 5 and at the same
time promote a less cohesive hierarchical transla-
tion, such as Derivation 6.
Compared with constituency constraints based on
the phrase structure, our cohesion penalty derived
from the binary dependency parsing has two differ-
ent characteristics.
First, our cohesion penalty is by nature more tol-
erant to some meaningful noncontituent translations.
For example, constituency constraints in (Chiang,
2005; Marton and Resnik, 2008; Chiang et al, 2009)
would penalize Rule 7 below which is useful for
German?English translation (Koehn et al, 2003),
and Rule 8 which can be applied to the Figure 1
sentence. Fuzzy constituency constraints can solve
this problem with a combination of product cate-
gories and slash categories (Chiang, 2010). Yet
our cohesion penalty by nature admits these trans-
lations as cohesive (with no extra cost from es and
Aozhou since both are locally resolved). Admittedly,
our current implementation of the cohesion penalty
is blind to some other meaningful nonconstituent
collocations, such as neighbouring siblings of a
common uncovered head (regulated as the ?floating
structure? in (Shen et al, 2008)). A concrete exam-
ple is Rule 9 which is useful for the Figure 1 sen-
tence. To address this problem, another feature can
be defined in the same manner to capture how each
head word is translated with its children.
X ? (es1 gibt2, there1 is2) (7)
X ? (Aozhou1 shi2, Australia1 is2) (8)
X ? (shaoshu1 guojia2, few1 countries2) (9)
Second, our cohesion penalty can be by na-
ture more discriminative. Compared with the
constituency constraints, the cohesion penalty is
integer-valued, and can be made sensitive to the
depth of each word in the dependency hierarchy (see
Section 2.4). Inspired by (Marton and Resnik, 2008;
Chiang et al, 2009), the cohesion penalty could
also be made sensitive to the dependency relation
of each word. However, this drastically increases
the number of features and requires a tuning algo-
rithm which scales better to high-dimensional model
spaces, such as MIRA (Watanabe et al, 2007; Chi-
ang et al, 2008).
860
pobj
 
?
pobj
dobjprep
 Aus t top
cpm
nummod nn
attr punct
rcmod
?
?? .
?
??
?
??
?? ??
??
 Aus r 
aliwh t 
aliwh r 
aliwh N 
aliwh o 
aliwh K 
Figure 3: Using 2 bins for the dependency parse tree of
the Figure 1 sentence.
2.3 Unaligned Penalty
The dependency orientation and cohesion penalty
cannot be applied to unaligned source words. This
may lead to search error, such as dropping (i.e., un-
aligning) key content words that are important for
lexical translation and reordering. The problem is
mitigated by an unaligned penalty applicable to all
words in the dependency hierarchy.
2.4 Grouping Words into Bins
Having defined dependency orientation, cohesion
penalty and unaligned penalty, we section the source
dependency tree uniformly by depth, group words at
different depths into bins and only add the feature
scores of a word into its respective bin. In this way
one feature is split into several sub-features and each
can be trained discriminatively by MERT.
There are two motivations for binning. The pri-
mary motivation is to distinguish long-distance re-
ordering which is still problematic for the hiero-
style model, since local reorderings generally op-
erate at low levels of the tree while high tree lev-
els tend to take more care of long-distance reorder-
ing. Parsing accuracy is another concern, yet its
impact on feature performance is intricate and our
MaxEnt-trained dependency orientation feature also
buffers against odd parsing. Using bins, we simply
let the tuning process decide how much to trust fea-
ture scores coming from different levels of parsing.
We experiment with 1, 2 and 3 bins. An example
of binning for the Figure 1 sentence can be found in
Figure 3. With 2 bins (hereafter ?bin-2?), words at
Depth 1 and 2 are grouped into Bin 1, and words at
Depth 3, 4, 5 are grouped into Bin 2. As a simple
approach, binning does not take into account how
the tree levels spread out.
3 Experiments
3.1 General Settings
We used a parallel training corpus with 2.1 mil-
lion Chinese?English sentence pairs, aligned by
GIZA++. The Chinese side was parsed by the Stan-
ford Parser. Then we extracted 33.8 million exam-
ples from the parsed Chinese side to discriminatively
train 1.1 million features (using the MegaM soft-
ware2) for dependency orientation classification.
We trained three 5-gram language models with
modified Kneser-Ney smoothing (Kneser and Ney,
1995): one on the English half of the parallel cor-
pus, one on the Xinhua part of the Gigaword corpus,
one on the AFP part, and interpolated them for best
fit to the tuning set (Schwenk and Koehn, 2008).
We used NIST MT06 evaluation data (1664 lines)
as our tuning set, and tested on NIST MT02 (878
lines), MT05 (1082 lines) and MT08 (1357 lines).
Our baseline system was the Moses implemen-
tation of the hierarchical phrase-based model with
standard settings (Hoang et al, 2009). When only
1 bin was used, 3 additional features were added to
the baseline, one each from the soft dependency con-
straints. When we used 2 or 3 bins, the additional
feature counts doubled or tripled. We preserved ter-
minal alignment alongside nonterminal alignment
during the rule extraction and output word align-
ments together with translated strings. Since the fea-
tures we currently define are based entirely on the
source side, we used preprocessing to speed up de-
coding of our feature-augmented model. All experi-
ments were tuned with MERT (Och, 2003).
3.2 Using BLEU as the Tuning Metric
As a standard practice, we first used BLEU (Pap-
ineni et al, 2002) as the objective function for tun-
ing. Table 2 shows the results of the baseline model
as well as our complete feature-augmented model
with different bin numbers. With the ?bin-2? setting,
we get substantial improvement of up to 1.03 BLEU
points (on MT02 data), and 0.84 BLEU points on
average. Using more than one bin (i.e., differentiat-
ing tree depths) is generally beneficial, although the
2http://www.umiacs.umd.edu/?hal/megam/index.html
861
Setting BLEU / LRscore / TERMT02 MT05 MT08 Average
baseline 34.01 / 41.85 / 68.93 32.23 / 40.50 / 68.15 28.09 / 37.17 / 66.82 31.44 / 39.84 / 67.97
bin-2 35.04 / 43.07 / 65.58 33.18 / 41.62 / 65.59 28.63 / 38.12 / 65.36 32.28 / 40.94 / 65.51
baseline-lr 34.23 / 42.06 / 68.08 32.28 / 40.61 / 67.61 27.99 / 37.27 / 66.98 31.50 / 39.98 / 67.56
bin-2-lr 35.42 / 43.25 / 64.82 33.44 / 41.80 / 64.88 29.10 / 38.38 / 64.14 32.65 / 41.14 / 64.61
Table 4: Results for the baseline model and the complete feature-augmented model with 2 bins (?bin-2?), using BLEU
and LRscore (?-lr?) as the tuning function. The BLEU scores of ?bin-2? and ?bin-2-lr? are significantly better than
baseline (p < 0.05), computed by paired bootstrap resampling (Koehn, 2004).
Setting BLEUMT02 MT05 MT08 Average
baseline 34.01 32.23 28.09 31.44
bin-1 34.20 32.13 28.41 31.58(+.14)
bin-2 35.04 33.18 28.63 32.28(+.84)
bin-3 34.35 32.79 28.37 31.84(+.40)
Table 2: Results of the baseline model as well as our
complete feature-augmented model with 1, 2 and 3 bins.
BLEU is the tuning function.
Setting BLEUMT02 MT05 MT08 Average
baseline 34.01 32.23 28.09 31.44
dep 34.26 32.58 28.07 31.64(+.20)
dep+coP 34.47 32.81 28.61 31.96(+.52)
dep+coP+unP 35.04 33.18 28.63 32.28(+.84)
Table 3: Contributions of the three soft dependency con-
straints, with the ?bin-2? setting
problem of overfitting sets in when we use 3 bins
(with slightly higher tuning BLEU, not shown here).
We also studied the effect of adding features in-
crementally onto the baseline with the ?bin-2? set-
ting, as shown in Table 3. On average, all three fea-
tures seem to have similar contributions.
3.3 Using LRscore as the Tuning Metric
Since our features are proposed to address the re-
ordering problem and BLEU is not sensitive enough
to reordering (especially in long-distance cases), we
have also tried tuning with a metric that highlights
reordering, i.e., the LRscore (Birch and Osborne,
2010). LRscore is a linear interpolation of a lexi-
cal metric and a reordering metric. We interpolated
BLEU (as the lexical metric) with the Kendall?s
tau permutation distance (as the reordering metric).
The Kendall?s tau permutation distance measures the
relative word order difference between the transla-
tion output and the reference(s) and is particularly
sensitive to long-distance reordering. Testing re-
sults in terms of BLEU, LRscore and TER (Snover
et al, 2006) are shown in Table 4. Tuned with
the LRscore, our feature-augmented model achieves
further average improvements (compare ?bin-2? and
?bin-2-lr?) of 0.20 LRscore as well as 0.37 BLEU
and 0.90 TER. Note that while the BLEU increase
can largely be seen as a projection of the LRscore
increase back into its lexical component, the consis-
tent TER drop confirms that our improvement is not
metric-specific3. Altogether the final improvement
is 1.21 BLEU, 1.30 LRscore and 3.36 TER on aver-
age over the baseline.
However, an important question is how our fea-
tures affect short, medium and long-distance re-
orderings. In the next section, we conduct quanti-
tative analysis on reordering precision and recall, as
well as qualitative analysis on translation examples.
4 Analysis
4.1 Precision and Recall of Reordering
The key to obtaining precision and recall for reorder-
ing is to investigate whether reorderings in the refer-
ences are reproduced in the translations. We calcu-
late precision as the number of reproduced reorder-
ings divided by the total number of reorderings in
the translation, and recall as the number of repro-
duced reorderings divided by the number of reorder-
3One of our reviewers points out that according to the in-
ductive learning theory, it is counter-intuitive to improve on
BLEU and TER if we optimize by the LRscore. Yet we do
observe some other papers reporting increased TER or other
metric scores when BLEU is used for tuning (Carpuat and Wu,
2007; Shen et al, 2008), suggesting that MT evaluation might
be too complicated to be characterized just with inductive learn-
ing. Similar results based on extensive experiments can also be
found in (Birch and Osborne, 2011).
862
Setting MT02 MT05 MT08 Average
baseline 37.0 35.3 35.6 36.0
bin-2 42.7 40.8 38.7 40.7 (+4.7)
baseline-lr 37.3 35.0 34.2 35.5 (-0.5)
bin-2-lr 44.1 42.0 42.5 42.9 (+6.9)
Table 5: Overall precision for the test sets.
Setting MT02 MT05 MT08 Average
baseline 37.5 36.2 33.2 35.6
bin-2 36.8 35.9 31.8 34.8 (-0.8)
baseline-lr 37.0 35.6 32.2 34.9 (-0.7)
bin-2-lr 37.7 36.7 33.2 35.9 (+0.3)
Table 6: Overall recall for the test sets.
ings in the reference. Then we average the precision
and recall over all four reference translations.
Details of measuring reproduced reordering can
be found in Birch et al (2008). An important dif-
ference in this work is in handling many-to-one and
one-to-many alignments, as we only retain the first
word alignment for any source or target word which
has multiple alignments. This is consistent with our
treatment in dependency orientation classification,
and results in more reorderings being extracted.
From Table 5 we can see that our features im-
prove precision by an average of 4.7 absolute points
when BLEU is used for tuning (?bin-2?). Switch-
ing from BLEU to the LRscore (?bin-2-lr?), we gain
2.2 points more and have a total improvement of 6.9
absolute points on average. This is a novel and im-
portant finding as we directly show that the quality
of reordering has been improved.
From Table 6, we observe a small but consistent
increase in recall with the ?bin-2-lr? setting, averag-
ing 0.3 absolute points. However, the drop of recall
with the ?bin-2? setting (by an average 0.8 points
from the baseline) is unexpected. It seems that when
applying our features alone, we are trading a small
drop in recall for a large gain in precision.
In Figure 4 we break down the precision and re-
call statistics in MT08 by the reordering width on
the source side. We find that our features con-
sistently help precision over all word ranges, with
more substantial improvement in the medium and
long word ranges. When recall is concerned, our
model does not help for short ranges of up to Width
4, but improves consistently for longer distance re-
2 3 4 5 6 7 8 9 10 11 12 13 14 15
baselinebin?2bin?2?lr
Reordering Widths
Prec
ision
0.0
0.1
0.2
0.3
0.4
0.5
0.6
2 3 4 5 6 7 8 9 10 11 12 13 14 15
baselinebin?2bin?2?lr
Reordering Widths
Reca
ll
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 4: Precision and recall breakdown for the source-
side reordering width 2-15 for the NIST MT08 dataset.
orderings. Once again, it seems that the feature-
augmented model is able to benefit from tuning with
a metric that is more sensitive to reordering, as the
performance of ?bin-2-lr? is the best in all reorder-
ing statistics.
4.2 Translation Examples
We observe a number of outputs with improved
word order and more cohesive derivation, as the one
in Figure 5. The baseline translation is fragmented
and requires more glue rule applications. Specifi-
cally, it fails to translate the boxed area as a whole
into ?the relations between the palestinian national
authority (pna) and the european union (eu)?. The
key dependency orientation that controls the global
reordering is between the prepositional modifier dui
(English to) and its head word, the verb gandao (En-
glish feel). The baseline system translates dui (En-
glish to) as ?of the? and misorders the sentence. In
contrast, the feature-augmented model ?bin-2? cap-
863
 AustraliwshhhhhhhhNhhhoKeishhhhhhvAs
ethtwdhiepdhtspdhNhhhoKKeihhhhhtr .stwhhhhfchhhhhhhhhhhhKdt.dda
 AhhhhhhhnApdalhhhzwsysea
BebditsaseahgetbjhoAtwrmst 
e hhhhpsazAhhhhAeabshhhyslrA vdhhhhhlAea s
!fhhhhmdbetsrai
????    , ??? ? ?  ??  ??  ?? ?   ??    ?? ?  ?? ??  ??     . 
leaverhhpea shhhhhhhhhhj
"ddbhhhhhhhhietsi"eutsrahhj
pobp
baseline: [at the same time , abbas] [of the] [palestinian [national authority ( pna )]] [and] [is satisfied
with the [relations] [between [the european union ( eu )]]] [.]
bin-2: [at the same time , abbas] [expressed satisfaction with [the relations between the [palestinian
[national authority ( pna ) [and the european union ( eu )]]]] .]
Figure 5: Example translations from the NIST MT08 set, output by the baseline model and ?bin-2? model. The ?-lr?
version outputs are quite similar and not shown here. Translation outputs are in lower case.
tures the boxed area as a whole and uses Rule 10 to
perform the right global reordering.
X ? (dui1 X2 gandao3 manyi4 .5 ,
expressed3 satisfaction4 with1 X2 .5 ) (10)
5 Related Work
In recent years, there has been a growing body of re-
search on using dependency for statistical machine
translation. Some directly encodes dependency in
the translation model (Ding and Palmer, 2005; Quirk
et al, 2005; Xiong et al, 2007; Shen et al, 2008; Mi
and Liu, 2010), while others use dependency as a
soft constraint (Cherry, 2008; Bach et al, 2009a,b;
Chang et al, 2009). Among them, Shen et al (2008)
report that just filtering the phrase table by the so-
called well-formed target dependency structure does
not help, yet adding a target dependency language
model improves performance significantly. Our in-
tuitive interpretation is that the target dependency
language model capitalizes on two characteristics of
the dependency structure: it is based on words and it
directly connects head and child. Therefore, the tar-
get dependency language model makes good use of
the dependency representation as well as the target
side training data.
We follow the second line of research, and derive
three word-based soft constraints from the source
dependency parsing. Note that although we reuse
the word ?cohesion? to name one of the constraints,
our work is different from (Cherry, 2008; Bach
et al, 2009a,b) which have successfully defined an-
other cohesion constraint from the source depen-
dency structure, with the aim of improving reorder-
ing in phrase-based MT.
To take a glance, Cherry (2008) and Bach et al
(2009b) define cohesion as translating a source de-
pendency subtree contiguously into the target side
without interruption (span or subtree overlapping),
following Fox (2002). This span-based cohesion
constraint has a different criterion from our word-
based cohesion penalty and often leads to opposite
conclusions. Bach et al (2009a) also use cohesion to
correlate with the lexicalized reordering model (Till-
man, 2004; Koehn et al, 2005), whereas we define
an orthogonal dependency orientation feature to ex-
plicitly model head-dependent reordering.
The fundamental difference, however, is rooted
in the translation model. Their span-based cohe-
sion constraint is implemented as an ?interruption
check? to encourage finishing a subtree before trans-
lating something else. This check is very effective
for phrase-based decoding which searches over an
entire space within the distortion limit in order to
advance a hypothesis. In fact, it constrains reorder-
ing for the phrase-based model, as Cherry finds that
the cohesion constraint is used ?primarily to prevent
distortion? and to provide ?an intelligent estimate as
to when source order must be respected? (Cherry,
2008). However, since the hierarchical phrase-
based model already conducts principled reorder-
ing search with rules through the more constrained
chart-decoding, ill-formed derivations exhibit them-
selves more often as nonconstituent translation than
interrupted translation as defined in (Cherry, 2008;
Bach et al, 2009a,b) (They do have a non-empty in-
tersection, but neither subsumes the other). There-
864
fore, our cohesion penalty is better suited for the hi-
erarchical phrase-based model.
To discourage nonconstituent translation, Chiang
(2005) has proposed a constituency feature to exam-
ine whether a source rule span matches the source
constituent as defined by phrase structure parsing.
Finer-grained constituency constraints significantly
improve hierarchical phrase-based MT when ap-
plied on the source side (Marton and Resnik, 2008;
Chiang et al, 2009), or on the target side in a
more tolerant fashion (Zollmann and Venugopal,
2006). Using both source and target syntax, but
relaxing on rule extraction and substitution enables
HPBMT to produce more well-formed and syntac-
tically richer derivations (Chiang, 2010). Softening
constituency matching with latent syntactic distribu-
tions proves to be helpful (Huang et al, 2010). Com-
pared to constituency-based approaches, our cohe-
sion penalty based on the dependency structure nat-
urally supports constituent translations as well as
some nonconstituent translations, if not all of them
(as discussed in Section 2.2).
Our dependency orientation feature is similar to
the order model within dependency treelet trans-
lation (Quirk et al, 2005). Yet instead of a
head-relative position number for each modifier
word, we simply predict the head-dependent ori-
entation which is either monotone or reversed.
Our coarser-grained approach is more robust from
a machine learning perspective, yet still captures
prominent and long-distance reordering patterns ob-
served in Chinese?English (Wang et al, 2007),
German?English (Collins et al, 2005), Japanese?
English (Katz-Brown and Collins, 2008) and trans-
lation from English to a group of SOV lan-
guages (Xu et al, 2009). Not committed to spe-
cific language pairs, we learn orientation classifi-
cation from the word-aligned parallel data through
maximum entropy training as Zens and Ney (2006)
and Chang et al (2009) for phrase-based translation
and Xiong et al (2006) for the BTG model (Wu,
1996). While Chang et al (2009) also make use
of source dependency, their orientation classifica-
tion concerns two subsequent phrase pairs in the left-
to-right phrase-based decoding (as apposed to each
dependent word and its head) and is therefore less
linguistically-motivated.
6 Conclusion
We have derived three novel features from the source
dependency structure for hierarchical phrase-based
MT. They work as a whole to capitalize on two char-
acteristics of the dependency representation: it is di-
rectly based on words and it directly connects head
and child. The effectiveness of our approach has
been demonstrated by a final average improvement
of 1.21 BLEU, 1.30 LRscore and 3.36 TER. On av-
erage we improve reordering precision and recall by
6.9 and 0.3 absolute points, respectively, over the
baseline. Moreover, our approach is found to be es-
pecially effective for long-distance reodering.
As mentioned in Section 2.2, the cohesion penalty
can be extended to also account for how a head
word is translated with its children so that we are
not biased towards one form of cohesive noncon-
stituent translation. All our features can be made
sensitive to the dependency relations or even words.
This fine-grainedness is especially desirable when
we want to reward words for being unaligned or un-
resolved, such as punctuations and function words
in certain context. Word alignment quality is crucial
for the performance of our features as well as the
LRscore which uses word alignment to compute the
permutation distance. As an alternative to GIZA++,
we would like to experiment with syntactically in-
formed aligners that better handle function words
which often exhibit high alignment ambiguity due
to low cross-lingual correspondence.
Finally, since our soft dependency constraints
promote reordering without increasing model com-
plexity, further gains can be achieved when combin-
ing our approach with orthogonal studies to improve
the quantity and quality of hierarchical (reordering)
rules, such as relaxing hierarchical rule extraction
constraints (Setiawan and Resnik, 2010) and selec-
tively lexicalizing rules with function words (Seti-
awan et al, 2009).
Acknowledgments
We would like to thank Miles Osborne, Adam
Lopez, Barry Haddow, Hieu Hoang, Philip Williams
and Michael Auli in the Edinburgh SMT group
as well as Kevin Knight, David Chiang and An-
drew Dai for inspiring discussions. We appreci-
ate Pichuan Chang, Huihsin Tseng, Richard Zens,
Matthew Snover and Nguyen Bach for helping us
865
understand their brilliant work. Many thanks to the
anonymous reviewers for their insightful comments
and suggestions. This work was supported in part
by the EuroMatrixPlus project funded by the Euro-
pean Commission (7th Framework Programme) and
in part under the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-C-0022.
References
Bach, N., Gao, Q., and Vogel, S. (2009a). Source-
side dependency tree reordering models with sub-
tree movements and constraints. In Proceedings of
the Twelfth Machine Translation Summit (MTSummit-
XII), Ottawa, Canada. International Association for
Machine Translation.
Bach, N., Vogel, S., and Cherry, C. (2009b). Cohesive
constraints in a beam search phrase-based decoder. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 1?4, Boul-
der, Colorado.
Birch, A., Blunsom, P., and Osborne, M. (2009). A quan-
titative analysis of reordering phenomena. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 197?205, Athens, Greece.
Birch, A. and Osborne, M. (2010). LRscore for evaluat-
ing lexical and reordering quality in MT. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 327?332, Upp-
sala, Sweden.
Birch, A. and Osborne, M. (2011). Reordering metrics
for mt. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Techologies, pages 1027?1035, Port-
land, Oregon, USA.
Birch, A., Osborne, M., and Koehn, P. (2008). Predict-
ing success in machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 745?754, Honolulu,
Hawaii.
Carpuat, M. and Wu, D. (2007). Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Chang, P.-C., Tseng, H., Jurafsky, D., andManning, C. D.
(2009). Discriminative reordering with Chinese gram-
matical relations features. In Proceedings of the Third
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST-3) at NAACL HLT 2009, pages 51?59,
Boulder, Colorado.
Cherry, C. (2008). Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of ACL-
08: HLT, pages 72?80, Columbus, Ohio.
Chiang, D. (2005). A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA.
Chiang, D. (2007). Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
Chiang, D. (2010). Learning to translate with source and
target syntax. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1443?1452, Uppsala, Sweden.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001
new features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218?226, Boulder, Colorado.
Chiang, D., Marton, Y., and Resnik, P. (2008). Online
large-margin training of syntactic and structural trans-
lation features. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 224?233, Honolulu, Hawaii.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
531?540, Ann Arbor, Michigan.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC-06.
Ding, Y. and Palmer, M. (2005). Machine translation
using probabilistic synchronous dependency insertion
grammars. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 541?548, Ann Arbor, Michigan.
Fox, H. (2002). Phrasal cohesion and statistical machine
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 304?3111.
Galley, M., Hopkins, M., Knight, K., and Marcu, D.
(2004). What?s in a translation rule? In HLT-NAACL
2004: Main Proceedings, pages 273?280, Boston,
Massachusetts, USA.
866
Hayashi, K., Tsukada, H., Sudoh, K., Duh, K., and Ya-
mamoto, S. (2010). Hierarchical phrase-based ma-
chine translation with word-based reordering model.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
439?446, Beijing, China.
Hoang, H., Koehn, P., and Lopez, A. (2009). A unified
framework for phrase-based, hierarchical, and syntax-
based statistical machine translation. In Proceedings
of the International Workshop on Spoken Language
Translation, pages 152?159, Tokyo, Japan.
Huang, Z., Cmejrek, M., and Zhou, B. (2010). Soft syn-
tactic constraints for hierarchical phrase-based trans-
lation using latent syntactic distributions. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 138?147, Cam-
bridge, MA.
Katz-Brown, J. and Collins, M. (2008). Syntactic reorder-
ing in preprocessing for japanese-to-english transla-
tion: Mit system description for ntcir-7 patent transla-
tion task. In Proceedings of NTCIR-7 Workshop Meet-
ing, Tokyo, Japan.
Kneser, R. and Ney, H. (1995). Improved backing-off
for m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Koehn, P. (2004). Statistical significance tests for ma-
chine translation evaluation. In Lin, D. and Wu, D.,
editors, Proceedings of EMNLP 2004, pages 388?395,
Barcelona, Spain. Association for Computational Lin-
guistics.
Koehn, P., Axelrod, A., Birch, A., Callison-burch, C., Os-
borne, M., and Talbot, D. (2005). Edinburgh system
description for the 2005 iwslt speech translation eval-
uation. In Proceedings of IWSLT2005.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal phrase based translation. In Proceedings of the
Joint Conference on Human Language Technologies
and the Annual Meeting of the North American Chap-
ter of the Association of Computational Linguistics
(HLT-NAACL).
Marton, Y. and Resnik, P. (2008). Soft syntactic con-
straints for hierarchical phrased-based translation. In
Proceedings of ACL-08: HLT, pages 1003?1011,
Columbus, Ohio.
Mi, H. and Liu, Q. (2010). Constituency to dependency
translation with forests. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1433?1442, Uppsala, Sweden.
Och, F. J. (2003). Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics (ACL).
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation of
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 311?318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.
Quirk, C., Menezes, A., and Cherry, C. (2005). De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 271?279, Ann Arbor, Michigan.
Schwenk, H. and Koehn, P. (2008). Large and diverse
language models for statistical machine translation. In
Proceedings of International Joint Conference on Nat-
ural Language Processing.
Setiawan, H., Kan, M. Y., Li, H., and Resnik, P. (2009).
Topological ordering of function words in hierarchical
phrase-based translation. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 324?332, Sun-
tec, Singapore.
Setiawan, H. and Resnik, P. (2010). Generalizing hierar-
chical phrase-based translation using rules with adja-
cent nonterminals. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 349?352, Los Angeles, California.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pages 577?585, Columbus,
Ohio.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of As-
sociation for Machine Translation in the Americas.
Tillman, C. (2004). A unigram orientation model for
statistical machine translation. In HLT-NAACL 2004:
Short Papers, pages 101?104, Boston, Massachusetts,
USA.
Tromble, R. and Eisner, J. (2009). Learning linear order-
ing problems for better translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1007?1016, Singapore.
Wang, C., Collins, M., and Koehn, P. (2007). Chinese
syntactic reordering for statistical machine translation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
867
Computational Natural Language Learning (EMNLP-
CoNLL), pages 737?745, Prague, Czech Republic.
Wang, W., May, J., Knight, K., and Marcu, D. (2010).
Re-structuring, re-labeling, and re-aligning for syntax-
based machine translation. Computational Linguistics,
36(2).
Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki, H.
(2007). Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 764?773,
Prague, Czech Republic.
Wu, D. (1996). A polynomial-time algorithm for statis-
tical machine translation. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 152?158, Santa Cruz, California,
USA.
Xiong, D., Liu, Q., and Lin, S. (2006). Maximum en-
tropy based phrase reordering model for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 521?528, Sydney, Australia.
Xiong, D., Liu, Q., and Lin, S. (2007). A dependency
treelet string correspondence model for statistical ma-
chine translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 40?47,
Prague, Czech Republic.
Xu, P., Kang, J., Ringgaard, M., and Och, F. (2009). Us-
ing a dependency parser to improve smt for subject-
object-verb languages. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 245?253, Boulder,
Colorado.
Zens, R. and Ney, H. (2006). Discriminative reordering
models for statistical machine translation. In Proceed-
ings on the Workshop on Statistical Machine Transla-
tion, pages 55?63, New York City.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City.
868
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425?429,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Aligning English Strings with Abstract Meaning Representation Graphs
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{damghani,yanggao,ulf,knight}@isi.edu
Abstract
We align pairs of English sentences and
corresponding Abstract Meaning Repre-
sentations (AMR), at the token level. Such
alignments will be useful for downstream
extraction of semantic interpretation and
generation rules. Our method involves
linearizing AMR structures and perform-
ing symmetrized EM training. We obtain
86.5% and 83.1% alignment F score on de-
velopment and test sets.
1 Introduction
Banarescu et al. (2013) describe a semantics bank
of English sentences paired with their logical
meanings, written in Abstract Meaning Represen-
tation (AMR). The designers of AMR leave open
the question of how meanings are derived from
English sentences (and vice-versa), so there are
no manually-annotated alignment links between
English words and AMR concepts. This paper
studies how to build such links automatically, us-
ing co-occurrence and other information. Auto-
matic alignments may be useful for downstream
extraction of semantic interpretation and genera-
tion rules.
AMRs are directed, acyclic graphs with labeled
edges, e.g., the sentence The boy wants to go is
represented as:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
We have hand-aligned a subset of the 13,050
available AMR/English pairs. We evaluate our
automatic alignments against this gold standard.
A sample hand-aligned AMR is here (??n? speci-
fies a link to the nth English word):
the boy wants to go
(w / want-01?3
:arg0 (b / boy?2)
:arg1 (g / go-01?5
:arg0 b))
This alignment problem resembles that of statisti-
cal machine translation (SMT). It is easier in some
ways, because AMR and English are highly cog-
nate. It is harder in other ways, as AMR is graph-
structured, and children of an AMR node are un-
ordered. There are also fewer available training
pairs than in SMT.
One approach is to define a generative model
from AMR graphs to strings. We can then use
EM to uncover hidden derivations, which align-
ments weakly reflect. This approach is used in
string/string SMT (Brown et al., 1993). How-
ever, we do not yet have such a generative graph-
to-string model, and even if we did, there might
not be an efficient EM solution. For exam-
ple, in syntax-based SMT systems (Galley et al.,
2004), the generative tree/string transduction story
is clear, but in the absence of alignment con-
straints, there are too many derivations and rules
for EM to efficiently consider.
We therefore follow syntax-based SMT custom
and use string/string alignment models in align-
ing our graph/string pairs. However, while it is
straightforward to convert syntax trees into strings
data (by taking yields), it is not obvious how to do
this for unordered AMR graph elements. The ex-
ample above also shows that gold alignment links
reach into the internal nodes of AMR.
Prior SMT work (Jones et al., 2012) describes
alignment of semantic graphs and strings, though
their experiments are limited to the GeoQuery do-
main, and their methods are not described in de-
tail. Flanigan et al (2014) describe a heuristic
AMR/English aligner. While heuristic aligners
can achieve good accuracy, they will not automat-
ically improve as more AMR/English data comes
425
online.
The contributions of this paper are:
? A set of gold, manually-aligned
AMR/English pairs.
? An algorithm for automatically aligning
AMR/English pairs.
? An empirical study establishing alignment
accuracy of 86.5% and 83.1% F score for de-
velopment and test sets respectively.
2 Method
We divide the description of our method into three
parts: preprocessing, training, and postprocessing.
In the preprocessing phase, we linearize the AMR
graphs to change them into strings, clean both the
AMR and English sides by removing stop words
and simple stemming, and add a set of correspond-
ing AMR/English token pairs to the corpus to help
the training phase. The training phase is based
on IBM models, but we modify the learning algo-
rithm to learn the parameters symmetrically. Fi-
nally, in the postprocessing stage we rebuild the
aligned AMR graph. These components are de-
scribed in more detail below.
2.1 Preprocessing
The first step of the preprocessing component is to
linearize the AMR structure into a string. In this
step we record the original structure of nodes in
the graph for later reconstruction of AMR. AMR
has a rooted graph structure. To linearize this
graph we run a depth first search from the root and
print each node as soon as it it visited. We print
but not expand the nodes that are seen previously.
For example the AMR:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
is linearized into this order: w / want-01 :arg0 b /
boy :arg1 g / go-01 :arg0 b.
Note that semantically related nodes often stay
close together after linearization.
After linearizing the AMR graph into a string,
we perform a series of preprocessing steps includ-
ing lowercasing the letters, removing stop words,
and stemming.
The AMR and English stop word lists are gen-
erated based on our knowledge of AMR design.
We know that tokens like an, the or to be verbs
will very rarely align to any AMR token; similarly,
AMR role tokens like :arg0, :quant, :opt1 etc. as
well as the instance-of token /, and tokens like
temporal-quantity or date-entity rarely align to any
English token. We remove these tokens from the
parallel corpus, but remember their position to be
able to convert the resulting string/string align-
ment back into a full AMR graph/English string
alignment. Although some stopwords participate
in gold alignments, by removing them we will buy
a large precision gain for some recall cost.
We remove the word sense indicator and quo-
tation marks for AMR concepts. For instance we
will change want-01 to want and ?ohio? to ohio.
Then we stem AMR and English tokens into their
first four letters, except for role tokens in AMR.
The purpose of stemming is to normalize English
morphological variants so that they are easier to
match to AMR tokens. For example English to-
kens wants, wanting, wanted, and want as well as
the AMR token want-01 will all convert to want
after removing the AMR word sense indicator and
stemming.
In the last step of preprocessing, we benefit
from the fact that AMR concepts and their cor-
responding English ones are frequently cognates.
Hence, after stemming, an AMR token often can
be translated to a token spelled similarly in En-
glish. This is the case for English token want and
AMR token want in the previous paragraph. To
help the training model learn from this fact, we
extend our sentence pair corpus with the set of
AMR/English token pairs that are spelled identi-
cally after preprocessing. Also, for English tokens
that can be translated into multiple AMR tokens,
like higher and high :degree more we add the cor-
responding string/string pairs to the corpus. This
set is extracted from existing lexical resources, in-
cluding lists of comparative/superlative adjectives,
negative words, etc.
After preprocessing, the AMR at the start of
this section will change into: want boy go and
the sentence The boy wants to go changes into boy
want to go, and we will also add the identity pairs
want/want, boy/boy, and go/go to the corpus.
2.2 Training
Our training method is based on IBM word align-
ment models (Brown et al., 1993). We modify
the objective functions of the IBM models to en-
426
courage agreement between learning parameters
in English-to-AMR and AMR-to-English direc-
tions of EM. The solution of this objective func-
tion can be approximated in an extremely simple
way that requires almost no extra coding effort.
Assume that we have a set of sentence pairs
{(E,A)}, where each E is an English sentence
and each A is a linearized AMR. According to
IBM models, A is generated from E through a
generative story based on some parameters.
For example, in IBM Model 2, given E we
first decide the length of A based on some prob-
ability l = p(len(A)|len(E)), then we decide
the distortions based on a distortion table: d =
p(i|j, len(A), len(E)). Finally, we translate En-
glish tokens into AMR ones based on a translation
table t = p(a|e) where a and e are AMR and En-
glish tokens respectively.
IBM models estimate these parameters to max-
imize the conditional likelihood of the data:
?
A|E
= argmaxL
?
A|E
(A|E) or ?
E|A
=
argmaxL
?
E|A
(E|A) where ? denotes the set of
parameters. The conditional likelihood is intrinsic
to the generative story of IBM models. However,
word alignment is a symmetric problem. Hence it
is more reasonable to estimate the parameters in a
more symmetric manner.
Our objective function in the training phase is:
?
A|E
, ?
E|A
= argmaxL
?
A|E
(A|E)+L
?
E|A
(E|A)
subject to ?
A|E
?
E
= ?
E|A
?
A
= ?
A,E
We approximate the solution of this objective
function with almost no change to the existing
implementation of the IBM models. We relax
the constraint to ?
A|E
= ?
E|A
, then apply the
following iterative process:
1. Optimize the first part of the objective func-
tion: ?
A|E
= argmaxL
?
A|E
(A|E) using EM
2. Satisfy the constraint: set ?
E|A
? ?
A|E
3. Optimize the second part of the objective
function: ?
E|A
= argmaxL
?
E|A
(E|A)
using EM
4. Satisfy the constraint: set ?
A|E
? ?
E|A
5. Iterate
Note that steps 1 and 3 are nothing more than
running the IBM models, and steps 2 and 4 are
just initialization of the EM parameters, using ta-
bles from the previous iteration. The initialization
steps only make sense for the parameters that in-
volve both sides of the alignment (i.e., the transla-
tion table and the distortion table). For the trans-
lation table we set t
E|A
(e|a) = t
A|E
(a|e) for En-
glish and AMR tokens e and a and then normalize
the t table. The distortion table can also be initial-
ized in a similar manner. We initialize the fertility
table with its value in the previous iteration.
Previously Liang et al. (2006) also presented a
symmetric method for training alignment parame-
ters. Similar to our work, their objective function
involves summation of conditional likelihoods in
both directions; however, their constraint is on
agreement between predicted alignments while we
directly focus on agreement between the parame-
ters themselves. Moreover their method involves a
modification of the E step of EM algorithm which
is very hard to implement for IBM Model 3 and
above.
After learning the parameters, alignments are
computed using the Viterbi algorithm in both di-
rections of the IBM models. We tried merging
the alignments of the two directions using meth-
ods like grow-diag-final heuristic or taking inter-
section of the alignments and adding some high
probability links in their union. But these methods
did not help the alignment accuracy.
2.3 Postprocessing
The main goal of the postprocessing component is
to rebuild the aligned AMR graph. We first insert
words removed as stop words into their positions,
then rebuild the graph using the recorded original
structure of the nodes in the AMR graph.
We also apply a last modification to the align-
ments in the postprocessing. Observing that pairs
like worker and person :arg0-of work-01 appear
frequently, and in all such cases, all the AMR to-
kens align to the English one, whenever we see
any of AMR tokens person, product, thing or com-
pany is followed by arg0-of, arg1-of or arg2-of
followed by an AMR concept, we align the two
former tokens to what the concept is aligned to.
3 Experiments
3.1 Data Description
Our data consists of 13,050 publicly available
AMR/English sentence pairs
1
. We have hand
1
LDC AMR release 1.0, Release date: June 16, 2014
https://catalog.ldc.upenn.edu/LDC2014T12
427
aligned 200 of these pairs to be used as develop-
ment and test sets
2
. We train the parameters on
the whole data. Table 1 presents a description of
the data. We do not count parenthesis, slash and
AMR variables as AMR tokens. Role tokens are
those AMR tokens that start with a colon. They
do not represent any concept, but provide a link
between concepts. For example in:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
the first :arg0 states that the first argument of the
concept wanting is the boy and the second argu-
ment is going.
train dev test
Sent. pairs 13050 100 100
AMR tokens 465 K 3.8 K (52%) 2.3 K (%55)
AMR role tokens 226 K 1.9 K (23%) 1.1 K (%22)
ENG tokens 248 K 2.3 K (76%) 1.7 K (%74)
Table 1: AMR/English corpus. The number in
parentheses is the percent of the tokens aligned in
gold annotation. Almost half of AMR tokens are
role tokens, and less than a quarter of role tokens
are aligned.
3.2 Experiment Results
We use MGIZA++ (Gao and Vogel, 2008) as
the implementation of the IBM models. We run
Model 1 and HMM for 5 iterations each, then run
our training algorithm on Model 4 for 4 iterations,
at which point the alignments become stable. As
alignments are usually many to one from AMR to
English, we compute the alignments from AMR to
English in the final step.
Table 2 shows the alignment accuracy for
Model 1, HMM, Model 4, and Model 4 plus the
modification described in section 2.2 (Model 4+).
The alignment accuracy on the test set is lower
than the development set mainly because it is in-
trinsically a harder set, as we only made small
modifications to the system based on the develop-
ment set. Recall error due to stop words is one
difference.
2
The development and test AMR/English pairs can be
found in /data/split/dev/amr-release-1.0-dev-consensus.txt
and /data/split/test/amr-release-1.0-test-consensus.txt, re-
spectively. The gold alignments are not included in these
files but are available separately.
model precision recall F score
Dev
Model 1 70.9 71.1 71.0
HMM 87.6 80.1 83.7
Model 4 89.7 80.4 84.8
Model 4+ 94.1 80.0 86.5
Test
Model 1 74.8 71.8 73.2
HMM 83.8 73.8 78.5
Model 4 85.8 74.9 80.0
Model 4+ 92.4 75.6 83.1
Table 2: Results on different models. Our training
method (Model 4+) increases the F score by 1.7
and 3.1 points on dev and test sets respectively.
Table 3 breaks down precision, recall, and
F score for role and non-role AMR tokens, and
also shows in parentheses the amount of recall er-
ror that was caused by removing either side of the
alignment as a stop word.
token type precision recall F score
Dev
role 77.1 48.7 59.7
non-role 97.2 88.2 92.5
all 94.1 80.0 (34%) 86.5
Test
role 71.0 37.8 49.3
non-role 95.5 84.7 89.8
all 92.4 75.6 (36%) 83.1
Table 3: Results breakdown into role and non-
role AMR tokens. The numbers in the parentheses
show the percent of recall errors caused by remov-
ing aligned tokens as stop words.
While the alignment method works very well on
non-role tokens, it works poorly on the role tokens.
Role tokens are sometimes matched with a word
or part of a word in the English sentence. For ex-
ample :polarity is matched with the un part of the
word unpopular, :manner is matched with most
adverbs, or even in the pair:
thanks
(t / thank-01
:arg0 (i / i)
:arg1 (y / you))
all AMR tokens including :arg0 and :arg1 are
matched to the only English word thanks. Incon-
sistency in aligning role tokens has made this a
hard problem even for human experts.
428
4 Conclusions and Future Work
In this paper we present the first set of manually
aligned English/AMR pairs, as well as the first
published system for learning the alignments be-
tween English sentences and AMR graphs that
provides a strong baseline for future research in
this area. As the proposed system learns the
alignments automatically using very little domain
knowledge, it can be applied in any domain and
for any language with minor adaptations.
Computing the alignments between English
sentences and AMR graphs is a first step for ex-
traction of semantic interpretation and generation
rules. Hence, a natural extension to this work
will be automatically parsing English sentences
into AMR and generating English sentences from
AMR.
Acknowledgments
This work was supported by DARPA con-
tracts HR0011-12-C-0014 and FA-8750-13-2-
0045. The authors would like to thank David Chi-
ang, Tomer Levinboim, and Ashish Vaswani (in
no particular order) for their comments and sug-
gestions.
References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Linguistic Annotation Workshop
(LAW VII-ID), ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing Workshop, ACL.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
429
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1723?1732,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Deceptive Answer Prediction with User Preference Graph
Fangtao Li?, Yang Gao?, Shuchang Zhou??, Xiance Si?, and Decheng Dai?
?Google Research, Mountain View
?State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS
{lifangtao,georgezhou,sxc,decheng}@google.com
?Department of Computer Science and Technology, Tsinghua University
gao young@163.com
Abstract
In Community question answering (QA)
sites, malicious users may provide decep-
tive answers to promote their products or
services. It is important to identify and fil-
ter out these deceptive answers. In this
paper, we first solve this problem with
the traditional supervised learning meth-
ods. Two kinds of features, including tex-
tual and contextual features, are investi-
gated for this task. We further propose
to exploit the user relationships to identify
the deceptive answers, based on the hy-
pothesis that similar users will have simi-
lar behaviors to post deceptive or authentic
answers. To measure the user similarity,
we propose a new user preference graph
based on the answer preference expressed
by users, such as ?helpful? voting and
?best answer? selection. The user prefer-
ence graph is incorporated into traditional
supervised learning framework with the
graph regularization technique. The ex-
periment results demonstrate that the user
preference graph can indeed help improve
the performance of deceptive answer pre-
diction.
1 Introduction
Currently, Community QA sites, such as Yahoo!
Answers1 and WikiAnswers2, have become one of
the most important information acquisition meth-
ods. In addition to the general-purpose web search
engines, the Community QA sites have emerged as
popular, and often effective, means of information
seeking on the web. By posting questions for other
participants to answer, users can obtain answers
to their specific questions. The Community QA
1http://answers.yahoo.com
2http://wiki.answers.com
sites are growing rapidly in popularity. Currently
there are hundreds of millions of answers and mil-
lions of questions accumulated on the Community
QA sites. These resources of past questions and
answers are proving to be a valuable knowledge
base. From the Community QA sites, users can di-
rectly get the answers to meet some specific infor-
mation need, rather than browse the list of returned
documents to find the answers. Hence, in recent
years, knowledge mining in Community QA sites
has become a popular topic in the field of artifi-
cial intelligence (Adamic et al, 2008; Wei et al,
2011).
However, some answers may be deceptive. In
the Community QA sites, there are millions of
users each day. As the answers can guide the
user?s behavior, some malicious users are moti-
vated to give deceptive answers to promote their
products or services. For example, if someone
asks for recommendations about restaurants in the
Community QA site, the malicious user may post a
deceptive answer to promote the target restaurant.
Indeed, because of lucrative financial rewards, in
several Community QA sites, some business own-
ers provide incentives for users to post deceptive
answers for product promotion.
There are at least two major problems that the
deceptive answers cause. On the user side, the
deceptive answers are misleading to users. If
the users rely on the deceptive answers, they will
make the wrong decisions. Or even worse, the pro-
moted link may lead to illegitimate products. On
the Community QA side, the deceptive answers
will hurt the health of the Community QA sites. A
Community QA site without control of deceptive
answers could only benefit spammers but could
not help askers at all. If the asker was cheated by
the provided answers, he will not trust and visit
this site again. Therefore, it is a fundamental task
to predict and filter out the deceptive answers.
In this paper, we propose to predict deceptive
1723
answer, which is defined as the answer, whose pur-
pose is not only to answer the question, but also
to promote the authors? self-interest. In the first
step, we consider the deceptive answer prediction
as a general binary-classification task. We extract
two types of features: one is textual features from
answer content, including unigram/bigram, URL,
phone number, email, and answer length; the other
is contextual features from the answer context, in-
cluding the relevance between answer and the cor-
responding question, the author of the answer, an-
swer evaluation from other users and duplication
with other answers. We further investigate the user
relationship for deceptive answer prediction. We
assume that similar users tend to have similar be-
haviors, i.e. posting deceptive answers or post-
ing authentic answers. To measure the user rela-
tionship, we propose a new user preference graph,
which is constructed based on the answer evalu-
ation expressed by users, such as ?helpful? vot-
ing and ?best answer? selection. The user prefer-
ence graph is incorporated into traditional super-
vised learning framework with graph regulariza-
tion, which can make answers, from users with
same preference, tend to have the same category
(deceptive or authentic). The experiment results
demonstrate that the user preference graph can fur-
ther help improve the performance for deceptive
answer prediction.
2 Related Work
In the past few years, it has become a popular task
to mine knowledge from the Community QA sites.
Various studies, including retrieving the accumu-
lated question-answer pairs to find the related an-
swer for a new question, finding the expert in a
specific domain, summarizing single or multiple
answers to provide a concise result, are conducted
in the Community QA sites (Jeon et al, 2005;
Adamic et al, 2008; Liu et al, 2008; Song et
al., 2008; Si et al, 2010a; Figueroa and Atkin-
son, 2011). However, an important issue which
has been neglected so far is the detection of decep-
tive answers. If the acquired question-answer cor-
pus contains many deceptive answers, it would be
meaningless to perform further knowledge mining
tasks. Therefore, as the first step, we need to pre-
dict and filter out the deceptive answers. Among
previous work, answer quality prediction (Song et
al., 2010; Harper et al, 2008; Shah and Pomer-
antz, 2010; Ishikawa et al, 2010) is most related to
the deceptive answer prediction task. But these are
still significant differences between two tasks. An-
swer quality prediction measures the overall qual-
ity of the answers, which refers to the accuracy,
readability, completeness of the answer. While
the deceptive answer prediction aims to predict if
the main purpose of the provided answer is only
to answer the specific question, or includes the
user?s self-interest to promote something. Some
of the previous work (Song et al, 2010; Ishikawa
et al, 2010; Bian et al, 2009) views the ?best
answer? as high quality answers, which are se-
lected by the askers in the Community QA sites.
However, the deceptive answer may be selected as
high-quality answer by the spammer, or because
the general users are mislead. Meanwhile, some
answers from non-native speakers may have lin-
guistic errors, which are low-quality answers, but
are still authentic answers. Our experiments also
show that answer quality prediction is much dif-
ferent from deceptive answer prediction.
Previous QA studies also analyze the user graph
to investigate the user relationship (Jurczyk and
Agichtein, 2007; Liu et al, 2011). They mainly
construct the user graph with asker-answerer rela-
tionship to estimate the expertise score in Commu-
nity QA sites. They assume the answerer is more
knowledgeable than the asker. However, we don?t
care which user is more knowledgeable, but are
more likely to know if two users are both spam-
mers or authentic users. In this paper, we pro-
pose a novel user preference graph based on their
preference towards the target answers. We assume
that the spammers may collaboratively promote
the target deceptive answers, while the authen-
tic users may generally promote the authentic an-
swers and demote the deceptive answers. The user
preference graph is constructed based on their an-
swer evaluation, such as ?helpful? voting or ?best
answer? selection.
3 Proposed Features
We first view the deceptive answer prediction as a
binary-classification problem. Two kinds of fea-
tures, including textual features and contextual
features, are described as follows:
3.1 Textual Features
We first aim to predict the deceptive answer by an-
alyzing the answer content. Several textual fea-
tures are extracted from the answer content:
3.1.1 Unigrams and Bigrams
The most common type of feature for text classi-
fication is the bag-of-word. We use an effective
1724
feature selection method ?2 (Yang and Pedersen,
1997) to select the top 200 unigrams and bigrams
as features. The top ten unigrams related to decep-
tive answers are shown on Table 1. We can see that
these words are related to the intent for promotion.
professional service advice address
site telephone therapy recommend
hospital expert
Table 1: Top 10 Deceptive Related Unigrams
3.1.2 URL Features
Some malicious users may promote their products
by linking a URL. We find that URL is good indi-
cator for deceptive answers. However, some URLs
may provide the references for the authentic an-
swers. For example, if you ask the weather in
mountain view, someone may just post the link
to ?http://www.weather.com/?. Therefore, besides
the existence of URL, we also use the following
URL features:
1). Length of the URLs: we observe that the
longer urls are more likely to be spam.
2). PageRank Score: We employ the PageRank
(Page et al, 1999) score of each URL as popularity
score.
3.1.3 Phone Numbers and Emails
There are a lot of contact information mentioned
in the Community QA sites, such as phone num-
bers and email addresses, which are very likely to
be deceptive, as good answers are found to be less
likely to refer to phone numbers or email addresses
than the malicious ones. We extract the number of
occurrences of email and phone numbers as fea-
tures.
3.1.4 Length
We have also observed some interesting patterns
about the length of answer. Deceptive ones tend
to be longer than authentic ones. This can be ex-
plained as the deceptive answers may be well pre-
pared to promote the target. We also employ the
number of words and sentences in the answer as
features.
3.2 Contextual Features
Besides the answer textual features, we further in-
vestigate various features from the context of the
target answer:
3.2.1 Question Answer Relevance
The main characteristic of answer in Community
QA site is that the answer is provided to answer
the corresponding question. We can use the corre-
sponding question as one of the context features by
measuring the relevance between the answer and
the question. We employ three different models
for Question-Answer relevance:
Vector Space Model
Each answer or question is viewed as a word
vector. Given a question q and the answer a, our
vector model uses weighted word counts(e.g.TF-
IDF) as well as the cosine similarity (q ? a) of
their word vectors as relevant function (Salton and
McGill, 1986). However, vector model only con-
sider the exact word match, which is a big prob-
lem, especially when the question and answer are
generally short compared to the document. For ex-
ample, Barack Obama and the president of the US
are the same person. But the vector model would
indicate them to be different. To remedy the word-
mismatch problem, we also look for the relevance
models in higher semantic levels.
Translation Model
A translation model is a mathematical model in
which the language translation is modeled in a sta-
tistical way. The probability of translating a source
sentence (as answer here) into target sentence (as
question here) is obtained by aligning the words
to maximize the product of all the word probabil-
ities. We train a translation model (Brown et al,
1990; Och and Ney, 2003) using the Community
QA data, with the question as the target language,
and the corresponding best answer as the source
language. With translation model, we can com-
pute the translation score for new question and an-
swer.
Topic Model
To reduce the false negatives of word mismatch
in vector model, we also use the topic models to
extend matching to semantic topic level. The topic
model, such as Latent Dirichlet Allocation (LDA)
(Blei et al, 2003), considers a collection of doc-
uments with K latent topics, where K is much
smaller than the number of words. In essence,
LDA maps information from the word dimen-
sion to a semantic topic dimension, to address the
shortcomings of the vector model.
3.2.2 User Profile Features
We extract several user?s activity statistics to con-
struct the user profile features, including the level
1725
of the user in the Community QA site, the number
of questions asked by this user, the number of an-
swers provided by this user, and the best answer
ratio of this user.
3.2.3 User Authority Score
Motivated by expert finding task (Jurczyk and
Agichtein, 2007; Si et al, 2010a; Li et al, 2011),
the second type of author related feature is author-
ity score, which denotes the expertise score of this
user. To compute the authority score, we first con-
struct a directed user graph with the user interac-
tions in the community. The nodes of the graph
represent users. An edge between two users in-
dicates a contribution from one user to the other.
Specifically, on a Q&A site, an edge from A to
B is established when user B answered a question
asked by A, which shows user B is more likely to
be an expert than A. The weight of an edge indi-
cates the number of interactions. We compute the
user?s authority score (AS) based on the link anal-
ysis algorithm PageRank:
AS(ui) =
1? d
N + d
?
uj?M(ui)
AS(uj)
L(uj)
(1)
where u1, . . . , uN are the users in the collection,
N is the total number of users, M(ui) is the set
of users whose answers are provided by user ui,
L(ui) is the number of users who answer ui?s
questions, d is a damping factor, which is set as
0.85. The authority score can be computed itera-
tively with random initial values.
3.2.4 Robot Features
The third type of author related feature is used for
detecting whether the author is a robot, which are
scripts crafted by malicious users to automatically
post answers. We observe that the distributions of
the answer-posting time are very different between
general user and robot. For example, some robots
may make posts continuously and mechanically,
hence the time increment may be smaller that hu-
man users who would need time to think and pro-
cess between two posts. Based on this observa-
tion, we design an time sequence feature for robot
detection. For each author, we can get a list of
time points to post answers, T = {t0, t1, ..., tn},
where ti is the time point when posting the ith an-
swer. We first convert the time sequence T to time
interval sequence ?T = {?t0,?t1, ...,?tn?1},
where ?ti = ti+1 ? ti. Based on the interval
sequences for all users, we then construct a ma-
trix Xm?b whose rows correspond to users and
columns correspond to interval histogram with
predefined range. We can use each row vector as
time sequence pattern to detect robot. To reduce
the noise and sparse problem, we use the dimen-
sion reduction techniques to extract the latent se-
mantic features with Singular Value Decomposi-
tion (SVD) (Deerwester et al, 1990; Kim et al,
2006).
3.2.5 Evaluation from Other Users
In the Community QA sites, other users can ex-
press their opinions or evaluations on the answer.
For example, the asker can choose one of the an-
swers as best answer. We use a bool feature to de-
note if this answer is selected as the best answer.
In addition, other users can label each answer as
?helpful? or ?not helpful?. We also use this helpful
evaluation by other users as the contextual feature,
which is defined as the ratio between the number
of ?helpful? votes and the number of total votes.
3.2.6 Duplication with Other Answers
The malicious user may post the pre-written prod-
uct promotion documents to many answers, or just
change the product name. We also compute the
similarity between different answers. If the two
answers are totally same, but the question is differ-
ent, these answer is potentially as a deceptive an-
swer. Here, we don?t want to measure the semantic
similarity between two answers, but just measure
if two answers are similar to the word level, there-
fore, we apply BleuScore (Papineni et al, 2002),
which is a standard metric in machine translation
for measuring the overlap between n-grams of two
text fragments r and c. The duplication score of
each answer is the maximum BleuScore compared
to all other answers.
4 Deceptive Answer Prediction with User
Preference Graph
Besides the textual and contextual features, we
also investigate the user relationship for decep-
tive answer prediction. We assume that similar
users tend to perform similar behaviors (posting
deceptive answers or posting authentic answers).
In this section, we first show how to compute the
user similarity (user preference graph construc-
tion), and then introduce how to employ the user
relationship for deceptive answer prediction.
4.1 User Preference Graph Construction
In this section, we propose a new user graph to de-
scribe the relationship among users. Figure 1 (a)
shows the general process in a question answering
1726
Question 
Answer1 
Answer2 
Best Answer 
u1 
u2 
u3 
u4 
u5 
u6 
(a) Question Answering (b) User Preference Relation (c) User Preference Graph
Figure 1: User Preference Graph Construction
thread. The asker, i.e. u1, asks a question. Then,
there will be several answers to answer this ques-
tion from other users, for example, answerers u2
and u3. After the answers are provides, users can
also vote each answer as ?helpful? or ?not help-
ful? to show their evaluation towards the answer .
For example, users u4, u5 vote the first answer as
?not helpful?, and user u6 votes the second answer
as ?helpful?. Finally, the asker will select one an-
swer as the best answer among all answers. For
example, the asker u1 selects the first answer as
the ?best answer?.
To mine the relationship among users, previous
studies mainly focus on the asker-answerer rela-
tionship (Jurczyk and Agichtein, 2007; Liu et al,
2011). They assume the answerer is more knowl-
edgeable than the asker. Based on this assump-
tion, they can extract the expert in the commu-
nity, as discussed in Section 3.2.3. However, we
don?t care which user is more knowledgeable, but
are more interested in whether two users are both
malicious users or authentic users. Here, we pro-
pose a new user graph based on the user prefer-
ence. The preference is defined based on the an-
swer evaluation. If two users show same pref-
erence towards the target answer, they will have
the user-preference relationship. We mainly use
two kinds of information: ?helpful? evaluation and
?best answer? selection. If two users give same
?helpful? or ?not helpful? to the target answer, we
view these two users have same user preference.
For example, user u4 and user u5 both give ?not
helpful? evaluation towards the first answer, we
can say that they have same user preference. Be-
sides the real ?helpful? evaluation, we also assume
the author of the answer gives the ?helpful? evalu-
ation to his or her own answer. Then if user u6 give
?helpful? evaluation to the second answer, we will
view user u6 has same preference as user u3, who
is the author of the second answer. We also can ex-
tract the user preference with ?best answer? selec-
tion. If the asker selects the ?best answer? among
all answers, we will view that the asker has same
preference as the author of the ?best answer?. For
example, we will view user u1 and user u2 have
same preference.
Based on the two above assumptions, we can
extract three user preference relationships (with
same preference) from the question answering ex-
ample in Figure 1 (a): u4 ? u5, u3 ? u6, u1 ? u2,
as shown in Figure1 (b). After extracting all user
preference relationships, we can construct the user
preference graph as shown in Figure 1 (c). Each
node represents a user. If two users have the user
preference relationship, there will be an edge be-
tween them. The edge weight is the number of
user preference relationships.
In the Community QA sites, the spammers
mainly promote their target products by promoting
the deceptive answers. The spammers can collab-
oratively make the deceptive answers look good,
by voting them as high-quality answer, or select-
ing them as ?best answer?. However, the authen-
tic users generally have their own judgements to
the good and bad answers. Therefore, the evalu-
ation towards the answer reflects the relationship
among users. Although there maybe noisy rela-
tionship, for example, an authentic user may be
cheated, and selects the deceptive answer as ?best
answer?, we hope the overall user preference rela-
tion can perform better results than previous user
interaction graph for this task.
1727
4.2 Incorporating User Preference Graph
To use the user graph, we can just compute the
feature value from the graph, and add it into the
supervised method as the features introduced in
Section 3. Here, we propose a new technique to
employ the user preference graph. We utilize the
graph regularizer (Zhang et al, 2006; Lu et al,
2010) to constrain the supervised parameter learn-
ing. We will introduce this technique based on
a commonly used model f(?), the linear weight
model, where the function value is determined by
linear combination of the input features:
f(xi) = wT ? xi =
?
k
wk ? xik (2)
where xi is a K dimension feature vector for the
ith answer, the parameter value wk captures the
effect of the kth feature in predicting the deceptive
answer. The best parameters w? can be found by
minimizing the following objective function:
?1(w) =
?
i
L(wTxi, yi) + ? ? |w|2F (3)
where L(wTxi, yi) is a loss function that mea-
sures discrepancy between the predicted label
wT ? xi and the true label yi, where yi ?
{+1,?1}. The common used loss functions in-
clude L(p, y) = (p?y)2 (least square), L(p, y) =
ln (1 + exp (?py)) (logistic regression). For sim-
plicity, here we use the least square loss function.
|w|2F =
?
k w2k is a regularization term defined
in terms of the Frobenius norm of the parameter
vector w and plays the role of penalizing overly
complex models in order to avoid fitting.
We want to incorporate the user preference re-
lationship into the supervised learning framework.
The hypothesis is that similar users tend to have
similar behaviors, i.e. posting deceptive answers
or authentic answers. Here, we employ the user
preference graph to denote the user relationship.
Based on this intuition, we propose to incorporate
the user graph into the linear weight model with
graph regularization. The new objective function
is changed as:
?2(w) =
?
i
L(wTxi, yi) + ? ? |w|2F +
?
?
ui,uj?Nu
?
x?Aui ,y?Auj
wui,uj (f(x)? f(y))2 (4)
where Nu is the set of neighboring user pairs in
user preference graph, i.e, the user pairs with same
preference. Aui is the set of all answers posted by
user ui. wui,uj is the weight of edge between ui
and uj in user preference graph. In the above ob-
jective function, we impose a user graph regular-
ization term
?
?
ui,uj?Nu
?
x?Aui ,y?Auj
wui,uj (f(x)? f(y))2
to minimize the answer authenticity difference
among users with same preference. This regu-
larization term smoothes the labels on the graph
structure, where adjacent users with same prefer-
ence tend to post answers with same label.
5 Experiments
5.1 Experiment Setting
5.1.1 Dataset Construction
In this paper, we employ the Confucius (Si et
al., 2010b) data to construct the deceptive an-
swer dataset. Confucius is a community question
answering site, developed by Google. We first
crawled about 10 million question threads within
a time range. Among these data, we further sam-
ple a small data set, and ask three trained annota-
tors to manually label the answer as deceptive or
not. If two or more people annotate the answer as
deceptive, we will extract this answer as a decep-
tive answer. In total, 12446 answers are marked
as deceptive answers. Similarly, we also manu-
ally annotate 12446 authentic answers. Finally,
we get 24892 answers with deceptive and authen-
tic labels as our dataset. With our labeled data,
we employ supervised methods to predict decep-
tive answers. We conduct 5-fold cross-validation
for experiments. The larger question threads data
is employed for feature learning, such as transla-
tion model, and topic model training.
5.1.2 Evaluation Metrics
The evaluation metrics are precision, recall and
F -score for authentic answer category and de-
ceptive answer category: precision = Sp?ScSp ,
recall = Sp?ScSc , and F = 2?precision?recallprecision+recall , where
Sc is the set of gold-standard positive instances for
the target category, Sp is the set of predicted re-
sults. We also use the accuracy as one metric,
which is computed as the number of answers pre-
dicted correctly, divided by the number of total an-
swers.
1728
Deceptive Answer Authentic Answer Overall
Prec. Rec. F-Score Prec. Rec. F-Score Acc.
Random 0.50 0.50 0.50 0.50 0.50 0.50 0.50
Unigram/Bigram (UB) 0.61 0.71 0.66 0.66 0.55 0.60 0.63
URL 0.93 0.26 0.40 0.57 0.98 0.72 0.62
Phone/Mail 0.94 0.15 0.25 0.53 0.99 0.70 0.57
Length 0.56 0.91 0.69 0.76 0.28 0.41 0.60
All Textual Features 0.64 0.67 0.66 0.66 0.63 0.64 0.65
QA Relevance 0.66 0.57 0.61 0.62 0.71 0.66 0.64
User Profile 0.62 0.53 0.57 0.59 0.67 0.63 0.60
User Authority 0.54 0.80 0.65 0.62 0.33 0.43 0.56
Robot 0.66 0.62 0.64 0.61 0.66 0.64 0.64
Answer Evaluation 0.55 0.53 0.54 0.55 0.57 0.56 0.55
Answer Duplication 0.69 0.71 0.70 0.70 0.68 0.69 0.69
All Contextual Feature 0.78 0.74 0.76 0.75 0.79 0.77 0.77
Textutal + Contextual 0.80 0.82 0.81 0.82 0.79 0.80 0.81
Table 2: Results With Textual and Contextual Features
5.2 Results with Textual and Contextual
Features
We tried several different classifiers, including
SVM, ME and the linear weight models with least
square and logistic regression. We find that they
can achieve similar results. For simplicity, the lin-
ear weight with least square is employed in our
experiment. Table 2 shows the experiment results.
For textual features, it achieves much better re-
sult with unigram/bigram features than the ran-
dom guess. This is very different from the an-
swer quality prediction task. The previous stud-
ies (Jeon et al, 2006; Song et al, 2010) find that
the word features can?t improve the performance
on answer quality prediction. However, from Ta-
ble 1, we can see that the word features can pro-
vide some weak signals for deceptive answer pre-
diction, for example, words ?recommend?, ?ad-
dress?, ?professional? express some kinds of pro-
motion intent. Besides unigram and bigram, the
most effective textual feature is URL. The phone
and email features perform similar results with
URL. The observation of length feature for decep-
tive answer prediction is very different from previ-
ous answer quality prediction. For answer quality
prediction, length is an effective feature, for exam-
ple, long-length provides very strong signals for
high-quality answer (Shah and Pomerantz, 2010;
Song et al, 2010). However, for deceptive answer
prediction, we find that the long answers are more
potential to be deceptive. This is because most of
deceptive answers are well prepared for product
promotion. They will write detailed answers to at-
tract user?s attention and promote their products.
Finally, with all textual features, the experiment
achieves the best result, 0.65 in accuracy.
For contextual features, we can see that, the
most effective contextual feature is answer dupli-
cation. The malicious users may copy the pre-
pared deceptive answers or just simply edit the tar-
get name to answer different questions. Question-
answer relevance and robot are the second most
useful single features for deceptive answer predic-
tion. The main characteristics of the Community
QA sites is to accumulate the answers for the tar-
get questions. Therefore, all the answers should be
relevant to the question. If the answer is not rel-
evant to the corresponding question, this answer
is more likely to be deceptive. Robot is one of
main sources for deceptive answers. It automat-
ically post the deceptive answers to target ques-
tions. Here, we formulate the time series as in-
terval sequence. The experiment result shows that
the robot indeed has his own posting behavior pat-
terns. The user profile feature also can contribute
a lot to deceptive answer prediction. Among the
user profile features, the user level in the Com-
munity QA site is a good indicator. The other
two contextual features, including user authority
and answer evaluation, provide limited improve-
ment. We find the following reasons: First, some
malicious users post answers to various questions
for product promotion, but don?t ask any question.
From Equation 1, when iteratively computing the
1729
Deceptive Answer Authentic Answer Overall
Prec. Rec. F-Score Prec. Rec. F-Score Acc.
Interaction Graph as Feature 0.80 0.82 0.81 0.82 0.79 0.80 0.81
Interaction Graph as Regularizer 0.80 0.83 0.82 0.82 0.80 0.81 0.82
Preference Graph as Feature 0.79 0.83 0.81 0.82 0.78 0.80 0.81
Preference Graph as Regularizer 0.83 0.86 0.85 0.85 0.83 0.84 0.85
Table 3: Results With User Preference Graph
final scores, the authority scores for these mali-
cious users will be accumulated to large values.
Therefore, it is hard to distinguish whether the
high authority score represents real expert or mali-
cious user. Second, the ?best answer? is not a good
signal for deceptive answer prediction. This may
be selected by malicious users, or the authentic
asker was misled, and chose the deceptive answer
as ?best answer?. This also demonstrates that the
deceptive answer prediction is very different from
the answer quality prediction. When combining
all the contextual features, it can achieve the over-
all accuracy 0.77, which is much better than the
textual features. Finally, with all the textual and
contextual features, we achieve the overall result,
0.81 in accuracy.
5.3 Results with User Preference Graph
Table 3 shows the results with user preference
graph. We compare with several baselines. Inter-
action graph is constructed by the asker-answerer
relationship introduced in Section 3.2.3. When
using the user graph as feature, we compute the
authority score for each user with PageRank as
shown in Equation 1. We also incorporating the
interaction graph with a regularizer as shown in
Equation 4. Note that we didn?t consider the edge
direction when using interaction graph as a regu-
larizer. From the table, we can see that when in-
corporating user preference graph as a feature, it
can?t achieve a better result than the interaction
graph. The reason is similar as the interaction
graph. The higher authority score may boosted
by other spammer, and can?t be a good indica-
tor to distinguish deceptive and authentic answers.
When we incorporate the user preference graph
as a regularizer, it can achieve about 4% further
improvement, which demonstrates that the user
evaluation towards answers, such as ?helpful? vot-
ing and ?best answer? selection, is a good signal
to generate user relationship for deceptive answer
prediction, and the graph regularization is an ef-
fective technique to incorporate the user prefer-
ence graph. We also analyze the parameter sen-
10?5 10?4 10?3 10?2 10?1 1000.76
0.78
0.8
0.82
0.84
0.86
0.88
Acc
urac
y
 
 General supervised methodInteraction Graph as RegularizerPreference Graph as Regularizer
Figure 2: Results with different values of ?
sitivity. ? is the tradeoff weight for graph regular-
ization term. Figure 2 shows the results with dif-
ferent values of ?. We can see that when ? ranges
from 10?4 ? 10?2, the deceptive answer predic-
tion can achieve best results.
6 Conclusions and Future Work
In this paper, we discuss the deceptive answer
prediction task in Community QA sites. With
the manually labeled data set, we first predict the
deceptive answers with traditional classification
method. Two types of features, including textual
features and contextual features, are extracted and
analyzed. We also introduce a new user prefer-
ence graph, constructed based on the user evalua-
tions towards the target answer, such as ?helpful?
voting and ?best answer? selection. A graph reg-
ularization method is proposed to incorporate the
user preference graph for deceptive answer predic-
tion. The experiments are conducted to discuss
the effects of different features. The experiment
results also show that the method with user pref-
erence graph can achieve more accurate results for
deceptive answer prediction.
In the future work, it is interesting to incorpo-
rate more features into deceptive answer predic-
tion. It is also important to predict the deceptive
question threads, which are posted and answered
both by malicious users for product promotion.
Malicious user group detection is also an impor-
tant task in the future.
1730
References
Lada A. Adamic, Jun Zhang, Eytan Bakshy, and
Mark S. Ackerman. 2008. Knowledge sharing
and yahoo answers: everyone knows something. In
Proceedings of the 17th international conference on
World Wide Web, WWW ?08, pages 665?674, New
York, NY, USA. ACM.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World wide
web, WWW ?09, pages 51?60, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Comput. Linguist., 16:79?85, June.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American society
for information science, 41(6):391?407.
A. Figueroa and J. Atkinson. 2011. Maximum entropy
context models for ranking biographical answers to
open-domain definition questions. In Twenty-Fifth
AAAI Conference on Artificial Intelligence.
F. Maxwell Harper, Daphne Raban, Sheizaf Rafaeli,
and Joseph A. Konstan. 2008. Predictors of answer
quality in online q&a sites. In Proceedings of the
twenty-sixth annual SIGCHI conference on Human
factors in computing systems, CHI ?08, pages 865?
874, New York, NY, USA. ACM.
Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando,
2010. Overview of the NTCIR-8 Community QA Pi-
lot Task (Part I): The Test Collection and the Task,
pages 421?432. Number Part I.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM
CIKM conference, 05, pages 84?90, NY, USA.
ACM.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with
non-textual features. In Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 228?235. ACM.
P. Jurczyk and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of the sixteenth ACM
CIKM conference, pages 919?922. ACM.
H. Kim, P. Howland, and H. Park. 2006. Dimension
reduction in text classification with support vector
machines. Journal of Machine Learning Research,
6(1):37.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
2011. Learning to identify review spam. In Pro-
ceedings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume Volume
Three, pages 2488?2493. AAAI Press.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understand-
ing and summarizing answers in community-based
question answering services. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 497?
504, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011.
Competition-based user expertise score estimation.
In Proceedings of the 34th international ACM SI-
GIR conference on Research and development in In-
formation Retrieval, pages 425?434. ACM.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, and
Livia Polanyi. 2010. Exploiting social context for
review quality prediction. In Proceedings of the
19th international conference on World wide web,
pages 691?700. ACM.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51, March.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November. SIDL-WP-
1999-0120.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. ACL.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and predicting answer quality in community qa.
In Proceedings of the 33rd international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?10, pages 411?418, New
York, NY, USA. ACM.
X. Si, Z. Gyongyi, and E. Y. Chang. 2010a. Scal-
able mining of topic-dependent user reputation for
improving user generated content search quality. In
Google Technical Report.
1731
Xiance Si, Edward Y. Chang, Zolta?n Gyo?ngyi, and
Maosong Sun. 2010b. Confucius and its intelli-
gent disciples: integrating social with search. Proc.
VLDB Endow., 3:1505?1516, September.
Young-In Song, Chin-Yew Lin, Yunbo Cao, and Hae-
Chang Rim. 2008. Question utility: a novel static
ranking of question search. In Proceedings of the
23rd national conference on Artificial intelligence
- Volume 2, AAAI?08, pages 1231?1236. AAAI
Press.
Y.I. Song, J. Liu, T. Sakai, X.J. Wang, G. Feng, Y. Cao,
H. Suzuki, and C.Y. Lin. 2010. Microsoft research
asia with redmond at the ntcir-8 community qa pilot
task. In Proceedings of NTCIR.
Wei Wei, Gao Cong, Xiaoli Li, See-Kiong Ng, and
Guohui Li. 2011. Integrating community question
and answer archives. In AAAI.
Y. Yang and J.O. Pedersen. 1997. A compara-
tive study on feature selection in text categoriza-
tion. In MACHINE LEARNING-INTERNATIONAL
WORKSHOP THEN CONFERENCE-, pages 412?
420. MORGAN KAUFMANN PUBLISHERS.
Tong Zhang, Alexandrin Popescul, and Byron Dom.
2006. Linear prediction models with graph regu-
larization for web-page categorization. In Proceed-
ings of the 12th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 821?826. ACM.
1732
