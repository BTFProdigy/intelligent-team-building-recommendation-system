Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 221?225, New York City, June 2006. c?2006 Association for Computational Linguistics
Labeled Pseudo-Projective Dependency Parsing
with Support Vector Machines
Joakim Nivre
Johan Hall
Jens Nilsson
School of Mathematics
and Systems Engineering
Va?xjo? University
35195 Va?xjo?, Sweden
{nivre,jha,jni}@msi.vxu.se
Gu?ls?en Eryig?it
Department of
Computer Engineering
Istanbul Technical University
34469 Istanbul, Turkey
gulsen@cs.itu.edu.tr
Svetoslav Marinov
School of Humanities
and Informatics
University of Sko?vde
Box 408
54128 Sko?vde, Sweden
svetoslav.marinov@his.se
Abstract
We use SVM classifiers to predict the next
action of a deterministic parser that builds
labeled projective dependency graphs in
an incremental fashion. Non-projective
dependencies are captured indirectly by
projectivizing the training data for the
classifiers and applying an inverse trans-
formation to the output of the parser. We
present evaluation results and an error
analysis focusing on Swedish and Turkish.
1 Introduction
The CoNLL-X shared task consists in parsing texts
in multiple languages using a single dependency
parser that has the capacity to learn from treebank
data. Our methodology for performing this task is
based on four essential components:
? A deterministic algorithm for building labeled
projective dependency graphs (Nivre, 2006).
? History-based feature models for predicting the
next parser action (Black et al, 1992).
? Support vector machines for mapping histories
to parser actions (Kudo and Matsumoto, 2002).
? Graph transformations for recovering non-
projective structures (Nivre and Nilsson, 2005).
All experiments have been performed using Malt-
Parser (Nivre et al, 2006), version 0.4, which is
made available together with the suite of programs
used for pre- and post-processing.1
1www.msi.vxu.se/users/nivre/research/MaltParser.html
2 Parsing Methodology
2.1 Parsing Algorithm
The parsing algorithm used for all languages is the
deterministic algorithm first proposed for unlabeled
dependency parsing by Nivre (2003) and extended
to labeled dependency parsing by Nivre et al (2004).
The algorithm builds a labeled dependency graph in
one left-to-right pass over the input, using a stack
to store partially processed tokens and adding arcs
using four elementary actions (where top is the token
on top of the stack and next is the next token):
? SHIFT: Push next onto the stack.
? REDUCE: Pop the stack.
? RIGHT-ARC(r): Add an arc labeled r from top
to next; push next onto the stack.
? LEFT-ARC(r): Add an arc labeled r from next
to top; pop the stack.
Although the parser only derives projective graphs,
the fact that graphs are labeled allows non-projective
dependencies to be captured using the pseudo-
projective approach of Nivre and Nilsson (2005) .
Another limitation of the parsing algorithm is that
it does not assign dependency labels to roots, i.e., to
tokens having HEAD=0. To overcome this problem,
we have implemented a variant of the algorithm that
starts by pushing an artificial root token with ID=0
onto the stack. Tokens having HEAD=0 can now
be attached to the artificial root in a RIGHT-ARC(r)
action, which means that they can be assigned any
label. Since this variant of the algorithm increases
the overall nondeterminism, it has only been used
for the data sets that include informative root labels
(Arabic, Czech, Portuguese, Slovene).
221
FO L C P FE D
S: top + + + + + +
S: top?1 +
I: next + + + + +
I: next+1 + +
I: next+2 +
I: next+3 +
G: head of top +
G: leftmost dep of top +
G: rightmost dep of top +
G: leftmost dep of next +
Table 1: Base model; S: stack, I: input, G: graph;
FO: FORM, L: LEMMA , C: CPOS, P: POS,
FE: FEATS, D: DEPREL
2.2 History-Based Feature Models
History-based parsing models rely on features of the
derivation history to predict the next parser action.
The features used in our system are all symbolic
and extracted from the following fields of the data
representation: FORM, LEMMA, CPOSTAG, POSTAG,
FEATS, and DEPREL. Features of the type DEPREL
have a special status in that they are extracted during
parsing from the partially built dependency graph
and may therefore contain errors, whereas all the
other features have gold standard values during both
training and parsing.2
Based on previous research, we defined a base
model to be used as a starting point for language-
specific feature selection. The features of this model
are shown in Table 1, where rows denote tokens in
a parser configuration (defined relative to the stack,
the remaining input, and the partially built depen-
dency graph), and where columns correspond to data
fields. The base model contains twenty features, but
note that the fields LEMMA, CPOS and FEATS are not
available for all languages.
2.3 Support Vector Machines
We use support vector machines3 to predict the next
parser action from a feature vector representing the
history. More specifically, we use LIBSVM (Chang
and Lin, 2001) with a quadratic kernel K(xi, xj) =
(?xTi xj +r)2 and the built-in one-versus-all strategy
for multi-class classification. Symbolic features are
2The fields PHEAD and PDEPREL have not been used at all,
since we rely on pseudo-projective parsing for the treatment of
non-projective structures.
3We also ran preliminary experiments with memory-based
learning but found that this gave consistently lower accuracy.
converted to numerical features using the standard
technique of binarization, and we split values of the
FEATS field into its atomic components.4
For some languages, we divide the training data
into smaller sets, based on some feature s (normally
the CPOS or POS of the next input token), which may
reduce training times without a significant loss in
accuracy (Yamada and Matsumoto, 2003). To avoid
too small training sets, we pool together categories
that have a frequency below a certain threshold t.
2.4 Pseudo-Projective Parsing
Pseudo-projective parsing was proposed by Nivre
and Nilsson (2005) as a way of dealing with
non-projective structures in a projective data-driven
parser. We projectivize training data by a minimal
transformation, lifting non-projective arcs one step
at a time, and extending the arc label of lifted arcs
using the encoding scheme called HEAD by Nivre
and Nilsson (2005), which means that a lifted arc is
assigned the label r?h, where r is the original label
and h is the label of the original head in the non-
projective dependency graph.
Non-projective dependencies can be recovered by
applying an inverse transformation to the output of
the parser, using a left-to-right, top-down, breadth-
first search, guided by the extended arc labels r?h
assigned by the parser. This technique has been used
without exception for all languages.
3 Experiments
Since the projective parsing algorithm and graph
transformation techniques are the same for all data
sets, our optimization efforts have been focused on
feature selection, using a combination of backward
and forward selection starting from the base model
described in section 2.2, and parameter optimization
for the SVM learner, using grid search for an optimal
combination of the kernel parameters ? and r, the
penalty parameter C and the termination criterion ?,
as well as the splitting feature s and the frequency
threshold t. Feature selection and parameter opti-
mization have to some extent been interleaved, but
the amount of work done varies between languages.
4Preliminary experiments showed a slight improvement for
most languages when splitting the FEATS values, as opposed to
taking every combination of atomic values as a distinct value.
222
Ara Bul Chi Cze Dan Dut Ger Jap Por Slo Spa Swe Tur Total
LAS 66.71 87.41 86.92 78.42 84.77 78.59 85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.19
UAS 77.52 91.72 90.54 84.80 89.80 81.35 88.76 93.10 91.22 78.72 84.67 89.50 75.82 85.48
LAcc 80.34 90.44 89.01 85.40 89.16 83.69 91.03 94.34 91.54 80.54 90.06 87.39 78.49 86.75
Table 2: Evaluation on final test set; LAS = labeled attachment score, UAS = unlabeled attachment score,
LAcc = label accuracy score; total score excluding Bulgarian
The main optimization criterion has been labeled
attachment score on held-out data, using ten-fold
cross-validation for all data sets with 100k tokens
or less, and an 80-20 split into training and devtest
sets for larger datasets. The number of features in
the optimized models varies from 16 (Turkish) to 30
(Spanish), but the models use all fields available for
a given language, except that FORM is not used for
Turkish (only LEMMA). The SVM parameters fall
into the following ranges: ?: 0.12?0.20; r: 0.0?0.6;
C: 0.1?0.7; ?: 0.01?1.0. Data has been split on the
POS of the next input token for Czech (t = 200),
German (t = 1000), and Spanish (t = 1000), and
on the CPOS of the next input token for Bulgarian
(t = 1000), Slovene (t = 600), and Turkish (t = 100).
(For the remaining languages, the training data has
not been split at all.)5 A dry run at the end of the
development phase gave a labeled attachment score
of 80.46 over the twelve required languages.
Table 2 shows final test results for each language
and for the twelve required languages together. The
total score is only 0.27 percentage points below the
score from the dry run, which seems to indicate that
models have not been overfitted to the training data.
The labeled attachment score varies from 91.65 to
65.68 but is above average for all languages. We
have the best reported score for Japanese, Swedish
and Turkish, and the score for Arabic, Danish,
Dutch, Portuguese, Spanish, and overall does not
differ significantly from the best one. The unlabeled
score is less competitive, with only Turkish having
the highest reported score, which indirectly indicates
that the integration of labels into the parsing process
primarily benefits labeled accuracy.
4 Error Analysis
An overall error analysis is beyond the scope of this
paper, but we will offer a few general observations
5Detailed specifications of the feature models and learning
algorithm parameters can be found on the MaltParser web page.
before we turn to Swedish and Turkish, focusing on
recall and precision of root nodes, as a reflection of
global syntactic structure, and on attachment score
as a function of arc length. If we start by considering
languages with a labeled attachment score of 85% or
higher, they are characterized by high precision and
recall for root nodes, typically 95/90, and by a grace-
ful degradation of attachment score as arcs grow
longer, typically 95?90?85, for arcs of length 1, 2
and 3?6. Typical examples are Bulgarian (Simov
et al, 2005; Simov and Osenova, 2003), Chinese
(Chen et al, 2003), Danish (Kromann, 2003), and
Swedish (Nilsson et al, 2005). Japanese (Kawata
and Bartels, 2000), despite a very high accuracy, is
different in that attachment score drops from 98%
to 85%, as we go from length 1 to 2, which may
have something to do with the data consisting of
transcribed speech with very short utterances.
A second observation is that a high proportion of
non-projective structures leads to fragmentation in
the parser output, reflected in lower precision for
roots. This is noticeable for German (Brants et al,
2002) and Portuguese (Afonso et al, 2002), which
still have high overall accuracy thanks to very high
attachment scores, but much more conspicuous for
Czech (Bo?hmova? et al, 2003), Dutch (van der Beek
et al, 2002) and Slovene (Dz?eroski et al, 2006),
where root precision drops more drastically to about
69%, 71% and 41%, respectively, and root recall is
also affected negatively. On the other hand, all three
languages behave like high-accuracy languages with
respect to attachment score. A very similar pattern
is found for Spanish (Civit Torruella and Mart?? An-
ton??n, 2002), although this cannot be explained by
a high proportion of non-projective structures. One
possible explanation in this case may be the fact that
dependency graphs in the Spanish data are sparsely
labeled, which may cause problem for a parser that
relies on dependency labels as features.
The results for Arabic (Hajic? et al, 2004; Smrz?
et al, 2002) are characterized by low root accuracy
223
as well as a rapid degradation of attachment score
with arc length (from about 93% for length 1 to 67%
for length 2). By contrast, Turkish (Oflazer et al,
2003; Atalay et al, 2003) exhibits high root accu-
racy but consistently low attachment scores (about
88% for length 1 and 68% for length 2). It is note-
worthy that Arabic and Turkish, being ?typological
outliers?, show patterns that are different both from
each other and from most of the other languages.
4.1 Swedish
A more fine-grained analysis of the Swedish results
reveals a high accuracy for function words, which
is compatible with previous studies (Nivre, 2006).
Thus, the labeled F-score is 100% for infinitive
markers (IM) and subordinating conjunctions (UK),
and above 95% for determiners (DT). In addition,
subjects (SS) have a score above 90%. In all these
cases, the dependent has a configurationally defined
(but not fixed) position with respect to its head.
Arguments of the verb, such as objects (DO, IO)
and predicative complements (SP), have a slightly
lower accuracy (about 85% labeled F-score), which
is due to the fact that they ?compete? in the same
structural positions, whereas adverbials (labels that
end in A) have even lower scores (often below 70%).
The latter result must be related both to the relatively
fine-grained inventory of dependency labels for ad-
verbials and to attachment ambiguities that involve
prepositional phrases. The importance of this kind
of ambiguity is reflected also in the drastic differ-
ence in accuracy between noun pre-modifiers (AT)
(F > 97%) and noun post-modifiers (ET) (F ? 75%).
Finally, it is worth noting that coordination, which
is often problematic in parsing, has high accuracy.
The Swedish treebank annotation treats the second
conjunct as a dependent of the first conjunct and as
the head of the coordinator, which seems to facil-
itate parsing.6 The attachment of the second con-
junct to the first (CC) has a labeled F-score above
80%, while the attachment of the coordinator to the
second conjunct (++) has a score well above 90%.
4.2 Turkish
In Turkish, very essential syntactic information is
contained in the rich morphological structure, where
6The analysis is reminiscent of the treatment of coordination
in the Collins parser (Collins, 1999).
concatenated suffixes carry information that in other
languages may be expressed by separate words. The
Turkish treebank therefore divides word forms into
smaller units, called inflectional groups (IGs), and
the task of the parser is to construct dependencies
between IGs, not (primarily) between word forms
(Eryig?it and Oflazer, 2006). It is then important
to remember that an unlabeled attachment score
of 75.8% corresponds to a word-to-word score of
82.7%, which puts Turkish on a par with languages
like Czech, Dutch and Spanish. Moreover, when
we break down the results according to whether the
head of a dependency is part of a multiple-IG word
or a complete (single-IG) word, we observe a highly
significant difference in accuracy, with only 53.2%
unlabeled attachment score for multiple-IG heads
versus 83.7% for single-IG heads. It is hard to say
at this stage whether this means that our methods
are ill-suited for IG-based parsing, or whether it is
mainly a case of sparse data for multiple-IG words.
When we break down the results by dependency
type, we can distinguish three main groups. The first
consists of determiners and particles, which have
an unlabeled attachment score over 80% and which
are found within a distance of 1?1.4 IGs from their
head.7 The second group mainly contains subjects,
objects and different kinds of adjuncts, with a score
in the range 60?80% and a distance of 1.8?5.2 IGs to
their head. In this group, information about case and
possessive features of nominals is important, which
is found in the FEATS field in the data representation.
We believe that one important explanation for our
relatively good results for Turkish is that we break
down the FEATS information into its atomic com-
ponents, independently of POS and CPOS tags, and
let the classifier decide which one to use in a given
situation. The third group contains distant depen-
dencies, such as sentence modifiers, vocatives and
appositions, which have a much lower accuracy.
5 Conclusion
The evaluation shows that labeled pseudo-projective
dependency parsing, using a deterministic parsing
algorithm and SVM classifiers, gives competitive
parsing accuracy for all languages involved in the
7Given that the average IG count of a word is 1.26 in the
treebank, this means that they are normally adjacent to the head
word.
224
shared task, although the level of accuracy varies
considerably between languages. To analyze in
depth the factors determining this variation, and to
improve our parsing methods accordingly to meet
the challenges posed by the linguistic diversity, will
be an important research goal for years to come.
Acknowledgments
We are grateful for the support from T ?UB?ITAK
(The Scientific and Technical Research Council of
Turkey) and the Swedish Research Council. We also
want to thank Atanas Chanev for assistance with
Slovene, the organizers of the shared task for all
their hard work, and the creators of the treebanks
for making the data available.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora, volume 20 of Text, Speech and Language
Technology. Kluwer Academic Publishers, Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta
sinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC-
2002, pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. of LINC-2003.
E. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-
cer, and S. Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
5th DARPA Speech and Natural Language Workshop, pages
31?37.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002.
The TIGER treebank. In Proc. of TLT-2002.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. Design
principles for a Spanish treebank. In Proc. of TLT-2002.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?Zabokrtsky, and
A. ?Zele. 2006. Towards a Slovene dependency treebank. In
Proc. of LREC-2006.
G. Eryig?it and K. Oflazer. 2006. Statistical dependency parsing
of Turkish. In Proc. of EACL-2006.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of NEMLAR-2004, pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank and
the underlying linguistic theory. In Proc. of TLT-2003.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency anal-
ysis using cascaded chunking. In Proc. of CoNLL-2002,
pages 63?69.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER:
Reconstructing a Swedish treebank from antiquity. In Proc.
of the NODALIDA Special Session on Treebanks.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency
parsing. In Proc. of ACL-2005, pages 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL-2004, pages 49?56.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A data-
driven parser-generator for dependency parsing. In Proc. of
LREC-2006.
J. Nivre. 2003. An efficient algorithm for projective depen-
dency parsing. In Proc. of IWPT-2003, pages 149?160.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. of LINC-2003,
pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005.
Design and implementation of the Bulgarian HPSG-based
treebank. In Journal of Research on Language and Com-
putation ? Special Issue, pages 495?522. Kluwer Academic
Publishers.
O. Smrz?, J. ?Snaidauf, and P. Zema?nek. 2002. Prague depen-
dency treebank for Arabic: Multi-level annotation of Arabic
corpus. In Proc. of the Intern. Symposium on Processing of
Arabic, pages 147?155.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of IWPT-
2003, pages 195?206.
225
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1144?1148,
Prague, June 2007. c?2007 Association for Computational Linguistics
Covington Variations
Svetoslav Marinov
School of Humanities and Informatics,
University College Sko?vde, 54128 Sko?vde &
GSLT, Go?teborg University, 40530 Go?teborg,
Sweden
Svetoslav.Marinov@his.se
Abstract
Three versions of the Covington algorithm
for non-projective dependency parsing have
been tested on the ten different languages
for the Multilingual track of the CoNLL-
X Shared Task. The results were achieved
by using only information about heads and
daughters as features to guide the parser
which obeys strict incrementality.
1 Introduction
In this paper we focus on two things. First, we in-
vestigate the impact of using different flavours of
Covington?s algorithm (Covington, 2001) for non-
projective dependency parsing on the ten differ-
ent languages provided for CoNLL-X Shared Task
(Nivre et al, 2007). Second, we test the perfor-
mance of a pure grammar-based feature model in
strictly incremental fashion. The grammar model re-
lies only on the knowledge of heads and daughters
of two given words, as well as the words themselves,
in order to decide whether they can be linked with a
certain dependency relation. In addition, none of the
three parsing algorithms guarantees that the output
dependency graph will be projective.
2 Covington?s algorithm(s)
In his (2001) paper, Covington presents a ?funda-
mental? algorithm for dependency parsing, which
he claims has been known since the 1960s but has,
up to his paper-publication, not been presented
systematically in the literature. We take three
of its flavours, which enforce uniqueness (a.k.a.
single-headedness) but do not observe projectivity.
The algorithms work one word at a time and
attempt to build a connected dependency graph with
only a single left-to-right pass through the input.
The three flavours are: Exhaustive Search, Head
First with Uniqueness (ESHU), Exhaustive Search
Dependents First with Uniqueness (ESDU) and
List-based search with Uniqueness (LSU).
ESHU ESDU
for i = 1 to n for i = 1 to n
for j = i-1 downto 0 for j = i-1 downto 0
if HEAD?(j,i) if HEAD?(i,j)
LINK(j,i) LINK(i,j)
if HEAD?(i,j) if HEAD?(j,i)
LINK(i,j) LINK(j,i)
The yes/no function HEAD?(w1,w2), checks
whether a word w1 can be a head of a word w2 ac-
cording to a grammar G. It also respects the single-
head and no-cycle conditions. The LINK(w1,w2)
procedure links word w1 as the head of word w2
with a dependency relation as proposed by G. When
traversing Headlist and Wordlist we start with the
last word added. (Nivre, 2007) describes an op-
timized version of Covington?s algorithm imple-
mented in MaltParser (Nivre, 2006) with a running
time c(n22 ? n2 ) for an n-word sentence, where c is
some constant time in which the LINK operation
can be performed. However, due to time constraints,
we will not bring this version of the algorithm into
focus, but see some preliminary remarks on it with
respect to our parsing model in 6.
1144
LSU1
Headlist := []
Wordlist := []
while (!end-of-sentence)
W := next input word;
foreach D in Headlist
if HEAD?(W,D)
LINK(W,D);
delete D from Headlist;
end
foreach H in Wordlist
if HEAD?(H,W)
LINK(H,W);
terminate this foreach loop;
end
if no head for W was found then
Headlist := W + Headlist;
end
Wordlist := W + Wordlist;
end
3 Classifier as an Instant Grammar
The HEAD? function in the algorithms presented
in 2, requires an ?instant grammar? (Covington,
2001) of some kind, which can tell the parser
whether the two words under scrutiny can be linked
and with what dependency relation. To satisfy
this requirement, we use TiMBL - a Memory-based
learner (Daelemans et al, 2004) - as a classifier to
predict the relation (if any) holding between the two
words.
Building heavily on the ideas of History-based
parsing (Black et al, 1993; Nivre, 2006), training
the parser means essentially running the parsing al-
gorithms in a learning mode on the data in order
to gather training instances for the memory-based
learner. In a learning mode, the HEAD? function
has access to a fully parsed dependency graph. In
the parsing mode, the HEAD? function in the algo-
rithms issues a call to the classifier using features
from the parsing history (i.e. a partially built depen-
dency graph PG).
Given words i and j to be linked, and a PG, the
call to the classifier is a feature vector ?(i,j,PG) =
(?1,. . . ,?m) (cf. (Nivre, 2006; Nivre, 2007)). The
1Covington adds W to the Wordlist as soon as it has been
seen, however we have chosen to wait until after all tests have
been completed.
classifier then attempts to map this feature vector to
any of predefined classes. These are all the depen-
dency relations, as defined by the treebank and the
class ?NO? in the cases where no link between the
two words is possible.
4 The Grammar model
The features used in our history-based model are re-
stricted only to the partially built graph PG. We call
this model a pure grammar-based model since the
only information the parsing algorithms have at their
disposal is extracted from the graph, such as the head
and daughters of the current word. Preceding words
not included in the PG as well as words following
the current word are not available to the algorithm.
In this respect such a model is very restrictive and
suffers from the pitfalls of the incremental process-
ing (Nivre, 2004).
The motivation for the chosen model, was to ap-
proximate a Data Oriented Parsing (DOP) model
(e.g. (Bod et al, 2003)) for Dependency Gram-
mar. Under DOP, analyses of new sentences are pro-
duced by combining previously seen tree fragments.
However, the tree fragments under the original DOP
model are static, i.e. we have a corpus of all possi-
ble subtrees derived from a treebank. Under our ap-
proach, these tree fragments are built dynamically,
as we try to parse the sentence. Because of the cho-
sen DOP approximation, we have not included in-
formation about the preceding and following words
of the two words to be linked in our feature model.
To exemplify our approach, (1) shows a partially
build graph and all the words encountered so far and
Fig. 1 shows two examples of the tree-building op-
erations for linking words f and d, and f and a.
(1) a b c d e f . . .
Given two words i and j to be linked with a
dependency relation, such that word j precedes
word i, the following features describe the models
on which the algorithms have been trained and
tested:
Word form: i, j, ds(i), ds(j), h(j/i), h(h(j/i))
Lemma (if available): i, j, ds(i), ds(j), h(j/i),
h(h(j/i))
1145
HEAD?
a
d
b                  c
f
e
HEAD? HEAD?
a
d
f
e
Figure 1: Application of the HEAD? function on an
input from the PG in (1)
Part-of-Speech: i, j, ds(i), ds(j), h(j/i), h(h(j/i))
Dependency type: i, j, ds(i), ds(j), h(j/i),
h(h(j/i))
Features (if available): i, j, ds(i), ds(j), h(j/i),
h(h(j/i))
ds(i) means any two daughters (if available)
of word i, h(i/j) refers to the head of word i or
word j, depending on the direction of applying the
HEAD? function (see Fig 1) and h(h(i/j)) stands
for the head of the head of word i or word j.
The basic model, which was used for the largest
training data sets of Czech and Chinese, includes
only the first four features in every category. A
larger model used for the datasets of Catalan and
Hungarian adds the h(j/i) feature from every cate-
gory. The enhanced model used for Arabic, Basque,
English, Greek, Italian and Turkish uses the full set
of features. This tripartite division of models was
motivated only by time- and resource-constraints.
The simplest model is for Chinese and uses only 5
features while the enhanced model for Arabic for ex-
ample uses a total of 39 features.
5 Results and Setup
Table 1 summarizes the results of testing the three
algorithms on the ten different languages.
The parser was written in C#. Training and
testing were performed on a MacOSX 10.4.9 with
2GHz Intel Core2Duo processor and 1GB mem-
ory, and a Dell Dimension with 2.80GHz Pentium
4 processor and 1GB memory running Mepis Linux.
TiMBL was run in client-server mode with default
settings (IB1 learning algorithm, extrapolation from
the most similar example, i.e. k = 1, initiated
with the command ?Timbl -S <portnumber> -f
ESHU ESDU LSU
Arabic LA: 53.72 LA: 54.00 LA: 53.86
UA: 63.58 UA: 63.76 UA: 63.78
Basque LA: 49.52 LA: 50.20 LA: 51.24
UA: 56.83 UA: 57.81 UA: 58.53
Catalan LA: 69.56 LA: 69.80 LA: 69.42
UA: 74.32 UA: 74.46 UA: 74.22
Chinese LA: 47.57 LA: 50.61 LA: 49.82
UA: 53.46 UA: 56.75 UA: 56.02
Czech LA: 44.41 LA: 53.66 LA: 53.47
UA: 49.20 UA: 60.01 UA: 59.55
English LA: 51.05 LA: 51.35 LA: 52.11
UA: 53.41 UA: 53.65 UA: 54.33
Greek LA: 54.68 LA: 54.62 LA: 55.02
UA: 61.55 UA: 61.45 UA: 61.80
Hungarian LA: 44.34 LA: 45.11 LA: 44.57
UA: 50.12 UA: 50.78 UA: 50.46
Italian LA: 61.60 LA: 60.95 LA: 61.52
UA: 67.01 UA: 66.25 UA: 66.39
Turkish LA: 55.57 LA: 57.01 LA: 56.59
UA: 62.13 UA: 63.77 UA: 63.17
Table 1: Test results for the 10 languages. LA is
the Labelled Attachment Score and UA is the Unla-
belled Attachment Score
<training file>?). Additionally, we attempted to
use Support Vector Machines (SVM) as an alter-
native classifier. However, due to the long training
time, results from using SVM were not included but
training an SVM classifier for some of the languages
has started.
6 Discussion
Before we attempt a discussion on the results pre-
sented in Table 1, we give a short summary of the ba-
sic word order typology of these languages accord-
ing to (Greenberg, 1963). Table 2 shows whether
the languages are SVO (subject-verb-object) or SOV
(subject-object-verb), or VSO (verb-subject-object);
contain Pr (prepositions) or Po (postpositions); NG
(noun precedes genitive) or GN (genitive precedes
noun); AN (adjective precedes noun) or NA (noun
precedes adjective).
2Greenberg had give varying for the word-order typology of
English. However, we trusted our own intuition as well as the
hint of one of the reviewers.
1146
Arabic VSO Pr NG NA
Basque SOV Po GN NA
Catalan SVO Pr NG NA
Chinese SVO Po GN AN
Czech SVO Pr NG AN
English2 SVO Pr GN AN
Greek SVO Pr NG AN
Hungarian SOV Po GN AN
Italian SVO Pr NG NA
Turkish SOV Po ? AN
Table 2: Basic word order typology of the ten lan-
guages following Greenberg?s Universals
Looking at the data in Table 1, several obser-
vations can be made. One is the different perfor-
mance of languages from the same language fam-
ily, i.e. Italian, Greek and Catalan. However, the
head-first (ESHU) algorithm presented better than
the dependents-first (ESDU) one in all of these lan-
guages. The SOV languages like Hungarian, Basque
and Turkish had preference for the dependent?s first
algorithms (ESDU and LSU). The ESDU algorithm
also fared better with the SVO languages, except for
Italian.
However, the Greenberg?s basic word order ty-
pology cannot shed enough light into the perfor-
mance of the three parsing algorithms. One ques-
tion that pops up immediately is whether a differ-
ent feature-model using the same parsing algorithms
would achieve similar results. Can the different
performance be attributed to the treebank annota-
tion? Would another classifier fare better than the
Memory-based one? These questions remain for fu-
ture research though.
Finally, for the Basque data we attempted to
test the optimized version of the Covington algo-
rithm (Nivre, 2007) against the three other ver-
sions discussed here. Additionally, since our fea-
ture vectors differed from those described in (Nivre,
2007), head-dependent-features vs. j-i-features, we
changed them so that all the four algorithms send a
similar feature vector, j-i-features, to the classifier.
The preliminary result was that Nivre?s version was
the fastest, with fewer calls to the LINK procedure
and with the smallest training data-set. However, all
the four algorithms showed about 20% decrease in
LA/UA scores.
Our first intuition about the results from the tests
done on all the 10 languages was that the classifi-
cation task suffered from a highly skewed class dis-
tribution since the training instances that correspond
to a dependency relation are largely outnumbered by
the ?NO? class (Canisius et al, 2006). The recall
was low and we expected the classifier to be able to
predict more of the required links. However, the re-
sults we got from additional optimizations we per-
formed on Hungarian, following recommendation
from the anonymous reviewers, may lead to a differ-
ent conclusion. The chosen grammar model, relying
only on connecting dynamically built partial depen-
dency graphs, is insufficient to take us over a certain
threshold.
7 Conclusion
In this paper we showed the performance of three
flavours of Covington?s algorithm for non-projective
dependency parsing on the ten languages provided
for the CoNLL-X Shared Task (Nivre et al, 2007).
The experiment showed that given the grammar
model we have adopted it does matter which version
of the algorithm one uses. The chosen model,
however, showed a poor performance and suffered
from two major flaws - the use of only partially
built graphs and the pure incremental processing.
It remains to be seen how these parsing algorithms
will perform in a parser, with a much richer feature
model and whether it is worth using different
flavours when parsing different languages or the
differences among them are insignificant.
Acknowledgements
We would like to thank the two anonymous re-
viewers for their valuable comments. We are
grateful to Joakim Nivre for discussion on the
Covington algorithm, Bertjan Busser for help
with TiMBL, Antal van den Bosch for help with
paramsearch, Matthew Johnson for providing the
necessary functionality to his .NET implementation
of SVM and Patrycja Jab?on?ska for discussion on
the Greenberg?s Universals.
1147
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
Ezra Black, Frederick Jelinek, John D. Lafferty, David M.
Magerman, Robert L. Mercer, and Salim Roukos.
1993. Towards history-based grammars: Using richer
models for probabilistic parsing. In Meeting of the As-
sociation for Computational Linguistics, pages 31?37.
R. Bod, R. Scha, and K. Sima?an, editors. 2003. Data
Oriented Parsing. CSLI Publications, Stanford Uni-
versity, Stanford, CA, USA.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
Sander Canisius, Toine Bogers, Antal van den Bosch,
Jeroen Geertzen, and Erik Tjong Kim Sang. 2006.
Dependency Parsing by Inference over High-recall
Dependency Predictions. In CoNLL-X Shared Task on
Multitlingual Dependency Parsing.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Michael A. Covington. 2001. A Fundamental Algo-
rithm for Dependency Parsing. In Proceedings of the
39th Annual ACM Southeast Conference, pages 95?
102, Athens, Georgia, USA.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. Techni-
cal report, ILK Technical Report 04-02, available from
http://ilk.uvt.nl/downloads/pub/papers/ilk0402.pdf.
Joseph H. Greenberg. 1963. Universals of Language.
London: MIT Press.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Joakim Nivre. 2004. Incrementality in Deterministic
Dependency Parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together, Workshop at
ACL-2004, pages 50?57, Barcelona, Spain, July, 25.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2007. Incremental Non-Projective Depen-
dency Parsing. In Proceedings of NAACL-HLT 2007,
Rochester, NY, USA, April 22?27.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
1148
