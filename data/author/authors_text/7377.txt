In: Proceedings of CoNLL-2000 and LLL-2000, pages 194-198, Lisbon, Portugal, 2000. 
Learning from Parsed Sentences with INTHELEX 
F. Espos i to  and S. Feri l l i  and N. Fanizz i  and G. Semeraro  
Dipartimento di Informatica 
Universit? di Bari 
via E. Orabona, 4 - 70126 Bari - Italia 
{esposito, ferilli, fanizzi, semeraro}~di.uniba.it 
Abst ract  
In the context of language l arning, we address a
logical approach to information extraction. The 
system INTHELEX, used to carry out this task, 
requires a logic representation f sentences to 
run the learning algorithm. Hence, the need 
for parsers to produce structured representa- 
tions from raw text. This led us to develop a 
prototypical Italian language parser, as a pre- 
processor in order to obtain the structured rep- 
resentation of sentences required for the sym- 
bolic learner to work. A preliminary exper- 
imentation proved that the logic approach to 
learning from language is able to capture the 
semantics underlying the kind of sentences that 
were processed, even if a comparison with clas- 
sical methods as regards efficiency has still to 
be done. 
1 In t roduct ion  
Language learning has gained growing attention 
in the last years. Statistical approaches, o far 
extensively used - -  see (Saitta and Neri, 1997) 
for an overview of the research in this area --,  
have severe limitations, whereas the flexibility 
and expressivity of logical representations make 
them highly suitable for natural anguage anal- 
ysis (Cussens, 1999). Indeed, logical approaches 
may have a relevant impact at the level of se- 
mantic interpretation, where a logical represen- 
tation of the meaning of a sentence is important 
and useful (Mooney, 1999). 
Logical approaches have been already em- 
ployed in Text Categorization and/or Informa- 
tion Extraction. Yet they try to use an expres- 
sive representation language such as first-order 
logic to define simple properties about tex- 
tual sources, regarded, for instance, as bags of 
words (Junker et al, 1999) or as semi-structured 
texts (Freitag, 2000). These properties are of- 
ten loosely related with the grammar of the 
underlying language, often relying on extra- 
grammatical features (Cohen, 1996). We intend 
to exploit a logic representation for exploiting 
the grammatical structure of texts, as it could 
be detected using a proper parser. Indeed, a 
more knowledge intensive technique is likely to 
perform better applied on the tasks mentioned 
above. 
When no background knowledge about the 
language structure is assumed to be available, 
one of the fundamental problems with the adop- 
tion of logic learning techniques i  that a struc- 
tured representation of sentences is required 
on which the learning algorithm can be run. 
Thus, the need arises for parsers that are able 
to discover such a structure starting from raw, 
unstructured text. Research in this field has 
produced a variety of tools and techniques for 
English, that cannot be applied to other lan- 
guages, such as Italian, because of the differ- 
ent, and sometimes much more complex, gram- 
matical structure. Such considerations led us to 
develop a prototypical Italian language parser, 
that could serve as a pre-processor f texts in 
order to obtain the structured representation f 
sentences that is needed for the symbolic learner 
to work. It is fundamental to note that the fo- 
cus of this paper is not the parser, that does not 
adopt sophisticated NLP techniques. The aim 
here is investigating the feasibility of learning 
semantic definitions for some kinds of sentences. 
Even more so, the availability of a professional 
parser will further enhance the performance of
the whole process. 
Further problems in applying relational learn- 
ing to language are due to the intrinsic compu- 
tational complexity of these methods, as a draw- 
194 
back of the expressive power gained through re- 
lations. Moreover, another weakness of our ap- 
proach could be the dependence on the quality 
of the data coming from the preprocessing step: 
it is possible that noise coming from wrongly 
parsed sentences be present, thus having a neg- 
ative influence towards the model to be induced. 
After briefly presenting in Section 2 the 
parser performance, in order to establish the de- 
gree of reliability of the data on which the learn- 
ing step is performed, Section 3 shows the re- 
sults of applying the first-order learning system 
INTHELEX (Esposito et al, 2000) for the in- 
ference of some simple events related to foreign 
commerce. Lastly, Section 4 draws some prelim- 
inary conclusions on this research and outlines 
future work issues to be addressed. 
2 A S t ra t i f i ed  Parser  for I ta l ian 
Language 
This section presents a parser for the Italian 
language, based on context-free grammars and 
designed to manage texts having a simple and 
standard phrase structure (e.g., foreign com- 
merce texts as opposed to poetry texts). It 
is composed by 12 parsing levels and 106 pro- 
duction rules, and uses the longest-match tech- 
nique, which complies with the typical ambigu- 
ity of Italian language. Syntactic lookahead is 
used to overcome ambiguity and to prevent he 
parsing from stopping in case of grammatically 
wrong input. 
The text is segmented in progressively arger 
syntactic constructs. Subject, main verb, di- 
rect or indirect object and clauses referring to 
them are identified. Nested syntactic onstructs 
at the same abstraction level (e.g., expressions 
including a sentence in parentheses) are sup- 
ported. 
Plain text documents are provided to a lexical 
analyzer and a noun-recognizer (XEROX MUL- 
TEXT), whose output is the document ext 
tagged with parts of speech to be fed to the 
parser. Since Italian grammar is very differ- 
ent from the English one, some terms do not 
have an English equivalent and, hence, cannot 
be translated. 
The parser was validated on a set of 72 
sentences drawn from a corpus of articles on 
foreign commerce available on the Internet, 
and the results obtained were evaluated with 
Parsing Phase Precision Recall 
Noun Groups 0.984 0.992 
lst-level NPs 0.994 0.994 
2nd level NPs 0.983 0.983 
PPs 0.951 0.951 
Clauses 0.840 0.840 
Refined clauses 0.913 0.913 
Sentences 0.736 0.736 
Table 1: Summary of parser validation results 
(Precision/Recall) 
Parsing Phase Errorl  Error2 
Noun Groups 0.787% 0.793% 
1st level NPs 0.596% 0.596% 
2nd level NPs 1.666% 1.666% 
PPs 4.918% 4.918% 
Clauses 15.941% 15.941% 
Refined clauses 8.695% 8.695% 
Sentences 26.384% 26.384% 
Table 2: Summary of parser validation results 
(Errorl/Error2) 
respect o precision, recall (reported in Table 1) 
and two measures about error ratio: 
Errorl = # errors/# total constituents extracted 
Error2 = # errors/# total correct constituents expected 
(see Table 2). 
3 In fo rmat ion  ext rac t ion  
The grammar above was used to parse Italian 
texts downloaded from the Internet, and con- 
cerning foreign commerce. Through such pre- 
processing, the aim was to obtain some struc- 
ture for those texts that could then be trans- 
lated in the input language of the learning sys- 
tem INTHELEX (Esposito et al, 2000) in order 
to make it learn simple events concerning that 
domain. 
INTHELEX (INcremental THEory Learner 
from EXamples) is a fully incremental, multi- 
conceptual closed loop learning system for the 
induction of hierarchical theories from exam- 
ples. In detail, full incrementality avoids the 
need of a previously generated version of the 
theory to be available, so that learning can start 
from an empty theory and from the first exam- 
195 
ple; multi-conceptual means that it ,:an learn si- 
multaneously various concepts, possibly related 
to each other; a closed loop system is a system 
in which the learned theory is checked to be 
valid on any new example available, and in case 
of failure a revision process is activated on it, 
in order to restore the completeness and consis- 
tency properties. 
Incremental learning is necessary when either 
incomplete information is available at the time 
of initial theory generation, or the nature of the 
concepts evolves dynamically. The latter situ- 
ation is the most difficult to handle since time 
evolution needs to be considered. In any case, 
it is useful to consider learning as a closed loop 
process, where feedback on performance is used 
to activate the theory revision phase. 
INTHELEX learns theories, from positive and 
negative examples described in the same lan- 
guage. It adopts a full memory storage strategy 
- -  i.e., it retains all the available xamples, thus 
the learned theories are guaranteed to be valid 
on the whole set of known examples. 
In the formal representation of texts, we used 
the following descriptors: 
? sent  (e l ,e2)  e2 is a sentence fi:om el 
? subj  (e l ,e2)  e2 is the subject of el 
? obj (e l ,e2)  e2 is the (direct) object of el 
? i nd i rec t_ob j  (e l  ,e2) e2 is an indirect ob- 
ject of el 
? re l _sub j  (e l ,e2)  e2 is a clause related to 
the subject el 
? re l _ob j (e l ,e2)  e2 is a clause related to 
the object el 
? verb(e l ,e2)  e2 is the verb of el 
? lemma(e2) word e2 has lemma lemma 
? i n f in i te (e2)  verb e2 is in an infinite 
mood 
? f in i te  (e2) verb e2 is in a finite mood 
? a f f i rmat ive(e2)  verb e2 is in an affirma- 
tive mood 
? negat ive(e2)  verb e2 is in a negative 
mood 
? np(e l ,e2)  e2 is a 2nd level NP of el 
? pp(e l ,e2)  e2 is a PP  of el 
where lemma is a meta-predicate. This allows 
the system to exploit information about word 
lemmas in generalizations/specializations, a d
in the recognition of higher level concepts of 
which lemma is an instance. 
Thus, the following Horn clause is an instance 
of an example: 
imports (example) ~-- 
sent (example ,el), 
subj (example, e2), 
np(e2,e3), 
impresa(e3), 
rel_subj (el ,e4), 
verb (e4, e5), 
specializzare (e5), 
infinite (e5), 
affirmative (e5), 
pp(e4,e6), 
distrubuzione (e6), 
componente (e6), 
verb(el,eT), 
interessare (e7), 
finite (eT), 
affirmative (eT), 
indirect_obj (el, e8), 
pp(e8,e9), 
importazione (eg), 
macchina(e9) , 
produzione (e9), 
ombrello (eg). 
A first experiment aimed at learning the con- 
cept of specialization (of someone in some field). 
The system was run on 40 examples, 24 posi- 
tive and 16 negative. The resulting theory was 
made up by 5 clauses, some of which differ just 
in one literal (e.g., the lemma of the word in the 
subject). By exploiting the background knowl- 
edge that terms 'impresa', 'societY', 'ditta' and 
'agenzia' are all instances of the concept 'per- 
sona giuridica', i.e. clauses: 
persona_g iur id ica  (X) +- 
d i t ta (X) .  
persona_g iur id ica  (X) +-- 
soc ie ta (X) .  
persona_g iur id ica (X)  ~- 
impresa(X) .  
persona_g iur id ica (X)  +- 
agenz ia (X) .  
196 
the theory becomes more compact, yielding 
the following rules: 
specialization(A)~- 
sent(A,B), 
subj(B,C), np(C,D), 
persona_giuridica(D), 
verb(B,E), 
specializzare(E), 
finite(E), 
affirmative(E), 
indirect_obj(B,F), 
pp(F,_). 
specialization(A)~- 
sent(A,B), 
subj(B,C), np(C,D), 
persona_giuridica(D), 
rel_subj(B,E), 
verb(E,F), 
specializzare(F), 
affirmative(F), 
pp(E,_), verb(B,_). 
Another experiment aimed at learning the 
concept of "imports". INTHELEX was run 
starting from the empty theory, and was fed 
with a total of 67 examples (39 positive and 28 
negative). It should be noted that not all posi- 
tive examples explicitly use verb 'importare' (to 
import): e.g., in the sentence "Societ& belga, 
specializzata nella lavorazione del legno, cerca 
fornitori di legname" the imports event is char- 
acterized by the noun 'societ&' (society) as the 
sentence subject, by the verb 'cercare' (to look 
for) and by the object including the noun 'for- 
nitore' (provider). We obtained the following 
results (in which the above background knowl- 
edge was used to compress more rules into one, 
too): 
imports(A) ~- sent(A,B), 
subj(B,C), np(C,D), 
persona_giuridica(D), 
verb(B,E), 
cercare(E), 
finite(E), 
affirmative(E), 
obj(B,F), np(F,G), 
fornitore(G). 
imports(A) ~-sent(A,B), 
subj(B,C), np(C,D), 
persona_giuridica(D), 
societa(D), 
verb(B,E), 
cercare(E), 
finite(E), 
affirmative(E), 
obj(B,F), np(F,G), 
distributore(G). 
imports(A) ~-sent(A,B), 
subj(B,C), np(C,D), 
persona_giuridica(D), 
verb(B,E), 
interessare(E), 
finite(E), 
affirmative(E), 
indirect_obj(B,F), 
pp(F,G), 
importazione(G). 
imports(A) ~-sent (A ,B) ,  
subj(B,C),  np(C,D), 
persona_giur id ica(D) ,  
verb(B,E) ,  
acqu is tare (E) ,  
f in i te (E ) ,  
a f f i rmat ive(E) .  
imports(A) ~-sent(A,B), 
subj(B,C), np(C,D), 
persona_giuridica(D), 
impresa(D), 
verb(B,E), 
importare(E), 
finite(E), 
affirmative(E). 
For instance, the third clause means: "Text A 
deals with imports if it contains a sentence with 
a subject composed by a NP containing a per- 
sona giuridica, the verb of the main sentence 
is interessare (to interest) in finite affirmative 
mood, and the indirect object consists of a PP 
containing the word importazione". Note that, 
by exploiting a background knowledge that rep- 
resents a more complex ontology than the cur- 
rent one, it would be possible to further merge 
conceptual descriptors and, as a consequence, 
clauses in the theory. For example, 'fornitore' 
(provider) and 'distributore' (distributor) could 
be recognized as instances of a common higher 
level concept; the same applies to 'acquistaxe' 
(to buy) and 'importare' (to import). 
197 
4 Conc lus ions  & Future  Work  
We have addressed the problem of learning logic 
theories for information extraction so to bene- 
fit by the semantic interpretation provided by 
a logical approach. This has required struc- 
tured sentences in a logic representation on 
which to run our learning algorithms. Hence, 
we needed a parser to produce structured repre- 
sentations from raw unstructured text. Though 
many techniques have been developed for En- 
glish, they cannot be applied to other languages, 
such as Italian, because of the different gram- 
matical structure. This has led us to develop a 
prototypical Italian language parser, as a pre- 
processor in order to obtain the structured rep- 
resentation ofsentences needed for the symbolic 
learner to work. 
Future work will concern a more extensive ex- 
perimentation, an empirical evaluation of our 
approach, and application of the same kind of 
experiments on English parsed texts. If good 
results will be obtained, it is possible thinking 
to carry out experiments hat take advantage 
also from the structure of semi-structured doc- 
uments. Indeed, we are involved in the project 
CDL (Esposito et al, 1998; Costabile et al, 
1999), that could profit by this kind of tech- 
niques as regard semantic indexing of the stored 
documents (cf. (Chanod, 1999)). 
Re ferences  
J.-P. Chanod. 1999. Natural language processing 
and digital libraries. In M.T. Pazienza, editor, 
Information Extraction, volume 1714 of Lecture 
Notes in Artificial Intelligence Tutorial, pages 17- 
31. Springer. 
W. Cohen. 1996. Learning to classify english text 
with ilp methods. In Luc de Raedt, editor, Ad- 
vances in Inductive Logic Programming, pages 
124-143. IOS Press, Amsterdam, NL. 
M.F. Costabile, F. Esposito, G. Semeraro, and 
N. Fanizzi. 1999. An adaptive visual environment 
for digital libraries. International Journal of Dig- 
ital Libraries, 2:124-143. 
J. Cussens, editor. 1999. Learning Language in 
Logic. Workshop on Learning Language in Logic, 
Workshop Notes. 
F. Esposito, D. Malerba, G. Semeraro, N. Fanizzi, 
and S. Ferilli. 1998. Adding machine learning 
and knowledge intensive techniques to a digital 
library service. International Journal of Digital 
Libraries, 2(1):3-19. 
F. Esposito, G. Semeraro, N. Fanizzi, and S. Ferilli. 
2000. Multistrategy Theory Revision: Induction 
and abduction in INTHELEX. Machine Learn- 
ing, 38(1/2):133-156. 
D. Freitag. 2000. Machine learning for information 
extraction i  informal domains. Machine Learn- 
ing, 39(2/3):169-202. 
M. Junker, M. Sintek, and M. Rinck. 1999. Learning 
for text categorization a d information extraction 
with ILP. In James Cussens, editor, Proceedings 
of the First International Workshop on Learning 
Language in Logic - LLL99. 
R. Mooney. 1999. Learning for semantic interpre- 
tation: Scaling up without dumbing down. In 
Cussens (Cussens, 1999), pages 7-15. Workshop 
on Learning Language in Logic, Workshop Notes. 
L. Saitta and F. Neff. 1997. Machine learning for 
information extraction. In M.T. Pazienza, editor, 
Information Extraction, volume 1299 of Lecture 
Notes in Artificial Intelligence Tutorial, pages 
171-191. Springer. 
198 
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398?401,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNIBA: JIGSAW algorithm for Word Sense Disambiguation
P. Basile and M. de Gemmis and A.L. Gentile and P. Lops and G. Semeraro
Department of Computer Science - University of Bari - Via E. Orabona, 4 70125 Bari ITALY
{basilepp, degemmis, al.gentile, lops, semeraro}@di.uniba.it
Abstract
Word Sense Disambiguation (WSD) is tra-
ditionally considered an AI-hard problem.
A breakthrough in this field would have a
significant impact on many relevant web-
based applications, such as information re-
trieval and information extraction. This pa-
per describes JIGSAW, a knowledge-based
WSD system that attemps to disambiguate
all words in a text by exploiting WordNet1
senses. The main assumption is that a spe-
cific strategy for each Part-Of-Speech (POS)
is better than a single strategy. We evalu-
ated the accuracy of JIGSAW on SemEval-
2007 task 1 competition2. This task is an
application-driven one, where the applica-
tion is a fixed cross-lingual information re-
trieval system. Participants disambiguate
text by assigning WordNet synsets, then the
system has to do the expansion to other lan-
guages, index the expanded documents and
run the retrieval for all the languages in
batch. The retrieval results are taken as a
measure for the effectiveness of the disam-
biguation.
1 The JIGSAW algorithm
The goal of a WSD algorithm consists in assigning
a word w
i
occurring in a document d with its appro-
priate meaning or sense s, by exploiting the context
C in where w
i
is found. The context C for w
i
is de-
fined as a set of words that precede and follow w
i
.
The sense s is selected from a predefined set of pos-
sibilities, usually known as sense inventory. In the
proposed algorithm, the sense inventory is obtained
from WordNet 1.6, according to SemEval-2007 task
1 instructions. JIGSAW is a WSD algorithm based
on the idea of combining three different strategies to
disambiguate nouns, verbs, adjectives and adverbs.
The main motivation behind our approach is that
1http://wordnet.princeton.edu/
2http://www.senseval.org/
the effectiveness of a WSD algorithm is strongly
influenced by the POS tag of the target word. An
adaptation of Lesk dictionary-based WSD algorithm
has been used to disambiguate adjectives and ad-
verbs (Banerjee and Pedersen, 2002), an adaptation
of the Resnik algorithm has been used to disam-
biguate nouns (Resnik, 1995), while the algorithm
we developed for disambiguating verbs exploits the
nouns in the context of the verb as well as the nouns
both in the glosses and in the phrases that WordNet
utilizes to describe the usage of a verb. JIGSAW
takes as input a document d = {w
1
, w
2
, . . . , w
h
} and
returns a list of WordNet synsets X = {s
1
, s
2
, . . . ,
s
k
} in which each element s
i
is obtained by disam-
biguating the target word w
i
based on the informa-
tion obtained from WordNet about a few immedi-
ately surrounding words. We define the context C of
the target word to be a window of n words to the left
and another n words to the right, for a total of 2n
surrounding words. The algorithm is based on three
different procedures for nouns, verbs, adverbs and
adjectives, called JIGSAW
nouns
, JIGSAW
verbs
,
JIGSAW
others
, respectively. More details for each
one of the above mentioned procedures follow.
1.1 JIGSAW
nouns
The procedure is obtained by making some varia-
tions to the algorithm designed by Resnik (1995) for
disambiguating noun groups. Given a set of nouns
W = {w
1
, w
2
, . . . , w
n
}, obtained from document
d, with each w
i
having an associated sense inven-
tory S
i
= {s
i1
, s
i2
, . . . , s
ik
} of possible senses, the
goal is assigning each w
i
with the most appropri-
ate sense s
ih
? S
i
, according to the similarity of
w
i
with the other words in W (the context for w
i
).
The idea is to define a function ?(w
i
, s
ij
), w
i
? W ,
s
ij
? S
i
, that computes a value in [0, 1] representing
the confidence with which word w
i
can be assigned
with sense s
ij
. The intuition behind this algorithm
is essentially the same exploited by Lesk (1986) and
other authors: The most plausible assignment of
senses to multiple co-occurring words is the one that
maximizes relatedness of meanings among the cho-
398
sen senses. JIGSAW
nouns
differs from the original
algorithm by Resnik (1995) in the similarity mea-
sure used to compute relatedness of two senses. We
adopted the Leacock-Chodorow measure (Leacock
and Chodorow, 1998), which is based on the length
of the path between concepts in an IS-A hierarchy.
The idea behind this measure is that similarity be-
tween two synsets, s
1
and s
2
, is inversely propor-
tional to their distance in the WordNet IS-A hierar-
chy. The distance is computed by finding the most
specific subsumer (MSS) between s
1
and s
2
(each
ancestor of both s
1
and s
2
in the WordNet hierar-
chy is a subsumer, the MSS is the one at the lowest
level) and counting the number of nodes in the path
between s
1
and s
2
that traverse their MSS. We ex-
tended this measure by introducing a parameter k
that limits the search for the MSS to k ancestors (i.e.
that climbs the WordNet IS-A hierarchy until either
it finds the MSS or k + 1 ancestors of both s
1
and
s
2
have been explored). This guarantees that ?too
abstract? (i.e. ?less informative?) MSSs will be ig-
nored. In addition to the semantic similarity func-
tion, JIGSAW
nouns
differs from the Resnik algo-
rithm in the use of:
1. a Gaussian factor G, which takes into account the dis-
tance between the words in the text to be disambiguated;
2. a factor R, which gives more importance to the synsets
that are more common than others, according to the fre-
quency score in WordNet;
3. a parametrized search for the MSS between two concepts
(the search is limited to a certain number of ancestors).
Algorithm 1 describes the complete procedure for
the disambiguation of nouns. This algorithm consid-
ers the words in W pairwise. For each pair (w
i
,w
j
),
the most specific subsumer MSS
ij
is identified, by
reducing the search to depth1 ancestors at most.
Then, the similarity sim(w
i
, w
j
, depth2) between
the two words is computed, by reducing the search
for the MSS to depth2 ancestors at most. MSS
ij
is
considered as supporting evidence for those synsets
s
ik
in S
i
and s
jh
in S
j
that are descendants of
MSS
ij
. The MSS search is computed choosing the
nearest MSS in all pairs of synsets s
ik
,s
jh
. Like-
wise, the similarity for (w
i
,w
j
) is the max similarity
computed in all pairs of s
ik
,s
jh
and is weighted by
a gaussian factor that takes into account the posi-
tion of w
i
and w
j
in W (the shorter is the distance
Algorithm 1 The procedure for disambiguating
nouns derived from the algorithm by Resnik
1: procedure JIGSAW
nouns
(W, depth1, depth2) 
finds the proper synset for each polysemous noun in the set
W = {w
1
, w
2
, . . . , w
n
}, depth1 and depth2 are used in
the computation of MSS
2: for all w
i
, w
j
? W do
3: if i < j then
4: sim ? sim(w
i
, w
j
, depth1) ?
G(pos(w
i
), pos(w
j
))  G(x, y) is a Gaussian
function which takes into account the difference between
the positions of w
i
and w
j
5: MSS
ij
? MSS(w
i
, w
j
, depth2) 
MSS
ij
is the most specific subsumer between w
i
and w
j
,
search for MSS restricted to depth2 ancestors
6: for all s
ik
? S
i
do
7: if is-ancestor(MSS
ij
,s
ik
) then  if
MSS
ij
is an ancestor of s
ik
8: sup
ik
? sup
ik
+ sim
9: end if
10: end for
11: for all s
jh
? S
j
do
12: if is-ancestor(MSS
ij
,s
jh
) then
13: sup
jh
? sup
jh
+ sim
14: end if
15: end for
16: norm
i
? norm
i
+ sim
17: norm
j
? norm
j
+ sim
18: end if
19: end for
20: for all w
i
? W do
21: for all s
ik
? S
i
do
22: if norm
i
> 0 then
23: ?(i, k) ? ? ? sup
ik
/norm
i
+ ? ? R(k)
24: else
25: ?(i, k) ? ?/|S
i
| + ? ? R(k)
26: end if
27: end for
28: end for
29: end procedure
between the words, the higher is the weight). The
value ?(i, k) assigned to each candidate synset s
ik
for the word w
i
is the sum of two elements. The
first one is the proportion of support it received, out
of the support possible, computed as sup
ik
/norm
i
in Algorithm 1. The other element that contributes
to ?(i, k) is a factor R(k) that takes into account
the rank of s
ik
in WordNet, i.e. how common is the
sense s
ik
for the word w
i
. R(k) is computed as:
R(k) = 1 ? 0.8 ?
k
n ? 1
(1)
where n is the cardinality of the sense inventory S
i
for w
i
, and k is the rank of s
ik
in S
i
, starting from 0.
Finally, both elements are weighted by two pa-
rameters: ?, which controls the contribution given
399
to ?(i, k) by the normalized support, and ?, which
controls the contribution given by the rank of s
ik
.
We set ? = 0.7 and ? = 0.3. The synset assigned
to each word in W is the one with the highest ?
value. Notice that we used two different parameters,
depth1 and depth2 for setting the maximum depth
for the search of the MSS: depth1 limits the search
for the MSS computed in the similarity function,
while depth2 limits the computation of the MSS
used for assigning support to candidate synsets. We
set depth1 = 6 and depth2 = 3.
1.2 JIGSAW
verbs
Before describing the JIGSAW
verbs
procedure, the
description of a synset must be defined. It is the
string obtained by concatenating the gloss and the
sentences that WordNet uses to explain the usage
of a synset. First, JIGSAW
verbs
includes, in the
context C for the target verb w
i
, all the nouns in
the window of 2n words surrounding w
i
. For each
candidate synset s
ik
of w
i
, the algorithm computes
nouns(i, k), that is the set of nouns in the descrip-
tion for s
ik
.
max
jk
= max
w
l
?nouns(i,k)
{sim(w
j
,w
l
,depth)} (2)
where sim(w
j
,w
l
,depth) is defined as in
JIGSAWnouns. In other words, max
jk
is the
highest similarity value for w
j
wrt the nouns related
to the k-th sense for w
i
. Finally, an overall simi-
larity score among s
ik
and the whole context C is
computed:
?(i, k) = R(k) ?
P
w
j
?C
G(pos(w
i
), pos(w
j
)) ? max
jk
P
h
G(pos(w
i
), pos(w
h
))
(3)
where R(k) is defined as in Equation 1 with a differ-
ent constant factor (0.9) and G(pos(w
i
), pos(w
j
)) is
the same Gaussian factor used in JIGSAWnouns,
that gives a higher weight to words closer to the tar-
get word. The synset assigned to w
i
is the one with
the highest ? value. Algorithm 2 provides a detailed
description of the procedure.
1.3 JIGSAW
others
This procedure is based on the WSD algorithm pro-
posed by Banerjee and Pedersen (2002). The idea is
to compare the glosses of each candidate sense for
Algorithm 2 The procedure for the disambiguation
of verbs
1: procedure JIGSAW
verbs
(w
i
, d, depth)  finds the
proper synset of a polysemous verb w
i
in document d
2: C ? {w
1
, ..., w
n
}  C is
the context for w
i
. For example, C = {w
1
, w
2
, w
4
, w
5
},
if the sequence of words {w
1
, w
2
, w
3
, w
4
, w
5
} occurs in d,
w
3
being the target verb, w
j
being nouns, j 6= 3
3: S
i
? {s
i1
, ...s
im
}  S
i
is the sense inventory for w
i
, that is the set of all candidate
synsets for w
i
returned by WordNet
4: s ? null  s is the synset to be returned
5: score ? ?MAXDOUBLE  score is the
similarity score assigned to s
6: p ? 1  p is the position of the synsets for w
i
7: for all s
ik
? S
i
do
8: max ? {max
1k
, ..., max
nk
}
9: nouns(i, k) ? {noun
1
, ..., noun
z
} 
nouns(i, k) is the set of all nouns in the description of s
ik
10: sumGauss ? 0
11: sumTot ? 0
12: for all w
j
? C do  computation of the similarity
between C and s
ik
13: max
jk
? 0  max
jk
is the highest similarity
value for w
j
, wrt the nouns related to the k-th sense for w
i
.
14: sumGauss ? G(pos(w
i
), pos(w
j
)) 
Gaussian function which takes into account the difference
between the positions of the nouns in d
15: for all noun
l
? nouns(i, k) do
16: sim ? sim(w
j
, noun
l
, depth)  sim is
the similarity between the j-th noun in C and l-th noun in
nouns(i, k)
17: if sim > max
jk
then
18: max
jk
? sim
19: end if
20: end for
21: end for
22: for all w
j
? C do
23: sumTot ? sumTot+G(pos(w
i
), pos(w
j
))?
max
jk
24: end for
25: sumTot ? sumTot/sumGauss
26: ?(i, k) ? R(k) ? sumTot  R(k) is defined as in
JIGSAW
nouns
27: if ?(i, k) > score then
28: score ? ?(i, k)
29: p ? k
30: end if
31: end for
32: s ? s
ip
33: return s
34: end procedure
the target word to the glosses of all the words in its
context. Let W
i
be the sense inventory for the tar-
get word w
i
. For each s
ik
? W
i
, JIGSAW
others
computes the string targetGloss
ik
that contains the
words in the gloss of s
ik
. Then, the procedure
computes the string contextGloss
i
, which contains
the words in the glosses of all the synsets corre-
400
sponding to each word in the context for w
i
. Fi-
nally, the procedure computes the overlap between
contextGloss
i
and targetGloss
ik
, and assigns the
synset with the highest overlap score to w
i
. This
score is computed by counting the words that occur
both in targetGloss
ik
and in contextGloss
i
. If ties
occur, the most common synset in WordNet is cho-
sen.
2 Experiment
We performed the experiment following the instruc-
tions for SemEval-2007 task 1 (Agirre et al, 2007).
JIGSAW is implemented in JAVA, by using JWNL
library3 in order to access WordNet 1.6 dictionary.
We ran the experiment on a Linux-based PC with
Intel Pentium D processor having a speed of 3 GHz
and 2 GB of RAM. The dataset consists of 29,681
documents, including 300 topics. Results are re-
ported in Table 1. Only two systems (PART-A and
PART-B) partecipated to the competition, thus the
organizers decided to add a third system (ORGA-
NIZERS) developed by themselves. The systems
were scored according to standard IR/CLIR mea-
sures as implemented in the TREC evaluation pack-
age4. Our system is labelled as PART-A.
system IR documents IR topics CLIR
no expansion 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
PART-A 0.3030 0.1521 0.1373
PART-B 0.3036 0.1482 0.1734
Table 1: SemEval-2007 task 1 Results
All systems show similar results in IR tasks, while
their behaviour is extremely different on CLIR task.
WSD results are reported in Table 2. These re-
sults are encouraging as regard precision, consid-
ering that our system exploits only WordNet as
kwnoledge-base, while ORGANIZERS uses a su-
pervised method that exploits SemCor to train a
kNN classifier.
3 Conclusions
In this paper we have presented a WSD algorithm
that exploits WordNet as knowledge-base and uses
3http://sourceforge.net/projects/jwordnet
4http://trec.nist.gov/
system precision recall attempted
SENSEVAL-2
ORGANIZERS 0.584 0.577 93.61%
PART-A 0.498 0.375 75.39%
PART-B 0.388 0.240 61.92%
SENSEVAL-3
ORGANIZERS 0.591 0.566 95.76%
PART-A 0.484 0.338 69.98%
PART-B 0.334 0.186 55.68%
Table 2: WSD results on all-words task
three different methods for each part-of-speech. The
algorithm has been evaluated by SemEval-2007 task
1. The system shows a good performance in all
tasks, but low precision in CLIR evaluation. Prob-
ably, the negative result in CLIR task depends on
complex interaction of WSD, expansion and index-
ing. Contrarily to other tasks, organizers do not plan
to provide a ranking of systems on SemEval-2007
task 1. As a consequence, the goal of this task - what
is the best WSD system in the context of a CLIR
system? - is still open. This is why the organizers
stressed in the call that this was ?a first try?.
References
E. Agirre, B. Magnini, o. Lopez de Lacalle, A. Otegi,
G. Rigau, and Vossen. 2007. Semeval-2007 task
1: Evaluating wsd on cross-language information re-
trieval. In Proceedings of SemEval-2007. Association
for Computational Linguistics.
S. Banerjee and T. Pedersen. 2002. An adapted lesk
algorithm for word sense disambiguation using word-
net. In CICLing?02: Proc. 3rd Int?l Conf. on Com-
putational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense identifi-
cation. In C. Fellbaum (Ed.), WordNet: An Electronic
Lexical Database, pages 305?332. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of the 1986
SIGDOC Conference, pages 20?29. ACM Press.
P. Resnik. 1995. Disambiguating noun groupings with
respect to WordNet senses. In Proceedings of the
Third Workshop on Very Large Corpora, pages 54?68.
Association for Computational Linguistics.
401
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1591?1600, Dublin, Ireland, August 23-29 2014.
An Enhanced Lesk Word Sense Disambiguation Algorithm through a
Distributional Semantic Model
Pierpaolo Basile Annalina Caputo
Department of Computer Science, University of Bari Aldo Moro
Via E. Orabona, 4, Bari - 70125 Italy
{pierpaolo.basile,annalina.caputo,giovanni.semeraro}@uniba.it
Giovanni Semeraro
Abstract
This paper describes a new Word Sense Disambiguation (WSD) algorithm which extends two
well-known variations of the Lesk WSD method. Given a word and its context, Lesk algorithm
exploits the idea of maximum number of shared words (maximum overlaps) between the context
of a word and each definition of its senses (gloss) in order to select the proper meaning. The main
contribution of our approach relies on the use of a word similarity function defined on a distribu-
tional semantic space to compute the gloss-context overlap. As sense inventory we adopt Babel-
Net, a large multilingual semantic network built exploiting both WordNet and Wikipedia. Besides
linguistic knowledge, BabelNet also represents encyclopedic concepts coming from Wikipedia.
The evaluation performed on SemEval-2013 Multilingual Word Sense Disambiguation shows
that our algorithm goes beyond the most frequent sense baseline and the simplified version of the
Lesk algorithm. Moreover, when compared with the other participants in SemEval-2013 task,
our approach is able to outperform the best system for English.
1 Introduction
Unsupervised Word Sense Disambiguation (WSD) algorithms aim at resolving word ambiguity with-
out the use of annotated corpora. Among these, two categories of knowledge-based algorithms gained
popularity: overlap- and graph-based methods. The former owes its success to the simple intuition un-
derlying that family of algorithms, while the diffusion of the latter started growing after the development
of semantic networks.
The overlap-based algorithms stem from the Lesk (1986) one, which inspired a whole family of meth-
ods that exploit the number of common words in two sense definitions (glosses) to select the proper
meaning in a context. Glosses play a key role in Lesk algorithm, which exploits only two types of in-
formation: 1) the set of dictionary entries, one for each possible word meaning, and 2) the information
about the context in which the word occurs. The idea is simple: given two words, the algorithm selects
those senses whose definitions have the maximum overlap, i.e. the highest number of common words in
the definition of the senses. In order to extract definitions, Lesk adopted the Oxford Advanced Learner?s
dictionary. This approach suffers from two problems: 1) complexity, the number of comparisons in-
creases combinatorially with the number of words in a text; and 2) definition expressiveness, the overlap
is based only on word co-occurrences in glosses. The first problem was tackled by a ?simplified? version
of Lesk algorithm (Kilgarriff and Rosenzweig, 2000), which disambiguated each word separately. Given
a word, the meaning whose gloss shows the maximum overlap with the current context, represented by
the surrounding words, is selected. The simplified Lesk significantly outperforms the original Lesk al-
gorithm as proved by Vasilescu et al. (2004). The second problem was faced by Banerjee and Pedersen
(2002), who proposed an ?adapted? Lesk algorithm. The adapted variation exploits relationships among
meanings: each gloss is extended by the definitions of semantically related meanings. Banerjee and
Pedersen adopt WordNet as semantic network and their algorithm takes into account several relations:
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1591
hypernym, hyponym, holonym, meronym, troponym and attribute relation. The adapted algorithm out-
performs the Lesk one in disambiguating nouns in SensEval-2 English task. Despite these improvements,
overlap-based algorithms failed to stand the pace with figures achieved by graph approaches. Their abil-
ity to disambiguate all words in a sequence at once, meanwhile exploiting the existing interconnections
(edges) between senses (nodes), has made these algorithms more principled than methods that use the
sense definition overlaps. Moreover, the success of PageRank in web search has inspired a new vein of
algorithms for sense disambiguation that blossomed during the past years. Although graph-based algo-
rithms have taken advantage of the rich set of relationships available in dictionaries like WordNet, they
completely neglected the role of glosses in the disambiguation process.
From our standpoint, glosses are an important piece of information since they extensionally define a
word meaning. In this paper we propose a revised version of the simplified and adapted Lesk variations
that overcomes limits due to the definition expressiveness. Our method replaces the concept of overlap
with that of similarity. Similarity is computed on a Distributional Semantic Space (DSS) in order to
account for semantic relationships between words occurring in the definition and context, for as they
emerge from the use in the language. Indeed, Distributional Semantics Models (DSM) exploit the ge-
ometric metaphor of meanings, which are represented through points into a space where distance is a
measure of semantic similarity. The point representation inherits information about all co-occurring con-
text words, and then it is suitable for computing the overlap where no exact word matching can occur. In
addition, we introduce two functions: the former assigns an inverse gloss frequency (IGF) score to each
term occurring in the extended gloss, the latter exploits information about sense frequencies extracted
from an annotated corpus.
We choose BabelNet (Navigli and Ponzetto, 2012) as sense inventory. BabelNet is a very large se-
mantic network built up exploiting both WordNet and Wikipedia. Besides linguistic knowledge, it also
represents encyclopedic concepts coming from Wikipedia and information about named entities. This
makes our approach inherently multilingual and suitable for tasks such as named entity disambiguation.
The evaluation on SemEval-2013 Multilingual Word Sense Disambiguation (Navigli et al., 2013) proves
that our method is able to outperform both baselines (simplified Lesk and most frequent sense) and, for
English language, also the best SemEval-2013 participant.
The paper is structured as follows. Section 2 provides a brief introduction to Distributional Semantic
Models, while Section 3 describes the proposed methodology. Evaluation and details about the algo-
rithm implementation are reported in Section 4, while related work is described in Section 5. Finally,
conclusions close the paper.
2 Distributional Semantic Models
Semantic (or Word) Spaces are geometrical spaces of words where vectors express concepts, and their
proximity is a measure of the semantic relatedness. One of their greatest virtues is that they can be
built using entirely unsupervised analysis of free text. Moreover, they make few language-specific as-
sumptions since only tokenized text is needed. WordSpaces are inspired by the distributional hypothesis
(Harris, 1968) whereby the meaning of a word is determined by the rules of its use in the context of
ordinary and concrete language behaviour. This means that words are semantically similar if they share
contexts (surrounding words). Building a WordSpace involves the definition of a distributional model,
that is a quadruple (Lowe, 2001) consisting of: the space basis (word vectors) and dimension; the func-
tion that takes into account word co-occurrences and how these are represented in the final vector; a
similarity function defined over vectors; and eventually a map that transforms the vector space.
Our idea is to apply DSMs to WSD for computing the overlap between the gloss of the meaning and the
context as a similarity measure between their corresponding vector representations in a SemanticSpace.
In this paper we build a SemanticSpace (co-occurrences matrix M ) by analysing the distribution of words
in a large corpus, then M is reduced using Latent Semantic Analysis (LSA) (Landauer and Dumais,
1997). LSA collects the text data in a co-occurrence matrix, which is then decomposed into smaller
matrices with Singular-Value Decomposition (SVD). Hence, LSA represents high-dimensional vectors
in a lower-dimensional space while capturing latent semantic structures in the text data.
1592
Given the vector representation of two words, DSMs usually compute their similarity as the cosine of
the angle between them. In our case, the gloss and the context are composed by several terms, so in order
to compute their similarity we need a method to compose the words occurring in these sentences. It is
possible to combine words through vector addition (+) that corresponds to the point-wise sum of vector
components. For each set of terms, phrase or sentence, we build its vector representation by adding the
vectors associated to the words it is composed of. Then, the similarity measure is computed as the cosine
similarity between the two phrases/sentences. More formally, if g = g
1
g
2
...g
n
and c = c
1
c
2
...c
m
are
the gloss and the context respectively, we build their vector representation g and c in the SemanticSpace
through addition of word vectors belonging to them:
g = g
1
+ g
2
+ . . . + g
n
c = c
1
+ c
2
. . . + c
m
(1)
The cosine similarity between g and c is a measure of the similarity of the two sentences that we consider
as a score associated to the candidate meaning with respect to the context.
3 Methodology
At the heart of our approach there is the simplified Lesk algorithm. Given a text w
1
w
2
...w
n
of n words,
we disambiguate one at a time taking into account the similarity between the gloss associated to each
sense of the target word w
i
and the context. The meaning whose gloss has the highest similarity is
selected. The context could be represented by a subset of surrounding words or the whole text where the
word occurs. Moreover, taking into account the idea of the Banerjee?s adaptation, we expand each gloss
with those of related meanings.
Our sense inventory is BabelNet, a very large multilingual semantic network built relying on both
WordNet and Wikipedia. In BabelNet linguistic knowledge is enriched with encyclopedic concepts
coming from Wikipedia. WordNet synsets and Wikipedia concepts (pages) are connected in an auto-
matic way. We choose BabelNet for three reasons: 1) glosses are richer and contain text from Wikipedia,
2) it is multilingual, thus the proposed algorithm can be applied to several languages, and 3) it also con-
tains information about named entities, thus an algorithm using BabelNet could be potentially used to
disambiguate entities.
Our algorithm consists of five steps:
1. Look-up. For each word w
i
, the set of possible word meanings is retrieved from BabelNet. First,
we look for senses coming from WordNet (or WordNet translated into languages different from
English). If no sense is found, we retrieve senses from Wikipedia. We adopt this strategy because
mixing up all senses from Wikipedia and WordNet results in worse performance. Conversely, if a
word does not occur in WordNet it is probably a named entity, thus Wikipedia could provide useful
information to disambiguate it.
2. Building the context. The context C is represented by the l words to the left and to the right of w
i
.
We also adopt a particular configuration in which the context is represented by all the words that
occur in the text.
3. Gloss expansion. We indicate with s
ij
the j-th sense associated to the target word w
i
. We expand
the gloss g
ij
that describes the j-th sense using the function ?getRelatedMap? provided by BabelNet
API. This method returns all the meanings related to a particular sense. For each related meaning,
we retrieve its gloss and concatenate it to the original gloss g
ij
of s
ij
. During this step we remove
glosses belonging to synsets related by the ?antonym? relationship. The result of this step is an
extended gloss denoted by g
?
ij
. In order to give more importance to terms occurring in the original
gloss, the words in the expanded gloss are weighed taking into account both the distance between
s
ij
and the related synsets and the word frequencies. More details about term scoring are reported
in Subsection 3.2.
1593
4. Building semantic vectors. Exploiting the DSM described in Section 2, we build the vector repre-
sentation for each gloss g
?
ij
associated with the senses of w
i
and the context C.
5. Selecting the correct meaning. For each gloss g
?
ij
, the algorithm computes the cosine similarity
between its vector representation and context vector C. The similarity is linearly combined with the
probability p(s
ij
|w
i
) that takes into account the sense distribution of s
ij
given the word w
i
; details
are reported in Subsection 3.1. The sense with the highest similarity is chosen.
In order to compare our approach to the simplified Lesk algorithm, we developed a variation of our
method in which, rather than building the semantic vectors, we count the common words between each
extended gloss g
?
ij
and the context C. In this case, we apply stemming to maximize the overlap.
3.1 Sense Distribution
The selection of the correct meaning takes also into account the senses distribution of the word w
i
. We
retrieve information about sense occurrences from WordNet (Fellbaum, 1998), which reports for each
word w
i
its sense inventory S
i
with the number of times that the word w
i
was tagged with s
ij
in SemCor.
SemCor is a collection of 352 documents manually annotated with WordNet synsets. We introduce the
sense distribution factor in order to consider the probability that a word w
i
can be tagged with the sense
s
ij
. Moreover, since some synsets do not occur in SemCor and can cause zero probabilities, we adopt an
additive smoothing (also called Laplace smoothing). Finally the probability is computed as follow:
p(s
ij
|w
i
) =
t(w
i
, s
ij
) + 1
#w
i
+ |S
i
|
(2)
where t(w
i
, s
ij
) is the number of times the word w
i
is tagged with s
ij
and #w
i
is the number of occur-
rences of w
i
in SemCor.
3.2 Gloss Term Scoring
The extended gloss conflates words from the gloss directly associated with the synset s
ij
with those of
the glosses appearing in the related synsets. When we add words to the extended gloss, we weigh them
by a factor inversely proportional to the distance in the graph (number of edges) between s
ij
and the
related synsets so to reflect their different origin. Let d be that distance, then the weight is computed
as
1
1+d
. Finally, we re-weigh words using a strategy similar to the inverse document frequency (IDF )
that we call inverse gloss frequency (IGF ). The idea is that if a word occurs in all the extended glosses
associated with a word, then it poorly characterizes the meaning description. Let gf
?
k
be the number of
extended glosses that contain a word w
k
, then IGF is computed as follow:
IGF
k
= 1 + log
2
|S
i
|
gf
?
k
(3)
This approach is similar to the idea proposed by Vasilescu et al. (2004), where TF-IDF of terms is
computed taking into account the glosses in the whole WordNet, while we compute IGF considering
only the glosses associated to each word. Finally, the weight for the word w
k
appearing h times in the
extended gloss g
?
ij
is given by:
weight(w
k
, g
?
ij
) = h? IGF
k
?
1
1 + d
(4)
4 Evaluation
The evaluation is performed using the dataset provided by the organizers of the Task-12 of SemEval-
2013. The task concerns Multilingual Word Sense Disambiguation, a traditional WSD ?all-words? ex-
periment in which systems are expected to assign the correct BabelNet synset to all occurrences of noun
phrases (which refer to both disambiguated nouns and named entities) within arbitrary texts in different
languages.
1594
The goal of our evaluation is twofold: to prove that our strategy outperforms the simplified Lesk
approach, and then to compare our system with respect to the other task participants.
In both the experiments we consider two languages, English and Italian, to evaluate performance in a
multilingual setting. It is important to underline that in our approach only stemming and the corpus used
to build the distributional model are language dependent.
4.1 System Setup
Our method
1
is completely developed in JAVA using BabelNet API 1.1.1 provided by the authors
2
. We
adopt the standard Lucene analyzer to tokenize both glosses and the context, while Snowball library
3
is
used for stemming in several European languages. The SemanticSpaces for the two languages are built
using proprietary code relying on two Lucene indexes, which contain documents from British National
Corpus (BNC) for English, and from Wikipedia dump for Italian, respectively. For each language, the
co-occurrences matrix M contains information about the top 100,000 most frequent words in the corpus.
M is reduced by LSA using the SVDLIBC tool
4
and setting a reduced dimension equal to 200. The
result of the SVD decomposition is stored in a binary format used by our algorithm implementation.
It is important to underline that BabelNet Italian glosses are taken from MultiWordNet, which does
not contain glosses for all the synsets. Then, we replace each missing gloss by the words that belong to
the synset.
We evaluate our system by setting two parameters: 1) the context size (3, 5, 10, 20 and the whole text);
2) the use of information about sense distribution (see Formula (2) in Subsection 3.1).
The gloss term scoring function is always applied, since it provides better results. The synset distance
d used to expand the gloss is fixed to 1; we experimented with a distance d set to 2 without any improve-
ment. The sense distribution is linearly combined with the cosine similarity score through a coefficient
set to 0.5.
Some notes about sense frequency: by using only sense distribution to select a sense we obtain an
algorithm that performs like the most frequent sense (MFS). In other words, the algorithm always assigns
the most probable meaning. It is well known that this approach obtains very good performance and it is
hard to be outperformed especially by unsupervised approaches.
4.2 English Evaluation
Table 1 reports results of our algorithm (DSM) compared with the best simplified Lesk approaches
(LESK) in terms of precision (P), recall (R), F-measure (F) and attempt (A). Attempt is the percentage
of words disambiguated by the algorithm. SenseDistr. column reports the information about when the
sense distribution formula (see Subsection 3.1) is used (Y) or not (N); it is also important to point out
that MSF produces the same results of using only sense distribution i.e. the first sense is the most likely
one. We have experimented different context sizes also for the Lesk algorithm, although for the sake of
readability we report only the best Lesk with and without sense distribution.
All our runs always obtain an attempt of 100%; thus the precision and recall values are always the
same. The run EN.DSM.10 obtains the best result using both sense distribution information and the
whole text (W ) as context. Another important outcome is the result obtained by the run EN.DSM.5 that,
without information about sense distribution, is able to overcome the MFS baseline. To the best of our
knowledge, this is the first completely unsupervised system able to overcome the MFS baseline without
using sense frequency. Both results (EN.DSM.10 and EN.DSM.5) suggest that the vector representation
of the whole text helps the system to achieve the best performance.
Considering the Lesk method, generally, the best size for the context is 3, then a larger set of sur-
rounding words results in worse performance, differently to what happens in the distributional approach.
This is probably due to the fact that words distant from the target one match some incorrect glosses. It is
important to note that no simplified Lesk run is able to overcome the MFS baseline.
1
Available on line: https://github.com/pippokill/lesk-wsd-dsm
2
Available on line: http://lcl.uniroma1.it/babelnet/download.jsp
3
Available on line: http://snowball.tartarus.org/
4
Available on line: http://tedlab.mit.edu/\textasciitildedr/SVDLIBC/
1595
Run ContextSize SenseDistr. P R F A
MFS - - 0.656 0.656 0.656 100%
EN.LESK.1 3 N 0.525 0.525 0.525 100%
EN.LESK.6 3 Y 0.633 0.633 0.633 100%
EN.DSM.1 3 N 0.536 0.536 0.536 100%
EN.DSM.2 5 N 0.605 0.605 0.605 100%
EN.DSM.3 10 N 0.633 0.633 0.633 100%
EN.DSM.4 20 N 0.650 0.650 0.650 100%
EN.DSM.5 W N 0.687 0.687 0.687 100%
EN.DSM.6 3 Y 0.669 0.669 0.669 100%
EN.DSM.7 5 Y 0.677 0.677 0.677 100%
EN.DSM.8 10 Y 0.689 0.689 0.689 100%
EN.DSM.9 20 Y 0.696 0.696 0.696 100%
EN.DSM.10 W Y 0.715 0.715 0.715 100%
Table 1: Results of the English evaluation.
Comparing DSM-based with simplified Lesk approaches, the former consistently outperform Lesk-
based algorithms when considering the use (or not) of sense distribution.
4.3 Italian Evaluation
Table 2 reports results of our algorithm for the Italian language. In this case we still obtain the best result
(IT.DSM.10) using DSM and sense distribution. As for English, the systems without sense distribution
overcome the MFS baseline. However, in this case simplified Lesk with sense distribution is able to
outperform the MFS. We ascribe this different behaviour to the problem of missing glosses that we
solved by adding the words in the synset.
Run ContextSize SenseDistr. P R F A
MFS - - 0.572 0.572 0.572 100%
IT.LESK.2 5 N 0.531 0.530 0.530 99.71%
IT.LESK.10 W Y 0.608 0.606 0.607 99.71%
IT.DSM.1 3 N 0.611 0.609 0.610 99.71%
IT.DSM.2 5 N 0.608 0.607 0.607 99.71%
IT.DSM.3 10 N 0.627 0.625 0.626 99.71%
IT.DSM.4 20 N 0.629 0.627 0.628 99.71%
IT.DSM.5 W N 0.634 0.632 0.633 99.71%
IT.DSM.6 3 Y 0.632 0.630 0.631 99.71%
IT.DSM.7 5 Y 0.631 0.629 0.630 99.71%
IT.DSM.8 10 Y 0.636 0.634 0.635 99.71%
IT.DSM.9 20 Y 0.640 0.638 0.639 99.71%
IT.DSM.10 W Y 0.642 0.640 0.641 99.71%
Table 2: Results of the Italian evaluation.
4.4 Task Results
In this subsection, we report our best performance (Table 3) with respect to the other participants in the
SemEval-2013 Task-12 on multilingual Word Sense Disambiguation, for both English (Table 3a) and
Italian (Table 3b).
Regarding the English language, our best methods are able to outperform all the systems. It is im-
portant to underline that our method without knowledge about sense distribution (EN.DSM.5) outper-
forms both the MFS and all the task participants. This is a very important outcome because generally
1596
System F
EN.DSM.10 0.715
EN.DSM.5 0.687
UMCC-DLSI-2 0.685
UMCC-DLSI-3 0.680
UMCC-DLSI-1 0.677
MFS 0.656
DAEBAK 0.604
GETALP-BN-1 0.263
GETALP-BN-2 0.266
(a) English
System F
UMCC-DLSI-2 0.658
UMCC-DLSI-1 0.657
IT.DSM.10 0.641
IT.DSM.5 0.633
DAEBAK 0.613
MFS 0.572
GETALP-BN-2 0.325
GETALP-BN-1 0.324
(b) Italian
Table 3: Results of our best systems with respect to the Semeval-2013 participants.
knowledge-base approaches without information about sense frequencies obtain low results. For exam-
ple, the UMCC-DLSI system (Guti?errez et al., 2013) exploits sense frequency to modify prior probability
of synset nodes in the PageRank, and DAEBAK system (Manion and Sainudiin, 2013) uses MFS when it
is not able to select a meaning. Our experiments show that a dictionary-based approach and the adoption
of a distributional semantic model for computing the similarity are enough to obtain good results. More-
over, by adding information about sense frequencies we are able to boost our performance and obtain
over 70% of F-measure.
For Italian, our systems are not able to reach the same performance as for English, although they still
outperform the MFS and two task participants. We think that these results are due to the same problem
observed for the Italian evaluation (Subsection 4.3), that is to say, the poor quality of glosses.
5 Related Work
WSD has been an active area of NLP whose roots stem from early work in Machine Translation. Am-
biguity resolution has been pursued as a way to improve retrieval systems, and generally to get better
information access. Despite its ancient roots and perceived importance, this task is still far from being
resolved.
Our WSD method relies on both the Lesk algorithm and its two variants: simplified (Kilgarriff and
Rosenzweig, 2000) and adapted (Banerjee and Pedersen, 2002). Several approaches have modified the
Lesk algorithm to reduce is exponential complexity, like the one based on Simulated Annealing (Cowie
et al., 1992). Basile et al. (2007) adopted the simplified Lesk algorithm to disambiguate adjectives
and adverbs, combining it with other two methods for nouns and verbs: the combination of different
approaches for each part-of-speech resulted in better performance with respect to the use of a single
strategy. More recently, Schwab et al. (2013) proposed GETALP, another unsupervised WSD algorithm
inspired by Lesk. Their approach computes a local similarity using the classical Lesk measure (overlap
between glosses), and then the local similarity is propagated to the whole text (global similarity) using
an algorithm inspired by the Ant Colony. This approach got the lowest results during the SemEval-2013
Task 12 evaluation due to a bug in the system. However, the correct implementation achieves 0.583 of
F-measure for English and 0.528 for Italian.
Another problem with the Lesk-based approaches is to maximize the chances of overlap between
glosses or between the gloss and the context. To solve this problem, Ponzetto and Navigli (2010) ex-
tended WordNet with Wikipedia pages (WordNet++) in order to produce a richer lexical resource, ob-
taining the English portion of BabelNet. The simple Lesk algorithm built over WordNet++ outperformed
the WordNet-base version, although it was not successful in overtaking the MFS baseline. Our approach
tries to extend glosses using related synsets and adopts distributional semantics to address the problem
of data sparsity. A different perspective has been recently proposed by Wang and Hirst (2014), where the
limits of the exact string matching between glosses and context are overcome by a Naive Bayes-based
similarity measure.
1597
Other unsupervised approaches rely on graph algorithms that exploit the graph generated by a semantic
network where the senses are connected through semantic relations. For example, DAEBAK (Manion
and Sainudiin, 2013) adopts a sub-graph of BabelNet generated taking into account the surrounding
words of the target word. A measure of connectivity computed on the sub-graph is used to extract the
most probable sense. The MFS is used when the algorithm is not able to choose any sense. Also Navigli
and Lapata (2010) exploit the idea of graph connectivity measures for identifying the most important
node (sense) in the graph. Experiments conducted on SemCor show that the Degree Centrality provides
best results compared to other well known techniques, such as PageRank, HITS, Key Player Problem
and Betweenness Centrality. Graph-based methods also showed their validity during the SemEval 2013
Multilingual Word Sense Disambiguation task. The best system, UMCC-DLSI (Guti?errez et al., 2013),
builds a graph using several resources: WordNet, WordNet Domains and the eXtended WordNet. Then,
the best sense is selected using the PageRank algorithm where the a priori probability of senses is es-
timated according to the sense frequencies. This is an extension of UBK algorithm (Agirre and Soroa,
2009), the first application of personalized PageRank to the WSD problem.
On the distributional side, Brody and Lapata (2008) use distributional similarity to automatically an-
notate a corpus for training a supervised method. Each target word in the corpus is paired with a list
of neighbours selected via distributional similarity. A neighbour is linked to a sense in WordNet and
then it is used for the annotation. Differently from our approach, distributional similarity is used to auto-
matically annotate a training corpus rather than directly disambiguate terms. Miller et al. (2012) exploit
a distributional thesaurus to expand both glosses and the context, then they apply the classical word
overlap adopted in the simplified Lesk. This approach is strongly related to our, although our approach
directly computes the overlap in the geometric space that implements the distributional semantic model.
In particular, we build a vector representation for both the gloss and the context. In recent years, other
approaches have tried to solve unsupervised WSD relying on distributional information. Gliozzo et al.
(2005) build a matrix taking into account words and WordNet domains. The matrix is reduced using LSA
and then it is combined in a kernel exploited in a supervised approach. Martinez et al. (2008) propose a
method based on topic signatures automatically constructed using the Web, while Li et al. (2010) adopt
Latent Dirichlet Allocation (LDA) to compute a conditional probability between a sense and the context.
In this model, a sense is represented by its paraphrases used to build the LDA model.
6 Conclusions and Future Work
In this paper we describe an unsupervised WSD approach which selects the best sense according to the
distributional similarity with respect to the context. In particular, the similarity is computed representing
both the gloss and the context as vectors in a geometric space generated by a distributional semantic
model based on LSA. The evaluation, conducted on the Task-12 of SemEval-2013, shows promising
results: our method is able to overcome both the most frequent sense baseline and, for English, also the
other task participants. We provide two implementations of our approach, with and without exploiting
sense frequencies. For English, both implementations outperform the SemEval-2013 participants and the
MFS. Differently, for Italian both implementations do not reach the figures of the best participant, but
they are able to defeat the MFS. As future work, we plan to extend our evaluation to other languages, and
to investigate how to adapt our approach to a specific domain. In particular, distributional models built
upon a domain corpus, and sense frequencies extracted from the same corpus, could result in a domain
adaptation of our algorithm.
Acknowledgements
This work fulfils the research objectives of the projects PON 01 00850 ASK-Health (Advanced Sys-
tem for the interpretation and sharing of knowledge in health care) and PON 02 00563 3470993 project
?VINCENTE - A Virtual collective INtelligenCe ENvironment to develop sustainable Technology En-
trepreneurship ecosystems? funded by the Italian Ministry of University and Research (MIUR).
1598
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of
the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 33?41, Athens, Greece, March.
Association for Computational Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using
WordNet. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume
2276 of Lecture Notes in Computer Science, pages 136?145. Springer Berlin Heidelberg.
Pierpaolo Basile, Marco de Gemmis, Anna Lisa Gentile, Pasquale Lops, and Giovanni Semeraro. 2007. UNIBA:
JIGSAW algorithm for Word Sense Disambiguation. In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 398?401, Prague, Czech Republic, June. Association for Compu-
tational Linguistics.
Samuel Brody and Mirella Lapata. 2008. Good Neighbors Make Good Senses: Exploiting Distributional Similar-
ity for Unsupervised WSD. In Proceedings of the 22nd International Conference on Computational Linguistics
(Coling 2008) - Volume 1, pages 65?72, Manchester, United Kingdom. The Coling 2008 Organizing Committee.
Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lexical Disambiguation Using Simulated Annealing. In
Proceedings of the 15th Conference on Computational Linguistics (Coling 1992) - Volume 1, pages 359?365,
Nantes, France. The COLING 1992 Organizing Committee.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain Kernels for Word Sense Disambiguation.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages
403?410, Stroudsburg, PA, USA. Association for Computational Linguistics.
Yoan Guti?errez, Yenier Casta?neda, Andy Gonz?alez, Rainel Estrada, Dennys D. Piug, Jose I. Abreu, Roger P?erez,
Antonio Fern?andez Orqu??n, Andr?es Montoyo, Rafael Mu?noz, and Franc Camara. 2013. UMCC DLSI: Re-
inforcing a Ranking Algorithm with Sense Frequencies and Multidimensional Semantic Resources to solve
Multilingual Word Sense Disambiguation. In Second Joint Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval
2013), pages 241?249, Atlanta, Georgia, USA, June. Association for Computational Linguistics.
Zellig Harris. 1968. Mathematical Structures of Language. New York: Interscience Publishers John Wiley &
Sons.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and Results for English SENSEVAL. Computers and
the Humanities, 34(1-2):15?48.
Thomas K Landauer and Susan T Dumais. 1997. A Solution to Plato?s Problem: The Latent Semantic Analysis
Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review, 104(2):211?240.
Michael Lesk. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings of the 5th Annual International Conference on Systems
Documentation, SIGDOC ?86, pages 24?26, New York, NY, USA. ACM.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic Models for Word Sense Disambiguation and
Token-based Idiom Detection. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1138?1147, Stroudsburg, PA, USA. Association for Computational Linguistics.
Will Lowe. 2001. Towards a Theory of Semantic Space. In Johanna T. Moore and Keith Stenning, editors,
Proceedings of the 23rd Conference of the Cognitive Science Society, pages 576?581.
Steve L. Manion and Raazesh Sainudiin. 2013. DAEBAK!: Peripheral Diversity for Multilingual Word Sense
Disambiguation. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 250?254,
Atlanta, Georgia, USA, June. Association for Computational Linguistics.
David Martinez, Oier Lopez de Lacalle, and Eneko Agirre. 2008. On the Use of Automatically Acquired Ex-
amples for All-nouns Word Sense Disambiguation. Journal of Artificial Intelligence Research, 33(1):79?107,
September.
1599
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using Distributional Similarity for
Lexical Expansion in Knowledge-based Word Sense Disambiguation. In Proceedings of the 24th International
Conference on Computational Linguistics (Coling 2012), pages 1781?1796, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Roberto Navigli and Mirella Lapata. 2010. An experimental study of graph connectivity for unsupervised word
sense disambiguation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217?250.
Roberto Navigli, David Jurgens, and Daniele Vannella. 2013. SemEval-2013 Task 12: Multilingual Word Sense
Disambiguation. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 222?231,
Atlanta, Georgia, USA, June. Association for Computational Linguistics.
Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich Word Sense Disambiguation Rivaling Super-
vised Systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,
ACL ?10, pages 1522?1531, Stroudsburg, PA, USA. Association for Computational Linguistics.
Didier Schwab, Andon Tchechmedjiev, J?er?ome Goulian, Mohammad Nasiruddin, Gilles S?erasset, and Herv?e Blan-
chon. 2013. GETALP System : Propagation of a Lesk Measure through an Ant Colony Algorithm. In Second
Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh In-
ternational Workshop on Semantic Evaluation (SemEval 2013), pages 232?240, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Florentina Vasilescu, Philippe Langlais, and Guy Lapalme. 2004. Evaluating Variants of the Lesk Approach for
Disambiguating Words. In Proceedings of the 4th Conference on Language Resources and Evaluation (LREC
?04), pages 633?636.
Tong Wang and Graeme Hirst. 2014. Applying a Naive Bayes Similarity Measure to Word Sense Disambiguation.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD,
USA, June.
1600
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 242?247,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UBA: Using Automatic Translation and Wikipedia for
Cross-Lingual Lexical Substitution
Pierpaolo Basile
Dept. of Computer Science
University of Bari ?Aldo Moro?
Via E. Orabona, 4
70125 Bari (ITALY)
basilepp@di.uniba.it
Giovanni Semeraro
Dept. of Computer Science
University of Bari ?Aldo Moro?
Via E. Orabona, 4
70125 Bari (ITALY)
semeraro@di.uniba.it
Abstract
This paper presents the participation of the
University of Bari (UBA) at the SemEval-
2010 Cross-Lingual Lexical Substitution
Task. The goal of the task is to substi-
tute a word in a language L
s
, which oc-
curs in a particular context, by provid-
ing the best synonyms in a different lan-
guage L
t
which fit in that context. This
task has a strict relation with the task of
automatic machine translation, but there
are some differences: Cross-lingual lexi-
cal substitution targets one word at a time
and the main goal is to find as many good
translations as possible for the given tar-
get word. Moreover, there are some con-
nections with Word Sense Disambiguation
(WSD) algorithms. Indeed, understand-
ing the meaning of the target word is nec-
essary to find the best substitutions. An
important aspect of this kind of task is
the possibility of finding synonyms with-
out using a particular sense inventory or a
specific parallel corpus, thus allowing the
participation of unsupervised approaches.
UBA proposes two systems: the former is
based on an automatic translation system
which exploits Google Translator, the lat-
ter is based on a parallel corpus approach
which relies on Wikipedia in order to find
the best substitutions.
1 Introduction
The goal of the Cross-Lingual Lexical Substitu-
tion (CLLS) task is to substitute a word in a lan-
guage L
s
, which occurs in a particular context,
by providing the best substitutions in a different
language L
t
. In SemEval-2010 the source lan-
guage L
s
is English, while the target language L
t
is Spanish. Clearly, this task is related to Lexical
Substitution (LS) (McCarthy and Navigli, 2007)
which consists in selecting an alternative word for
a given one in a particular context by preserving
its meaning. The main difference between the LS
task and the CLLS one is that in LS source and
target languages are the same. CLLS is not a easy
task since neither a list of candidate words nor a
specific parallel corpus are supplied by the orga-
nizers. However, this opens the possibility of us-
ing several knowledge sources, instead of a single
one fixed by the task organizers. Therefore, the
system must identify a set of candidate words in
L
t
and then select only those words which fit the
context. From another point of view, the cross-
lingual nature of the task allows to exploit auto-
matic machine translation methods, hence the goal
is to find as many good translations as possible for
the given target word. A thorough description of
the task can be found in (Mihalcea et al, 2010;
Sinha et al, 2009).
To easily understand the task, an example fol-
lows. Consider the sentence:
During the siege, George Robertson had
appointed Shuja-ul-Mulk , who was a bright
boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
In the previous sentence the target word is
?bright?. Taking into account the meaning of
the word ?bright? in this particular context, the
best substitutions in Spanish are: ?inteligente?,
?brillante? and ?listo?.
We propose two systems to tackle the problem
of CLLS: the first is based on an automatic trans-
lation system which exploits the API of Google
Translator
1
, the second is based on a parallel cor-
pus approach which relies on Wikipedia. In par-
ticular, in the second approach we use a struc-
tured version of Wikipedia called DBpedia (Bizer
1
http://code.google.com/p/google-api-translate-java/
242
et al, 2009). Both systems adopt several lexical
resources to select the list of possible substitutions
for a given word. Specifically, we use three differ-
ent dictionaries: Google Dictionary, Babylon Dic-
tionary and Spanishdict. Then, we combine the
dictionaries into a single one, as described in Sec-
tion 2.1.
The paper is organized as follows: Section 2 de-
scribes the strategy we adopted to tackle the CLLS
task, while results of an experimental session we
carried out in order to evaluate the proposed ap-
proaches are presented in Section 3. Conclusions
are discussed in Section 4.
2 Methodology
Generally speaking, the problem of CLLS can be
coped with a strategy which consists of two steps,
as suggested in (Sinha et al, 2009):
? candidate collection: in this step several re-
sources are queried to retrieve a list of po-
tential translation candidates for each target
word and part of speech;
? candidate selection: this step concerns the
ranking of potential candidates, which are the
most suitable ones for each instance, by using
information about the context.
Regarding the candidate collection, we exploit
three dictionaries: Google Dictionary, Babylon
Dictionary and Spanishdict. Each dictionary is
modeled using a strategy described in Section 2.1.
We use the same approach to model each dictio-
nary in order to make easy both the inclusion of fu-
ture dictionaries and the integration with the can-
didate selection step.
Candidate selection is performed in two dif-
ferent ways. The first one relies on the auto-
matic translation of the sentence in which the tar-
get word occurs, in order to find the best substitu-
tions. The second method uses a parallel corpus
built on DBpedia to discover the number of doc-
uments in which the target word is translated by
one of the potential translation candidates. Details
about both methods are reported in Section 2.2
2.1 Candidate collection
This section describes the method adopted to re-
trieve the list of potential translation candidates for
each target word and part of speech.
Our strategy combines several bi-lingual dictio-
naries and builds a single list of candidates for
each target word. The involved dictionaries meet
the following requirements:
1. the source language L
s
must be English and
the target one L
t
must be Spanish;
2. each dictionary must provide information
about the part of speech;
3. the dictionary must be freely available.
Moreover, each candidate has a score s
ij
com-
puted by taking into account its rank in the list
of possible translations supplied by the i ? th
dictionary. Formally, let us denote by D =
{d
1
, d
2
, . . . , d
n
} the set of n dictionaries and by
L
i
= {c
1
, c
2
, . . . , c
m
i
} the list of potential candi-
dates provided by d
i
. The score s
ij
is computed
by the following equation:
s
ij
= 1?
j
m
i
j ? {1, 2, . . . ,m
i
} (1)
Since each list L
i
has a different size, we adopt
a score normalization strategy based on Z-score
to merge the lists in a unique one. Z-score nor-
malizes the scores according to the average ? and
standard deviation ?. Given the list of scores
L = {s
1
, s
2
, . . . , s
n
}, ? and ? are computed on
L and the normalized score is defined as:
s
i
=
s
i
? ?
?
(2)
Then, all the lists L
i
are merged in a single list
M . The list M contains all the potential candi-
dates belonging to all the dictionaries with the re-
lated score. If a candidate occurs in more than one
dictionary, only the occurrence with the maximum
score is chosen.
At the end of the candidate collection step the
list M of potential translation candidates for each
target word is computed. It is important to point
out that the list M is sorted and supplies an initial
rank, which can be then modified by the candidate
selection step.
2.2 Candidate selection
While the candidate collection step is common to
the two proposed systems, the problem of candi-
date selection is faced by using different strategies
in the two systems.
243
The first system, called unibaTranslate,
uses a method based on google-api-translate-java
2
.
The main idea behind unibaTranslate is to
look for a potential candidate in the translation of
the target sentence. Sometimes, no potential can-
didates occur into the translation. When this hap-
pens the system uses some heuristics to discover a
possible translation.
For example, given the target word ?raw? and
the potential candidates M ={puro, crudo, sin re-
finar, de baja calidad, agrietado, al natural, bozal,
asado, frito and bruto}, the two possible scenarios
are:
1. a potential candidate occurs into the transla-
tion:
? S
en
: The raw honesty of that basic
crudeness makes you feel stronger in a
way.
? S
es
: La cruda honestidad de esa
crudeza de base que te hace sentir mas
fuerte en un camino.
2. no potential candidates occur into the trans-
lation, but a correct translation of the target
word is provided:
? S
en
: Many institutional investors are
now deciding that they are getting a raw
deal from the company boards of Aus-
tralia.
? S
es
: Muchos inversores institucionales
estan ahora decidiendo que estan recibi-
endo un trato injusto de los directorios
de las empresas de Australia.
In detail, the strategy can be split in several
steps:
1. Retrieve the list M of potential translation
candidates using the method described in
Section 2.1.
2. Translate the target sentence S
en
from En-
glish to Spanish, using the google-api-
translate-java, which results into the sentence
S
es
.
3. Enrich M by adding multiword expressions.
To implement this step, the two bigrams
which contain the target word and the only
trigram in which the target word is the 2
nd
term are taken into to account.
2
http://code.google.com/p/google-api-translate-java/
Coming back to the first sentence in the previ-
ous example, the following n-grams are built:
?the raw?, ?raw honesty? and ?the raw hon-
esty?. For each n-gram, candidate transla-
tions are looked for using Google Dictionary.
If translations are found, they are added toM
with an initial score equal to 0.
4. Fix a window W
3
of n words to the right and
to the left of the target word, and perform the
following steps:
(a) for each candidate c
k
in M , try to find
c
k
in W . If c
k
occurs in W , then add 2
to the score of c
k
in M ;
(b) if no exact match is found in the previ-
ous step, perform a new search by com-
paring c
k
with the words in W using
the Levenshtein distance
4
(Levenshtein,
1966). If the Levenshtein distance is
greater than 0.8, then add 2 to the score
of c
k
in M .
5. If no exact/partial match is found in the pre-
vious steps, probably the target word is trans-
lated with a word which does not belong to
M . To overcome this problem, we implement
a strategy able to discover a possible transla-
tion in S
es
which is not in M . This approach
involves three steps:
(a) for each word w
i
in S
en
, a list of poten-
tial translations P
i
is retrieved;
(b) if a word in P
i
is found in S
es
, the word
is removed from S
es
5
;
(c) at this point, S
es
contains a list R of
words with no candidate translations. A
score is assigned to those words by tak-
ing into account their position in S
es
with respect to the position of the target
word in S
en
, using the following equa-
tion:
1?
|pos
c
? pos
t
|
L
max
(3)
where pos
c
is the translation candidate
position in S
es
, pos
t
is the target word
position in S
en
and L
max
is the maxi-
mum length between the length of S
en
and S
es
.
3
The window W is the same for both S
en
and S
es
.
4
A normalized Levenshtein distance is adopted to obtain
a value in [0, 1].
5
A partial match based on normalized Levenshtein dis-
tance is implemented.
244
Moreover, the words not semanti-
cally related to the potential candidates
(found using Spanish WordNet
6
) are re-
moved fromR. In detail, for each candi-
date in M a list of semantically related
words in Spanish WordNet
7
is retrieved
which results in a set WN of related
words. Words in R but not in WN are
removed from R. In the final step, the
list R is sorted and the first word in R is
added toM assigning a score equal to 2.
6. In the last step, the list M is sorted. The out-
put of this process is the ranked list of poten-
tial candidates.
It is important to underline that both S
en
and
S
es
are tokenized, part-of-speech tagged and lem-
matized. Lemmatization plays a key role in the
matching step, while part-of-speech tagging is
needed to query both the dictionaries and the
Spanish WordNet. We adopt META (Basile et al,
2008) and FreeLing (Atserias et al, 2006) to per-
form text processing for English and Spanish re-
spectively.
The second proposed system, called
unibaWiki, is based on the idea of automati-
cally building a parallel corpus from Wikipedia.
We use a structured version of Wikipedia called
DBpedia (Bizer et al, 2009). The main idea be-
hind DBpedia is to extract structured information
from Wikipedia and then to make this information
available. The main goal is to have access easily
to the large amount of information in Wikipedia.
DBpedia opens new and interesting ways to use
Wikipedia in NLP applications.
In CLLS task, we use the extended abstracts of
English and Spanish provided by DBpedia. For
each extended abstract in Spanish which has the
corresponding extended abstract in English, we
build a document composed by two fields: the for-
mer contains the English text (text
en
) and the lat-
ter contains the Spanish text (text
es
). We adopt
Lucene
8
as storage and retrieval engine to make
the documents access fast and easy.
The idea behind unibaWiki is to count, for
each potential candidate, the number of docu-
ments in which the target word occurs in text
en
and the potential candidate occurs in text
es
. A
6
http://www.lsi.upc.edu/?nlp/projectes/ewn.html
7
The semantic relations of hyperonymy, hyponymy and
?similar to? are exploited.
8
http://lucene.apache.org/
score equal to the number of retrieved documents
is assigned, then the candidates are sorted accord-
ing to that score.
Given the list M of potential candidates and the
target word t, for each c
k
? M we perform the
following query:
text
en
: t AND text
es
: c
k
where the field name is followed by a colon and
by the term you are looking for.
It is important to underline here that multiword
expressions require a specific kind of query. For
each multiword expression we adopt the Phrase-
Query which is able to retrieve documents that
contain a specific sequence of words instead of a
single keyword.
2.3 Implementation
To implement the candidate collection step we de-
veloped a Java application able to retrieve infor-
mation from dictionaries. For each dictionary, a
different strategy has been adopted. In particular:
1. Google Dictionary: Google Dictionary web-
site is queried by using the HTTP protocol
and the answer page is parsed;
2. Spanishdict: the same strategy adopted for
Google Dictionary is used for the Spanishdict
website
9
;
3. Babylon Dictionary: the original file avail-
able from the Babylon website
10
is converted
to obtain a plain text file by using the Unix
utility dictconv. After that, an application
queries the text file in an efficient way by
means of a hash map.
Both candidate selection systems are developed
in Java. Regarding the unibaWiki system, we
adopt Lucene to index DBpedia abstracts. The
output of Lucene is an index of about 680 Mbytes,
277,685 documents and about 1,500,000 terms.
3 Evaluation
The goal of the evaluation is to measure the sys-
tems? ability to find correct Spanish substitutions
for a given word. The dataset supplied by the or-
ganizers contains 1,000 instances in XML format.
9
http://www.spanishdict.com/
10
www.babylon.com
245
Moreover, the organizers provide trial data com-
posed by 300 instances to help the participants
during the development of their systems.
The systems are evaluated using two scoring
types: best scores the best guessed substitution,
while out-of-ten (oot) scores the best 10 guessed
substitutions. For each scoring type, precision (P)
and recall (R) are computed. Mode precision (P-
mode) and mode recall (R-mode) calculate preci-
sion and recall against the substitution chosen by
the majority of the annotators (if there is a ma-
jority), respectively. Details about evaluation and
scoring types are provided in the task guidelines
(McCarthy et al, 2009).
Results of the evaluation using trial data are
reported in Table 1 and Table 2. Our systems
are tagged as UBA-T and UBA-W, which de-
note unibaTranslate and unibaWiki, re-
spectively. Systems marked as BL-1 and BL-2
are the two baselines provided by the organiz-
ers. The baselines use Spanishdict dictionary to
retrieve candidates. The system BL-1 ranks the
candidates according to the order returned on the
online query page, while the BL-2 rank is based on
candidate frequencies in the Spanish Wikipedia.
Table 1: best results (trial data)
System P R P-mode R-Mode
BL-1 24.50 24.50 51.80 51.80
BL-2 14.10 14.10 28.38 28.38
UBA-T 26.39 26.39 59.01 59.01
UBA-W 22.18 22.18 48.65 48.65
Table 2: oot results (trial data)
System P R P-mode R-Mode
BL-1 38.58 38.58 71.62 71.62
BL-2 37.83 37.83 68.02 68.02
UBA-T 44.16 44.16 78.38 78.38
UBA-W 45.15 45.15 72.52 72.52
Results obtained using trial data show that our
systems are able to overcome the baselines. Only
the best score achieved by UBA-W is below BL-1.
Moreover, our strategy based on Wikipedia (UBA-
W) works better than the one proposed by the or-
ganizers (BL-2).
Results of the evaluation using test data are re-
ported in Table 3 and Table 4, which include all the
participants. Results show that UBA-T obtains the
highest recall using best scoring strategy. More-
over, both systems UBA-T and UBA-W achieve
the highest R-mode and P-mode using oot scoring
strategy. It is worthwhile to point out that the pres-
ence of duplicates affect recall (R) and precision
(P), but not R-mode and P-mode. For this reason
some systems, such as SWAT-E, obtain very high
recall (R) and low R-mode using oot scoring. Du-
plicates are not produced by our systems, but we
performed an a posteriori experiment in which du-
plicates are allowed. In that experiment, the first
candidate provided by UBA-T has been duplicated
ten times in the results. Using that strategy, UBA-T
achieves a recall (and precision) equal to 271.51.
This experiment proves that also our system is able
to obtain the highest recall when duplicates are al-
lowed into the results. Moreover, it is important to
underline here that we do not know how other par-
ticipants generate duplicates in their results. We
adopted a trivial strategy to introduce duplicates.
Table 3: best results (test data)
System P R P-mode R-Mode
BL-1 24.34 24.34 50.34 50.34
BL-2 15.09 15.09 29.22 29.22
UBA-T 27.15 27.15 57.20 57.20
UBA-W 19.68 19.68 39.09 39.09
USPWLV 26.81 26.81 58.85 58.85
Colslm 27.59 25.99 59.16 56.24
WLVUSP 25.27 25.27 52.81 52.81
SWAT-E 21.46 21.46 43.21 43.21
UvT-v 21.09 21.09 43.76 43.76
CU-SMT 21.62 20.56 45.01 44.58
UvT-g 19.59 19.59 41.02 41.02
SWAT-S 18.87 18.87 36.63 36.63
ColEur 19.47 18.15 40.03 37.72
IRST-1 22.16 15.38 45.95 33.47
IRSTbs 22.51 13.21 45.27 28.26
TYO 8.62 8.39 15.31 14.95
Table 4: oot results (test data)
System P R P-mode R-Mode
BL-1 44.04 44.04 73.53 73.53
BL-2 42.65 42.65 71.60 71.60
UBA-T 47.99 47.99 81.07 81.07
UBA-W 52.75 52.75 83.54 83.54
USPWLV 47.60 47.60 79.84 79.84
Colslm 46.61 43.91 69.41 65.98
WLVUSP 48.48 48.48 77.91 77.91
SWAT-E 174.59 174.59 66.94 66.94
UvT-v 58.91 58.91 62.96 62.96
UvT-g 55.29 55.29 73.94 73.94
SWAT-S 97.98 97.98 79.01 79.01
ColEur 44.77 41.72 71.47 67.35
IRST-1 33.14 31.48 58.30 55.42
IRSTbs 29.74 8.33 64.44 19.89
TYO 35.46 34.54 59.16 58.02
FCC-LS 23.90 23.90 31.96 31.96
Finally, Table 5 reports some statistics about
UBA-T and the number of times (N) the candi-
246
date translation is taken from Spanish WordNet
(Spanish WN) or multiword expressions (Multi-
word exp.). The number of instances in which
the candidate is a correct substitution is reported
in column C. Analyzing the results we note that
most errors are due to part-of-speech tagging. For
example, given the following sentence:
S
en
: You will still be responsible for the shipping
and handling fees, and for the cost of return-
ing the merchandise.
S
es
: Usted seguira siendo responsable de los gas-
tos de envio y manipulacion y, para los gastos
de devolucion de la mercancia.
where the target word is the verb return. In this
case the verb is used as noun and the algorithm
suggests correctly devolucion (noun) as substitu-
tion instead of devolver (verb). The gold standard
provided by the organizers contains devolver as
substitution and there is no match between devolu-
cion and devolver during the scoring.
Table 5: UBA-T statistics.
Strategy N C
Spanish WN 34 11
Multiword exp. 21 11
4 Conclusions
We described our participation at SemEval-2
Cross-Lingual Lexical Substitution Task, propos-
ing two systems called UBA-T and UBA-W. The
first relies on Google Translator, the second
is based on DBpedia, a structured version of
Wikipedia. Moreover, we exploited several dictio-
naries to retrieve the list of candidate substitutions.
UBA-T achieves the highest recall among all
the participants to the task. Moreover, the results
proved that the method based on Google Transla-
tor is more effective than the one based on DBpe-
dia.
Acknowledgments
This research was partially funded by Regione
Puglia under the contract POR PUGLIA 2007-
2013 - Asse I Linea 1.1 Azione 1.1.2 - Bando
?Aiuti agli Investimenti in Ricerca per le PMI? -
Fondo per le Agevolazioni alla Ricerca, project ti-
tle: ?Natural Browsing?.
References
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonz?alez, Llu??s Padr?o, and Muntsa Padr?o.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC06), pages 48?55.
Pierpaolo Basile, Marco de Gemmis, Anna Lisa Gen-
tile, Leo Iaquinta, Pasquale Lops, and Giovanni Se-
meraro. 2008. META - MultilanguagE Text Ana-
lyzer. In Proceedings of the Language and Speech
Technnology Conference - LangTech 2008, Rome,
Italy, February 28-29, pages 137?140.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia-A crys-
tallization point for the Web of Data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, Issue 7:154?165.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. In Soviet Physics-Doklady, volume 10.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Sub-
stitution Task. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 48?53, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Diana McCarthy, Rada Sinha, and Ravi Mihal-
cea. 2009. Cross Lingual Lexical Sub-
stitution. http://lit.csci.unt.edu/DOCS/task2clls-
documentation.pdf.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. SemEval-2010 Task 2: Cross-Lingual
Lexical Substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010). Association for Computational
Linguistics.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea.
2009. SemEval-2010 Task 2: cross-lingual lexical
substitution. In SEW ?09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 76?81, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
247
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591?596,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNIBA: Distributional Semantics for Textual Similarity
Annalina Caputo Pierpaolo Basile
Department of Computer Science
University of Bari ?Aldo Moro?
Via E. Orabona, 4 - 70125 Bari, Italy
{acaputo, basilepp, semeraro}@di.uniba.it
Giovanni Semeraro
Abstract
We report the results of UNIBA participation
in the first SemEval-2012 Semantic Textual
Similarity task. Our systems rely on distribu-
tional models of words automatically inferred
from a large corpus. We exploit three differ-
ent semantic word spaces: Random Indexing
(RI), Latent Semantic Analysis (LSA) over RI,
and vector permutations in RI. Runs based on
these spaces consistently outperform the base-
line on the proposed datasets.
1 Background and Related Research
SemEval-2012 Semantic Textual Similarity (STS)
task (Agirre et al, 2012) aims at providing a gen-
eral framework to ?examine the degree of semantic
equivalence between two sentences.?
We propose an approach to Semantic Textual
Similarity based on distributional models of words,
where the geometrical metaphor of meaning is ex-
ploited. Distributional models are grounded on the
distributional hypothesis (Harris, 1968), according
to which the meaning of a word is determined by
the set of textual contexts in which it appears. These
models represent words as vectors in a high dimen-
sional vector space. Word vectors are built from a
large corpus in such a way that vector dimensions
reflect the different uses (or contexts) of a word in
the corpus. Hence, the meaning of a word is de-
fined by its use, and words used in similar contexts
are represented by vectors near in the space. In this
way, semantically related words like ?basketball?
and ?volleyball?, which occur frequently in similar
contexts, say with words ?court, play, player?, will
be represented by near points. Different definitions
of contexts give rise to different (semantic) spaces.
A context can be a document, a sentence or a fixed
window of surrounding words. Contexts and words
can be stored through a co-occurrence matrix, whose
columns correspond to contexts, and rows to words.
Therefore, the strength of the semantic association
between words can be computed as the cosine simi-
larity of their vector representations.
Latent Semantic Analysis (Deerwester et al,
1990), BEAGLE (Jones and Mewhort, 2007),
Random Indexing (Kanerva, 1988), Hyperspace
Analogue to Language (Burgess et al, 1998),
WordSpace (Sch?tze and Pedersen, 1995) are all
techniques conceived to build up semantic spaces.
However, all of them intend to represent semantics at
a word scale. Although vectors addition and multi-
plication are two well defined operations suitable for
composing words in semantic spaces, they miss tak-
ing into account the underlying syntax, which regu-
lates the compositionality of words. Some efforts to-
ward this direction are emerging (Clark and Pulman,
2007; Clark et al, 2008; Mitchell and Lapata, 2010;
Coecke et al, 2010; Basile et al, 2011; Clarke,
2012), which resulted in theoretical work corrob-
orated by empirical evaluation on how small frag-
ments of text compose (e.g. noun-noun, adjective-
noun, and verb-noun pairs).
2 Methodology
Our approach to STS is inspired by the latest devel-
opments about semantic compositionality and distri-
butional models. The general methodology is based
on the construction of a semantic space endowed
591
with a vector addition operator. The vector addition
sums the word vectors of each pair of sentences in-
volved in the evaluation. The result consists of two
vectors whose similarity can be computed by co-
sine similarity. However, this simple methodology
translates a text into a mere bag-of-word representa-
tion, depriving the text of its syntactic construction,
which also influences the overall meaning of the sen-
tence. In order to deal with this limit, we experi-
ment two classical methods for building a semantic
space, namely Random Indexing and Latent Seman-
tic Analysis, along with a new method based on vec-
tor permutations, which tries to encompass syntactic
information directly into the resulting space.
2.1 Random Indexing
Our first method is based on Random Indexing (RI),
introduced by Kanerva (Kanerva, 1988). This tech-
nique allows us to build a semantic space with no
need for (either term-document or term-term) ma-
trix factorization, because vectors are inferred by
using an incremental strategy. Moreover, it allows
us to solve efficiently the problem of reducing di-
mensions, which is one of the key features used to
uncover the ?latent semantic dimensions? of a word
distribution.
RI1 (Widdows and Ferraro, 2008) is based on
the concept of Random Projection according to
which high dimensional vectors chosen randomly
are ?nearly orthogonal?.
Formally, given an n ? m matrix A and an m ?
k matrix R made up of k m-dimensional random
vectors, we define a new n? k matrix B as follows:
Bn,k = An,m?Rm,k k << m (1)
The new matrix B has the property to preserve the
distance between points scaled by a multiplicative
factor (Johnson and Lindenstrauss, 1984).
Specifically, RI creates the semantic space Bn,k
in two steps (we consider a fixed window w of terms
as context):
1. A context vector is assigned to each term. This
vector is sparse, high-dimensional and ternary,
which means that its elements can take values
1An implementation of RI can be found at:
http://code.google.com/p/semanticvectors/
in {-1, 0, 1}. A context vector contains a small
number of randomly distributed non-zero ele-
ments, and the structure of this vector follows
the hypothesis behind the concept of Random
Projection;
2. Context vectors are accumulated by analyzing
co-occurring terms in a window w. The seman-
tic vector for a term is computed as the sum of
the context vectors for terms which co-occur in
w.
2.2 Latent Semantic Analysis
Latent Semantic Analysis (Deerwester et al, 1990)
relies on the Singular Value Decomposition (SVD)
of a term-document co-occurrence matrix. Given
a matrix M, it can be decomposed in the product
of three matrices U?V>, where U and V are the
orthonormal matrices and ? is the diagonal matrix
of singular values of M placed in decreasing order.
Computing the LSA on the co-occurrence matrix M
can be a computationally expensive task, as a corpus
can contain thousands of terms. Hence, we decided
to apply LSA to the reduced approximation gener-
ated by RI. It is important to point out that no trun-
cation of singular values is performed. Since com-
puting the similarity between any two words is equal
to taking the corresponding entry in the MM> ma-
trix, we can exploit the relation
MM> = U?V>V?>U> = U??>U> =
(U?)(U?)>
Hence, the application of LSA to RI makes possible
to represent each word in the U? space.
A similar approach was investigated by Sellberg
and J?nsson (2008) for retrieval of similar FAQs in
a Question Answering system. Authors showed that
halving the matrix dimension by applying the RI re-
sulted in a drastic reduction of LSA computation
time. Certainly there was also a performance price
to be paid, however general performance was bet-
ter than VSM and RI respectively. We also experi-
mented LSA computed on RI versus LSA applied to
the original matrix during the tuning of our systems.
Surprisingly, we found that LSA applied on the re-
duced matrix gives better results than LSA. How-
ever, these results are not reported as they are not
the focus of this evaluation.
592
2.3 Vector Permutations in RI
The classical distributional models can handle only
one definition of context at a time, such as the whole
document or the window w. A method to add infor-
mation about context in RI is proposed in (Sahlgren
et al, 2008). The authors describe a strategy to en-
code word order in RI by the permutation of coor-
dinates in context vector. When the coordinates are
shuffled using a random permutation, the resulting
vector is nearly orthogonal to the original one. That
operation corresponds to the generation of a new
random vector. Moreover, by applying a predeter-
mined mechanism to obtain random permutations,
such as elements rotation, it is always possible to
reconstruct the original vector using the reverse per-
mutations. By exploiting this strategy it is possible
to obtain different random vectors for each context
in which the term occurs.
Our idea is to encode syntactic dependen-
cies using vector permutations. A syntactic
dependency between two words is defined as
dep(head, dependent), where dep is the syntac-
tic link which connects the dependent word to the
head word. Generally speaking, dependent is the
modifier, object or complement, while head plays a
key role in determining the behavior of the link. For
example, subj(eat, cat) means that ?cat? is the sub-
ject of ?eat?. In that case the head word is ?eat?,
which plays the role of verb.
The key idea is to encode in the semantic space in-
formation about syntactic dependencies which link
words together. Rather than representing the kind
of dependency, our focus is to encompass informa-
tion about the existence of such a relation between
words in the construction of the space. The method
adopted to construct a semantic space that takes into
account both syntactic dependencies and Random
Indexing can be defined as follows:
1. a context vector is assigned to each term, as de-
scribed in Section 2.1 (Random Indexing);
2. context vectors are accumulated by analyzing
terms which are linked by a dependency. In
particular the semantic vector for each term ti
is computed as the sum of the inverse-permuted
context vectors for the terms tj which are de-
pendents of ti, and the permuted vectors for
the terms tj which are heads of ti. Moreover,
the context vector of ti, and those of tj terms
which appears in a dependency relation with
it, are sum to the final semantic vector in or-
der to provide distributional evidence of co-
occurrence. Each permutation is computed as
a forward/backward rotation of one element. If
?1 is a permutation of one element, the inverse-
permutation is defined as ??1: the elements
rotation is performed by one left-shifting step.
Formally, denoting with x the context vector
for a term, we compute the semantic vector for
the term ti as follows:
si = xi +
?
j
?dep(ti,tj)
(
??1xj + xj
)
+
?
k
?dep(tk,ti)
(
?1xk + xk
)
Adding permuted vectors to the head word and
inverse-permuted vectors to the corresponding de-
pendent word allows to encode the information
about both heads and dependents into the space.
This approach is similar to the one investigated by
(Cohen et al, 2010) to encode relations between
medical terms.
3 Evaluation
Dataset Description. SemEval-2012 STS is a first
attempt to provide a ?unified framework for the eval-
uation of modular semantic components.? The task
consists in computing the similarity between pair
of texts, returning a similarity score. Sentences
are extracted from five publicly available datasets:
MSR (Paraphrase Microsoft Research Paraphrase
Corpus, 750 pairs), MSR (Video Microsoft Research
Video Description Corpus, 750 pairs), SMTeuroparl
(WMT2008 development dataset, Europarl section,
459 pairs), SMTnews (news conversation sentence
pairs from WMT, 399 pairs), and OnWN (pairs of
sentences from Ontonotes and WordNet definition,
750 pairs). Humans rated each pair with values from
0 to 5. The evaluation is performed by comparing
humans scores against systems performance through
Pearson?s correlation. The organizers propose three
different ways to aggregate values from the datasets:
593
ALL Rank-ALL ALLnrm Rank-ALLNrm Mean Rank-Mean
baseline .3110 87 .6732 85 .4356 70
UNIBA-RI .6285 41 .7951 43 .5651 45
UNIBA-LSARI .6221 44 .8079 30 .5728 40
UNIBA-DEPRI .6141 46 .8027 38 .5891 31
Table 1: Evaluation results of Pearson?s correlation.
MSRpar MSRvid SMT-eur On-WN SMT-news
baseline .4334 .2996 .4542 .5864 .3908
UNIBA- RI .4128 .7612 .4531 .6306 .4887
UNIBA- LSARI .3886 .7908 .4679 .6826 .4238
UNIBA- DEPRI .4542 .7673 .5126 .6593 .4636
Table 2: Evaluation results of Pearson?s correlation for individual datasets.
ALL Pearson correlation with the gold standard for
the five datasets.
ALLnrm Pearson correlation after the system out-
puts for each dataset are fitted to the gold stan-
dard using least squares.
Mean Weighted mean across the five datasets,
where the weight depends on the number of
pairs in the dataset.
Experimental Setting. For the evaluation, we
built Distributional Spaces using the WaCkype-
dia_EN corpus2. WaCkypedia_EN is based on a
2009 dump of the English Wikipedia (about 800 mil-
lion tokens) and includes information about: part-of-
speech, lemma and a full dependency parsing per-
formed by MaltParser (Nivre et al, 2007). The three
spaces described in Section 2 are built exploiting
information about term windows and dependency
parsing supplied by WaCkypedia. The total number
of dependencies amounts to about 200 million.
The RI system is implemented in Java and re-
lies on some portions of code publicly available in
the Semantic Vectors package (Widdows and Fer-
raro, 2008), while for LSA we exploited the publicly
available C library SVDLIBC3.
We restricted the vocabulary to the 50,000 most
frequent terms, with stop words removal and forc-
ing the system to include terms which occur in the
dataset. Hence, the dimension of the original matrix
would have been 50,000?50,000.
2http://wacky.sslmit.unibo.it/doku.php?id=corpora
3http://tedlab.mit.edu/ dr/SVDLIBC/
Our approach involves some parameters. In par-
ticular, each semantic space needs to set up the di-
mension k of the space. All spaces use a dimen-
sion of 500 (resulting in a 50,000?500 matrix). The
number of non-zero elements in the random vector
is set to 10. When we apply LSA to the output space
generated by the Random Indexing we hold all the
500 dimensions since during the tuning we observed
a drop in performance when a lower dimension was
set. The co-occurrence distance w between terms
was set up to 4.
In order to compute the similarity between the
vector representations of sentences we used the co-
sine similarity, and then we multiplied by 5 the ob-
tained value.
Results. Table 1 shows the overall results obtained
exploiting the different semantic spaces. We re-
port the three proposed evaluation measures with the
corresponding overall ranks with respect to the 89
runs submitted by participants. We submitted three
different runs, each exploring a different semantic
space: UNIBA-RI (based on Random Indexing),
UNIBA-LSARI (based on LSA performed over RI
outcome), and UNIBA-DEPRI (based on Random
Indexing and vector permutations). Each proposed
measure stresses different aspects. ALL is the Pear-
son?s correlation computed over the concatenated
dataset. As a consequence this measure ranks higher
systems which obtain consistent better results. Con-
versely, ALLNrm normalizes results by scaling val-
ues obtained from each dataset, in this way it tries
to give emphasis to systems trained on each dataset.
594
The result of these different perspective is that our
three spaces rank differently according to each mea-
sure. It seems that UNIBA-RI is able to work better
across all datasets, while UNIBA-LSARI gives the
best results on specific datasets, even though all our
methods are unsupervised and do not need training
steps. A deeper analysis on each dataset is reported
on Table 2. Here results seem to be at odds with
Table 1.
Considering individual datasets, UNIBA-RI gives
only once the best result, while UNIBA-LSARI and
UNIBA-DEPRI are able to provide the best results
twice. Generally, all results outperform the base-
line, based on a simple keyword overlap. Lower re-
sults are obtained in MSRpar, we ascribe this result
to the notably long sentences here involved. In par-
ticular, UNIBA-LSARI gives a result lower than the
baseline, and in line with the one obtained by LSA
during the tuning. Hence, we ascribe this low per-
formance to the application of LSA method to this
specific dataset. Only UNIBA-DEPRI was able to
outperform the baseline in this dataset. This shows
the usefulness of encoding syntactic features in se-
mantic word space where longer sentences are in-
volved. Generally, it is interesting to be noticed that
our spaces perform rather well on short and similarly
structured sentences, such as MSRvid and On-WN.
4 Conclusion
We reported evaluation results of our participation in
Semantic Textual Similarity task. Our systems ex-
ploit distributional models to represent the seman-
tics of words. Two of such spaces are based on a
classical definition of context, such as a fixed win-
dow of surrounding words. A third spaces tries to
encompass more definitions of context at once, as
the syntactic structure that relates words in a cor-
pus. Although simple, our methods have achieved
generally good results, outperforming the baseline
provided by the organizers.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-
aro. 2011. Encoding syntactic dependencies by vec-
tor permutation. In Proceedings of the EMNLP 2011
Workshop on GEometrical Models of Natural Lan-
guage Semantics, GEMS ?11, pages 43?51, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Ex-
plorations in context space: Words, sentences, dis-
course. Discourse Processes, 25(2-3):211?257.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of the AAAI Spring Symposium on Quan-
tum Interaction, pages 52?55.
Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.
2008. A compositional distributional model of mean-
ing. In Proceedings of the Second Quantum Interac-
tion Symposium (QI-2008), pages 133?140.
Daoud Clarke. 2012. A context?theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41?71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a composi-
tional distributional model of meaning. CoRR,
abs/1003.4394.
Trevor Cohen, Dominic Widdows, Roger W. Schvan-
eveldt, and Thomas C. Rindflesch. 2010. Logical
leaps and quantum connectives: Forging paths through
predication space. In AAAI-Fall 2010 Symposium on
Quantum Informatics for Cognitive, Social, and Se-
mantic Processes, pages 11?13.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. New York: Interscience.
William B. Johnson and Joram Lindenstrauss. 1984. Ex-
tensions of Lipschitz mappings into a Hilbert space.
Conference on Modern Analysis and Probability, Con-
temporary Mathematics, 26:189?206.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114(1):1?37.
Pentti Kanerva. 1988. Sparse Distributed Memory. MIT
Press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
595
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?lsen Eryigit, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95?135.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In V. Sloutsky, B. Love, and K. Mcrae,
editors, Proceedings of the 30th Annual Meeting of
the Cognitive Science Society (CogSci?08), July 23-26,
Washington D.C., USA, pages 1300?1305. Cognitive
Science Society, Austin, TX.
Hinrich Sch?tze and Jan O. Pedersen. 1995. Informa-
tion retrieval based on word senses. In Proceedings of
the 4th Annual Symposium on Document Analysis and
Information Retrieval, pages 161?175.
Linus Sellberg and Arne J?nsson. 2008. Using random
indexing to improve singular value decomposition
for latent semantic analysis. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan
Odjik, Stelios Piperidis, and Daniel Tapias, editors,
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC2008),
pages 2335?2338, Marrakech, Morocco. European
Language Resources Association (ELRA).
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic Vectors: A Scalable Open Source Package
and Online Technology Management Application. In
Nicoletta Calzolari, Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odjik, Stelios Piperidis, and
Daniel Tapias, editors, Proceedings of the 6th Interna-
tional Conference on Language Resources and Eval-
uation (LREC2008), pages 1183?1190, Marrakech,
Morocco. European Language Resources Association
(ELRA).
596
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 169?175, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UNIBA-CORE: Combining Strategies for Semantic Textual Similarity
Annalina Caputo Pierpaolo Basile
Department of Computer Science
University of Bari Aldo Moro
Via E. Orabona, 4 - 70125 Bari, Italy
{annalina.caputo, pierpaolo.basile, giovanni.semeraro}@uniba.it
Giovanni Semeraro
Abstract
This paper describes the UNIBA participation
in the Semantic Textual Similarity (STS) core
task 2013. We exploited three different sys-
tems for computing the similarity between two
texts. A system is used as baseline, which rep-
resents the best model emerged from our pre-
vious participation in STS 2012. Such system
is based on a distributional model of seman-
tics capable of taking into account also syn-
tactic structures that glue words together. In
addition, we investigated the use of two dif-
ferent learning strategies exploiting both syn-
tactic and semantic features. The former uses
ensemble learning in order to combine the
best machine learning techniques trained on
2012 training and test sets. The latter tries to
overcome the limit of working with different
datasets with varying characteristics by select-
ing only the more suitable dataset for the train-
ing purpose.
1 Introduction
Semantic Textual Similarity is the task of comput-
ing the similarity between any two given texts. The
task, in its core formulation, aims at capturing the
different kinds of similarity that emerge from texts.
Machine translation, paraphrasing, synonym substi-
tution or text entailment are some fruitful methods
exploited for this purpose. These techniques, along
with other methods for estimating the text similar-
ity, were successfully employed via machine learn-
ing approaches during the 2012 task.
However, the STS 2013 core task (Agirre et al,
2013) differs from the 2012 formulation in that it
provides a test set which is similar to the training,
but not drawn from the same set of data. Hence,
in order to generalize the machine learning models
trained on a group of datasets, we investigate the use
of combination strategies. The objective of combi-
nation strategies, known under the name of ensem-
ble learning, is that of reducing the bias-variance
decomposition through reducing the variance error.
Hence, this class of methods should be more ro-
bust with respect to previously unseen data. Among
the several ensemble learning alternatives, we ex-
ploit the stacked generalization (STACKING) algo-
rithm (Wolpert, 1992). Moreover, we investigate the
use of a two-steps learning algorithm (2STEPSML).
In this method the learning algorithm is trained us-
ing only the dataset most similar to the instance to
be predicted. The first step aims at predicting the
dataset more similar to the given pair of texts. Then
the second step makes use of the previously trained
algorithm to predict the similarity value. The base-
line for the evaluation is represented by our best sys-
tem (DSM PERM) resulting from our participation
in the 2012 task. After introducing the general mod-
els behind our systems in Section 2, Section 3 de-
scribes the evaluation setting of our systems along
with the experimental results. Then, some conclu-
sions and remarks close the paper.
2 General Models
2.1 Dependency Encoding via Vector
Permutations
Distributional models are effective methods for rep-
resenting word paradigmatic relations in a simple
169
way through vector spaces (Mitchell and Lapata,
2008). These spaces are built taking into account
the word context, hence the resulting vector repre-
sentation is such that the distance between vectors
reflects their similarity. Although several definitions
of context are possible (e.g. a sliding window of
text, the word order or syntactic dependencies), in
their plain definition these kinds of models account
for just one type of context at a time. To overcome
this limitation, we exploit a method to encode more
definitions of context in the same vector exploiting
the vector permutations (Caputo et al, 2012). This
technique, which is based on Random Indexing as
a means for computing the distributional model, is
based on the idea that when the components of a
highly sparse vector are shuffled, the resulting vec-
tor is nearly orthogonal to the original one. Hence,
vector permutation represents a way for generat-
ing new random vectors in a predetermined manner.
Different word contexts can be encoded using dif-
ferent types of permutations. In our distributional
model system (DSM PERM), we encode the syn-
tactic dependencies between words rather than the
mere co-occurrence information. In this way, word-
vector components bear the information about both
co-occurring and syntactically related words. In this
distributional space, a text can be easily represented
as the superposition of its words. Then, the vec-
tor representation of a text is given by adding the
vector representation of its words, and the similarity
between texts come through the cosine of the angle
between their vector representations.
2.2 Stacking
Stacking algorithms (Wolpert, 1992) are a way of
combining different types of learning algorithms re-
ducing the variance of the system. In this model,
the meta-learner tries to predict the real value of
an instance combining the outputs of other machine
learning methods.
Figure 1 shows how the learning process takes
place. The level-0 represents the ensemble of dif-
ferent models to be trained on the same dataset. The
level-0 outputs build up the level-1 dataset: an in-
stance at this level is represented by the numeric
values predicted by each level-0 model along with
the gold standard value. Then, the objective of the
level-1 learning model is to learn how to combine
the level-0 outputs in order to provide the best pre-
diction.
le
ve
l-
0
le
ve
l-
1
model1 model2 ? ? ? modeln
meta-learner
prediction
Figure 1: Stacking algorithm
2.3 Two steps learning algorithm
Given an ensemble of datasets with different charac-
teristics, this method is based on the idea that when
instances come from a specific dataset, the learn-
ing algorithm trained on that dataset outperforms the
same algorithm trained on the whole ensemble.
Hence, the two steps algorithm tries to overcome
the problem of dealing with different datasets hav-
ing different characteristics through a classification
model.
st
ep
-1
st
ep
-2
dataset1 dataset2 ? ? ? datasetn
classifier
input
output: dataset class
learning algorithm
predicted dataset
prediction
Figure 2: Two steps machine learning algorithm
In the first step (Figure 2), a different class is as-
signed to each dataset. The classifier is trained on
170
a set of instances whose classes correspond to the
dataset numbers. Then, given a new instance the
output of this step will be the dataset to be used
for training the learning algorithm in the step 2. In
the second step, the learning algorithm is trained on
the dataset choose in the first step. The output of
this step is the predicted similarity between the two
texts. Through these steps, it is possible to select
the dataset with the characteristics more similar to
a given instance, and exploit just this set of data for
learning the algorithm.
2.4 Features
Both STACKING and 2STEPSML systems rely on
several kinds of features, which vary from lexical to
semantic ones. Features are grouped in seven main
classes, as follows:
1. Character/string/annotation-based features:
the length of the longest common contiguous
substring between the texts; the Jaccard index
of both tokens and lemmas; the Levenshtein
distance between texts; the normalized number
of common 2-grams, 3-grams and 4-grams; the
total number of tokens and characters; the dif-
ference in tokens and characters between texts;
the normalized difference with respect to the
max text length in tokens and characters be-
tween texts. Exploiting other linguistic anno-
tations extracted by Stanford CoreNLP1, we
compute the Jaccard index between PoS-tags
and named entities. Using WordNet we extract
the Jaccard index between the first sense and its
super-sense tag.
2. Textual Similarity-based features: a set of fea-
tures based on the textual similarity proposed
by Mihalcea (Mihalcea et al, 2006). Given two
texts T1 and T2 the similarity is computed as
follows:
sim(T1, T2) =
1
2
(
?
w?T1 maxSim(w, T2)?
w?T1 idf(w)
+
?
w?T2 maxSim(w, T1)?
w?T2 idf(w)
)
(1)
1Available at: http://nlp.stanford.edu/software/corenlp.shtml
We adopt several similarity measures using
semantic distributional models (see Section
2.5), the Resnik?s knowledge-based approach
(Resnik, 1995) and the point-wise mutual infor-
mation as suggested by Turney (Turney, 2001)
computed on British National Corpus2. For all
the features, the idf is computed relying on
UKWaC corpus3 (Baroni et al, 2009).
3. Head similarity-based features: this measure
takes into account the maximum similarity be-
tween the roots of each text. The roots are ex-
tracted using the dependency parser provided
by Stanford CoreNLP. The similarity is com-
puted according to the distributional semantic
models proposed in Section 2.5.
4. ESA similarity: computes the similarity
between texts using the Explicit Semantic
Analysis (ESA) approach (Gabrilovich and
Markovitch, 2007). For each text we extract the
ESA vector built using the English Wikipedia,
and then we compute the similarity as the co-
sine similarity between the two ESA vectors.
5. Paraphrasing features: this is a very simple
measure which counts the number of possi-
ble paraphrasings belonging to the two texts.
Given two texts T1 and T2, for each token in T1
a list of paraphrasings is extracted using a dic-
tionary4. If T2 contains one of the paraphrasing
in the list, the score is incremented by one. The
final score is divided by the number of tokens
in T1. The same score is computed taking into
account T2. Finally, the two score are added
and divided by 2.
6. Greedy Lemma Aligning Overlap features:
this measure computes the similarity between
texts using the semantic alignment of lemmas
as proposed by S?aric? et al (2012). In order
to compute the similarity between lemmas, we
exploit the distributional semantic models de-
scribed in Section 2.5.
2Available at: http://www.natcorp.ox.ac.uk/
3Available at: http://wacky.sslmit.unibo.it/
4English Thesaurus for StarDict available at
https://aur.archlinux.org/packages/stardict-thesaurus-ee/
171
7. Compositional features: we build several simi-
larity features using the distributional semantic
models described in Section 2.5 and a compo-
sitional operator based on sum. This approach
is thoroughly explained in Section 2.6
2.5 Distributional semantic models
In several features proposed in our approaches, the
similarity between words is computed using Dis-
tributional Semantic Models. These models repre-
sent word meanings through contexts: the different
meanings of a word can be accounted for by look-
ing at the different contexts wherein the word oc-
curs. This insight can beautifully be expressed by
the geometrical representation of words as vectors
in a semantic space. Each term is represented as a
vector whose components are contexts surrounding
the term. In this way, the meaning of a term across
a corpus is thoroughly conveyed by the contexts it
appears in, where a context may typically be the set
of co-occurring words in a document, in a sentence
or in a window of surrounding terms.
In particular, we take into account two main
classes of models: Simple Distributional Spaces and
Structured Semantic Spaces. The former considers
as context the co-occurring words, the latter takes
into account both co-occurrence and syntactic de-
pendency between words.
Simple Distributional Spaces rely on Latent
Semantic Analysis (LSA) and Random Indexing
(RI) in order to reduce the dimension of the co-
occurrences matrix. Moreover, we use an approach
which applies LSA to the matrix produced by RI.
Structured Semantic Spaces are based on two
techniques to encode syntactic information into the
vector space. The first approach uses the vector per-
mutation of random vector in RI to encode the syn-
tactic role (head or dependent) of a word. The sec-
ond method is based on Holographic Reduced Rep-
resentation, in particular using convolution between
vectors, to encode syntactic information.
Adopting distributional semantic models, each
word can be represented as a vector in a geomet-
ric space. The similarity between two words can be
easily computed taking into account the cosine sim-
ilarity between word vectors.
All models are described in Basile et al (2012).
2.6 Compositional features
In Distributional Semantic Models, given the vector
representations of two words, it is always possible
to compute their similarity as the cosine of the angle
between them.
However, texts are composed by several terms,
so in order to compute the similarity between them
we need a method to compose words occurring in
these texts. It is possible to combine words through
the vector addition (+). This operator is similar to
the superposition defined in connectionist systems
(Smolensky, 1990), and corresponds to the point-
wise sum of components:
p = u + v (2)
where pi = ui + vi
The addition is a commutative operator, which
means that it does not take into account any order
or underlying structures existing between words. In
this first study, we do not exploit more complex
methods to combine word vectors. We plan to in-
vestigate them in future work.
Given a text p, we denote with p its vector repre-
sentation obtained applying addition operator (+) to
the vector representation of terms it is composed of.
Furthermore, it is possible to compute the similar-
ity between two texts exploiting the cosine similarity
between vectors.
Formally, if a = a1, a2...an and b = b1, b2...bm
are two texts, we build two vectors a and b which
represent respectively the two texts in a semantic
space. Vector representations for the two texts are
built applying the addition operator to the vector rep-
resentation of words belonging to them:
a = a1 + a2 + . . . + an
b = b1 + b2 . . . + bm
(3)
The similarity between a and b is computed as the
cosine similarity between them.
3 Experimental evaluation
SemEval-2013 STS is the second attempt to provide
a ?unified framework for the evaluation of modular
semantic textual similarity and to characterize their
impact on NLP applications?. The task consists
in computing the similarity between pair of texts,
172
returning a similarity score. The test set is com-
posed by data coming from the following datasets:
news headlines (headlines); mapping of lexical re-
sources from Ontonotes to Wordnet (OnWN) and
from FrameNet to WordNet (FNWN); and evalua-
tion of machine translation (SMT).
The training data for STS-2013 is made up by
training and testing data from the previous edition
of STS-2012 task. During the 2012 edition, STS
provided participants with three training data: MSR-
Paraphrase, MSR-Video, STMeuropar; and five test-
ing data: MSR-Paraphrase, MSR-Video, STMeu-
ropar, SMTnews and OnWN. It is important to note
that part of 2012 test sets were made up from the
same sources of the training sets. On the other
hand, STS-2013 training and testing are very differ-
ent, making the prediction task a bit harder.
Humans rated each pair of texts with values from
0 to 5. The evaluation is performed by compar-
ing the humans scores against system performance
through Pearson?s correlation with the gold standard
for the four datasets.
3.1 System setup
For the evaluation, we built the distributional spaces
using the WaCkypedia EN corpus5. WaCkype-
dia EN is based on a 2009 dump of the English
Wikipedia (about 800 million tokens) and includes
information about: part-of-speech, lemma and a full
dependency parsing performed by MaltParser (Nivre
et al, 2007). The structured spaces described in
Subsections 2.1 and 2.5 are built exploiting infor-
mation about term windows and dependency pars-
ing supplied by WaCkypedia. The total number of
dependencies amounts to about 200 million.
The RI system is implemented in Java and re-
lies on some portions of code publicly available in
the Semantic Vectors package (Widdows and Fer-
raro, 2008), while for LSA we exploited the publicly
available C library SVDLIBC6.
We restricted the vocabulary to the 50,000 most
frequent terms, with stop words removal and forc-
ing the system to include terms which occur in the
dataset.
Semantic space building involves some parame-
5http://wacky.sslmit.unibo.it/doku.php?id=corpora
6http://tedlab.mit.edu/ dr/SVDLIBC/
ters. In particular, each semantic space needs to set
up the dimension k of the space. All spaces use a
dimension of 500 (resulting in a 50,000?500 ma-
trix). The number of non-zero elements in the ran-
dom vector is set to 10. When we apply LSA to the
output space generated by the Random Indexing we
hold all the 500 dimensions, since during the tuning
we observed a drop in performance when a lower
dimension was set. The co-occurrence distance w
between terms was set up to 4.
In order to compute the similarity between
the vector representations of text using UNIBA-
DSM PERM, we used the cosine similarity, and
then we multiplied by 5 the obtained value.
The two supervised methods, UNIBA-2STEPML
and UNIBA-STACKING, are developed in Java
using Weka7 to implement the learning algo-
rithms. Regarding the stacking approach (UNIBA-
STACKING) we used for the level-0 the follow-
ing models: Gaussian Process with polynomial ker-
nel, Gaussian Process with RBF kernel, Linear Re-
gression, Support Vector regression with polynomial
kernel, and decision tree. The level-1 model uses
a Gaussian Process with RBF kernel. In the first
step of UNIBA-2STEPML we adopt Support Vec-
tor Machine, while in the second one we use Sup-
port Vector Machine for regression. In both steps,
the RBF-Kernel is used. Features are normalized
removing non alphanumerics characters. In all the
learning algorithms, we use the default parameters
set by Weka. As future work, we plan to perform a
tuning step in order to set the best parameters.
The choice of the learning algorithms for both
UNIBA-STACKING and UNIBA-2STEPSML sys-
tems was performed after a tuning phase where only
the STS-2012 training datasets were exploited. Ta-
ble 1 reports the values obtained by our three sys-
tems on the STS-2012 test sets. After the tuning,
we came up with the learning algorithms to employ
in the level-0 and level-1 of UNIBA-STACKING
and in step-1 and step-2 of UNIBA-2STEPSML.
Then, the training of both UNIBA-STACKING and
UNIBA-2STEPSML was performed on all STS-
2012 datasets (training and test data).
173
MSRpar MSRvid SMTeuroparl OnWN SMTnews mean
UNIBA-2STEPSML .6056 .8573 .6233 .5079 .4533 .7016
UNIBA-DSM PERM .4349 .7592 .5324 .6593 .4559 .6172
UNIBA-STACKING .6473 .8727 .5344 .6646 .4604 .7714
Table 1: STS-2012 test results of Pearson?s correlation.
headlines OnWN FNWN SMT mean rank
UNIBA- 2STEPSML .4255 .4801 .1832 .2710 .3673 71
UNIBA- DSM PERM .6319 4910 .2717 .3155 .4610 54
UNIBA- STACKING .6275 .4658 .2111 .2588 .4293 61
Table 2: Evaluation results of Pearson?s correlation for individual datasets.
3.2 Evaluation results
Evaluation results on the STS-2013 data are reported
in Table 2. Among the three systems, UNIBA-
DSM PERM obtained the best performances on
both individual datasets and in the overall evalua-
tion metric (mean), which computes the Pearson?s
correlation considering all datasets combined in a
single one. The best system ranked 54 over a to-
tal of 90 submissions, while UNIBA-STACKING
and UNIBA-2STEPSML ranked 61 and 71 re-
spectively. These results are at odds with those
reported in Table 1. During the test on 2012
dataset, UNIBA-STACKING gave the best result,
followed by UNIBA-2STEPSML, while UNIBA-
DSM PERM gave the worst performance. The
UNIBA-STACKING system corroborated our hy-
pothesis giving also the best results on those datasets
not exploited during the training phase of the sys-
tem (OnWN, SMTnews). Conversely, UNIBA-
2STEPSML reported a different trend showing its
weakness with respect to a high variance in the data,
and performing worse than UNIBA-DSM PERM on
the OnWN and SMTnews datasets.
However, the evaluation results have refuted our
hypothesis, even with the use of the stacking sys-
tem. The independence from a training set makes
the UNIBA-DSM PERM system more robust than
other supervised algorithms, even though it is not
able to give always the best performance on individ-
ual datasets, as highlighted by results in Table 1.
7http://www.cs.waikato.ac.nz/ml/weka/
4 Conclusions
This paper reports on UNIBA participation in Se-
mantic Textual Similarity 2013 core task. In this
task edition, we exploited both distributional mod-
els and machine learning techniques to build three
systems. A distributional model, which takes into
account the syntactic structure that relates words in a
corpus, has been used as baseline. Moreover, we in-
vestigate the use of two machine learning techniques
as a means to make our systems more independent
from the training data. However, the evaluation re-
sults have highlighted the higher robustness of the
distributional model with respect to these systems.
Acknowledgments
This work fulfils the research objectives of the PON
02 00563 3470993 project ?VINCENTE - A Virtual
collective INtelligenCe ENvironment to develop
sustainable Technology Entrepreneurship ecosys-
tems? funded by the Italian Ministry of University
and Research (MIUR).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
174
Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-
aro. 2012. A study on compositional semantics of
words in distributional spaces. In Sixth IEEE Inter-
national Conference on Semantic Computing, ICSC
2012, Palermo, Italy, September 19-21, 2012, pages
154?161. IEEE Computer Society.
Annalina Caputo, Pierpaolo Basile, and Giovanni Semer-
aro. 2012. Uniba: Distributional semantics for tex-
tual similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 591?596, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th in-
ternational joint conference on artificial intelligence,
volume 6, page 12.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
pages 775?780. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Kathleen McKe-
own, Johanna D. Moore, Simone Teufel, James Allan,
and Sadaoki Furui, editors, ACL 2008, Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, June 15-20, 2008, Columbus,
Ohio, USA, pages 236?244. The Association for Com-
puter Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
pages 448?453.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1-
2):159?216, November.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic Vectors: A Scalable Open Source Package
and Online Technology Management Application. In
Nicoletta Calzolari, Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odjik, Stelios Piperidis, and
Daniel Tapias, editors, Proceedings of the 6th Interna-
tional Conference on Language Resources and Eval-
uation (LREC2008), pages 1183?1190, Marrakech,
Morocco. European Language Resources Association
(ELRA).
David H. Wolpert. 1992. Stacked generalization. Neural
networks, 5(2):241?259, February.
175
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 748?753,
Dublin, Ireland, August 23-24, 2014.
UNIBA: Combining Distributional Semantic Models and Word Sense
Disambiguation for Textual Similarity
Pierpaolo Basile and Annalina Caputo and Giovanni Semeraro
Department of Computer Science
University of Bari Aldo Moro
Via, E. Orabona, 4 - 70125 Bari (Italy)
{firstname.surname}@uniba.it
Abstract
This paper describes the UNIBA team
participation in the Cross-Level Semantic
Similarity task at SemEval 2014. We pro-
pose to combine the output of different se-
mantic similarity measures which exploit
Word Sense Disambiguation and Distribu-
tional Semantic Models, among other lex-
ical features. The integration of similar-
ity measures is performed by means of
two supervised methods based on Gaus-
sian Process and Support Vector Machine.
Our systems obtained very encouraging
results, with the best one ranked 6
th
out
of 38 submitted systems.
1 Introduction
Cross-Level Semantic Similarity (CLSS) is the
task of computing the similarity between two text
fragments of different sizes. The task focuses on
the comparison between texts at different lexical
levels, i.e. between a larger and a smaller text.
The task comprises four different levels: 1) para-
graph to sentence; 2) sentence to phrase; 3) phrase
to word; 4) word to sense. The task objective is
to provide a framework for evaluating general vs.
level-specialized methods.
Our general approach consists in combining
scores coming from different semantic similarity
algorithms. The combination is performed by a
supervised method using the training data pro-
vided by the task organizers. The data set com-
prises pairs of text fragments that can be rated with
a score between 0 and 4, where 4 indicates the
maximum level of similarity.
We select algorithms which provide similarities
at different levels of semantics: surface (or string-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
based), lexical (word sense disambiguation), and
distributional level. The idea is to combine in a
unique system the semantic aspects that pertain
text fragments.
The following section gives more details about
the similarity measures and their combination in a
unique score through supervised methods (Section
2). Section 3 describes the system set up for the
evaluation and comments on the reported results,
while Section 4 concludes the paper.
2 System Description
The idea behind our system is to combine the
output of several similarity measures/features by
means of a supervised algorithm. Those features
were grouped in three main categories. The fol-
lowing three sub-sections describe in detail each
feature exploited by the system.
2.1 Distributional Semantics Level
Distributional Semantic Models (DSM) are an
easy way for building geometrical spaces of con-
cepts, also known as Semantic (or Word) Spaces,
by skimming through huge corpora of text in or-
der to learn the context of word usage. In the re-
sulting space, semantic relatedness/similarity be-
tween two words is expressed by the opposite of
the distance between points that represent those
words. Thus, the semantic similarity can be com-
puted as the cosine of the angle between the two
vectors that represent the words. This concept
of similarity can be extended to whole sentences
by combining words through vector addition (+),
which corresponds to the point-wise sum of the
vector components. Our DSM measure (DSM)
is based on a SemanticSpace, represented by a
co-occurrences matrix M , built by analysing the
distribution of words in the British National Cor-
pus (BNC). Then, M is reduced using the Latent
Semantic Analysis (LSA) (Landauer and Dumais,
1997). Vector addition and cosine similarity are
748
then used for building the vector representation of
each text fragment and computing their pairwise
similarity, respectively.
2.2 Lexical Semantics Level
Word Sense Disambiguation. Most of our
measures rely on the output of a Word Sense Dis-
ambiguation (WSD) algorithm. Our newest ap-
proach to WSD, recently presented in Basile et
al. (2014), is based on the simplified Lesk algo-
rithm (Vasilescu et al., 2004). Each word w
i
in
a sequence w
1
w
2
...w
n
is disambiguated individ-
ually by choosing the sense that maximizes the
similarity between the gloss and the context of w
i
(i.e. the whole text where w
i
occurs). To boost
the overlap between the context and the gloss,
this last is expanded with glosses of related mean-
ings, following the approach described in Baner-
jee and Pedersen (2002). As sense inventory we
choose BabelNet 1.1, a huge multilingual seman-
tic network which comprises both WordNet and
Wikipedia (Navigli and Ponzetto, 2012). The al-
gorithm consists of the following steps:
1. Building the glosses. We retrieve all possible
word meanings for the target word w
i
that are
listed in BabelNet. BabelNet mixes senses
in WordNet and Wikipedia. First, senses
in WordNet are searched for; if no sense is
found (as often happens with named enti-
ties), senses for the target word are sought in
Wikipedia. We preferred that strategy rather
than retrieving senses from both sources at
once because this last approach produced
worse results when tuning the system. Once
the set of senses S
i
= {s
i1
, s
i2
, ..., s
ik
} as-
sociated to the target word w
i
has been re-
trieved, gloss expansion occurs. For each
sense s
ij
of w
i
, the algorithm builds the sense
extended gloss g
?
ij
by appending the glosses
of meanings related to s
ij
to its original gloss
g
ij
. The related meanings, with the exception
of ?antonym? senses, are the output of the
BabelNet function ?getRelatedMap?. More-
over, each word in g
?
ij
is weighted by a func-
tion inversely proportional to the distance be-
tween s
ij
and its related meaning. The dis-
tance d is computed as the number of edges
linking two senses in the graph. The func-
tion takes also into account the frequencies
of the words in all the glosses giving more
emphasis to the most discriminative words;
this can be considered as a variation of the in-
verse document frequency (idf ) for retrieval
that we named inverse gloss frequency (igf ).
The igf for a word w
k
occurring gf
?
k
times in
the set of extended glosses for all the senses
in S
i
, the sense inventory of w
i
, is computed
as IGF
k
= 1 + log
2
|S
i
|
gf
?
k
. The final weight
for the word w
k
appearing h times in the ex-
tended gloss g
?
ij
is given by:
weight(w
k
, g
?
ij
) = h? IGF
k
?
1
1 + d
(1)
2. Building the context. The context C for the
word w
i
is represented by all the words that
occur in the text.
3. Building the vector representations. The con-
text C and each extended gloss g
?
ij
are repre-
sented as vectors in the SemanticSpace built
through the DSM described in Subsection
2.1.
4. Sense ranking. The algorithm computes the
cosine similarity between the vector repre-
sentation of each extended gloss g
?
ij
and that
of the context C. Then, the cosine similar-
ity is linearly combined with the probability
p(s
ij
|w
i
), which takes into account the sense
distribution of s
ij
given the word w
i
. The
sense distribution is computed as the num-
ber of times the word w
i
was tagged with
the sense s
ij
in SemCor, a collection of 352
documents manually annotated with Word-
Net synsets. T he additive (Laplace) smooth-
ing prevents zero probabilities, which can oc-
cur when some synsets do not appear in Sem-
Cor. The probability is computed as follows:
p(s
ij
|w
i
) =
t(w
i
, s
ij
) + 1
#w
i
+ |S
i
|
(2)
The output of this step is a ranked list of
synsets.
The WSD measure (WSD) is computed on the top
of the output of the last step. For each text frag-
ment, we build a Bag-of-Synset (BoS) as the sum,
over the whole text, of the weighted synsets as-
sociated with each word. Then, we compute the
WSD similarity as the cosine similarity between
the two BoS.
749
Graph. A sub-graph of BabelNet is built for
each text fragment starting from the synsets pro-
vided by the WSD algorithm. For each word the
synset with the highest score is selected, then this
initial set is expanded with the related synsets in
BabelNet. We apply the Personalized Page Rank
(Haveliwala, 2002) to each sub-graph where the
synset scores computed by the WSD algorithm are
exploited as prior probabilities. The weighted rank
of synsets provided by Page Rank is used to build
the BoS of the two text fragments, then the Person-
alized Page Rank (PPR) is computed as the cosine
similarity between them.
Synset Distributional Space. Generally, sim-
ilarity measures between synsets rely on the
synsets hierarchy in a semantic network (e.g.
WordNet). We define a new approach that is com-
pletely different, and represents synsets as points
in a geometric space that we call SDS (Synset Dis-
tributional Space). SDS is generated taking into
account the synset relationships, and similarity is
defined as the synsets closeness in the space. We
build a symmetric matrix S which contains synsets
on both rows and columns. Each cell in the matrix
is set to one if a semantic relation exists between
the corresponding synsets. The relationships are
extracted from BabelNet limiting synsets to those
occurring also in WordNet, while synsets coming
from Wikipedia are removed to reduce the size
of S. The method for building the matrix S re-
lies on Reflective Random Indexing (RRI) (Co-
hen et al., 2010), a variation of the Random In-
dexing technique for matrix reduction (Kanerva,
1988). RRI retains the advantages of RI which
incrementally builds a reduced space where dis-
tance between points is nearly preserved. More-
over, cyclical training, i.e. the retraining of a new
space exploiting the RI output as basis vectors,
makes indirect inference to emerge. Two differ-
ent similarity measures can be defined by exploit-
ing this space for representing synsets: WSD-SDS
and PPR-SDS, based on WSD and PPR respec-
tively. Each BoS is represented as the sum of the
synset vectors in the SDS space. Then, the simi-
larity is computed as the cosine similarity between
the two vector representations.
2.3 Surface Level
At the surface level, we compute the following
features:
EDIT The edit, or Levenshtein, distance between
the two texts;
MCS The most common subsequence between
the two texts;
2-gram, 3-gram For each text fragment, we
build the Bag-of-n-gram (with n varying in
{2, 3}); then we compute the cosine similar-
ity between the two Bag-of-n-gram repre-
sentations.
BOW For each tokenized text fragment, we build
its Bag-of-Word, and then compute the co-
sine similarity between the two BoW.
L1 The length in characters of the first text frag-
ment;
L2 The length in caracters of the second text frag-
ment;
DIFF The difference between L1 and L2.
2.4 Word to Sense
The word to sense level is different from the other
ones: in this case the similarity is computed be-
tween a word and a particular word meaning.
Since a word meaning is not a text fragment, this
level poses a new challenge with respect to the
classical text similarity task. In this case we de-
cide to consider the word on its own as the first
text fragment, while for the second text fragment
we build a dummy text using the BabelNet gloss
assigned to the word sense. In that way, the distri-
butional and the lexical measures (WSD, Graph,
and DSM) can be applied to both fragments. Ta-
ble 1 recaps the features used for each task.
3 Evaluation
Dataset Description. The SemEval-2014 Task 3
Cross-Level Semantic Similarity is designed for
evaluating systems on their ability to capture the
semantic similarity between lexical items of dif-
ferent length (Jurgens et al., 2014). To this ex-
tent, the organizers provide four different levels
of comparison which correspond to four different
datasets: 1) Paragraph to Sentence (Par2Sent); 2)
Sentence to Phrase (Sent2Ph); 3) Phrase to Word
(Ph2W); and 4) Word to Sense (W2Sense).
For each dataset, the organizer released trial,
training and test data. While the trial includes a
few examples (approximately 40), both training
and test data comprise 500 pairs of text fragments.
750
Run Par2Sent Sent2Ph Ph2W W2Sense
Official
Rank
Spearman
Correlation
bestTrain .861 .793 .555 .420 - -
LCS .527 .562 .165 .109 - -
run1 .769 .729 .229 .165 7 10
run2 .784 .734 .255 .180 6 8
run3 .769 .729 .229 .165 8 11
Table 2: Task results.
Feature
Par2Sent
Sent2Ph
Ph2W
W2Sense
DSM
? ?
WSD
? ?
PPR
? ?
WSD-SDS
? ?
PPR-SDS
? ?
EDIT
?
-
MCS
?
-
2-gram
?
-
3-gram
?
-
BOW
?
-
L1
?
-
L2
?
-
DIFF
?
-
Table 1: Features per task.
Each pair is associated with a human-assigned
similarity score, which varies from 4 (very similar)
to 0 (unrelated). Organizers provide the normal-
ized Longest Common Substring (LCS) as base-
line. The evaluation is performed through the
Pearson (official rank) and the Spearman?s rank
correlation.
System setup. We develop our system in JAVA
relying on the following resources:
? Stanford CoreNLP to pre-process the text:
tokenization, lemmatization and PoS-tagging
are applied to the two text fragments;
? BabelNet 1.1 as knowledge-base in the WSD
algorithm;
? JAVA JUNG library for Personalized Page
Rank;
? British National Corpus (tokenized text with
stop word removal) and SVDLIB to build the
SemanticSpace described in Subsection 2.1;
? A proprietary implementation of Reflective
Random Indexing to build the distributional
space based on synsets (SDS) extracted from
BabelNet (we used two cycles of retraining);
? Weka for the supervised approach.
After a tuning step using both training and trial
data provided by organizers, we selected three dif-
ferent supervised systems: Gaussian Process with
Puk kernel (run1), Gaussian Process with RBF
kernel (run2), and Support Vector Machine Re-
gression with Puk kernel (run3). All the sys-
tems are implemented with the default parame-
ters set by Weka. We trained a different model on
each dataset. The DSM is built using the 100, 000
most frequent terms in the BNC, while the co-
occurrences are computed on a window size of 5
words. The vector dimension is set to 400, the
same value is adopted for building the SDS, where
the number of seeds (no zero components) gener-
ated in the random vectors is set to 10 with one
step of retraining. The total number of synset vec-
tors in the SDS is 576, 736. In the WSD algorithm,
we exploited the whole sentence as context. The
linear combination between the cosine similarity
and the probability p(s
ij
|w
i
) is performed with a
factor of 0.5. The distance for expanding a synset
with its related meaning is set to one. The same
depth is used for building the graph in the PPR
method, where we fixed the maximum number of
iterations up to 50 and the dumpling factor to 0.85.
Results. Results of our three systems for
each similarity level are reported in Table 2 with
the baseline provided by the organizer (LCS).
Our three systems always outperform the LCS
baseline. Table 2 also shows the best results
(bestTrain) obtained on the training data by a
Gaussian Process with Puk kernel and a 10-fold
cross-validation. Support Vector Machine and
Gaussian Process with Puk kernel, run1 and run3
respectively, produce the same results. Comparing
751
T
a
s
k
D
S
M
W
S
D
P
P
R
W
S
D
-
S
D
S
P
P
R
-
S
D
S
E
D
I
T
M
C
S
2
-
g
r
a
m
3
-
g
r
a
m
B
O
W
L
1
L
2
D
I
F
F
P ar2Sent .612 .697 -.580 .129 .129 .461 .44 .630 .478 .585 .002 .231 .116
Sent2Ph .540 .649 -.641 .110 .110 .526 .474 .376 .236 .584 .069 .357 .218
Ph2W .228 .095 -.094 .087 .087 .136 .120 - - .095 .079 .013 .071
W2Sense .147 .085 -.062 .084 .062
Table 3: Individual measures for each task.
these figures with those obtained on training data
(run1 and run3 vs. bestTrain), we can observe
that the Puk kernel tends to over-fit on training
data, while RBF kernel seems to be less sensitive
to this problem.
We analysed also the performance of each mea-
sure on its own; results in Table 3 are obtained by
training the best supervised system (run2) with
default parameters on each feature individually.
WSD obtains the best results in the first two lev-
els, while DSM is the best method in the last two
ones. This behaviour can be ascribed to the size
of the text fragments. In large text fragments the
WSD algorithm can rely on wider contexts to ob-
tain good performance; while in short texts infor-
mation about context is poor. At the W2Sense
level, the measure based on the Personalized Page
Rank obtains the worst results; however, we no-
ticed that the ablation of that feature causes a drop
in performance of the supervised systems.
After the submission deadline, we noticed that
sometimes PoS-tagging produced wrong results
on small texts. This incorrect behaviour influenced
negatively the correct retrieval of synsets from Ba-
belNet. Thus, we decided to exclude PoS-tagging
for text fragments with less than three words. In
such a case, all the synsets for a given word are
retrieved. Making this adjustment, we were able
to obtain the improvements (?%) with respect to
the submitted runs reported on Table 4.
Run Ph2W ?% W2Sense ?%
run1 .263 +14.85 .242 +46.67
run2 .257 +00.78 .237 +31.67
run3 .263 +14.85 .242 +46.66
Table 4: Results after PoS-tagging removal for
short text (< 3 words).
4 Conclusions
We have reported the results of our participa-
tion in the cross-level semantic similarity task
of SemEval-2014. Our systems combine differ-
ent similarity measures based on string-matching,
word sense disambiguation and distributional se-
mantic models. Our best system ranks 6th out
of the 38 participants in the task with respect to
the Pearson correlation, while it ranks 8th when
Spearman was used. These results suggest that our
methods are robust with respect to the evaluation
measures.
Acknowledgments
This work fulfils the research objectives of the
PON 02 00563 3470993 project ?VINCENTE -
A Virtual collective INtelligenCe ENvironment to
develop sustainable Technology Entrepreneurship
ecosystems? funded by the Italian Ministry of Uni-
versity and Research (MIUR).
References
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Computational linguis-
tics and intelligent text processing, pages 136?145.
Springer.
Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An Enhanced Lesk Word Sense
Disambiguation algorithm through a Distributional
Semantic Model. In Proceedings of COLING
2014, Dublin, Ireland, August. (http://www.coling-
2014.org/accepted-papers.php, in press).
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2010. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43(2):240 ? 256.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of the 11th International Conference
752
on World Wide Web, WWW ?02, pages 517?526,
New York, NY, USA. ACM.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland, August 23?
24.
Pentti Kanerva. 1988. Sparse Distributed Memory.
MIT Press.
Thomas K. Landauer and Susan T. Dumais. 1997. A
Solution to Plato?s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. Psychological Review,
104(2):211?240.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Florentina Vasilescu, Philippe Langlais, and Guy La-
palme. 2004. Evaluating variants of the lesk ap-
proach for disambiguating words. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC), pages 633?636.
753
Combining Knowledge-based
Methods and Supervised
Learning for Effective Italian
Word Sense Disambiguation
Pierpaolo Basile
Marco de Gemmis
Pasquale Lops
Giovanni Semeraro
University of Bari (Italy)
email: basilepp@di.uniba.it
Abstract
This paper presents a WSD strategy which combines a knowledge-based
method that exploits sense definitions in a dictionary and relations among
senses in a semantic network, with supervised learning methods on anno-
tated corpora. The idea behind the approach is that the knowledge-based
method can cope with the possible lack of training data, while supervised
learning can improve the precision of a knowledge-based method when
training data are available. This makes the proposed method suitable for
disambiguation of languages for which the available resources are lacking
in training data or sense definitions. In order to evaluate the effectiveness
of the proposed approach, experimental sessions were carried out on the
dataset used for the WSD task in the EVALITA 2007 initiative, devoted to
the evaluation of Natural Language Processing tools for Italian. The most
effective hybrid WSD strategy is the one that integrates the knowledge-
based approach into the supervised learning method, which outperforms
both methods taken singularly.
5
6 Basile, de Gemmis, Lops, and Semeraro
1 Background and Motivations
The inherent ambiguity of human language is a greatly debated problem in many
research areas, such as information retrieval and text categorization, since the presence
of polysemous words might result in a wrong relevance judgment or classification of
documents. These problems call for alternative methods that work not only at the
lexical level of the documents, but also at the meaning level.
The task of Word Sense Disambiguation (WSD) consists in assigning the most ap-
propriate meaning to a polysemous word within a given context. Applications such
as machine translation, knowledge acquisition, common sense reasoning and others,
require knowledge about word meanings, and WSD is essential for all these applica-
tions. The assignment of senses to words is accomplished by using two major sources
of information (Nancy and V?ronis, 1998):
1. the context of the word to be disambiguated, e.g. information contained within
the text in which the word appears;
2. external knowledge sources, including lexical resources, as well as hand-devised
knowledge sources, which provide data useful to associate words with senses.
All disambiguation work involves matching the context of the instance of the word
to be disambiguated with either information from an external knowledge source (also
known as knowledge-driven WSD), or information about the contexts of previously
disambiguated instances of the word derived from corpora (data-driven or corpus-
based WSD).
Corpus-basedWSD exploits semantically annotated corpora to train machine learn-
ing algorithms to decide which word sense to choose in which context. Words in such
annotated corpora are tagged manually using semantic classes chosen from a particu-
lar lexical semantic resource (e.g. WORDNET (Fellbaum, 1998)). Each sense-tagged
occurrence of a particular word is transformed into a feature vector, which is then used
in an automatic learning process. The applicability of such supervised algorithms is
limited to those few words for which sense tagged data are available, and their accu-
racy is strongly influenced by the amount of labeled data available.
Knowledge-based WSD has the advantage of avoiding the need of sense-annotated
data, rather it exploits lexical knowledge stored in machine-readable dictionaries or
thesauri. Systems adopting this approach have proved to be ready-to-use and scalable,
but in general they reach lower precision than corpus-based WSD systems.
Our hypothesis is that the combination of both types of strategies can improveWSD
effectiveness, because knowledge-based methods can cope with the possible lack of
training data, while supervised learning can improve the precision of knowledge-based
methods when training data are available.
This paper presents a method for solving the semantic ambiguity of all words con-
tained in a text1. We propose a hybrid WSD algorithm that combines a knowledge-
based WSD algorithm, called JIGSAW, which we designed to work by exploiting
WORDNET-like dictionaries as sense repository, with a supervised machine learning
1all words task tries to disambiguate all the words in a text, while lexical sample task tries to disam-
biguate only specific words
Combining Knowledge-based Methods and Supervised Learning 7
algorithm (K-Nearest Neighbor classifier). WORDNET-like dictionaries are used be-
cause they combine the characteristics of both a dictionary and a structured semantic
network, supplying definitions for the different senses of words and defining groups
of synonymous words by means of synsets, which represent distinct lexical concepts.
WORDNET also organize synsets in a conceptual structure by defining a number of
semantic relationship (IS-A, PART-OF, etc.) among them.
Mainly, the paper concentrates on two investigations:
1. First, corpus-based WSD is applied to words for which training examples are
provided, then JIGSAW is applied to words not covered in the first step, with the
advantage of knowing the senses of the context words already disambiguated in
the first step;
2. First, JIGSAW is applied to assign the most appropriate sense to those words
that can be disambiguated with a high level of confidence (by setting a specific
parameter in the algorithm), then the remaining words are disambiguated by the
corpus-based method.
The paper is organized as follows: After a brief discussion about the main works
related to our research, Section 3 gives the main ideas underlying the proposed hybrid
WSD strategy. More details about the K-NN classification algorithm and JIGSAW,
on which the hybrid WSD approach is based, are provided in Section 4 and Section
5, respectively. Experimental sessions have been carried out in order to evaluate the
proposed approach in the critical situation when training data are not much reliable,
as for Italian. Results are presented in Section 6, while conclusions and future work
close the paper.
2 Related Work
For some Natural Language Processing (NLP) tasks, such as part of speech tagging or
named entity recognition, there is a consensus on what makes a successful algorithm,
regardless of the approach considered. Instead, no such consensus has been reached
yet for the task of WSD, and previous work has considered a range of knowledge
sources, such as local collocational clues, common membership in semantically or
topically related word classes, semantic density, and others. In recent SENSEVAL-3
evaluations2, the most successful approaches for all wordsWSD relied on information
drawn from annotated corpora. The system developed by Decadt et al (2002) uses two
cascaded memory-based classifiers, combined with the use of a genetic algorithm for
joint parameter optimization and feature selection. A separate word expert is learned
for each ambiguous word, using a concatenated corpus of English sense tagged texts,
including SemCor, SENSEVAL datasets, and a corpus built from WORDNET exam-
ples. The performance of this system on the SENSEVAL-3 English all words dataset
was evaluated at 65.2%. Another top ranked system is the one developed by Yuret
(2004), which combines two Na?ve Bayes statistical models, one based on surround-
ing collocations and another one based on a bag of words around the target word.
The statistical models are built based on SemCor and WORDNET, for an overall dis-
ambiguation accuracy of 64.1%. All previous systems use supervised methods, thus
2http://www.senseval.org
8 Basile, de Gemmis, Lops, and Semeraro
requiring a large amount of human intervention to annotate the training data. In the
context of the current multilingual society, this strong requirement is even increased,
since the so-called ?sense-tagged data bottleneck problem? is emphasized.
To address this problem, different methods have been proposed. This includes
the automatic generation of sense-tagged data using monosemous relatives (Leacock
et al, 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002),
parallel texts as a way to point out word senses bearing different translations in a sec-
ond language (Diab, 2004), and the use of volunteer contributions over the Web (Mi-
halcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of
sense annotations for building a sense annotated corpus which can be used to train
accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense
annotations were found reliable, leading to accurate sense classifiers, one of the lim-
itations of the approach is that definitions and annotations in Wikipedia are available
almost exclusively for nouns.
On the other hand, the increasing availability of large-scale rich (lexical) knowledge
resources seems to provide new challenges to knowledge-based approaches (Navigli
and Velardi, 2005; Mihalcea, 2005). Our hypothesis is that the complementarity of
knowledge-based methods and corpus-based ones is the key to improve WSD effec-
tiveness. The aim of the paper is to define a cascade hybrid method able to exploit
both linguistic information coming from WORDNET-like dictionaries and statistical
information coming from sense-annotated corpora.
3 A Hybrid Strategy for WSD
The goal of WSD algorithms consists in assigning a word wi occurring in a document
d with its appropriate meaning or sense s. The sense s is selected from a predefined set
of possibilities, usually known as sense inventory. We adopt ITALWORDNET (Roven-
tini et al, 2003) as sense repository. The algorithm is composed by two procedures:
1. JIGSAW - It is a knowledge-based WSD algorithm based on the assumption
that the adoption of different strategies depending on Part-of-Speech (PoS) is
better than using always the same strategy. A brief description of JIGSAW is
given in Section 5, more details are reported in Basile et al (2007b), Basile et al
(2007a) and Semeraro et al (2007).
2. Supervised learning procedure - A K-NN classifier (Mitchell, 1997), trained
on MultiSemCor corpus3 is adopted. Details are given in Section 4. MultiSem-
Cor is an English/Italian parallel corpus, aligned at the word level and annotated
with PoS, lemma and word senses. The parallel corpus is created by exploiting
the SemCor corpus4, which is a subset of the English Brown corpus containing
about 700,000 running words. In SemCor, all the words are tagged by PoS, and
more than 200,000 content words are also lemmatized and sense-tagged with
reference to the WORDNET lexical database. SemCor has been used in several
supervised WSD algorithms for English with good results. MultiSemCor con-
tains less annotations than SemCor, thus the accuracy and the coverage of the
supervised learning for Italian might be affected by poor training data.
3http://multisemcor.itc.it/
4http://www.cs.unt.edu/~rada/downloads.html\#semcor
Combining Knowledge-based Methods and Supervised Learning 9
The idea is to combine both procedures in a hybrid WSD approach. A first choice
might be the adoption of the supervised method as first attempt, then JIGSAW could
be applied to words not covered in the first step. Differently, JIGSAWmight be applied
first, then leaving the supervised approach to disambiguate the remaining words. An
investigation is required in order to choose the most effective combination.
4 Supervised Learning Method
The goal of supervised methods is to use a set of annotated data as little as possible,
and at the same time to make the algorithm general enough to be able to disambiguate
all content words in a text. We use MultiSemCor as annotated corpus, since at present
it is the only available semantic annotated resource for Italian. The algorithm starts
with a preprocessing stage, where the text is tokenized, stemmed, lemmatized and
annotated with PoS.
Also, the collocations are identified using a sliding window approach, where a
collocation is considered to be a sequence of words that forms a compound concept
defined in ITALWORDNET (e.g. artificial intelligence). In the training step, a semantic
model is learned for each PoS, starting with the annotated corpus. These models are
then used to disambiguate words in the test corpus by annotating them with their
correspondingmeaning. The models can only handle words that were previously seen
in the training corpus, and therefore their coverage is not 100%. Starting with an
annotated corpus formed by all annotated files in MultiSemCor, a separate training
dataset is built for each PoS. For each open-class word in the training corpus, a feature
vector is built and added to the corresponding training set. The following features
are used to describe an occurrence of a word in the training corpus as in Hoste et al
(2002):
? Nouns - 2 features are included in feature vector: the first noun, verb, or adjec-
tive before the target noun, within a window of at most three words to the left,
and its PoS;
? Verbs - 4 features are included in feature vector: the first word before and the
first word after the target verb, and their PoS;
? Adjectives - all the nouns occurring in two windows, each one of six words
(before and after the target adjective) are included in the feature vector;
? Adverbs - the same as for adjectives, but vectors contain adjectives rather than
nouns.
The label of each feature vector consists of the target word and the corresponding
sense, represented as word#sense. Table 1 describes the number of vectors for each
PoS.
To annotate (disambiguate) new text, similar vectors are built for all content-words
in the text to be analyzed. Consider the target word bank, used as a noun. The algo-
rithm catches all the feature vectors of bank as a noun from the training model, and
builds the feature vector v f for the target word. Then, the algorithm computes the sim-
ilarity between each training vector and v f and ranks the training vectors in decreasing
order according to the similarity value.
10 Basile, de Gemmis, Lops, and Semeraro
Table 1: Number of feature vectors
PoS #feature vectors
Noun 38,546
Verb 18,688
Adjective 6,253
Adverb 1,576
The similarity is computed as Euclidean distance between vectors, where POS dis-
tance is set to 1, if POS tags are different, otherwise it is set to 0. Word distances
are computed by using the Levenshtein metric, that measures the amount of difference
between two strings as the minimum number of operations needed to transform one
string into the other, where an operation is an insertion, deletion, or substitution of a
single character (Levenshtein, 1966). Finally, the target word is labeled with the most
frequent sense in the first K vectors.
5 JIGSAW - Knowledge-based Approach
JIGSAW is a WSD algorithm based on the idea of combining three different strategies
to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind
our approach is that the effectiveness of a WSD algorithm is strongly influenced by
the POS tag of the target word.
JIGSAW takes as input a document d = (w1, w2, . . . , wh) and returns a list of
synsets X = (s1, s2, . . . , sk) in which each element si is obtained by disambiguating
the target word wi based on the information obtained from the sense repository about
a few immediately surrounding words. We define the context C of the target word
to be a window of n words to the left and another n words to the right, for a total
of 2n surrounding words. The algorithm is based on three different procedures for
nouns, verbs, adverbs and adjectives, called JIGSAWnouns, JIGSAWverbs, JIGSAWothers,
respectively.
JIGSAWnouns - Given a set of nouns W = {w1,w2, . . . ,wn}, obtained from docu-
ment d, with each wi having an associated sense inventory Si = {si1,si2, . . . ,sik} of
possible senses, the goal is assigning each wi with the most appropriate sense sih ? Si,
according to the similarity of wi with the other words in W (the context for wi). The
idea is to define a function ?(wi,si j), wi ?W , si j ? Si, that computes a value in [0,1]
representing the confidence with which word wi can be assigned with sense si j. In
order to measure the relatedness of two words we adopted a modified version of the
Leacock and Chodorow (1998) measure, which computes the length of the path be-
tween two concepts in a hierarchy by passing through their Most Specific Subsumer
(MSS). We introduced a constant factor depth which limits the search for the MSS to
depth ancestors, in order to avoid ?poorly informative? MSSs. Moreover, in the simi-
larity computation, we introduced both a Gaussian factor G(pos(wi), pos(w j)), which
takes into account the distance between the position of the words in the text to be dis-
ambiguated, and a factor R(k), which assigns sik with a numerical value, according to
the frequency score in ITALWORDNET.
JIGSAWverbs - We define the description of a synset as the string obtained by
Combining Knowledge-based Methods and Supervised Learning 11
concatenating the gloss and the sentences that ITALWORDNET uses to explain the
usage of a synset. JIGSAWverbs includes, in the contextC for the target verb wi, all the
nouns in the window of 2n words surrounding wi. For each candidate synset sik of wi,
the algorithm computes nouns(i,k), that is the set of nouns in the description for sik.
Then, for each w j inC and each synset sik, the following value is computed:
(1) max jk = maxwl?nouns(i,k)
{
sim(w j,wl,depth)
}
where sim(w j,wl,depth) is the same similarity measure adopted by JIGSAWnouns.
Finally, an overall similarity score among sik and the whole contextC is computed:
(2) ?(i,k) = R(k) ?
?w j?CG(pos(wi), pos(w j)) ?max jk
?hG(pos(wi), pos(wh))
where both R(k) and G(pos(wi), pos(w j)), that gives a higher weight to words closer
to the target word, are defined as in JIGSAWnouns. The synset assigned to wi is the one
with the highest ? value.
JIGSAWothers - This procedure is based on the WSD algorithm proposed in Baner-
jee and Pedersen (2002). The idea is to compare the glosses of each candidate sense
for the target word to the glosses of all the words in its context.
6 Experiments
The main goal of our investigation is to study the behavior of the hybrid algorithm
when available training resources are not much reliable, e.g. when a lower number
of sense descriptions is available, as for Italian. The hypothesis we want to evaluate
is that corpus-based methods and knowledge-based ones can be combined to improve
the accuracy of each single strategy.
Experiments have been performed on a standard test collection in the context of the
All-Words-Task, in whichWSD algorithms attempt to disambiguate all words in a text.
Specifically, we used the EVALITA WSD All-Words-Task dataset5, which consists of
about 5,000 words labeled with ITALWORDNET synsets.
An important concern for the evaluation of WSD systems is the agreement rate
between human annotators on word sense assignment.
While for natural language subtasks like part-of-speech tagging, there are relatively
well defined and agreed-upon criteria of what it means to have the ?correct? part of
speech assigned to a word, this is not the case for word sense assignment. Two human
annotators may genuinely disagree on their sense assignment to a word in a context,
since the distinction between the different senses for a commonly used word in a
dictionary like WORDNET tend to be rather fine.
What we would like to underline here is that it is important that human agreement
on an annotated corpus is carefully measured, in order to set an upper bound to the
performance measures: it would be futile to expect computers to agree more with the
reference corpus that human annotators among them. For example, the inter-annotator
agreement rate during the preparation of the SENSEVAL-3 WSD English All-Words-
Task dataset (Agirre et al, 2007) was approximately 72.5%.
5http://evalita.itc.it/tasks/wsd.html
12 Basile, de Gemmis, Lops, and Semeraro
Unfortunately, for EVALITA dataset, the inter-annotator agreement has not been
measured, one of the reasons why the evaluation for Italian WSD is very hard. In our
experiments, we reasonably selected different baselines to compare the performance
of the proposed hybrid algorithm.
6.1 Integrating JIGSAW into a supervised learning method
The design of the experiment is as follows: firstly, corpus-based WSD is applied to
words for which training examples are provided, then JIGSAW is applied to words
not covered by the first step, with the advantage of knowing the senses of the context
words already disambiguated in the first step. The performance of the hybrid method
was measured in terms of precision (P), recall (R), F-measure (F) and the percentage
A of disambiguation attempts, computed by counting the words for which a disam-
biguation attempt is made (the words with no training examples or sense definitions
cannot be disambiguated). Table 2 shows the baselines chosen to compare the hybrid
WSD algorithm on the All-Words-Task experiments.
Table 2: Baselines for Italian All-Words-Task
Setting P R F A
1stsense 58.45 48.58 53.06 83.11
Random 43.55 35.88 39.34 83.11
JIGSAW 55.14 45.83 50.05 83.11
K-NN 59.15 11.46 19.20 19.38
K-NN + 1stsense 57.53 47.81 52.22 83.11
The simplest baseline consists in assigning a random sense to each word (Random),
another common baseline inWord Sense Disambiguation is first sense (1stsense): each
word is tagged using the first sense in ITALWORDNET that is the most commonly
(frequent) used sense. The other baselines are the two methods combined in the hybrid
WSD, taken separately, namely JIGSAW and K-NN, and the basic hybrid algorithm
?K-NN + 1stsense?, which applies the supervised method, and then adopts the first
sense heuristic for the words without examples into training data. The K-NN baseline
achieves the highest precision, but the lowest recall due to the low coverage in the
training data (19.38%) makes this method useless for all practical purposes. Notice
that JIGSAW was the only participant to EVALITA WSD All-Words-Task, therefore
it currently represents the only available system performing WSD All-Words task for
the Italian language.
Table 3: Experimental results of K-NN+JIGSAW
Setting P R F A
K-NN + JIGSAW 56.62 47.05 51.39 83.11
K-NN + JIGSAW (? ? 0.90) 61.88 26.16 36.77 42.60
K-NN + JIGSAW (? ? 0.80) 61.40 32.21 42.25 52.06
K-NN + JIGSAW (? ? 0.70) 60.02 36.29 45.23 60.46
K-NN + JIGSAW (? ? 0.50) 59.58 37.38 45.93 62.74
Combining Knowledge-based Methods and Supervised Learning 13
Table 3 reports the results obtained by the hybrid method on the EVALITA dataset.
We study the behavior of the hybrid approach with relation to that of JIGSAW, since
this specific experiment aims at evaluating the potential improvements due to the in-
clusion of JIGSAW into K-NN. Different runs of the hybrid method have been per-
formed, each run corresponding to setting a specific value for ? (the confidence with
which a word wi is correctly disambiguated by JIGSAW). In each different run, the
disambiguation carried out by JIGSAW is considered reliable only when ? values ex-
ceed a certain threshold, otherwise any sense is assigned to the target word (this the
reason why A decreases by setting higher values for ?).
A positive effect on precision can be noticed by varying ? between 0.50 and 0.90. It
tends to grow and overcomes all the baselines, but a corresponding decrease of recall
is observed, as a consequence of more severe constraints set on ?. Anyway, recall is
still too low to be acceptable.
Better results are achieved when no restriction is set on ? (K-NN+JIGSAW in Ta-
ble 3): the recall is significantly higher than that obtained in the other runs. On the
other hand, the precision reached in this run is lower than in the others, but it is still
acceptable.
To sum up, two main conclusions can be drawn from the experiments:
? when no constraint is set on the knowledge-basedmethod, the hybrid algorithm
K-NN+JIGSAW in general outperforms both JIGSAW and K-NN taken singu-
larly (F values highlighted in bold in Tables 3 and 4);
? when thresholding is introduced on ?, no improvement is observed on the whole
compared to K-NN+JIGSAW.
A deep analysis of results revealed that lower recall was achieved for verbs and
adjectives rather than for nouns. Indeed, disambiguation of Italian verbs and adjec-
tives is very hard, but the lower recall is probability due also to the fact that JIGSAW
uses glosses for verbs and adjectives disambiguation. As a consequence, the perfor-
mance depends on the accuracy of word descriptions in the glosses, while for nouns
the algorithm relies only the semantic relations between synsets.
6.2 Integrating supervised learning into JIGSAW
In this experiment we test whether the supervised algorithm can help JIGSAW to dis-
ambiguate more accurately. The experiment has been organized as follows: JIGSAW
is applied to assign the most appropriate sense to the words which can be disam-
biguated with a high level of confidence (by setting the ? threshold), then the remain-
ing words are disambiguated by the K-NN classifier. The dataset and the baselines are
the same as in Section 6.1.
Note that, differently from the experiments described in Table 3, run JIGSAW+K-
NN has not been reported since JIGSAW covered all the target words in the first step
of the cascade hybrid method, then the K-NN method is not applied at all. Therefore,
for this run, results obtained by JIGSAW+K-NN correspond to those get by JIGSAW
alone (reported in Table 2).
Table 4 reports the results of all the runs. Results are very similar to those obtained
in the runs K-NN+JIGSAW with the same settings on ?. Precision tends to grow,
14 Basile, de Gemmis, Lops, and Semeraro
Table 4: Experimental results of JIGSAW+K-NN
Setting P R F A
JIGSAW (? ? 0.90) + K-NN 61.48 27.42 37.92 44.61
JIGSAW (? ? 0.80) + K-NN 61.17 32.59 42.52 53.28
JIGSAW (? ? 0.70) + K-NN 59.44 36.56 45.27 61.52
while a corresponding decrease in recall is observed. The main outcome is that the
overall accuracy of the best combination JIGSAW+K-NN (? ? 0.70, F value high-
lighted in bold in Table 4) is outperformed by K-NN+JIGSAW. Indeed, this result
was largely expected because the small size of the training set does not allow to cover
words not disambiguated by JIGSAW.
Even if K-NN+JIGSAW is not able to achieve the baselines set on the 1stsense
heuristic (first and last row in Table 2), we can conclude that a step toward these hard
baselines has been moved. The main outcome of the study is that the best hybrid
method on which further investigations are possible is K-NN+JIGSAW.
7 Conclusions and Future Work
This paper presented a method for solving the semantic ambiguity of all words con-
tained in a text. We proposed a hybrid WSD algorithm that combines a knowledge-
based WSD algorithm, called JIGSAW, which we designed to work by exploiting
WORDNET-like dictionaries as sense repository, with a supervised machine learning
algorithm (K-Nearest Neighbor classifier). The idea behind the proposed approach is
that JIGSAW can cope with the possible lack of training data, while K-NN can im-
prove the precision of JIGSAW method when training data are available. This makes
the proposed method suitable for disambiguation of languages for which the available
resources are lacking in training data or sense definitions, such as Italian.
Extensive experimental sessions were performed on the EVALITAWSDAll-Words-
Task dataset, the only dataset available for the evaluation of WSD systems for the
Italian language. An investigation was carried out in order to evaluate several com-
binations of JIGSAW and K-NN. The main outcome is that the most effective hybrid
WSD strategy is the one that runs JIGSAW after K-NN, which outperforms both JIG-
SAW and K-NN taken singularly. Future work includes new experiments with other
combination methods, for example the JIGSAW output could be used as feature into
supervised system or other different supervised methods could be exploited.
References
Agirre, E., B. Magnini, O. L. de Lacalle, A. Otegi, G. Rigau, and P. Vossen (2007).
SemEval-2007 Task 1: EvaluatingWSD on Cross-Language Information Retrieval.
In Proceedings of SemEval-2007. Association for Computational Linguistics.
Banerjee, S. and T. Pedersen (2002). An adapted lesk algorithm for word sense disam-
biguation using wordnet. In CICLing ?02: Proceedings of the Third International
Combining Knowledge-based Methods and Supervised Learning 15
Conference on Computational Linguistics and Intelligent Text Processing, London,
UK, pp. 136?145. Springer-Verlag.
Basile, P., M. de Gemmis, A. Gentile, P. Lops, and G. Semeraro (2007a). JIGSAW
algorithm for Word Sense Disambiguation. In SemEval-2007: 4th International
Workshop on Semantic Evaluations, pp. 398?401. ACL press.
Basile, P., M. de Gemmis, A. L. Gentile, P. Lops, and G. Semeraro (2007b). The JIG-
SAW Algorithm for Word Sense Disambiguation and Semantic Indexing of Doc-
uments. In R. Basili and M. T. Pazienza (Eds.), AI*IA, Volume 4733 of Lecture
Notes in Computer Science, pp. 314?325. Springer.
Decadt, B., V. Hoste, W. Daelemans, and A. V. den Bosch (2002). Gambl, Genetic
Algorithm optimization of Memory-based WSD. In Senseval-3: 3th International
Workshop on the Evaluation of Systems for the Semantic Analysis of Text.
Diab, M. (2004). Relieving the data acquisition bottleneck in word sense disambigua-
tion. In Proceedings of ACL. Barcelona, Spain.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Hoste, V., W. Daelemans, I. Hendrickx, and A. van den Bosch (2002). Evaluating the
results of a memory-based word-expert approach to unrestricted word sense dis-
ambiguation. In Proceedings of the ACL-02 workshop on Word sense disambigua-
tion: recent successes and future directions, Volume 8, pp. 95?101. Association for
Computational Linguistics Morristown, NJ, USA.
Leacock, C. and M. Chodorow (1998). Combining local context and WordNet simi-
larity for word sense identification, pp. 305?332. MIT Press.
Leacock, C., M. Chodorow, and G. Miller (1998). Using corpus statistics and Word-
Net relations for sense identification. Computational Linguistics 24(1), 147?165.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions,
and reversals. Soviet Physics Doklady 10(8), 707?710.
Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. In Proceedings of the
3rd International Conference on Language Resources and Evaluations.
Mihalcea, R. (2005). Unsupervised large-vocabulary word sense disambiguation with
graph-based algorithms for sequence data labeling. In HLT ?05: Proceedings of
the conference on Human Language Technology and Empirical Methods in Nat-
ural Language Processing, Morristown, NJ, USA, pp. 411?418. Association for
Computational Linguistics.
Mihalcea, R. (2007). Using Wikipedia for AutomaticWord Sense Disambiguation. In
Proceedings of the North American Chapter of the Association for Computational
Linguistics.
Mihalcea, R. and T. Chklovski (2003). Open Mind Word Expert: Creating Large
Annotated Data Collections with Web Users? Help. In Proceedings of the EACL
Workshop on Linguistically Annotated Corpora, Budapest.
16 Basile, de Gemmis, Lops, and Semeraro
Mitchell, T. (1997). Machine Learning. New York: McGraw-Hill.
Nancy, I. and J. V?ronis (1998). Introduction to the special issue on word sense
disambiguation: The state of the art. Computational Linguistics 24(1), 1?40.
Navigli, R. and P. Velardi (2005). Structural semantic interconnections: A knowledge-
based approach to word sense disambiguation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 27(7), 1075?1086.
Roventini, A., A. Alonge, F. Bertagna, N. Calzolari, J. Cancila, C. Girardi, B. Magnini,
R. Marinelli, M. Speranza, and A. Zampolli (2003). ItalWordNet: building a large
semantic database for the automatic treatment of Italian. Computational Linguis-
tics in Pisa - Linguistica Computazionale a Pisa. Linguistica Computazionale, Spe-
cial Issue XVIII-XIX, Tomo II, 745?791.
Semeraro, G., M. Degemmis, P. Lops, and P. Basile (2007). Combining learning and
word sense disambiguation for intelligent user profiling. In Proceedings of the
Twentieth International Joint Conference on Artificial Intelligence IJCAI-07, pp.
2856?2861. M. Kaufmann, San Francisco, California. ISBN: 978-I-57735-298-3.
Yuret, D. (2004). Some experiments with a naive bayes WSD system. In Senseval-3:
3th Internat. Workshop on the Evaluation of Systems for the Semantic Analysis of
Text.
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 43?51,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Encoding syntactic dependencies by vector permutation
Pierpaolo Basile
Dept. of Computer Science
University of Bari
Via Orabona, 4
I-70125, Bari (ITALY)
basilepp@di.uniba.it
Annalina Caputo
Dept. of Computer Science
University of Bari
Via Orabona, 4
I-70125, Bari (ITALY)
acaputo@di.uniba.it
Giovanni Semeraro
Dept. of Computer Science
University of Bari
Via Orabona, 4
I-70125, Bari (ITALY)
semeraro@di.uniba.it
Abstract
Distributional approaches are based on a sim-
ple hypothesis: the meaning of a word can be
inferred from its usage. The application of that
idea to the vector space model makes possi-
ble the construction of a WordSpace in which
words are represented by mathematical points
in a geometric space. Similar words are rep-
resented close in this space and the definition
of ?word usage? depends on the definition of
the context used to build the space, which can
be the whole document, the sentence in which
the word occurs, a fixed window of words,
or a specific syntactic context. However, in
its original formulation WordSpace can take
into account only one definition of context at
a time. We propose an approach based on
vector permutation and Random Indexing to
encode several syntactic contexts in a single
WordSpace. Moreover, we propose some op-
erations in this space and report the results
of an evaluation performed using the GEMS
2011 Shared Evaluation data.
1 Background and motivation
Distributional approaches usually rely on the
WordSpace model (Schu?tze, 1993). An overview
can be found in (Sahlgren, 2006). This model is
based on a vector space in which points are used to
represent semantic concepts, such as words.
The core idea behind WordSpace is that words
and concepts are represented by points in a math-
ematical space, and this representation is learned
from text in such a way that concepts with sim-
ilar or related meanings are near to one an-
other in that space (geometric metaphor of mean-
ing). The semantic similarity between concepts can
be represented as proximity in an n-dimensional
space. Therefore, the main feature of the geomet-
ric metaphor of meaning is not that meanings can
be represented as locations in a semantic space, but
rather that similarity between word meanings can be
expressed in spatial terms, as proximity in a high-
dimensional space.
One of the great virtues of WordSpaces is that
they make very few language-specific assumptions,
since just tokenized text is needed to build semantic
spaces. Even more important is their independency
of the quality (and the quantity) of available train-
ing material, since they can be built by exploiting an
entirely unsupervised distributional analysis of free
text. Indeed, the basis of the WordSpace model is
the distributional hypothesis (Harris, 1968), accord-
ing to which the meaning of a word is determined by
the set of textual contexts in which it appears. As a
consequence, in distributional models words can be
represented as vectors built over the observable con-
texts. This means that words are semantically related
as much as they are represented by similar vectors.
For example, if ?basketball? and ?tennis? occur fre-
quently in the same context, say after ?play?, they
are semantically related or similar according to the
distributional hypothesis.
Since co-occurrence is defined with respect to a
context, co-occurring words can be stored into ma-
trices whose rows represent the terms and columns
represent contexts. More specifically, each row cor-
responds to a vector representation of a word. The
strength of the semantic association between words
43
can be computed by using cosine similarity.
A weak point of distributional approaches is that
they are able to encode only one definition of con-
text at a time. The type of semantics represented in
WordSpace depends on the context. If we choose
documents as context we obtain a semantics differ-
ent from the one we would obtain by selecting sen-
tences as context. Several approaches have inves-
tigated the above mentioned problem: (Baroni and
Lenci, 2010) use a representation based on third-
order tensors and provide a general framework for
distributional semantics in which it is possible to
represent several aspects of meaning using a sin-
gle data structure. (Sahlgren et al, 2008) adopt
vector permutations as a means to encode order in
WordSpace, as described in Section 2. BEAGLE
(Jones and Mewhort, 2007) is a very well-known
method to encode word order and context informa-
tion in WordSpace. The drawback of the BEAGLE
model is that it relies on a complex model to build
vectors which is computational expensive. This
problem is solved by (De Vine and Bruza, 2010)
in which the authors propose an approach similar
to BEAGLE, but using a method based on Circu-
lar Holographic Reduced Representations to com-
pute vectors.
All these methods tackle the problem of repre-
senting word order in WordSpace, but they do not
take into account syntactic context. A valuable at-
tempt in this direction is described in (Pado? and La-
pata, 2007). In this work, the authors propose a
method to build WordSpace using information about
syntactic dependencies. In particular, they consider
syntactic dependencies as context and assign dif-
ferent weights to each kind of dependency. More-
over, they take into account the distance between
two words into the graph of dependencies. The re-
sults obtained by the authors support our hypothesis
that syntactic information can be useful to produce
effective WordSpace. Nonetheless, their methods
are not able to directly encode syntactic dependen-
cies into the space.
This work aims to provide a simple approach to
encode syntactic relations dependencies directly into
the WordSpace, dealing with both the scalability
problem and the possibility to encode several con-
text information. To achieve that goal, we devel-
oped a strategy based on Random Indexing and vec-
tor permutations. Moreover, this strategy opens new
possibilities in the area of semantic composition as
a result of the inherent capability of encoding rela-
tions between words.
The paper is structured as follows. Section 2
describes Random Indexing, the strategy for build-
ing our WordSpace, while details about the method
used to encode syntactic dependencies are reported
in Section 3. Section 4 describes the formal defi-
nition of some operations over the WordSpace and
shows a first attempt to define a model for semantic
composition. Finally, the results of the evaluation
performed using the GEMS 2011 Shared Evaluation
data1 is presented in Section 5, while conclusions
are reported in Section 6.
2 Random Indexing
We exploit Random Indexing (RI), introduced by
Kanerva (Kanerva, 1988), for creating a WordSpace.
This technique allows us to build a WordSpace with
no need for (either term-document or term-term)
matrix factorization, because vectors are inferred by
using an incremental strategy. Moreover, it allows
to solve efficiently the problem of reducing dimen-
sions, which is one of the key features used to un-
cover the ?latent semantic dimensions? of a word
distribution.
RI is based on the concept of Random Projection
according to which high dimensional vectors chosen
randomly are ?nearly orthogonal?.
Formally, given an n ?m matrix A and an m ?
k matrix R made up of k m-dimensional random
vectors, we define a new n? k matrix B as follows:
Bn,k = An,m?Rm,k k << m (1)
The new matrix B has the property to preserve
the distance between points. This property is known
as Johnson-Lindenstrauss lemma: if the distance be-
tween two any points of A is d, then the distance dr
between the corresponding points in B will satisfy
the property that dr = c ? d. A proof of that property
is reported in (Dasgupta and Gupta, 1999).
Specifically, RI creates a WordSpace in two steps
(in this case we consider the document as context):
1Available on line:
http://sites.google.com/site/geometricalmodels/shared-
evaluation
44
1. a context vector is assigned to each document.
This vector is sparse, high-dimensional and
ternary, which means that its elements can take
values in {-1, 0, 1}. A context vector contains a
small number of randomly distributed non-zero
elements, and the structure of this vector fol-
lows the hypothesis behind the concept of Ran-
dom Projection;
2. context vectors are accumulated by analyzing
terms and documents in which terms occur. In
particular, the semantic vector for a term is
computed as the sum of the context vectors for
the documents which contain that term. Con-
text vectors are multiplied by term occurrences.
Formally, given a collection of documents D
whose vocabulary of terms is V (we denote with
dim(D) and dim(V ) the dimension of D and V ,
respectively) the above steps can be formalized as
follows:
1. ?di ? D, i = 0, .., dim(D) we built the cor-
respondent randomly generated context vector
as:
??rj = (ri1, ..., rin) (2)
where n  dim(D), ri? ? {?1, 0, 1} and
??rj
contains only a small number of elements dif-
ferent from zero;
2. the WordSpace is made up of all term vectors
??
tj where:
??
tj = tfj
?
di?D
tj?di
??ri (3)
and tfj is the number of occurrences of tj in
di;
By considering a fixed window W of terms as
context, the WordSpace is built as follows:
1. a context vector is assigned to each term;
2. context vectors are accumulated by analyzing
terms in which terms co-occur in a window W .
In particular, the semantic vector for each term
is computed as the sum of the context vectors
for terms which co-occur in W .
It is important to point out that the classical RI
approach can handle only one context at a time, such
as the whole document or the window W .
A method to add information about context in RI
is proposed in (Sahlgren et al, 2008). The authors
describe a strategy to encode word order in RI by the
permutation of coordinates in random vector. When
the coordinates are shuffled using a random permu-
tation, the resulting vector is nearly orthogonal to the
original one. That operation corresponds to the gen-
eration of a new random vector. Moreover, by apply-
ing a predetermined mechanism to obtain random
permutations, such as elements rotation, it is always
possible to reconstruct the original vector using the
reverse permutations. By exploiting this strategy it is
possible to obtain different random vectors for each
context2 in which the term occurs. Let us consider
the following example ?The cat eats the mouse?. To
encode the word order for the word ?cat? using a
context window W = 3, we obtain:
< cat >= (??1the) + (?+1eat)+
+(?+2the) + (?+3mouse)
(4)
where ?nx indicates a rotation by n places of the
elements in the vector x. Indeed, the rotation is per-
formed by n right-shifting steps.
3 Encoding syntactic dependencies
Our idea is to encode syntactic dependencies, in-
stead of words order, in the WordSpace using vector
permutations.
A syntactic dependency between two words is de-
fined as:
dep(head, dependent) (5)
where dep is the syntactic link which connects
the dependent word to the head word. Gener-
ally speaking, dependent is the modifier, object or
complement, while head plays a key role in de-
termining the behavior of the link. For example,
subj(eat, cat) means that ?cat? is the subject of
?eat?. In that case the head word is ?eat?, which
plays the role of verb.
The key idea is to assign a permutation function
to each kind of syntactic dependencies. Formally,
2In the case in point the context corresponds to the word
order
45
let D be the set of all dependencies that we take into
account. The function f : D ? ? returns a schema
of vector permutation for each dep ? D. Then, the
method adopted to construct a semantic space that
takes into account both syntactic dependencies and
Random Indexing can be defined as follows:
1. a context vector is assigned to each term, as de-
scribed in Section 2 (Random Indexing);
2. context vectors are accumulated by analyzing
terms which are linked by a dependency. In
particular the semantic vector for each term ti
is computed as the sum of the permuted con-
text vectors for the terms tj which are depen-
dents of ti and the inverse-permuted vectors
for the terms tj which are heads of ti. The
permutation is computed according to f . If
f(d) = ?n the inverse-permutation is defined
as f?1(d) = ??n: the elements rotation is per-
formed by n left-shifting steps.
Adding permuted vectors to the head word and
inverse-permuted vectors to the corresponding de-
pendent word allows to encode the information
about both heads and dependents into the space.
This approach is similar to the one investigated by
(Cohen et al, 2010) to encode relations between
medical terms.
To clarify, we provide an example. Given the fol-
lowing definition of f :
f(subj) = ?+3 f(obj) = ?+7 (6)
and the sentence ?The cat eats the mouse?, we obtain
the following dependencies:
det(the, cat) subj(eat, cat)
obj(eat,mouse) det(the,mouse)
(7)
The semantic vector for each word is computed as:
? eat:
< eat >= (?+3cat) + (?+7mouse) (8)
? cat:
< cat >= (??3eat) (9)
? mouse:
< mouse >= (??7eat) (10)
In the above examples, the function f does not
consider the dependency det.
4 Query and vector operations
In this section, we propose two types of queries
that allow us to compute semantic similarity be-
tween two words exploiting syntactic dependencies
encoded in our space. Before defining query and
vector operations, we introduce a small set of nota-
tions:
? R denotes the original space of random vectors
generated during the WordSpace construction;
? S is the space of terms built using our strategy;
? rti ? R denotes the random vector of the term
ti;
? sti ? S denotes the semantic vector of the term
ti;
? sim(v1, v2) denotes the similarity between two
vectors; in our approach we adopt cosine simi-
larity;
? ?dep is the permutation returned from f(dep).
??dep is the inverse-permutation.
The first family of queries is dep(ti, ?). The idea
is to find all the dependents which are in relation
with the head ti, given the dependency dep. The
query can be computed as follows:
1. retrieve the vector sti from S;
2. for each rtj ? R compute the similarity be-
tween sti and < ?
deprtj >:
sim(sti , < ?
deprtj >);
3. rank in descending order all tj according to the
similarity computed in step 2.
The idea behind this operation is to compute how
each possible dependent tj contributes to the vector
ti, which is the sum of all the dependents related to
ti. It is important to note that we must first apply the
permutation to each rtj in order to take into account
the dependency relation (context). This operation
has a semantics different from performing the query
by applying first the inverse permutation to ti in R
and then computing the similarity with respect to all
the vectors tj in S. Indeed, the last approach would
46
compute how the head ti contributes to the vector tj ,
which differs from the goal of our query.
Using the same approach it is possible to compute
the query dep(?, tj), in which we want to search all
the heads related to the dependent tj fixed the de-
pendency dep. In detail:
1. retrieve the vector stj from S;
2. for each rti ? R compute the similarity be-
tween stj and the inverse-permutation of rti ,
< ??deprti >: sim(stj , < ?
?deprti >);
3. rank in descending order all ti according to the
similarity computed in step 2.
In this second query, we compute how the inverse-
permutation of each ti (head) affects the vector stj ?
S. In the following sub-section we provide some
initial idea about semantic composition.
4.1 Compositional semantics
Distributional approaches represent words in isola-
tion and they are typically used to compute similar-
ities between words. They are not able to represent
complex structures such as phrases or sentences. In
some applications, such as Question Answering and
Text Entailment, representing text by single words is
not enough. These applications would benefit from
the composition of words in more complex struc-
tures. The strength of our approach lies on the ca-
pability of codify syntactic relations between words
overcoming the ?word isolation? issue.
A lot of recent work argue that tensor product (?)
could be useful to combine word vectors. In (Wid-
dows, 2008) some preliminary investigations about
product and tensor product are provided, while an
interesting work by Clark and Pulman (Clark and
Pulman, 2007) proposes an approach to combine
symbolic and distributional models. The main idea
is to use tensor product to combine these two as-
pects, but the authors do not describe a method to
represent symbolic features, such as syntactic de-
pendencies. Conversely, our approach is able to en-
code syntactic information directly into the distri-
butional model. The authors in (Clark and Pulman,
2007) propose a strategy to represent a sentence like
?man reads magazine? by tensor product:
man? subj ? read? obj ?magazine (11)
They also propose a solid model for composition-
ality, but they do not provide a strategy to repre-
sent symbolic relations, such as subj and obj. They
wrote: ?How to obtain vectors for the dependency
relations - subj, obj, etc. - is an open question?. We
believe that our approach can tackle this problem by
encoding the dependency directly in the space, be-
cause each semantic vector in our space contains in-
formation about syntactic roles.
The representation based on tensor product is
useful to compute sentence similarity. Given the
previous sentence and the following one ?woman
browses newspaper?, we want to compute the sim-
ilarity between the two sentences. The sentence
?woman browses newspaper?, using the composi-
tional model, is represented by:
woman?subj?browse?obj?newspaper (12)
Computing the similarity of two representations
by inner product is a complex task, but exploiting
the following property of the tensor product:
(w1?w2) ?(w3?w4) = (w1 ?w3)?(w2 ?w4) (13)
the similarity between two sentences can be com-
puted by taking into account the pairs in each depen-
dency and multiplying the inner products as follows:
man ? woman? read ? browse?
?magazine ? newspaper
(14)
According to the property above mentioned, we
can compute the similarity between sentences with-
out using the tensor product. However, some open
questions arise. This simple compositional strategy
allows to compare sentences which have similar de-
pendency trees. For example, the sentence ?the dog
bit the man? cannot can be compared to ?the man
was bitten by the dog?. This problem can be easily
solved by identifying active and passive forms of a
verb. When two sentences have different trees, Clark
and Pulman (Clark and Pulman, 2007) propose to
adopt the convolution kernel (Haussler, 1999). This
strategy identifies all the possible ways of decom-
posing the two trees, and sums up the similarities be-
tween all the pairwise decompositions. It is impor-
tant to point out that, in a more recent work, Clark
47
et al (Clark et al, 2008) propose a model based
on (Clark and Pulman, 2007) combined with a com-
positional theory for grammatical types, known as
Lambek?s pregroup semantics, which is able to take
into account grammar structures. It is important to
note that this strategy is not able to encode gram-
matical roles into the WordSpace. This peculiarity
makes our approach completely different. In the fol-
lowing section we provide some examples of com-
positionality.
5 Evaluation
The goal of the evaluation is twofold: proving the
capability of our approach by means of some exam-
ples and providing results of the evaluation exploit-
ing the ?GEMS 2011 Shared Evaluation?, in particu-
lar the compositional semantics dataset. We propose
two semantic spaces built from two separate corpora
using our strategy. To achieve the first goal we pro-
vide several examples for each family of queries de-
scribed in Section 4. Concerning the second goal,
we evaluate our approach to compositional seman-
tics using the dataset proposed by Mitchell and Lap-
ata (Mitchell and Lapata, 2010), which is part of the
?GEMS 2011 Shared Evaluation?. The dataset is a
list of two pairs of adjective-noun combinations or
verb-object combinations or compound nouns. Hu-
mans rated pairs of combinations according to simi-
larity. The dataset contains 5,833 rates which range
from 1 to 7. Examples of pairs follow:
support offer help provide 7
old person right hand 1
where the similarity between offer-support and
provide-help (verb-object) is higher than the one be-
tween old-person and right-hand (adjective-noun).
As suggested by the authors, the goal of the eval-
uation is to compare the system performace against
humans scores by means of Spearman correlation.
5.1 System setup
The system is implemented in Java and relies on
some portions of code publicly available in the
Semantic Vectors package (Widdows and Ferraro,
2008). For the evaluation of the system, we build
two separate WordSpaces using the following cor-
pora: ukWaC (Baroni et al, 2009) and TASA.
ukWaC contains 2 billion words and it is constructed
from the Web by limiting the crawling to the .uk
domain and using medium-frequency words from
the BNC corpus as seeds. We use only a por-
tion of ukWaC corpus consisting of 7,025,587 sen-
tences (about 220,000 documents). The TASA cor-
pus (compiled by Touchstone Applied Science As-
sociates) was kindly made available to us by Prof.
Thomas Landauer from the University of Colorado.
The TASA corpus contains a collection of English
texts that is approximately equivalent to what the av-
erage college-level student has read in his/her life-
time. The TASA corpus consists of about 800,000
sentences.
To extract syntactic dependencies, we adopt
MINIPAR3 (Lin, 2003). MINIPAR is an efficient
English parser, which is suitable for parsing a large
amount of data. The total amount of extracted de-
pendencies is about 112,500,000 for ukWaC and
8,850,000 for TASA.
Our approach involves some parameters. We set
the random vector dimension to 4,000 and the num-
ber of non-zero elements in the random vector equal
to 10. We restrict the WordSpace to the 40,000 most
frequent words4. Another parameter is the set of de-
pendencies that we take into account. In this prelim-
inary investigation we consider the four dependen-
cies described in Table 1, that reports also the kind
of permutation5 applied to vectors.
5.2 Results
In this section we report some results of queries per-
formed in ukWaC and TASA corpus.
Table 2 and Table 3 report the results respectively
for the queries dep(ti, ?) and dep(?, tj). The effects
of encoding syntactic information is clearly visible,
as can be inferred by results in the tables. Moreover,
the results with the two corpora are different, as ex-
pected, but in many cases the first result of the query
is the same.
Our space can be also exploited to perform classi-
cal queries in which we want to find ?similar? words.
Tables 4 and 5 report results for TASA and ukWaC
3MINIPAR is available at
http://webdocs.cs.ualberta.ca/?lindek/minipar.htm
4Word frequency is computed taking into account the se-
lected dependencies.
5The number of rotations is randomly chosen.
48
Dependency Description Permutation
obj object of verbs ?+7
subj subject of verbs ?+3
mod the relationship between a word and its adjunct modifier ?+11
comp complement ?+23
Table 1: The set of dependencies used in the evaluation.
corpus, respectively. The results obtained by similar
test are not the typical results expected by classical
WordSpace. In fact, in Table 5 the word most simi-
lar to ?good? is ?bad?, because they are used in the
same syntactic context, but have opposite meaning.
The similarity between words in our space strongly
depends on their syntactic role. For example, the
words similar to ?food? are all the nouns which are
object/subject of the same verbs in syntactic relation
with ?food?.
Finally, we provide the results of semantic com-
position. Table 6 reports the Spearman correlation
between the output of our system and the mean
similarity scores given by the humans. The table
shows results for each types of combination: verb-
object, adjective-noun and compound nouns. To per-
form the experiment on compound nouns, we re-
build the spaces encoding the ?nn? relation provided
by MINIPAR which refers to compound nouns de-
pendency. Table 6 shows the best result obtained
by Mitchell and Lapata (Mitchell and Lapata, 2008)
using the same dataset. Our method is able to out-
perform MLbest and obtains very high results when
adjective-noun combination is involved.
Corpus Combination ?
TASA
verb-object 0.260
adjective-noun 0.637
compound nouns 0.341
overall 0.275
ukWaC
verb-object 0.292
adjective-noun 0.445
compound nouns 0.227
overall 0.261
- MLbest 0.190
Table 6: GEMS 2011 Shared Evaluation results.
The experiments reported in this preliminary eval-
uation are only a small fraction of the experiments
that are required to make a proper evaluation of the
effectiveness of our semantic space and to compare
it with other approaches. This will be the main fo-
cus of our future research. The obtained results seem
to be encouraging and the strength of our approach,
capturing syntactic relations, allows to implement
several kind of queries using only one WordSpace.
We believe that the real advantage of our approach,
that is the possibility to represent several syntactic
relations, has much room for exploration.
6 Conclusions
In this work, we propose an approach to encode syn-
tactic dependencies in WordSpace using vector per-
mutations and Random Indexing. In that space, a set
of operations is defined, which relies on the possibil-
ity of exploiting syntactic dependencies to perform
some particular queries, such as the one for retriev-
ing all similar objects of a verb. We propose an early
attempt to use that space for semantic composition
of short sentences. The evaluation using the GEMS
2011 shared dataset provides encouraging results,
but we believe that there are open points which de-
serve more investigation. We planned a deeper eval-
uation of our WordSpace and a more formal study
about semantic composition.
Acknowledgements
This research was partially funded by MIUR (Min-
istero dell?Universita` e della Ricerca) under the
contract Fondo per le Agevolazioni alla Ricerca,
DM19410 ?Laboratorio? di Bioinformatica per la
Biodiversita` Molecolare (2007-2011).
49
obj(provide, ?) mod(people, ?)
TASA ukWaC TASA ukWaC
information 0.344 information 0.351 young 0.288 young 0.736
food 0.208 service 0.260 black 0.118 with 0.360
support 0.143 you 0.176 old 0.089 other 0.223
energy 0.143 opportunity 0.141 conquered 0.086 handling 0.164
job 0.142 support 0.127 deaf 0.086 impressive 0.162
Table 2: Examples of query dep(ti, ?).
obj(?, food) mod(?, good)
TASA ukWaC TASA ukWaC
eat 0.604 eat 0.429 idea 0.350 practice 0.510
make 0.389 serve 0.256 place 0.320 idea 0.363
grow 0.311 provide 0.230 way 0.269 news 0.274
need 0.272 have 0.177 friend 0.246 for 0.269
store 0.161 buy 0.169 time 0.234 very 0.228
Table 3: Examples of query dep(?, tj).
food provide good
food 1.000 provide 1.000 good 0.999
foods 0.698 make 0.702 best 0.498
meat 0.654 restructure 0.693 excellent 0.471
meal 0.651 ready 0.680 wrong 0.453
bread 0.606 leave 0.673 main 0.430
wheato 0.604 mean 0.672 nice 0.428
thirty percent 0.604 work 0.672 safe 0.428
mezas 0.604 offer 0.671 new 0.428
orgy 0.604 relate 0.667 proper 0.400
chocolatebar 0.604 gather 0.667 surrounded 0.400
Table 4: Find similar words, TASA corpus.
food provide good
food 1.000 provide 0.999 good 1.000
meal 0.724 offer 0.855 bad 0.603
meat 0.656 supply 0.819 best 0.545
pie 0.578 deliver 0.801 anti-discriminatory 0.507
tea 0.576 give 0.787 nice 0.478
fresh food 0.576 contain 0.786 reflective 0.470
supper 0.556 require 0.784 brilliant 0.464
porridge 0.553 present 0.782 great 0.462
entertainment 0.533 gather 0.778 evidence-based 0.453
soup 0.532 work 0.777 unsafe 0.444
Table 5: Find similar words, ukWaC corpus.
50
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A collection of very
large linguistically processed Web-crawled corpora.
Language Resources and Evaluation, 43(3):209?226.
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the AAAI Spring Symposium on Quantum Interaction,
pages 52?55.
S. Clark, B. Coecke, and M. Sadrzadeh. 2008. A com-
positional distributional model of meaning. In Pro-
ceedings of the Second Quantum Interaction Sympo-
sium (QI-2008), pages 133?140.
T. Cohen, D. Widdows, R.W. Schvaneveldt, and T.C.
Rindflesch. 2010. Logical leaps and quantum con-
nectives: Forging paths through predication space. In
AAAI-Fall 2010 Symposium on Quantum Informatics
for Cognitive, Social, and Semantic Processes, pages
11?13.
S. Dasgupta and A. Gupta. 1999. An elementary proof of
the Johnson-Lindenstrauss lemma. Technical report,
Technical Report TR-99-006, International Computer
Science Institute, Berkeley, California, USA.
L. De Vine and P. Bruza. 2010. Semantic Oscillations:
Encoding Context and Structure in Complex Valued
Holographic Vectors. Quantum Informatics for Cog-
nitive, Social, and Semantic Processes (QI 2010).
Z. Harris. 1968. Mathematical Structures of Language.
New York: Interscience.
D. Haussler. 1999. Convolution kernels on discrete
structures. Technical Report UCSC-CRL-99-10.
M.N. Jones and D.J.K. Mewhort. 2007. Representing
word meaning and order information in a composite
holographic lexicon. Psychological review, 114(1):1?
37.
P. Kanerva. 1988. Sparse Distributed Memory. MIT
Press.
D. Lin. 2003. Dependency-based evaluation of MINI-
PAR. Treebanks: building and using parsed corpora.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
J. Mitchell and M. Lapata. 2010. Composition in distri-
butional models of semantics. Cognitive Science. To
appear.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cogni-
tive Science Society (CogSci?08).
M. Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm:
Stockholm University, Faculty of Humanities, Depart-
ment of Linguistics.
H. Schu?tze. 1993. Word space. In Stephen Jose? Hanson,
Jack D. Cowan, and C. Lee Giles, editors, Advances in
Neural Information Processing Systems, pages 895?
902. Morgan Kaufmann Publishers.
D. Widdows and K. Ferraro. 2008. Semantic Vectors: A
Scalable Open Source Package and Online Technology
Management Application. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC 2008).
D. Widdows. 2008. Semantic vector products: Some
initial investigations. In The Second AAAI Symposium
on Quantum Interaction.
51
