
c? 2002 Association for Computational Linguistics
Efficiently Computed Lexical Chains as an
Intermediate Representation for
Automatic Text Summarization
H. Gregory Silber? Kathleen F. McCoy?
University of Delaware University of Delaware
While automatic text summarization is an area that has received a great deal of attention in recent
research, the problem of efficiency in this task has not been frequently addressed. When the size
and quantity of documents available on the Internet and from other sources are considered, the
need for a highly efficient tool that produces usable summaries is clear. We present a linear-time
algorithm for lexical chain computation. The algorithm makes lexical chains a computationally
feasible candidate as an intermediate representation for automatic text summarization. A method
for evaluating lexical chains as an intermediate step in summarization is also presented and carried
out. Such an evaluation was heretofore not possible because of the computational complexity of
previous lexical chains algorithms.
1. Introduction
The overall motivation for the research presented in this article is the development of
a computationally efficient system to create summaries automatically. Summarization
has been viewed as a two-step process. The first step is the extraction of important
concepts from the source text by building an intermediate representation of some sort.
The second step uses this intermediate representation to generate a summary (Sparck
Jones 1993).
In the research presented here, we concentrate on the first step of the summariza-
tion process and follow Barzilay and Elhadad (1997) in employing lexical chains to
extract important concepts from a document. We present a linear-time algorithm for
lexical chain computation and offer an evaluation that indicates that such chains are
a promising avenue of study as an intermediate representation in the summarization
process.
Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in
the text summarization process. Attempts to determine the benefit of this proposal
have been faced with a number of difficulties. First, previous methods for computing
lexical chains have either been manual (Morris and Hirst 1991) or automated, but
with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997).
Because of this, computing lexical chains for documents of any reasonable size has
been impossible. We present here an algorithm for computing lexical chains that is
linear in space and time. This algorithm makes the computation of lexical chains
computationally feasible even for large documents.
? Department of Computer and Information Sciences, Newark, DE 19711. E-mail: silber@udel.edu
? Department of Computer and Information Sciences, Newark, DE 19711. E-mail: mccoy@mail.eecis.
udel.edu
488
Computational Linguistics Volume 28, Number 4
A second difficulty faced in evaluating Barzilay and Elhadad?s proposal is that it
is a proposal for the first stage of the summarization process, and it is not clear how
to evaluate this stage independent of the second stage of summarization. A second
contribution of this article is a method for evaluating lexical chains as an intermedi-
ate representation. The intuition behind the method is as follows. The (strong) lexical
chains in a document are intended to identify important (noun) concepts in the docu-
ment. Our evaluation requires access to documents that have corresponding human-
generated summaries. We run our lexical chain algorithm both on the document and
on the summary and examine (1) how many of the concepts from strong lexical chains
in the document also occur in the summary and (2) how many of the (noun) concepts
appearing in the summary are represented in strong lexical chains in the document.
Essentially, if lexical chains are a good intermediate representation for text sum-
marization, we expect that concepts identified as important according to the lexical
chains will be the concepts found in the summary. Our evaluation of 24 documents
with summaries indicates that indeed lexical chains do appear to be a promising av-
enue of future research in text summarization.
1.1 Description of Lexical Chains
The concept of lexical chains was first introduced by Morris and Hirst. Basically, lex-
ical chains exploit the cohesion among an arbitrary number of related words (Morris
and Hirst 1991). Lexical chains can be computed in a source document by grouping
(chaining) sets of words that are semantically related (i.e., have a sense flow). Iden-
tities, synonyms, and hypernyms/hyponyms (which together define a tree of ?is a?
relations between words) are the relations among words that might cause them to be
grouped into the same lexical chain. Specifically, words may be grouped when:
? Two noun instances are identical, and are used in the same sense.
(The house on the hill is large. The house is made of wood.)
? Two noun instances are used in the same sense (i.e., are synonyms).
(The car is fast. My automobile is faster.)
? The senses of two noun instances have a hypernym/hyponym relation
between them. (John owns a car. It is a Toyota.)
? The senses of two noun instances are siblings in the hypernym/hyponyn
tree. (The truck is fast. The car is faster.)
In computing lexical chains, the noun instances must be grouped according to the
above relations, but each noun instance must belong to exactly one lexical chain. There
are several difficulties in determining which lexical chain a particular word instance
should join. For instance, a particular noun instance may correspond to several differ-
ent word senses, and thus the system must determine which sense to use (e.g., should
a particular instance of ?house? be interpreted as sense 1, dwelling, or sense 2, legisla-
ture). In addition, even if the word sense of an instance can be determined, it may be
possible to group that instance into several different lexical chains because it may be
related to words in different chains. For example, the word?s sense may be identical
to that of a word instance in one grouping while having a hypernym/hyponym rela-
tionship with that of a word instance in another. What must happen is that the words
must be grouped in such a way that the overall grouping is optimal in that it creates
the longest/strongest lexical chains. It is our contention that words are grouped into
a single chain when they are ?about? the same underlying concept.
489
Silber and McCoy Efficient Lexical Chains for Summarization
2. Algorithm Definition
We wish to extract lexical chains from a source document using the complete method
that Barzilay and Elhadad implemented in exponential time, but to do so in linear
time. Barzilay and Elhadad define an interpretation as a mapping of noun instances
to specific senses, and further, of these senses to specific lexical chains. Each unique
mapping is a particular ?way of interpreting? the document, and the collection of
all possible mappings defines all of the interpretations possible. In order to compute
lexical chains in linear time, instead of computing every interpretation of a source
document as Barzilay and Elhadad did, we create a structure that implicitly stores
every interpretation without actually creating it, thus keeping both the space and time
usage of the program linear. We then provide a method for finding that interpretation
which is best from within this representation. As was the case with Barzilay and
Elhadad, we rely on WordNet1 to provide sense possibilities for, and semantic relations
among, word instances in the document.
Before we could actually compute the interpretations, one issue we had to tackle
was the organization and speed of the WordNet dictionary. In order to provide expe-
dient access to WordNet, we recompiled the noun database into a binary format and
memory-mapped it so that it could be accessed as a large array, changing the WordNet
sense numbers to match the array indexes.
2.1 Chain Computation
Before computation can begin, the system uses a part-of-speech tagger2 to find the
noun instances within the document. Processing a document involves creating a large
array of ?metachains,? the size of which is the number of noun senses in WordNet
plus the number of nouns in the document, to handle the possibility of words not
found in WordNet. (This is the maximum size that could possibly be needed.) A
metachain represents all possible chains that can contain the sense whose number is
the index of the array. When a noun is encountered, for every sense of the noun in
WordNet, the noun sense is placed into every metachain for which it has an iden-
tity, synonym, or hyperonym relation with that sense. These metachains represent
every possible interpretation of the text. Note that each metachain has an index that
is a WordNet sense number, so in a very real way, we can say that a chain has an
?overriding sense.? Table 1 shows the structure of such an array, with each row being
a metachain based on the sense listed in the first column. In each node of a given
metachain, appropriate pointers are kept to allow fast lookup. In addition, in associa-
tion with each word, a list of the metachains to which it belongs is also kept (not shown
in table).
The second step, finding the best interpretation, is accomplished by making a
second pass through the source document. For each noun, each chain to which the
noun belongs is examined and a determination is made, based on the type of relation
and distance factors, as to which metachain the noun contributes to most. In the
event of a tie, the higher sense number is used, since WordNet is organized with more
specific concepts indexed with higher numbers. The noun is then deleted from all other
metachains. Once all of the nouns have been processed, what is left is the interpretation
whose score is maximum. From this interpretation, the best (highest-scoring) chains
can be selected. The algorithm in its entirety is outlined in Table 2.
1 WordNet is available at ?http://www.cogsci.princeton.edu/?wn?.
2 The part-of-speech tagger we used is available from ?http://www.rt66.com/gcooke/SemanTag?.
490
Computational Linguistics Volume 28, Number 4
Table 1
Example of metachains.
Index Meaning Chain
0 person John Machine
1 unit Computer IBM
2 device Computer Machine IBM
3 organization Machine IBM
4 unknown IBM
...
N
Note: Assume the sentences ?John has a computer.
The machine is an IBM.? and that the nouns have the
following senses (meanings): John (0), computer (1,2),
machine (0,2,3), IBM (1,2,3,4), and that words are put
in a chain if they have an identity relation. This table
then depicts the metachains after the first step.
Table 2
Basic linear-time lexical chains algorithm.
Step 1 For each noun instance
For each sense of the noun instance
Compute all scored metachains
Step 2 For each noun instance
For each metachain to which the noun belongs
Keep word instance in the metachain to which it contributes most
Update the scores of each other metachain
2.2 Scoring System
Our scoring system allows different types of relations within a lexical chain to con-
tribute to that chain differently. Further, our scoring system allows the distance be-
tween word instances in the original document to affect the word?s contribution to
a chain. Table 3 shows the scoring values used by our algorithm. These values were
obtained through empirical testing, and although not optimal, appear to give good
results.
Each noun instance is included in a chain because either it is the first noun instance
to be inserted or it is related to some word that is already in the chain. If it is the first
word, then the ?identical word? relation score is used. If not, then the type of relation is
determined, and the closest noun in the chain to which it is related is found. Using the
distance between these two words and the relation type, we look up the contribution
of the word instance to the overall chain score.
Once chains are computed, some of the high-scoring ones must be picked as
representing the important concepts from the original document. To select these, we
use the idea of ?strong chains? introduced by Barzilay and Elhadad (1997). They define
a strong chain as any chain whose score is more than two standard deviations above
the mean of the scores computed for every chain in the document.
491
Silber and McCoy Efficient Lexical Chains for Summarization
Table 3
Scoring system tuned by empirical methods.
One Sentence Three Sentences Same Paragraph Default
Identical word 1 1 1 1
Synonym 1 1 1 1
Hypernym 1 .5 .5 .5
Sibling 1 .3 .2 0
Table 4
Constants from WordNet 1.6.
Value Worst Case Average Case
C1 = # of senses for a given word 30 2
C2 = parent/child ?is a? relations of a word sense 45,147 14
C3 = # of nouns in WordNet 94,474 94,474
C4 = # of synsets in WordNet 66,025 66,025
C5 = # of siblings of a word sense 397 39
C6 = # of chains to which a word instance can belong 45,474 55
3. Linear Runtime Proof
In this analysis, we will not consider the computational complexity of part-of-speech
tagging, since it is quite fast. The runtime of the full algorithm will be O(pos tagging)
+ O(our algorithm). Also, as it does not change from execution to execution of the
algorithm, we shall take the size and structure of WordNet to be constant. We will
examine each phase of our algorithm to show that the extraction of these lexical chains
can indeed be performed in linear time. Table 4 defines constants for this analysis.
3.1 Collection of WordNet Information
For each noun in the source document that appears in WordNet, each sense that the
word can take must be examined. Additionally, for each sense, we must walk up and
down the hypernym/hyponym graph collecting all parent and child information. It
is important to note that we are interested not only in direct parents and children,
but in all parents and children in the graph from most specific to most general. Lastly
we must collect all of the senses in WordNet that are siblings (i.e., share immediate
parents) with the word being processed. All of the complexity in this step is related
to the size of WordNet, which is constant. Lookups in WordNet use a binary search;
hence a search in WordNet is O(log(C3)). The runtime is given by
n ? (log2(C3) + C1 ? C2 + C1 ? C5).
3.2 Building the Graph
The graph of all possible interpretations is nothing more than an array of sense values
(66, 025 + n in size) that we will call the sense array. For each word, we examine each
relation computed as above from WordNet. For each of these relations, we modify
the list that is indexed in the sense array by the sense number of the noun?s sense
involved in the relation. This list is then modified by adding the word to the list and
updating the list?s associated score. Additionally, we add the chain?s pointer (stored
in the array) to a list of such pointers in the word object. Lastly, we add the value of
492
Computational Linguistics Volume 28, Number 4
how this word affects the score of the chain based on the scoring system to an array
stored within the word structure. The runtime for this phase of the algorithm is
n ? C6 ? 4,
which is also clearly O(n).
3.3 Extracting the Best Interpretation
For each word in the source document, we look at each chain to which the word can
belong. A list of pointers to these chains is stored within the word object, so looking
them up takes O(1) time. For each of these, we simply look at the maximal score
component value in all of these chains. We then set the scores of all of the nodes that
did not contain the maximum to zero and update all the chain scores appropriately.
The operation takes
n ? C6 ? 4,
which is also O(n).
3.4 Overall Runtime Performance
The overall runtime performance of this algorithm is given by the sum of the steps
listed above, for an overall runtime of
n ? (1, 548, 216 + log2(94, 474) + 45, 474 ? 4) [worst case]
n ? (326 + log2(94, 474) + 55 ? 4) [average case].
Initially, we may be greatly concerned with the size of these constants; upon further
analysis, however, we see that most synsets have very few parent-child relations.
Thus the worst-case values may not reflect the actual performance of our application.
In addition, the synsets with many parent-child relations tend to represent extremely
general concepts such as ?thing? and ?object.? These synsets will most likely not
appear very often in a document.
Whereas in the worst case these constants are quite large, in the average case
they are reasonable. This algorithm is O(n) in the number of nouns within the source
document. Considering the size of most documents, the linear nature of this algorithm
makes it usable for generalized summarization of large documents (Silber and McCoy
2000). For example, in a test, our algorithm calculated a lexical chain interpretation of a
40,000-word document in 11 seconds on a Sparc Ultra 10 Creator. It was impossible to
compute lexical chains for such a document under previous implementations because
of computational complexity. Thus documents tested by Barzilay and Elhadad were
significantly smaller in size. Our method affords a considerable speedup for these
smaller documents. For instance, a document that takes 300 seconds using Barzilay
and Elhadad?s method takes only 4 seconds using ours (Silber and McCoy 2000).
4. Evaluation Design
Our algorithm now makes it feasible to use lexical chains as the method for identifying
important concepts in a document, and thus they may now form the basis of an
intermediate representation for summary generation, as proposed by Barzilay and
Elhadad. An important consequence of this is that Barzilay and Elhadad?s proposal
can now be evaluated on documents of substantial size. We propose an evaluation of
this intermediate stage that is independent of the generation phase of summarization.
493
Silber and McCoy Efficient Lexical Chains for Summarization
This said, we make no attempt to claim that a summary can actually be generated
from this representation; we do attempt, however, to show that the concepts found in
a human-generated summary are indeed the concepts identified by our lexical chains
algorithm.
The basis of our evaluation is the premise that if lexical chains are a good in-
termediate representation for summary generation, then we would expect that each
noun in a given summary should be used in the same sense as some word instance
grouped into a strong chain in the original document on which the summary is based.
Moreover, we would expect that all (most) strong chains in the document should be
represented in the summary.
For this analysis, a corpus of documents with their human-generated summaries
are required. Although there are many examples of document and summary types,
for the purposes of this experiment, we focus on two general categories of summaries
that are readily available. The first, scientific documents with abstracts, represents a
readily available class of summaries often discussed in the literature (Marcu 1999).
The second class of document selected was chapters from university level textbooks
that contain chapter summaries. To prevent bias, textbooks from several fields were
chosen.
In this analysis, we use the term concept to denote a noun in a particular sense (a
given sense number in the WordNet database). It is important to note that different
nouns with the same sense number3 are considered to be the same concept. It is also
important to note that for the purposes of this analysis, when we refer to the ?sense? of
a word, we mean the sense as determined by our lexical chain analysis. The basic idea
of our experiment is to try to determine whether the concepts represented by (strong)
lexical chains in an original document appear in the summary of that document and
whether the concepts appearing in the summary (as determined by the lexical chain
analysis of the summary) come from strong chains in the document. If both of these
give 100% coverage, this would mean that all and only the concepts identified by
strong lexical chains in the document occur in the summary. Thus the higher these
numbers turn out to be, the more likely it is that lexical chains are a good intermediate
representation of the text summarization task.
A corpus was compiled containing the two specific types of documents, ranging
in length from 2,247 to 26,320 words each. These documents were selected at random,
with no screening by the authors. The scientific corpus consisted of 10 scientific articles
(5 computer science, 3 anthropology, and 2 biology) along with their abstracts. The
textbook corpus consisted of 14 chapters from 10 university level textbooks in various
subjects (4 computer science, 6 anthropology, 2 history, and 2 economics), including
chapter summaries.
For each document in the corpus, the document and its summary were analyzed
separately to produce lexical chains. In both cases we output the sense numbers spec-
ified for each word instance as well as the overriding sense number for each chain.
By comparing the sense numbers of (words in) each chain in the document with the
computed sense of each noun instance in the summary, we can determine whether the
summary indeed contains the same ?concepts? as indicated by the lexical chains. For
the analysis, the specific metrics we are interested in are
? The number and percentage of strong chains from the original text that
are represented in the summary. Here we say a chain is represented if a
3 Recall that synonyms in the WordNet database are identified by a synset (sense) number.
494
Computational Linguistics Volume 28, Number 4
word occurs in the summary in the same sense as in the document
strong chain. (Analogous to recall)
? The number and percentage of noun instances in the summary that
represent strong chains in the document. (Analogous to precision)
By analyzing these two metrics, we can determine how well lexical chains represent
the information that appears in these types of human-generated summaries. We will
loosely use the terms recall and precision to describe these two metrics.
4.1 Experimental Results
Each document in the corpus was analyzed by running our lexical chain algorithm and
collecting the overriding sense number of each strong lexical chain computed. Each
summary in the corpus was analyzed by our algorithm, and the disambiguated sense
(i.e., the sense of the noun instance that was selected in order to insert it into a chain)
of each noun was collected. Table 5 shows the results of this analysis. The number of
strong chains computed for the document is shown in column 2. Column 3 shows the
Table 5
Evaluation results.
Total Total Strong Chains Noun Instances
Number Number with with
of Strong of Noun Corresponding Corresponding
Chains in Instances Noun Instances Strong Chains in
Document Document in Summary in Summary Document
CS Paper 1 10 22 7 (70.00%) 19 (86.36%)
CS Paper 2 7 19 6 (71.43%) 17 (89.47%)
CS Paper 3 5 31 4 (80.00%) 27 (87.19%)
CS Paper 4 6 25 5 (83.33%) 24 (96.00%)
CS Paper 5 8 16 6 (75.00%) 12 (75.00%)
ANTH Paper 1 7 20 7 (100.00%) 17 (85.00%)
ANTH Paper 2 5 17 4 (80.00%) 13 (76.47%)
ANTH Paper 3 7 21 6 (28.57%) 7 (33.33%)
BIO Paper 1 4 19 4 (100.00%) 17 (89.47%)
BIO Paper 2 5 31 5 (80.00%) 28 (90.32%)
CS Chapter 1 9 55 8 (88.89%) 49 (89.09%)
CS Chapter 2 7 49 6 (85.71%) 42 (85.71%)
CS Chapter 3 11 31 9 (81.82%) 25 (80.65%)
CS Chapter 4 14 47 5 (35.71%) 21 (44.68%)
ANTH Chapter 1 5 61 4 (80.00%) 47 (77.05%)
ANTH Chapter 2 8 74 7 (87.50%) 59 (79.73%)
ANTH Chapter 3 12 58 11 (91.67%) 48 (82.76%)
ANTH Chapter 4 13 49 11 (84.62%) 42 (85.71%)
ANTH Chapter 5 7 68 5 (71.43%) 60 (88.24%)
ANTH Chapter 6 9 59 8 (88.89%) 48 (81.36%)
HIST Chapter 1 12 71 10 (83.33%) 67 (94.37%)
HIST Chapter 2 8 65 7 (87.50%) 55 (84.62%)
ECON Chapter 1 14 68 12 (85.71%) 63 (92.65%)
ECON Chapter 2 9 51 7 (77.78%) 33 (64.71%)
Mean 79.12% 80.83%
Median 82.58% 85.35%
Note: ANTH?anthropology, BIO?biology, CS?computer science, ECON?economics,
HIST?history.
495
Silber and McCoy Efficient Lexical Chains for Summarization
total number of noun instances found in the summary. Column 4 shows the number,
and percentage overall, of strong chains from the document that are represented by
noun instances in the summary (recall). The number, and the percentage overall, of
nouns of a given sense from the summary that have a corresponding strong chain
with the same overriding sense number (representing the chain) in the original text
are presented in column 5 (precision). Summary statistics are also presented.
In 79.12% of the cases, lexical chains appropriately represent the nouns in the sum-
mary. In 80.83% of the cases, nouns in the summary would have been predicted by the
lexical chains. The algorithm performs badly on two documents, anthropology paper
3 and computer science chapter 4, under this analysis. Possible reasons for this will
be discussed below, but our preliminary analysis of these documents leads us to be-
lieve that they contain a greater number of pronouns and other anaphoric expressions
(which need to be resolved to compute lexical chains properly). These potential rea-
sons need to be examined further to determine why our algorithm performs so poorly
on these documents. Excluding these two documents, our algorithm has a recall of
83.39% and a precision of 84.63% on average. It is important to note that strong chains
represent only between 5% and 15% of the total chains computed for any document.
The evaluation presented here would be enhanced by having a baseline for com-
parison. It is not clear, however, what this baseline should be. One possibility would
be to use straight frequency counts as an indicator and use these frequency counts for
comparison.
5. Discussion and Future Work
Some problems that cause our algorithm to have difficulty, specifically proper nouns
and anaphora resolution, need to be addressed. Proper nouns (people, organization,
company, etc.) are often used in naturally occurring text, but since we have no in-
formation about them, we can only perform frequency counts on them. Anaphora
resolution, especially in certain domains, is a bigger issue. Much better results are
anticipated with the addition of anaphora resolution to the system.
Other issues that may affect the results we obtained stem from WordNet?s coverage
and the semantic information it captures. Clearly, no semantically annotated lexicon
can be complete. Proper nouns and domain-specific terms, as well as a number of
other words likely to be in a document, are not found in the WordNet database. The
system defaults to word frequency counts for terms not found. Semantic distance in
the ?is a? graph, a problem in WordNet, does not affect our implementation, since
we don?t use this information. It is important to note that although our system uses
WordNet, there is nothing specific to the algorithm about WordNet per se, and any
other appropriate lexicon could be ?plugged in? and used.
Issues regarding generation of a summary based on lexical chains need to be ad-
dressed and are the subject of our current work. Recent research has begun to look at
the difficult problem of generating a summary text from an intermediate representa-
tion. Hybrid approaches such as extracting phrases instead of sentences and recom-
bining these phrases into salient text have been proposed (Barzilay, McKeown, and
Elhadad 1999). Other recent work looks at summarization as a process of revision; in
this work, the source text is revised until a summary of the desired length is achieved
(Mani, Gates, and Bloedorn 1999). Additionally, some research has explored cutting
and pasting segments of text from the full document to generate a summary (Jing and
McKeown 2000). It is our intention to use lexical chains as part of the input to a more
classical text generation algorithm to produce new text that captures the concepts from
the extracted chains. The lexical chains identify noun (or argument) concepts for the
496
Computational Linguistics Volume 28, Number 4
summary. We are examining ways for predicates to be identified and are concentrating
on situations in which strong lexical chains intersect in the text.
6. Conclusions
In this article, we have outlined an efficient, linear-time algorithm for computing lexical
chains as an intermediate representation for automatic machine text summarization.
This algorithm is robust in that it uses the method proposed by Barzilay and Elhadad,
but it is clearly O(n) in the number of nouns in the source document.
The benefit of this linear-time algorithm is its ability to compute lexical chains
in documents significantly larger than could be handled by Barzilay and Elhadad?s
implementation. Thus, our algorithm makes lexical chains a computationally feasi-
ble intermediate representation for summarization. In addition, we have presented a
method for evaluating lexical chains as an intermediate representation and have eval-
uated the method using 24 documents that contain human-generated summaries. The
results of these evaluations are promising.
An operational sample of our algorithm is available on the Web; a search engine
that uses our algorithm can be accessed there as well (available at ?http://www.eecis.
udel.edu/?silber/research.htm?).
Acknowledgments
The authors wish to thank the Korean
Government, Ministry of Science and
Technology, whose funding, as part of the
Bilingual Internet Search Machine Project,
has made this research possible.
Additionally, special thanks to Michael
Elhadad and Regina Barzilay for their
advice and for generously making their data
and results available, and to the anonymous
reviewers for their helpful comments.
References
Barzilay, Regina and Michael Elhadad. 1997.
Using lexical chains for text
summarization. In Proceedings of the
Intelligent Scalable Text Summarization
Workshop (ISTS-97), Madrid, Spain.
Barzilay, Regina, Kathleen R. McKeown,
and Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Conference of the Association for
Computational Linguistics, College Park,
MD. Association for Computational
Linguistics, New Brunswick, NJ.
Hirst, Graeme and David St.-Onge. 1997.
Lexical chains as representation of context
for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, Wordnet: An electronic lexical database
and some of its applications. MIT Press,
Cambridge, pages 305?332.
Jing, H. and K. McKeown. 2000. Cut and
paste based text summarization. In
Proceedings of NAACL-00, Seattle.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Conference of the Association for
Computational Linguistics, College Park,
MD. Association for Computational
Linguistics, New Brunswick, NJ.
Marcu, Daniel. 1999. The automatic creation
of large scale corpora for summarization
research. In The 22nd International ACM
SIGIR Conference on Research and
Development in Information Retrieval,
Berkeley. ACM Press, New York.
Morris, J. and G. Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indecator of the structure of text.
Computational Linguistics, 18(1):21?45.
Silber, H. Gregory and Kathleen F. McCoy.
2000. Efficient text summarization using
lexical chains. In 2000 International
Conference on Intelligent User Interfaces,
New Orleans, January.
Sparck Jones, Karen. 1993. What might be in
summary? Information Retrieval ?93,
Regensburg, Germany, September, pages
9?26.
An Efficient Text Summarizer Using Lexical Chains 
H.Gregory  SHber  and  Kath leen  F.  McCoy  
Computer  and in fo rmat ion  Sciences 
Univers i ty of Delaware 
Newark,  DE  19711 
{ silber, mccoy} @cis. udel. edu 
? A b s t r a c t  . . . . . . . .  
We present a system which uses lexical chains as an 
intermediate r presentation for automatic text sum- 
marization. This system builds on previous research 
by implementing a lexical chain extraction algorithm 
in linear time. The system is reasonably domain in- 
dependent and takes as input any text or HTML 
document. The system outputs a short summary 
based on the most salient concepts from the origi- 
nal document. The length of the extracted summary 
can be either controlled automatically, or manually 
based on length or percentage ofcompression. While 
still under development, the system provides useful 
summaries which compare well in information con- 
tent to human generated summaries. Additionally, 
the system provides a robust est bed for future sum- 
mary generation research. 
1 In t roduct ion  
Automatic text summarization has long been viewed 
as a two-step process. First, an intermediate r pre- 
sentation of the summary must be created. Second, 
a natural language representation f the summary 
must be generated using the intermediate r presen- 
tation(Sparek Jones, 1993). Much of the early re- 
search in automatic text summarization has involved 
generation of the intermediate representation. The 
natural language generation problem has only re- 
cently received substantial attention in the context 
of summarization. 
1.1 Mot ivat ion 
In order to consider methods for generating natural 
text summaries from large documents, everal issues 
must be examined in detail. First, an analysis of the 
quality of the intermediate r presentation for use in 
generation must be examined. Second, a detailed 
examination of the processes which link the inter- 
mediate representation to a potential final summary 
must be undertaken. 
The system presented here provides a useful first 
step towards these ends. By developing a robust and 
efficient tool to generate these intermediate repre- 
sentations, we can both evaluate the representation 
......... andcormider the difficult problem of generatiiig nat- 
ural language texts from the representation. 
1.2 Background Research 
Much research as been conducted in the area of au- 
tomatic text summarization. Specifically, research 
using lexical chains and related techniques has re- 
ceived much attention. 
Early methods using word frequency counts did 
not consider the relations between similar words. 
Finding the aboutness of a document requires find- 
ing these relations. How these relations occur within 
a document is referred to as cohesion (Halliday 
and Hasan, 1976). First introduced by Morris and 
Hirst (1991), lexical chains represent lexical cohe- 
sion among related terms within a corpus. These 
relations can be recognized by identifying arbitrary 
size sets of words which are semantically related (i.e., 
have a sense flow). These lexical chains provide an 
interesting method for summarization because their 
recognition is easy within the source text and vast 
knowledge sources are not required in order to con> 
pure them. 
Later work using lexical chains was conducted by 
Hirst and St-Onge (1997) using lexical chains to cor- 
rect malapropisms. They used WordNet, a lexical 
database which contains ome semantic information 
(http://www.cs.princeton.edu/wn). 
Also using WordNet in their implenmntation. 
Barzilay and Elhadad (1997) dealt with some of tile 
limitations in Hirst and St-Onge's algorithm by ex- 
amining every possible lexical chain which could be 
computed, not just those possible at a given point 
in the text. That is to say, while Hirst and St.Onge 
would compute the chain in which a word should 
be placed when a word was first encountered, Barzi- 
lay and Elhadad computed ever:,' possible chain a 
word could become a member of when the word was 
encountered, and later determined the best interpre- 
tation. 
268 
2 A Linear T ime A lgor i thm for Intra Intra Adjacent Other 
Comput ing  Lexical  Chains Pgrph. Segment Segment 
2.1 Overv iew Same 1 1 1 1 
Our research on lexical chains as an intermediate Synonym 1 1 0 "O " 
representation forautomatic text summarization fol- Hypernym I 1 0 0 
lows the research of Barzilay and Elhadad (1997). Hyponym 1 1 0 0 
We use their results as a basis for the utility of Sibling 1 0 0 0 
the methodology. The most substantial difference is 
that Barzi lay and Elhadad create all possible chains 
explicit ly and then choose the best possible chain, 
whereas we compute them implicitly. 
Table 1: Dynamic Scoring Metrics Set to Mimic 
B+E's  Algorithm 
the word itself. These scores are dynamic and can . . . . . . .  2~2 
As mentioned above, WordNet is a lexical database 
that contains substantial semantic information. In 
order to  facilitate fficient access, the WordNet noun 
database was re-indexed by line number as opposed 
to file position and the file was saved in a binary in- 
dexed format. The database access tools were then 
rewritten to take advantage of this new structure. 
The result  of this work is that  accesses to the Word- 
Net noun database can be accomplished an order 
of magnitude faster than with the original imple- 
mentation. No additional changes to the WordNet 
databases were made. The re-indexing also provided 
a zero-based continuous numbering scheme that is 
important  o our linear time algorithm. This impor- 
tance will be noted below. 
Modifications ~to. Word.Net . . . . . . . . . . . . . .  be set ~ased ,on:segmentation information, dista.nce, 
2.3 Our  A lgor i thm 
Step 1 For each word instance that is a noun 
For every sense of that word 
Compute all scored "meta-chains" 
Step 2 For each word instance 
Figure out which "meta-chain" 
it contributes most to 
Keep the word instance in that chain 
and remove it from all other 
Chains updating the scores 
of each "meta-chain" 
Figure 1: Basic linear time Algorithm for Comput- 
ing Lexical Chains 
Our basic lexical chain algorithm is described 
briefly in Figure 1. The algorithm takes a part of 
speech tagged corpus and extracts the nouns. Us- 
ing WordNet to collect sense information for each of 
these noun instances, the algorithm then computes 
scored "nmta-chains" based on the collected infor- 
mation. A "meta-chain" is a representation f every 
possible lexical chain that can be computed start-  
ing with a word of a given sense. These meta-chains 
are scored in the following manner. As each word in- 
stance is added, its contribution, which is dependent 
on the scoring metrics used, is added to the "meta- 
chain" score. The contribution is then stored within 
and type of relation. 
Currently, segmentation is accomplished prior to 
using our algorithm by executing Hearst's text tiler 
(Hearst, 1994). The sentence numbers of each seg- 
ment boundary are stored for use by our algorithm. 
These sentence numbers are used in conjunction 
with relation type as keys into a table of potential 
scores. Table 1 denotes ample metrics tuned to sim- 
ulate the system devised by Barzilay and Elhadad 
(1997). 
At this point, the collection of "meta-chains" con- 
talns all possible interpretations of the source doc- 
ument. The problem is that  in our final represen- 
tation, each word instance can exist in only one 
chain. To figure out which chain is the correct one, 
each word is examined.using the score contribution 
stored in Step 1 to determine which chain the given 
word instance contributes to most. By deleting the 
word instance from all the other chains, a represen- 
tation where each word instance exists in precisely 
one chain remains. Consequently, the sum of the 
scores of all the chains is maximal. This method is 
analogous to finding a maximal spanning tree in a 
graph of noun senses. These noun senses are all of 
the senses of each noun instance in the document. 
From this representation, the highest scored 
chains correspond to the important concepts in the 
original document. These important concepts can 
be used to generate a summary from the source 
text. Barzilay and Elhadad use the notion of strong 
chains (i.e., chains whose scores are in excess of two 
standard eviations above the mean of all scores) to 
determine which chains to include in a summary. 
Our system can use this method, as well as sev- 
eral other methods including percentage compres- 
sion and number of sentences. 
For a more detailed description of our algorithm 
please consult our previous work (Silber and McCoy, 
2000). 
2.4 Runt ime Ana lys i s  
In this analysis, we will not consider the computa- 
tional complexity of part of speech tagging, as that is 
not the focus of this research. Also, because the size 
269 
Worst Average 
Case Case 
C1 =No. of senses 30 2 
C2 =Parent/chi ld isa relations ,45147 t4 
Ca =No. of nouns in WordNet 94474 94474 
C4 =No. of synsets in WordNet 66025 66025 
C5 =No. of siblings 397 39 
C6 =Chains word can belong to 45474 55 
Table 2: Constants from WordNet 
and structure of WordNet does not change from ex- 
ecution to execution of.aJae.algorit, hm, we shall take 
these aspects of WordNet to be constant. We will 
examine each phase of our algorithm to show that 
the extraction of these lexical chains can indeed be 
done in linear time. For this analysis, we define con- 
stants from WordNet 1.6 as denoted in Table 2. 
Extracting information from WordNet entails 
looking up each noun and extracting all synset, Hy- 
ponym/Hypernym, and sibling information. The 
runtime of these lookups over the entire document 
is: 
n * (log(Ca) + Cl * C2 + Cl * C5) 
When building the graph of all possible chains, we 
simply insert the word into all chains where a rela- 
tion exists, which is clearly bounded by a constant 
(C6). The only consideration is the computation 
of the chain score. Since we store paragraph num- 
bers represented within the chain as well as segment 
boundaries, we can quickly determine whether the 
relations are intra-paragraph, intra-segment, or ad- 
jacent segment. We then look up the appropriate 
score contribution from the table of metrics. There- 
fore, computing the score contribution of a given 
word is constant. The runtime of building the graph 
of all possible chains is: 
n*C6 .5  
Finding the best chain is equally efficient. For 
each word, each chain to which it belongs is exam- 
ined. Then, the word is marked as deleted from 
all but the single chain whose score the word con- 
tributes to most. In the case of a tie, the lower sense 
nmnber from WordNet is used, since this denotes a 
more general concept. The runtime for this step is: 
n*C6 .4  
This analysis gives an overall worst case runtime 
of: 
n * 1548216 + log(94474 ) + 227370 
and an average case runtime of: 
n ? 326 + log(94474) + 275 
While the constants are quite large, the algorithm 
is clearly O(n) in the number of nouns in the original 
document. 
A t  "first glance, "the'constants ~involved seem pro- 
hibitively large. Upon further analysis, however, we 
see that most synsets have very few parent child re- 
lations. Thus the worst case values maynot  reflect 
the actual performance of our application. In ad- 
dition, the synsets with many parent child relations 
tend to represent extremely general concepts. These 
synsets will most likely not appear very often as a 
direct synset for words appearing in a document. 
"2,;5 User  ~Interface 
Our system currently can be used as a command 
line utility. The arguments allow the user to specify 
scoring metrics, summary length, and whether or 
not to search for collocations. Additionally, a web 
CGI interface has been added as a front end which 
allows a user to specify not just text documents, but 
html documents as well, and summarize them from 
the Internet. Finally, our system has been attached 
to a search engine. The search engine uses data from 
existing search engines on the Internet o download 
and summarize ach page from the results. These 
summaries are then compiled and returned to the 
user on a single page.  The final result is that a 
search results page is returned with automatically 
generated summaries. 
2.6 Compar i son  w i th  P rev ious  Work  
As mentioned above, this research is based on the 
work of Barzilay and Elhadad (1997) on lexical 
chains. Several differences exist between our method 
and theirs. First and foremost, the linear run-time 
of our algorithm allows documents to be summarized 
much faster. Our algorithm can summarize a 40,000 
word document in eleven seconds on a Sun SPARC 
Ultra10 Creator. By comparison, our first version 
of the algorithm which computed lexical chains by 
building every possible interpretation like Barzilay 
and Elhadad took sLx minutes to extract chains from 
5,000 word documents. 
The linear nature of our algorithm also has sev- 
eral other advantages. Since our algorithm is also 
linear in space requirements, we can consider all pos- 
sible chains. Barzilay and Elhadad had to prune in- 
terpretations (enid thus chains) which did not seem 
promising. Our algorithm does not require pruning 
of chains. 
Our algorithm also allows us to analyze the iin- 
portance of segmentation. Barzilay and Elhadad 
used segmentation to reduce the complexity of the 
problem of extracting chains. They basically built 
chains within a segment and combined these chains 
later when chains across segment boundaries hared 
a word in the same sense in common. While we in- 
clude segmentation i formation in our algorithm, it 
270 
is merely because it might prove useful in disam- 
biguating chains. The fact that we can use it or not 
allows our algorithm to test the importance of seg- 
mentation to proper-word ~ense disambiguation. It 
is important o note that on short documents, like 
those analyzed by Barzilay and Elhadad, segmen- 
tation appears to have little effect. There is some 
linguistic justification for this fact. Segmentation 
is generally computed using word frequencies, and 
our lexical chains algorithm generally captures the 
same type of information. On longer documents, 
our research as shown segmentation to have a much 
greater effect. 
3 Cur rent  Research  and  Future  
D i rec t ions  
Some issues which are not currently addressed by 
this research are proper name disambiguation and 
anaphora resolution. Further, while we attempt o 
locate two-word collocations using WordNet, a more 
robust collocation extraction technique iswarranted. 
One of the goals of this research is to eventually 
create a system which generates natural language 
summaries. Currently, the system uses sentence se- 
lection as its method of generation. It is our con- 
tention that regardless of how well an algorithm for 
extracting sentences may be, it cannot possibly cre- 
ate quality summaries. It seems obvious that sen- 
tence selection will not create fluent, coherent text. 
Further, our research shows that completeness is a 
problem. Because information extraction is only at 
the sentence boundary, information which may be 
very important may be left out if a highly com- 
pressed summary is required. 
Our current research is examining methods of us- 
ing all of the important sentences determined by our 
lexical chains algorithm as a basis for a generation 
system. Our intent is to use the lexical chains algo- 
rithm to determine what to summarize, and then a 
more classical generation system to present he in- 
formation as coherent text. The goal is to combine 
and condense all significant information pertaining 
to a given concept which can then be used in gener- 
ation. 
4 Conc lus ions  
\Ve have described a domain independent summa- 
rization engine which allows for efficient summariza- 
tion of large documents. The algorithm described is 
clearly O(n) in the number of nouns in the original 
document. 
In their research, Barzilay and Elhadad showed 
that lexieal chains could be an effective tool for 
automatic text summarization (Barzilay and EI- 
hadad, 1997). By developing a linear time al- 
gorithm to compute these chains, we have pro- 
dueed a front end to a summarization system which 
can be implemented efficiently. An operational 
sample of this demo is available on the web at 
http://www.eecis.udel.edu/- silber/research.htm. 
..... While. ,usable currenlfly, the-system provides a 
platform for generation research on automatic text 
summarization by providing an intermediate r pre- 
sentation which has been shown to capture impor- 
tant concepts from the source text (Barzilay and 
Elhadad, 1997). The algorithm's peed and effec- 
tiveness allows research into summarization f larger 
documents. Moreover, its domain independence al- 
lows for research into the inherent differences be- 
tween domains. 
5 Acknowledgements  
The authors wish to thank the Korean Government, 
Ministry of Science and Technology, whose funding, 
as part of the Bilingual Internet Search Machine 
Project, has made this research possible. Addition- 
ally, special thanks to Michael Elhadad and Regina 
Barzilay for their advice, and for generously making 
their data and results available. 
Re ferences  
Regina Barzilay and Michael Elhadad. 1997. Us- 
ing lexical chains for text summarization. Pro- 
ceedings of the Intelligent Scalable Text Summa- 
rization Workshop, A CL Madrid. 
Michael Halliday and Ruqaiya Hasan. 1976. Cohe- 
sion in English. Longman, London. 
Marti A. Hearst. 1994. Multi-paragraph segmenta- 
tion of expository text. Proceedings o\] the 32nd 
Annual Meeting of the ACL. 
Gramme Hirst and David St-Onge. 1997. Lexical 
chains as representation f context for the detec- 
tion and correction of malapropisms. Wordnet: 
An electronic lexical database and some of its ap- 
plications. 
J. Morris and G. Hirst. 1991. Lexical cohesion com- 
puted by thesanral relations an an indecator of 
the structure of text. Computational Linguistics, 
18:21--45. 
H. Gregory Silber and Kathleen F. McCoy. 2000. 
Efficient text summarization using lexical chains. 
Conference on bztelligent User b~terfaces 2000. 
271 

