Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1025?1036, Dublin, Ireland, August 23-29 2014.
Inclusive yet Selective: Supervised Distributional Hypernymy Detection
Stephen Roller
?
, Katrin Erk
?
, Gemma Boleda
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
roller@cs.utexas.edu, katrin.erk@mail.utexas.edu,
gemma.boleda@upf.edu
Abstract
We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in
a superset of contexts in which their hyponyms are found. We find that this hypothesis only
holds when it is applied to relevant dimensions. We propose a robust supervised approach that
achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting
the dimensions that are relevant for distributional inclusion.
1 Introduction
One of the main criticisms of distributional models has been that they fail to distinguish between semantic
relations: Typical nearest neighbors of dog are words like cat, animal, puppy, tail, or owner, all obviously
related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002)
argues that distributional models cannot be a valid model of conceptual representation. Distinguishing
semantic relations are also crucial for drawing inferences from distributional data, as different semantic
relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as
Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004).
For these reasons, research has in recent years started to attempt the detection of specific semantic
relationships, and current results suggest that distributional models can, in fact, distinguish between
semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci
and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE
and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation
between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog).
Distributional approaches to date for detecting hypernymy, and the related but broader relation of
lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based
on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet
and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts
in which more general terms appear. So, animal can occur in all the contexts in which dog can occur,
plus some contexts in which dog cannot ? for instance, rights can be a typical cooccurrence for animal
(e.g. ?animal rights?), but not so much for dog (e.g. #?dog rights?).
This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We
show that the current best unsupervised approach is brittle in that their performance depends on the space
they are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct,
and if so, under what circumstances it holds. We use a simple supervised approach to relation detection
that has good performance (accuracy .84 on BLESS, .85 on the lexical entailment dataset of Baroni et
al. (2012)) and works well across different spaces.
1
Furthermore, we show that it can be interpreted
as selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer is
to propose the Selective Distributional Inclusion Hypothesis: The Distributional Inclusion Hypothesis
holds, but only for relevant dimensions.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Code and data are available at http://stephenroller.com/research/coling14.
1025
2 Background
Distributional models. Distributional models represent a word through the contexts in which it has
been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word
is represented as a vector in a high-dimensional space in which the dimensions are context items (for
example, other words) and the coordinates of the vector indicate the target?s degree of association with
each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not
stand for individual context items anymore.
Pattern-based approaches to inducing semantic relations. Early work on automatically inducing se-
mantic relations between words, starting with Hearst (1992), uses textual patterns. For example, ?[NP
1
]
and other [NP
2
]? implies that NP
2
is a hypernym of NP
1
. Pattern-based approaches have been applied
to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al.,
2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations be-
tween verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide
variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations.
A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al.,
2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors.
Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004)
introduce the notion of distributional generality, where v is distributionally more general than u if u
appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more
distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the
term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy
or the more loosely defined relation of lexical entailment.
Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and
in all equations below, u is the narrower term, and v the more general one. Abusing notation, we write u
for both a word and its associated vector ?u
1
, . . . , u
n
?. Kotlerman et al. (2010) predict lexical entailment
with the balAPinc measure, a modification of the Average Precision (AP) measure (eq. 2). The general
notion is that scores should increase with the number of dimensions of v that u shares, and also give more
weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This is captured
in APinc by computing precision P (r) at every rank r among u?s dimensions ? where precision is the
fraction of dimensions shared with v ?, and weighting by the rank of the same dimension in the broader
term, rel
?
(v, r, u). The final measure, balAPinc, smooths using the LIN similarity measure (Lin, 1998).
(We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).)
1(x) =
{
1 if x > 0;
0 otherwise
WeedsPrec(u, v) =
?
n
i=1
u
i
? 1(v
i
)
?
n
i=1
u
i
(1)
APinc(u, v) =
?
|1(u)|
r=1
P (r) ? rel
?
(v, r, u))
|1(u)|
(2)
balAPinc(u, v) =
?
APinc(u, v) ? LIN(u, v)
The ClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the nar-
rower term u has lower values than v across all dimensions (eq. 3). Lenci and Benotto (2012) introduce
the invCL measure, which uses ClarkeDE to measure both distributional inclusion of u in v and distri-
butional non-inclusion of v in u (eq. 4). While all other measures interpret the Distributional Inclusion
Hypothesis as the degree to which a ? relation holds, Lenci and Benotto test the degree to which proper
inclusion ( holds. They consider not only the degree to which the contexts of the narrower terms are
included in the contexts of the wider term, but also determine the degree to which the wider term has
contexts that the narrower term does not have.
1026
CL(u, v) =
?
n
i=1
min(u
i
, v
i
)
?
n
i=1
u
i
(3)
invCL(u, v) =
?
CL(u, v) ? (1? CL(v, u)) (4)
Like Lenci and Benotto, we focus on the stricter hypernymy relation, rather than lexical entailment.
We believe that the different relations that make up lexical entailment have different distributional indi-
cations and that, for that reason, it will be easier to detect the relations separately than together.
Baroni et al. (2012) proposes a supervised approach to hypernymy detection that represents two words
as the concatenation of their vectors. They also mention in passing another supervised approach that
represents two words as the component-wise difference of their vectors. These are broadly the two
approaches that we test, though we introduce significant modifications.
3 Data
3.1 Distributional Vector Spaces
We use three standard types of distributional spaces.
U+W2: This space is based on a concatenation of the Gigaword, BNC, English Wackypedia and
ukWaC corpora (Baroni et al., 2009). The corpora are POS-tagged and lemmatized. We keep only
content words (nouns, proper nouns, adjectives and verbs) with a corpus frequency of 500 or larger. The
resulting U+ corpus has roughly 133K word types and 2.8B word tokens. We created a vector space by
counting co-occurrences of these word types within a window of two words on the left and the right,
using the top 20k most frequent content words as dimensions. The space was transformed using Positive
Pointwise Mutual Information (PPMI).
U+Sent: The U+Sent space is constructed the same way as U+W2, but uses full sentence contexts
instead of 2-word windows.
TypeDM: This space is extracted from the TypeDM tensors (Baroni and Lenci, 2011). TypeDM con-
tains a list of weighted tuples, ??w
1
, l, w
2
?, ??, where w
1
and w
2
are content words, l is a corpus-derived
syntagmatic relationship between the words, and ? is a weight estimating saliency of the relationship. We
construct vectors for every unique w
1
using the set of ?l, w
2
? pairs as dimensions and corresponding ?
values as dimension weights. We select TypeDM for its excellent performance in previous comparisons
of distributional hypernymy measures (Lenci and Benotto, 2012).
Reduced Spaces: In some experiments, we use dimensionality reduced spaces. We reduce all three
spaces to 300 dimensions using Singular Value Decomposition. We use a subscript to denote reduced
spaces, e.g. U+W2
300
. When necessary, we use the term original dimensions to refer to the vector
dimensions from the original, non-reduced spaces (e.g. U+W2); the term latent dimensions refers to the
dimensions in the reduced spaces (e.g. U+W2
300
).
3.2 Evaluation Data Sets
BLESS: The BLESS data set (Baroni and Lenci, 2011) covers 200 concepts, or concrete and unambigu-
ous terms (divided into 17 different general concept classes, including vehicle and ground mammal), and
their relationships to other nouns, called relata. Example concepts include van and horse. Each concept
is related to several relata through different semantic relations. Following Lenci and Benotto (2012), we
focus on the four semantic relations where both concepts and relata are nouns, for a total 14K data points:
Hypernymy, denoting a superset relationship (e.g. animal-dog); Co-hyponymy, denoting words that share
a common hypernym (e.g. dog-cat); Meronymy, denoting a part-whole relationship (e.g. tail-dog); and
Random, denoting no relationship between the words (e.g. dog-computer).
1027
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2, invCL
l
l
ll
l
l
l
l
l
ll
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+Sent, invCL
ll
l
l
l
llll
l
l
l
l
l
l
ll
l
ll
l
ll
l
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
TypeDM, invCL
Figure 1: Distributions of relata invCL scores for the U+W2, U+Sent, and TypeDM spaces for each of
the semantic relations, after per-concept z-normalization.
ENTAILMENT: (Baroni et al., 2012): The ENTAILMENT data set consists of 2,770 word pairs, bal-
anced between positive (house-building) and negative (leader-rider) examples of hypernymy, with 1376
unique hyponyms and 1016 unique hypernyms. The positive examples were generated by selecting direct
hypernym relationships from WordNet, the negative examples by randomly permuting the hypernyms of
the positive examples, and then manually checking correctness.
4 Distributional Inclusion across Spaces
We test several unsupervised distributional approaches to hypernymy detection from the literature, fo-
cusing on the underlying vector space representation as the main parameter that we vary. We use the
three spaces described in Section 3. We test four hypernymy detection approaches, all of them similarity
measures based on the Distributional Inclusion Hypothesis: WeedsPrec, balAPinc, ClarkeDE, and invCL.
Our baseline is the standard cosine measure. We evaluate on the BLESS dataset.
To evaluate on BLESS, we follow the evaluation scheme laid out in Baroni and Lenci (2011). Given a
space and similarity measure, we compute similarity for each concept and relatum. For each concept, we
select its nearest neighbors (according to the given similarity measure) in each of the four relations (CO-
HYP, HYPER, MERO, RANDOM), and transform the corresponding four similarities to z-scores. Across
all concepts, this yields four sets of z-normalized similarity scores, one for each relation. These four sets
describe the relative similarity of concepts to their nearest neighbors in different relations. Tukey?s Hon-
estly Significant Difference test is used for testing whether scores differ significantly between relations
(threshold: p < 0.05).
Figure 1 shows the distributions of z-scores for invCL for the four relations, with one graph for each
of the three spaces we consider. For this illustration, we focus on invCL because it shows the overall best
performance at identifying hypernymy. The rightmost plot in Figure 1 replicates the analysis of Lenci
and Benotto (2012), who used the TypeDM space. It confirms their finding that invCL gives significantly
higher values to hypernyms than co-hyponyms ? at least on this space. However, in the U+W2 and
U+Sent spaces (leftmost and middle plot), invCL clearly loses any ability to rank hypernyms the highest;
indeed, in both spaces, co-hyponymy and meronymy both have significantly higher z-scores than hyper-
nymy. Concerning the other measures, we found that they patterned with invCL. On TypeDM, ClarkeDE
and WeedsPrec had significantly higher nearest-neighbor values for hypernyms than co-hyponyms.
2
On
U+W2 and U+Sent, all measures ranked co-hyponyms significantly higher than hypernyms. With the
baseline measure, cosine, the similarity ratings for the CO-HYP relation are always the highest, no matter
the space, followed by HYPER, MERO, RANDOM in this order.
Following Kotlerman et al. (2010) and Lenci and Benotto (2012), we also report the performance of
the measures using Mean Average Precision (MAP). Average Precision (AP) is a measure often used in
2
balAPinc could not be evaluated on TypeDM due to computational issues.
1028
Measure CO-HYP HYPER MERO RANDOM
U+W2
cosine .68 .20 .27 .27
ClarkeDE .66 .19 .28 .28
invCL .60 .18 .31 .28
U+Sent
cosine .66 .18 .28 .28
ClarkeDE .66 .15 .29 .28
invCL .59 .13 .34 .29
TypeDM
cosine .78 .19 .20 .29
ClarkeDE .45 .35 .25 .32
invCL .38 .36 .27 .33
Table 1: Mean Average Precision for the unsupervised measures on three spaces.
the Information Retrieval community with a maximal AP score of 1 when all relevant documents (relata
with the right relationship, in our case) are ranked at the top. We compute AP on a per-concept basis and
report the mean over all 200 AP values. An advantage of MAP is that, while the BLESS analysis method
focuses on nearest neighbors, MAP evaluates the ranking of all relata. A disadvantage of MAP is that it
does not test the degree to which a similarity measure separates different semantic relations, like Tukey
does, so it may overstate the discriminative power of a particular measure. However, it provides a more
intuitive accuracy-like number compared to the BLESS evaluation.
Table 1 shows the Mean Average Precision values for cosine, ClarkeDE, and invCL on all three spaces.
We also computed WeedsPrec and balAPinc results, obtaining the same picture; we focus on ClarkeDE
and invCL because ClarkeDE is a component of invCL, and invCL is the current best measure. The results
corresponding to Lenci and Benotto?s are shown in the lowest part of Table 1, where we report numbers
for TypeDM. Like Lenci and Benotto, we find that unsupervised measures other than invCL rank co-
hyponyms the highest, and obtain relatively low results for hypernyms. For invCL in TypeDM, Lenci
and Benotto obtain 0.38 MAP for co-hyponyms and a slightly higher 0.40 for hypernyms, though they
do not report significance testing results. We obtain 0.38 for co-hyponyms and 0.36 for hypernyms, and
the difference is not significant.
3
Even though our results are slightly different from those in Lenci and
Benotto (2012), both our results and theirs point to at most a weak preference of invCL for hypernyms
over co-hyponyms. Moreover, in the U+W2 and U+Sent spaces we see that all three measures are very
poor at identifying hypernyms, and the co-hyponymy relation stubbornly persists as most relevant to all
three measures, by a large margin.
Our results thus constitute a puzzle for the Distributional Inclusion Hypothesis. It seems that there
must be some merit to the hypothesis: On one particular space, namely TypeDM, the nearest neighbors
in the hypernymy relation had higher similarity scores than any other relation by a significant margin.
This was true for all the hypernymy detectors we studied. But even on TypeDM, the MAP evaluation
showed at most a weak hypernymy signal, and when spaces other than TypeDM were used, the effect
vanished altogether. So how strong an indication for hypernymy can we expect from distributional
inclusion measures in general? We will return to this question below, where our answer will be: The
Distributional Inclusion Hypothesis seems to hold after all, but it needs to be applied to the right kind of
dimensions ? and a supervised approach can help in picking the right dimensions.
As the unsupervised approaches struggle to detect hypernymy and do not seem robust to changes in
standard space parameters, we think it is time to consider supervised approaches. In the next section, we
explore two simple supervised approaches that show good performance and are robust to changes in the
underlying space.
3
Wilcoxon signed-rank test.
1029
5 Supervised Hypernymy Detection
We use two simple, supervised models for predicting BLESS and ENTAILMENT relations. The first
(Concat) is a model previously proposed by Baroni et al. (2012). The second (Diff) takes up an idea
from a footnote in Baroni et al. (2012), but while that footnote stated that the approach in question did
not work, we find that, with a few modifications, it obtains the best performance ? and can be interpreted
as a supervised version of the Distributional Inclusion Hypothesis. Note that while we used unreduced
spaces in the previous section, we now use reduced spaces throughout (these are the spaces with the
300
subscript), in order not to have more features than data points.
5.1 Models, Features, and Method
Concat: We use a standard Support Vector Machine (SVM) classifier with a concatenation of vectors as
input features. SVMs are binary classifiers which learn the maximum margin hyperplane separating the
two classes. SVMs employ kernel functions to find the hyperplanes in higher dimensional spaces which
are nonlinear in the original space. As feature vectors for the classifier, we follow Baroni et al. (2012)
and use the concatenation of the latent dimension vectors representing words. For the ENTAILMENT
dataset, we use the concatenation of the hyponym latent vector and the hypernym latent vector for each
word pair as training features, and the entails/doesn?t entail annotations as binary targets. For BLESS,
we use the concatenation of the concept latent vector and the relatum latent vector as training features,
and the four relationship classes as targets. We choose the four-way task rather than a ?hypernymy vs.
other? classification because BLESS contains many more co-hyponymy and random than hypernymy
pairs, which would give a very high baseline in the two-way task. Additionally, the other relations in
BLESS, in particular meronymy, may be interesting in their own right.
Since SVMs are binary classifiers, we use SciKit-Learn?s default setting to train 6 pairwise-relation
one-vs-one classifiers which vote on the final answer. We use a polynomial kernel with a degree of 3
and a penalty term of C = 1.0, and all other hyperparameters are chosen using the SciKit-Learn default
values (Pedregosa et al., 2011). No hyperparameters are tuned in any experiment.
Diff: Our second classifier is a Logistic Regression (aka MaxEnt) model trained on difference vectors.
Logistic Regression is a statistical model for binary classification. It learns a linear hyperplane sepa-
rating the classes and estimates a probability for classes using a logistic function. We selected Logistic
Regression over other possible linear classifiers for its natural ability to give likelihood estimates, which
we believe will be useful in future work in an application of hypernymy classification to RTE.
As feature vectors, we use a Mikolov-inspired method of representing word pairs as the difference
vectors between the two words.
4
Baroni et al. (2012) suggested the use of difference vectors as input
to a classifier, but reported them as unsuccessful. We found difference vectors to be excellent features,
with three important modifications: a linear classifier is better than a nonlinear one; vectors must be
normalized to have a magnitude of 1 before taking the difference; and squared difference vectors must
also be included as features. So, we represent each word pair with latent vectors (u, v) as a two part
vector ?f ; g?, where
f
i
=
u
i
?u?
?
v
i
?v?
,
g
i
= f
2
i
.
These differences features
5
are analogous to a supervised distributional inclusion measure. The dif-
ference between two words on a particular dimension captures the degree of distributional inclusion on
that dimension. The primary distinction between the difference features and the unsupervised measures
is that the supervised classifier learns to weight the importance of different dimensions. The f features
encode directional aspects of distributional inclusion: that the hyponym contexts should be included in
4
After recent work using subtraction to represent analogy in certain neural-network spaces (Mikolov et al., 2013).
5
We also tried variations, such as not normalizing vectors and removing the difference squared vector, but found this setting
the best. We also tried the Diff features with an SVM and other nonlinear classifiers, but they performed worse.
1030
Data set BLESS ENTAILMENT
Baseline .46 .50
Classifier Concat Diff Concat Diff
U+W2
300
.76 .84 .81 .85
U+Sent
300
.73 .80 .78 .82
TypeDM
300
- .82 .65 .85
Table 2: Average accuracy of Concat and Diff on BLESS and ENTAILMENT using different spaces for
feature generation.
those of the hypernym (the weight learned is positive), and the hypernym contexts should not be in-
cluded in those of the hyponym (the weight learned is negative). So like invCL, this model uses a ?proper
subset? interpretation of the Distributional Inclusion Hypothesis, but only considers selected dimensions
(i.e. those that the model assigns nonzero weights).
The difference-squared features (g), on the other hand, typically identify dimensions that are not in-
dicative of hypernymy, by learning negative weights on them (more about this in Section 6). Thus, rather
than helping identify hypernyms, they help separate random relations from the rest.
We use a L1 regularizer with a strength of C = 1.0. All other hyperparameters are chosen using
the SciKit-Learn defaults. Since Diff is also a binary classifier, we use SciKit-Learn?s default setting of
training 4 one-vs-all classifiers for BLESS, with the most confident classifier choosing the final answer.
Method: For evaluation on BLESS, we hold out one concept and train on the remaining 199 concepts.
We also exclude from the training set any pair containing a relatum which appears in the test set. This
way, no word that appears in the test set has been seen in training. We report the average accuracy across
all concepts. We use the most frequent relation type (random) as our baseline. For the ENTAILMENT data
set, we hold out one hyponym and train on all remaining hyponyms. Again, we exclude from training
any pair containing a hypernym which appears in the test set. We report average accuracy across all
hyponyms. The data set is balanced, so the baseline is 0.5.
5.2 Results
Table 2 shows the performance of the two classifiers, Concat and Diff, on both the BLESS and ENTAIL-
MENT datasets, using three underlying spaces. We use the reduced versions of the three spaces, indicated
by the subscript
300
. Note that the Concat classifier could not converge using features from TypeDM
300
,
so we omit the result. With both methods, we obtain a high accuracy on the two datasets, with results
around .8 against baselines around .5. Our best result is .84 on BLESS and .85 on ENTAILMENT. More-
over, both approaches are in general robust to changes in space parameters (with TypeDM/Concat an
outlier). Still, the U+W2
300
space seems to be the best for this task: Its scores are significantly
6
higher
than the rest, except for TypeDM on ENTAILMENT, which achieves the same score as U+W2
300
. Diff
achieves significantly higher results than Concat.
When provided more information, Concat outperforms Diff. For instance, if cross-validation is done
over all pairs in BLESS in the U+W2
300
space, Concat achieves .98 accuracy, while Diff obtains .90.
However, in this setting the same words appear in the training and test sets (albeit in different pairs).
We take this to mean that Concat is memorizing, rather than learning the hypernymy relation. This
emphasizes the need for our stricter evaluation that prevents repetition between training and test sets.
Clearly, both classifiers do fairly well at predicting hypernymy relations between words, regardless
of space. Naturally, one should ask what are the classifiers capturing that the unsupervised measures
are missing? We propose that the supervised classifiers perform essentially the same operation as the
unsupervised measures, but are learning to determine the relevance of dimensions. In particular, Diff
is learning weights on vector difference features. This is equivalent to doing selective distributional
inclusion. In the next section, we test this Selective Distributional Inclusion Hypothesis.
6
Wilcoxon signed-rank test, p < .001.
1031
ll
l
l
l
l
ll
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2 proj, cosine
ll
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
ll
l
l
ll
ll
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2 proj, ClarkeDE
llll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
?1.5
?1.0
?0.5
0.0
0.5
1.0
1.5
Co?hyp Hyper Mero Random
z
U+W2 proj, invCL
Figure 2: Distributions of relata scores across concepts using the cosine, ClarkeDE, and invCL measures
(after per-concept z-normalization). Here we use the selected dimensions of the U+W2
proj
space.
6 Selective Distributional Inclusion
In order to test how well our supervised model is capturing the notion of selective distributional inclusion,
we test each of the unsupervised measures on a smaller space, limited only to the dimensions preferred
by the classifier. We emphasize that we do not aim to show that our supervised method outperforms
unsupervised methods, but rather that the unsupervised methods benefit greatly from feature selection.
Additionally, we analyze which dimensions are selected by the classifier to facilitate understanding of
why these dimensions are important.
6.1 Experiment
We train the Diff classifier using the dimensionality-reduced U+W2
300
space with the same method we
use in Section 5. We take the classifier?s learned hyperplane separating hypernyms from other relations,
and project the hyperplane back into the original U+W2 space.
7
We select the 500 dimensions in the orig-
inal space that are most relevant according to the classifier weights, and test the unsupervised measures
on this new space, which we denote as U+W2
proj
.
8
The 500 most relevant dimensions are selected as follows: We select the 250 most negatively weighted
original dimensions using the difference features f . These are the features that have smaller values for
hyponyms (e.g. dog) than for hypernyms (e.g. animal), so they characterize hypernymy. We further select
the 250 most positively weighted original dimensions using the squared-differences features g. These
are the ones where a large difference does not indicate hypernymy.
Figure 2 shows the boxplots for the BLESS analysis: the distributions of nearest-neighbor similarity
scores for the four different semantic relations, for the measures cosine, ClarkeDE, and invCL. We see
that invCL now easily discriminates hypernymy from the other relations in the backprojected space. (The
difference of HYPER and CO-HYP is significant.) This is even though the space is based on U+W2, where
invCL failed to rate hypernyms higher than co-hypernyms in Section 4. Unsurprisingly, cosine, which
does not measure distributional inclusion, still prefers CO-HYP.
Table 3 shows the MAP scores for three of the measures in the new U+W2
proj
space. (The results
for balAPinc and WeedsPrec are slightly worse than ClarkeDE.) All measures except for cosine assign
higher scores to hypernyms than they did in the original space (compare to U+W2 part of Table 1). But
it is only invCL that ranks hypernyms significantly higher than co-hyponyms.
9
7
Ideally we would train on the original space to inspect the relevant dimensions. However, there are more dimensions than
examples, so we train on the SVD space and backproject.
8
Note that U+W2
proj
varies slightly from concept to concept, since the hyperplane is learned on a per-concept basis. It is
important that we use the linear Diff classifier for this reverse-projection procedure, as the separating hyperplane must be linear
in order to complete the projection. In particular, the hyperplane in the Concat classifier cannot be easily backprojected, since
it exists in a higher dimensional space than the projection matrix. Furthermore, it is important that we use a classifier trained
using the difference features because of its analogy to the Distributional Inclusion Hypothesis.
9
Wilcoxon signed-rank test, p < .001. To check that the measures are being improved by the dimension selection and not
1032
Measure CO-HYP HYPER MERO RANDOM
U+W2
proj
cosine .69 .20 .24 .28
ClarkeDE .55 .39 .24 .29
invCL .42 .58 .24 .29
Table 3: Mean Average Precision for the unsupervised measures after selecting the top dimensions from
a supervised model.
For this experiment, we train on all of BLESS except for one concept and then evaluate the unsuper-
vised models on the held-out concept ? that is a setting that could, in principle, be used as a hypernymy
detector. If we instead train the supervised model on all of BLESS to determine an upper bound of how
well dimension selection can do on this dataset, MAP for invCL rises to .67.
Overall, these experiments provide strong evidence for the Selective Distributional Inclusion Hypoth-
esis: The Distributional Inclusion Hypothesis holds, but only for relevant dimensions. In addition, hy-
pernymy detectors need to test for ?proper inclusion? of distributional contexts in order to really find
hypernyms.
Analysis of Selected Dimensions. We examine the 500 dimensions selected by the above procedure,
in order to see what the classifier is learning. As this is for analysis only, the dimensions were selected
by training on all data.
Recall that the difference-squared g features can be interpreted as dimensions that the classifier deems
not indicative of hypernymy. 200 out of the 250 most relevant dimensions by g are Computer Science
related terms like software, configure, or Linux. Since ukWaC, the largest corpus we use, is web-based,
it makes sense that it has many CS-related terms, which are noise when it comes to hypernymy detection
for BLESS concepts. Also, we find that while the supervised approach needs the negative information
from the g features (for Diff in the U+W2
300
space, omitting g features yields a drop from .84 to .8),
the unsupervised measures cannot use it. Dropping g features improves invCL results from .58 to .61.
The g-based dimensions are explicitly those for which distributional inclusion should not hold, so they
constitute noise to the unsupervised approaches.
The f features can be interpreted as dimensions that characterize hypernyms. An inspection reveals
two clear patterns. First, the features are topically relevant for the BLESS dataset. The 17 concept classes
in the dataset belong to three broader groups: animals, plants, and artifacts. An annotation of the 250
dimensions by one of the authors showed that 58 dimensions are typical of animals (parasite, extinct), 14
typical of vegetables (flora, nutrient), 80 typical of artifacts (repair, mechanical), 49 are general terms
(find, worthy), and 49 have no clear interpretation (thee, enigmatic). Second, the features are general
terms. For instance, for animals we find terms like animal, insect, creature, fauna, species, evolutionary,
pathogen, nature, ecology. We also find many hypernyms, including many concept class names.
Clearly, the selected features are domain dependent; most are directly related to the concepts and
concept classes of BLESS. We expect that our method should work well for other data sets, given its high
accuracy and the strict training procedure. However, these features are unlikely to be global indicators of
hypernymy. This emphasizes the need, in future work, to find a way to automatically determine relevance
on a per-word basis.
7 Conclusion
In this paper, we have tested the Distributional Inclusion Hypothesis, the basis for distributional ap-
proaches to hypernymy. We have found that the hypothesis only works if inclusion is selectively applied
to a set of relevant dimensions.
just by restricting to a smaller space, we evaluated the similarity measures on a variation of the U+W2 space which uses 500
randomly selected dimensions from the original space. The results are approximately unchanged from those on the original
U+W2 space.
1033
We have tested two simple supervised approaches to distributional hypernymy detection and have
found that they show good performance, and are robust to changes in the underlying space. Our best
classifier achieves .84 accuracy on BLESS and .85 on the ENTAILMENT dataset of Baroni et al. (2012). It
uses features that encode dimension-wise difference between vectors. This classifier can be interpreted
as selecting the dimensions necessary for the Distributional Inclusion Hypothesis to work, thus as an
effective way to implement selective distributional inclusion.
The next natural step is to use the supervised features to guide development of an unsupervised mea-
sure for hypernymy detection: Now that we have examples, we hope to propose a method which selects
relevant features automatically. We also would like to explore detection of other relationships, such
as meronymy. Finally, we would like to perform an extrinsic evaluation of our hypernymy detection
approach in an actual RTE system.
Acknowledgements
This research was supported by the DARPA DEFT program under AFRL grant FA8750-13-2-0026. The
authors acknowledge the Texas Advanced Computing Center (TACC)
10
for providing grid resources that
have contributed to these results. We thank the anonymous reviewers and the UTexas NLP group for
their helpful comments and suggestions.
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1?10, Edinburgh,
UK, July. Association for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,
43(3):209?226.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 23?32, Avignon, France, April. Association for Computational
Linguistics.
Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational Linguistics, pages 57?64, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini. 2005. Ontology Learning from Text: Methods, Evaluation
and Applications. Frontiers in Artificial Intelligence and Applications Series. IOS Press, Amsterdam.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations.
In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 33?40.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-Thieme, and Steffen Staab. 2005. Learning taxonomic relations
from heterogeneous sources of evidence. Ontology Learning from Text: Methods, evaluation and applications.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop on Geometrical Models of Natural Language Semantics, pages 112?119, Athens, Greece, March.
Association for Computational Linguistics.
Maayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. In Proceedings of the
20th International Conference on Computational Linguistics, page 247. Association for Computational Linguis-
tics.
Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Language Technology-Volume 1, pages 1?8. Association
for Computational Linguistics.
10
http://www.tacc.utexas.edu
1034
Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Com-
putational Linguistics, 32(1):83?135.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th
Conference on Computational Linguistics, pages 539?545, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Aur?elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 440?445, Sofia, Bulgaria, August. Association for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language Engineering, 16:359?389, 10.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In *SEM
2012: The First Joint Conference on Lexical and Computational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 75?79, Montr?eal, Canada, 7-8 June. Association for Computational
Linguistics.
Alessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research. Italian Journal of Lin-
guistics, 20(1):1?31.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th international Joint Conference on Artificial intelligence, pages 1492?
1493.
Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 98, pages 296?304.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Gregory L. Murphy. 2002. The Big Book of Concepts. MIT Press, Boston, MA.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting
semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the
44th annual meeting of the Association for Computational Linguistics.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertran Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Courna-
peau, Matthieu Brucher, MMatthieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Enrico Santus. 2013. SLQS: An entropy measure. Master?s thesis, University of Pisa.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym dis-
covery. In Lawrence K. Saul, Yair Weiss, and L?eon Bottou, editors, Advances in Neural Information Processing
Systems 17, pages 1297?1304, Cambridge, MA. MIT Press.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics, ACL-44, pages 801?808, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379?416.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
pages 81?88.
1035
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of the 20th International Conference on Computational Linguistics, pages 1015?1021, Geneva,
Switzerland, Aug 23?Aug 27. Association for Computational Linguistics, COLING.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 107?114,
Ann Arbor, Michigan, June. Association for Computational Linguistics.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Computa-
tional linguistics, 35(3):435?461.
1036
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1500?1510, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Supervised Text-based Geolocation
Using Language Models on an Adaptive Grid
Stephen Roller? Michael Speriosu ? Sarat Rallapalli ?
Benjamin Wing ? Jason Baldridge ?
?Department of Computer Science, University of Texas at Austin
?Department of Linguistics, University of Texas at Austin
{roller, sarat}@cs.utexas.edu, {speriosu, jbaldrid}@utexas.edu, ben@benwing.com
Abstract
The geographical properties of words have re-
cently begun to be exploited for geolocating
documents based solely on their text, often in
the context of social media and online content.
One common approach for geolocating texts is
rooted in information retrieval. Given training
documents labeled with latitude/longitude co-
ordinates, a grid is overlaid on the Earth and
pseudo-documents constructed by concatenat-
ing the documents within a given grid cell;
then a location for a test document is chosen
based on the most similar pseudo-document.
Uniform grids are normally used, but they are
sensitive to the dispersion of documents over
the earth. We define an alternative grid con-
struction using k-d trees that more robustly
adapts to data, especially with larger training
sets. We also provide a better way of choosing
the locations for pseudo-documents. We eval-
uate these strategies on existing Wikipedia and
Twitter corpora, as well as a new, larger Twit-
ter corpus. The adaptive grid achieves com-
petitive results with a uniform grid on small
training sets and outperforms it on the large
Twitter corpus. The two grid constructions
can also be combined to produce consistently
strong results across all training sets.
1 Introduction
The growth of the Internet in recent years has
provided unparalleled access to informational re-
sources. It is often desirable to extract summary
metadata from such resources, such as the date of
writing or the location of the author ? yet only a
small portion of available documents are explicitly
annotated in this fashion. With sufficient training
data, however, it is often possible to infer this infor-
mation directly from a document?s text. For exam-
ple, clues to the geographic location of a document
may come from a variety of word features, e.g. to-
ponyms (Toronto), geographic features (mountain),
culturally local features (hockey), and stylistic or di-
alectical differences (cool vs. kewl vs. kool).
This article focuses on text-based document ge-
olocation, the prediction of the latitude and lon-
gitude of a document. Among the uses for this
are region-based search engines; tracing the sources
of historical documents; location attribution while
summarizing large documents; tailoring of ads while
browsing; phishing detection when a user account is
accessed from an unexpected location; and ?activist
mapping? (Cobarrubias, 2009), as in the Ushahidi
project.1 Geolocation has also been used as a fea-
ture in automatic news story identification systems
(Sankaranarayanan et al 2009).
One of the first works on document geolocation is
Ding et al(2000), who attempt to automatically de-
termine the geographic scope of web pages. They
focus on named locations, e.g. cities and states,
found in gazetteers. Locations are predicted based
on toponym detection and heuristic resolution al-
gorithms. A related, recent effort is Cheng et al
(2010), who geolocate Twitter users by resolving
their profile locations against a gazetteer of U.S.
cities and training a classifier to identify geographi-
cally local words.
An alternative to using a discrete set of locations
from a gazetteer is to use information retrieval (IR)
techniques on a set of geolocated training docu-
ments. A new test document is compared with each
1http://ushahidi.com/
1500
training document and a location chosen based on
the location(s) of the most similar training docu-
ment(s). For image geolocation, Chen and Grauman
(2011) perform mean-shift clustering over training
images to discretize locations, then estimate a test
image?s location with weighted voting from the k
most similar documents. For text, both Serdyukov
et al(2009) and Wing and Baldridge (2011) use a
similar approach, but compute document similarity
based on language models rather than image fea-
tures. Additionally, they group documents via a uni-
form geodesic grid rather than a clustered set of lo-
cations. This reduces the number of similarity com-
putations and removes the need to perform location
clustering altogether, but introduces a new param-
eter controlling the granularity of the grid. Kinsella
et al(2011) predict the locations of tweets and users
by comparing text in tweets to language models as-
sociated with zip codes and broader geopolitical en-
closures. Sadilek et al(2012) discretize by simply
clustering data points within a small distance thresh-
old, but only perform geolocation within fixed city
limits.
While the above approaches discretize the contin-
uous surface of the earth, Eisenstein et al(2010)
predict locations based on Gaussian distributions
over the earth?s surface as part of a hierarchical
Bayesian model. This model has many advantages
(e.g. the ability to compute a complete probability
distribution over locations), but we suspect it will be
difficult to scale up to the large document collections
needed for high accuracy.
We build on the IR approach with grids while ad-
dressing some of the shortcomings of a uniform grid.
Uniform grids are problematic in that they ignore the
geographic dispersion of documents and forgo the
possibility of greater-granularity geographic resolu-
tion in document-rich areas. Instead, we construct
a grid using a k-d tree, which adapts to the size of
the training set and the geographic dispersion of the
documents it contains. This can better benefit from
more data, since it enables the training set to support
more pseudo-documents when there is sufficient ev-
idence to do so, while still ensuring that all pseudo-
documents contain comparable amounts of data. It
also has the desirable property of generally requiring
fewer active cells than a uniform grid, drastically re-
ducing the computation time required to label a test
document.
We show that consistently strong results, robust
across both Wikipedia and Twitter datasets, are ob-
tained from the union of the pseudo-documents from
a uniform and adaptive grid. In addition, a sim-
ple difference in the choice of location for a given
grid cell ? the centroid of the training documents
in the cell, rather than the cell midpoint ? results
in across-the-board improvements. We also con-
struct and evaluate on a much larger dataset of ge-
olocated tweets than has been used in previous pa-
pers, demonstrating the scalability and robustness of
our methods and confirming the ability of the adap-
tive grid to more effectively use larger datasets.
2 Data
We work with three datasets: a corpus of geotagged
Wikipedia articles and two corpora of geotagged
tweets.
GEOWIKI is a collection of 1,019,490 geotagged
English articles from Wikipedia. The dump from
Wikimedia requires significant processing to obtain
article text and location, so we rely on the prepro-
cessed data used by Wing and Baldridge (2011).
GEOTEXT is a small dataset consisting of
377,616 messages from 9,475 users tweeting across
48 American states, compiled by Eisenstein et al
(2010). A document in this dataset is the concate-
nation of all tweets by a single user, with a location
derived from the earliest tweet with specific, GPS-
assigned latitude/longitude coordinates.
UTGEO2011 is a new dataset designed to ad-
dress the sparsity problems resulting from the size
of the previous dataset. It is based on 390 mil-
lion tweets collected across the entire globe be-
tween September 4th and November 29th, 2011, us-
ing the publicly available Twitter Spritzer feed and
global search API. Not all collected tweets were
geotagged. To be comparable to GEOTEXT, we
discarded tweets outside of North America (out-
side of the bounding box with latitude/longitude
corners at (25,?126) and (49,?60)). Following
Eisenstein et al(2010), we consider all tweets
of a user concatenated as a single document, and
use the earliest collected GPS-assigned location as
the gold location. Users without a gold location
were discarded. To remove many spammers and
1501
robots, we only kept users following 5 to 1000
people, followed by at least 5 users, and author-
ing no more than 1000 tweets in the three month
period. The resulting dataset contains 38 million
tweets from 449,694 users, or roughly 85 tweets
per user on average. We randomly selected 10,000
users each for development and held-out test eval-
uation. The remaining 429,694 users serve as a
training set termed UTGEO2011-LARGE. We also
randomly selected a 10,000 user training subset
(UTGEO2011-SMALL) to facilitate comparisons
with GEOTEXT and allow us to investigate the rel-
ative improvements for different models with more
training data.
Our code and the UTGEO2011 data set are both
available for download.2
3 Model
Assume we have a collection d of documents and
their associated location labels l. These docu-
ments may be actual texts, or they can be pseudo-
documents comprised of a number of texts grouped
via some algorithm (such as the grids discussed in
the next section).
For a test document di, its similarity to each la-
beled document is computed, and the location of the
most similar document assigned to di. Given an ab-
stract function sim that can be instantiated with an
appropriate similarity function (e.g. cosine distance
or Kullback-Leibler divergence),
loc(di) = loc(argmax
dj?d
sim(di, dj)).
This is a winner-takes-all strategy, which we follow
in this paper. In related work on image geoloca-
tion, Hays and Efros (2008) use the same general
framework, but compute the location based on the
k-nearest neighbors (kNN) rather than the top one.
They compute a distribution from the 120 nearest
neighbors using mean shift clustering (Comaniciu
and Meer, 2002) and choose the cluster with the
most members. This produced slightly better re-
sults than choosing only the closest image. In future
work, we will explore the kNN approach to see if it
is more effective for text geolocation.
2https://github.com/utcompling/
textgrounder/wiki/RollerEtAl_EMNLP2012
Following previous work in document geoloca-
tion, particularly Serdyukov et al(2009) (hence-
forth SMvZ) and Wing and Baldridge (2011)
(henceforth W&B), we geolocate texts using a lan-
guage modeling approach to information retrieval
(Ponte and Croft, 1998; Zhai and Lafferty, 2001).
For each document di, we construct a unigram prob-
ability distribution ?di over the vocabulary.
We smooth documents using the pseudo-Good-
Turing method of W&B, a nonparametric discount-
ing model that backs off from the unsmoothed distri-
bution ??di of the document to the unsmoothed distri-
bution ??D of all documents. A general discounting
model is as follows:
P (w|?di) =
{
(1? ?di)P (w|??di), if P (w|??di) > 0
?di
P (w|??D)
Udi
, otherwise,
where Udi = 1 ?
?
w?di
P (w|??D) is a normaliza-
tion factor that is precomputed when the distribution
for di is constructed. The discount factor ?di indi-
cates how much probability mass to reserve for un-
seen words. For pseudo-Good-Turing, it is
?di =
|w ? di s.t. count(w ? di) = 1|
|w ? di|
,
i.e. the fraction of words seen once in di.
We experimented with other smoothing methods,
including Jelinek-Mercer and Dirichlet smoothing.
A disadvantage of these latter two methods is that
they have an additional tuning parameter to which
their performance is highly sensitive, and even with
optimal parameter settings neither consistently out-
performed pseudo-Good-Turing. We also found no
consistent improvement from using interpolation in
place of backoff.
We also follow W&B in using Kullback-Leibler
(KL) divergence as the similarity metric, since it out-
performed both naive Bayes classification probabil-
ity and cosine similarity:
KL(?di ||?dj ) =
?
k
?di(k) log
?di(k)
?dj (k)
.
The motivation for computing similarity with KL is
that it is a measure of how well each document in
the labeled set explains the word distribution found
in the test document.
1502
4 Collapsing Documents with an Adaptive
Grid
In the previous section, we used the term ?docu-
ment? loosely when speaking of training documents.
A simplistic approach might indeed involve com-
paring a test document to each training document.
However, in the winner-takes-all model described
above, we can rely only on the result of comparing
with the single best training document, which may
not contain enough information to make a good pre-
diction.
A standard strategy to deal with this problem is
to collapse groups of geographically nearby docu-
ments into larger pseudo-documents. This also has
the advantage of reducing the computation time,
as fewer training documents need to be compared
against. Formally, this involves partitioning the
training documents into a set of sets of documents
G = {g1 . . . gn}. A collection d? of pseudo-
documents is formed from this set, such that the
pseudo-document for a particular group gi is simply
the concatenation of the documents in the group:
d?gi =
?
dj?gi
dj .
A location must be associated with each pseudo-
document. This can be chosen based on the parti-
tioning function itself or the locations of the docu-
ments in each group.
Both W&B and SMvZ use uniform grids consist-
ing of cells of equal degree size to partition doc-
uments. We explore an alternative that uses k-d
(k-dimensional) trees to construct a non-uniform
grid that adapts to training sets of different sizes
more gracefully. It ensures a roughly equal num-
ber of documents in each cell, which means that all
pseudo-documents compete on similar footing with
respect to the amount of training data.
W&B define the location for a cell to be its ge-
ographic center, while SMvZ only perform error
analysis in terms of choosing the correct cell. We
obtain consistently improved results using the cen-
troid of the cell?s documents, which takes into ac-
count where the documents are concentrated.
4.1 k-d Trees
A k-d tree is a space-partitioning data structure for
storing points in k-dimensional space, which groups
nearby points into buckets. As one moves down the
tree, the space is split into smaller regions along
chosen dimensions. In this way, it is a generaliza-
tion of a binary search tree to multiple dimensions.
The k-d tree was first introduced by Bentley (1975)
and has since been applied to numerous problems,
e.g. Barnes-Hut simulation (Anderson, 1999) and
nearest-neighbors search (Friedman et al 1977).
Partitioning geolocated documents using a k-d
tree provides finer granularity in dense regions and
coarser granularity elsewhere. For example, doc-
uments from Queens and Brooklyn may show sig-
nificant cultural distinctions, while documents sepa-
rated by the same distance in rural Montana may ap-
pear culturally identical. A uniform grid with large
cells will mash Queens and Brooklyn together, while
small cells will create unnecessarily sparse regions
in Montana.
An important parameter for a k-d tree is its bucket
size, which determines the maximum number of
points (documents in our case) that a cell may con-
tain. By varying the bucket size, the cells can be
made fine- or coarse-grained.
4.2 Partitioning with a k-d Tree
For geolocation, we consider the surface of earth to
be a 2-dimensional space (k=2) over latitude, longi-
tude pairs. We form a k-d tree by a recursive proce-
dure over the training data. Initially, all documents
are placed in the root node of the tree. If the number
of documents in the node exceeds the bucket size,
the node is split into two nodes along a chosen split
dimension and point. This procedure is recursively
called on each of the new child nodes, and repeats
until no node is overflowing. The resulting leaves of
the k-d tree form a patchwork of rectangles which
cover the entire earth.3
When splitting an overflowing node, the choice of
splitting dimension and point can greatly impact the
structure of the resulting k-d tree. Following Fried-
man et al(1977), we choose to always split a node
3We note that the grid ?rectangles? are actually trapezoids
due to the nature of the latitude/longitude coordinate system.
We assume the effect of this is negligible, since most documents
are away from the poles, where distortion is the most extreme.
1503
Figure 1: View of North America showing k-d leaves cre-
ated from GEOWIKI with a bucket size of 600 and the
MIDPOINT method, as visualized in Google Earth.
Figure 2: k-d leaves over the New York City and nearby
areas from the same dataset and parameter settings as in
Figure 1.
along the dimension exhibiting the greatest range of
values. However, there still exist multiple methods
for determining the split point, i.e. the point separat-
ing documents into ?left? and ?right? nodes. In this
paper, we consider two possibilities for selecting this
point: the MIDPOINT method, and the FRIEDMAN
method. The latter splits at the median of all the
points, resulting in an equal number of points in both
the left and right nodes and a perfectly balanced k-d
tree. The former splits at the midpoint between the
two furthest points, allowing for a greater difference
in the number of points in each bin. For geolocation,
the FRIEDMAN splitting method will likely lead to
less sparsity, and therefore more accurate cell selec-
tion. On the other hand, the MIDPOINT method is
likely to draw more geographically desirable bound-
aries.
Figure 1 shows the leaves of the k-d tree formed
over North America using the GEOWIKI dataset,
the MIDPOINT node division method, and a bucket
size of 600. Figure 2 shows the leaves over New
York City and its surrounding area for the same
dataset and settings. More densely populated ar-
eas of the earth (which in turn tend to have more
Wikipedia documents associated with them) contain
smaller and more numerous leaf cells. The cells
over Manhattan are significantly smaller than those
of Queens, the Bronx, and East Jersey, even at such
a coarse bucket size. Though the leaves of the k-d
tree implicitly cover the entire surface of the earth,
our illustrations limit the size of each box by its data,
leaving gaps where no training documents exist.
4.3 Selecting a Representative Location
W&B use the geographic center of a cell as the
geolocation for the pseudo-document it represents.
However, this ignores the fact that many cells will
have imbalances in the dispersion of the documents
they contain ? typically, they will be clumpy, with
documents clustering around areas of high popula-
tion or activity. An alternative is to select the cen-
troid of the locations of all the documents contained
within a cell. Uniform grids with small cells are
not especially sensitive to this choice since the abso-
lute distance between a center or centroid prediction
will not be great, and empty cells are simply dis-
carded. Nonetheless, using the centroid has the ben-
efit of making a uniform grid less sensitive to cell
size, such that larger cells can be used more reliably
? especially important when there are few training
documents.
In contrast, when choosing representative loca-
tions for the leaves of a k-d tree, it is quite important
to use the centroid because the leaves necessarily
span the entire earth and none are discarded (since
all have a roughly similar number of documents in
them). Some areas with low document density are
thus assigned very large cells, such as those over
the oceans, as seen in Figures 1 and 2. Using the
centroid allows these large leaves to be in the mix,
while still predicting the locations in them that have
the greatest document density.
5 Experimental Setup
Configurations. We experiment with several con-
figurations of grids and representative locations.
1504
0 200 400 600 8002
00
250
300
350
Bucket Size
Mean
 Erro
r (km)
ooo
o o
o o
o o
o o
xxx
x
x x
x x x x
xox MidpointFriedman
200 400 600 800 1000 1200
850
900
950
1000
Bucket Size
Mean
 Erro
r (km)
MidpointFriedman
200 400 600 800 1000 1200
1100
1120
1140
1160
1180
Bucket Size
Mean
 Erro
r (km)
MidpointFriedman
(a) (b) (c)
Figure 3: Development set comparisons for (a) GEOWIKI, (b) GEOTEXT, and (c) UTGEO2011-SMALL.
W&B refers to a uniform grid and geographic-
center location selection, UNIFCENTROID to a
uniform grid with centroid location selection,
KDCENTROID to a k-d tree grid with centroid
location selection, and UNIFKDCENTROID to
the union of pseudo-documents constructed by
UNIFCENTROID and KDCENTROID.
We also provide two baselines, both of which are
based on a uniform grid with centroid location selec-
tion. RANDOM predicts a grid cell chosen at random
uniformly; MOSTCOMMONCELL always predicts
the grid cell containing the most training documents.
Note that a most-common k-d leaf baseline does not
make sense, as all k-d leaves contain approximately
the same number of documents.
Evaluation. We use three metrics to measure ge-
olocation performance. The output of each exper-
iment is a predicted coordinate for each test docu-
ment. For each prediction, we compute the error dis-
tance along the surface of the earth to the gold coor-
dinate. We report the mean and median of all such
distances as in W&B and Eisenstein et al(2011).
We also report the fraction of error distances less
than 161 km, corresponding to Cheng et al(2010)?s
measure of predictions within 100 miles of the true
location. This third measure can reveal differences
between models not obvious from just mean and me-
dian.
6 Results
This section provides results for the datasets
described previously: GEOWIKI, GEOTEXT,
UTGEO2011-LARGE and UTGEO2011-SMALL.
We first give details for how we tuned parameters
and algorithmic choices using the development sets,
and then provide performance on the test sets based
on these determinations.
6.1 Tuning
The specific parameters are (1) the partition location
method; (2) the bucket size for k-d partitioning; (3)
the node division method for k-d partitioning; (4) the
degree size for uniform grid partitioning. We tune
with respect to mean error, like W&B.
Partition Location Method. Development set
results show that the centroid always performs bet-
ter than the center for all datasets, typically by a
wide margin (especially for large partition sizes). To
save space, we do not provide details, but point the
reader to the differences in test set results between
W&B and UNIFCENTROID (which are identical ex-
cept that the former uses the center and the latter
uses the centroid) in Tables 1 and 2. All further pa-
rameter tuning is done using the centroid method.
k-d Tree Bucket Size. Bucket size should not be
too large as a proportion of the total number of train-
ing documents. Larger bucket sizes tend to produce
larger leaves, so documents in a partition will have
a higher average distance to the center or centroid
point. This will result in predictions being made at
too coarse a granularity, greatly limiting obtainable
precision even when the correct leaf is chosen.
Conversely, small bucket sizes lead to fewer train-
ing documents per partition. A bucket size of one
reduces to the situation where no pseudo-documents
are used. While this might work well if location pre-
diction is done using the kNNs for a test document, it
1505
Test dataset GEOWIKI GEOTEXT
Method Parameters Mean Med. Acc. Parameters Mean Med. Acc.
RANDOM 0.1? 7056 7145 0.3 5? 2008 1866 1.6
MOSTCOMMONCELL 0.1? 4265 2193 5.0 5? 1158 757 31.3
Eisenstein et al- - - - - 845 501 -
Wing & Baldridge 0.1? 221 11.8 - 5? 967 479 -
UNIFCENTROID 0.1? 181 11.0 90.3 5? 897 432 35.9
KDCENTROID B100, MIDPT. 192 22.5 87.9 B530, FRIED. 958 549 35.3
UNIFKDCENTROID 0.1?, B100, MIDPT. 176 13.4 90.3 5?, B530, FRIED. 890 473 34.1
Table 1: Performance on the held-out test sets of GEOWIKI and GEOTEXT, comparing to the results of Wing and
Baldridge (2011) and Eisenstein et al(2011).
is likely to perform very poorly for the 1NN rule we
adopt. It would also require efficient similarity com-
parisons, using techniques such as locality-sensitive
hashing (Kulis and Grauman, 2009).
The graphs in Figure 3 show development set per-
formance when varying bucket size. For GEOWIKI
and UTGEO2011-LARGE (not shown), increments
of 100 were used, but for the smaller GEOTEXT
and UTGEO2011-SMALL, more fine-grained incre-
ments of 10 were used. In the case of plateaus, as
was common with the FRIEDMAN method, we chose
the middle of the plateau as the bucket size. Overall,
we found optimal bucket sizes of 100 for GEOWIKI,
530 for GEOTEXT, 460 for UTGEO2011-SMALL,
and 1050 for UTGEO2011-LARGE. That the
Wikipedia data requires a smaller bucket size is un-
surprising: the documents themselves are generally
longer and there are many more of them, so a small
bucket size provides good coverage and granularity
without sacrificing the ability to estimate good lan-
guage models for each partition.
Node Division Method. The graphs in Fig-
ure 3 also display the difference between the
two splitting methods. MIDPOINT is clearly bet-
ter for GEOWIKI, while FRIEDMAN is better for
GEOTEXT in the range of bucket sizes produc-
ing the best results. FRIEDMAN is best for
UTGEO2011-LARGE (not shown), but MIDPOINT
is best for UTGEO2011-SMALL.
These results only partly confirm our expecta-
tions. We expected FRIEDMAN to perform bet-
ter on smaller datasets, as it distributes the doc-
uments evenly and avoids many sparsity issues.
We expected MIDPOINT to win on larger datasets,
where all nodes receive plentiful data and the k-d
tree would choose more representative geographical
boundaries.
Cell Size. Following W&B, we choose a
cell degree size of 0.1? for GEOWIKI, and a
cell degree size of 5.0? for GEOTEXT. For
UTGEO2011-LARGE and UTGEO2011-SMALL,
we follow the procedure of W&B, trying sizes
0.1?, 0.5?, 1.0?, 5.0?, and 10.0?, selecting the one
which performed best on the development set. For
UTGEO2011-SMALL, this resulted in coarse cells
of 10.0?, while for UTGEO2011-LARGE, cell sizes
of 0.1? were best.
With these tuned parameters, the average num-
ber of training tokens per k-d leaf was approx-
imately 26k for GEOWIKI, 197k for GEOTEXT,
250k for UTGEO2011-SMALL, and 954k for
UTGEO2011-LARGE.
6.2 Held-out Test Sets
Table 1 shows the performance on the test sets of
GEOWIKI and GEOTEXT of the different configu-
rations, along with that of W&B and Eisenstein et
al. (2011) where possible. The results obtained by
W&B on GEOWIKI are already very strong, but we
do see a clear improvement by changing from the
center-based locations for pseudo-documents they
used to the centroid-based locations we employ:
mean error drops from 221 km to 181 km, and me-
dian error from 11.8 km to 11.0 km. Also, we reduce
the mean error further to 176 km for the configu-
ration that combines the uniform grid and the k-d
partitions, though at the cost of increasing median
error somewhat. The 161 km accuracy is around
90% for all configurations, indicating that the gen-
eral language modeling approach we employ is very
1506
Test dataset UTGEO2011
Training dataset UTGEO2011-SMALL UTGEO2011-LARGE
Method Parameters Mean Med. Acc. Parameters Mean Med. Acc.
RANDOM 10? 1975 1833 2.3 0.1? 1627 1381 2.0
MOSTCOMMONCELL 10? 1522 1186 9.3 0.1? 1525 1185 11.8
Wing & Baldridge 10? 1223 825 3.4 0.1? 956 570 30.9
UNIFCENTROID 10? 1147 782 12.3 0.1? 956 570 30.9
KDCENTROID B460, MIDPT. 1098 733 18.1 B1050, FRIED. 860 463 34.6
UNIFKDCENTROID 10?, B460, MIDPT. 1080 723 18.1 0.1?, B1050, FRIED. 913 532 33.0
Table 2: Performance on the held-out test set of UTGEO2011 for different configurations trained on
UTGEO2011-SMALL (comparable in size to GEOTEXT) and UTGEO2011-LARGE. The numbers given for W&B
were produced from their implementation, and correspond to uniform grid partitioning with locations from centers
rather than centroids.
robust for fact-oriented texts that are rich in explicit
toponyms and geographically relevant named enti-
ties.
For GEOTEXT, the results show that the uniform
grid with centroid locations is the most effective of
our configurations. It improves on Eisenstein et al
(2011) by 69 km with respect to median error, but
has 52 km worse performance than their model with
respect to mean error. This indicates that our model
is generally more accurate, but that it is compara-
tively more wildly off on some documents. Their
model is a sophisticated one that attempts to build
detailed models of the geographic linguistic varia-
tion found in the dataset. Dialectal cues are actually
the most powerful ones in the GEOTEXT dataset,
and it seems our general approach of winner-takes-
all (1NN) hurts performance in this respect, espe-
cially with a very small training set.
Table 2 shows the performance on the test set of
UTGEO2011 with the UTGEO2011-SMALL and
UTGEO2011-LARGE training sets. (Performance
for W&B is obtained from their code.4) With
the small training set, error is worse than with
GEOTEXT, reflecting the wider geographic scope of
UTGEO2011. KDCENTROID is much more effec-
tive than the uniform grids, but combining it with the
uniform grid in UNIFKDCENTROID edges it out by
a small amount. More interestingly, KDCENTROID
is the strongest on all measures when using the large
training set, beating UNIFCENTROID by an even
larger margin for mean and median error than with
4https://bitbucket.org/utcompling/
textgrounder/wiki/WingBaldridge2011
the small training set. The bucket size used with the
large training set is double that for the small one,
but there are many more leaves created since there
are 42 times more training documents. With the ex-
tra data, the model is able to adapt better to the dis-
persion of documents and still have strong language
models for each leaf that work well even with our
greedy winner-takes-all decision method.
Note that the accuracy measurements for all
UTGEO2011 experiments are substantially lower
than those reported by Cheng et al(2010), who
report a best accuracy within 100 miles of 51%.
While UTGEO2011-LARGE contains a substan-
tially larger number of tweets, Cheng et al(2010)
limit themselves to users with at least 1,000
tweets, while we have an average of 85 tweets
per user. Their reported mean error distance of
862 km (versus our best mean of 860 km on
UTGEO2011-LARGE) indicates that their perfor-
mance is hurt by a relatively small number of ex-
tremely incorrect guesses, as ours appears to be.
Figure 4 provides a learning curve on
UTGEO2011?s development set for KDCENTROID.
Performance improves greatly with more data,
indicating that GEOTEXT performance would also
improve with more training data. Parameters, espe-
cially bucket size, need retuning as data increases,
which we hope to estimate automatically in future
work
Finally, we note that the KDCENTROID
method was faster than other methods. While
UNIFCENTROID took nearly 19 hours to com-
plete the test run on GEOWIKI (approximately
1507
0e+00 1e+05 2e+05 3e+05 4e+05
900
950
105
0
Training Set Size (# users)
Mea
n E
rror
 (km
)
l
l
l
l
l
l
l
Figure 4: Learning curve of KDCENTROID on the
UTGEO2011 development set.
1.38 seconds per test document), KDCENTROID
took only 80 minutes (.078 s/doc). Similarly,
UNIFCENTROID took about 67 minutes to
run on UTGEO2011-LARGE (0.34 s/doc), but
KDCENTROID took only 27 minutes (0.014 s/doc).
Generally, the KDCENTROID partitioning results
in fewer cells, and therefore fewer KL-divergence
comparisons. As expected, the UNIFKDCENTROID
model needs as much time as the two together,
taking roughly 21 hours for GEOWIKI (1.52 s/doc)
and 85 minutes for UTGEO2011-LARGE (0.36
s/doc).
7 Discussion
7.1 Error Analysis
We examine some of the greatest error distances
to better understand and improve our models. In
many cases, landmarks in Australia or New Zealand
are predicted in European locations with similarly-
named landmarks, or vice versa ? e.g. the Theatre
Royal, Hobart in Australia is predicted to be in Lon-
don?s theater district, and the Embassy of Australia,
Paris is predicted to be in the capital city of Aus-
tralia. Thus, our model may be inadvertently cap-
turing what Clements et al(2010) call wormholes,
places that are related but not necessarily adjacent.
Some of the other large errors stem from incorrect
gold labels, in particular due to sign errors in latitude
or longitude, which can place documents 10,000 or
more km from their correct locations.
Word Error Word Error
paramus 78 6100 130
ludlow 79 figueroa 133
355 99 dundas 138
ctfuuu 101 120th 139
74th 105 mississauga 140
5701 105 pulaski 144
bloomingdale 122 cucina 146
covina 133 56th 153
lawrenceville 122 403 157
ctfuuuu 124 428 161
Table 3: The 20 words with least average error
(km) in the UTGEO2011 development set, trained
on the UTGEO2011-SMALL training set, using the
KDCENTROID approach with our best parameters. Only
words that occur in at least 10 documents are shown.
Word Error Word Error
seniorpastor 1.1 KS01 2.4
prebendary 1.6 Keio 2.5
Wornham 1.7 Vrah 2.5
Owings 1.9 overspill 2.5
Londoners 2.0 Oriel 2.5
Sandringham 2.1 Holywell 2.6
Sheffield?s 2.2 \?vr&h 2.6
Oxford?s 2.2 operetta 2.6
Belair 2.3 Supertram 2.6
Beckton 2.4 Chanel 2.7
Table 4: Top 20 words with the least average er-
ror (km) in the GEOWIKI development set, using the
UNIFKDCENTROID approach with our best parameters.
Only words occurring in at least 10 documents are shown.
7.2 Most Predictive Words
Our approach relies on the idea that the use of certain
words correlates with a Twitter user or Wikipedia
article?s location. To investigate which words tend
to be good indicators of location, we computed, for
each word in a development set, the average error
distance of documents containing that word. Table 3
gives the 20 words with the least error, among
those that occur in at least 10 documents (users),
for the UTGEO2011 development set, trained on
UTGEO2011-SMALL.
Many of the best words are town names (paramus,
ludlow, bloomingdale), street names (74th, figueroa,
1508
120th), area codes (403), and street numbers (5701,
6100). All are highly locatable terms, as we would
expect. Many of the street addresses are due to
check-ins with the location-based social networking
service Foursquare (e.g. the tweet I?m at Starbucks
(7301 164th Ave NE, Redmond Town Center, Red-
mond)), where the user is literally broadcasting his
or her location. The token ctfuuu(u)?an elongation
of the internet abbreviation ctfu, or cracking the fuck
up?is a dialectal or stylistic feature highly indica-
tive of the Washington, D.C. area.
Similarly, several place names (Wornham, Belair,
Holywell) appear in GEOWIKI. Operettas are a cul-
tural phenomenon largely associated with France,
Germany, and England and particularly with specific
theaters in these countries. However, other highly
specific tokens such as KS01 have a very low aver-
age error because they occur in few documents and
are thus highly unambiguous indicators of location.
Other terms, like seniorpastor and \?vr&h, are due
to extraction errors in the dataset created by W&B,
and are carried along because of a high correlation
with specific documents.
8 Conclusion
We have shown how to construct an adaptive grid
with k-d trees that enables robust text geolocation
and scales well to large training sets. It will be inter-
esting to consider how it interacts with other strate-
gies for improving the IR-based approach. For ex-
ample, the pseudo-document word distributions can
be smoothed based on nearby documents or on the
structure of the k-d tree itself. Integrating our system
with topic models or Bayesian methods would likely
provide more insight with regard to the most dis-
criminative and geolocatable words. We also expect
predicting locations based on multiple most similar
documents (kNN) to be more effective in predict-
ing document location, as the second and third most
similar training documents together may sometimes
be a better estimation of its distribution than just the
first alone. Employing k Nearest Neighbors also al-
lows for more sophisticated methods of location es-
timation than a single leaf?s centroid. Other possi-
bilities include constructing multiple k-d trees using
random subsets of the training data to reduce sensi-
tivity to the bucket size.
In this article, we have considered each user in
isolation. However, Liben-Nowell et al(2005) show
that roughly 70% of social network links can be de-
scribed using geographic information and that the
probability of a social link is inversely proportional
to geographic distance. Backstrom et al(2010) ver-
ify these results on a much larger scale using ge-
olocated Facebook profiles: their algorithm geolo-
cates users with only the social graph and signif-
icantly outperforms IP-based geolocation systems.
Given that both Twitter and Wikipedia have rich,
linked document/user graphs, a natural extension to
our work here will be to combine text and network
prediction for geolocation. Sadilek et al(2012)
also show that a combination of textual and so-
cial data can accurately geolocate individual tweets
when scope is limited to a single city.
Tweets are temporally ordered and the geographic
distance between consecutive tweeting events is
constrained by the author?s movement. For tweet-
level geolocation, it will be useful to build on work
in geolocation that considers the temporal dimen-
sion (Chen and Grauman, 2011; Kalogerakis et al
2009; Sadilek et al 2012) to make better predictions
for documents/images that are surrounded by others
with excellent cues, but which are hard to resolve
themselves.
9 Acknowledgments
We would like to thank Matt Lease and the three
anonymous reviewers for their feedback. This re-
search was supported by a grant from the Morris
Memorial Trust Fund of the New York Community
Trust.
References
Richard J. Anderson. 1999. Tree data structures for
n-body simulation. SIAM Journal on Computing,
28(6):1923?1940.
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
the 19th International Conference on World Wide Web,
pages 61?70.
Jon Louis Bentley. 1975. Multidimensional binary
search trees used for associative searching. Commu-
nications of the ACM, 18(9):509?517.
1509
Chao-Yeh Chen and Kristen Grauman. 2011. Clues from
the beaten path: Location estimation with bursty se-
quences of tourist photos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 1569?1576.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: A content-based approach
to geo-locating twitter users. In Proceedings of the
19th ACM International Conference on Information
and Knowledge Management, pages 759?768.
Martin Clements, Pavel Serdyukov, Arjen P. de Vries, and
Marcel J.T. Reinders. 2010. Finding wormholes with
flickr geotags. In Proceedings of the 32nd European
Conference on Information Retrieval, pages 658?661.
Sebastian Cobarrubias. 2009. Mapping machines: ac-
tivist cartographies of the border and labor lands of
Europe. Ph.D. thesis, University of North Carolina at
Chapel Hill.
Dorin Comaniciu and Peter Meer. 2002. Mean shift: a
robust approach toward feature space analysis. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 24(5):603?619.
Junyan Ding, Luis Gravano, and Narayanan Shivaku-
mar. 2000. Computing geographical scopes of web
resources. In Proceedings of the 26th International
Conference on Very Large Data Bases, pages 545?556.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1277?1287.
Jacon Eisenstein, Ahmed Ahmed, and Eric P. Xing.
2011. Sparse additive generative models of text. In
Proceedings of the 28th International Conference on
Machine Learning, pages 1041?1048.
Jerome H. Friedman, Jon Louis Bentley, and Raphael Ari
Finkel. 1977. An algorithm for finding best matches
in logarithmic expected time. ACM Transactions on
Mathematical Software, 3:209?226.
James Hays and Alexei A. Efros. 2008. im2gps: esti-
mating geographic information from a single image.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1?8.
Evangelos Kalogerakis, Olga Vesselova, James Hays,
Alexei Efros, and Aaron Hertzmann. 2009. Image se-
quence geolocation with human travel priors. In Pro-
ceedings of the IEEE 12th International Conference on
Computer Vision, pages 253?260.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in Glasgow?: Model-
ing locations with tweets. In Proceedings of the 3rd
International Workshop on Search and Mining User-
generated Contents, pages 61?68.
Brian Kulis and Kristen Grauman. 2009. Kernelized
locality-sensitive hashing for scalable image search.
In Proceedings of the 12th International Conference
on Computer Vision, pages 2130?2137.
David Liben-Nowell, Jasmine Novak, Ravi Kumar, Prab-
hakar Raghavan, and Andrew Tomkins. 2005. Geo-
graphic routing in social networks. Proceedings of the
National Academy of Sciences of the United States of
America, 102(33):11623?11628.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proceedings of the 5th ACM Inter-
national Conference on Web Search and Data Mining,
pages 723?732.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings
of the 17th ACM SIGSPATIAL International Confer-
ence on Advances in Geographic Information Systems,
pages 42?51.
Pavel Serdyukov, Vanessa Murdock, and Roelof van
Zwol. 2009. Placing flickr photos on a map. In Pro-
ceedings of the 32nd International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 484?491.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 955?964.
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 334?342.
1510
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146?1157,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Multimodal LDA Model Integrating
Textual, Cognitive and Visual Modalities
Stephen Roller
Department of Computer Science
The University of Texas at Austin
roller@cs.utexas.edu
Sabine Schulte im Walde
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
schulte@ims.uni-stuttgart.de
Abstract
Recent investigations into grounded models of
language have shown that holistic views of
language and perception can provide higher
performance than independent views. In this
work, we improve a two-dimensional multi-
modal version of Latent Dirichlet Allocation
(Andrews et al, 2009) in various ways. (1) We
outperform text-only models in two different
evaluations, and demonstrate that low-level
visual features are directly compatible with
the existing model. (2) We present a novel
way to integrate visual features into the LDA
model using unsupervised clusters of images.
The clusters are directly interpretable and im-
prove on our evaluation tasks. (3) We provide
two novel ways to extend the bimodal mod-
els to support three or more modalities. We
find that the three-, four-, and five-dimensional
models significantly outperform models using
only one or two modalities, and that nontex-
tual modalities each provide separate, disjoint
knowledge that cannot be forced into a shared,
latent structure.
1 Introduction
In recent years, an increasing body of work has been
devoted to multimodal or ?grounded? models of lan-
guage where semantic representations of words are
extended to include perceptual information. The un-
derlying hypothesis is that the meanings of words
are explicitly tied to our perception and understand-
ing of the world around us, and textual-information
alone is insufficient for a complete understanding of
language.
The language grounding problem has come in
many different flavors with just as many different ap-
proaches. Some approaches apply semantic parsing,
where words and sentences are mapped to logical
structure meaning (Kate and Mooney, 2007). Oth-
ers provide automatic mappings of natural language
instructions to executable actions, such as interpret-
ing navigation directions (Chen and Mooney, 2011)
or robot commands (Tellex et al, 2011; Matuszek et
al., 2012). Some efforts have tackled tasks such as
automatic image caption generation (Feng and La-
pata, 2010a; Ordonez et al, 2011), text illustration
(Joshi et al, 2006), or automatic location identifica-
tion of Twitter users (Eisenstein et al, 2010; Wing
and Baldridge, 2011; Roller et al, 2012).
Another line of research approaches grounded
language knowledge by augmenting distributional
approaches of word meaning with perceptual infor-
mation (Andrews et al, 2009; Steyvers, 2010; Feng
and Lapata, 2010b; Bruni et al, 2011; Silberer and
Lapata, 2012; Johns and Jones, 2012; Bruni et al,
2012a; Bruni et al, 2012b; Silberer et al, 2013).
Although these approaches have differed in model
definition, the general goal in this line of research
has been to enhance word meaning with perceptual
information in order to address one of the most com-
mon criticisms of distributional semantics: that the
?meaning of words is entirely given by other words?
(Bruni et al, 2012b).
In this paper, we explore various ways to integrate
new perceptual information through novel computa-
tional modeling of this grounded knowledge into a
multimodal distributional model of word meaning.
The model we rely on was originally developed by
1146
Andrews et al (2009) and is based on a general-
ization of Latent Dirichlet Allocation. This model
has previously been shown to provide excellent per-
formance on multiple tasks, including prediction of
association norms, word substitution errors, seman-
tic inferences, and word similarity (Andrews et al,
2009; Silberer and Lapata, 2012). While prior work
has used the model only with feature norms and vi-
sual attributes, we show that low-level image fea-
tures are directly compatible with the model and
provide improved representations of word meaning.
We also show how simple, unsupervised clusters of
images can act as a semantically useful and qualita-
tively interesting set of features. Finally, we describe
two ways to extend the model by incorporating three
or more modalities. We find that each modality pro-
vides useful but disjoint information for describing
word meaning, and that a hybrid integration of mul-
tiple modalities provides significant improvements
in the representations of word meaning. We release
both our code and data to the community for future
research.1
2 Related Work
The language grounding problem has received sig-
nificant attention in recent years, owed in part to the
wide availability of data sets (e.g. Flickr, Von Ahn
(2006)), computing power, improved computer vi-
sion models (Oliva and Torralba, 2001; Lowe, 2004;
Farhadi et al, 2009; Parikh and Grauman, 2011)
and neurological evidence of ties between the lan-
guage, perceptual and motor systems in the brain
(Pulvermu?ller et al, 2005; Tettamanti et al, 2005;
Aziz-Zadeh et al, 2006).
Many approaches to multimodal research have
succeeded by abstracting away raw perceptual in-
formation and using high-level representations in-
stead. Some works abstract perception via the us-
age of symbolic logic representations (Chen et al,
2010; Chen and Mooney, 2011; Matuszek et al,
2012; Artzi and Zettlemoyer, 2013), while others
choose to employ concepts elicited from psycholin-
guistic and cognition studies. Within the latter cat-
egory, the two most common representations have
been association norms, where subjects are given a
1http://stephenroller.com/research/
emnlp13
cue word and name the first (or several) associated
words that come to mind (e.g., Nelson et al (2004)),
and feature norms, where subjects are given a cue
word and asked to describe typical properties of the
cue concept (e.g., McRae et al (2005)).
Griffiths et al (2007) helped pave the path for
cognitive-linguistic multimodal research, showing
that Latent Dirichlet Allocation outperformed La-
tent Semantic Analysis (Deerwester et al, 1990) in
the prediction of association norms. Andrews et al
(2009) furthered this work by showing that a bi-
modal topic model, consisting of both text and fea-
ture norms, outperformed models using only one
modality on the prediction of association norms,
word substitution errors, and semantic interference
tasks. In a similar vein, Steyvers (2010) showed that
a different feature-topic model improved predictions
on a fill-in-the-blank task. Johns and Jones (2012)
take an entirely different approach by showing that
one can successfully infer held out feature norms
from weighted mixtures based on textual similarity.
Silberer and Lapata (2012) introduce a new method
of multimodal integration based on Canonical Cor-
relation Analysis, and performs a systematic com-
parison between their CCA-based model and others
on association norm prediction, held out feature pre-
diction, and word similarity.
As computer vision techniques have improved
over the past decade, other research has begun di-
rectly using visual information in place of feature
norms. The first work to do this with topic models is
Feng and Lapata (2010b). They use a Bag of Visual
Words (BoVW) model (Lowe, 2004) to create a bi-
modal vocabulary describing documents. The topic
model using the bimodal vocabulary outperforms a
purely textual based model in word association and
word similarity prediction. Bruni et al (2012a) show
how a BoVW model may be easily combined with
a distributional vector space model of language us-
ing only vector concatenation. Bruni et al (2012b)
show that the contextual visual words (i.e. the visual
features around an object, rather than of the object
itself) are even more useful at times, suggesting the
plausibility of a sort of distributional hypothesis for
images. More recently, Silberer et al (2013) show
that visual attribute classifiers, which have been im-
mensely successful in object recognition (Farhadi
et al, 2009), act as excellent substitutes for feature
1147
norms. Other work on modeling the meanings of
verbs using video recognition has also begun show-
ing great promise (Mathe et al, 2008; Regneri et al,
2013).
The Computer Vision community has also bene-
fited greatly from efforts to unify the two modalities.
To name a few examples, Rohrbach et al (2010)
and Socher et al (2013) show how semantic infor-
mation from text can be used to improve zero-shot
classification (i.e., classifying never-before-seen ob-
jects), and Motwani and Mooney (2012) show that
verb clusters can be used to improve activity recog-
nition in videos.
3 Data
Our experiments use several existing and new data
sets for each of our modalities. We employ a large
web corpus and a large set of association norms. We
also introduce two new overlapping data sets: a col-
lection of feature norms and a collection of images
for a number of German nouns.
3.1 Textual Modality
For our Text modality, we use deWaC, a large Ger-
man web corpus created by the WaCKy group (Ba-
roni et al, 2009) containing approximately 1.7B
word tokens. We filtered the corpus by: removing
words with unprintable characters or encoding trou-
bles; removing all stopwords; removing word types
with a total frequency of less than 500; and remov-
ing documents with a length shorter than 100. The
resulting corpus has 1,038,883 documents consist-
ing of 75,678 word types and 466M word tokens.
3.2 Cognitive Modalities
Association Norms (AN) is a collection of asso-
ciation norms collected by Schulte im Walde et al
(2012). In association norm experiments, subjects
are presented with a cue word and asked to list the
first few words that come to mind. With enough sub-
jects and responses, association norms can provide a
common and detailed view of the meaning compo-
nents of cue words. After removing responses given
only once in the entire study, the data set contains
a total of 95,214 cue-response pairs for 1,012 nouns
and 5,716 response types.
Feature Norms (FN) is our new collection of fea-
ture norms for a group of 569 German nouns. We
present subjects on Amazon Mechanical Turk with
a cue noun and ask them to give between 4 and 8
typical descriptive features of the noun. Subjects
are given ten example responses; one such exam-
ple is a cue of Tisch ?table? and a response of hat
Beine ?has legs?. After collection, subjects who
are obvious spammers or did not follow instructions
are manually filtered. Responses are manually cor-
rected for spelling mistakes and semantically nor-
malized.2 Finally, responses which are only given
once in the study are removed. The final data set
contains 11,714 cue-response pairs for 569 nouns
and 2,589 response types.
Note that the difference between association
norms and feature norms is subtle, but important. In
AN collection, subjects simply name related words
as fast as possible, while in FN collection, subjects
must carefully describe the cue.
3.3 Visual Modalities
BilderNetle (?little ImageNet? in Swabian German)
is our new data set of German noun-to-ImageNet
synset mappings. ImageNet is a large-scale and
widely used image database, built on top of Word-
Net, which maps words into groups of images,
called synsets (Deng et al, 2009). Multiple synsets
exist for each meaning of a word. For example, Im-
ageNet contains two different synsets for the word
mouse: one contains images of the animal, while
the other contains images of the computer periph-
eral. This BilderNetle data set provides mappings
from German noun types to images of the nouns via
ImageNet.
Starting with a set of noun compounds and their
nominal constituents von der Heide and Borgwaldt
(2009), five native German speakers and one native
English speaker (including the authors of this paper)
work together to map German nouns to ImageNet
synsets. With the assistance of a German-English
dictionary, the participants annotate each word with
all its possible meanings. After discussing the an-
notations with the German speakers, the English
speaker manually map the word meanings to synset
senses in ImageNet. Finally, the German speakers
review samples of the images for each word to en-
2For brevity, we include the full details of the spammer iden-
tification, cleansing process and normalization techniques in the
Supplementary Materials.
1148
sure the pictures accurately reflect the original noun
in question. Not all words or meanings are mapped
to ImageNet, as there are a number of words with-
out entries in ImageNet, but the resulting data set
contains a considerable amount of polysemy. The fi-
nal data set contains 2022 word-synset mappings for
just 309 words. All but three of these words overlap
with our data set of feature norms. After extract-
ing sections of images using bounding boxes when
available by ImageNet (and using the entire image
when bounding boxes are unavailable), the data set
contains 1,305,602 images.
3.3.1 Image Processing
After the collection of all the images, we extracted
simple, low-level computer vision features to use as
modalities in our experiments.
First, we compute a simple Bag of Visual Words
(BoVW) model for our images using SURF key-
points (Bay et al, 2008). SURF is a method
for selecting points-of-interest within an image. It
is faster and more forgiving than the commonly
known SIFT algorithm. We compute SURF key-
points for every image in our data set using Sim-
pleCV3 and randomly sample 1% of the keypoints.
The keypoints are clustered into 5,000 visual code-
words (centroids) using k-means clustering (Sculley,
2010), and images are then quantized over the 5,000
codewords. All images for a given word are summed
together to provide an average representation for the
word. We refer to this representation as the SURF
modality.
While this is a standard, basic BoVW model,
each individual codeword on its own may not pro-
vide a large degree of semantic information; typi-
cally a BoVW representation acts predominantly as
a feature space for a classifier, and objects can only
be recognize using collections of codewords. To
test that similar concepts should share similar vi-
sual codewords, we cluster the BoVW representa-
tions for all our images into 500 clusters with k-
means clustering, and represent each word as mem-
bership over the image clusters, forming the SURF
Clusters modality. The number of clusters is chosen
arbitrarily. Ideally, each cluster should have a com-
mon object or clear visual attribute, and words are
express in terms of these visual commonalities.
3http://simplecv.org
We also compute GIST vectors (Oliva and Tor-
ralba, 2001) for every image using LearGIST
(Douze et al, 2009). Unlike SURF descriptors,
GIST produces a single vector representation for an
image. The vector does not find points of interest
in the image, but rather attempts to provide a rep-
resentation for the overall ?gist? of the whole im-
age. It is frequently used in tasks like scene iden-
tification, and Deselaers and Ferrari (2011) shows
that distance in GIST space correlates well with se-
mantic distance in WordNet. After computing the
GIST vectors, each textual word is represented as
the centroid GIST vector of all its images, forming
the GIST modality.
Finally, as with the SURF features, we clustered
the GIST representations for our images into 500
clusters, and represented words as membership in
the clusters, forming the GIST Clusters modality.
4 Model Definition
Our experiments are based on the multimodal ex-
tension of Latent Dirichlet Allocation developed by
Andrews et al (2009). Previously LDA has been
successfully used to infer unsupervised joint topic
distributions over words and feature norms together
(Andrews et al, 2009; Silberer and Lapata, 2012).
It has also been shown to be useful in joint infer-
ence of text with visual attributes obtained using vi-
sual classifiers (Silberer et al, 2013). These mul-
timodal LDA models (hereafter, mLDA) have been
shown to be qualitatively sensible and highly pre-
dictive of several psycholinguistic tasks (Andrews et
al., 2009). However, prior work using mLDA is lim-
ited to two modalities at a time. In this section, we
describe bimodal mLDA and define two methods for
extending it to three or more modalities.
4.1 Latent Dirichlet Allocation
Latent Dirichlet Allocation (Blei et al, 2003), or
LDA, is an unsupervised Bayesian probabilistic
model of text documents. It assumes that all docu-
ments are probabilistically generated from a shared
set ofK common topics, where each topic is a multi-
nomial distribution over the vocabulary (notated as
?), and documents are modeled as mixtures of these
shared topics (notated as ?). LDA assumes every
document in the corpus is generated using the fol-
1149
lowing generative process:
1. A document-specific topic distribution, ?d ?
Dir(?) is drawn.
2. For the ith word in the document,
(a) A topic assignment zi ? ?d is drawn,
(b) and a word wi ? ?zi is drawn and ob-
served.
The task of Latent Dirichlet Allocation is then to
automatically infer the latent document distribution
?d for each document d ? D, and the topic distri-
bution ?k for each of the k = {1, . . . ,K} topics,
given the data. The probability that the ith word of
document d is
p(wi, ?d) =
?
k
p(wi|?k)p(zi = k|?d).
4.2 Multimodal LDA
Andrews et al (2009) extend LDA to allow for the
inference of document and topic distributions in a
multimodal corpus. In their model, a document con-
sists of a set of (word, feature) pairs,4 rather than just
words, and documents are still modeled as mixtures
of shared topics. Topics consist of multinomial dis-
tributions over words, ?k, but are extended to also
include multinomial distributions over features, ?k.
The generative process is amended to include these
feature distributions:
1. A document-specific topic distribution, ?d ?
Dir(?) is drawn.
2. For the ith (word, feature) pair in the document,
(a) A topic assignment zi ? ?d is drawn;
(b) a word wi ? ?zi is drawn;
(c) a feature fi ? ?zi is drawn;
(d) the pair (wi, fi) is observed.
The conditional probability of the ith pair (wi, fi)
is updated appropriately:
p(wi, fi, ?d) =
?
k
p(wi|?k)p(fi|?k)p(zi = k|?d).
The key aspect to notice is that the observed
word wi and feature fi are conditionally indepen-
dent given the topic selection, zi. This powerful ex-
tension allows for joint inference over both words
4Here, and elsewhere, feature and f simply refer to a token
from a nontextual modality and should not be confused with the
machine learning sense of feature.
and features, and topics become the key link be-
tween the text and feature modalities.
4.3 3D Multimodal LDA
We can easily extend the bimodal LDA model to in-
corporate three or more modalities by simply per-
forming inference over n-tuples instead of pairs, and
still mandating that each modality is conditionally
independent given the topic. We consider the ith tu-
ple (wi, fi, f ?i , . . .) in document d to have a condi-
tional probability of:
p(wi, fi, f
?
i , . . . , ?d) =
?
k
p(wi|?i)p(fi|?k)p(f
?
i |?
?
i) ? ? ? p(zi = k|?d)
That is, we simply take the original mLDA model
of Andrews et al (2009) and generalize it in the
same way they generalize LDA. At first glance, it
seems that the inference task should become more
difficult as the number of modalities increases and
observed tuples become sparser, but the task remains
roughly the same difficulty, as all of the observed
elements of a tuple are conditionally independent
given the topic assignment zi.
4.4 Hybrid Multimodal LDA
3D Multimodal LDA assumes that all modalities
share the same latent topic structure, ?d. It is pos-
sible, however, that all modalities do not share some
latent structure, but the modalities can still combine
in order to enhance word meaning. The intuition
here is that language usage is guided by all informa-
tion gained in all modalities, but knowledge gained
from one modality may not always relate to another
modality. For example, the color red and the feature
?is sweet? both enhance our understanding of straw-
berries. However, one cannot see that strawberries
are sweet, so one should not correlate the color red
with the feature ?is sweet.?
To this end, we define Hybrid Multimodal LDA.
In this setting, we perform separate, bimodal mLDA
inference according to Section 4.2 for each of the
different modalities, and then concatenate the topic
distributions for the words. In this way, Hybrid
mLDA assumes that every modality shares some la-
tent structure with the text in the corpus, but the la-
tent structures are not shared between non-textual
modalities.
1150
For example, to generate a hybrid model for text,
feature norms and SURF, we separately perform bi-
modal mLDA for the text/feature norms modalities
and the text/SURF modalities. This provides us with
two topic-word distributions: ?FNk,w and ?
S
k?,w, and
the hybrid model is simply the concatenation of the
two distributions,
?FN&Sj,w =
{
?FNj,w 1 ? j ? K
FN ,
?Sj?KFN ,w K
FN < j ? KFN +KS ,
where KFN indicates the number of topics for the
Feature Norm modality, and likewise for KS .
4.5 Inference
Analytical inference of the posterior distribution of
mLDA is intractable, and must be approximated.
Prior work using mLDA has used Gibbs Sampling to
approximate the posterior, but we found this method
did not scale with larger values of K, especially
when applied to the relatively large deWaC corpus.
To solve these scaling issues, we implement On-
line Variational Bayesian Inference (Hoffman et al,
2010; Hoffman et al, 2012) for our models. In
Variational Bayesian Inference (VBI), one approx-
imates the true posterior using simpler distributions
with free variables. The free variables are then op-
timized in an EM-like algorithm to minimize differ-
ence between the true and approximate posteriors.
Online VBI differs from normal VBI by using ran-
domly sampled minibatches in each EM step rather
than the entire data set. Online VBI easily scales
and quickly converges in all of our experiments. A
listing of the inference algorithm may be found in
the Supplementary Materials and the source code is
available as open source.
5 Experimental Setup
5.1 Generating Multimodal Corpora
In order to evaluate our algorithms, we first need to
generate multimodal corpora for each of our non-
textual modalities. We use the same method as An-
drews et al (2009) for generating our multimodal
corpora: for each word token in the text corpus,
a feature is selected stochastically from the word?s
feature distribution, creating a word-feature pair.
Words without grounded features are all given the
same placeholder feature, also resulting in a word-
feature pair.5 That is, for the feature norm modal-
ity, we generate (word, feature norm) pairs; for
the SURF modality, we generate (word, codeword)
pairs, etc. The resulting stochastically generated
corpus is used in its corresponding experiments.
The 3D text-feature-association norm corpus is
generated slightly differently: for each word in
the original text corpus, we check the existence
of multimodal features in either modality. If a
word had no features, it is represented as a triple
(word, placeholderFN , placeholderAN ). If the
word had only feature norms, but no associations,
it is generated as (word, feature, placeholderAN ),
and similarly for association norms without feature
norms. In the case of words with presence in both
modalities, we generate two triples: (word, feature,
placeholderAN ) and (word, placeholderFN , associ-
ation). This allows association norms and feature
norms to influence each other via the document mix-
tures ?, but avoids falsely labeling explicit relation-
ships between randomly selected feature norms and
associations.6 Other 3D corpora are generated using
the same general procedure.
5.2 Evaluation
We evaluate each of our models with two data sets: a
set of compositionality ratings for a number of Ger-
man noun-noun compounds, and the same associa-
tion norm data set used as one of our training modal-
ities in some settings.
Compositionality Ratings is a data set of com-
positionality ratings originally collected by von der
Heide and Borgwaldt (2009). The data set con-
sists of 450 concrete, depictable German noun com-
pounds along with compositionality ratings with re-
gard to their constituents. For each compound, 30
native German speakers are asked to rate how re-
lated the meaning of the compound is to each of
its constituents on a scale from 1 (highly opaque;
entirely noncompositional) to 7 (highly transparent;
very compositional). The mean of the 30 judgments
5Placeholder features must be hardcoded to have equal prob-
ability over all topics to prevent all placeholder pairs from ag-
gregating into a single topic.
6We did try generating the random triples without placehold-
ers, but the generated explicit relationships are overwhelmingly
detrimental in the settings we attempted.
1151
is taken as the gold compositionality rating for each
of the compound-constituent pairs. For example,
Ahornblatt ?maple leaf? is rated highly transparent
with respect to its constituents, Ahorn ?maple? and
Blatt ?leaf?, but Lo?wenzahn ?dandelion? is rated non-
compositional with respect to its constituents, Lo?we
?lion? and Zahn ?tooth?.
We use a subset of the original data, compris-
ing of all two-part noun-noun compounds and their
constituents. This data set consists of 488 com-
positionality ratings (244 compound-head and 244
compound-modifier ratings) for 571 words. 309 of
the targets have images (the entire image data set);
563 have feature norms; and all 571 of have associ-
ation norms.
In order to predict compositional-
ity, for each compound-constituent pair
(wcompound, wconstituent), we compute nega-
tive symmetric KL divergence between the two
words? topic distributions, where symmetric KL
divergence is defined as
sKL(w1||w2) = KL(w1||w2) +KL(w2||w1),
and KL divergence is defined as
KL(w1||w2) =
?
k
ln
(
p(t = k|w1)
p(t = k|w2)
)
p(t = k|w1).
The values of ?sKL for all compound-
constituent word pairs are correlated with the human
judgments of compositionality using Spearman?s ?,
a rank-order correlation coefficient. Note that, since
KL divergence is a measure of dissimilarity, we
use negative symmetric KL divergence so that our
? correlation coefficient is positive. For exam-
ple, we compute both ?sKL(Ahornblatt, Ahorn)
and ?sKL(Ahornblatt, Blatt), and so on for all
488 compound-constituent pairs, and then correlate
these values with the human judgments.
Additionally, we also evaluate using the Associa-
tion Norms data set described in Section 3. Since
it is not sensible to evaluate association norm pre-
diction when they are also used as training data,
we omit this evaluation for this modality. Follow-
ing Andrews et al (2009), we measure association
norm prediction as an average of percentile ranks.
For all possible pairs of words in our vocabulary,
we compute the negative symmetric KL divergence
between the two words. We then compute the per-
centile ranks of similarity for each word pair, e.g.,
?cat? is more similar to ?dog? than 97.3% of the
rest of the vocabulary. We report the weighted mean
percentile ranks for all cue-association pairs, i.e.,
if a cue-association is given more than once, it is
counted more than once.
5.3 Model Selection and Hyperparameter
Optimization
In all settings, we fix all Dirichlet priors at 0.1, use
a learning rate 0.7, and use minibatch sizes of 1024
documents. We do not optimize these hyperparame-
ters or vary them over time. The high Dirichlet pri-
ors are chosen to prevent sparsity in topic distribu-
tions, while the other parameters are selected as the
best from Hoffman et al (2010).
In order to optimize the number of topics K, we
run five trials of each modality for 2000 iterations
for K = {50, 100, 150, 200, 250} (a total of 25
runs per setup). We select the value or K for each
model which minimizes the average perplexity esti-
mate over the five trials.
6 Results
6.1 Predicting Compositionality Ratings
Table 1 shows our results for each of our selected
models with our compositionality evaluation. The
2D models employing feature norms and associa-
tion norms do significantly better than the text-only
model (two-tailed t-test). This result is consistent
with other works using this model with these fea-
tures (Andrews et al, 2009; Silberer and Lapata,
2012).
We also see that the SURF visual words are able
to provide notable, albeit not significant, improve-
ments over the text-only modality. This confirms
that the low-level BoVW features do carry semantic
information, and are useful to consider individually.
The GIST vectors, on the other hand, perform al-
most exactly the same as the text-only model. These
features, which are usually more useful for compar-
ing overall image likeness than object likeness, do
not individually contain semantic information useful
for compositionality prediction.
The performance of the visual modalities reverses
when we look at our cluster-based models. Text
1152
Modality K ?
Text Only
Text Only (LDA) 200 .204
Bimodal mLDA
Text + Feature Norms 150 .310 ***
Text + Assoc. Norms 200 .328 **
Text + SURF 50 .251
Text + GIST 100 .204
Text + SURF Clusters 200 .159
Text + GIST Clusters 150 .233
3D mLDA
Text + FN + AN 250 .259
Text + FN + SURF 100 .286 *
Text + FN + GC 200 .261 *
Hybrid mLDA
FN, AN 150+200 .390 ***
FN, SURF 150+50 .350 ***
FN, GC 150+150 .340 ***
FN, AN, GC 150+200+150 .395 ***
FN, AN, SURF 150+200+50 .404 ***
FN, AN, SURF, GC 150+200+50+150 .406 ***
Table 1: Average rank correlations between
?sKL(wcompound, wconstituent) and our Composi-
tionality gold standard. The Hybrid models are the
concatenation of the corresponding Bimodal mLDA
models. Stars indicate statistical significance compared
to the text-only setting at the .05, .01 and .001 levels
using a two-tailed t-test.
combined with SURF clusters is our worst perform-
ing system, indicating our clusters of images with
common visual words are actively working against
us. The clusters based on GIST, on the other hand,
provide a minor improvement in compositionality
prediction.
All of our 3D models are better than the text-only
model, but they show a performance drop relative
to one or both of their comparable bimodal models.
The model combining text, feature norms, and as-
sociation norms is especially surprising: despite the
excellent performance of each of the bimodal mod-
els, the 3D model performs significantly worse than
either of its components (p < .05). This indicates
that these modalities provide new insight into word
meaning, but cannot be forced into the same latent
structure.
The hybrid models show massive performance in-
Modality K Assoc.
Text Only
Text Only (LDA) 200 .679
Bimodal mLDA
Text + Feature Norms 150 .676
Text + SURF 50 .789 ***
Text + GIST 100 .739 ***
Text + SURF Clusters 200 .618 ***
Text + GIST Clusters 150 .690
3D mLDA
Text + FN + SURF 100 .722 ***
Text + FN + GC 200 .601 ***
Hybrid mLDA
FN, SURF 150+50 .800 ***
FN, GC 150+150 .742 ***
FN, GC, SURF 150+150+50 .804 ***
Table 2: Average predicted rank similarity between cue
words and their associates. Stars indicate statistical sig-
nificance compared to the text-only modality, with gray
stars indicating the model is statistically worse than the
text model. The Hybrid models are the concatenation of
the corresponding Bimodal mLDA models.
creases across the board. Indeed, our 5 modality
hybrid model obtains a performance nearly twice
that of the text-only model. Not only do all 6 hy-
brid models do significantly better than the text-only
models, they show a highly significant improvement
over their individual components (p < .001 for all
16 comparisons). Furthermore, improvements gen-
erally continue to grow significantly with each addi-
tional modality we incorporate into the hybrid model
(p < .001 for all but the .404 to .406 compari-
son, which is not significant). Clearly, there is a
great deal to learn from combining three, four and
even five modalities, but the modalities are learn-
ing disjoint knowledge which cannot be forced into
a shared, latent structure.
6.2 Predicting Association Norms
Table 2 shows the average weighted predicted rank
similarity between all cue words and associates and
trials. Here we see that feature norms do not seem to
be improving performance on the association norms.
This is slightly unexpected, but consistent with the
result that feature norms seem to provide helpful, but
disjoint semantic information as association norms.
1153
We see that the image modalities are much more
useful than they are in compositionality prediction.
The SURF modality does extremely well in partic-
ular, but the GIST features also provide statistically
significant improvements over the text-only model.
Since the SURF and GIST image features tend to
capture object-likeness and scene-likeness respec-
tively, it is possible that words which share asso-
ciates are likely related through common settings
and objects that appear with them. This seems to
provide additional evidence of Bruni et al (2012b)?s
suggestion that something like a distributional hy-
pothesis of images is plausible.
Once again, the clusters of images using SURF
causes a dramatic drop in performance. Combined
with the evidence from the compositionality assess-
ment, this shows that the SURF clusters are actively
confusing the models and not providing semantic in-
formation. GIST clusters, on the other hand, are pro-
viding a marginal improvement over the text-only
model, but the result is not significant. We take a
qualitative look into the GIST clusters in the next
section.
Once again, we see that the 3D models are inef-
fective compared to their bimodal components, but
the hybrid models provide at least as much informa-
tion as their components. The Feature Norms and
GIST Clusters hybrid model significantly improves
over both components.7 The final four-modality hy-
brid significantly outperforms all comparable mod-
els. As with the compositionality evaluation, we
conclude that the image and and feature norm mod-
els are providing disjoint semantic information that
cannot be forced into a shared latent structure, but
still augment each other when combined.
7 Qualitative Analysis of Image Clusters
In all research connecting word meaning with per-
ceptual information, it is desirable that the inferred
representations be directly interpretable. One nice
property of the cluster-based modalities is that we
may represent each cluster as its prototypical im-
ages, and examine whether the prototypes are re-
lated to the topics.
We chose to limit our analysis to the GIST clus-
7The gain is smaller than compared to SURF Hybrid, but
there is much less variance in the trials.
ters for two primary reasons: first, the SURF clusters
did not perform well in our evaluations, and sec-
ond, preliminary investigation into the SURF clus-
ters show that the majority of SURF clusters are
nearly identical. This indicates our SURF clusters
are likely hindered by poor initialization or param-
eter selection, and may partially explain their poor
performance in evaluations.
We select our single best Text + GIST Clusters
trial from the Compositionality evaluation and look
at the topic distributions for words and image clus-
ters. For each topic, we select the three clusters with
the highest weight for the topic, p(c|?k). We extract
the five images closest to the cluster centroids, and
select two topics whose prototypical images are the
most interesting and informative. Figure 1 shows
these selected topics.
The first example topic contains almost exclu-
sively water-related terms. The first image, extracted
from the most probable cluster, does not at first seem
related to water. Upon further inspection, we find
that many of the water-related pictures are scenic
views of lakes and mountains, often containing a
cloudy sky. It seems that the GIST cluster does
not tend to group images of water, but rather nature
scenes that may contain water. This relationship is
more obvious in the second picture, especially when
one considers the water itself contains reflections of
the trees and mountain.
The second topic contains time-related terms. The
?@card@? term is a special token for all non-zero
and non-one numbers. The second word, ?Uhr?, is
polysemous: it can mean clock, an object which tells
the time, or o?clock, as in We meet at 2 o?clock (?Wir
treffen uns um 2 Uhr.?) The three prototypical pic-
tures are not pictures of clocks, but round, detailed
objects similar to clocks. We see GIST has a prefer-
ence toward clustering images based on the predom-
inant shape of the image. Here we see the clusters
of GIST images are not providing a definite seman-
tic relationship, but an overwhelming visual one.
8 Conclusions
In this paper, we evaluated the role of low-level im-
age features, SURF and GIST, for their compatibil-
ity with the multimodal Latent Dirichlet Allocation
model of Andrews et al (2009). We found both fea-
1154
Most Probable Words Translations Prototypical Images
Wasser water
Schiff ship
See lake
Meer sea
Meter meter
Flu? river
@card@ (number)
Uhr clock
Freitag Friday
Sonntag Sunday
Samstag Saturday
Montag Monday
Figure 1: Example topics with prototypical images for the Text + GIST Cluster modality. The first topic shows water-
related words, as well scenes which often appear with water. The second shows clock-like objects, but not clocks.
ture sets were directly compatible with multimodal
LDA and provided significant gains in their ability to
predict association norms over traditional text-only
LDA. SURF features also provided significant gains
over text-only LDA in predicting the compositional-
ity of noun compounds.
We also showed that words may be represented
in terms of membership of image clusters based on
the low-level image features. Image clusters based
on GIST features were qualitatively interesting, and
were able to give improvements over the text-only
model.
Finally, we showed two methods for extending
multimodal LDA to three or more modalities: the
first as a 3D model with a shared latent structure
between all modalities, and the second where latent
structures were inferred separately for each modal-
ity and joined together into a hybrid model. Al-
though the 3D model was unable to compete with
its bimodal components, we found the hybrid model
consistently improved performance over its compo-
nent modalities. We conclude that the combination
of many modalities provides the best representation
of word meaning, and that each nontextual modal-
ity is discovering disjoint information about word
meaning that cannot be forced into a global latent
structure.
Acknowledgments
We would like to thank the UT Natural Lan-
guage Learning Reading Group and the anonymous
EMNLP reviewers for their most helpful comments
and suggestions. We would also like to thank the
members of the SemRel group at IMS for their con-
siderable help in the construction of BilderNetle.
The authors acknowledge the Texas Advanced
Computing Center (TACC) for providing the grid
computing resources necessary for these results.
The research presented in this paper was funded by
the DFG Collaborative Research Centre SFB 732
(Stephen Roller) and the DFG Heisenberg Fellow-
ship SCHU-2580/1-1 (Sabine Schulte im Walde).
References
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly super-
vised learning of semantic parsers for mapping instruc-
tions to actions. In Transactions of the Association for
Computational Linguistics, volume 1, pages 49?62.
Lisa Aziz-Zadeh, Stephen M. Wilson, Giacomo Rizzo-
latti, and Marco Iacoboni. 2006. Congruent embodied
representations for visually presented actions and lin-
guistic phrases describing actions. Current Biology,
16(18):1818?1823.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
1155
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van
Gool. 2008. Surf: Speeded up robust features. Com-
puter Vision and Image Understanding, 110(3):346?
359, June.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. Pro-
ceedings of the EMNLP 2011 Geometrical Models for
Natural Language Semantics, pages 22?32.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012a. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, pages
136?145.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes: Us-
ing image analysis to improve computational represen-
tations of word meaning. In Proceedings of the 20th
ACM International Conference on Multimedia, pages
1219?1228.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. Proceedings of the 25th AAAI
Conference on Artificial Intelligence, pages 859?865,
August.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37(1):397?436.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In Computer Vision and Pat-
tern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 248?255. IEEE.
Thomas Deselaers and Vittorio Ferrari. 2011. Visual and
semantic similarity in imagenet. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
1777?1784.
Matthijs Douze, Herve? Je?gou, Harsimrat Sandhawalia,
Laurent Amsaleg, and Cordelia Schmid. 2009. Eval-
uation of gist descriptors for web-scale image search.
In Proceedings of the ACM International Conference
on Image and Video Retrieval, pages 19:1?19:8.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1277?1287.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In IEEE Conference on Computer Vision and Pattern
Recognition, pages 1778?1785.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1239?1249.
Yansong Feng and Mirella Lapata. 2010b. Visual in-
formation in semantic representation. In Human Lan-
guage Technologies: the 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91?99.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211.
Matthew Hoffman, David M. Blei, and Francis Bach.
2010. Online learning for latent dirichlet alocation.
Advances in Neural Information Processing Systems,
23:856?864.
Matthew Hoffman, David M. Blei, Chong Wang, and
John Paisley. 2012. Stochastic variational inference.
ArXiv e-prints, June.
Brendan T. Johns and Michael N. Jones. 2012. Percep-
tual inference through global lexical similarity. Topics
in Cognitive Science, 4(1):103?120.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The story
picturing engine?a system for automatic text illustra-
tion. ACM Transactions on Multimedia Computing,
Communications, and Applications, 2(1):68?89.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambiguous supervision. In
Proceedings of the 22nd Conference on Artificial In-
telligence, volume 7, pages 895?900.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91?110.
Stefan Mathe, Afsaneh Fazly, Sven Dickinson, and
Suzanne Stevenson. 2008. Learning the abstract mo-
tion semantics of verbs from captioned videos. In
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition Workshops, pages 1?8.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and
Dieter Fox. 2012. Learning to parse natural language
commands to a robot control system. In Proceedings
of the 13th International Symposium on Experimental
Robotics.
1156
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Tanvi S. Motwani and Raymond J. Mooney. 2012. Im-
proving video activity recognition using object recog-
nition and text mining. In ECAI, pages 600?605.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 2004. The University of South Florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, & Computers,
36(3):402?407.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42(3):145?175.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Infor-
mation Processing Systems, pages 1143?1151.
Devi Parikh and Kristen Grauman. 2011. Relative at-
tributes. In International Conference on Computer Vi-
sion, pages 503?510. IEEE.
Friedemann Pulvermu?ller, Olaf Hauk, Vadim V. Nikulin,
and Risto J Ilmoniemi. 2005. Functional links be-
tween motor and language systems. European Journal
of Neuroscience, 21(3):793?797.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
2013. Grounding action descriptions in videos. In
Transactions of the Association for Computational
Linguistics, volume 1, pages 25?36.
Marcus Rohrbach, Michael Stark, Gyo?rgy Szarvas, Iryna
Gurevych, and Bernt Schiele. 2010. What helps
where?and why? Semantic relatedness for knowledge
transfer. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 910?917.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1500?1510.
Sabine Schulte im Walde, Susanne Borgwaldt, and
Ronny Jauch. 2012. Association norms of german
noun compounds. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 632?639, Istanbul, Turkey.
D. Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th International Conference on
World Wide Web, pages 1177?1178.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433, Jeju
Island, Korea, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of the 51th Annual Meet-
ing of the Association for Computational Linguistics,
Sofia, Bulgaria, August.
Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert
Bastani, Christopher D. Manning, and Andrew Y. Ng.
2013. Zero-shot learning through cross-modal trans-
fer. International Conference on Learning Represen-
tations.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?243.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. Proceedings of the 25th AAAI
Conference on Artificial Intelligence.
Marco Tettamanti, Giovanni Buccino, Maria Cristina
Saccuman, Vittorio Gallese, Massimo Danna, Paola
Scifo, Ferruccio Fazio, Giacomo Rizzolatti, Stefano F.
Cappa, and Daniela Perani. 2005. Listening to action-
related sentences activates fronto-parietal motor cir-
cuits. Journal of Cognitive Neuroscience, 17(2):273?
281.
Luis Von Ahn. 2006. Games with a purpose. Computer,
39(6):92?94.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages 51?
74.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, volume 11, pages 955?964.
1157
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796?801,
Dublin, Ireland, August 23-24, 2014.
UTexas: Natural Language Semantics using Distributional Semantics and
Probabilistic Logic
Islam Beltagy
?
, Stephen Roller
?
, Gemma Boleda
?
, Katrin Erk
?
, Raymond J. Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
{beltagy, roller, mooney}@cs.utexas.edu
gemma.boleda@upf.edu, katrin.erk@mail.utexas.edu
Abstract
We represent natural language semantics
by combining logical and distributional in-
formation in probabilistic logic. We use
Markov Logic Networks (MLN) for the
RTE task, and Probabilistic Soft Logic
(PSL) for the STS task. The system is
evaluated on the SICK dataset. Our best
system achieves 73% accuracy on the RTE
task, and a Pearson?s correlation of 0.71 on
the STS task.
1 Introduction
Textual Entailment systems based on logical infer-
ence excel in correct reasoning, but are often brit-
tle due to their inability to handle soft logical in-
ferences. Systems based on distributional seman-
tics excel in lexical and soft reasoning, but are un-
able to handle phenomena like negation and quan-
tifiers. We present a system which takes the best
of both approaches by combining distributional se-
mantics with probabilistic logical inference.
Our system builds on our prior work (Belt-
agy et al., 2013; Beltagy et al., 2014a; Beltagy
and Mooney, 2014; Beltagy et al., 2014b). We
use Boxer (Bos, 2008), a wide-coverage semantic
analysis tool to map natural sentences to logical
form. Then, distributional information is encoded
in the form of inference rules. We generate lexical
and phrasal rules, and experiment with symmetric
and asymmetric similarity measures. Finally, we
use probabilistic logic frameworks to perform in-
ference, Markov Logic Networks (MLN) for RTE,
and Probabilistic Soft Logic (PSL) for STS.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature.
2.2 Distributional Semantics
Distributional models use statistics of word co-
occurrences to predict semantic similarity of
words and phrases (Turney and Pantel, 2010;
Mitchell and Lapata, 2010), based on the obser-
vation that semantically similar words occur in
similar contexts. Words are represented as vec-
tors in high dimensional spaces generated from
their contexts. Also, it is possible to compute vec-
tor representations for larger phrases composition-
ally from their parts (Mitchell and Lapata, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010). Distributional similarity is usually a
mixture of semantic relations, but particular asym-
metric similarity measures can, to a certain ex-
tent, predict hypernymy and lexical entailment
distributionally (Kotlerman et al., 2010; Lenci and
Benotto, 2012; Roller et al., 2014). Distribu-
tional models capture the graded nature of mean-
ing, but do not adequately capture logical struc-
ture (Grefenstette, 2013).
2.3 Markov Logic Network
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
796
MLNs define a probability distribution over pos-
sible worlds, where the probability of a world in-
creases exponentially with the total weight of the
logical clauses that it satisfies. A variety of in-
ference methods for MLNs have been developed,
however, computational overhead is still an issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms (i.e., atoms without vari-
ables) have soft, continuous truth values on the
interval [0, 1] rather than binary truth values as
used in MLNs and most other probabilistic logics.
Given a set of weighted inference rules, and with
the help of Lukasiewicz?s relaxation of the logical
operators, PSL builds a graphical model defining a
probability distribution over the continuous space
of values of the random variables in the model
(Kimmig et al., 2012). Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the overall
interpretation with the maximum probability given
a set of evidence. This optimization problem is a
second-order cone program (SOCP) (Kimmig et
al., 2012) and can be solved in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or is not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
3.1 Logical Representation
The first component in the system is Boxer (Bos,
2008), which maps the input sentences into logical
form, in which the predicates are words in the sen-
tence. For example, the sentence ?A man is driving
a car? in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
3.2 Distributional Representation
Next, distributional information is encoded in
the form of weighted inference rules connecting
words and phrases of the input sentences T and H .
For example, for sentences T : ?A man is driving
a car?, and H: ?A guy is driving a vehicle?, we
would like to generate rules like ?x. man(x) ?
guy(x) |w
1
, ?x.car(x)? vehicle(x) |w
2
, where
w
1
and w
2
are weights indicating the similarity of
the antecedent and consequent of each rule.
Inferences rules are generated as in Beltagy et
al. (2013). Given two input sentences T and H ,
for all pairs (a, b), where a and b are words or
phrases of T and H respectively, generate an infer-
ence rule: a ? b | w, where the rule weight w is
a function of sim(
??
a ,
??
b ), and sim is a similarity
measure of the distributional vectors
??
a ,
??
b . We
experimented with the symmetric similarity mea-
sure cosine, and asym, the supervised, asymmet-
ric similarity measure of Roller et al. (2014).
The asym measure uses the vector difference
(
??
a ?
??
b ) as features in a logistic regression clas-
sifier for distinguishing between four different
word relations: hypernymy, cohyponymy, meron-
omy, and no relation. The model is trained us-
ing the noun-noun subset of the BLESS data set
(Baroni and Lenci, 2011). The final similarity
weight is given by the model?s estimated probabil-
ity that the word relationship is either hypernymy
or meronomy: asym(
??
a ,
??
b ) = P (hyper(a, b))+
P (mero(a, b)).
Distributional representations for words are de-
rived by counting co-occurrences in the ukWaC,
WaCkypedia, BNC and Gigaword corpora. We
use the 2000 most frequent content words as ba-
sis dimensions, and count co-occurrences within
a two word context window. The vector space is
weighted using Positive Pointwise Mutual Infor-
mation.
Phrases are defined in terms of Boxer?s output
to be more than one unary atom sharing the same
variable like ?a little kid? (little(k) ? kid(k)),
or two unary atoms connected by a relation like
?a man is driving? (man(m) ? agent(d,m) ?
drive(d)). We compute vector representations of
797
phrases using vector addition across the compo-
nent predicates. We also tried computing phrase
vectors using component-wise vector multiplica-
tion (Mitchell and Lapata, 2010), but found it per-
formed marginally worse than addition.
3.3 Probabilistic Logical Inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
3.4 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE classification prob-
lem for the relation between T and H can be
split into two inference tasks. The first is test-
ing if T entails H , Pr(H|T,RB). The second
is testing if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this in-
dicates Contradicts. If both values are close, this
means T does not affect the probability of H and
indicative of Neutral. We train an SVM classifier
with LibSVM?s default parameters to map the two
probabilities to the final decision.
The MLN implementation we use is
Alchemy (Kok et al., 2005). Queries in Alchemy
can only be ground atoms. However, in our
case the query is a complex formula (H). We
extended Alchemy to calculate probabilities of
queries (Beltagy and Mooney, 2014). Probability
of a formula Q given an MLN K equals the ratio
between the partition function Z of the ground
network of K with and without Q added as a hard
rule (Gogate and Domingos, 2011)
P (Q | K) =
Z(K ? {(Q,?)})
Z(K)
(1)
We estimate Z of the ground networks using Sam-
pleSearch (Gogate and Dechter, 2011), an ad-
vanced importance sampling algorithm that is suit-
able for ground networks generated by MLNs.
A general problem with MLN inference is
its computational overhead, especially for the
complex logical formulae generated by our ap-
proach. To make inference faster, we reduce the
size of the ground network through an automatic
type-checking technique proposed in Beltagy and
Mooney (2014). For example, consider the ev-
idence ground atom man(M) denoting that the
constant M is of type man. Then, consider an-
other predicate like car(x). In case there are no in-
ference rule connecting man(x) and car(x), then
we know that M which we know is a man cannot
be a car, so we remove the ground atom car(M)
from the ground network. This technique reduces
the size of the ground network dramatically and
makes inference tractable.
Another problem with MLN inference is that
quantifiers sometimes behave in an undesir-
able way, due to the Domain Closure Assump-
tion (Richardson and Domingos, 2006) that MLNs
make. For example, consider the text-hypothesis
pair: ?There is a black bird? and ?All birds are
black?, which in logic are T : bird(B)?black(B)
and H : ?x. bird(x) ? black(x). Because of
the Domain Closure Assumption, MLNs conclude
that T entails H because H is true for all constants
in the domain (in this example, the single constant
B). We solve this problem by introducing extra
constants and evidence in the domain. In the ex-
ample above, we introduce evidence of a new bird
bird(D), which prevents the hypothesis from be-
ing true. The full details of the technique of deal-
ing with the domain closure is beyond the scope of
this paper.
3.5 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach for computing similarity between struc-
tured objects. We showed in Beltagy et al. (2014a)
how to perform the STS task using PSL. PSL
does not work ?out of the box? for STS, be-
cause Lukasiewicz?s equation for the conjunction
is very restrictive. We address this by replacing
Lukasiewicz?s equation for conjunction with an
averaging equation, then change the optimization
problem and grounding technique accordingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and an-
other where E = S
2
, Q = S
1
, and output the two
scores. The final similarity score is produced from
798
an Additive Regression model with WEKA?s de-
fault parameters trained to map the two PSL scores
to the overall similarity score (Friedman, 1999;
Hall et al., 2009).
3.6 Task 3: RTE and STS using Vector
Spaces and Keyword Counts
As a baseline, we also attempt both the RTE and
STS tasks using only vector representations and
unigram counts. This baseline model uses a super-
vised regressor with features based on vector sim-
ilarity and keyword counts. The same input fea-
tures are used for performing RTE and STS, but a
SVM classifier and Additive Regression model is
trained separately for each task. This baseline is
meant to establish whether the task truly requires
the sophisticated logical inference of MLNs and
PSL, or if merely checking for logical keywords
and textual similarity is sufficient.
The first two features are simply the cosine and
asym similarities between the text and hypothesis,
using vector addition of the unigrams to compute
a single vector for the entire sentence.
We also compute vectors for both the text and
hypothesis using vector addition of the mutually
exclusive unigrams (MEUs). The MEUs are de-
fined as the unigrams of the premise and hypoth-
esis with common unigrams removed. For exam-
ple, if the premise is ?A dog chased a cat? and the
hypothesis is ?A dog watched a mouse?, the MEUs
are ?chased cat? and ?watched mouse.? We com-
pute vector addition of the MEUs, and compute
similarity using both the cosine and asym mea-
sures. These form two features for the regressor.
The last feature of the model is a keyword
count. We count how many times 13 different
keywords appear in either the text or the hypoth-
esis. These keywords include negation (no, not,
nobody, etc.) and quantifiers (a, the, some, etc.)
The counts of each keyword form the last 13 fea-
tures as input to the regressor. In total, there are
17 features used in this baseline system.
4 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014 (Marelli et al.,
2014a; Marelli et al., 2014b). The dataset is
10,000 pairs of sentences, 5000 training and 5000
for testing. Sentences are annotated for both tasks.
SICK-RTE SICK-STS
Baseline 70.0 71.1
MLN/PSL + Cosine 72.8 68.6
MLN/PSL + Asym 73.2 68.9
Ensemble 73.2 71.5
Table 1: Test RTE accuracy and STS Correlation.
4.1 Systems Compared
We compare multiple configurations of our proba-
bilistic logic system.
? Baseline: Vector- and keyword-only baseline
described in Section 3.6;
? MLN/PSL + Cosine: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using cosine as a similarity measure;
? MLN/PSL + Asym: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using asym as a similarity measure;
? Ensemble: An ensemble method which uses
all of the features in the above methods as in-
puts for the RTE and STS classifiers.
4.2 Results and Discussion
Table 1 shows our results on the held-out test set
for SemEval 2014 Task 1.
On the RTE task, we see that both the MLN +
Cosine and MLN + Asym models outperformed
the Baseline, indicating that textual entailment re-
quires real inference to handle negation and quan-
tifiers. The MLN + Asym and Ensemble sys-
tems perform identically on RTE, further suggest-
ing that the logical inference subsumes keyword
detection.
The MLN + Asym system outperforms the
MLN + Cosine system, emphasizing the impor-
tance of asymmetric measures for predicting lex-
ical entailment. Intuitively, this makes perfect
sense: dog entails animal, but not vice versa.
In an error analysis performed on a development
set, we found our RTE system was extremely con-
servative: we rarely confused the Entails and Con-
tradicts classes, indicating we correctly predict the
direction of entailment, but frequently misclassify
examples as Neutral. An examination of these ex-
amples showed the errors were mostly due to miss-
ing or weakly-weighted distributional rules.
On STS, our vector space baseline outperforms
both PSL-based systems, but the ensemble outper-
forms any of its components. This is a testament to
799
the power of distributional models in their ability
to predict word and sentence similarity. Surpris-
ingly, we see that the PSL + Asym system slightly
outperforms the PSL + Cosine system. This may
indicate that even in STS, some notion of asymme-
try plays a role, or that annotators may have been
biased by simultaneously annotating both tasks.
As with RTE, the major bottleneck of our system
appears to be the knowledge base, which is built
solely using distributional inference rules.
Results also show that our system?s perfor-
mance is close to the baseline system. One of
the reasons behind that could be that sentences are
not exploiting the full power of logical represen-
tations. On RTE for example, most of the con-
tradicting pairs are two similar sentences with one
of them being negated. This way, the existence
of any negation cue in one of the two sentences is
a strong signal for contradiction, which what the
baseline system does without deeply representing
the semantics of the negation.
5 Conclusion & Future Work
We showed how to combine logical and distribu-
tional semantics using probabilistic logic, and how
to perform the RTE and STS tasks using it. The
system is tested on the SICK dataset.
The distributional side can be extended in many
directions. We would like to use longer phrases,
more sophisticated compositionality techniques,
and contextualized vectors of word meaning. We
also believe inference rules could be dramatically
improved by integrating from paraphrases collec-
tions like PPDB (Ganitkevitch et al., 2013).
Finally, MLN inference could be made more ef-
ficient by exploiting the similarities between the
two ground networks (the one with Q and the one
without). PLS inference could be enhanced by us-
ing a learned, weighted average of rules, rather
than the simple mean.
Acknowledgements
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Some experiments were run on the Mastodon
Cluster supported by NSF Grant EIA-0303609.
The authors acknowledge the Texas Advanced
Computing Center (TACC)
1
for providing grid re-
sources that have contributed to these results. We
thank the anonymous reviewers and the UTexas
1
http://www.tacc.utexas.edu
Natural Language and Learning group for their
helpful comments and suggestions.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10, Edinburgh, UK, July. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy and Raymond J. Mooney. 2014. Ef-
ficient Markov logic inference for natural language
semantics. In Proceedings of AAAI 2014 Workshop
on Statistical Relational AI (StarAI-14).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014a. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014b. Semantic parsing using distributional se-
mantics and probabilistic logic. In Proceedings
of ACL 2014 Workshop on Semantic Parsing (SP-
2014).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
J.H. Friedman. 1999. Stochastic gradient boosting.
Technical report, Stanford University.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Vibhav Gogate and Rina Dechter. 2011. Sample-
search: Importance sampling in presence of deter-
minism. Artificial Intelligence, 175(2):694?729.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In 27th Conference on Un-
certainty in Artificial Intelligence (UAI-11).
800
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Stanley Kok, Parag Singla, Matthew Richardson, and
Pedro Domingos. 2005. The Alchemy system
for statistical relational AI. Technical report, De-
partment of Computer Science and Engineering,
University of Washington. http://www.cs.
washington.edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation
of compositional distributional semantic models.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Hrafn Loftsson, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, Ice-
land, may. European Language Resources Associa-
tion (ELRA).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(3):1388?1429.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Twenty
Fifth International Conference on Computational
Linguistics (COLING-14), Dublin, Ireland.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
801
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 32?41,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
The (Un)expected Effects of Applying Standard Cleansing Models to
Human Ratings on Compositionality
Stephen Roller?? Sabine Schulte im Walde ? Silke Scheible ?
?Department of Computer Science ?Institut fu?r Maschinelle Sprachverarbeitung
The University of Texas at Austin Universita?t Stuttgart
roller@cs.utexas.edu {schulte,scheible}@ims.uni-stuttgart.de
Abstract
Human ratings are an important source for
evaluating computational models that predict
compositionality, but like many data sets of
human semantic judgements, are often fraught
with uncertainty and noise. However, despite
their importance, to our knowledge there has
been no extensive look at the effects of cleans-
ing methods on human rating data. This paper
assesses two standard cleansing approaches on
two sets of compositionality ratings for Ger-
man noun-noun compounds, in their ability
to produce compositionality ratings of higher
consistency, while reducing data quantity. We
find (i) that our ratings are highly robust
against aggressive filtering; (ii) Z-score filter-
ing fails to detect unreliable item ratings; and
(iii) Minimum Subject Agreement is highly
effective at detecting unreliable subjects.
1 Introduction
Compounds have long been a reoccurring focus of
attention within theoretical, cognitive, and compu-
tational linguistics. Recent manifestations of inter-
est in compounds include the Handbook of Com-
pounding (Lieber and Stekauer, 2009) on theoretical
perspectives, and a series of workshops1 and spe-
cial journal issues with respect to the computational
perspective (Journal of Computer Speech and Lan-
guage, 2005; Language Resources and Evaluation,
2010; ACM Transactions on Speech and Language
Processing, to appear). Some work has focused
on modeling meaning and compositionality for spe-
cific classes, such as particle verbs (McCarthy et al,
1www.multiword.sourceforge.net
2003; Bannard, 2005; Cook and Stevenson, 2006);
adjective-noun combinations (Baroni and Zampar-
elli, 2010; Boleda et al, 2013); and noun-noun com-
pounds (Reddy et al, 2011b; Reddy et al, 2011a).
Others have aimed at predicting the compositional-
ity of phrases and sentences of arbitrary type and
length, either by focusing on the learning approach
(Socher et al, 2011); by integrating symbolic mod-
els into distributional models (Coecke et al, 2011;
Grefenstette et al, 2013); or by exploring the arith-
metic operations to predict compositionality by the
meaning of the parts (Widdows, 2008; Mitchell and
Lapata, 2010).
An important resource in evaluating composition-
ality has been human compositionality ratings, in
which human subjects are asked to rate the degree to
which a compound is transparent or opaque. Trans-
parent compounds, such as raincoat, have a meaning
which is an obvious combination of its constituents,
e.g., a raincoat is a coat against the rain. Opaque
compounds, such as hot dog, have little or no rela-
tion to one or more of their constituents: a hot dog
need not be hot, nor is it (hopefully) made of dog.
Other words, such as ladybug, are transparent with
respect to just one constituent. As many words do
not fall clearly into one category or the other, sub-
jects are typically asked to rate the compositionality
of words or phrases on a scale, and the mean of sev-
eral judgements is taken as the gold standard.
Like many data sets of human judgements, com-
positionality ratings can be fraught with large quan-
tities of uncertainty and noise. For example, partici-
pants typically agree on items that are clearly trans-
parent or opaque, but will often disagree about the
32
gray areas in between. Such uncertainty represents
an inherent part of the semantic task and is the major
reason for using the mean ratings of many subjects.
Other types of noise, however, are undesirable,
and should be eliminated. In particular, we wish
to examine two types of potential noise in our data.
The first type of noise (Type I noise: uncertainty),
comes from when a subject is unfamiliar or un-
certain about particular words, resulting in sporad-
ically poor judgements. The second type of noise
(Type II noise: unreliability), occurs when a sub-
ject is consistently unreliable or uncooperative. This
may happen if the subject misunderstands the task,
or if a subject simply wishes to complete the task
as quickly as possible. Judgements collected via
crowdsourcing are especially prone to this second
kind of noise, when compared to traditional pen-
and-paper experiments, since participants aim to
maximize their hourly wage.2
In this paper, we apply two standard cleans-
ing methods (Ben-Gal, 2005; Maletic and Marcus,
2010), that have been used on similar rating data be-
fore (Reddy et al, 2011b), on two data sets of com-
positionality ratings of German noun-noun com-
pounds. We aim to address two main points. The
first is to assess the cleansing approaches in their
ability to produce compositionality ratings of higher
quality and consistency, while facing a reduction of
data mass in the cleansing process. In particular, we
look at the effects of removing outlier judgements
resulting from uncertainty (Type I noise) and drop-
ping unreliable subjects (Type II noise). The second
issue is to assess the overall reliability of our two
rating data sets: Are they clean enough to be used
as gold standard models in computational linguistics
approaches?
2 Compositionality Ratings
Our focus of interest is on German noun-noun com-
pounds (see Fleischer and Barz (2012) for a detailed
overview), such as Ahornblatt ?maple leaf? and
Feuerwerk ?fireworks?, and Obstkuchen ?fruit cake?
where both the head and the modifier are nouns.
We rely on a subset of 244 noun-noun compounds
2See Callison-Burch and Dredze (2010) for a collection of
papers on data collected with AMT. While the individual ap-
proaches deal with noise in individual ways, there is no general
approach to clean crowdsourcing data.
collected by von der Heide and Borgwaldt (2009),
who created a set of 450 concrete, depictable Ger-
man noun compounds according to four composi-
tionality classes (transparent+transparent, transpar-
ent+opaque, opaque+transparent, opaque+opaque).
We are interested in the degrees of composition-
ality of the German noun-noun compounds, i.e., the
relation between the meaning of the whole com-
pound (e.g., Feuerwerk) and the meaning of its con-
stituents (e.g., Feuer ?fire? and Werk ?opus?). We
work with two data sets of compositionality rat-
ings for the compounds. The first data set, the
individual compositionality ratings, consists of
participants rating the compositionality of a com-
pound with respect to each of the individual con-
stituents. These judgements were collected within
a traditional controlled, pen-and-paper setting. For
each compound-constituent pair, 30 native German
speakers rated the compositionality of the com-
pound with respect to its constituent on a scale
from 1 (opaque/non-compositional) to 7 (transpar-
ent/compositional). The subjects were allowed to
omit ratings for unfamiliar words, but very few did;
of the 14,640 possible ratings judgements, only 111
were left blank. Table 1 gives several examples of
such ratings. We can see that Fliegenpilz ?toadstool?
is an example of a very opaque (non-compositional)
word with respect to Fliege ?housefly/bow tie?; it has
little to do with either houseflies or bow ties. On
the other hand Teetasse ?teacup? is highly composi-
tional: it is a Tasse ?cup? intended for Tee ?tea?.
The second data set, the whole compositional-
ity ratings consists of participants giving a single
rating for the entire compound. These ratings, pre-
viously unpublished, reflect a very different view
of the same compounds. Rather than rating com-
pounds with respect to their constituents, subjects
were asked to give a single rating for the entire com-
pound using the same 1-7 scale as before. The rat-
ings were collected via Amazon Mechanical Turk
(AMT). The data was controlled for spammers by
removing subjects who failed to identify a number
of fake words. Subjects who rated less than 10 com-
pounds or had a low AMT reputation were also re-
moved. The resulting data represents 150 differ-
ent subjects with roughly 30 ratings per compound.
Most participants rated only a few dozen items. We
can see examples of these ratings in Table 2.
33
Compound W.R.T. Subject 1 Subject 2 Subject 3 Subject 4 Mean Comb.
Fliegenpilz ?toadstool? Fliege ?housefly/bow tie? 3 1 1 2 1.75
3.37
Fliegenpilz ?toadstool? Pilz ?mushroom? 5 7 7 7 6.50
Sonnenblume ?sunflower? Sonne ?sun? 4 3 1 2 2.50
4.11
Sonnenblume ?sunflower? Blume ?flower? 7 7 7 6 6.75
Teetasse ?teacup? Tee ?tea? 6 6 4 2 4.50
4.50
Teetasse ?teacup? Tasse ?cup? 7 6 4 1 4.50
Table 1: Sample compositionality ratings for three compounds with respect to their constituents. We list the mean rat-
ing for only these 4 subjects to facilitate examples. The Combined column is the geometric mean of both constituents.
Compound Subject 1 Subject 2 Subject 3 Subject 4 Mean
Fliegenpilz ?toadstool? - 2 1 2 2.67
Sonnenblume ?sunflower? 3 3 1 2 2.75
Teetasse ?teacup? 7 7 7 6 6.75
Table 2: Example whole compositionality ratings for three compounds. Note that Subject 1 chose not to rate Fliegen-
pilz, so the mean is computed using only the three available judgements.
3 Methodology
In order to check on the reliability of composition-
ality judgements in general terms as well as with re-
gard to our two specific collections, we applied two
standard cleansing approaches3 to our rating data: Z-
score filtering is a method for filtering Type I noise,
such as random guesses made by individuals when a
word is unfamiliar. Minimum Subject Agreement is
a method for filtering out Type II noise, such as sub-
jects who seem to misunderstand the rating task or
rarely agree with the rest of the population. We then
evaluated the original vs. cleaned data by one intrin-
sic and one extrinsic task. Section 3.1 presents the
two evaluations and the unadulterated, baseline mea-
sures for our experiments. Sections 3.2.1 and 3.2.2
describe the cleansing experiments and results.
3.1 Evaluations and Baselines
For evaluating the cleansing methods, we propose
two metrics, an intrinsic and an extrinsic measure.
3.1.1 Intrinsic Evaluation:
Consistency between Rating Data Sets
The intrinsic evaluation measures the consistency
between our two ratings sets individual and whole.
Assuming that the compositionality ratings for a
compound depend heavily on both constituents, we
expect a strong correlation between the two data
sets. For a compound to be rated transparent as a
3See Ben-Gal (2005) or Maletic and Marcus (2010) for
overviews of standard cleansing approaches.
whole, it should be transparent with respect to both
of its constituents. Compounds which are highly
transparent with respect to only one of their con-
stituents should be penalized appropriately.
In order to compute a correlation between the
whole ratings (which consist of one average rating
per compound) and the individual ratings (which
consist of two average ratings per compound, one for
each constituent), we need to combine the individual
ratings to arrive at a single value. We use the geo-
metric mean to combine the ratings, which is effec-
tively identical to the multiplicative methods in Wid-
dows (2008), Mitchell and Lapata (2010) and Reddy
et al (2011b). 4 For example, using our means listed
in Table 1, we may compute the combined rating for
Sonnenblume as
?
6.75 ? 2.50 ? 4.11. These com-
bined ratings are computed for all compounds, as
listed in the ?Comb.? column of Table 1. We then
compute our consistency measure as the Spearman?s
? rank correlation between these combined individ-
ual ratings with the whole ratings (?Mean? in Table
2). The original, unadulterated data sets have a con-
sistency measure of 0.786, indicating that, despite
the very different collection methodologies, the two
ratings sets largely agree.
3.1.2 Extrinsic Evaluation:
Correlation with Association Norms
The extrinsic evaluation compares the consistency
4We also tried the arithmetic mean, but the multiplicative
method always performs better.
34
Word Example Associations
Fliegenpilz ?toadstool? giftig ?poisonous?, rot ?red?, Wald ?forest?
Fliege ?housefly/bow tie? nervig ?annoying?, summen ?to buzz?, Insekt ?insect?
Pilz ?mushroom? Wald ?forest?, giftig ?poisonous?, sammeln ?to gather?
Sonnenblume ?sunflower? gelb ?yellow?, Sommer ?summer?, Kerne ?seeds?
Sonne ?sun? Sommer ?summer?, warm ?warm?, hell ?bright?
Blume ?flower? Wiese ?meadow?, Duft ?smell?, Rose ?rose?
Table 3: Example association norms for two German compounds and their constituents.
between our two rating sets individual and whole
with evidence from a large collection of associa-
tion norms. Association norms have a long tradition
in psycholinguistic research to investigate semantic
memory, making use of the implicit notion that asso-
ciates reflect meaning components of words (Deese,
1965; Miller, 1969; Clark, 1971; Nelson et al, 1998;
Nelson et al, 2000; McNamara, 2005; de Deyne and
Storms, 2008). They are collected by presenting a
stimulus word to a subject and collecting the first
words that come to mind.
We rely on association norms that were collected
for our compounds and constituents via both a large
scale web experiment and Amazon Mechanical Turk
(Schulte im Walde et al, 2012) (unpublished). The
resulting combined data set contains 85,049/34,560
stimulus-association tokens/types for the compound
and constituent stimuli. Table 3 gives examples of
associations from the data set for some stimuli.
The guiding intuition behind comparing our rat-
ing data sets with association norms is that a com-
pound which is compositional with respect to a con-
stituent should have similar associations as its con-
stituent (Schulte im Walde et al, 2012).
To measure the correlation of the rating data with
the association norms, we first compute the Jac-
card similarity that measures the overlap in two sets,
ranging from 0 (perfectly dissimilar) to 1 (perfectly
similar). The Jaccard is defined for two sets, A and
B, as
J(A,B) =
|A ?B|
|A ?B|
.
For example, we can use Table 3 to compute the
Jaccard similarity between Sonnenblume and Sonne:
|{Sommer}|
|{gelb, Sommer,Kerne,warm, hell}|
= 0.20.
After computing the Jaccard similarity between
all compounds and constituents across the associ-
ation norms, we correlate this association overlap
with the average individual ratings (i.e., column
?Mean? in Table 1) using Spearman?s ?. This cor-
relation ?Assoc Norm (Indiv)? reaches ? = 0.638
for our original data. We also compute a combined
Jaccard similarity using the geometric mean, e.g.
?
J(Fliegenpilz, F liege) ? J(Fliegenpilz, P ilz),
and calculate Spearman?s ? with the whole ratings
(i.e., column ?Mean? in Table 2). This correlation
?Assoc Norm (Whole)? reaches ? = 0.469 for our
original data.
3.2 Data Cleansing
We applied the two standard cleansing approaches,
Z-score Filtering and Minimum Subject Agreement,
to our rating data, and evaluated the results.
3.2.1 Z-score Filtering
Z-score filtering is a method to filter out Type I
noise, such as random guesses made by individu-
als when a word is unfamiliar. It makes the sim-
ple assumption that each item?s ratings should be
roughly normally distributed around the ?true? rat-
ing of the item, and throws out all outliers which
are more than z? standard deviations from the item?s
mean. With regard to our compositionality ratings,
for each item i (i.e., a compound in the whole data,
or a compound?constituent pair in the individual
data) we compute the mean x?i and standard devia-
tion ?i of the ratings for the given item. We then
remove all values from xi where
|xi ? x?i| > ?iz
?,
with the parameter z? indicating the maximum al-
lowed Z-score of the item?s ratings. For example, if
a particular item has ratings of xi = (1, 2, 1, 6, 1, 1),
then the mean x?i = 2 and the standard deviation
35
ll
lll
l
llllllll
lllll
l
llllll
l
l
l
ll
lll
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l l lCleaned Indiv Cleaned Whole Cleaned Indiv & Whole
(a) Intrinsic Evaluation of Z?score Filtering
l
ll
ll
lllllllll
l ll
lllllllllll
0.40
0.45
0.50
0.55
0.60
0.65
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Cor
rela
tion
 wit
h A
sso
ciat
ion
 No
rm 
Ove
rlap
(Spe
arm
an
's r
ho)
l lAssoc Norms (Indiv) Assoc Norms (Whole)
(b) Extrinsic Evaluation of Z?score Filtering
Figure 1: Intrinsic and Extrinsic evaluation of Z-score fil-
tering. We see that Z-score filtering makes a minimal dif-
ference when filtering is strict, and is slightly detrimental
with more aggressive filtering.
?i = 2. If we use a z? of 1, then we would filter rat-
ings outside of the range [2? 1 ? 2, 2 + 1 ? 2]. Thus,
the resulting new xi would be (1, 2, 1, 1, 1) and the
new mean x?i would be 1.2.
Filtering Outliers Figure 1a shows the results for
the intrinsic evaluation of Z-score filtering. The
solid black line represents the consistency of the fil-
tered individual ratings with the unadulterated whole
ratings. The dotted orange line shows the consis-
tency of the filtered whole ratings with the unadul-
terated individual ratings, and the dashed purple line
shows the consistency between the data sets when
both are filtered. In comparison, the consistency be-
tween the unadulterated data sets is provided by the
horizontal gray line. We see that Z-score filtering
overall has a minimal effect on the consistency of
l
l
l
ll
lllllllll
ll
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Fra
ctio
n D
ata
 Re
tain
ed
l l lIndiv Whole Both
Data Retention with Z?score Filtering
Figure 2: The data retention rate of Z-score filtering. Data
retention drops rapidly with aggressive filtering.
the two data sets. It provides very small improve-
ments with high Z-scores, but is slightly detrimental
at more aggressive levels.
Figure 1b shows the effects of Z-score filtering
with our extrinsic evaluation of correlation with as-
sociation norms. At all levels of filtering, we see that
correlation with association norms remains mostly
independent of the level of filtering.
An important factor to consider when evaluating
these results is the amount of data dropped at each
of the filtering levels. Figure 2 shows the data re-
tention rate for the different data sets and levels. As
expected, more aggressive filtering results in a sub-
stantially lower data retention rate. Comparing this
curve to the consistency ratings gives a clear picture:
the decrease in consistency is probably mostly due to
the decrease in available data but not due to filtering
outliers. As such, we believe that Z-score filtering
does not substantially improve data quality, but may
be safely applied with a conservative maximum al-
lowed Z-score.
Filtering Artificial Noise Z-score filtering has lit-
tle impact on the consistency of the data, but we
would like to determine whether this is due because
our data being very clean, so the filtering does not
apply, or Z-score filtering not being able to detect the
Type I noise. To test these two possibilities, we arti-
ficially introduce noise into our data sets: we create
100 variations of the original ratings matrices, where
with 0.25 probability, each entry in the matrix was
36
l l l l l l l ll ll ll l l ll l
l
0.65
0.70
0.75
0.80
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Indiv Noisy Indiv
(a) Removing Indiv Judgements with Uniform Noise
l ll ll ll ll ll ll ll l ll l
l ll ll
l
l
0.65
0.70
0.75
0.80
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Whole Noisy Whole
(b) Removing Whole Judgements with Uniform Noise
Figure 3: Ability of Z-score filtering at removing artificial noise added in the (a) individual and (b) whole judgements.
The orange lines represent the consistency of the data with the noise, but no filtering, while the black lines indicate
the consistency after Z-score filtering. Z-score filtering appears to be unable to find uniform random noise in either
situation.
l l l l l l
l
l
l
l l
l l
l
l
l
l l l
l
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
0.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l l lCleaned Indiv Cleaned Whole Cleaned Indiv & Whole
(a) Intrinsic Evaluation of MSA Filtering
l l l l l l l l l
l
l
l l l l l l l l l l l
0.40
0.45
0.50
0.55
0.60
0.65
0.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)
Cor
rela
tion
 wit
h A
sso
ciat
ion
 No
rm 
Ove
rlap
(Spe
arm
an
's r
ho)
l lAssoc Norms (Indiv) Assoc Norms (Whole)
(b) Extrinsic Evaluation of MSA Filtering
Figure 4: Intrinsic and Extrinsic evaluation of Minimum Subject Agreement filtering. We see virtually no gains using
subject filtering, and the individual judgements are quite hindered by aggressive filtering.
37
replaced with a uniform random integer between 1
and 7. That is, roughly 1 in 4 of the entries in the
original matrix were replaced with random, uniform
noise. We then apply Z-score filtering on each of
these noisy matrices and report their average con-
sistency with its companion, unadulterated matrix.
That is, we add noise to the individual ratings ma-
trix, and then compare its consistency with the orig-
inal whole ratings matrix, and vice versa. Thus if we
are able to detect and remove the artificial noise, we
should see higher consistencies in the filtered matrix
over the noisy matrix.
Figure 3 shows the results of adding noise to the
original data sets. The lines indicate the averages
over all 100 matrix variations, while the shaded ar-
eas represent the 95% confidence intervals. Surpris-
ingly, even though 1/4 entries in the matrix were re-
placed with random values, the decrease in consis-
tency is relatively low in both settings. This likely
indicates our data already has high variance. Fur-
thermore, in both settings, we do not see any in-
crease in consistency from Z-score filtering. We
must conclude that Z-score appears ineffective at re-
moving Type I noise in compositionality ratings.
We also tried introducing artificial noise in a sec-
ond way, where judgements were not replaced with a
uniformly random value, but a fixed offset of either
+3 or -3, e.g., 4?s became either 1?s or 7?s. Again,
the values were changed with probability of 0.25.
The results were remarkably similar, so we do not
include them here.
3.2.2 Minimum Subject Agreement
Minimum Subject Agreement is a method for fil-
tering out subjects who seem to misunderstand the
rating task or rarely agree with the rest of the pop-
ulation. For each subject in our data, we compute
the average ratings for each item excluding the sub-
ject. The subject?s rank agreement with the exclu-
sive averages is computed using Spearman?s ?. We
can then remove subjects whose rank agreement is
below a threshold, or remove the n subjects with the
lowest rank agreement.
Filtering Unreliable Subjects Figure 4 shows the
effect of subject filtering on our intrinsic and extrin-
sic evaluations. We can see that mandating mini-
mum subject agreement has a strong, negative im-
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
lll
llllllll
lllll
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 5 10 15 20 25Number of Subjects Randomized/Removed
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Indiv Noisy Indiv
(a) Removing Indiv Subjects with Artificial Noise
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
llllllllll
lllllll
0.65
0.70
0.75
0.80
0 5 10 15 20 25Number of Subjects Randomized/Removed
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Whole Noisy Whole
(b) Removing Whole Subjects with Artificial Noise
Figure 5: Ability of subject filtering at detecting highly
deviant subjects. We see that artificial noise strongly
hurts the quality of the individual judgements, while hav-
ing a much weaker effect on the whole judgements. The
process is effective at identifying deviants in both set-
tings.
pact on the individual ratings after a certain thresh-
old is reached, but virtually no effect on the whole
ratings. When we consider the corresponding data
retention curve in Figure 6, the result is not surpris-
ing: the dip in performance for the individual ratings
comes with a data retention rate of roughly 25%. In
this way, it?s actually surprising that it does so well:
with only 25% of the original data, consistency is
only 5 points lower. The effects are more dramatic
in the extrinsic evaluation.
On the other hand, subject filtering has almost no
effect on the whole ratings. This is not surprising, as
most subjects have only rated at most a few dozen
items, so removing subjects corresponds to a smaller
reduction in data, as seen in Figure 6. Furthermore,
the subjects with the highest deviations tend to be
38
l l l l l l l l
l
l
l
l l l l l l l l l l
l
l l l
l
l
l
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)
Fra
ctio
n D
ata
 Re
tain
ed
l l lIndiv Whole Both
Data Retention with MSA Filtering
Figure 6: Data retention rates for various levels of mini-
mum subject agreement. The whole ratings remain rela-
tively untouched by mandating high levels of agreement,
but individual ratings are aggressively filtered after a sin-
gle breaking point.
the subjects who rated the fewest items since their
agreement is more sensitive to small changes. As
such, the subjects removed tend to be the subjects
with the least influence on the data set.
Removing Artificial Subject-level Noise To test
the hypothesis that minimum subject agreement fil-
tering is effective at removing Type II noise, we in-
troduce artificial noise at the subject level. For these
experiments, we create 100 variations of our ma-
trices where n subjects have all of their ratings re-
placed with random, uniform ratings. We then apply
subject-level filtering where we remove the n sub-
jects who agree least with the overall averages.
Figure 5a shows the ability of detecting Type II
noise in the individual ratings. The results are un-
surprising, but encouraging. We see that increasing
the number of randomized subjects rapidly lowers
the consistency with the whole ratings. However, the
cleaned whole ratings matrix maintains a fairly high
consistency, indicating that we are doing a nearly
perfect job at identifying the noisy individuals.
Figure 5b shows the ability of detecting Type II
noise in the whole ratings. Again, we see that the
cleaned noisy ratings have a higher consistency than
the noisy ratings, indicating the efficacy of subject
agreement filtering at detecting unreliable subjects.
The effect is less pronounced in the whole ratings
than the individual ratings due to the lower propor-
tion of subjects being randomized.
Identification of Spammers Removing subjects
with the least agreement lends itself to another sort
of evaluation: predicting subjects rejected during
data collection. As discussed in Section 2, subjects
who failed to identify the fake words or had an over-
all low reputability were filtered from the data before
any analysis. To test the quality of minimum sub-
ject agreement, we reconstructed the data set where
these previously rejected users were included, rather
than removed. Subjects who rated fewer than 10
items were still excluded.
The resulting data set had a total of 242 users: 150
(62.0%) which were included in the original data,
and 92 (38.0%) which were originally rejected. Af-
ter constructing the modified data set, we sorted the
subjects by their agreement. Of the 92 subjects with
the lowest agreement, 75 of them were rejected in
the original data set (81.5%). Of the 150 subjects
with the highest agreement, only 17 of them were
rejected from the original data set (11.3%). The typ-
ical precision-recall tradeoff obviously applies.
Curiously, we note that the minimum subject
agreement at this 92nd subject was 0.457. Compar-
ing with the curves for the individual ratings in Fig-
ures 4a and 6, we see this is the point where intrinsic
consistency and data retention both begin dropping
rapidly. While this may be a happy coincidence, it
does seem to suggest that the ideal minimum sub-
ject agreement is roughly where the data retention
rate starts rapidly turning.
Regardless, we can definitely say that minimum
subject agreement is a highly effective way of root-
ing out spammers and unreliable participants.
4 Conclusion
In this paper, we have performed a thorough anal-
ysis of two sets of compositionality ratings to Ger-
man noun-noun compounds, and assessed their reli-
ability from several perspectives. We conclude that
asking for ratings of compositionality of compound
words is reasonable and that such judgements are
notably reliable and robust. Even when composi-
tionality ratings are collected in two very different
settings (laboratory vs. AMT) and with different dy-
namics, the produced ratings are highly consistent.
This is shown by the high initial correlation of the
two sets of compositionality ratings. We believe this
39
provides strong evidence that human judgements of
compositionality, or at least these particular data
sets, are reasonable as gold standards for other com-
putational linguistic tasks.
We also find that such ratings can be highly ro-
bust against large amounts of data loss, as in the
case of aggressive Z-score and minimum subject
agreement filtering: despite data retention rates of
10-70%, consistency between our data sets never
dropped more than 6 points. In addition, we find that
the correlation between compositionality ratings and
association norms is substantial, but generally much
lower and less sensitive than internal consistency.
We generally find Type I noise to be very diffi-
cult to detect, and Z-score filtering is mostly inef-
fective at eliminating unreliable item ratings. This
is confirmed by both our natural and artificial exper-
iments. At the same time, Z-score filtering seems
fairly harmless at conservative levels, and probably
can be safely applied in moderation with discretion.
On the other hand, we have confirmed that mini-
mum subject agreement is highly effective at filter-
ing out incompetent and unreliable subjects, as evi-
denced by both our artificial and spammer detection
experiments. We conclude that, as we have defined
it, Type II noise is easily detected, and removing this
noise produces much higher quality data. We recom-
mend using subject agreement as a first-pass identi-
fier of likely unreliable subjects in need of manual
review.
We would also like to explore other types of
compounds, such as adjective-noun compounds (e.g.
Gro?eltern ?grandparents?), and compounds with
more than two constituents (e.g. Bleistiftspitzma-
chine ?automatic pencil sharpener?).
Acknowledgments
We thank the SemRel group, Alexander Fraser, and
the reviewers for helpful comments and feedback.
The authors acknowledge the Texas Advanced Com-
puting Center (TACC) for providing grid resources
that have contributed to these results.5
5http://www.tacc.utexas.edu
References
Collin Bannard. 2005. Learning about the Meaning of
Verb?Particle Constructions from Corpora. Computer
Speech and Language, 19:467?478.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October.
Irad Ben-Gal. 2005. Outlier detection. In O. Maimon
and L. Rockach, editors, Data Mining and Knowledge
Discobery Handbook: A Complete Guide for Practi-
tioners and Researchers. Kluwer Academic Publish-
ers.
Gemma Boleda, Marco Baroni, Nghia The Pham, and
Louise McNally. 2013. On adjective-noun compo-
sition in distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics, Potsdam, Germany.
Chris Callison-Burch and Mark Dredze, editors. 2010.
Proceedings of the NAACL/HLT Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk, Los Angeles, California.
Herbert H. Clark. 1971. Word Associations and Lin-
guistic Theory. In John Lyons, editor, New Horizon in
Linguistics, chapter 15, pages 271?286. Penguin.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2011. Mathematical foundations for a compositional
distributional model of meaning. Linguistic Analysis,
36(1-4):345?384.
Paul Cook and Suzanne Stevenson. 2006. Classifying
Particle Semantics in English Verb-Particle Construc-
tions. In Proceedings of the ACL/COLING Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, Sydney, Australia.
Simon de Deyne and Gert Storms. 2008. Word associ-
ations: Norms for 1,424 dutch words in a continuous
task. Behavior Research Methods, 40(1):198?205.
James Deese. 1965. The Structure of Associations in
Language and Thought. The John Hopkins Press, Bal-
timore, MD.
Wolfgang Fleischer and Irmhild Barz. 2012. Wortbil-
dung der deutschen Gegenwartssprache. de Gruyter.
Edward Grefenstette, G. Dinu, Y. Zhang, Meernoosh
Sadrzadeh, and Marco Baroni. 2013. Multi-step re-
gression learning for compositional distributional se-
mantics. In Proceedings of the 10th International
Conference on Computational Semantics, Potsdam,
Germany.
Rochelle Lieber and Pavol Stekauer, editors. 2009. The
Oxford Handbook of Compounding. Oxford Univer-
sity Press.
40
Jonathan I. Maletic and Adrian Marcus. 2010. Data
cleansing: A prelude to knowledge discovery. In
O. Maimon and L. Rokach, editors, Data Mining
and Knowledge Discovery Handbook. Springer Sci-
ence and Business Media, 2 edition.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan.
Timothy P. McNamara. 2005. Semantic Priming: Per-
spectives from Memory and Word Recognition. Psy-
chology Press, New York.
George Miller. 1969. The Organization of Lexical Mem-
ory: Are Word Associations sufficient? In George A.
Talland and Nancy C. Waugh, editors, The Pathol-
ogy of Memory, pages 223?237. Academic Press, New
York.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34:1388?1429.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The University of South Florida
Word Association, Rhyme, and Word Fragment
Norms.
Douglas L. Nelson, Cathy L. McEvoy, and Simon Den-
nis. 2000. What is Free Association and What does it
Measure? Memory and Cognition, 28:887?899.
Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011a. Dynamic and Static Pro-
totype Vectors for Semantic Composition. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing, pages 705?713, Chiang
Mai, Thailand.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011b. An Empirical Study on Compositionality in
Compound Nouns. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 210?218, Chiang Mai, Thailand.
Sabine Schulte im Walde, Susanne Borgwaldt, and
Ronny Jauch. 2012. Association Norms of German
Noun Compounds. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 632?639, Istanbul, Turkey.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic Pooling and Unfolding Recursive Autoencoders
for Paraphrase Detection. In Advances in Neural In-
formation Processing Systems 24.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages 51?
74.
Dominic Widdows. 2008. Semantic Vector Products:
Some Initial Investigations. In Proceedings of the 2nd
Conference on Quantum Interaction, Oxford, UK.
41
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 104?108,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Feature Norms of German Noun Compounds
Stephen Roller
Department of Computer Science
The University of Texas at Austin
roller@cs.utexas.edu
Sabine Schulte im Walde
Institut f?ur Maschinelle Sprachverarbeitung
Universit?at Stuttgart, Germany
schulte@ims.uni-stuttgart.de
Abstract
This paper presents a new data collection
of feature norms for 572 German noun-
noun compounds. The feature norms com-
plement existing data sets for the same
targets, including compositionality rat-
ings, association norms, and images. We
demonstrate that the feature norms are po-
tentially useful for research on the noun-
noun compounds and their semantic trans-
parency: The feature overlap of the com-
pounds and their constituents correlates
with human ratings on the compound?
constituent degrees of compositionality,
? = 0.46.
1 Introduction
Feature norms are short descriptions of typical at-
tributes for a set of objects. They often describe
the visual appearance (a firetruck is red), function
or purpose (a cup holds liquid), location (mush-
rooms grow in forests), and relationships between
objects (a cheetah is a cat). The underlying fea-
tures are usually elicited by asking a subject to
carefully describe a cue object, and recording their
responses.
Feature norms have been widely used in psy-
cholinguistic research on conceptual representa-
tions in semantic memory. Prominent collections
have been pursued by McRae et al. (2005) for liv-
ing vs. non-living basic-level concepts; by Vin-
son and Vigliocco (2008) for objects and events;
and by Wu and Barsalou (2009) for noun and noun
phrase objects. In recent years, feature norms have
also acted as a loose proxy for perceptual infor-
mation in data-intensive computational models of
semantic tasks, in order to bridge the gap between
language and the real world (Andrews et al., 2009;
Silberer and Lapata, 2012; Roller and Schulte im
Walde, 2013).
In this paper, we present a new resource of fea-
ture norms for a set of 572 concrete, depictable
German nouns. More specifically, these nouns in-
clude 244 noun-noun compounds and their corre-
sponding constituents. For example, we include
features for ?Schneeball? (?snowball?), ?Schnee?
(?snow?), and ?Ball? (?ball?). Table 1 presents
the most prominent features of this example com-
pound and its constituents. Our collection com-
plements existing data sets for the same targets,
including compositionality ratings (von der Heide
and Borgwaldt, 2009); associations (Schulte im
Walde et al., 2012; Schulte im Walde and Borg-
waldt, 2014); and images (Roller and Schulte im
Walde, 2013).
The remainder of this paper details the col-
lection process of the feature norms, discusses
two forms of cleansing and normalization we em-
ployed, and performs quantitative and qualitative
analyses. We find that the normalization proce-
dures improve quality in terms of feature tokens
per feature type, that the normalized feature norms
have a desirable distribution of features per cue,
and that the feature norms are useful in semantic
models to predict compositionality.
2 Feature Norm Collection
We employ Amazon Mechanical Turk (AMT)
1
for
data collection. AMT is an online crowdsourc-
ing platform where requesters post small, atomic
tasks which require manual completion by hu-
mans. Workers can complete these tasks, called
HITs, in order to earn a small bounty.
2.1 Setup and Data
Workers were presented with a simple page asking
them to describe the typical attributes of a given
noun. They were explicitly informed in English
that only native German speakers should complete
1
http://www.mturk.com
104
Schneeball ?snowball? Schnee ?snow? Ball ?ball?
ist kalt ?is cold? 8 ist kalt ?is cold? 13 ist rund ?is round? 14
ist rund ?is round? 7 ist wei? ?is white? 13 zum Spielen ?for playing? 4
aus Schnee ?made from snow? 7 im Winter ?in the winter? 6 rollt ?rolls? 2
ist wei? ?is white? 7 f?allt ?falls? 3 wird geworfen ?is thrown? 2
formt man ?is formed? 2 schmilzt ?melts? 2 ist bunt ?is colorful? 2
wirft man ?is thrown? 2 hat Flocken ?has flakes? 2 Fu?ball ?football? 2
mit den H?anden ?with hands? 2 ist w?assrig ?is watery? 1 Basketball ?basketball? 2
Table 1: Most frequent features for example compound Schneeball and its constituents.
the tasks. All other instructions were given in Ger-
man. Workers were given 7 example features for
the nouns ?Tisch? (?table?) and ?Katze? (?cat?), and
instructed to provide typical attributes per noun.
Initially, workers were required to provide 6-10
features per cue and were only paid $0.02 per hit,
but very few workers completed the hits. After
lowering the requirements and increasing the re-
ward, we received many more workers and col-
lected the data more quickly. Workers could also
mark a word as unfamiliar or provide additional
commentary if desired.
We collected responses from September 21,
2012 until January 31, 2013. Workers who were
obvious spammers were rejected and not rewarded
payment. Typically spammers pasted text from
Google, Wikipedia, or the task instructions and
were easy to spot. Users who failed to follow in-
structions (responded in English, did not provide
the minimum number of features, or gave nonsen-
sical responses) were also rejected without pay-
ment. Users who put in a good faith effort and
consistently gave reasonable responses had all of
their responses accepted and rewarded.
In total, 98 different workers completed at least
one accepted hit, but the top 25 workers accounted
for nearly 90% of the responses. We accepted
28,404 different response tokens over 18,996 re-
sponse types for 572 different cues, or roughly 50
features per cue.
3 Cleansing and Normalization
We provide two cleaned and normalized versions
of our feature norms.
2
In the first version, we cor-
rect primarily orthographic mistakes such as in-
consistent capitalization, spelling errors, and sur-
face usage, but feature norms remain otherwise
unchanged. This version will likely be more useful
to researchers interested in more subtle variations
2
The norms can be downloaded from
www.ims.uni-stuttgart.de/forschung/ressourcen/
experiment-daten/feature-norms.en.html.
and distinctions made by the workers.
The second version of our feature norms are
more aggressively normalized, to reduce the quan-
tity of unique and low frequency responses while
maintaining the spirit of the original response. The
resulting data is considerably less sparse than the
orthographically normalized version. This version
is likely to be more useful for research that is
highly affected by sparse data, such as multimodal
experiments (Andrews et al., 2009; Silberer and
Lapata, 2012; Roller and Schulte im Walde, 2013).
3.1 Orthographic Normalization
Orthographic normalization is performed in four
automatic passes and one manual pass in the fol-
lowing order:
Letter Case Normalization: Many workers
inconsistently capitalize the first word of feature
norms as though they are writing a complete sen-
tence. For example, ?ist rund? and ?Ist rund? (?is
round?) were both provided for the cue ?Ball?.
We cannot normalize capitalization by simply us-
ing lowercase everywhere, as the first letter of
German nouns should always be capitalized. To
handle the most common instances, we lowercase
the first letter of features that began with articles,
modal verbs, prepositions, conjunctions, or the
high-frequency verbs ?kommt?, ?wird?, and ?ist?.
Umlaut Normalization: The same German
word may sometimes be spelled differently be-
cause some workers use German keyboards
(which have the letters ?a, ?o, ?u, and ?), and oth-
ers use English keyboards (which do not). We
automatically normalize to the umlaut form (i.e.
?gruen? to ?gr?un?, ?weiss? to ?wei??) whenever two
workers gave both versions for the same cue.
Spelling Correction: We automatically correct
common misspellings (such as errecihen? erre-
ichen), using a list from previous collection exper-
iments (Schulte im Walde et al., 2008; Schulte im
Walde et al., 2012). The list was created semi-
automatically, and manually corrected.
105
Usage of ?ist? and ?hat?: Workers sometimes
drop the verbs ?ist? (?is?) and ?hat? (?has?), e.g. the
worker writes only ?rund? (?round?) instead of ?ist
rund?, or ?Obst? (?fruit?) instead of ?hat Obst?. We
normalize to the ?ist? and ?hat? forms when two
workers gave both versions for the same cue. Note
that we cannot automatically do this across sepa-
rate cues, as the relationship may change: a tree
has fruit, but a banana is fruit.
Manual correction: Following the above auto-
matic normalizations, we manually review all non-
unique responses. In this pass, responses are nor-
malized and corrected with respect to punctuation,
capitalization, spelling, and orthography. Roughly
170 response types are modified in this phase.
3.2 Variant Normalization
The second manual pass consists of more aggres-
sive normalization of expression variants. In this
pass, features are manually edited to minimize the
number of feature types while preserving as much
semantic meaning as possible:
? Replacing plurals with singulars;
? Removing modal verbs, e.g. ?kann Kunst
sein? (?can be art?) to ?ist Kunst?;
? Removing quantifiers and hedges, e.g. ?ist
meistens blau? (?is mostly blue?) to ?ist blau?;
? Splitting into atomic norms, e.g. ?ist wei?
oder schwarz? (?is white or black?) to ?ist
wei?? and ?ist schwarz?, or ?jagt im Wald?
(?hunts in forest?) to ?jagt? and ?im Wald?;
? Simplifying verbiage, e.g. ?ist in der Farbe
schwarz? (?is in the color black?) to ?ist
schwarz?.
These selected normalizations are by no means
comprehensive or exhaustive, but do handle a
large portion of the cases. In total, we modify
roughly 5400 tokens over 1300 types.
4 Quantitative Analysis
In the following two analyses, we explore the type
and token counts of our feature norms across the
steps in the cleansing process, and analyze the un-
derlying distributions of the features per cues.
Type and Token counts Table 2 shows the to-
ken and type counts for all features in each step
of the cleansing process. We also present the
counts for non-idiosyncratic features, or features
which are provided for at least two distinct cues.
The orthographic normalizations generally lower
the number of total and non-idiosyncratic types,
and increase the number of non-idiosyncratic to-
kens. This indicates we are successfully identify-
ing and correcting many simple orthographic er-
rors, resulting in a less sparse matrix. The nec-
essary amount of manual correction is relatively
low, indicating we are able to catch the majority
of mistakes using simple, automatic methods.
Data Version Total Non-idiosyncratic
of Responses Types Tokens Types Tokens
Raw 18,996 28,404 2,029 10,675
Case 18,848 28,404 2,018 10,801
Umlaut 18,700 28,404 1,967 10,817
Spelling 18,469 28,404 1,981 11,072
ist/hat 18,317 28,404 1,924 11,075
Manual 18,261 28,404 1,889 11,106
Aggressive 17,503 28,739 1,374 11,848
Table 2: Counts in the cleansing process.
The more aggressively normalized norms are
considerably different than the orthographically
normalized norms. Notably, the number of total
tokens increases from the atomic splits. The data
is also less sparse and more robust, as indicated by
the drops in both total and non-idiosyncratic types.
Furthermore, the number of non-idiosyncratic to-
kens also increases considerably, indicating we
were able to find numerous edge cases and place
them in existing, frequently-used bins.
Number of Features per Cue Another impor-
tant aspect of the data set is the number of features
per cue. An ideal feature norm data set would con-
tain a roughly equal number of (non-idiosyncratic)
features for every cue; if most of the features are
underrepresented, with a majority of the features
lying in only a few cues, then our data set may
only properly represent for these few, heavily rep-
resented cues.
Figure 1 shows the number of features per cue
for (a) all features and (b) the non-idiosyncratic
features, for the aggressively normalized data set.
In the first histogram, we see a clear bimodal dis-
tribution around the number of features per cue.
This is an artifact of the two parts of our collec-
tion process: the shorter, wider distribution corre-
sponds to the first part of collection, where work-
ers gave more responses for less reward. The
taller, skinnier distribution corresponds to the sec-
ond half of collection, when workers were re-
warded more for less work. The second collec-
tion procedure was clearly effective in raising the
number of hits completed, but resulted in fewer
features per cue.
106
020
40
60
80
0 25 50 75 100 125Features / Cue
Cou
nt
(a) All Norms
0
25
50
75
0 20 40 60Features / Cue
Cou
nt
(b) Non?idiosyncratic Norms
Figure 1: Distribution of features per cue.
In the second histogram, we see only the non-
idiosyncratic features for each cue. Unlike the first
histogram, we see only one mode with a relatively
long tail. This indicates that mandating more fea-
tures per worker (as in the first collection process)
often results in more idiosyncratic features, and
not necessarily a stronger representation of each
cue. We also see that roughly 85% of the cues have
at least 9 non-idiosyncratic features each. In sum-
mary, our representations are nicely distributed for
the majority of cues.
5 Qualitative Analysis
Our main motivation to collect the feature norms
for the German noun compounds and their con-
stituents was that the features provide insight into
the semantic properties of the compounds and
their constituents and should therefore represent a
valuable resource for cognitive and computational
linguistics research on compositionality. The fol-
lowing two case studies demonstrate that the fea-
ture norms indeed have that potential.
Predicting the Compositionality The first case
study relies on a simple feature overlap measure
to predict the degree of compositionality of the
compound?constituent pairs of nouns: We use the
proportion of shared features of the compound and
a constituent with respect to the total number of
features of the compound. The degree of compo-
sitionality of a compound noun is calculated with
respect to each constituent of the compound.
For example, if a compound nounN
0
received a
total of 30 features (tokens), out of which it shares
20 with the first constituent N
1
and 10 with the
second constituent N
2
, the predicted degrees of
compositionality are
20
30
= 0.67 for N
0
?N
1
, and
10
30
= 0.33 for N
0
?N
2
. The predicted degrees of
compositionality are compared against the mean
compositionality judgments as collected by von
der Heide and Borgwaldt (2009), using the Spear-
man rank-order correlation coefficient. The result-
ing correlations are ? = 0.45, p < .000001 for the
standard normalized norms, and ? = 0.46, p <
.000001 for the aggressively normalized norms,
which we consider a surprisingly successful result
concerning our simple measure. Focusing on the
compound?head pairs, the feature norms reached
? = 0.57 and ? = 0.59, respectively.
Perceptual Model Information As mentioned
in the Introduction, feature norms have also acted
as a loose proxy for perceptual information in
data-intensive computational models of semantic
tasks. The second case study is taken from Roller
and Schulte im Walde (2013), who integrated fea-
ture norms as one type of perceptual informa-
tion into an extension and variations of the LDA
model by Andrews et al. (2009). A bimodal LDA
model integrating textual co-occurrence features
and our feature norms significantly outperformed
the LDA model that only relied on the textual co-
occurrence. The evaluation of the LDA models
was performed on the same compositionality rat-
ings as described in the previous paragraph.
6 Conclusion
This paper presented a new collection of feature
norms for 572 German noun-noun compounds.
The feature norms complement existing data sets
for the same targets, including compositionality
ratings, association norms, and images.
We have described our collection process, and
the cleaning and normalization, and we have
shown both the orthographically normalized and
more aggressively normalized feature norms to be
of higher quality than the raw responses in terms
of types per token, and that the normalized feature
norms have a desirable distribution of features per
cue. We also demonstrated by two case studies
that the norms represent a valuable resource for
research on compositionality.
107
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547?
559.
Stephen Roller and Stephen Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of
the 2013 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1146?
1157, Seattle, Washington, USA.
Sabine Schulte im Walde and Susanne Borgwaldt.
2014. Association norms for German noun com-
pounds and their constituents. Under review.
Sabine Schulte im Walde, Alissa Melinger, Michael
Roth, and Andrea Weber. 2008. An empirical char-
acterisation of response types in German association
norms. Research on Language and Computation,
6(2):205?238.
Sabine Schulte im Walde, Susanne Borgwaldt, and
Ronny Jauch. 2012. Association norms of German
noun compounds. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 632?639, Istanbul, Turkey.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433, Jeju
Island, Korea.
David Vinson and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set of
objects and events. Behavior Research Methods,
40(1):183?190.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegrif-
fen. Eine explorative Studie. In Proceedings of
the 9th Norddeutsches Linguistisches Kolloquium,
pages 51?74.
Ling-ling Wu and Lawrence W. Barsalou. 2009. Per-
ceptual simulation in conceptual combination: Evi-
dence from property generation. Acta Psychologica,
132:173?189.
108
