Proceedings of NAACL HLT 2007, Companion Volume, pages 149?152,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Agenda-Based User Simulation for
Bootstrapping a POMDP Dialogue System
Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye and Steve Young
Cambridge University Engineering Department
Trumpington Street, Cambridge, CB21PZ, United Kingdom
{js532, brmt2, kw278, hy216, sjy}@eng.cam.ac.uk
Abstract
This paper investigates the problem of boot-
strapping a statistical dialogue manager with-
out access to training data and proposes a new
probabilistic agenda-based method for simu-
lating user behaviour. In experiments with a
statistical POMDP dialogue system, the simu-
lator was realistic enough to successfully test
the prototype system and train a dialogue pol-
icy. An extensive study with human subjects
showed that the learned policy was highly com-
petitive, with task completion rates above 90%.
1 Background and Introduction
1.1 Bootstrapping Statistical Dialogue Managers
One of the key advantages of a statistical approach to Dia-
logue Manager (DM) design is the ability to formalise de-
sign criteria as objective reward functions and to learn an
optimal dialogue policy from real dialogue data. In cases
where a system is designed from scratch, however, it is
often the case that no suitable in-domain data is available
for training the DM. Collecting dialogue data without a
working prototype is problematic, leaving the developer
with a classic chicken-and-egg problem.
Wizard-of-Oz (WoZ) experiments can be carried out to
record dialogues, but they are often time-consuming and
the recorded data may show characteristics of human-
human conversation rather than typical human-computer
dialogue. Alternatively, human-computer dialogues can
be recorded with a handcrafted DM prototype but neither
of these two methods enables the system designer to test
the implementation of the statistical DM and the learn-
ing algorithm. Moreover, the size of the recorded corpus
(typically  103 dialogues) usually falls short of the re-
quirements for training a statistical DM (typically 104
dialogues).
1.2 User Simulation-Based Training
In recent years, a number of research groups have inves-
tigated the use of a two-stage simulation-based setup. A
statistical user model is first trained on a limited amount
of dialogue data and the model is then used to simulate
dialogues with the interactively learning DM (see Schatz-
mann et al (2006) for a literature review).
The simulation-based approach assumes the presence
of a small corpus of suitably annotated in-domain dia-
logues or out-of-domain dialogues with a matching dia-
logue format (Lemon et al, 2006). In cases when no such
data is available, handcrafted values can be assigned to
the model parameters given that the model is sufficiently
simple (Levin et al, 2000; Pietquin and Dutoit, 2005) but
the performance of dialogue policies learned this way has
not been evaluated using real users.
1.3 Paper Outline
This paper presents a new probabilistic method for simu-
lating user behaviour based on a compact representation
of the user goal and a stack-like user agenda. The model
provides an elegant way of encoding the relevant dialogue
history from a user?s point of view and has a very small
parameter set so that manually chosen priors can be used
to bootstrap the DM training and testing process.
In experiments presented in this paper, the agenda-
based simulator was used to train a statistical POMDP-
based (Young, 2006; Young et al, 2007) DM. Even with-
out any training of its model parameters, the agenda-
based simulator was able to produce dialogue behaviour
realistic enough to train a competitive dialogue policy.
An extensive study1 with 40 human subjects showed that
task completion with the learned policy was above 90%
despite a mix of native and non-native speakers.
1This research was partly funded by the EU FP6 TALK
Project. The system evaluation was conducted in collabora-
tion with O. Lemon, K. Georgila and J. Henderson at Edinburgh
University and their work is gratefully acknowledged.
149
2 Agenda-Based Simulation
2.1 User Simulation at a Semantic Level
Human-machine dialogue can be formalised on a seman-
tic level as a sequence of state transitions and dialogue
acts2. At any time t, the user is in a state S, takes ac-
tion a
u
, transitions into the intermediate state S?, receives
machine action a
m
, and transitions into the next state S??
where the cycle restarts.
S ? a
u
? S
?
? a
m
? S
??
? ? ? ? (1)
Assuming a Markovian state representation, user be-
haviour can be decomposed into three models: P (a
u
|S)
for action selection, P (S?|a
u
, S) for the state transition
into S?, and P (S??|a
m
, S
?
) for the transition into S??.
2.2 Goal- and Agenda-Based State Representation
Inspired by agenda-based methods to dialogue manage-
ment (Wei and Rudnicky, 1999) the approach described
here factors the user state into an agenda A and a goal G.
S = (A,G) and G = (C,R) (2)
During the course of the dialogue, the goal G ensures that
the user behaves in a consistent, goal-directed manner.
G consists of constraints C which specify the required
venue, eg. a centrally located bar serving beer, and re-
quests R which specify the desired pieces of information,
eg. the name, address and phone number (cf. Fig. 1).
The user agenda A is a stack-like structure containing
the pending user dialogue acts that are needed to elicit the
information specified in the goal. At the start of the dia-
logue a new goal is randomly generated using the system
database and the agenda is initially populated by convert-
ing all goal constraints into inform acts and all goal re-
quests into request acts. A bye act is added at the bottom
of the agenda to close the dialogue.
As the dialogue progresses the agenda and goal are dy-
namically updated and acts are selected from the top of
the agenda to form user acts a
u
. In response to incom-
ing machine acts a
m
, new user acts are pushed onto the
agenda and no longer relevant ones are removed. The
agenda thus serves as a convenient way of tracking the
progress of the dialogue as well as encoding the rele-
vant dialogue history. Acts can also be temporarily stored
when actions of higher priority need to be issued first,
hence providing the simulator with a simple model of
user memory3.
2In this paper, the terms dialogue act and dialogue action
are used interchangeably. The notation act(a=x, b=y,...) is used
to represent a dialogue act of a given type act (such as inform
or request with items a = x, b = y, etc.
3See Fig. 1, turns 1-3, for an example: System turn 1 ?What
pricerange?? triggers the user act inform(pricerange=cheap)
to be pushed onto the agenda but it is not executed until turn 3
because negate(drinks=beer) is issued first.
C
0
=
[
type = bar
drinks = beer
area = central
]
R
0
=
[
name =
addr =
phone =
]
Sys 0 Hello, how may I help you?
A
1
=
?
?
?
?
?
?
?
inform(type = bar)
inform(drinks = beer)
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
?
?
?
?
?
?
?
Usr 1 I?m looking for a nice bar serving beer.
Sys 1 Ok, a wine bar. What pricerange?
A
2
=
?
?
?
?
?
?
?
negate(drinks = beer)
inform(pricerange = cheap)
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
?
?
?
?
?
?
?
Usr 2 No, beer please!
Sys 2 A bar serving beer, correct?
A
3
=
?
?
?
?
?
?
?
affirm()
inform(pricerange = cheap)
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
?
?
?
?
?
?
?
Usr 3 Yeah something cheap in the town centre.
Sys 3 Murphy?s on Main Square serves cheap beer.
A
4
=
[
request(phone)
bye()
]
Usr 4 Ok, and what?s the phone number?
Sys 4 The number is 796 69 94.
A
5
=
[
bye()
]
Usr 5 Thanks, goodbye!
Figure 1: Sample dialogue and agenda sequence
2.3 User Act Selection
At any time during the dialogue, the updated agenda of
length N contains all dialogue acts the user intends to
convey to the system. Since the agenda is ordered ac-
cording to priority, with A[N ] denoting the top and A[1]
denoting the bottom item, selecting the next user act sim-
plifies to popping n items off the top of the stack. Hence,
letting a
u
[i] denote the ith item in the user act a
u
a
u
[i] := A[N ? n + i] ?i ? [1..n], 1 ? n ? N. (3)
Using A[N?n+1..N ] as a Matlab-like shorthand nota-
150
tion for the top n items on A, the action selection model
becomes a Dirac delta function
P (a
u
|S) = P (a
u
|A,G) = ?(a
u
, A[N? n+1..N ]) (4)
where the random variable n corresponds to the level
of initiative taken by the simulated user. In a statistical
model the probability distribution over integer values for
n should be conditioned on A and learned from dialogue
data. For the purposes of bootstrapping the system, n can
be assumed independent of A and any distribution P (n)
that places the majority of its probability mass on small
values of n can be used.
2.4 State Transition Model
The factorisation of S into A and G can now be ap-
plied to the state transition models P (S?|a
u
, S) and
P (S
??
|a
m
, S
?
). Letting A? denote the agenda after select-
ing a
u
(as explained in the previous subsection) and using
N
?
= N ? n to denote the size of A?, we have
A
?
[i] := A[i] ?i ? [1..N
?
]. (5)
Using this definition of A? and assuming that the goal
remains constant when the user executes a
u
, the first state
transition depending on a
u
simplifies to
P (S
?
|a
u
, S) = P (A
?
, G
?
|a
u
, A,G)
= ?(A
?
, A[1..N
?
])?(G
?
, G). (6)
Using S = (A,G), the chain rule of probability, and rea-
sonable conditional independence assumptions, the sec-
ond state transition based on a
m
can be decomposed into
goal update and agenda update modules:
P (S
??
|a
m
, S
?
)
= P (A
??
|a
m
, A
?
, G
??
)
? ?? ?
agenda update
P (G
??
|a
m
, G
?
)
? ?? ?
goal update
. (7)
When no restrictions are placed on A?? and G??, the space
of possible state transitions is vast. The model parame-
ter set is too large to be handcrafted and even substantial
amounts of training data would be insufficient to obtain
reliable estimates. It can however be assumed that A?? is
derived from A? and that G?? is derived from G? and that
in each case the transition entails only a limited number
of well-defined atomic operations.
2.5 Agenda Update Model for System Acts
The agenda transition from A? to A?? can be viewed as a
sequence of push-operations in which dialogue acts are
added to the top of the agenda. In a second ?clean-up?
step, duplicate dialogue acts, null() acts, and unnecessary
request() acts for already filled goal request slots must
be removed but this is a deterministic procedure so that it
can be excluded in the following derivation for simplicity.
Considering only the push-operations, the items 1 to N ?
at the bottom of the agenda remain fixed and the update
model can be rewritten as follows:
P (A
??
|a
m
, A
?
, G
??
)
= P (A
??
[1..N
??
]|a
m
, A
?
[1..N
?
], G
??
) (8)
= P (A
??
[N
?
+1..N
??
]|a
m
, G
??
)
? ?(A
??
[1..N
?
], A
?
[1..N
?
]). (9)
The first term on the RHS of Eq. 9 can now be further
simplified by assuming that every dialogue act item in
a
m
triggers one push-operation. This assumption can be
made without loss of generality, because it is possible to
push a null() act (which is later removed) or to push an
act with more than one item. The advantage of this as-
sumption is that the known number M of items in a
m
now determines the number of push-operations. Hence
N
??
= N
?
+ M and
P (A
??
[N
?
+1..N
??
]|a
m
, G
??
)
= P (A
??
[N
?
+1..N
?
+M ]|a
m
[1..M ], G
??
) (10)
=
M
?
i=1
P (A
??
[N
?
+i]|a
m
[i], G
??
) (11)
The expression in Eq. 11 shows that each item a
m
[i] in
the system act triggers one push operation, and that this
operation is conditioned on the goal. This model is now
simple enough to be handcrafted using heuristics. For ex-
ample, the model parameters can be set so that when the
item x=y in a
m
[i] violates the constraints in G??, one of
the following is pushed onto A??: negate(), inform(x=z),
deny(x=y, x=z), etc.
2.6 Goal Update Model for System Acts
The goal update model P (G??|a
m
, G
?
) describes how the
user constraints C ? and requests R? change with a given
machine action a
m
. Assuming that R?? is conditionally
independent of C? given C ?? it can be shown that
P (G
??
|a
m
, G
?
)
= P (R
??
|a
m
, R
?
, C
??
)P (C
??
|a
m
, R
?
, C
?
). (12)
To restrict the space of transitions from R? to R?? it
can be assumed that the request slots are independent and
each slot (eg. addr,phone,etc.) is either filled using infor-
mation in a
m
or left unchanged. Using R[k] to denote the
k?th request slot, we approximate that the value of R??[k]
only depends on its value at the previous time step, the
value provided by a
m
, and M(a
m
, C
??
) which indicates
a match or mismatch between the information given in
a
m
and the goal constraints.
P (R
??
|a
m
, R
?
, C
??
)
=
?
k
P (R
??
[k]|a
m
, R
?
[k],M(a
m
, C
??
)). (13)
151
To simplify P (C ??|a
m
, R
?
, C
?
) we assume that C ?? is
derived from C ? by either adding a new constraint, set-
ting an existing constraint slot to a different value (eg.
drinks=dontcare), or by simply changing nothing. The
choice of transition does not need to be conditioned on
the full space of possible a
m
, R
? and C ?. Instead it can
be conditioned on simple boolean flags such as ?Does a
m
ask for a slot in the constraint set??, ?Does a
m
signal that
no item in the database matches the given constraints??,
etc. The model parameter set is then sufficiently small for
handcrafted values to be assigned to the probabilities.
3 Evaluation
3.1 Training the HIS Dialogue Manager
The Hidden Information State (HIS) model is the first
trainable and scalable implementation of a statistical
spoken dialog system based on the Partially-Observable
Markov Decision Process (POMDP) model of dialogue
(Young, 2006; Young et al, 2007; Williams and Young,
2007). POMDPs extend the standard Markov-Decision-
Process model by maintaining a belief space, i.e. a proba-
bility distribution over dialogue states, and hence provide
an explicit model of the uncertainty present in human-
machine communication.
The HIS model uses a grid-based discretisation of the
continuous state space and online -greedy policy iter-
ation. Fig. 2 shows a typical training run over 60,000
simulated dialogues, starting with a random policy. User
goals are randomly generated and an (arbitrary) reward
function assigning 20 points for successful completion
and -1 for every dialogue turn is used. As can be seen, di-
alogue performance (defined as the average reward over
1000 dialogues) converges after roughly 25,000 iterations
and asymptotes to a value of approx. 14 points.
Figure 2: Training a POMDP system
3.2 Experimental Evaluation and Results
A prototype HIS dialogue system with a learned policy
was built for the Tourist Information Domain and exten-
sively evaluated with 40 human subjects including native
and non-native speakers. A total of 160 dialogues with
21667 words was recorded and the average Word-Error-
Rate was 29.8%. Task scenarios involved finding a spe-
cific bar, hotel or restaurant in a fictitious town (eg. the
address of a cheap, Chinese restaurant in the west).
The performance of the system was measured based
on the recommendation of a correct venue, i.e. a venue
matching all constraints specified in the given task (all
tasks were designed to have a solution). Based on this
definition, 145 out of 160 dialogues (90.6%) were com-
pleted successfully, and the average number of turns to
completion was 5.59 (if no correct venue was offered the
full number of turns was counted).
4 Summary and Future Work
This paper has investigated a new agenda-based user sim-
ulation technique for bootstrapping a statistical dialogue
manager without access to training data. Evaluation re-
sults show that, even with manually set model parame-
ters, the simulator produces dialogue behaviour realistic
enough for training and testing a prototype system. While
the results demonstrate that the learned policy works well
for real users, it is not necessarily optimal. The next step
is hence to use the recorded data to train the simulator,
and to then retrain the DM policy.
References
O. Lemon, K. Georgila, and J. Henderson. 2006. Evaluating
Effectiveness and Portability of Reinforcement Learned Di-
alogue Strategies with real users: the TALK TownInfo Eval-
uation. In Proc. of IEEE/ACL SLT, Palm Beach, Aruba.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochastic
Model of Human-Machine Interaction for Learning Dialog
Strategies. IEEE Trans. on Speech and Audio Processing,
8(1):11?23.
O. Pietquin and T. Dutoit. 2005. A probabilistic framework for
dialog simulation and optimal strategy learning. IEEE Trans.
on Speech and Audio Processing, Special Issue on Data Min-
ing of Speech, Audio and Dialog.
J. Schatzmann, K. Weilhammer, M.N. Stuttle, and S. Young.
2006. A Survey of Statistical User Simulation Tech-
niques for Reinforcement-Learning of Dialogue Manage-
ment Strategies. Knowledge Engineering Review, 21(2):97?
126.
X Wei and AI Rudnicky. 1999. An agenda-based dialog man-
agement architecture for spoken language systems. In Proc.
of IEEE ASRU. Seattle, WA.
J. Williams and S. Young. 2007. Partially Observable Markov
Decision Processes for Spoken Dialog Systems. Computer
Speech and Language, 21(2):231?422.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye. 2007.
The Hidden Information State Approach to Dialog Manage-
ment. In Proc. of ICASSP, Honolulu, Hawaii.
S. Young. 2006. Using POMDPs for Dialog Management. In
Proc. of IEEE/ACL SLT, Palm Beach, Aruba.
152
NAACL HLT Demonstration Program, pages 27?28,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
The Hidden Information State Dialogue Manager:
A Real-World POMDP-Based System
Steve Young, Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye
Cambridge University Engineering Department
Trumpington Street, Cambridge, CB21PZ, United Kingdom
{sjy, js532, brmt2, kw278, hy216}@eng.cam.ac.uk
Abstract
The Hidden Information State (HIS)
Dialogue System is the first trainable
and scalable implementation of a spoken
dialog system based on the Partially-
Observable Markov-Decision-Process
(POMDP) model of dialogue. The system
responds to n-best output from the speech
recogniser, maintains multiple concurrent
dialogue state hypotheses, and provides
a visual display showing how competing
hypotheses are ranked. The demo is
a prototype application for the Tourist
Information Domain and achieved a task
completion rate of over 90% in a recent
user study.
1 Partially Observable Markov Decision
Processes for Dialogue Systems
Recent work on statistical models for spoken di-
alogue systems has argued that Partially Observ-
able Markov Decision Processes (POMDPs) provide
a principled mathematical framework for modeling
the uncertainty inherent in human-machine dialogue
(Williams, 2006; Young, 2006; Williams and Young,
2007). Briefly speaking, POMDPs extend the tra-
ditional fully-observable Markov Decision Process
(MDP) framework by maintaining a belief state, ie.
a probability distribution over dialogue states. This
enables the dialogue manager to avoid and recover
from recognition errors by sharing and shifting prob-
ability mass between multiple hypotheses of the cur-
rent dialogue state. The framework also naturally
incorporates n-best lists of multiple recognition hy-
potheses coming from the speech recogniser.
Due to the vast number of possible dialogue states
and policies, the use of POMDPs in practical dia-
logue systems is far from straightforward. The size
of the belief state scales linearly with the number of
dialogue states and belief state updates at every turn
during a dialogue require all state probabilities to be
recomputed. This is too computationally intensive
to be practical with current technology. Worse than
that, the complexity involved in policy optimisation
grows exponentially with the number of states and
system actions and neither exact nor approximate al-
gorithms exist that provide a tractable solution for
systems with thousands of states.
2 The Hidden Information State (HIS)
Dialogue Manager
The Hidden Information State (HIS) dialogue man-
ager presented in this demonstration is the first train-
able and scalable dialogue system based on the
POMDP model. As described in (Young, 2006;
Young et al, 2007) it partitions the state space using
a tree-based representation of user goals so that only
a small set of partition beliefs needs to be updated
at every turn. In order to make policy optimisation
tractable, a much reduced summary space is main-
tained in addition to the master state space. Policies
are optimised in summary space and the selected
summary actions are then mapped back to master
space to form system actions. Apart from some very
simple ontology definitions, the dialog manager has
no application dependent heuristics.
The system uses a grid-based discretisation of the
27
Figure 1: The HIS Demo System is a Tourist Infor-
mation application for a fictitious town
state space and online -greedy policy optimisation.
While this offers the potential for online adaptation
with real users at a later stage, a simulated user is
needed to bootstrap the training process. A novel
agenda-based simulation technique was used for this
step, as described in (Schatzmann et al, 2007).
3 The HIS Demo System
The HIS demo system is a prototype application for
the Tourist Information domain. Users are assumed
to be visiting a fictitious town called ?Jasonville?
(see Fig. 1) and need to find a suitable hotel, bar
or restaurant subject to certain constraints. Exam-
ples of task scenarios are ?finding a cheap Chinese
restaurant near the post office in the centre of town?
or ?a wine bar with Jazz music on the riverside?.
Once a venue is found, users may request further in-
formation such as the phone number or the address.
At run-time, the system provides a visual display
(see Fig. 2) which shows how competing dialogue
state hypotheses are being ranked. This allows de-
velopers to gain a better understanding of the inter-
nal operation of the system.
4 Demo System Performance
In a recent user study the demo system was evalu-
ated by 40 human subjects. In total, 160 dialogues
were recorded with an average Word-Error-Rate of
29.8%. The performance of the system was mea-
sured based on the recommendation of a correct
venue and achieved a task completion rate of 90.6%
with an average number of 5.59 dialogue turns to
completion (Thomson et al, 2007).
Figure 2: A system screenshot showing the ranking
of competing dialogue state hypotheses
The results demonstrate that POMDPs facilitate
design and implementation of spoken dialogue sys-
tems, and that the implementation used in the HIS
dialogue manager can be scaled to handle real world
tasks. The user study results also show that a
simulated user can be successfully used to train a
POMDP dialogue policy that performs well in ex-
periments with real users.
5 Accompanying materials
The demo system and related materials are accessi-
ble online at our website
http://mi.eng.cam.ac.uk/research/dialogue/.
References
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-Based User Simulation for Boot-
strapping a POMDP Dialogue System. In Proceedings of
HLT/NAACL, Rochester, NY.
B. Thomson, J. Schatzmann, K. Weilhammer, H. Ye, and
S. Young. 2007. Training a real-world POMDP-based Di-
alogue System. In Proceedings of Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technology, Work-
shop at HLT/NAACL, Rochester, NY.
J. D. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Systems.
Computer Speech and Language, 21(2):231?422.
J. D. Williams. 2006. Partially Observable Markov Decision
Processes for Spoken Dialogue Management. Ph.D. thesis,
University of Cambridge.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye. 2007.
The Hidden Information State Approach to Dialog Manage-
ment. In Proc. of ICASSP (forthcoming), Honolulu, Hawaii.
S. Young. 2006. Using POMDPs for Dialog Management. In
Proc. of IEEE/ACL SLT, Palm Beach, Aruba.
28
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 9?16,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
9
10
11
12
13
14
15
16
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112?119,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Training and Evaluation of the HIS POMDP Dialogue System in Noise
M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, K. Yu, S. Young
Machine Intelligence Laboratory
Engineering Department
Cambridge University
United Kingdom
Abstract
This paper investigates the claim that a di-
alogue manager modelled as a Partially Ob-
servable Markov Decision Process (POMDP)
can achieve improved robustness to noise
compared to conventional state-based dia-
logue managers. Using the Hidden Infor-
mation State (HIS) POMDP dialogue man-
ager as an exemplar, and an MDP-based dia-
logue manager as a baseline, evaluation results
are presented for both simulated and real dia-
logues in a Tourist Information Domain. The
results on the simulated data show that the
inherent ability to model uncertainty, allows
the POMDP model to exploit alternative hy-
potheses from the speech understanding sys-
tem. The results obtained from a user trial
show that the HIS system with a trained policy
performed significantly better than the MDP
baseline.
1 Introduction
Conventional spoken dialogue systems operate by
finding the most likely interpretation of each user
input, updating some internal representation of the
dialogue state and then outputting an appropriate re-
sponse. Error tolerance depends on using confidence
thresholds and where they fail, the dialogue manager
must resort to quite complex recovery procedures.
Such a system has no explicit mechanisms for rep-
resenting the inevitable uncertainties associated with
speech understanding or the ambiguities which natu-
rally arise in interpreting a user?s intentions. The re-
sult is a system that is inherently fragile, especially
in noisy conditions or where the user is unsure of
how to use the system.
It has been suggested that Partially Observable
Markov Decision Processes (POMDPs) offer a nat-
ural framework for building spoken dialogue sys-
tems which can both model these uncertainties
and support policies which are robust to their ef-
fects (Young, 2002; Williams and Young, 2007a).
The key idea of the POMDP is that the underlying
dialogue state is hidden and dialogue management
policies must therefore be based not on a single state
estimate but on a distribution over all states.
Whilst POMDPs are attractive theoretically, in
practice, they are notoriously intractable for any-
thing other than small state/action spaces. Hence,
practical examples of their use were initially re-
stricted to very simple domains (Roy et al, 2000;
Zhang et al, 2001). More recently, however, a num-
ber of techniques have been suggested which do al-
low POMDPs to be scaled to handle real world tasks.
The two generic mechanisms which facilitate this
scaling are factoring the state space and perform-
ing policy optimisation in a reduced summary state
space (Williams and Young, 2007a; Williams and
Young, 2007b).
Based on these ideas, a number of real-world
POMDP-based systems have recently emerged. The
most complex entity which must be represented in
the state space is the user?s goal. In the Bayesian
Update of Dialogue State (BUDS) system, the user?s
goal is further factored into conditionally indepen-
dent slots. The resulting system is then modelled
as a dynamic Bayesian network (Thomson et al,
2008). A similar approach is also developed in
112
(Bui et al, 2007a; Bui et al, 2007b). An alterna-
tive approach taken in the Hidden Information State
(HIS) system is to retain a complete representation
of the user?s goal, but partition states into equiva-
lence classes and prune away very low probability
partitions (Young et al, 2007; Thomson et al, 2007;
Williams and Young, 2007b).
Whichever approach is taken, a key issue in a real
POMDP-based dialogue system is its ability to be
robust to noise and that is the issue that is addressed
in this paper. Using the HIS system as an exem-
plar, evaluation results are presented for a real-world
tourist information task using both simulated and
real users. The results show that a POMDP system
can learn noise robust policies and that N-best out-
puts from the speech understanding component can
be exploited to further improve robustness.
The paper is structured as follows. Firstly, in Sec-
tion 2 a brief overview of the HIS system is given.
Then in Section 3, various POMDP training regimes
are described and evaluated using a simulated user at
differing noise levels. Section 4 then presents results
from a trial in which users conducted various tasks
over a range of noise levels. Finally, in Section 5,
we discuss our results and present our conclusions.
2 The HIS System
2.1 Basic Principles
A POMDP-based dialogue system is shown in Fig-
ure 1 where sm denotes the (unobserved or hidden)
machine state which is factored into three compo-
nents: the last user act au, the user?s goal su and
the dialogue history sd. Since sm is unknown, at
each time-step the system computes a belief state
such that the probability of being in state sm given
belief state b is b(sm). Based on this current belief
state b, the machine selects an action am, receives
a reward r(sm, am), and transitions to a new (un-
observed) state s?m, where s?m depends only on sm
and am. The machine then receives an observation
o? consisting of an N-best list of hypothesised user
actions. Finally, the belief distribution b is updated
based on o? and am as follows:
b?(s?m) = kP (o?|s?m, am)
?
sm?Sm
P (s?m|am, sm)b(sm)
(1)
where k is a normalisation constant (Kaelbling et al,
1998). The first term on the RHS of (1) is called the
observation model and the term inside the summa-
tion is called the transition model. Maintaining this
belief state as the dialogue evolves is called belief
monitoring.
Speech
Understanding
Speech
Generation
User
au
am
~
su
am
Belief
Estimator
Dialog
Policy
sd
b(     )sm
sm = <au,s u,s d>
au
1
.
.

au
N
Figure 1: Abstract view of a POMDP-based spoken dia-
logue system
At each time step t, the machine receives a reward
r(bt, am,t) based on the current belief state bt and the
selected action am,t. Each action am,t is determined
by a policy ?(bt) and building a POMDP system in-
volves finding the policy ?? which maximises the
discounted sum R of the rewards
R =
??
t=0
?tr(bt, am,t) (2)
where ?t is a discount coefficient.
2.2 Probability Models
In the HIS system, user goals are partitioned and
initially, all states su ? Su are regarded as being
equally likely and they are placed in a single par-
tition p0. As the dialogue progresses, user inputs
result in changing beliefs and this root partition is
repeatedly split into smaller partitions. This split-
ting is binary, i.e. p ? {p?, p ? p?} with probability
P (p?|p). By replacing sm by its factors (su, au, sd)
and making reasonable independence assumptions,
it can be shown (Young et al, 2007) that in parti-
113
tioned form (1) becomes
b?(p?, a?u, s?d) = k ? P (o?|a?u)
? ?? ?
observation
model
P (a?u|p?, am)
? ?? ?
user action
model
?
?
sd
P (s?d|p?, a?u, sd, am)
? ?? ?
dialogue
model
P (p?|p)b(p, sd)
? ?? ?
partition
splitting
(3)
where p is the parent of p?.
In this equation, the observation model is approx-
imated by the normalised distribution of confidence
measures output by the speech recognition system.
The user action model allows the observation prob-
ability that is conditioned on a?u to be scaled by the
probability that the user would speak a?u given the
partition p? and the last system prompt am. In the
current implementation of the HIS system, user dia-
logue acts take the form act(a = v) where act is the
dialogue type, a is an attribute and v is its value [for
example, request(food=Chinese)]. The user action
model is then approximated by
P (a?u|p?, am) ? P (T (a?u)|T (am))P (M(a?u)|p?)
(4)
where T (?) denotes the type of the dialogue act
and M(?) denotes whether or not the dialogue act
matches the current partition p?. The dialogue
model is a deterministic encoding based on a simple
grounding model. It yields probability one when the
updated dialogue hypothesis (i.e., a specific combi-
nation of p?, a?u, sd and am) is consistent with the
history and zero otherwise.
2.3 Policy Representation
Policy representation in POMDP-systems is non-
trivial since each action depends on a complex prob-
ability distribution. One of the simplest approaches
to dealing with this problem is to discretise the state
space and then associate an action with each dis-
crete grid point. To reduce quantisation errors, the
HIS model first maps belief distributions into a re-
duced summary space before quantising. This sum-
mary space consists of the probability of the top
two hypotheses plus some status variables and the
user act type associated with the top distribution.
Quantisation is then performed using a simple dis-
tance metric to find the nearest grid point. Ac-
tions in summary space refer specifically to the top
two hypotheses, and unlike actions in master space,
they are limited to a small finite set: greet, ask, ex-
plicit confirm, implicit confirm, select confirm, of-
fer, inform, find alternative, query more, goodbye.
A simple heuristic is then used to map the selected
next system action back into the full master belief
space.
1
Observation
From
User
Ontology Rules
2
N
ua
ma
~
From
System
1
1
2
2
2
1
ds
2
ds
1
ds
2
ds
3
ds
1
up
2
up
3
up
POMDP
Policy
2h
3h
4h
5h
1h







~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
Belief
State
Application Database
Action
Refinement
(heuristic)
ma
^
Strategic
Action
Specific
Action
Map to
Summary
Space
ma
b
b^
Summary Space
Figure 2: Overview of the HIS system dialogue cycle
The dialogue manager is able to support nega-
tions, denials and requests for alternatives. When the
selected summary action is to offer the user a venue,
the summary-to-master space mapping heuristics
will normally offer a venue consistent with the most
likely user goal hypothesis. If this hypothesis is then
rejected its belief is substantially reduced and it will
no longer be the top-ranking hypothesis. If the next
system action is to make an alternative offer, then
the new top-ranking hypothesis may not be appro-
priate. For example, if an expensive French restau-
rant near the river had been offered and the user asks
for one nearer the centre of town, any alternative of-
fered should still include the user?s confirmed de-
sire for an expensive French restaurant. To ensure
this, all of the grounded features from the rejected
hypothesis are extracted and all user goal hypothe-
ses are scanned starting at the most likely until an
alternative is found that matches the grounded fea-
tures. For the current turn only, the summary-to-
master space heuristics then treat this hypothesis as
if it was the top-ranking one. If the system then of-
fers a venue based on this hypothesis, and the user
accepts it, then, since system outputs are appended
to user inputs for the purpose of belief updating, the
114
alternative hypothesis will move to the top, or near
the top, of the ranked hypothesis list. The dialogue
then typically continues with its focus on the newly
offered alternative venue.
2.4 Summary of Operation
To summarise, the overall processing performed by
the HIS system in a single dialogue turn (i.e. one cy-
cle of system output and user response) is as shown
in Figure 2. Each user utterance is decoded into an
N-best list of dialogue acts. Each incoming act plus
the previous system act are matched against the for-
est of user goals and partitions are split as needed.
Each user act au is then duplicated and bound to
each partition p. Each partition will also have a
set of dialogue histories sd associated with it. The
combination of each p, au and updated sd forms a
new dialogue hypothesis hk whose beliefs are eval-
uated using (3). Once all dialogue hypotheses have
been evaluated and any duplicates merged, the mas-
ter belief state b is mapped into summary space b?
and the nearest policy belief point is found. The as-
sociated summary space machine action a?m is then
heuristically mapped back to master space and the
machine?s actual response am is output. The cycle
then repeats until the user?s goal is satisfied.
3 Training and Evaluation with a
Simulated User
3.1 Policy optimisation
Policy optimisation is performed in the discrete
summary space described in the previous section us-
ing on-line batch ?-greedy policy iteration. Given
an existing policy ?, dialogs are executed and ma-
chine actions generated according to ? except that
with probability ? a random action is generated. The
system maintains a set of belief points {b?i}. At each
turn in training, the nearest stored belief point b?k to
b? is located using a distance measure. If the distance
is greater than some threshold, b? is added to the set
of stored belief points. The sequence of points b?k
traversed in each dialogue is stored in a list. As-
sociated with each b?i is a function Q(b?i, a?m) whose
value is the expected total reward obtained by choos-
ing summary action a?m from state b?i. At the end
of each dialogue, the total reward is calculated and
added to an accumulator for each point in the list,
discounted by ? at each step. On completion of a
batch of dialogs, the Q values are updated accord-
ing to the accumulated rewards, and the policy up-
dated by choosing the action which maximises each
Q value. The whole process is then repeated until
the policy stabilises.
In our experiments, ? was fixed at 0.1 and ? was
fixed at 0.95. The reward function used attempted
to encourage short successful dialogues by assign-
ing +20 for a successful dialogue and ?1 for each
dialogue turn.
3.2 User Simulation
To train a policy, a user simulator is used to gen-
erate responses to system actions. It has two main
components: a User Goal and a User Agenda. At
the start of each dialogue, the goal is randomly
initialised with requests such as ?name?, ?addr?,
?phone? and constraints such as ?type=restaurant?,
?food=Chinese?, etc. The agenda stores the di-
alogue acts needed to elicit this information in a
stack-like structure which enables it to temporarily
store actions when another action of higher priority
needs to be issued first. This enables the simulator
to refer to previous dialogue turns at a later point. To
generate a wide spread of realistic dialogs, the sim-
ulator reacts wherever possible with varying levels
of patience and arbitrariness. In addition, the sim-
ulator will relax its constraints when its initial goal
cannot be satisfied. This allows the dialogue man-
ager to learn negotiation-type dialogues where only
an approximate solution to the user?s goal exists.
Speech understanding errors are simulated at the di-
alogue act level using confusion matrices trained on
labelled dialogue data (Schatzmann et al, 2007).
3.3 Training and Evaluation
When training a system to operate robustly in noisy
conditions, a variety of strategies are possible. For
example, the system can be trained only on noise-
free interactions, it can be trained on increasing lev-
els of noise or it can be trained on a high noise level
from the outset. A related issue concerns the gener-
ation of grid points and the number of training itera-
tions to perform. For example, allowing a very large
number of points leads to poor performance due to
over-fitting of the training data. Conversely, having
too few point leads to poor performance due to a lack
115
of discrimination in its dialogue strategies.
After some experimentation, the following train-
ing schedule was adopted. Training starts in a
noise free environment using a small number of grid
points and it continues until the performance of the
policy levels off. The resulting policy is then taken
as an initial policy for the next stage where the noise
level is increased, the number of grid points is ex-
panded and the number of iterations is increased.
This process is repeated until the highest noise level
is reached. This approach was motivated by the ob-
servation that a key factor in effective reinforcement
learning is the balance between exploration and ex-
ploitation. In POMDP policy optimisation which
uses dynamically allocated grid points, maintaining
this balance is crucial. In our case, the noise intro-
duced by the simulator is used as an implicit mech-
anism for increasing the exploration. Each time ex-
ploration is increased, the areas of state-space that
will be visited will also increase and hence the num-
ber of available grid points must also be increased.
At the same time, the number of iterations must be
increased to ensure that all points are visited a suf-
ficient number of times. In practice we found that
around 750 to 1000 grid points was sufficient and
the total number of simulated dialogues needed for
training was around 100,000.
A second issue when training in noisy conditions
is whether to train on just the 1-best output from the
simulator or train on the N-best outputs. A limit-
ing factor here is that the computation required for
N-best training is significantly increased since the
rate of partition generation in the HIS model in-
creases exponentially with N. In preliminary tests,
it was found that when training with 1-best outputs,
there was little difference between policies trained
entirely in no noise and policies trained on increas-
ing noise as described above. However, policies
trained on 2-best using the incremental strategy did
exhibit increased robustness to noise. To illustrate
this, Figures 3 and 4 show the average dialogue suc-
cess rates and rewards for 3 different policies, all
trained on 2-best: a hand-crafted policy (hdc), a pol-
icy trained on noise-free conditions (noise free) and
a policy trained using the incremental scheme de-
scribed above (increm). Each policy was tested us-
ing 2-best output from the simulator across a range
of error rates. In addition, the noise-free policy was
also tested on 1-best output.
Figure 3: Average simulated dialogue success rate as a
function of error rate for a hand-crafted (hdc), noise-free
and incrementally trained (increm) policy.
Figure 4: Average simulated dialogue reward as a func-
tion of error rate for a hand-crafted (hdc), noise-free and
incrementally trained (increm) policy.
As can be seen, both the trained policies improve
significantly on the hand-crafted policies. Further-
more, although the average rewards are all broadly
similar, the success rate of the incrementally trained
policy is significantly better at higher error rates.
Hence, this latter policy was selected for the user
trial described next.
4 Evaluation via a User Trial
The HIS-POMDP policy (HIS-TRA) that was incre-
mentally trained on the simulated user using 2-best
lists was tested in a user trial together with a hand-
crafted HIS-POMDP policy (HIS-HDC). The strat-
egy used by the latter was to first check the most
likely hypothesis. If it contains sufficient grounded
116
keys to match 1 to 3 database entities, then offer is
selected. If any part of the hypothesis is inconsis-
tent or the user has explicitly asked for another sug-
gestion, then find alternative action is selected. If
the user has asked for information about an offered
entity then inform is selected. Otherwise, an un-
grounded component of the top hypothesis is identi-
fied and depending on the belief, one of the confirm
actions is selected.
In addition, an MDP-based dialogue manager de-
veloped for earlier trials (Schatzmann, 2008) was
also tested. Since considerable effort has been put in
optimising this system, it serves as a strong baseline
for comparison. Again, both a trained policy (MDP-
TRA) and a hand-crafted policy (MDP-HDC) were
tested.
4.1 System setup and confidence scoring
The dialogue system consisted of an ATK-based
speech recogniser, a Phoenix-based semantic parser,
the dialogue manager and a diphone based speech
synthesiser. The semantic parser uses simple phrasal
grammar rules to extract the dialogue act type and a
list of attribute/value pairs from each utterance.
In a POMDP-based dialogue system, accurate
belief-updating is very sensitive to the confidence
scores assigned to each user dialogue act. Ideally
these should provide a measure of the probability of
the decoded act given the true user act. In the evalu-
ation system, the recogniser generates a 10-best list
of hypotheses at each turn along with a compact con-
fusion network which is used to compute the infer-
ence evidence for each hypothesis. The latter is de-
fined as the sum of the log-likelihoods of each arc
in the confusion network and when exponentiated
and renormalised this gives a simple estimate of the
probability of each hypothesised utterance. Each ut-
terance in the 10-best list is passed to the semantic
parser. Equivalent dialogue acts output by the parser
are then grouped together and the dialogue act for
each group is then assigned the sum of the sentence-
level probabilities as its confidence score.
4.2 Trial setup
For the trial itself, 36 subjects were recruited (all
British native speakers, 18 male, 18 female). Each
subject was asked to imagine himself to be a tourist
in a fictitious town called Jasonville and try to find
particular hotels, bars, or restaurants in that town.
Each subject was asked to complete a set of pre-
defined tasks where each task involved finding the
name of a venue satisfying a set of constraints such
as food type is Chinese, price-range is cheap, etc.,
and getting the value of one or more additional at-
tributes of that venue such as the address or the
phone number.
For each task, subjects were given a scenario to
read and were then asked to solve the task via a di-
alogue with the system. The tasks set could either
have one solution, several solutions, or no solution
at all in the database. In cases where a subject found
that there was no matching venue for the given task,
he/she was allowed to try and find an alternative
venue by relaxing one or more of the constraints.
In addition, subjects had to perform each task at
one of three possible noise levels. These levels cor-
respond to signal/noise ratios (SNRs) of 35.3 dB
(low noise), 10.2 dB (medium noise), or 3.3 dB
(high noise). The noise was artificially generated
and mixed with the microphone signal, in addition
it was fed into the subject?s headphones so that they
were aware of the noisy conditions.
An instructor was present at all times to indicate
to the subject which task description to follow, and
to start the right system with the appropriate noise-
level. Each subject performed an equal number of
tasks for each system (3 tasks), noise level (6 tasks)
and solution type (6 tasks for each of the types 0, 1,
or multiple solutions). Also, each subject performed
one task for all combinations of system and noise
level. Overall, each combination of system, noise
level, and solution type was used in an equal number
of dialogues.
4.3 Results
In Table 1, some general statistics of the corpus re-
sulting from the trial are given. The semantic error
rate is based on substitutions, insertions and dele-
tions errors on semantic items. When tested after the
trial on the transcribed user utterances, the semantic
error rate was 4.1% whereas the semantic error rate
on the ASR input was 25.2%. This means that 84%
of the error rate was due to the ASR.
Tables 2 and 3 present success rates (Succ.) and
average performance scores (Perf.), comparing the
two HIS dialogue managers with the two MDP base-
117
Number of dialogues 432
Number of dialogue turns 3972
Number of words (transcriptions) 18239
Words per utterance 4.58
Word Error Rate 32.9
Semantic Error Rate 25.2
Semantic Error Rate transcriptions 4.1
Table 1: General corpus statistics.
line systems. For the success rates, also the stan-
dard deviation (std.dev) is given, assuming a bino-
mial distribution. The success rate is the percentage
of successfully completed dialogues. A task is con-
sidered to be fully completed when the user is able to
find the venue he is looking for and get al the addi-
tional information he asked for; if the task has no so-
lution and the system indicates to the user no venue
could be found, this also counts as full completion.
A task is considered to be partially completed when
only the correct venue has been given. The results on
partial completion are given in Table 2, and the re-
sults on full completion in Table 3. To mirror the re-
ward function used in training, the performance for
each dialogue is computed by assigning a reward of
20 points for full completion and subtracting 1 point
for the number of turns up until a successful recom-
mendation (i.e., partial completion).
Partial Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 68.52 (4.83) 4.80 8.91
MDP-TRA 70.37 (4.75) 4.75 9.32
HIS-HDC 74.07 (4.55) 7.04 7.78
HIS-TRA 84.26 (3.78) 4.63 12.22
Table 2: Success rates and performance results on partial
completion.
Full Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 64.81 (4.96) 5.86 7.10
MDP-TRA 65.74 (4.93) 6.18 6.97
HIS-HDC 63.89 (4.99) 8.57 4.20
HIS-TRA 78.70 (4.25) 6.36 9.38
Table 3: Success rates and performance results on full
completion.
The results show that the trained HIS dialogue
manager significantly outperforms both MDP based
dialogue managers. For success rate on partial com-
pletion, both HIS systems perform better than the
MDP systems.
4.3.1 Subjective Results
In the user trial, the subjects were also asked for
a subjective judgement of the systems. After com-
pleting each task, the subjects were asked whether
they had found the information they were looking
for (yes/no). They were also asked to give a score
on a scale from 1 to 5 (best) on how natural/intuitive
they thought the dialogue was. Table 4 shows the
results for the 4 systems used. The performance of
the HIS systems is similar to the MDP systems, with
a slightly higher success rate for the trained one and
a slightly lower score for the handcrafted one.
System Succ. Rate (std.dev) Score
MDP-HDC 78 (4.30) 3.52
MDP-TRA 78 (4.30) 3.42
HIS-HDC 71 (4.72) 3.05
HIS-TRA 83 (3.90) 3.41
Table 4: Subjective performance results from the user
trial.
5 Conclusions
This paper has described recent work in training a
POMDP-based dialogue manager to exploit the ad-
ditional information available from a speech under-
standing system which can generate ranked lists of
hypotheses. Following a brief overview of the Hid-
den Information State dialogue manager and pol-
icy optimisation using a user simulator, results have
been given for both simulated user and real user di-
alogues conducted at a variety of noise levels.
The user simulation results have shown that al-
though the rewards are similar, training with 2-best
rather than 1-best outputs from the user simulator
yields better success rates at high noise levels. In
view of this result, we would have liked to inves-
tigate training on longer N-best lists, but currently
computational constraints prevent this. We hope in
the future to address this issue by developing more
efficient state partitioning strategies for the HIS sys-
tem.
118
The overall results on real data collected from the
user trial clearly indicate increased robustness by the
HIS system. We would have liked to be able to
plot performance and success scores as a function
of noise level or speech understanding error rate,
but there is great variability in these kinds of com-
plex real-world dialogues and it transpired that the
trial data was insufficient to enable any statistically
meaningful presentation of this form. We estimate
that we need at least an order of magnitude more
trial data to properly investigate the behaviour of
such systems as a function of noise level. The trial
described here, including transcription and analysis
consumed about 30 man-days of effort. Increasing
this by a factor of 10 or more is not therefore an
option for us, and clearly an alternative approach is
needed.
We have also reported results of subjective suc-
cess rate and opinion scores based on data obtained
from subjects after each trial. The results were only
weakly correlated with the measured performance
and success rates. We believe that this is partly due
to confusion as to what constituted success in the
minds of the subjects. This suggests that for subjec-
tive results to be meaningful, measurements such as
these will only be really useful if made on live sys-
tems where users have a real rather than imagined
information need. The use of live systems would
also alleviate the data sparsity problem noted earlier.
Finally and in conclusion, we believe that despite
the difficulties noted above, the results reported in
this paper represent a first step towards establish-
ing the POMDP as a viable framework for develop-
ing spoken dialogue systems which are significantly
more robust to noisy operating conditions than con-
ventional state-based systems.
Acknowledgements
This research was partly funded by the UK EPSRC
under grant agreement EP/F013930/1 and by the
EU FP7 Programme under grant agreement 216594
(CLASSIC project: www.classic-project.org).
References
TH Bui, M Poel, A Nijholt, and J Zwiers. 2007a. A
tractable DDN-POMDP Approach to Affective Dia-
logue Modeling for General Probabilistic Frame-based
Dialogue Systems. In Proc 5th Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
pages 34?57.
TH Bui, B van Schooten, and D Hofs. 2007b. Practi-
cal dialogue manager development using POMDPs .
In 8th SIGdial Workshop on Discourse and Dialogue,
Antwerp.
LP Kaelbling, ML Littman, and AR Cassandra. 1998.
Planning and Acting in Partially Observable Stochastic
Domains. Artificial Intelligence, 101:99?134.
N Roy, J Pineau, and S Thrun. 2000. Spoken Dialogue
Management Using Probabilistic Reasoning. In Proc
ACL.
J Schatzmann, B Thomson, and SJ Young. 2007. Error
Simulation for Training Statistical Dialogue Systems.
In ASRU 07, Kyoto, Japan.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis, Uni-
versity of Cambridge.
B Thomson, J Schatzmann, K Weilhammer, H Ye, and
SJ Young. 2007. Training a real-world POMDP-based
Dialog System. In HLT/NAACL Workshop ?Bridging
the Gap: Academic and Industrial Research in Dialog
Technologies?, Rochester.
B Thomson, J Schatzmann, and SJ Young. 2008.
Bayesian Update of Dialogue State for Robust Dia-
logue Systems. In Int Conf Acoustics Speech and Sig-
nal Processing ICASSP, Las Vegas.
JD Williams and SJ Young. 2007a. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams and SJ Young. 2007b. Scaling POMDPs
for Spoken Dialog Management. IEEE Audio, Speech
and Language Processing, 15(7):2116?2129.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Dia-
log Management. In ICASSP 2007, Honolulu, Hawaii.
SJ Young. 2002. Talking to Machines (Statistically
Speaking). In Int Conf Spoken Language Processing,
Denver, Colorado.
B Zhang, Q Cai, J Mao, E Chang, and B Guo. 2001.
Spoken Dialogue Management as Planning and Acting
under Uncertainty. In Eurospeech, Aalborg, Denmark.
119
