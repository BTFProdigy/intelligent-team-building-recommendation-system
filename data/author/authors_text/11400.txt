Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 27?32,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 7: Argument Selection and Coercion
James Pustejovsky and Anna Rumshisky and Alex Plotnick
Dept. of Computer Science
Brandeis University
Waltham, MA, USA
Elisabetta Jezek
Dept. of Linguistics
University of Pavia
Pavia, Italy
Olga Batiukova
Dept. of Humanities
Carlos III University of Madrid
Madrid, Spain
Valeria Quochi
ILC-CNR
Pisa, Italy
Abstract
We describe the Argument Selection and
Coercion task for the SemEval-2010 eval-
uation exercise. This task involves char-
acterizing the type of compositional oper-
ation that exists between a predicate and
the arguments it selects. Specifically, the
goal is to identify whether the type that
a verb selects is satisfied directly by the
argument, or whether the argument must
change type to satisfy the verb typing. We
discuss the problem in detail, describe the
data preparation for the task, and analyze
the results of the submissions.
1 Introduction
In recent years, a number of annotation schemes
that encode semantic information have been de-
veloped and used to produce data sets for training
machine learning algorithms. Semantic markup
schemes that have focused on annotating entity
types and, more generally, word senses, have
been extended to include semantic relationships
between sentence elements, such as the seman-
tic role (or label) assigned to the argument by the
predicate (Palmer et al, 2005; Ruppenhofer et al,
2006; Kipper, 2005; Burchardt et al, 2006; Subi-
rats, 2004).
In this task, we take this one step further and
attempt to capture the ?compositional history? of
the argument selection relative to the predicate. In
particular, this task attempts to identify the oper-
ations of type adjustment induced by a predicate
over its arguments when they do not match its se-
lectional properties. The task is defined as fol-
lows: for each argument of a predicate, identify
whether the entity in that argument position satis-
fies the type expected by the predicate. If not, then
identify how the entity in that position satisfies the
typing expected by the predicate; that is, identify
the source and target types in a type-shifting or co-
ercion operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition, as in (1a). Notice, however, that through
a metonymic interpretation, this constraint can be
violated, as demonstrated in (1b).
(1) a. John reported in late from Washington.
b. Washington reported in late.
Neither the surface annotation of entity extents
and types nor assigning semantic roles associated
with the predicate would reflect in this case a cru-
cial point: namely, that in order for the typing
requirements of the predicate to be satisfied, a
type coercion or a metonymy (Hobbs et al, 1993;
Pustejovsky, 1991; Nunberg, 1979; Egg, 2005)
has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This
task involved two types with their metonymic
variants: categories-for-locations (e.g., place-
for-people) and categories-for-organizations (e.g.,
organization-for-members). One of the limitations
of this approach, however, is that while appropri-
ate for these specialized metonymy relations, the
annotation specification and resulting corpus are
not an informative guide for extending the annota-
tion of argument selection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (2) below, the sense annotation
for the verb enjoy should arguably assign similar
values to both (2a) and (2b).
27
Figure 1: The MATTER Methodology
(2) a. Mary enjoyed drinking her beer.
b. Mary enjoyed her beer.
The consequence of this is that under current sense
and role annotation strategies, the mapping to a
syntactic realization for a given sense is made
more complex, and is in fact perplexing for a clus-
tering or learning algorithm operating over subcat-
egorization types for the verb.
2 Methodology of Annotation
Before introducing the specifics of the argument
selection and coercion task, we will briefly review
our assumptions regarding the role of annotation
in computational linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough
to capture the desired behavior. These linguistic
descriptions are typically distilled from extensive
theoretical modeling of the phenomenon. The de-
scriptions in turn form the basis for the annota-
tion values of the specification language, which
are themselves the features used in a development
cycle for training and testing a labeling algorithm
over a text. Finally, based on an analysis and eval-
uation of the performance of a system, the model
of the phenomenon may be revised.
We call this cycle of development the MATTER
methodology (Fig. 1):
Model: Structural descriptions provide theoretically in-
formed attributes derived from empirical observations
over the data;
Annotate: Annotation scheme assumes a feature set that en-
codes specific structural descriptions and properties of
the input data;
Train: Algorithm is trained over a corpus annotated with the
target feature set;
Test: Algorithm is tested against held-out data;
Evaluate: Standardized evaluation of results;
Revise: Revisit the model, annotation specification, or algo-
rithm, in order to make the annotation more robust and
reliable.
Some of the current and completed annotation ef-
forts that have undergone such a development cy-
cle include PropBank (Palmer et al, 2005), Nom-
Bank (Meyers et al, 2004), and TimeBank (Puste-
jovsky et al, 2005).
3 Task Description
The argument selection and coercion (ASC) task
involves identifying the selectional mechanism
used by the predicate over a particular argument.
1
For the purposes of this task, the possible relations
between the predicate and a given argument are re-
stricted to selection and coercion. In selection, the
argument NP satisfies the typing requirements of
the predicate, as in (3):
(3) a. The spokesman denied the statement (PROPOSI-
TION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn?t believe the rumor (PROPOSI-
TION).
Coercion occurs when a type-shifting operation
must be performed on the complement NP in order
to satisfy selectional requirements of the predicate,
as in (4). Note that coercion operations may apply
to any argument position in a sentence, including
the subject, as seen in (4b). Coercion can also be
seen as an object of a proposition, as in (4c).
(4) a. The president denied the attack (EVENT? PROPO-
SITION).
b. The White House (LOCATION ? HUMAN) denied
this statement.
c. The Boston office called with an update (EVENT?
INFO).
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve (1) identifying the verb sense and the asso-
ciated syntactic frame, (2) identifying selectional
requirements imposed by that verb sense on the
target argument, and (3) identifying the semantic
type of the target argument.
4 Resources and Corpus Development
We prepared the data for this task in two phases:
the data set construction phase and the annotation
phase (see Fig. 2). The first phase consisted of
(1) selecting the target verbs to be annotated and
compiling a sense inventory for each target, and
(2) data extraction and preprocessing. The pre-
pared data was then loaded into the annotation in-
terface. During the annotation phase, the annota-
tion judgments were entered into the database, and
an adjudicator resolved disagreements. The result-
ing database was then exported in an XML format.
1
This task is part of a larger effort to annotate text with
compositional operations (Pustejovsky et al, 2009).
28
Figure 2: Corpus Development Architecture
4.1 Data Set Construction Phase: English
For the English data set, the data construction
phase was combined with the annotation phase.
The data for the task was created using the fol-
lowing steps:
1. The verbs were selected by examining the data
from the BNC, using the Sketch Engine (Kilgar-
riff et al, 2004) as described in (Rumshisky and
Batiukova, 2008). Verbs that consistently im-
pose semantic typing on one of their arguments
in at least one of their senses (strongly coercive
verbs) were included into the final data set: ar-
rive (at), cancel, deny, finish, and hear.
2. Sense inventories were compiled for each verb,
with the senses mapped to OntoNotes (Pradhan
et al, 2007) whenever possible. For each sense,
a set of type templates was compiled using a
modification of the CPA technique (Hanks and
Pustejovsky, 2005; Pustejovsky et al, 2004):
every argument in the syntactic pattern asso-
ciated with a given sense was assigned a type
specification. Although a particular sense is
often compatible with more than one semantic
type for a given argument, this was never the
case in our data set, where no disjoint types
were tested. The coercive senses of the chosen
verbs were associated with the following type
templates:
a. Arrive (at), sense reach a destination or goal : HU-
MAN arrive at LOCATION
b. Cancel, sense call off : HUMAN cancel EVENT
c. Deny, sense state or maintain that something is un-
true: HUMAN deny PROPOSITION
d. Finish, sense complete an activity: HUMAN finish
EVENT
e. Hear, sense perceive physical sound : HUMAN hear
SOUND
We used a subset of semantic types from the
Brandeis Shallow Ontology (BSO), which is a
shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky
et al, 2004; Rumshisky et al, 2006). Types
were selected for their prevalence in manually
identified selection context patterns developed
for several hundred English verbs. That is,
they capture common semantic distinctions as-
sociated with the selectional properties of many
verbs. The types used for annotation were:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
DOCUMENT,DRINK,EMOTION,ENTITY,EVENT, FOOD,
HUMAN,HUMAN GROUP, IDEA, INFORMATION, LOCA-
TION,OBLIGATION,ORGANIZATION, PATH, PHYSICAL
OBJECT, PROPERTY, PROPOSITION,RULE, SENSATION,
SOUND, SUBSTANCE, TIME PERIOD, VEHICLE
This set of types is purposefully shallow and
non-hierarchical. For example, HUMAN is a
subtype of both ANIMATE and PHYSICAL OB-
JECT, but annotators and system developers
were instructed to choose the most relevant type
(e.g., HUMAN) and to ignore inheritance.
3. A set of sentences was randomly extracted for
each target verb from the BNC (Burnard, 1995).
The extracted sentences were parsed automati-
cally, and the sentences organized according to
the grammatical relation the target verb was in-
volved in. Sentences were excluded from the set
if the target argument was expressed as anaphor,
or was not present in the sentence. The seman-
tic head for the target grammatical relation was
identified in each case.
4. Word sense disambiguation of the target predi-
cate was performed manually on each extracted
sentence, matching the target against the sense
inventory and the corresponding type templates
as described above. The appropriate senses
were then saved into the database along with the
associated type template.
5. The sentences containing coercive senses of the
target verbs were loaded into the Brandeis An-
notation Tool (Verhagen, 2010). Annotators
were presented with a list of sentences and
asked to determine whether the argument in
the specified grammatical relation to the target
belongs to the type associated with that sense
in the corresponding template. Disagreements
were resolved by adjudication.
29
Coerion Type Verb Train Test
EVENT?LOCATION arrive at 38 37
ARTIFACT?EVENT cancel 35 35
finish 91 92
EVENT?PROPOSITION deny 56 54
ARTIFACT?SOUND hear 28 30
EVENT?SOUND hear 24 26
DOCUMENT?EVENT finish 39 40
Table 1: Coercions in the English data set
6. To guarantee robustness of the data, two addi-
tional steps were taken. First, only the six most
recurrent coercion types were selected; these
are given in table 1. Preference was given to
cross-domain coercions, where the source and
the target types are not related ontologically.
Second, the distribution of selection and co-
ercion instances were skewed to increase the
number of coercions. The final English data set
contains about 30% coercions.
7. Finally, the data set was randomly split in half
into a training set and a test set. The training
data has 1032 instances, 311 of which are co-
ercions, and the test data has 1039 instances,
314 of which are coercions.
4.2 Data Set Construction Phase: Italian
In constructing the Italian data set, we adopted the
same methodology used for the English data set,
with the following differences:
1. The list of coercive verbs was selected by exam-
ining data from the ItWaC (Baroni and Kilgar-
riff, 2006) using the Sketch Engine (Kilgarriff
et al, 2004):
accusare ?accuse?, annunciare ?announce?, arrivare ?ar-
rive?, ascoltare ?listen?, avvisare ?inform?, chiamare
?call?, cominciare ?begin?, completare ?complete?, con-
cludere ?conclude?, contattare ?contact?, divorare ?de-
vour?, echeggiare ?echo?, finire ?finish?, informare ?in-
form?, interrompere ?interrupt?, leggere ?read?, raggiun-
gere ?reach?, recar(si) ?go to?, rimbombare ?resound?,
sentire ?hear?, udire ?hear?, visitare ?visit?.
2. The coercive senses of the chosen verbs were
associated with type templates, some of which
are listed listed below. Whenever possible,
senses and type templates were adapted from
the Italian Pattern Dictionary (Hanks and Jezek,
2007) and mapped to their SIMPLE equiva-
lents (Lenci et al, 2000).
a. arrivare, sense reach a location: HUMAN arriva
[prep] LOCATION
b. cominciare, sense initiate an undertaking: HUMAN
comincia EVENT
c. completare, sense finish an activity: HUMAN com-
pleta EVENT
d. udire, sense perceive a sound : HUMAN ode SOUND
e. visitare, sense visit a place: HUMAN visita LOCA-
TION
The following types were used to annotate
the Italian dataset:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY,
EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, IN-
FORMATION, LIQUID, LOCATION, ORGANIZATION,
PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND,
TIME PERIOD, VEHICLE
The annotators were provided with a set of def-
initions and examples of each type.
3. A set of sentences for each target verb was ex-
tracted and parsed from the PAROLE sottoin-
sieme corpus (Bindi et al, 2000). They were
skimmed to ensure that the final data set con-
tained a sufficient number of coercions, with
proportionally more selections than coercions.
Sentences were preselected to include instances
representing one of the chosen senses.
4. In order to exclude instances that may have been
wrongly selected, a judge performed word sense
disambiguation of the target predicate in the ex-
tracted sentences.
5. Annotators were presented with a list of sen-
tences and asked to determine the usual seman-
tic type associated with the argument in the
specified grammatical relation. Every sentence
was annotated by two annotators and one judge,
who resolved disagreements.
6. Some of the coercion types selected for Italian
were:
a. LOCATION? HUMAN (accusare, annunciare)
b. ARTIFACT? HUMAN (annunciare, avvisare)
c. EVENT? LOCATION (arrivare, raggiungere)
d. ARTIFACT? EVENT (cominciare, completare)
e. EVENT? DOCUMENT (leggere, divorare)
f. HUMAN? DOCUMENT (leggere, divorare)
g. EVENT? SOUND (ascoltare, echeggiare)
h. ARTIFACT? SOUND (ascoltare, echeggiare)
7. The Italian training data contained 1466 in-
stances, 381 of which are coercions; the test
data had 1463 instances, with 384 coercions.
5 Data Format
The test and training data were provided in XML.
The relation between the predicate (viewed as
a function) and its argument were represented
by composition link elements (CompLink), as
30
shown below. The test data differed from the train-
ing data in the omission of CompLink elements.
In case of coercion, there is a mismatch between
the source and the target types, and both types
need to be identified; e.g., The State Department
repeatedly denied the attack:
The State Department repeatedly
<SELECTOR sid="s1">denied</SELECTOR>
the <TARGET id="t1">attack</TARGET>.
<CompLink cid="cid1"
compType="COERCION"
selector_id="s1"
relatedToTarget="t1"
sourceType="EVENT"
targetType="PROPOSITION"/>
When the compositional operation is selection,
the source and target types must match; e.g., The
State Department repeatedly denied the statement:
The State Department repeatedly
<SELECTOR sid="s2">denied</SELECTOR>
the <TARGET id="t2">statement</TARGET>.
<CompLink cid="cid2"
compType="SELECTION"
selector_id="s2"
relatedToTarget="t2"
sourceType="PROPOSITION"
targetType="PROPOSITION"/>
6 Results & Analysis
We received only a single submission for the
ASC task. The UTDMet system was an SVM-
based system with features derived from two main
sources: a PageRank-style algorithm over Word-
Net hypernyms used to define semantic classes,
and statistics from a PropBank-style parse of some
8 million documents from the English Gigaword
corpus. The results, shown in Table 2, were
computed from confusion matrices constructed for
each of four classification tasks for the 1039 link
instances in the English test data: determination
of argument selection or coercion, identification of
the argument source type, identification of the ar-
gument target type, and the joint identification of
the source/target type pair.
Clearly, the UTDMet system did quite well at
this task. The one immediately noticeable outlier
is the macro-averaged precision for the joint type,
which reflects a small number of miscategoriza-
tions of rare types. For example, eliminating the
single miscategorized ARTIFACT-LOCATION link
in the submitted test data bumps this score up to
a respectable 94%. This large discrepancy can ex-
plained by the lack of any coercions with those
types in the gold-standard data.
Prec. Recall Averaging
Selection vs. 95 96 (macro)
Coercion: 96 96 (micro)
Source Type: 96 96 (macro)
96 96 (micro)
Target Type: 100 100 (both)
Joint Type: 86 95 (macro)
96 96 (micro)
Table 2: Results for the UTDMet submission.
In the absence of any other submissions, it is
difficult to provide a point of comparison for this
performance. However, we can provide a base-
line by taking each link to be a selection whose
source and target types are the most common type
(EVENT for the gold-standard English data). This
yields micro-averaged precision scores of 69% for
selection vs. coercion, 33% for source type iden-
tification, 37% for the target type identification,
and 22% for the joint type.
The performance of the UTDMet system sug-
gests that most of the type coercions were identifi-
able based largely on examination of lexical clues
associated with selection contexts. This is in fact
to be expected for the type coercions that were the
focus of the English data set. It will be interesting
to see how systems perform on the Italian data set
and an expanded corpus for English and Italian,
where more subtle and complex type exploitations
and manipulations are at play. These will hope-
fully be explored in future competitions.
7 Conclusion
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2010. This
task involves identifying the relation between a
predicate and its argument as one that encodes
the compositional history of the selection process.
This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of
a predicate from those that are coerced in context.
We described some details of a specification lan-
guage for selection, the annotation task using this
specification to identify argument selection behav-
ior, and the preparation of the data for the task.
Finally, we analyzed the results of the task sub-
missions.
31
References
M. Baroni and A. Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In Proceedings of European ACL.
R. Bindi, P. Baroni, M. Monachini, and E. Gola. 2000.
PAROLE-Sottoinsieme. ILC-CNR Internal Report.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC,
Genoa, Italy.
L. Burnard, 1995. Users? Reference Guide, British Na-
tional Corpus. British National Corpus Consortium,
Oxford, England.
Marcus Egg. 2005. Flexible semantics for reinterpre-
tation phenomena. CSLI, Stanford.
P. Hanks and E. Jezek. 2007. Building Pattern Dictio-
naries with Corpus Analysis. In International Col-
loquium on Possible Dictionaries, Rome, June, 6-7.
Oral Presentation.
P. Hanks and J. Pustejovsky. 2005. A pattern dic-
tionary for natural language processing. Revue
Franc?aise de Linguistique Appliqu?ee.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpre-
tation as abduction. Artificial Intelligence, 63:69?
142.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105?116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, Univer-
sity of Pennsylvania, PA.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowski, I. Peters, W. Peters,
N. Ruimy, et al 2000. SIMPLE: A general frame-
work for the development of multilingual lexicons.
International Journal of Lexicography, 13(4):249.
K. Markert and M. Nissim. 2007. SemEval-2007
task 8: Metonymy resolution. In Eneko Agirre,
Llu??s M`arquez, and Richard Wicentowski, editors,
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31.
Geoffrey Nunberg. 1979. The non-uniqueness of se-
mantic solutions: Polysemy. Linguistics and Phi-
losophy, 3:143?184.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, E. Hovy, MS Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In
International Conference on Semantic Computing,
2007, pages 517?526.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924?931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123?164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth Interna-
tional Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Compu-
tational Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy
in verbs: systematic relations between senses and
their effect on annotation. In COLING Workshop
on Human Judgement in Computational Linguistics
(HJCL-2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Carlos Subirats. 2004. FrameNet Espa?nol. Una red
sem?antica de marcos conceptuales. In VI Interna-
tional Congress of Hispanic Linguistics, Leipzig.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
32
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 33?41
Manchester, August 2008
Polysemy in verbs: systematic relations between senses
and their effect on annotation
Anna Rumshisky
?Dept. of Computer Science
Brandeis University
Waltham, MA USA
arum@cs.brandeis.edu
Olga Batiukova??
?Dept. of Spanish Philology
Madrid Autonomous University
Madrid, Spain
volha.batsiukova@uam.es
Abstract
Sense inventories for polysemous predicates
are often comprised by a number of related
senses. In this paper, we examine different
types of relations within sense inventories and
give a qualitative analysis of the effects they
have on decisions made by the annotators and
annotator error. We also discuss some common
traps and pitfalls in design of sense inventories.
We use the data set developed specifically for
the task of annotating sense distinctions depen-
dent predominantly on semantics of the argu-
ments and only to a lesser extent on syntactic
frame.
1 Introduction
Lexical ambiguity is pervasive in natural language, and
its resolution has been used to improve performance of
a number of natural language processing (NLP) appli-
cations, such as statistical machine translation (Chan
et al, 2007; Carpuat and Wu, 2007), cross-language
information retrieval and question answering (Resnik,
2006). Sense differentiation for the predicates depends
on a number of factors, including syntactic frame, se-
mantics of the arguments and adjuncts, contextual clues
from the wider context, text domain identification, etc.
Preparing sense-tagged data for training and evalua-
tion of word sense disambiguation (WSD) systems in-
volves two stages: (1) creating a sense inventory and
(2) applying it in annotation. Creating sense invento-
ries for polysemous words is a task that is notoriously
difficult to formalize. For polysemous verbs especially,
constellations of related meanings make this task even
more difficult. In lexicography, ?lumping and splitting?
senses during dictionary construction ? i.e. deciding
when to describe a set of usages as a separate sense
? is a well-known problem (Hanks and Pustejovsky,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2005; Kilgarriff, 1997). It is often resolved on an ad-
hoc basis, resulting in numerous cases of ?overlapping
senses?, i.e. instances when the same occurrence may
fall under more than one sense category simultaneously.
This problem has also been the subject of extensive
study in lexical semantics, addressing questions such
as when the context selects a distinct sense and when
it merely modulates the meaning, what is the regular
relationship between related senses, and what composi-
tional processes are involved in sense selection (Puste-
jovsky, 1995; Cruse, 1995; Apresjan, 1973). A num-
ber of syntactic and semantic tests are traditionally ap-
plied for sense identification, such as examining syn-
onym series, compatible syntactic environments, coor-
dination tests such as cross-understanding or zeugma
test (Cruse, 2000). None of these tests are conclu-
sive and normally a combination of factors is used.
At the recent Senseval competitions (Mihalcea et al,
2004; Snyder and Palmer, 2004; Preiss and Yarowsky,
2001), the choice of sense inventories frequently pre-
sented problems, spurring the efforts to create coarser-
grained sense inventories (Hovy et al, 2006; Palmer et
al., 2007; Navigli, 2006).
Part of the reason for such difficulties in establish-
ing a set of senses available to a lexical item is that
the meaning of a polysemous verb is often determined
in composition and depends to the same extent on se-
mantics of the particular arguments as it does on the
base meaning of the verb itself. A number of system-
atic relations often holds between different senses of a
polysemous verb. Depending on the kind of ambiguity
involved in each case, some senses are easier to dis-
tinguish than others. Sense-tagged data (e.g. SemCor
(Landes et al, 1998), PropBank (Palmer et al, 2005),
OntoNotes (Hovy et al, 2006)) typically provides no
way to differentiate between sense distinctions moti-
vated by different factors. Treating different disam-
biguation factors separately would allow one to exam-
ine the contribution of each factor, as well as the success
of a given algorithm in identifying the corresponding
senses.
Within the scope of a sentence, syntactic frame and
semantics of the arguments are most prominent in sense
33
disambiguation. The latter is often more subtle and
hence complex. Our goal in the present study was to tar-
get sense distinctions motivated strongly or exclusively
by differences in argument semantics. We base the
present discussion on the sense-tagged data set we de-
veloped for 20 polysemous verbs. We argue below that
cases which can not be reliably disambiguated by hu-
mans introduce noise into the data and therefore should
be kept out, a principle adhered to in the design of this
data set.
The choice of argument semantics as the target dis-
ambiguation factor was motivated by several consider-
ations. In automatic sense detection systems, argument
semantics is often represented using external resources
such as thesauri or shallow ontologies. Sense induction
systems using distributional information often do not
take into account the possible implications of induced
word clusters for sense disambiguation. Our goal was
to analyze differences in argument semantics that con-
tribute to disambiguation.
In this paper, we discuss different kinds of systematic
relations observed between senses of polysemous pred-
icates and examine the effects they have on decisions
made by the annotators. We also examine sense in-
ventories for other factors that influence inter-annotator
agreement rates and lead to annotation error. In Section
2, we discuss some of the factors that influence com-
pilation of sense inventories and the methodology in-
volved. In Section 3, we describe briefly the data set
and the annotation task. In Sections 4 and 5, we discuss
the relations observed between different senses within
sense inventories in our data set, their effect on deci-
sions made by the annotators, and the related annotation
errors.
2 Defining A Sense Inventory
Several current resource-oriented projects undertake to
formalize the procedure of identifying a word sense.
FrameNet (Ruppenhofer et al, 2006) attempts to orga-
nize lexical information in terms of script-like semantic
frames, with semantic and syntactic combinatorial pos-
sibilities specified for each frame-evoking lexical unit
(word/sense pairing). Semantics of the arguments is
represented by Fillmore?s case roles (frame elements)
which are derived on ad-hoc basis for each frame.
In OntoNotes project, annotators use small-scale cor-
pus analysis to create sense inventories derived by
grouping together WordNet senses. The procedure is
restricted to maintain 90% inter-annotator agreement
(Hovy et al, 2006).
Corpus Pattern Analysis (CPA) (Hanks and Puste-
jovsky, 2005; Pustejovsky et al, 2004) attempts to cat-
alog prototypical norms of usage for individual words,
specifying them in terms of context patterns. As a cor-
pus analysis technique, CPA has its origins in the anal-
ysis of large corpora for lexicographic purposes, of the
kind that was used for compiling the Cobuild dictionary
(Sinclair and Hanks, 1987). Each pattern gives a com-
bination of surface textual clues and argument specifi-
cations. A lexicographer creates a set of patterns by
sorting a concordance for the target predicate according
to the context features. In the present study, we use a
modification of the CPA technique in the way explained
in Section 3.
In CPA, syntactic and textual clues include argu-
ment structure and minor syntactic categories such as
locatives and adjuncts; collocates from wider context;
subphrasal cues such as genitives, partitives, bare plu-
ral/determiner, infinitivals, negatives, etc. Semantics
of the arguments is represented either through a set of
shallow semantic types corresponding to basic seman-
tic features (e.g. Person, Location, PhysObj, Abstract,
Event, etc.) or extensionally through lexical sets, which
are effectively collections of lexical items.1
Several CPA patterns may correspond to a single
sense. The patterns vary in syntactic structure or the en-
coding of semantic roles relative to the described event.
For example, for the verb treat, DOCTOR treating PA-
TIENT and DOCTOR treating DISEASE both correspond
to the medical sense of treat. Knowing which seman-
tic role is expressed by a particular argument is often
useful for performing inference. For instance, treating
a disease eliminates the disease, but not the patient. In
the present annotation task, each pattern is viewed as
sense in construction and labeled as a separate sense.
In the rest of the paper, we will use the term ?sense? to
refer also to such microsenses.
For the cases where sense differentiation depends
strongly on differences in semantics of the arguments,
several factors further complicate creating a sense in-
ventory. Prototypicality as a general principle of cat-
egory organization seems to play an important role in
defining both the boundaries of senses and the corre-
sponding argument groupings. The same sense of the
predicate is often activated by a number of semantically
diverse arguments. Such argument sets are frequently
organized around a core of typical members that are
a ?good fit? with respect to semantic requirements of
the corresponding sense of the target. The relevant se-
mantic feature is prominent for them, while other, more
peripheral members of the argument set, merely allow
the relevant interpretation (see Rumshisky (2008) for
discussion). For example, the verb absorb has a sense
involving absorbing a substance, and the typical mem-
bers of the corresponding argument set would be actual
substances, such as oil, oxygen, water, air, salt, etc. But
goodness, dirt, flavor, moisture would also activate the
same sense.
Each decision to split a sense and make another cat-
egory is to a certain extent an arbitrary decision. For
example, for the verb absorb, one can separate absorb-
ing a substance (oil, oxygen, water, air, salt) from ab-
sorbing energy (radiation, heat, sound, energy). The
latter sense may or may not be separated from absorb-
1See Rumshisky et al (2006) and Pustejovsky et al (2004)
for more detail.
34
ing impact (blow, shock, stress). But it is a marked con-
tinuum, i.e. certain points in the continuum are more
prominent, with necessity of a given concept reflected
in the frequency of use.
When several senses are postulated based on argu-
ment distinctions, there are almost always boundary
cases that can be seen to belong to both categories.
Consider, for example, two senses defined for the verb
launch and the corresponding direct objects in (1):
(1) a. Physically propel an object into the air or water
missile, rocket, torpedo, satellite, shuttle, craft
b. Begin or initiate an endeavor
campaign, initiative, investigation, expedition, drive,
competition, crusade, attack, assault, inquiry
The senses seem to be very clearly separated, yet ex-
amples like launch a ship clearly fall on the bound-
ary: while ships are physical objects propelled into wa-
ter, launching a ship can be virtually synonymous with
launching an expedition.
Similarly, for the verb conclude, two senses below
which are linked to nominal complements are clearly
separated:
(2) a. finish
meeting, debate, investigation, visit, tour, discussion;
letter, chapter, novel
b. reach an agreement
treaty, agreement, deal, contract, truce, alliance,
ceasefire, sale
However, conclude negotiations is clearly a boundary
case where both interpretations are equally possible
(negotiations may be concluded without reaching an
agreement). In fact, the two annotators chose different
senses for this example:2
(3) We were able to operate under a lease agreement until
purchase negotiations were concluded.
annoA: finish
annoB: reach an agreement
In many cases, postulating a separate sense for a co-
herent set of nominal complements is not justified, as
there are regular semantic processes that allow the com-
plements to satisfy selectional requirements of the verb.
For example, the verb conclude, in the finish sense ac-
cepts EVENT complements. Therefore, nouns such as
letter, chapter, novel in (2) must be coerced into events
corresponding to the activity that typically brings them
about, that is, re-interpreted as events of writing (their
Agentive quale, cf. Pustejovsky (1995)). Similarly, the
verb deny in the first sense (state or maintain that some-
thing is untrue) accepts PROPOSITION complements:
(4) a. state or maintain that something is untrue
allegations, reports, rumour; significance, impor-
tance, difference; attack, assault, involvement
b. refuse to grant something
access, visa, approval, funding, license
2All examples are taken from the annotated data set.
In some cases, sentence structure was slightly modified for
brevity.
Event nouns such as attack and assault are coerced into
a propositional reading, as are relational nouns such as
significance and importance.
Interestingly, as we have noted before (Rumshisky
et al, 2006), each predicate imposes its own gradation
with respect to prototypicality of elements of the ar-
gument set. As a result, even though basic semantic
types such as PHYSOBJ, ANIMATE, EVENT, are used
uniformly by many predicates, argument sets, while se-
mantically similar, typically differ between predicates.
For example, fall in the subject position and cut in the
direct object position select for things that can be de-
creased:
(5) a. cut (dobj): reduce or lessen
price, inflation, profits, cost, emission, spending,
deficit, wages overhead, production, consumption,
fees, staff
b. fall (subj): decrease
price, inflation, profits, attendance, turnover, temper-
ature, membership, import, demand, level
While there is a clear commonality between these argu-
ment sets, the overlap is only partial. To give another
example, consider INFORMATION-selecting predicates
explain (subj), grasp (dobj) and know (dobj). The nouns
book and note occur in the subject position of explain;
answer occurs both as the subject of explain and direct
object of know; however, grasp accepts neither of these
nouns as direct object. Thus, the actual selectional be-
havior of the predicates does not seem to be well de-
scribed in terms of a fixed set of types, which is what
is typically assumed by many ontologies used in auto-
matic WSD.
3 Task Description
We were interested specifically in those cases where
disambiguation needs to be made without relying on
syntactic frame, and the main source of disambiguation
is semantics of the arguments. Such cases are harder
to identify formally in the development of sense inven-
tories and harder for the annotators to determine. For
example, phrasal verbs or idiomatic constructions that
help identify a particular sense were intentionally ex-
cluded from our data set. Thus, for the verb cut, one of
the senses involves cutting out a shape or a form (e.g.
cut a suit), but the sentences with the corresponding
phrasal form cut out were thrown out.
Even so, syntactic clues that contribute to disam-
biguation in some cases overrule the interpretation sug-
gested by the argument. For example, for the verb deny,
in deny the attack, the direct object strongly suggests
a propositional interpretation for deny (that the attack
didn?t happen). However, the use of ditransitive con-
struction (indicated in the example below by the past
participle) overrules this interpretation, and we get the
refuse to grant sense:
(6) Astorre, denied his attack, had stayed in camp, uneasily
brooding.
35
In fact, during the actual annotation, one of the anno-
tators did not recognize the use of past participle, and
erroneously assigned the state or maintain something to
be untrue sense to this sentence.
3.1 Data set
The data set was developed using the British National
Corpus (BNC), which is more balanced than the more
commonly annotated Wall Street Journal data. We se-
lected 20 polysemous verbs with sense distinctions that
were judged to depend for disambiguation on seman-
tics of the argument in several argument positions, in-
cluding direct object (dobj), subject (subj), or indirect
object within a prepositional phrase governed by with
(iobj with):
dobj: absorb, acquire, admit, assume, claim, conclude,
cut, deny, dictate, drive, edit, enjoy, fire, grasp, know,
launch
subj: explain, fall, lead
iobj with: meet
We used the Sketch Engine (Kilgarriff et al, 2004)
both to select the verbs and to aid the creation of the
sense inventories. The Sketch Engine is a lexicographic
tool that lists collocates that co-occur with a given target
word in the specified grammatical relation. The collo-
cates are sorted by their association score with the tar-
get.
A set of senses was created for each verb using a
modification of the CPA technique (Pustejovsky et al,
2004). A set of complements was examined in the
Sketch Engine. If a clear division was observed be-
tween semantically different groups of collocates in a
certain argument position, the verb was selected. For
semantically distinct groups of collocates, a separate
sense was added to the sense inventory for the target.
For example, for the verb acquire, a separate sense was
added for each of the following sets of direct objects:
(7) a. Take on certain characteristics
shape, meaning, color, form, dimension, reality, sig-
nificance, identity, appearance, characteristic, flavor
b. Purchase or become the owner of property
land, stock, business, property, wealth, subsidiary, es-
tate, stake
The sense inventory for each verb was cross-checked
against several resources, including WordNet, Prop-
Bank, Merriam-Webster and Oxford English dictionar-
ies, and existing correspondences in FrameNet (Rup-
penhofer et al, 2006; Hiroaki, 2003), OntoNotes (Hovy
et al, 2006),3 and CPA patterns (Hanks and Puste-
jovsky, 2005; Rumshisky and Pustejovsky, 2006; Puste-
jovsky et al, 2004).
We performed test annotation on 100 instances, with
the sense inventory additionally modified upon exam-
ining the results of the annotation. This sense inven-
tory was provided to two annotators, along with 200
3Sense inventories released for the 65 verbs made avail-
able for SemEval-2007.
sentences for each verb. Each sentence was pre-parsed
with RASP (Briscoe and Carroll, 2002), and the head
of the target argument phrase was identified. Misparses
were manually corrected in post-processing.
3.2 Defining the task for the annotators
Data set creation for a WSD task is notoriously hard (cf.
Palmer et al (2007)), as the annotators are frequently
forced to perform disambiguation on sentences where
no disambiguation can really be performed. This is the
case, for example, for overlapping senses, where more
than one sense is activated simultaneously (Rumshisky,
2008; Pustejovsky and Boguraev, 1993). The goal was
to create, for each target word, a set of instances where
humans had no trouble disambiguating between differ-
ent senses.
Two undergraduate linguistics majors served as an-
notators. The annotators were instructed to mark each
sentence with the most fitting sense. The annotators
were allowed to mark the sentence as ?N/A? and were
instructed to do so if (i) the sense inventory was missing
the relevant sense, (ii) more than one sense seemed to
fit, or (iii) the sense was impossible to determine from
the context.
With respect to metaphoric senses, instructions were
to throw out cases of creative use where the interpreta-
tion was difficult or not immediately clear. The cases
where the target grammatical relation was actually ab-
sent from the sentence also had to be marked as ?N/A?
(e.g. for fire, sentences without direct object, e.g. a
stolen car was fired upon). The annotators were also
instructed to mark idiomatic expressions and phrasal
verbs as ?N/A?, e.g. for the verb fall: fall from favor,
fall through, fall in, fall back, fall silent, fall short, fall
in love.
Disagreements between the annotators were resolved
in adjudication by the co-authors. The average inter-
annotator agreement (ITA) for our data set was com-
puted as a macro-average of the percentage of instances
that were annotated with the same sense by both anno-
tators to the total number of instances retained in the
data set for each verb. The instances that were marked
as ?N/A? by one of the annotators (or thrown out during
the adjudication) were not included in the computation.
The ITA value for our data set was 95%. However, as
we will see below, the ITA values do not always reflect
the actual accuracy of annotation, due to some common
problems with sense inventories.
3.3 Glossing a sense
A very common problem with glossing a sense in-
volves the situation where a sense inventory includes
two senses one of which is an extension of the other.
The derived sense may be related to the primary sense
through metaphor, and this often results in the for-
mer taking on a semantically less specific interpreta-
tion. The problem with creating glosses in this situa-
tion is that the words used may have sense distinctions
36
parallel to the ones in the target verb being described.
This leaves the annotators free to choose either sense.
This seems to be the case, for example, with OntoNotes
sense inventory for fire, where ignite or become ignited
is the gloss under which very divergent examples are
grouped: oil fired the furnace (literal, primary sense)
and curiosity fired my imagination (metaphoric exten-
sion). Clearly, annotators were having a problem with
this sense due to the fact that the verb ignite has sense
distinctions which are based on the same metaphor (fire
= inspire) and therefore are very similar to those of the
verb fire.
In case of semantic underspecification, annotators
may be left free to choose the more generic sense,
which contaminates the data set while not being re-
flected in the inter-annotator agreement values. For ex-
ample, in our sense inventory for acquire, the gloss for
acquire a new customer has to be very generic. We
used the gloss ?become associated with something, of-
ten newly brought into being?. However, that led the
annotators to overuse this gloss and select this sense in
cases where a more specific gloss was more appropri-
ate:4
(8) By this treaty, Russia acquired a Black Sea coastline.
annoA: become associated with something, often newly
brought into being
annoB: become associated with something, ...
correct: purchase or become the owner of property
For a more detailed analysis of this phenomenon, see
Section 5.
4 Relations Between Senses
In this section, we discuss linguistic processes underly-
ing relations between senses within a single sense in-
ventory. We believe that a detailed analysis of these
processes should help to account for the annotator?s
ability to perform disambiguation. Some sense distinc-
tions appear more striking to the annotators, depending
on the type of relation involved.
In line with existing approaches to sense relations,
we will look at both the linguistic structures involved
in sense modification and the productive processes act-
ing on linguistic structures. For the purposes of our
present discussion, we interpret the literal (physical, di-
rect) senses to be primary, with respect to more abstract
or metaphorical senses.
4.1 Argument structure alternations
Some of the most striking differences between the
senses are related to the argument structure alternations:
1. Different case roles (frame elements) may be ex-
pressed in the same argument position (in this case, di-
rect object), corresponding to different perspectives on
the same event. For example, direct object position of
the verb drive may be filled by VEHICLE, DISTANCE,
4We will refer to annotators A and B as annoA and annoB.
or PHYSOBJ giving rise to three distinct senses: (i) op-
erate a vehicle controlling its motion, (ii) travel in a ve-
hicle a certain distance, and (iii) transport something or
someone. Similarly, for the verb fire, PROJECTILE or
WEAPON in direct object position give rise to two re-
lated senses: (i) shoot, discharge a weapon, (ii) shoot,
propel a projectile.
2. The distinction between propositional and non-
propositional complements, as for the verbs admit and
deny in (9) and (10):
(9) a. admit defeat, inconsistency, offense
(acknowledge the truth or reality of )
b. admit patients, students
(grant entry or allow into a community)
(10) a. deny reports, importance, allegations
(state or maintain to be untrue)
b. deny visa, access
(refuse to grant)
3. There is a mutual dependency between subcate-
gorization features of the complements in different ar-
gument positions. For example, the [+animate] subject
may combine with specific complements not available
for [?animate], as for the two senses of acquire: (i)
learn and (ii) take on certain characteristics. Compare
NP
subj
[-animate] acquire NP
dobj
(language, man-
ners, knowledge, skill) vs. NP
subj
[?animate] acquire
NP
dobj
(importance, significance). Similarly, for ab-
sorb, compare NP
subj
[?animate] absorb NP
dobj
(sub-
stance) and NP
subj
[+animate] absorb NP
dobj
(skill,
information). Note that, as one would expect, such de-
pendencies are inevitable even despite the fact that our
data set was developed specifically to target sense dis-
tinctions dependent on a single argument position.
4.2 Event structure modification
Event structure modifications (i.e. operations affecting
aspectual properties of the predicate) are another source
of sense differentiation. Two cases appear most promi-
nent:
1. The event structure is modified along with the
characteristics of the arguments. For example, for en-
joy, compare enjoy skiing, vacation (DYNAMIC EVENT)
with enjoying a status (STATE). Similarly, for lead,
compare a person leads smb somewhere (PROCESS) vs.
a road (PATH) leads somewhere (STATE); for explain,
compare something or somebody explains smth (= clar-
ifies, describes, makes comprehensible, PROCESS) vs.
something [?inanimate, +abstract] explains something
(= is a reason for something, STATE); for fall, compare
PHYSOBJ falls (TRANSITION or ACCOMPLISHMENT)
vs. a case falls into a certain category (STATE).
2. The aspectual nature of the predicate is the only
semantically relevant feature that remains unchanged
after consecutive sense modifications. For example, the
ingressive meaning of ?beginning something? is pre-
served in shifting from the physical sense of the verb
launch in launch a missile to launch a campaign and
launch a product.
37
4.3 Lexical semantic features
Sense distinctions often involve deeper semantic char-
acteristics of the verbs which could be accounted for by
means of lexical semantic features such as qualia struc-
ture roles in Generative Lexicon (Pustejovsky, 1995):5
1. Consider how the meaning component ?manner
of motion? (typically associated with the agentive role)
gets transformed in the different senses of drive. It is
obviously present in the physical uses of drive (such
as operate a vehicle, transport something or somebody,
etc.), but is completely lost in motivate the progress
of (as in drive the economy, drive the market forward,
etc.). The value of the agentive role of drive becomes
underspecified or semantically weak, so that the overall
meaning of drive is transformed to cause something to
move.
2. Information about semantic type contained in
qualia structure allows apparently diverse elements to
activate the same sense of the verb. For instance, the
verb absorb in the sense learn or incorporate skill or
information occurs with direct objects such as values,
atmosphere, information, idea, words, lesson, attitudes,
culture. The requisite semantic component is realized
differently for each of these words. Some of them are
complex types6 with INFORMATION as one of the con-
stituent types: words (ACOUSTIC/VISUAL ENTITY ?
INFO), lesson (EVENT ? INFO). Others, such as idea,
are polysemous, with one of the senses being INFOR-
MATION. Cases like culture and values are more diffi-
cult, but since they refer to knowledge, the INFORMA-
TION component is clearly present. Consequently, the
annotators are able to identify the corresponding sense
of absorb with a high degree of agreement.
4.4 Metaphor and metonymy
The processes causing the mentioned meaning trans-
formations in our corpus often involve metaphor and
metonymy. Below are some of the conventionalized ex-
tensions with metaphorical flavor:
(11) a. grasp object vs. grasp meaning
b. launch object vs. launch an event (campaign, as-
sault) or launch a product (newspaper, collection)
c. meet with a person vs. meet with success, resistance
d. lead somebody somewhere vs. lead to a consequence
Note that these metaphorical extensions involve ab-
stract or continuous objects (meaning, assault, success,
consequence), which in turn cause event structure mod-
ifications (lead as a process vs. lead as a state). Thus,
the processes and structures we are dealing with are
clearly interrelated.
The metonymical process can be exemplified by edit
as make changes to the text and as supervise publica-
5We will use the terminology from Generative Lexicon
(Pustejovsky, 1995; Pustejovsky, 2007) to discuss lexical se-
mantic properties, such as qualia roles, complex and func-
tional types, and so on.
6Complex type is a term used for concepts that inherently
refer to more than one semantic type.
tion, which are in a clear contiguity relationship.
One of the effects of the metaphorization and pro-
gressive emptying of the primary (physical, concrete)
senses is the distinction between generic and specific
senses. For example, compare acquire land, business
(specific sense) to acquire an infection, a boyfriend, a
following, which refers to some extremely light generic
association. Similar process is observed for the seman-
tically weak sense of fall, be associated with or get as-
signed to a person or location or for event to fall onto a
time:
(12) Birthdays, lunches, celebrations fall on a certain date or
time
Stress or emphasis fall on a given topic or a syllable
Responsibility, luck, suspicion fall on or to a person
The specificity often involves specialization within a
certain domain:
(13) a. conclude as finish vs. conclude as reach an agree-
ment (Law, Politics)
b. fire as shoot a weapon or a projectile vs. fire as kick
or pass an object of play in sports (Sport)
Thus, when concluding a pact or an agreement, a cer-
tain EVENT is also being finished (negotiation of that
agreement), necessarily with a positive outcome.
In the following section, we will try to show how dif-
ferent kinds of relations between senses influence dis-
ambiguation carried out by the annotators. In particular,
we look at different sources of disagreement and anno-
tator error as determined in adjudication.
5 Analysis of Annotation Decisions
As we have seen above, in many cases disambigua-
tion is impossible due to the nature of compositional-
ity. Also, as there are no clear answers to a number of
questions concerning sense identification, the annota-
tors deal with sense inventories that are imperfect. Re-
sults of the disambiguation task carried out by the an-
notators reflect all these defects.
In cases when a specific meaning from the data set
is not included into the sense inventory (e.g. due to its
low frequency or extreme fine-grainedness) the annota-
tors may use a more general meaning or pick the clos-
est meaning available. For example, within the sense
inventory for fire, there was no separate gloss for fire an
engine. Annotator A in our experiment chose the clos-
est specific meaning available, and Annotator B marked
it with a more generic sense:
(14) Engineers successfully fired thrusters to boost the re-
search satellite to an altitude of 507 km.
annoA: shoot, propel a projectile
annoB: apply fire to
As mentioned in Section 3.3, even when the appropriate
specific sense is available, annotators frequently chose
the more generic sense in its place, as in (15), (16) and
(17), and also in (8).
38
(15) Several referrals fell into this category.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: be categorized as or fall into a range
(16) The terrible silence had fallen.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: for a state (such as darkness or silence) to come,
to commence
(17) He acquired a taste for performing in public.
annoA: become associated with something, often
newly brought into being
annoB: become associated with something, ...
correct: learn
Note that in (8) this decision was probably motivated by
the annotators? uncertainty about the semantic ascrip-
tion of the relevant argument (coastline is not a proto-
typical owned property). The generic sense seems to be
the safest option to take for the annotators, as compared
to taking a chance with a specific meaning. Due to its
low degree of semantic specification, the generic sense
is potentially able to embrace almost every possible use.
This is not a desirable outcome because the generic
senses are introduced in the inventory to account only
for semantically underspecified cases. For instance, be-
come associated with something, often newly brought
into being is appropriate for acquire a grandchild, but
not for acquire a taste or acquire a proficiency.
Remarkable variation is also observed with respect to
non-literal uses as discussed in Section 4.4. For exam-
ple, in (18) and (19) abstract NPs panic and imbalance
of forces are equated with energy or impact by one an-
notator and with substance by the other.
(18) Her panic was absorbed by his warmth.
annoA: absorb energy or impact
annoB: absorb substance
(19) Alternatively, imbalance of forces can be absorbed into
the body.
annoA: absorb energy or impact
annoB: absorb substance
In some cases, the literal and the metaphoric senses
are activated simultaneously resulting in ambiguity (cf.
Cruse (2000)):
(20) For over 300 years this waterfall has provided the en-
ergy to drive the wheels of industry.
annoA: motivate the progress of
annoB: provide power for or physically move a mech-
anism
(21) But fashion changed and the short skirt fell ? literally ?
from favour and started skimming the ankles.
annoA: lose power or suffer a defeat
annoB: N/A
(22) She was delighted when the story of Hank fell into her
lap.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: physically drop; move or extend downward
Impact of subcategorization features on disam-
biguation (cf. Section 4.1 para 3) is illustrated in (23).
(23) The reggae tourist can easily absorb the current reggae
vibe.
annoA: absorb energy or impact
annoB: learn or incorporate skill or information
Both interpretations chosen here (absorb energy or im-
pact and learn or incorporate skill or information) were
possible due to the animacy of the subject, which acti-
vates two different subcategorization frames and subse-
quently two different senses.
Typically, cases where semantic type of the relevant
arguments (cf. Section 4.3 para 2) is not clear result in
annotator disagreement:
(24) The AAA launched education programs.
annoA: begin or initiate an endeavor (EVENT)
annoB: begin to produce or distribute; start a company
(PRODUCT)
(25) France plans to launch a remote-sensing vehicle called
Spot.
annoA: physically propel into the air, water or space
(PHYSOBJ)
annoB: begin to produce or distribute; start a company
(PRODUCT)
The two cases above are interesting in that both pro-
gram and vehicle are ambiguous and can be analyzed
semantically as members of different semantic classes.
This is what the annotators in fact do, and as a result,
ascribe them to different senses. Program can be cate-
gorized as EVENT (?series of steps?) or as INTELLEC-
TUAL ACTIVITY PRODUCT (?document or system of
projects?). It is a complex type, i.e. it is an inherently
polysemous word that represents at least two different
semantic types. Vehicle, in turn, is a functional type:
on the one hand, it represents an entity with certain for-
mal properties (PHYSOBJ interpretation), on the other
hand, it is an artifact, with a prominent practical pur-
pose (PRODUCT interpretation).
In fact, most problems the annotators had with the
task are due to the inherent semantic complexity of
words such as vehicle and program in (24) and (25) and
to the existence of boundary cases, where the relevant
noun does not properly belong to one or another seman-
tic category. This is the case with panic, imbalance or
reggae vibe in (18), (19), and (23), and also with taste
and coastline in (17) and (7).
In some of these cases, other contextual clues may
come into play and tip the balance in favor of one or an-
other sense. Note that disambiguation was influenced
by a wider context even despite the intentionally re-
strictive task design (targeting a particular syntactic re-
lation for each verb). For instance, in (26), domain-
specific clues referring to war or military conflict (such
as rebel control) could have motivated Annotator B?s
decision to ascribe it to the sense lose power or suffer
a defeat (even though a road is not typically an entity
that can lose power), while the other annotator chose a
more generic meaning:
39
(26) The road fell into rebel control.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: lose power or suffer a defeat
Other pragmatic and discourse-oriented clues played
a role, in particular, positive and negative connotation
of the senses and the relevant arguments, as well as
the temporal organization of discourse. For example, in
(27) and (28), positive or neutral interpretation of wave
of immigrants and change could have led to the choice
of take in or assimilate and learn or incorporate skill or
information senses, while the negatively-colored inter-
pretation might explain the choice of the bear the cost
of sense.
(27) ..help absorb the latest wave of immigrants.
annoA: bear the cost of; take on an expense
annoB: take in or assimilate, making part of a whole or
a group
(28) For senior management an important lesson was the
trade unions? capacity to absorb change and to become
its agents.
annoA: learn or incorporate skill or information
annoB: bear the cost of; take on an expense
Temporal organization of a broader discourse is an-
other important factor. For example, for the verb claim,
the senses claim the truth of and claim property you are
entitled to have different presuppositions with respect
to preexistence of the thing claimed. In (28), due to the
absence of a broader context, the annotators chose two
different temporal reference interpretations. For Anno-
tator B, success was something that has happened al-
ready, while for A this was not clear (success might
have been achieved or not):
(29) One area where the government can claim some success
involves debt repayment.
annoA: come in possession of or claim property you are
entitled to
annoB: claim the truth of
6 Conclusion
We have given a brief overview of different types of
sense relations commonly found in polysemous predi-
cates and analyzed their effect on different aspects of
the annotation task, including sense inventory design
and execution of the WSD annotation.
The present analysis suggests that theoretical tools
must be refined and further developed in order to give
an adequate account to the sense modifications found in
real corpus data. To this end, broader contextual clues
and discourse-oriented clues need to be included in the
analysis.
Semantically annotated corpora are routinely devel-
oped for the training and testing of automatic sense
detection and induction algorithms. But they do not
typically provide a way to distinguish between differ-
ent kinds of ambiguities. Consequently, it is difficult
to perform adequate error analysis for different sense
detection systems. Appropriate semantic annotation
that would allow one to determine which sense dis-
tinctions can be detected better by automatic systems
does not need to be highly specific and unnecessarily
complex, but requires development of robust general-
izations about sense relations.
One obvious conclusion is that data sets need to be
explicitly restricted to the instances where humans have
no trouble disambiguating between different senses.
Thus, prototypical cases can be accounted for reliably,
ensuring the clarity of annotated sense distinctions. At
face value, imposing such restrictions may appear to
negatively influence usability of the resulting data set
in particular applications requiring WSD, such as ma-
chine translation or information retrieval. However, this
decision impacts most strongly those boundary cases
which are not reliably disambiguated by human anno-
tators, and which rather introduce noise into the data
set.
Acknowledgments
This work was supported in part by NSF CRI grant to
Brandeis University. The work of O. Batiukova is sup-
ported by postdoctoral grant of the Ministry of Educa-
tion of Spain and Madrid Autonomous University.
References
Apresjan, Ju. 1973. Regular polysemy. Linguistics,
142(5):5?32.
Briscoe, T. and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), Las Palmas,
Canary Islands, May 2002, pages 1499?1504.
Carpuat, M. and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Proc. of EMNLP-CoNLL, pages 61?72.
Chan, Y. S., H. T. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical machine
translation. In Proc. of ACL, pages 33?40, Prague,
Czech Republic, June.
Cruse, D. A. 1995. Polysemy and related phenom-
ena from a cognitive linguistic viewpoint. In Dizier,
Patrick St. and Evelyne Viegas, editors, Computa-
tional Lexical Semantics, pages 33?49. Cambridge
University Press, Cambridge, England.
Cruse, D. A. 2000. Meaning in Language, an Intro-
duction to Semantics and Pragmatics. Oxford Uni-
versity Press, Oxford, United Kingdom.
Hanks, P. and J. Pustejovsky. 2005. A pattern
dictionary for natural language processing. Revue
Franc?aise de Linguistique Applique?e.
Hiroaki, S. 2003. FrameSQL: A software tool for
FrameNet. In Proceedigns of ASIALEX ?03, pages
251?258, Tokyo, Japan. Asian Association of Lexi-
cography.
40
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 57?60, New York City,
USA, June. Association for Computational Linguis-
tics.
Kilgarriff, A., P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105?116.
Kilgarriff, A. 1997. I don?t believe in word senses.
Computers and the Humanities, 31:91?113.
Landes, S., C. Leacock, and R.I. Tengi. 1998. Build-
ing semantic concordances. In Fellbaum, C., editor,
Wordnet: an electronic lexical database. MIT Press,
Cambridge (Mass.).
Mihalcea, R., T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Mi-
halcea, Rada and Phil Edmonds, editors, Senseval-3:
Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 25?
28, Barcelona, Spain, July. Association for Compu-
tational Linguistics.
Navigli, R. 2006. Meaningful clustering of senses
helps boost word sense disambiguation performance.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 105?112, Sydney, Australia, July. Association
for Computational Linguistics.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Palmer, M., H. Dang, and C. Fellbaum. 2007. Making
fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natu-
ral Language Engineering.
Preiss, J and D. Yarowsky, editors. 2001. Proceedings
of the Second Int. Workshop on Evaluating WSD Sys-
tems (Senseval 2). ACL2002/EACL2001.
Pustejovsky, J. and B. Boguraev. 1993. Lexical knowl-
edge representation and natural language processing.
Artif. Intell., 63(1-2):193?223.
Pustejovsky, J., P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924?931.
Pustejovsky, J. 1995. Generative Lexicon. Cambridge
(Mass.): MIT Press.
Pustejovsky, J. 2007. Type Theory and Lexical De-
composition. In Bouillon, P. and C. Lee, editors,
Trends in Generative Lexicon Theory. Kluwer Pub-
lishers (in press).
Resnik, P. 2006. Word sense disambiguation in NLP
applications. In Agirre, E. and P. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Rumshisky, A. and J. Pustejovsky. 2006. Induc-
ing sense-discriminating context patterns from sense-
tagged corpora. In LREC 2006, Genoa, Italy.
Rumshisky, A., P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
Rumshisky, A. 2008. Resolving polysemy in verbs:
Contextualized distributional approach to argument
semantics. Distributional Models of the Lexicon in
Linguistics and Cognitive Science, special issue of
Italian Journal of Linguistics / Rivista di Linguistica.
forthcoming.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Sinclair, J. and P. Hanks. 1987. The Collins Cobuild
English Language Dictionary. HarperCollins, 4th
edition (2003) edition. Published as Collins Cobuild
Advanced Learner?s English Dictionary.
Snyder, B. and M. Palmer. 2004. The english all-words
task. In Mihalcea, Rada and Phil Edmonds, edi-
tors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text, pages 41?43, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
41
Proceedings of the 8th International Conference on Computational Semantics, pages 169?180,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
GLML: Annotating Argument
Selection and Coercion
James Pustejovsky, Anna Rumshisky,
Jessica L. Moszkowicz, Olga Batiukova
Abstract
In this paper we introduce a methodology for annotating compo-
sitional operations in natural language text, and describe a mark-up
language, GLML, based on Generative Lexicon, for identifying such
relations. While most annotation systems capture surface relation-
ships, GLML captures the ?compositional history? of the argument
selection relative to the predicate. We provide a brief overview of GL
before moving on to our proposed methodology for annotating with
GLML. There are three main tasks described in the paper: (i) Com-
positional mechanisms of argument selection; (ii) Qualia in modifica-
tion constructions; (iii) Type selection in modification of dot objects.
We explain what each task includes and provide a description of the
annotation interface. We also include the XML format for GLML in-
cluding examples of annotated sentences.
1 Introduction
1.1 Motivation
In this paper, we introduce a methodology for annotating compositional
operations in natural language text. Most annotation schemes encoding
?propositional? or predicative content have focused on the identification
of the predicate type, the argument extent, and the semantic role (or label)
assigned to that argument by the predicate (see Palmer et al, 2005, Ruppen-
hofer et al, 2006, Kipper, 2005, Burchardt et al, 2006, Ohara, 2008, Subirats,
2004).
The emphasis here will be on identifying the nature of the composi-
tional operation rather than merely annotating the surface types of the en-
tities involved in argument selection.
169
Consider the well-known example below. The distinction in semantic
types appearing as subject in (1) is captured by entity typing, but not by any
sense tagging from, e.g., FrameNet (Ruppenhofer et al, 2006) or PropBank
(Palmer et al, 2005).
(1) a. Mary called yesterday.
b. The Boston office called yesterday.
While this has been treated as type coercion or metonymy in the literature (cf.
Hobbs et al, 1993 , Pustejovsky, 1991, Nunberg, 1979, Egg, 2005), the point
here is that an annotation using frames associated with verb senses should
treat the sentences on par with one another. Yet this is not possible if the
entity typing given to the subject in (1a) is HUMAN and that given for (1b)
is ORGANIZATION.
The SemEval Metonymy task (Markert and Nissim, 2007) was a good
attempt to annotate such metonymic relations over a larger data set. This
task involved two types with their metonymic variants:
(2) i. Categories for Locations: literal, place-for-people, place-for-event,
place-for-product;
ii. Categories for Organizations: literal, organization-for-members,
organization-for-event, organization-for-product, organization-for-fa-
cility.
One of the limitations with this approach, however, is that, while appropri-
ate for these specialized metonymy relations, the annotation specification
and resulting corpus are not an informative guide for extending the anno-
tation of argument selection more broadly.
In fact, the metonymy example in (1) is an instance of a much more
pervasive phenomenon of type shifting and coercion in argument selection.
For example, in (3) below, the sense annotation for the verb enjoy should
arguably assign similar values to both (3a) and (3b).
(3) a. Mary enjoyed drinking her beer .
b. Mary enjoyed her beer.
The consequence of this, however, is that, under current sense and role an-
notation strategies, the mapping to a syntactic realization for a given sense
is made more complex, and is, in fact, perplexing for a clustering or learn-
ing algorithm operating over subcategorization types for the verb.
170
1.2 Theoretical Preliminaries
The theoretical foundations for compositional operations within the sen-
tence have long been developed in considerable detail. Furthermore, type
shifting and type coercion operations have been recognized as playing an
important role in many formal descriptions of language, in order to main-
tain compositionality (cf. Partee and Rooth, 1983; Chierchia, 1998; Groe-
nendijk and Stokhof, 1989; Egg, 2005; Pinkal, 1999; Pustejovsky, 1995, and
many others). The goal of the present work is to: (a) create a broadly appli-
cable specification of the compositional operations involved in argument
selection; (b) apply this specification over a corpus of natural language
texts, in order to encode the selection mechanisms implicated in the com-
positional structure of the language.
The creation of a corpus that explicitly identifies the ?compositional his-
tory? associated with argument selection will be useful to computational
semantics in several respects: (a) the actual contexts within which type
coercions are allowed can be more correctly identified and perhaps gen-
eralized; (b) machine learning algorithms can take advantage of the map-
ping as an additional feature in the training phase; and (c) some consensus
might emerge on the general list of type-changing operations involved in
argument selection, as the tasks are revised and enriched.
For the purpose of this annotation task, we will adopt the general ap-
proach to argument selection within Generative Lexicon, as recently out-
lined in Pustejovsky (2006) and Asher and Pustejovsky (2006). We can dis-
tinguish the following modes of composition in natural language:
(4) a. PURE SELECTION (Type Matching): the type a function requires is
directly satisfied by the argument;
b. ACCOMMODATION: the type a function requires is inherited by the
argument;
c. TYPE COERCION: the type a function requires is imposed on the
argument type. This is accomplished by either:
i. Exploitation: taking a part of the argument?s type;
ii. Introduction: wrapping the argument with the required type.
Each of these will be identified as a unique relation between the predicate
and a given argument. In this annotation effort, we restrict the possible
relations between the predicate and a given argument to selection and coer-
cion. A more fine-grained typology of relations may be applied at a later
171
point. Furthermore, qualia structure values1 are identified in both argu-
ment selection and modification contexts.
The rest of this document proceeds as follows. In Section 2, we describe
our general methodology and architecture for GL annotation. Section 3
gives an overview of each of the annotation tasks as well as some details
on the resulting GLMLmarkup. Amore thorough treatment of thematerial
we present, including the complete GLML specification and updates on the
annotation effort can be found at www.glml.org.
2 General Methodology and Architecture
In this section, we describe the set of tasks for annotating compositional
mechanisms within the GL framework. The current GL markup will in-
clude the following tasks, each of which is described below in Section 3.
(5) a. Mechanisms of Argument Selection: Verb-based Annotation
b. Qualia in Modification Constructions
c. Type Selection in Modification of Dot Objects
2.1 System Architecture
Each GLML annotation task involves two phases: the data set construction
phase and the annotation phase. The first phase consists of (1) selecting the
target words to be annotated and compiling a sense inventory for each tar-
get, and (2) data extraction and preprocessing. The prepared data is then
loaded into the annotation interface. During the annotation phase, the an-
notation judgments are entered into the database, and the adjudicator re-
solves disagreements. The resulting database representation is used by the
exporting module to generate the corresponding XML markup, stand-off
annotation, or GL logical form.
These steps will differ slightly for each of the major GLML annotation
tasks. For example, Task 1 focuses on annotating compositional processes
between the verbs and their arguments. The first step for this task involves
(1) selecting the set of target verbs, (2) compiling a sense inventory for each
1The qualia structure, inspired by Moravcsik (1975)?s interpretation of the aitia of Aris-
totle, is defined as the modes of explanation of a word or phrase, and defined below (Puste-
jovsky, 1991): (a) FORMAL: the category distinguishing the meaning of a word within a
larger domain; (b) CONSTITUTIVE: the relation between an object and its constituent parts;
(c) TELIC: the purpose or function of the object, if there is one; (d) AGENTIVE: the factors
involved in the object?s origins or ?coming into being?.
172
target, and (3) associating a type template or a set of templates with each
sense. Since the objective of the task is to annotate coercion, our choices
must include the verbs that exhibit the coercive behavior at least in some of
their senses.
At the next step, the data containing the selected target words is ex-
tracted from a corpus and preprocessed. Since the GLML annotation is
intra-sentential, each extracted instance is a sentence. Sentences are parsed
to identify the relevant arguments, adjuncts or modifiers for each target.
The data is presented to the annotatator with the target word and the head-
word of the relevant phrase highlighted.
Due to the complexity of the GLML annotation, we chose to use the
task-based annotation architecture. The annotation environment is designed
so that the annotator can focus on one facet of the annotation at a time.
Thus, in Task 1, the verbs are disambiguated by the annotator in one sub-
task, and the annotation of the actual compositional relationship is done in
another subtask. Figure 1 shows an example of the interface for the verb-
based annotation task .
Figure 1: Example of Annotation Interface for GLML Annotation
173
2.2 The Type System for Annotation
The type system we have chosen for annotation is purposefully shallow,
but we also aimed to include types that would ease the complexity of the
annotation task. The type system is not structured in a hierarchy, but rather
it is presented as a set of types. For example, we include both HUMAN and
ANIMATE in the type system along with PHYSICAL OBJECT. While HUMAN
is a subtype of both ANIMATE and PHYSICAL OBJECT, the annotator does
not need to be concerned with this. This allows the annotator to simply
choose the HUMAN type when necessary rather than having to deal with
type inheritance.
While the set of types for GLML annotation can easily be modified, the
following list is currently being used:
(6) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT, ORGANIZATION, EVENT, PROPOSITION, IN-
FORMATION, SENSATION, LOCATION, TIME PERIOD, ABSTRACT ENTITY, ATTITUDE, EMOTION,
PROPERTY, OBLIGATION, AND RULE
3 Annotation Tasks
In this section, we describe the annotation process: the steps involved in
each task and the way they are presented to the annotators. In this paper,
we focus on the task descriptions rather than an in depth review of the
annotation interface and the resulting GLML markup.
The general methodology for each task is as follows: 1) Select a target
set of words and compile a sense inventory for each one, 2) Select a set of
sentences for each target, 3) Disambiguate the sense of the target in a given
sentence, and 4) Answer questions specific to the annotation task in order
to create the appropriate GLML link.
3.1 Mechanisms of Argument Selection: Verb-based Annotation
This annotation task involves choosing which selectional mechanism is
used by the predicate over a particular argument. The possible relations
between the predicate and a given argument will, for now, be restricted
to selection and coercion. In selection, the argument NP satisfies the typ-
ing requirements of the predicate, as in The child threw the stone (PHYS-
ICAL OBJECT). Coercion encompasses all cases when a type-shifting op-
eration (exploitation or introduction) must be performed on the comple-
ment NP in order to satisfy selectional requirements of the predicate, as in
The White House (LOCATION ? HUMAN) denied this statement.
174
An initial set of verbs and sentences containing them has been selected
for annotation. For each sentence, the compositional relationship of the
verb with every argument and adjunct will be annotated. The target types
for each argument are provided in a type template that is associated with
the sense of the verb in the given sentence. For example, one of the senses
of the verb deny (glossed as ?State or maintain that something is untrue?)
would have the following type template: HUMAN deny PROPOSITION.
In the first subtask, the annotator is presented with a set of sentences
containing the target verb and the chosen grammatical relation. The anno-
tator is asked to select the most fitting sense of the target verb, or to throw
out the example (pick the ?N/A? option) if no sense can be chosen either
due to insufficient context, because the appropriate sense does not appear
in the inventory, or simply no disambiguation can be made in good faith.
Next, the annotator is presented with a list of sentences in which the
target verb is used in the same sense and is asked to determine whether the
argument in the specified grammatical relation belongs to the type speci-
fied in the corresponding template. If the argument belongs to the appro-
priate type, the ?yes? box is clicked, generating a CompLink with comp-
Type=?SELECTION?. If ?no? is selected, a type selection menu pops up
below the first question, and the annotator is asked to pick a type from a
list of shallow types which is usually associated with the argument. Con-
sequently, a CompLink with compType=?COERCION? is created with the
corresponding source and target type.
The following example of GLMLmarkup is generated from the database2:
Sir Nicholas Lyell, Attorney General, denies a cover-up.
<SELECTOR sid="s1">denies</SELECTOR>
a <NOUN nid="n1">cover-up</NOUN> .
<CompLink cid="cid1" sID="s1" relatedToNoun="n1" gramRel="dobj"
compType="COERCION" sourceType="EVENT" targetType="PROPOSITION"/>
3.2 Qualia Selection in Modification Constructions
For this task, the relevant semantic relations are defined in terms of the
qualia structure. We examine two kinds of constructions in this task: adjec-
tival modification of nouns and nominal compounds3.
2While we present these examples as an inline annotation, a LAF (Ide and Romary, 2003)
compliant offset annotation is fully compatible with GLML.
3Since target nouns have already been selected for these two tasks, it is also possible
to annotate qualia selection in verb-noun contexts such as Can you shine the lamp over here?
(TELIC). However, here we focus solely on the modification contexts mentioned here.
175
3.2.1 Adjectival Modification of Nouns
This task involves annotating how particular noun qualia values are bound
by the adjectives. Following Pustejovsky (2000), we assume that the prop-
erties grammatically realized as adjectives ?bind into the qualia structure
of nouns, to select a narrow facet of the noun?s meaning.? For example, in
the NP ?a sharp metal hunting knife?, sharp refers to the knife as a physi-
cal object, its FORMAL type, metal is associated with a material part of the
knife (CONSTITUTIVE), and hunting is associatedwith how the knife is used
(TELIC). Similarly, forged in ?a forged knife? is associated with the creation
of the knife (AGENTIVE).
The task begins with sense disambiguation of the target nouns. Ques-
tions are then used to help the annotator identify which qualia relations are
selected. For example, the TELIC question for the noun table would be ?Is
this adjective associated with the inherent purpose of table?? These ques-
tions will change according to the type associated with the noun. Thus,
for natural types such as woman, the TELIC question would be ?Is this ad-
jective associated with a specific role of woman?? Similarly, for the AGEN-
TIVE role, the question corresponding to the PHYSICAL OBJECT-denoting
nouns refers to the ?making or destroying? the object, while for the EVENT-
denoting nouns, the same question involves ?beginning or ending? of the
event. QLinks are then created based on the annotator?s answers, as in the
following example:
The walls and the wooden table had all been lustily scrubbed.
<SELECTOR sid="s1">wooden</SELECTOR>
<NOUN nid="n1">table</NOUN>
<QLink qid="qid1" sID="s1" relatedToNoun="n1" qType="CONST"/>
3.2.2 Nominal Compounds
This task explores the semantic relationship between elements in nominal
compounds. The general relations presented in Levi (1978) are a useful
guide for beginning a classification of compound types, but the relations
between compound elements quickly prove to be too coarse-grained. War-
ren?s comprehensive work (Warren, 1978) is a valuable resource for differ-
entiating relation types between compound elements.
The class distinction in compound types in language can be broken
down into three forms (Spencer, 1991): endocentric compounds, exocen-
tric compounds, and dvandva compounds. Following Bisetto and Scalise
176
(2005), however, it is possible to distinguish three slightly differently con-
structed classes of compounds, each exhibiting endocentric and exocentric
behavior: subordinating, attributtive, and coordinate.
We will focus on the two classes of subordinating and attributive com-
pounds. Within each of these, we will distinguish between synthetic and
non-synthetic compounds. The former are deverbal nouns, and when act-
ing functionally (subordinating), take the sister noun as an argument, as
in bus driver and window cleaner. The non-synthetic counterparts of these
include pastry chef and bread knife, where the head is not deverbal in any
obvious way. While Bisetto and Scalise?s distinction is a useful one, it does
little to explain how non-relational sortal nouns such as chef and knife act
functionally over the accompanying noun in the compound, as above.
This construction has been examined within GL by Johnston and Busa
(1999). We will assume much of that analysis in our definition of the task
described here. Our basic assumption regarding the nature of the seman-
tic link between both parts of compounds is that it is generally similar to
the one present in adjectival modification. The only difference is that in
nominal compounds, for instance, the qualia of a head noun are activated
or exploited by a different kind of modifier, a noun. Given this similar-
ity, the annotation for this task is performed just as it is for the adjectival
modification task. A QLink is created as in the following example:
Our guest house stands some 100 yards away.
<SELECTOR sid="s1">guest</SELECTOR>
<NOUN nid="n1">house</NOUN>
<QLink qid="qid1" sID="s1" relatedToNoun="n1" qType="TELIC"/>
3.3 Type Selection in Modification of Dot Objects
This task involves annotating how particular types within dot objects are
exploited in adjectival and nominal modification constructions. Dot objects
or complex types (Pustejovsky, 1995) are defined as the product of a type
constructor ? (?dot?), which creates dot objects from any two types a and
b , creating a ? b. Complex types are unique because they are made up of
seemingly incompatible types such as FOOD and EVENT.
Given a complex type c = a ? b, there are three possible options: 1) the
modifier applies to both a and b, 2) the modifier applies to a only, or 3) the
modifier applies to b only. Option 1 would be illustrated by examples such
as good book [+info, +physobj] and long test [+info, +event]. Examples such as
177
delicious lunch [+food, -event] and long lunch [-food, +event] illustrate options
2 and 3. A listing of dot objects can be found in Pustejovsky (2005).
The sense inventory for the collection of dot objects chosen for this task
will include only homonyms. That is, only contrastive senses such as the
river bank versus financial institution for bank will need to be disambiguated.
Complementary senses such as the financial institution itself versus the
building where it is located are not included.
In order to create the appropriate CompLink, the annotator will select
which type from a list of component types for a given dot object is exploited
in the sentence. The resulting GLML is:
After a while more champagne and a delicious lunch was served.
<SELECTOR sid="s1">delicious</SELECTOR>
<NOUN nid="n1">lunch</NOUN>
<CompLink cid="cid1" sID="s1" relatedToNoun="n1" gramRel="mod"
compType="SELECTION" sourceType="[PHYS_OBJ,EVENT]"
targetType="PHYS_OBJ" />
4 Conclusion
In this paper, we approach the problem of annotating the relation between
a predicate and its argument as one that encodes the compositional history
of the selection process. This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of a predicate from those
that are accommodated or coerced in context. We described a specification
language for selection, GLML, based largely on the type selective opera-
tions in GL, and three annotation tasks using this specification to identify
argument selection behavior.
There are clearly many compositional operations in language that have
not been addressed in this paper. The framework is general enough, how-
ever, to describe a broad range of type selective behavior. As the tasks be-
come more refined, the extensions will also become clearer. Furthermore,
as other languages are examined for annotation, new tasks will emerge re-
flecting perhaps language-specific constructions.
Acknowledgements
The idea for annotating a corpus according to principles of argument selec-
tion within GL arose during a discussion at GL2007 in Paris, between one
178
of the authors (J. Pustejovsky) and Nicoletta Calzolari and Pierrette Bouil-
lon. Recently, the authors met with other members of the GLML Working
Group in Pisa at the ILC (September 23-25, 2008). We would like to thank
the members of that meeting for their fruitful feedback and discussion on
an earlier version of this document. In particular, we would like to thank
Nicoletta Calzolari, Elisabetta Jezek, Alessandro Lenci, Valeria Quochi, Jan
Odijk, Tommaso Caselli, Claudia Soria, Chu-Ren Huang, Marc Verhagen,
and Kiyong Lee.
References
N. Asher and J. Pustejovsky. 2006. A type composition logic for generative
lexicon. Journal of Cognitive Science, 6:1?38.
A. Bisetto and S. Scalise. 2005. The classification of compounds. Lingue e
Linguaggio, 2:319?332.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian
Pado, and Manfred Pinkal. 2006. The salsa corpus: a german corpus
resource for lexical semantics. In Proceedings of LREC, Genoa, Italy.
Gennaro Chierchia. 1998. Reference to kinds across language. Natural Lan-
guage Semantics, 6(4).
Marcus Egg. 2005. Flexible semantics for reinterpretation phenomena. CSLI,
Stanford.
Jeroen Groenendijk and Martin Stokhof, 1989. Type-shifting rules and the
semantics of interrogatives, volume 2, pages 21?68. Kluwer, Dordrecht.
Jerry R. Hobbs, Mark Stickel, and Paul Martin. 1993. Interpretation as ab-
duction. Artificial Intelligence, 63:69?142.
Nancy Ide and L. Romary. 2003. Outline of the international standard lin-
guistic annotation framework. In Proceedings of ACL?03Workshop on Lin-
guistic Annotation: Getting the Model Right.
M. Johnston and F. Busa. 1999. The compositional interpretation of com-
pounds. In E. Viegas, editor, Breadth and Depth of Semantics Lexicons,
pages 167?167. Dordrecht: Kluwer Academic.
Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon.
Phd dissertation, University of Pennsylvania, PA.
J. N. Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic
Press, New York.
K. Markert and M. Nissim. 2007. Metonymy resolution at semeval i:
Guidelines for participants. In Proceedings of the ACL 2007 Conference.
179
J. M. Moravcsik. 1975. Aitia as generative factor in aristotle?s philosophy.
Dialogue, 14:622?636.
Geoffrey Nunberg. 1979. The non-uniqueness of semantic solutions: Poly-
semy. Linguistics and Philosophy, 3:143?184.
Kyoko Hirose Ohara. 2008. Lexicon, grammar, and multilinguality in the
japanese framenet. In Proceedings of LREC, Marrakech, Marocco.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles. Computational Linguistics, 31(1):71?
106.
Barbara Partee and Mats Rooth, 1983. Generalized conjunction and type ambi-
guity, pages 361?383. de Gruyter, Berlin.
Manfred Pinkal. 1999. On semantic underspecification. In Harry Bunt and
Reinhard Muskens, editors, Proceedings of the 2nd International Workshop
on Computational Semantics (IWCS 2), January 13-15, Tilburg University,
The Netherlands.
J. Pustejovsky. 1991. The generative lexicon. Computational Linguistics,
17(4).
J. Pustejovsky. 1995. Generative Lexicon. Cambridge (Mass.): MIT Press.
J. Pustejovsky. 2000. Events and the semantics of opposition. In C. Tenny
and J. Pustejovsky, editors, Events as Grammatical Objects, pages 445?
482. Center for the Study of Language and Information (CSLI), Stan-
ford, CA.
J. Pustejovsky. 2005. A survey of dot objects. Technical report, Brandeis
University.
J. Pustejovsky. 2006. Type theory and lexical decomposition. Journal of Cog-
nitive Science, 6:39?76.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk.
2006. FrameNet II: Extended Theory and Practice.
A. Spencer. 1991. Morphological Theory: An Introduction to Word Structure
in Generative Grammar. Blackwell Textbooks in Linguistics, Oxford, UK
and Cambridge, USA.
Carlos Subirats. 2004. FrameNet Espan?ol. Una red sema?ntica de mar-
cos conceptuales. In VI International Congress of Hispanic Linguistics,
Leipzig.
B. Warren. 1978. Semantic Patterns of Noun-Noun Compounds. Acta Univer-
sitatis Gothoburgensis, Go?teborg.
180
