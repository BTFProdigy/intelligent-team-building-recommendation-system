Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894?904,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Role of Syntax in Vector Space Models of Compositional Semantics
Karl Moritz Hermann and Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
Abstract
Modelling the compositional process by
which the meaning of an utterance arises
from the meaning of its parts is a funda-
mental task of Natural Language Process-
ing. In this paper we draw upon recent
advances in the learning of vector space
representations of sentential semantics and
the transparent interface between syntax
and semantics provided by Combinatory
Categorial Grammar to introduce Com-
binatory Categorial Autoencoders. This
model leverages the CCG combinatory op-
erators to guide a non-linear transforma-
tion of meaning within a sentence. We use
this model to learn high dimensional em-
beddings for sentences and evaluate them
in a range of tasks, demonstrating that
the incorporation of syntax allows a con-
cise model to learn representations that are
both effective and general.
1 Introduction
Since Frege stated his ?Principle of Semantic
Compositionality? in 1892 researchers have pon-
dered both how the meaning of a complex expres-
sion is determined by the meanings of its parts,
and how those parts are combined. (Frege, 1892;
Pelletier, 1994). Over a hundred years on the
choice of representational unit for this process
of compositional semantics, and how these units
combine, remain open questions.
Frege?s principle may be debatable from a lin-
guistic and philosophical standpoint, but it has
provided a basis for a range of formal approaches
to semantics which attempt to capture meaning in
logical models. The Montague grammar (Mon-
tague, 1970) is a prime example for this, build-
ing a model of composition based on lambda-
calculus and formal logic. More recent work
in this field includes the Combinatory Categorial
Grammar (CCG), which also places increased em-
phasis on syntactic coverage (Szabolcsi, 1989).
Recently those searching for the right represen-
tation for compositional semantics have drawn in-
spiration from the success of distributional mod-
els of lexical semantics. This approach represents
single words as distributional vectors, implying
that a word?s meaning is a function of the envi-
ronment it appears in, be that its syntactic role or
co-occurrences with other words (Pereira et al,
1993; Schu?tze, 1998). While distributional se-
mantics is easily applied to single words, spar-
sity implies that attempts to directly extract distri-
butional representations for larger expressions are
doomed to fail. Only in the past few years has
it been attempted to extend these representations
to semantic composition. Most approaches here
use the idea of vector-matrix composition to learn
larger representations from single-word encodings
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al, 2012b). While
these models have proved very promising for com-
positional semantics, they make minimal use of
linguistic information beyond the word level.
In this paper we bridge the gap between recent
advances in machine learning and more traditional
approaches within computational linguistics. We
achieve this goal by employing the CCG formal-
ism to consider compositional structures at any
point in a parse tree. CCG is attractive both for its
transparent interface between syntax and seman-
tics, and a small but powerful set of combinatory
operators with which we can parametrise our non-
linear transformations of compositional meaning.
We present a novel class of recursive mod-
els, the Combinatory Categorial Autoencoders
(CCAE), which marry a semantic process pro-
vided by a recursive autoencoder with the syn-
tactic representations of the CCG formalism.
Through this model we seek to answer two ques-
894
tions: Can recursive vector space models be recon-
ciled with a more formal notion of compositional-
ity; and is there a role for syntax in guiding seman-
tics in these types of models? CCAEs make use of
CCG combinators and types by conditioning each
composition function on its equivalent step in a
CCG proof. In terms of learning complexity and
space requirements, our models strike a balance
between simpler greedy approaches (Socher et
al., 2011b) and the larger recursive vector-matrix
models (Socher et al, 2012b).
We show that this combination of state of the art
machine learning and an advanced linguistic for-
malism translates into concise models with com-
petitive performance on a variety of tasks. In both
sentiment and compound similarity experiments
we show that our CCAE models match or better
comparable recursive autoencoder models.1
2 Background
There exist a number of formal approaches to lan-
guage that provide mechanisms for composition-
ality. Generative Grammars (Jackendoff, 1972)
treat semantics, and thus compositionality, essen-
tially as an extension of syntax, with the generative
(syntactic) process yielding a structure that can be
interpreted semantically. By contrast Montague
grammar achieves greater separation between the
semantic and the syntactic by using lambda calcu-
lus to express meaning. However, this greater sep-
aration between surface form and meaning comes
at a price in the form of reduced computability.
While this is beyond the scope of this paper, see
e.g. Kracht (2008) for a detailed analysis of com-
positionality in these formalisms.
2.1 Combinatory Categorial Grammar
In this paper we focus on CCG, a linguistically
expressive yet computationally efficient grammar
formalism. It uses a constituency-based structure
with complex syntactic types (categories) from
which sentences can be deduced using a small
number of combinators. CCG relies on combi-
natory logic (as opposed to lambda calculus) to
build its expressions. For a detailed introduction
and analysis vis-a`-vis other grammar formalisms
see e.g. Steedman and Baldridge (2011).
CCG has been described as having a transpar-
ent surface between the syntactic and the seman-
1A C++ implementation of our models is available at
http://www.karlmoritz.com/
Tina likes tigers
N (S[dcl]\NP)/NP N
NP NP
>
S[dcl]\NP
<
S[dcl]
Figure 1: CCG derivation for Tina likes tigers with
forward (>) and backward application (<).
tic. It is this property which makes it attractive
for our purposes of providing a conditioning struc-
ture for semantic operators. A second benefit of
the formalism is that it is designed with computa-
tional efficiency in mind. While one could debate
the relative merits of various linguistic formalisms
the existence of mature tools and resources, such
as the CCGBank (Hockenmaier and Steedman,
2007), the Groningen Meaning Bank (Basile et al,
2012) and the C&C Tools (Curran et al, 2007) is
another big advantage for CCG.
CCG?s transparent surface stems from its cate-
gorial property: Each point in a derivation corre-
sponds directly to an interpretable category. These
categories (or types) associated with each term in a
CCG govern how this term can be combined with
other terms in a larger structure, implicitly making
them semantically expressive.
For instance in Figure 1, the word likes has type
(S[dcl]\NP)/NP, which means that it first looks
for a type NP to its right hand side. Subsequently
the expression likes tigers (as type S[dcl]\NP) re-
quires a second NP on its left. The final type of
the phrase S[dcl] indicates a sentence and hence a
complete CCG proof. Thus at each point in a CCG
parse we can deduce the possible next steps in the
derivation by considering the available types and
combinatory rules.
2.2 Vector Space Models of Semantics
Vector-based approaches for semantic tasks have
become increasingly popular in recent years.
Distributional representations encode an ex-
pression by its environment, assuming the context-
dependent nature of meaning according to which
one ?shall know a word by the company it keeps?
(Firth, 1957). Effectively this is usually achieved
by considering the co-occurrence with other words
in large corpora or the syntactic roles a word per-
forms.
Distributional representations are frequently
used to encode single words as vectors. Such rep-
895
resentations have then successfully been applied
to a number of tasks including word sense disam-
biguation (Schu?tze, 1998) and selectional prefer-
ence (Pereira et al, 1993; Lin, 1999).
While it is theoretically possible to apply the
same mechanism to larger expressions, sparsity
prevents learning meaningful distributional repre-
sentations for expressions much larger than single
words.2
Vector space models of compositional seman-
tics aim to fill this gap by providing a methodol-
ogy for deriving the representation of an expres-
sion from those of its parts. While distributional
representations frequently serve to encode single
words in such approaches this is not a strict re-
quirement.
There are a number of ideas on how to de-
fine composition in such vector spaces. A gen-
eral framework for semantic vector composition
was proposed in Mitchell and Lapata (2008), with
Mitchell and Lapata (2010) and more recently Bla-
coe and Lapata (2012) providing good overviews
of this topic. Notable approaches to this issue in-
clude Baroni and Zamparelli (2010), who com-
pose nouns and adjectives by representing them as
vectors and matrices, respectively, with the com-
positional representation achieved by multiplica-
tion. Grefenstette and Sadrzadeh (2011) use a sim-
ilar approach with matrices for relational words
and vectors for arguments. These two approaches
are combined in Grefenstette et al (2013), produc-
ing a tensor-based semantic framework with ten-
sor contraction as composition operation.
Another set of models that have very success-
fully been applied in this area are recursive autoen-
coders (Socher et al, 2011a; Socher et al, 2011b),
which are discussed in the next section.
2.3 Recursive Autoencoders
Autoencoders are a useful tool to compress in-
formation. One can think of an autoencoder
as a funnel through which information has to
pass (see Figure 2). By forcing the autoencoder
to reconstruct an input given only the reduced
amount of information available inside the funnel
it serves as a compression tool, representing high-
dimensional objects in a lower-dimensional space.
Typically a given autoencoder, that is the func-
tions for encoding and reconstructing data, are
2The experimental setup in (Baroni and Zamparelli, 2010)
is one of the few examples where distributional representa-
tions are used for word pairs.
Figure 2: A simple three-layer autoencoder. The
input represented by the vector at the bottom is
being encoded in a smaller vector (middle), from
which it is then reconstructed (top) into the same
dimensionality as the original input vector.
used on multiple inputs. By optimizing the two
functions to minimize the difference between all
inputs and their respective reconstructions, this au-
toencoder will effectively discover some hidden
structures within the data that can be exploited to
represent it more efficiently.
As a simple example, assume input vectors
xi ? Rn, i ? (0..N), weight matrices W enc ?
R(m?n),W rec ? R(n?m) and biases benc ? Rm,
brec ? Rn. The encoding matrix and bias are used
to create an encoding ei from xi:
ei = fenc(xi) =W encxi + benc (1)
Subsequently e ? Rm is used to reconstruct x as
x? using the reconstruction matrix and bias:
x?i = f rec(ei) =W recei + brec (2)
? = (W enc,W rec, benc, brec) can then be learned
by minimizing the error function describing the
difference between x? and x:
E = 12
N?
i
??x?i ? xi
??2 (3)
Now, if m < n, this will intuitively lead to ei
encoding a latent structure contained in xi and
shared across all xj , j ? (0..N), with ? encoding
and decoding to and from that hidden structure.
It is possible to apply multiple autoencoders on
top of each other, creating a deep autoencoder
(Bengio et al, 2007; Hinton and Salakhutdinov,
2006). For such a multi-layered model to learn
anything beyond what a single layer could learn, a
non-linear transformation g needs to be applied at
each layer. Usually, a variant of the sigmoid (?)
896
Figure 3: RAE with three inputs. Vectors with
filled (blue) circles represent input and hidden
units; blanks (white) denote reconstruction layers.
or hyperbolic tangent (tanh) function is used for
g (LeCun et al, 1998).
fenc(xi) = g (W encxi + benc) (4)
f rec(ei) = g (W recei + brec)
Furthermore, autoencoders can easily be used as
a composition function by concatenating two input
vectors, such that:
e = f(x1, x2) = g (W (x1?x2) + b) (5)
(x?1?x?2) = g
(
W ?e+ b?
)
Extending this idea, recursive autoencoders (RAE)
allow the modelling of data of variable size. By
setting the n = 2m, it is possible to recursively
combine a structure into an autoencoder tree. See
Figure 3 for an example, where x1, x2, x3 are re-
cursively encoded into y2.
The recursive application of autoencoders was
first introduced in Pollack (1990), whose recursive
auto-associative memories learn vector represen-
tations over pre-specified recursive data structures.
More recently this idea was extended and applied
to dynamic structures (Socher et al, 2011b).
These types of models have become increas-
ingly prominent since developments within the
field of Deep Learning have made the training
of such hierarchical structures more effective and
tractable (LeCun et al, 1998; Hinton et al, 2006).
Intuitively the top layer of an RAE will encode
aspects of the information stored in all of the input
vectors. Previously, RAE have successfully been
applied to a number of tasks including sentiment
analysis, paraphrase detection, relation extraction
Model CCG Elements
CCAE-A parse
CCAE-B parse + rules
CCAE-C parse + rules + types
CCAE-D parse + rules + child types
Table 1: Aspects of the CCG formalism used by
the different models explored in this paper.
and 3D object identification (Blacoe and Lapata,
2012; Socher et al, 2011b; Socher et al, 2012a).
3 Model
The models in this paper combine the power of
recursive, vector-based models with the linguistic
intuition of the CCG formalism. Their purpose is
to learn semantically meaningful vector represen-
tations for sentences and phrases of variable size,
while the purpose of this paper is to investigate
the use of syntax and linguistic formalisms in such
vector-based compositional models.
We assume a CCG parse to be given. Let C de-
note the set of combinatory rules, and T the set
of categories used, respectively. We use the parse
tree to structure an RAE, so that each combina-
tory step is represented by an autoencoder func-
tion. We refer to these models Categorial Com-
binatory Autoencoders (CCAE). In total this pa-
per describes four models making increasing use
of the CCG formalism (see table 1).
As an internal baseline we use model CCAE-
A, which is an RAE structured along a CCG parse
tree. CCAE-A uses a single weight matrix each for
the encoding and reconstruction step (see Table 2.
This model is similar to Socher et al (2011b), ex-
cept that we use a fixed structure in place of the
greedy tree building approach. As CCAE-A uses
only minimal syntactic guidance, this should al-
low us to better ascertain to what degree the use of
syntax helps our semantic models.
Our second model (CCAE-B) uses the compo-
sition function in equation (6), with c ? C.
fenc(x, y, c) = g (W cenc(x?y) + bcenc) (6)
f rec(e, c) = g (W crece+ bcrec)
This means that for every combinatory rule we de-
fine an equivalent autoencoder composition func-
tion by parametrizing both the weight matrix and
bias on the combinatory rule (e.g. Figure 4).
In this model, as in the following ones, we as-
sume a reconstruction step symmetric with the
897
Model Encoding Function
CCAE-A f(x, y)= g (W (x?y) + b)
CCAE-B f(x, y, c)= g (W c(x?y) + bc)
CCAE-C f(x, y, c, t)= g
(?
p?{c,t} (W p(x?y) + bp)
)
CCAE-D f(x, y, c, tx, ty)= g (W c (W txx+W tyy)+ bc)
Table 2: Encoding functions of the four CCAE models discussed in this paper.
? : X/Y ? : Y
>?? : X
g (W>enc(???) + b>enc)
Figure 4: Forward application as CCG combinator
and autoencoder rule respectively.
Figure 5: CCAE-B applied to Tina likes tigers.
Next to each vector are the CCG category (top)
and the word or function representing it (bottom).
lex describes the unary type-changing operation.
> and < are forward and backward application.
composition step. For the remainder of this paper
we will focus on the composition step and drop the
use of enc and rec in variable names where it isn?t
explicitly required. Figure 5 shows model CCAE-
B applied to our previous example sentence.
While CCAE-B uses only the combinatory
rules, we want to make fuller use of the linguis-
tic information available in CCG. For this pur-
pose, we build another model CCAE-C, which
parametrizes on both the combinatory rule c ? C
and the CCG category t ? T at every step (see
Figure 2). This model provides an additional de-
gree of insight, as the categories T are semanti-
cally and syntactically more expressive than the
CCG combinatory rules by themselves. Summing
over weights parametrised on c and t respectively,
adds an additional degree of freedom and also al-
lows for some model smoothing.
An alternative approach is encoded in model
CCAE-D. Here we consider the categories not of
the element represented, but of the elements it is
generated from together with the combinatory rule
applied to them. The intuition is that in the first
step we transform two expressions based on their
syntax. Subsequently we combine these two con-
ditioned on their joint combinatory rule.
4 Learning
In this section we briefly discuss unsupervised
learning for our models. Subsequently we de-
scribe how these models can be extended to allow
for semi-supervised training and evaluation.
Let ? = (W,B, L) be our model parameters
and ? a vector with regularization parameters for
all model parameters. W represents the set of all
weight matrices, B the set of all biases and L the
set of all word vectors. LetN be the set of training
data consisting of tree-nodes n with inputs xn, yn
and reconstruction rn. The error given n is:
E(n|?) = 12
???rn ? (xn?yn)
???
2 (7)
The gradient of the regularised objective func-
tion then becomes:
?J
?? =
1
N
N?
n
?E(n|?)
?? + ?? (8)
We learn the gradient using backpropagation
through structure (Goller and Ku?chler, 1996), and
minimize the objective function using L-BFGS.
For more details about the partial derivatives
used for backpropagation, see the documentation
accompanying our model implementation.3
3http://www.karlmoritz.com/
898
4.1 Supervised Learning
The unsupervised method described so far learns
a vector representation for each sentence. Such a
representation can be useful for some tasks such as
paraphrase detection, but is not sufficient for other
tasks such as sentiment classification, which we
are considering in this paper.
In order to extract sentiment from our models,
we extend them by adding a supervised classifier
on top, using the learned representations v as input
for a binary classification model:
pred(l=1|v, ?) = sigmoid(Wlabel v + blabel) (9)
Given our corpus of CCG parses with label pairs
(N, l), the new objective function becomes:
J = 1N
?
(N,l)
E(N, l, ?) + ?2 ||?||
2 (10)
Assuming each node n ? N contains children
xn, yn, encoding en and reconstruction rn, so that
n = {x, y, e, r} this breaks down into:
E(N, l, ?) = (11)
?
n?N
?Erec (n, ?) + (1??)Elbl(en, l, ?)
Erec(n, ?) =
1
2
???[xn?yn]? rn
???
2 (12)
Elbl(e, l, ?) =
1
2 ?l ? e?
2 (13)
This method of introducing a supervised aspect
to the autoencoder largely follows the model de-
scribed in Socher et al (2011b).
5 Experiments
We describe a number of standard evaluations
to determine the comparative performance of our
model. The first task of sentiment analysis allows
us to compare our CCG-conditioned RAE with
similar, existing models. In a second experiment,
we apply our model to a compound similarity eval-
uation, which allows us to evaluate our models
against a larger class of vector-based models (Bla-
coe and Lapata, 2012). We conclude with some
qualitative analysis to get a better idea of whether
the combination of CCG and RAE can learn se-
mantically expressive embeddings.
In our experiments we use the hyperbolic tan-
gent as nonlinearity g. Unless stated otherwise we
use word-vectors of size 50, initialized using the
embeddings provided by Turian et al (2010) based
on the model of Collobert and Weston (2008).4
We use the C&C parser (Clark and Curran,
2007) to generate CCG parse trees for the data
used in our experiments. For models CCAE-C and
CCAE-D we use the 25 most frequent CCG cate-
gories (as extracted from the British National Cor-
pus) with an additional general weight matrix in
order to catch all remaining types.
5.1 Sentiment Analysis
We evaluate our model on the MPQA opinion
corpus (Wiebe et al, 2005), which annotates ex-
pressions for sentiment.5 The corpus consists of
10,624 instances with approximately 70 percent
describing a negative sentiment. We apply the
same pre-processing as (Nakagawa et al, 2010)
and (Socher et al, 2011b) by using an additional
sentiment lexicon (Wilson et al, 2005) during the
model training for this experiment.
As a second corpus we make use of the sentence
polarity (SP) dataset v1.0 (Pang and Lee, 2005).6
This dataset consists of 10662 sentences extracted
from movie reviews which are manually labelled
with positive or negative sentiment and equally
distributed across sentiment.
Experiment 1: Semi-Supervised Training In
the first experiment, we use the semi-supervised
training strategy described previously and initial-
ize our models with the embeddings provided by
Turian et al (2010). The results of this evalua-
tion are in Table 3. While we achieve the best per-
formance on the MPQA corpus, the results on the
SP corpus are less convincing. Perhaps surpris-
ingly, the simplest model CCAE-A outperforms
the other models on this dataset.
When considering the two datasets, sparsity
seems a likely explanation for this difference in
results: In the MPQA experiment most instances
are very short with an average length of 3 words,
while the average sentence length in the SP corpus
is 21 words. The MPQA task is further simplified
through the use or an additional sentiment lexicon.
Considering dictionary size, the SP corpus has a
dictionary of 22k words, more than three times the
size of the MPQA dictionary.
4http://www.metaoptimize.com/projects/
wordreprs/
5http://mpqa.cs.pitt.edu/
6http://www.cs.cornell.edu/people/
pabo/movie-review-data/
899
Method MPQA SP
Voting with two lexica 81.7 63.1
MV-RNN (Socher et al, 2012b) - 79.0
RAE (rand) (Socher et al, 2011b) 85.7 76.8
TCRF (Nakagawa et al, 2010) 86.1 77.3
RAE (init) (Socher et al, 2011b) 86.4 77.7
NB (Wang and Manning, 2012) 86.7 79.4
CCAE-A 86.3 77.8
CCAE-B 87.1 77.1
CCAE-C 87.1 77.3
CCAE-D 87.2 76.7
Table 3: Accuracy of sentiment classification on
the sentiment polarity (SP) and MPQA datasets.
For NB we only display the best result among a
larger group of models analysed in that paper.
This issue of sparsity is exacerbated in the more
complex CCAE models, where the training points
are spread across different CCG types and rules.
While the initialization of the word vectors with
previously learned embeddings (as was previously
shown by Socher et al (2011b)) helps the mod-
els, all other model variables such as composition
weights and biases are still initialised randomly
and thus highly dependent on the amount of train-
ing data available.
Experiment 2: Pretraining Due to our analy-
sis of the results of the initial experiment, we ran a
second series of experiments on the SP corpus. We
follow (Scheible and Schu?tze, 2013) for this sec-
ond series of experiments, which are carried out on
a random 90/10 training-testing split, with some
data reserved for development.
Instead of initialising the model with external
word embeddings, we first train it on a large
amount of data with the aim of overcoming the
sparsity issues encountered in the previous exper-
iment. Learning is thus divided into two steps:
The first, unsupervised training phase, uses the
British National Corpus together with the SP cor-
pus. In this phase only the reconstruction signal
is used to learn word embeddings and transforma-
tion matrices. Subsequently, in the second phase,
only the SP corpus is used, this time with both the
reconstruction and the label error.
By learning word embeddings and composition
matrices on more data, the model is likely to gen-
eralise better. Particularly for the more complex
models, where the composition functions are con-
ditioned on various CCG parameters, this should
Training
Model Regular Pretraining
CCAE-A 77.8 79.5
CCAE-B 76.9 79.8
CCAE-C 77.1 81.0
CCAE-D 76.9 79.7
Table 4: Effect of pretraining on model perfor-
mance on the SP dataset. Results are reported on a
random subsection of the SP corpus; thus numbers
for the regular training method differ slightly from
those in Table 3.
help to overcome issues of sparsity.
If we consider the results of the pre-trained ex-
periments in Table 4, this seems to be the case.
In fact, the trend of the previous results has been
reversed, with the more complex models now per-
forming best, whereas in the previous experiments
the simpler models performed better. Using the
Turian embeddings instead of random initialisa-
tion did not improve results in this setup.
5.2 Compound Similarity
In a second experiment we use the dataset from
Mitchell and Lapata (2010) which contains sim-
ilarity judgements for adjective-noun, noun-noun
and verb-object pairs.7 All compound pairs have
been ranked for semantic similarity by a number of
human annotators. The task is thus to rank these
pairs of word pairs by their semantic similarity.
For instance, the two compounds vast amount
and large quantity are given a high similarity score
by the human judges, while northern region and
early age are assigned no similarity at all.
We train our models as fully unsupervised au-
toencoders on the British National Corpus for this
task. We assume fixed parse trees for all of the
compounds (Figure 6), and use these to compute
compound level vectors for all word pairs. We
subsequently use the cosine distance between each
compound pair as our similarity measure. We
use Spearman?s rank correlation coefficient (?) for
evaluation; hence there is no need to rescale our
scores (-1.0 ? 1.0) to the original scale (1.0 ? 7.0).
Blacoe and Lapata (2012) have an extensive
comparison of the performance of various vector-
based models on this data set to which we compare
our model in Table 5. The CCAE models outper-
7http://homepages.inf.ed.ac.uk/mlap/
resources/index.html
900
Verb Object
VB NN
(S\NP)/NP N
NP
>
S\NP
Noun Noun
NN NN
N/N N
>N
Adjective Noun
JJ NN
N/N N
>N
Figure 6: Assumed CCG parse structure for the compound similarity evaluation.
Method Adj-N N-N V-Obj
Human 0.52 0.49 0.55
(Blacoe and Lapata, 2012)
/+ 0.21 - 0.48 0.22 - 0.50 0.18 - 0.35
RAE 0.19 - 0.31 0.24 - 0.30 0.09 - 0.28
CCAE-B 0.38 0.44 0.34
CCAE-C 0.38 0.41 0.23
CCAE-D 0.41 0.44 0.29
Table 5: Correlation coefficients of model predic-
tions for the compound similarity task. Numbers
show Spearman?s rank correlation coefficient (?).
Higher numbers indicate better correlation.
form the RAE models provided by Blacoe and La-
pata (2012), and score towards the upper end of the
range of other models considered in that paper.
5.3 Qualitative Analysis
To get better insight into our models we also per-
form a small qualitative analysis. Using one of the
models trained on the MPQA corpus, we gener-
ate word-level representations of all phrases in this
corpus and subsequently identify the most related
expressions by using the cosine distance measure.
We perform this experiment on all expressions of
length 5, considering all expressions with a word
length between 3 and 7 as potential matches.
As can be seen in Table 6, this works with vary-
ing success. Linking expressions such as convey-
ing the message of peace and safeguard(ing) peace
and security suggests that the model does learn
some form of semantics.
On the other hand, the connection between ex-
pressed their satisfaction and support and ex-
pressed their admiration and surprise suggests
that the pure word level content still has an impact
on the model analysis. Likewise, the expressions
is a story of success and is a staunch supporter
have some lexical but little semantic overlap. Fur-
ther reducing this link between the lexical and the
semantic representation is an issue that should be
addressed in future work in this area.
6 Discussion
Overall, our models compare favourably with the
state of the art. On the MPQA corpus model
CCAE-D achieves the best published results we
are aware of, whereas on the SP corpus we achieve
competitive results. With an additional, unsuper-
vised training step we achieved results beyond the
current state of the art on this task, too.
Semantics The qualitative analysis and the ex-
periment on compounds demonstrate that the
CCAE models are capable of learning semantics.
An advantage of our approach?and of autoen-
coders generally?is their ability to learn in an
unsupervised setting. The pre-training step for
the sentiment task was essentially the same train-
ing step as used in the compound similarity task.
While other models such as the MV-RNN (Socher
et al, 2012b) achieve good results on a particu-
lar task, they do not allow unsupervised training.
This prevents the possiblity of pretraining, which
we showed to have a big impact on results, and fur-
ther prevents the training of general models: The
CCAE models can be used for multiple tasks with-
out the need to re-train the main model.
Complexity Previously in this paper we argued
that our models combined the strengths of other
approaches. By using a grammar formalism we
increase the expressive power of the model while
the complexity remains low. For the complex-
ity analysis see Table 7. We strike a balance be-
tween the greedy approaches (e.g. Socher et al
(2011b)), where learning is quadratic in the length
of each sentence and existing syntax-driven ap-
proaches such as the MV-RNN of Socher et al
(2012b), where the size of the model, that is the
number of variables that needs to be learned, is
quadratic in the size of the word-embeddings.
Sparsity Parametrizing on CCG types and rules
increases the size of the model compared to a
greedy RAE (Socher et al, 2011b). The effect
of this was highlighted by the sentiment analysis
task, with the more complex models performing
901
Expression Most Similar
convey the message of peace safeguard peace and security
keep alight the flame of keep up the hope
has a reason to repent has no right
a significant and successful strike a much better position
it is reassuring to believe it is a positive development
expressed their satisfaction and support expressed their admiration and surprise
is a story of success is a staunch supporter
are lining up to condemn are going to voice their concerns
more sanctions should be imposed charges being leveled
could fray the bilateral goodwill could cause serious damage
Table 6: Phrases from the MPQA corpus and their semantically closest match according to CCAE-D.
Complexity
Model Size Learning
MV-RNN O(nw2) O(l)
RAE O(nw) O(l2)
CCAE-* O(nw) O(l)
Table 7: Comparison of models. n is dictionary
size, w embedding width, l is sentence length. We
can assume l  n  w. Additional factors such
as CCG rules and types are treated as small con-
stants for the purposes of this analysis.
worse in comparison with the simpler ones. We
were able to overcome this issue by using addi-
tional training data. Beyond this, it would also be
interesting to investigate the relationships between
different types and to derive functions to incorpo-
rate this into the learning procedure. For instance
model learning could be adjusted to enforce some
mirroring effects between the weight matrices of
forward and backward application, or to support
similarities between those of forward application
and composition.
CCG-Vector Interface Exactly how the infor-
mation contained in a CCG derivation is best ap-
plied to a vector space model of compositionality
is another issue for future research. Our investi-
gation of this matter by exploring different model
setups has proved somewhat inconclusive. While
CCAE-D incorporated the deepest conditioning on
the CCG structure, it did not decisively outperform
the simpler CCAE-B which just conditioned on
the combinatory operators. Issues of sparsity, as
shown in our experiments on pretraining, have a
significant influence, which requires further study.
7 Conclusion
In this paper we have brought a more formal no-
tion of semantic compositionality to vector space
models based on recursive autoencoders. This was
achieved through the use of the CCG formalism
to provide a conditioning structure for the matrix
vector products that define the RAE.
We have explored a number of models, each of
which conditions the compositional operations on
different aspects of the CCG derivation. Our ex-
perimental findings indicate a clear advantage for
a deeper integration of syntax over models that use
only the bracketing structure of the parse tree.
The most effective way to condition the compo-
sitional operators on the syntax remains unclear.
Once the issue of sparsity had been addressed, the
complex models outperformed the simpler ones.
Among the complex models, however, we could
not establish significant or consistent differences
to convincingly argue for a particular approach.
While the connections between formal linguis-
tics and vector space approaches to NLP may not
be immediately obvious, we believe that there is a
case for the continued investigation of ways to best
combine these two schools of thought. This paper
represents one step towards the reconciliation of
traditional formal approaches to compositional se-
mantics with modern machine learning.
Acknowledgements
We thank the anonymous reviewers for their feed-
back and Richard Socher for providing additional
insight into his models. Karl Moritz would further
like to thank Sebastian Riedel for hosting him at
UCL while this paper was written. This work has
been supported by the EPSRC.
902
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of LREC, pages
3196?3200, Istanbul, Turkey.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems 19, pages 153?160.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP-CoNLL,
pages 546?556.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. CL, 33(4):493?552, December.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of ACL Demo and Poster
Sessions, pages 33?36.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. 1952-59:1?32.
Gottfried Frege. 1892. U?ber Sinn und Bedeutung. In
Mark Textor, editor, Funktion - Begriff - Bedeutung,
volume 4 of Sammlung Philosophie. Vandenhoeck
& Ruprecht, Go?ttingen.
Christoph Goller and Andreas Ku?chler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of the ICNN-96, pages 347?352. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Geoffrey E. Hinton, Simon Osindero, Max Welling,
and Yee Whye Teh. 2006. Unsupervised discovery
of nonlinear structure using contrastive backpropa-
gation. Cognitive Science, 30(4):725?731.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
CL, 33(3):355?396, September.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge, MA.
Marcus Kracht. 2008. Compositionality in Montague
Grammar. In Edouard Machery und Markus Wern-
ing Wolfram Hinzen, editor, Handbook of Composi-
tionality, pages 47 ? 63. Oxford University Press.
Yann LeCun, Leon Bottou, Genevieve Orr, and Klaus-
Robert Muller. 1998. Efficient backprop. In G. Orr
and Muller K., editors, Neural Networks: Tricks of
the trade. Springer.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL,
pages 317?324.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
of ACL, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
R. Montague. 1970. Universal grammar. Theoria,
36(3):373?398.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In NAACL-
HLT, pages 786?794.
Bo Pang and Lillian Lee. 2005. Seeing stars: exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115?124.
Francis Jeffry Pelletier. 1994. The principle of seman-
tic compositionality. Topoi, 13:11?24.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of ACL, ACL ?93, pages 183?190.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46:77?105.
Christian Scheible and Hinrich Schu?tze. 2013. Cutting
recursive autoencoder trees. In Proceedings of the
International Conference on Learning Representa-
tions.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. CL, 24(1):97?123, March.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems 24.
903
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Richard Socher, Brody Huval, Bharath Bhat, Christo-
pher D. Manning, and Andrew Y. Ng. 2012a.
Convolutional-Recursive Deep Learning for 3D Ob-
ject Classification. In Advances in Neural Informa-
tion Processing Systems 25.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012b. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of EMNLP-CoNLL, pages 1201?
1211.
Mark Steedman and Jason Baldridge, 2011. Combina-
tory Categorial Grammar, pages 181?224. Wiley-
Blackwell.
Anna Szabolcsi. 1989. Bound Variables in Syntax:
Are There Any? In Renate Bartsch, Johan van Ben-
them, and Peter van Emde Boas, editors, Semantics
and Contextual Expression, pages 295?318. Foris,
Dordrecht.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384?394.
Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: simple, good sentiment and topic
classification. In Proceedings of ACL, pages 90?94.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of EMNLP-
HLT, HLT ?05, pages 347?354.
904
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing Graphs with Hyperedge Replacement Grammars
David Chiang
Information Sciences Institute
University of Southern California
Jacob Andreas
Columbia University
University of Cambridge
Daniel Bauer
Department of Computer Science
Columbia University
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Bevan Jones
University of Edinburgh
Macquarie University
Kevin Knight
Information Sciences Institute
University of Southern California
Abstract
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm?s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
1 Introduction
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al, 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al, 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann?s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: ?As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.? Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
2 Hyperedge replacement grammars
We give a short example of how HRG works, fol-
lowed by formal definitions.
2.1 Example
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S . The first rewriting step replaces this edge
with a subgraph, which we might read as ?The
924
boy?girl?
want? arg0
arg1
boy?
believe? arg1
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 1: Sample members of a graph language,
representing the meanings of (clockwise from up-
per left): ?The girl wants the boy,? ?The boy is
believed,? and ?The boy wants the girl to believe
that he wants her.?
boy wants something (X) involving himself.? The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as ?The boy
wants the girl to believe something (Y) involving
both of them.? The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
2.2 Definitions
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = ?V, E, ??, where
? V is a finite set of nodes
? E ? V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
? ? : E ? C assigns a label (drawn from the
finite set C) to each edge.
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
Definition 2. A hypergraph fragment is a tuple
?V, E, ?, X?, where ?V, E, ?? is a hypergraph and
X ? V+ is a list of distinct nodes called the ex-
ternal nodes.
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = ?N,T, P, S ? where
? N and T are finite disjoint sets of nonterminal
and terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
A ? R, where A ? N and R is a graph frag-
ment over N ? T .
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ?G H? (or, H? is derived from H in one
step) as follows. Let e = (v1 ? ? ? vk) be an edge in
H with label A. Let (A? R) be a production ofG,
where R has external nodes XR = (u1 ? ? ? uk). Then
we write H ?G H? if H? is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1, . . . , k.
Let H ??G H? (or, H? is derived from H) be thereflexive, transitive closure of?G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
S ??G H.
When a HRG rule (A ? R) is applied to an
edge e, the mapping of external nodes in R to the
925
1
X ?
believe? arg1
girl?
arg0
1
Y
1 2
Y
?
12
want?
arg0
arg1
S
1
boy?
X
want? arg1
arg0
2 believe? arg1
want? arg1
girl?
arg0
boy?
arg0
Y
3
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?The
boy wants the girl to believe that he wants her.?
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = ?N,T, P, S , ??, where
?N, T, P, S ? is a HRG and ? : P ? K assigns a
weight in K to each production. The weight of a
derivation ofG is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
3 Parsing
Lautemann?s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
3.1 Representing subgraphs
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = ?V, E, ?? is, for some
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
926
subset E? ? E, the smallest subgraph containing
all edges in E?. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 ? ? ?w j. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple ?bn(I), be(I, v),m ? I?, where
? bn(I) is the set of boundary nodes of I
? be(I, v) be the set of boundary edges of v in I
? (m ? I) is a flag indicating whether the
marker node is in I.
The boundary representation of I suffices to
specify I compactly.
Proposition 1. If I and I? are two subgraphs of H
with the same boundary representation, then I =
I?.
Proof. Case 1: bn(I) is empty. If m ? I and m ? I?,
then all edges of H must belong to both I and I?,
that is, I = I? = H. Otherwise, if m < I and m < I?,
then no edges can belong to either I or I?, that is,
I = I? = ?.
Case 2: bn(I) is nonempty. Suppose I , I?;
without loss of generality, suppose that there is an
edge e that is in I \ I?. Let ? be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along ?
must be in I \ I?, or else there would be a boundary
node in the middle of ?, and ? would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of ?must be in I \ I?. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I?, which is a contradiction. 
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I ? J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I ? J iff it has a
boundary node of I ? J as an endpoint and is a
boundary edge of I or J.
Proof. (?) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(?) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I ? J, and an edge e? outside I, which
must also be outside J. For e < J (because I and
J are disjoint), and if e? ? J, then v would be a
boundary node of J. Therefore, e? < I ? J, so v is
a boundary node of I ? J. Case (ii): v has an edge
in I and therefore I ? J, and an edge not in either
I or J. 
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v ? bn(I) do
E ? be(I, v) ? be(J, v)
if v < bn(J) or v has an edge not in E then
add v to bn(I ? J)
be(I ? J, v)? E
for all v ? bn(J) do
if v < bn(I) then
add v to bn(I ? J)
be(I ? J, v)? be(I, v) ? be(J, v)
(m ? I ? J)? (m ? I) ? (m ? J)
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
3.2 Treewidth
Lautemann?s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
927
the notion of treewidth (Gildea, 2011), which we
review briefly here.
Definition 7. A tree decomposition of a graph
H = ?V, E? is a tree T , each of whose nodes ?
is associated with sets V? ? V and E? ? E, with
the following properties:
1. Vertex cover: For each v ? V , there is a node
? ? T such that v ? V?.
2. Edge cover: For each e = (v1 ? ? ? vk) ? E,
there is exactly one node ? ? T such that e ?
E?. We say that ? introduces e. Moreover,
v1, . . . , vk ? V?.
3. Running intersection: For each v ? V , the set
{? ? T | v ? V?} is connected.
The width of T is max |V?| ? 1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
?V, E, X? is a tree decomposition of ?V, E? that has
the additional property that all the external nodes
belong to V? for some ?. (Without loss of general-
ity, we assume that ? is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
K4 =
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al, 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a na??ve algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T . Then,
augment the V? as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SynSem algorithm (Jones et al, 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
method mean max
exact 1.491 2
approximate 1.494 3
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
(a) 0
a
believe? arg1
b
girl?
arg0
1
Y
(b) 0
1
0
b 1
0
a
b 1
arg1
a
b 1
Y
?
0
b
arg0
b
girl?
?
0believe?
?
Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al (2011)). Each tree node ?
must be one of:
? A leaf node, such that V? = ?.
? A unary node, which introduces exactly one
edge e.
? A binary node, which introduces no edges.
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A ?
R) ? G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
928
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let RD? be the subgraph of R in-
duced by the union of E?? for all ?? equal to or
dominated by ?. Then we can show the following.
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of RD? belong to V? ? Vparent(?).
Proof. Let v be a boundary node of RD?. Node v
must have an edge in RD? and therefore in R?? for
some ?? dominated by or equal to ?.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both V? and Vparent(?)
must contain v as well.
Case 2: v has an edge not in RD?. Therefore
there must be a tree node not dominated by or
equal to ? that contains this edge, and therefore
v. So by the running intersection property, ? and
its parent must contain v as well. 
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
3.3 Inference rules
We present the parsing algorithm as a deductive
system (Shieber et al, 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X ? V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A ??G I. Thus, the goalitem is [S ,H, ?]. An active item has the form
[A? R, ?, I, ?], where
? (A? R) is a production of G
? ? is a node of TR
? I is a subgraph of H
? ? is a bijection between the boundary nodes
of RD? and those of I.
The parser must ensure that ? is a bijection when
it creates a new item. Below, we use the notation
{e 7? e?} or {e 7? X} for the mapping that sends
each node of e to the corresponding node of e?
or X.
Passive items are generated by the following
rule:
? Root [B? Q, ?, J, ?]
[B, J, X]
where ? is the root of TQ, and X j = ?(XQ, j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
? Leaf
[A? R, ?, ?, ?]
where ? is a leaf node of TR.
? (Unary) Nonterminal
[A? R, ?1, I, ?] [B, J, X]
[A? R, ?, I ? J, ? ? {e 7? X}]
where ?1 is the only child of ?, and e is intro-
duced by ? and is labeled with nonterminal B.
? (Unary) Terminal
[A? R, ?1, I, ?]
[A? R, ?, I ? {e?}, ? ? {e 7? e?}]
where ?1 is the only child of ?, e is introduced
by ?, and e and e? are both labeled with ter-
minal a.
? Binary
[A? R, ?1, I, ?1] [A? R, ?2, J, ?2]
[A? R, ?, I ? J, ?1 ? ?2]
where ?1 and ?2 are the two children of ?.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key ?B, |bn(J)|?, so that when the next item on the
agenda is an active item [A ? R, ?1, I, ?], we
know that all possible matching passive items are
929
S ?
X
X
X
X ?
a
a a
a
a
(a) (b)
a
a a
a aa
(c)
Figure 4: Illustration of unsoundness in the recog-
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key ??(e), |e|?. Similarly, active items should
be indexed by key ??(e), |e|? so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (?1
or ?2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
3.4 The disjointness check
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H?, as well as a graph isomorphism
? : H? ? H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H? is isomorphic to the input graph H? as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
Condition (1) is enough to guarantee that ? is lo-
cally one-to-one in the sense that for all v ? H?, ?
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if ?I : I? ? H and
?J : J? ? H are locally one-to-one, then ?I ? ?J
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where ? changes (say) from being
one-to-one to two-to-one. So if ? is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that ? maps only
one node to the marker m. We can show this again
by induction: if ?I and ?J each map only one node
to m, then ?I??J must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that ?I , ?J , and ?I ? ?J
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H? are connected graphs,
? : H? ? H is locally one-to-one, and ??1 is de-
fined for some node of H, then ? is a bijection.
Proof. Suppose that ? is not a bijection. Then
there must be two nodes v?1, v?2 ? H? such that
?(v?1) = ?(v?2) = v ? H. We also know that thereis a node, namely, m, such that m? = ??1(m) is de-
fined.2 Choose a path ? (ignoring edge direction)
from v to m. Because ? is a local isomorphism,
we can construct a path from v?1 to m? that mapsto ?. Similarly, we can construct a path from v?2to m? that maps to ?. Let u? be the first node that
these two paths have in common. But u? must have
two edges that map to the same edge, which is a
contradiction. 
2If H were not connected, we would choose the marker in
the same connected component as v.
930
3.5 Complexity
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of RD? or RD?i , all
of which belong to V? (by Proposition 3), so there
are at most |V?| ? k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A? R, ?, I, ?], every boundary node of RD? must
belong to V??Vparent(?) (by Proposition 3). There-
fore the number of boundary nodes is at most k+1
(but typically less), and the number of possible
items is in O((2dn)k+1).
4 Synchronous Parsing
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
?N, T, T ?, P, S ?, where
? N is a finite set of nonterminal symbols
? T and T ? are finite sets of terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
(A? ?R,R?,??), where R is a graph fragment
over N ? T and R? is a graph fragment over
N ? T ?. The relation ? is a bijection linking
nonterminal mentions in R and R?, such that
if e ? e?, then they have the same label. We
call R the source side and R? the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A? ?R,R?,??), we con-
struct a nice tree decomposition of R?R? such that:
? All the external nodes of both R and R? be-
long to V? for some ?. (Without loss of gen-
erality, assume that ? is the root.)
? If e ? e?, then e and e? are introduced by the
same tree node.
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I?, X?] and active
items have the form [A? R : R?, ?, I, ?, I?, ??].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
[A? R : R?, ?1, I, ?, I?, ??] [B, J, X, J?, X?]
[A? R : R?, ?, I ? J, ? ? {e j 7? X j},
I? ? J?, ?? ? {e?j 7? X?j}]
where ?1 is the only child of ?, e and e? are both
introduced by ? and e ? e?, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
5 Conclusion
Although Lautemann?s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann?s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
3The Bolinas toolkit can be downloaded from
?http://www.isi.edu/licensed-sw/bolinas/?.
931
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
References
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ?97), pages 29?36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Micha? Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95?162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231?248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69?124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399?421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65?72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars?basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136?167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
932
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 58?68,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Multilingual Models for Compositional Distributed Semantics
Karl Moritz Hermann and Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
Abstract
We present a novel technique for learn-
ing semantic representations, which ex-
tends the distributional hypothesis to mul-
tilingual data and joint-space embeddings.
Our models leverage parallel data and
learn to strongly align the embeddings of
semantically equivalent sentences, while
maintaining sufficient distance between
those of dissimilar sentences. The mod-
els do not rely on word alignments or
any syntactic information and are success-
fully applied to a number of diverse lan-
guages. We extend our approach to learn
semantic representations at the document
level, too. We evaluate these models on
two cross-lingual document classification
tasks, outperforming the prior state of the
art. Through qualitative analysis and the
study of pivoting effects we demonstrate
that our representations are semantically
plausible and can capture semantic rela-
tionships across languages without paral-
lel data.
1 Introduction
Distributed representations of words provide the
basis for many state-of-the-art approaches to var-
ious problems in natural language processing to-
day. Such word embeddings are naturally richer
representations than those of symbolic or discrete
models, and have been shown to be able to capture
both syntactic and semantic information. Success-
ful applications of such models include language
modelling (Bengio et al, 2003), paraphrase detec-
tion (Erk and Pad?o, 2008), and dialogue analysis
(Kalchbrenner and Blunsom, 2013).
Within a monolingual context, the distributional
hypothesis (Firth, 1957) forms the basis of most
approaches for learning word representations. In
Figure 1: Model with parallel input sentences a and b. The
model minimises the distance between the sentence level en-
coding of the bitext. Any composition functions (CVM) can
be used to generate the compositional sentence level repre-
sentations.
this work, we extend this hypothesis to multilin-
gual data and joint-space embeddings. We present
a novel unsupervised technique for learning se-
mantic representations that leverages parallel cor-
pora and employs semantic transfer through com-
positional representations. Unlike most methods
for learning word representations, which are re-
stricted to a single language, our approach learns
to represent meaning across languages in a shared
multilingual semantic space.
We present experiments on two corpora. First,
we show that for cross-lingual document clas-
sification on the Reuters RCV1/RCV2 corpora
(Lewis et al, 2004), we outperform the prior state
of the art (Klementiev et al, 2012). Second,
we also present classification results on a mas-
sively multilingual corpus which we derive from
the TED corpus (Cettolo et al, 2012). The re-
sults on this task, in comparison with a number of
strong baselines, further demonstrate the relevance
of our approach and the success of our method
in learning multilingual semantic representations
over a wide range of languages.
58
2 Overview
Distributed representation learning describes the
task of learning continuous representations for dis-
crete objects. Here, we focus on learning seman-
tic representations and investigate how the use of
multilingual data can improve learning such rep-
resentations at the word and higher level. We
present a model that learns to represent each
word in a lexicon by a continuous vector in R
d
.
Such distributed representations allow a model to
share meaning between similar words, and have
been used to capture semantic, syntactic and mor-
phological content (Collobert and Weston, 2008;
Turian et al, 2010, inter alia).
We describe a multilingual objective function
that uses a noise-contrastive update between se-
mantic representations of different languages to
learn these word embeddings. As part of this, we
use a compositional vector model (CVM, hence-
forth) to compute semantic representations of sen-
tences and documents. A CVM learns seman-
tic representations of larger syntactic units given
the semantic representations of their constituents
(Clark and Pulman, 2007; Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Socher et al, 2012; Her-
mann and Blunsom, 2013, inter alia).
A key difference between our approach and
those listed above is that we only require sentence-
aligned parallel data in our otherwise unsuper-
vised learning function. This removes a number of
constraints that normally come with CVM mod-
els, such as the need for syntactic parse trees, word
alignment or annotated data as a training signal.
At the same time, by using multiple CVMs to
transfer information between languages, we en-
able our models to capture a broader semantic con-
text than would otherwise be possible.
The idea of extracting semantics from multilin-
gual data stems from prior work in the field of
semantic grounding. Language acquisition in hu-
mans is widely seen as grounded in sensory-motor
experience (Bloom, 2001; Roy, 2003). Based
on this idea, there have been some attempts at
using multi-modal data for learning better vec-
tor representations of words (e.g. Srivastava and
Salakhutdinov (2012)). Such methods, however,
are not easily scalable across languages or to large
amounts of data for which no secondary or tertiary
representation might exist.
Parallel data in multiple languages provides an
alternative to such secondary representations, as
parallel texts share their semantics, and thus one
language can be used to ground the other. Some
work has exploited this idea for transferring lin-
guistic knowledge into low-resource languages or
to learn distributed representations at the word
level (Klementiev et al, 2012; Zou et al, 2013;
Lauly et al, 2013, inter alia). So far almost all
of this work has been focused on learning multi-
lingual representations at the word level. As dis-
tributed representations of larger expressions have
been shown to be highly useful for a number of
tasks, it seems to be a natural next step to attempt
to induce these, too, cross-lingually.
3 Approach
Most prior work on learning compositional se-
mantic representations employs parse trees on
their training data to structure their composition
functions (Socher et al, 2012; Hermann and Blun-
som, 2013, inter alia). Further, these approaches
typically depend on specific semantic signals such
as sentiment- or topic-labels for their objective
functions. While these methods have been shown
to work in some cases, the need for parse trees and
annotated data limits such approaches to resource-
fortunate languages. Our novel method for learn-
ing compositional vectors removes these require-
ments, and as such can more easily be applied to
low-resource languages.
Specifically, we attempt to learn semantics from
multilingual data. The idea is that, given enough
parallel data, a shared representation of two paral-
lel sentences would be forced to capture the com-
mon elements between these two sentences. What
parallel sentences share, of course, are their se-
mantics. Naturally, different languages express
meaning in different ways. We utilise this di-
versity to abstract further from mono-lingual sur-
face realisations to deeper semantic representa-
tions. We exploit this semantic similarity across
languages by defining a bilingual (and trivially
multilingual) energy as follows.
Assume two functions f : X ? R
d
and
g : Y ? R
d
, which map sentences from lan-
guages x and y onto distributed semantic
representations in R
d
. Given a parallel corpus C,
we then define the energy of the model given two
sentences (a, b) ? C as:
E
bi
(a, b) = ?f(a)? g(b)?
2
(1)
59
We want to minimize E
bi
for all semantically
equivalent sentences in the corpus. In order to
prevent the model from degenerating, we fur-
ther introduce a noise-constrastive large-margin
update which ensures that the representations of
non-aligned sentences observe a certain margin
from each other. For every pair of parallel sen-
tences (a, b) we sample a number of additional
sentence pairs (?, n) ? C, where n?with high
probability?is not semantically equivalent to a.
We use these noise samples as follows:
E
hl
(a, b, n) = [m+ E
bi
(a, b)? E
bi
(a, n)]
+
where [x]
+
= max(x, 0) denotes the standard
hinge loss and m is the margin. This results in
the following objective function:
J(?) =
?
(a,b)?C
(
k
?
i=1
E
hl
(a, b, n
i
) +
?
2
???
2
)
(2)
where ? is the set of all model variables.
3.1 Two Composition Models
The objective function in Equation 2 could be cou-
pled with any two given vector composition func-
tions f, g from the literature. As we aim to apply
our approach to a wide range of languages, we fo-
cus on composition functions that do not require
any syntactic information. We evaluate the follow-
ing two composition functions.
The first model, ADD, represents a sentence by
the sum of its word vectors. This is a distributed
bag-of-words approach as sentence ordering is not
taken into account by the model.
Second, the BI model is designed to capture bi-
gram information, using a non-linearity over bi-
gram pairs in its composition function:
f(x) =
n
?
i=1
tanh (x
i?1
+ x
i
) (3)
The use of a non-linearity enables the model to
learn interesting interactions between words in a
document, which the bag-of-words approach of
ADD is not capable of learning. We use the hy-
perbolic tangent as activation function.
3.2 Document-level Semantics
For a number of tasks, such as topic modelling,
representations of objects beyond the sentence
level are required. While most approaches to com-
positional distributed semantics end at the word
Figure 2: Description of a parallel document-level composi-
tional vector model (DOC). The model recursively computes
semantic representations for each sentence of a document and
then for the document itself, treating the sentence vectors as
inputs for a second CVM.
level, our model extends to document-level learn-
ing quite naturally, by recursively applying the
composition and objective function (Equation 2)
to compose sentences into documents. This is
achieved by first computing semantic representa-
tions for each sentence in a document. Next, these
representations are used as inputs in a higher-level
CVM, computing a semantic representation of a
document (Figure 2).
This recursive approach integrates document-
level representations into the learning process.
We can thus use corpora of parallel documents?
regardless of whether they are sentence aligned or
not?to propagate a semantic signal back to the
individual words. If sentence alignment is avail-
able, of course, the document-signal can simply
be combined with the sentence-signal, as we did
with the experiments described in ?5.3.
This concept of learning compositional repre-
sentations for documents contrasts with prior work
(Socher et al, 2011; Klementiev et al, 2012, inter
alia) who rely on summing or averaging sentence-
vectors if representations beyond the sentence-
level are required for a particular task.
We evaluate the models presented in this paper
both with and without the document-level signal.
We refer to the individual models used as ADD and
BI if used without, and as DOC/ADD and DOC/BI
is used with the additional document composition
function and error signal.
4 Corpora
We use two corpora for learning semantic rep-
resentations and performing the experiments de-
scribed in this paper.
60
The Europarl corpus v7
1
(Koehn, 2005) was
used during initial development and testing of
our approach, as well as to learn the representa-
tions used for the Cross-Lingual Document Clas-
sification task described in ?5.2. We considered
the English-German and English-French language
pairs from this corpus. From each pair the final
100,000 sentences were reserved for development.
Second, we developed a massively multilin-
gual corpus based on the TED corpus
2
for IWSLT
2013 (Cettolo et al, 2012). This corpus contains
English transcriptions and multilingual, sentence-
aligned translations of talks from the TED confer-
ence. While the corpus is aimed at machine trans-
lation tasks, we use the keywords associated with
each talk to build a subsidiary corpus for multilin-
gual document classification as follows.
3
The development sections provided with the
IWSLT 2013 corpus were again reserved for de-
velopment. We removed approximately 10 per-
cent of the training data in each language to cre-
ate a test corpus (all talks with id ? 1,400). The
new training corpus consists of a total of 12,078
parallel documents distributed across 12 language
pairs
4
. In total, this amounts to 1,678,219 non-
English sentences (the number of unique English
sentences is smaller as many documents are trans-
lated into multiple languages and thus appear re-
peatedly in the corpus). Each document (talk) con-
tains one or several keywords. We used the 15
most frequent keywords for the topic classification
experiments described in section ?5.3.
Both corpora were pre-processed using the set
of tools provided by cdec
5
for tokenizing and low-
ercasing the data. Further, all empty sentences and
their translations were removed from the corpus.
5 Experiments
We report results on two experiments. First, we
replicate the cross-lingual document classification
task of Klementiev et al (2012), learning dis-
tributed representations on the Europarl corpus
and evaluating on documents from the Reuters
RCV1/RCV2 corpora. Subsequently, we design a
1
http://www.statmt.org/europarl/
2
https://wit3.fbk.eu/
3
http://www.clg.ox.ac.uk/tedcldc/
4
English to Arabic, German, French, Spanish, Italian,
Dutch, Polish, Brazilian Portuguese, Romanian, Russian and
Turkish. Chinese, Farsi and Slowenian were removed due to
the small size of those datasets.
5
http://cdec-decoder.org/
multi-label classification task using the TED cor-
pus, both for training and evaluating. The use of
a wider range of languages in the second experi-
ments allows us to better evaluate our models? ca-
pabilities in learning a shared multilingual seman-
tic representation. We also investigate the learned
embeddings from a qualitative perspective in ?5.4.
5.1 Learning
All model weights were randomly initialised us-
ing a Gaussian distribution (?=0, ?
2
=0.1). We
used the available development data to set our
model parameters. For each positive sample we
used a number of noise samples (k ? {1, 10, 50}),
randomly drawn from the corpus at each training
epoch. All our embeddings have dimensionality
d=128, with the margin set to m=d.
6
Further, we
use L2 regularization with ?=1 and step-size in
{0.01, 0.05}. We use 100 iterations for the RCV
task, 500 for the TED single and 5 for the joint
corpora. We use the adaptive gradient method,
AdaGrad (Duchi et al, 2011), for updating the
weights of our models, in a mini-batch setting (b ?
{10, 50}). All settings, our model implementation
and scripts to replicate our experiments are avail-
able at http://www.karlmoritz.com/.
5.2 RCV1/RCV2 Document Classification
We evaluate our models on the cross-lingual doc-
ument classification (CLDC, henceforth) task first
described in Klementiev et al (2012). This task in-
volves learning language independent embeddings
which are then used for document classification
across the English-German language pair. For this,
CLDC employs a particular kind of supervision,
namely using supervised training data in one lan-
guage and evaluating without further supervision
in another. Thus, CLDC can be used to establish
whether our learned representations are semanti-
cally useful across multiple languages.
We follow the experimental setup described in
Klementiev et al (2012), with the exception that
we learn our embeddings using solely the Europarl
data and use the Reuters corpora only during for
classifier training and testing. Each document in
the classification task is represented by the aver-
age of the d-dimensional representations of all its
sentences. We train the multiclass classifier using
an averaged perceptron (Collins, 2002) with the
same settings as in Klementiev et al (2012).
6
On the RCV task we also report results for d=40 which
matches the dimensionality of Klementiev et al (2012).
61
Model en? de de? en
Majority Class 46.8 46.8
Glossed 65.1 68.6
MT 68.1 67.4
I-Matrix 77.6 71.1
dim = 40
ADD 83.7 71.4
ADD+ 86.2 76.9
BI 83.4 69.2
BI+ 86.9 74.3
dim = 128
ADD 86.4 74.7
ADD+ 87.7 77.5
BI 86.1 79.0
BI+ 88.1 79.2
Table 1: Classification accuracy for training on English and
German with 1000 labeled examples on the RCV corpus.
Cross-lingual compositional representations (ADD, BI and
their multilingual extensions), I-Matrix (Klementiev et al,
2012) translated (MT) and glossed (Glossed) word baselines,
and the majority class baseline. The baseline results are from
Klementiev et al (2012).
We present results from four models. The ADD
model is trained on 500k sentence pairs of the
English-German parallel section of the Europarl
corpus. The ADD+ model uses an additional 500k
parallel sentences from the English-French cor-
pus, resulting in one million English sentences,
each paired up with either a German or a French
sentence, with BI and BI+ trained accordingly.
The motivation behind ADD+ and BI+ is to inves-
tigate whether we can learn better embeddings by
introducing additional data from other languages.
A similar idea exists in machine translation where
English is frequently used to pivot between other
languages (Cohn and Lapata, 2007).
The actual CLDC experiments are performed
by training on English and testing on German doc-
uments and vice versa. Following prior work, we
use varying sizes between 100 and 10,000 docu-
ments when training the multiclass classifier. The
results of this task across training sizes are in Fig-
ure 3. Table 1 shows the results for training on
1,000 documents compared with the results pub-
lished in Klementiev et al (2012). Our models
outperform the prior state of the art, with the BI
models performing slightly better than the ADD
models. As the relative results indicate, the addi-
tion of a second language improves model perfor-
mance. It it interesting to note that results improve
in both directions of the task, even though no addi-
tional German data was used for the ?+? models.
5.3 TED Corpus Experiments
Here we describe our experiments on the TED cor-
pus, which enables us to scale up to multilingual
learning. Consisting of a large number of rela-
tively short and parallel documents, this corpus al-
lows us to evaluate the performance of the DOC
model described in ?3.2.
We use the training data of the corpus to learn
distributed representations across 12 languages.
Training is performed in two settings. In the sin-
gle mode, vectors are learnt from a single lan-
guage pair (en-X), while in the joint mode vector-
learning is performed on all parallel sub-corpora
simultaneously. This setting causes words from
all languages to be embedded in a single semantic
space.
First, we evaluate the effect of the document-
level error signal (DOC, described in ?3.2), as well
as whether our multilingual learning method can
extend to a larger variety of languages. We train
DOC models, using both ADD and BI as CVM
(DOC/ADD, DOC/BI), both in the single and joint
mode. For comparison, we also train ADD and
DOC models without the document-level error sig-
nal. The resulting document-level representations
are used to train classifiers (system and settings as
in ?5.2) for each language, which are then evalu-
ated in the paired language. In the English case
we train twelve individual classifiers, each using
the training data of a single language pair only.
As described in ?4, we use 15 keywords for the
classification task. Due to space limitations, we
report cumulative results in the form of F1-scores
throughout this paper.
MT System We develop a machine translation
baseline as follows. We train a machine translation
tool on the parallel training data, using the devel-
opment data of each language pair to optimize the
translation system. We use the cdec decoder (Dyer
et al, 2010) with default settings for this purpose.
With this system we translate the test data, and
then use a Na??ve Bayes classifier
7
for the actual
experiments. To exemplify, this means the de?ar
result is produced by training a translation system
from Arabic to German. The Arabic test set is
translated into German. A classifier is then trained
7
We use the implementation in Mallet (McCallum, 2002)
62
Setting Languages
Arabic German Spanish French Italian Dutch Polish Pt-Br Roman. Russian Turkish
en? L2
MT System 0.429 0.465 0.518 0.526 0.514 0.505 0.445 0.470 0.493 0.432 0.409
ADD single 0.328 0.343 0.401 0.275 0.282 0.317 0.141 0.227 0.282 0.338 0.241
BI single 0.375 0.360 0.379 0.431 0.465 0.421 0.435 0.329 0.426 0.423 0.481
DOC/ADD single 0.410 0.424 0.383 0.476 0.485 0.264 0.402 0.354 0.418 0.448 0.452
DOC/BI single 0.389 0.428 0.416 0.445 0.473 0.219 0.403 0.400 0.467 0.421 0.457
DOC/ADD joint 0.392 0.405 0.443 0.447 0.475 0.453 0.394 0.409 0.446 0.476 0.417
DOC/BI joint 0.372 0.369 0.451 0.429 0.404 0.433 0.417 0.399 0.453 0.439 0.418
L2 ? en
MT System 0.448 0.469 0.486 0.358 0.481 0.463 0.460 0.374 0.486 0.404 0.441
ADD single 0.380 0.337 0.446 0.293 0.357 0.295 0.327 0.235 0.293 0.355 0.375
BI single 0.354 0.411 0.344 0.426 0.439 0.428 0.443 0.357 0.426 0.442 0.403
DOC/ADD single 0.452 0.476 0.422 0.464 0.461 0.251 0.400 0.338 0.407 0.471 0.435
DOC/BI single 0.406 0.442 0.365 0.479 0.460 0.235 0.393 0.380 0.426 0.467 0.477
DOC/ADD joint 0.396 0.388 0.399 0.415 0.461 0.478 0.352 0.399 0.412 0.343 0.343
DOC/BI joint 0.343 0.375 0.369 0.419 0.398 0.438 0.353 0.391 0.430 0.375 0.388
Table 2: F1-scores for the TED document classification task for individual languages. Results are re-
ported for both directions (training on English, evaluating on L2 and vice versa). Bold indicates best
result, underline best result amongst the vector-based systems.
Training
Language
Test Language
Arabic German Spanish French Italian Dutch Polish Pt-Br Rom?n Russian Turkish
Arabic 0.378 0.436 0.432 0.444 0.438 0.389 0.425 0.420 0.446 0.397
German 0.368 0.474 0.460 0.464 0.440 0.375 0.417 0.447 0.458 0.443
Spanish 0.353 0.355 0.420 0.439 0.435 0.415 0.390 0.424 0.427 0.382
French 0.383 0.366 0.487 0.474 0.429 0.403 0.418 0.458 0.415 0.398
Italian 0.398 0.405 0.461 0.466 0.393 0.339 0.347 0.376 0.382 0.352
Dutch 0.377 0.354 0.463 0.464 0.460 0.405 0.386 0.415 0.407 0.395
Polish 0.359 0.386 0.449 0.444 0.430 0.441 0.401 0.434 0.398 0.408
Portuguese 0.391 0.392 0.476 0.447 0.486 0.458 0.403 0.457 0.431 0.431
Romanian 0.416 0.320 0.473 0.476 0.460 0.434 0.416 0.433 0.444 0.402
Russian 0.372 0.352 0.492 0.427 0.438 0.452 0.430 0.419 0.441 0.447
Turkish 0.376 0.352 0.479 0.433 0.427 0.423 0.439 0.367 0.434 0.411
Table 3: F1-scores for TED corpus document classification results when training and testing on two
languages that do not share any parallel data. We train a DOC/ADD model on all en-L2 language pairs
together, and then use the resulting embeddings to train document classifiers in each language. These
classifiers are subsequently used to classify data from all other languages.
Setting Languages
English Arabic German Spanish French Italian Dutch Polish Pt-Br Roman. Russian Turkish
Raw Data NB 0.481 0.469 0.471 0.526 0.532 0.524 0.522 0.415 0.465 0.509 0.465 0.513
Senna 0.400
Polyglot 0.382 0.416 0.270 0.418 0.361 0.332 0.228 0.323 0.194 0.300 0.402 0.295
single Setting
DOC/ADD 0.462 0.422 0.429 0.394 0.481 0.458 0.252 0.385 0.363 0.431 0.471 0.435
DOC/BI 0.474 0.432 0.362 0.336 0.444 0.469 0.197 0.414 0.395 0.445 0.436 0.428
joint Setting
DOC/ADD 0.475 0.371 0.386 0.472 0.451 0.398 0.439 0.304 0.394 0.453 0.402 0.441
DOC/BI 0.378 0.329 0.358 0.472 0.454 0.399 0.409 0.340 0.431 0.379 0.395 0.435
Table 4: F1-scores on the TED corpus document classification task when training and evaluating on the
same language. Baseline embeddings are Senna (Collobert et al, 2011) and Polyglot (Al-Rfou? et al,
2013).
63
100 200 500 1000 5000
10k
60
70
80
Training Documents (de)
C
l
a
s
s
i
fi
c
a
t
i
o
n
A
c
c
u
r
a
c
y
(
%
)
100 200 500 1000 5000
10k
50
60
70
80
90
Training Documents (en)
ADD+ BI+ I-Matrix MT Glossed
Figure 3: Classification accuracy for a number of models (see Table 1 for model descriptions). The left chart shows results for
these models when trained on German data and evaluated on English data, the right chart vice versa.
on the German training data and evaluated on the
translated Arabic. While we developed this system
as a baseline, it must be noted that the classifier of
this system has access to significantly more infor-
mation (all words in the document) as opposed to
our models (one embedding per document), and
we do not expect to necessarily beat this system.
The results of this experiment are in Table 2.
When comparing the results between the ADD
model and the models trained using the document-
level error signal, the benefit of this additional sig-
nal becomes clear. The joint training mode leads
to a relative improvement when training on En-
glish data and evaluating in a second language.
This suggests that the joint mode improves the
quality of the English embeddings more than it
affects the L2-embeddings. More surprising, per-
haps, is the relative performance between the ADD
and BI composition functions, especially when
compared to the results in ?5.2, where the BI mod-
els relatively consistently performed better. We
suspect that the better performance of the additive
composition function on this task is related to the
smaller amount of training data available which
could cause sparsity issues for the bigram model.
As expected, the MT system slightly outper-
forms our models on most language pairs. How-
ever, the overall performance of the models is
comparable to that of the MT system. Consider-
ing the relative amount of information available
during the classifier training phase, this indicates
that our learned representations are semantically
useful, capturing almost the same amount of infor-
mation as available to the Na??ve Bayes classifier.
We next investigate linguistic transfer across
languages. We re-use the embeddings learned
with the DOC/ADD joint model from the previ-
ous experiment for this purpose, and train clas-
sifiers on all non-English languages using those
embeddings. Subsequently, we evaluate their per-
formance in classifying documents in the remain-
ing languages. Results for this task are in Table 3.
While the results across language-pairs might not
be very insightful, the overall good performance
compared with the results in Table 2 implies that
we learnt semantically meaningful vectors and in
fact a joint embedding space across thirteen lan-
guages.
In a third evaluation (Table 4), we apply the em-
beddings learnt with out models to a monolingual
classification task, enabling us to compare with
prior work on distributed representation learning.
In this experiment a classifier is trained in one lan-
guage and then evaluated in the same. We again
use a Na??ve Bayes classifier on the raw data to es-
tablish a reasonable upper bound.
We compare our embeddings with the SENNA
embeddings, which achieve state of the art per-
formance on a number of tasks (Collobert et al,
2011). Additionally, we use the Polyglot embed-
dings of Al-Rfou? et al (2013), who published
word embeddings across 100 languages, including
all languages considered in this paper. We repre-
sent each document by the mean of its word vec-
tors and then apply the same classifier training and
testing regime as with our models. Even though
both of these sets of embeddings were trained on
much larger datasets than ours, our models outper-
form these baselines on all languages?even out-
performing the Na??ve Bayes system on on several
64
Figure 4: t-SNE projections for a number of English, French
and German words as represented by the BI+ model. Even
though the model did not use any parallel French-German
data during training, it learns semantic similarity between
these two languages using English as a pivot, and semanti-
cally clusters words across all languages.
Figure 5: t-SNE projections for a number of short phrases in
three languages as represented by the BI+ model. The pro-
jection demonstrates linguistic transfer through a pivot by. It
separates phrases by gender (red for female, blue for male,
and green for neutral) and aligns matching phrases across lan-
guages.
languages. While this may partly be attributed to
the fact that our vectors were learned on in-domain
data, this is still a very positive outcome.
5.4 Linguistic Analysis
While the classification experiments focused on
establishing the semantic content of the sentence
level representations, we also want to briefly in-
vestigate the induced word embeddings. We use
the BI+ model trained on the Europarl corpus for
this purpose. Figure 4 shows the t-SNE projec-
tions for a number of English, French and German
words. Even though the model did not use any par-
allel French-German data during training, it still
managed to learn semantic word-word similarity
across these two languages.
Going one step further, Figure 5 shows t-SNE
projections for a number of short phrases in these
three languages. We use the English the presi-
dent and gender-specific expressions Mr President
and Madam President as well as gender-specific
equivalents in French and German. The projec-
tion demonstrates a number of interesting results:
First, the model correctly clusters the words into
three groups, corresponding to the three English
forms and their associated translations. Second, a
separation between genders can be observed, with
male forms on the bottom half of the chart and fe-
male forms on the top, with the neutral the presi-
dent in the vertical middle. Finally, if we assume
a horizontal line going through the president, this
line could be interpreted as a ?gender divide?, with
male and female versions of one expression mir-
roring each other on that line. In the case of the
president and its translations, this effect becomes
even clearer, with the neutral English expression
being projected close to the mid-point between
each other language?s gender-specific versions.
These results further support our hypothesis that
the bilingual contrastive error function can learn
semantically plausible embeddings and further-
more, that it can abstract away from mono-lingual
surface realisations into a shared semantic space
across languages.
6 Related Work
Distributed Representations Distributed repre-
sentations can be learned through a number of ap-
proaches. In their simplest form, distributional in-
formation from large corpora can be used to learn
embeddings, where the words appearing within a
certain window of the target word are used to com-
pute that word?s embedding. This is related to
topic-modelling techniques such as LSA (Dumais
et al, 1988), LSI, and LDA (Blei et al, 2003), but
these methods use a document-level context, and
tend to capture the topics a word is used in rather
than its more immediate syntactic context.
Neural language models are another popular ap-
proach for inducing distributed word representa-
tions (Bengio et al, 2003). They have received a
lot of attention in recent years (Collobert and We-
ston, 2008; Mnih and Hinton, 2009; Mikolov et
al., 2010, inter alia) and have achieved state of the
art performance in language modelling. Collobert
et al (2011) further popularised using neural net-
work architectures for learning word embeddings
from large amounts of largely unlabelled data by
showing the embeddings can then be used to im-
prove standard supervised tasks.
65
Unsupervised word representations can easily
be plugged into a variety of NLP related tasks.
Tasks, where the use of distributed representations
has resulted in improvements include topic mod-
elling (Blei et al, 2003) or named entity recogni-
tion (Turian et al, 2010; Collobert et al, 2011).
Compositional Vector Models For a number of
important problems, semantic representations of
individual words do not suffice, but instead a se-
mantic representation of a larger structure?e.g. a
phrase or a sentence?is required. Self-evidently,
sparsity prevents the learning of such representa-
tions using the same collocational methods as ap-
plied to the word level. Most literature instead fo-
cuses on learning composition functions that rep-
resent the semantics of a larger structure as a func-
tion of the representations of its parts.
Very simple composition functions have been
shown to suffice for tasks such as judging bi-
gram semantic similarity (Mitchell and Lapata,
2008). More complex composition functions us-
ing matrix-vector composition, convolutional neu-
ral networks or tensor composition have proved
useful in tasks such as sentiment analysis (Socher
et al, 2011; Hermann and Blunsom, 2013), rela-
tional similarity (Turney, 2012) or dialogue analy-
sis (Kalchbrenner and Blunsom, 2013).
Multilingual Representation Learning Most
research on distributed representation induction
has focused on single languages. English, with its
large number of annotated resources, has enjoyed
most attention. However, there exists a corpus of
prior work on learning multilingual embeddings
or on using parallel data to transfer linguistic in-
formation across languages. One has to differen-
tiate between approaches such as Al-Rfou? et al
(2013), that learn embeddings across a large va-
riety of languages and models such as ours, that
learn joint embeddings, that is a projection into a
shared semantic space across multiple languages.
Related to our work, Yih et al (2011) proposed
S2Nets to learn joint embeddings of tf-idf vectors
for comparable documents. Their architecture op-
timises the cosine similarity of documents, using
relative semantic similarity scores during learn-
ing. More recently, Lauly et al (2013) proposed a
bag-of-words autoencoder model, where the bag-
of-words representation in one language is used to
train the embeddings in another. By placing their
vocabulary in a binary branching tree, the prob-
abilistic setup of this model is similar to that of
Mnih and Hinton (2009). Similarly, Sarath Chan-
dar et al (2013) train a cross-lingual encoder,
where an autoencoder is used to recreate words in
two languages in parallel. This is effectively the
linguistic extension of Ngiam et al (2011), who
used a similar method for audio and video data.
Hermann and Blunsom (2014) propose a large-
margin learner for multilingual word representa-
tions, similar to the basic additive model proposed
here, which, like the approaches above, relies on a
bag-of-words model for sentence representations.
Klementiev et al (2012), our baseline in ?5.2,
use a form of multi-agent learning on word-
aligned parallel data to transfer embeddings from
one language to another. Earlier work, Haghighi
et al (2008), proposed a method for inducing
bilingual lexica using monolingual feature repre-
sentations and a small initial lexicon to bootstrap
with. This approach has recently been extended
by Mikolov et al (2013a), Mikolov et al (2013b),
who developed a method for learning transforma-
tion matrices to convert semantic vectors of one
language into those of another. Is was demon-
strated that this approach can be applied to im-
prove tasks related to machine translation. Their
CBOW model is also worth noting for its sim-
ilarities to the ADD composition function used
here. Using a slightly different approach, Zou et
al. (2013), also learned bilingual embeddings for
machine translation.
7 Conclusion
To summarize, we have presented a novel method
for learning multilingual word embeddings using
parallel data in conjunction with a multilingual ob-
jective function for compositional vector models.
This approach extends the distributional hypoth-
esis to multilingual joint-space representations.
Coupled with very simple composition functions,
vectors learned with this method outperform the
state of the art on the task of cross-lingual docu-
ment classification. Further experiments and anal-
ysis support our hypothesis that bilingual signals
are a useful tool for learning distributed represen-
tations by enabling models to abstract away from
mono-lingual surface realisations into a deeper se-
mantic space.
Acknowledgements
This work was supported by a Xerox Foundation
Award and EPSRC grant number EP/K036580/1.
66
References
R. Al-Rfou?, B. Perozzi, and S. Skiena. 2013. Poly-
glot: Distributed word representations for multilin-
gual nlp. In Proceedings of CoNLL.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155,
March.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
P. Bloom. 2001. Precis of how children learn the
meanings of words. Behavioral and Brain Sciences,
24:1095?1103.
M. Cettolo, C. Girardi, and M. Federico. 2012. Wit
3
:
Web inventory of transcribed and translated talks. In
Proceedings of EAMT.
S. Clark and S. Pulman. 2007. Combining symbolic
and distributional models of meaning. In Proceed-
ings of AAAI Spring Symposium on Quantum Inter-
action. AAAI Press.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: Making effective use of multi-parallel
corpora. In Proceedings of ACL.
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of ACL-
EMNLP.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Re-
search, 12:2121?2159, July.
S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deer-
wester, and R. Harshman. 1988. Using latent se-
mantic analysis to improve access to textual infor-
mation. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A Decoder, Alignment, and
Learning framework for finite-state and context-free
translation models. In Proceedings of ACL.
K. Erk and S. Pad?o. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. 1952-59:1?32.
E. Grefenstette and M. Sadrzadeh. 2011. Experi-
mental support for a categorical compositional dis-
tributional model of meaning. In Proceedings of
EMNLP.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-HLT.
K. M. Hermann and P. Blunsom. 2013. The Role of
Syntax in Vector Space Models of Compositional
Semantics. In Proceedings of ACL.
K. M. Hermann and P. Blunsom. 2014. Multilingual
Distributed Representations without Word Align-
ment. In Proceedings of ICLR.
N. Kalchbrenner and P. Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. Proceedings of the ACL Workshop on
Continuous Vector Space Models and their Compo-
sitionality.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the
Machine Translation Summit.
S. Lauly, A. Boulanger, and H. Larochelle. 2013.
Learning multilingual word representations using a
bag-of-words autoencoder. In Deep Learning Work-
shop at NIPS.
D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. 2004.
Rcv1: A new benchmark collection for text catego-
rization research. Journal of Machine Learning Re-
search, 5:361?397, December.
A. K. McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
T. Mikolov, M. Karafi?at, L. Burget, J.
?
Cernock?y, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of INTER-
SPEECH.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient Estimation of Word Representations in
Vector Space. CoRR.
T. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Ex-
ploiting Similarities among Languages for Machine
Translation. CoRR.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In In Proceedings of ACL.
67
A. Mnih and G. Hinton. 2009. A scalable hierarchi-
cal distributed language model. In Proceedings of
NIPS.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and
A. Y. Ng. 2011. Multimodal deep learning. In
ICML.
D. Roy. 2003. Grounded spoken language acquisition:
Experiments in word learning. IEEE Transactions
on Multimedia, 5(2):197?209, June.
A. P. Sarath Chandar, M. K. Mitesh, B. Ravindran,
V. Raykar, and A. Saha. 2013. Multilingual deep
learning. In Deep Learning Workshop at NIPS.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Proceedings of EMNLP-
CoNLL, pages 1201?1211.
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In Pro-
ceedings of NIPS.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL.
P. D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
W.-T. Yih, K. Toutanova, J. C. Platt, and C. Meek.
2011. Learning discriminative projections for text
similarity measures. In Proceedings of CoNLL.
W. Y. Zou, R. Socher, D. Cer, and C. D. Manning.
2013. Bilingual word embeddings for phrase-based
machine translation. In Proceedings of EMNLP.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448?1458,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Semantic Frame Identification with Distributed Word Representations
Karl Moritz Hermann
??
Dipanjan Das
?
Jason Weston
?
Kuzman Ganchev
?
?
Department of Computer Science, University of Oxford, Oxford OX1 3QD, United Kingdom
?
Google Inc., 76 9th Avenue, New York, NY 10011, United States
karl.moritz.hermann@cs.ox.ac.uk
{dipanjand,kuzman}@google.com jaseweston@gmail.com
Abstract
We present a novel technique for semantic
frame identification using distributed rep-
resentations of predicates and their syntac-
tic context; this technique leverages auto-
matic syntactic parses and a generic set
of word embeddings. Given labeled data
annotated with frame-semantic parses, we
learn a model that projects the set of word
representations for the syntactic context
around a predicate to a low dimensional
representation. The latter is used for se-
mantic frame identification; with a stan-
dard argument identification method in-
spired by prior work, we achieve state-of-
the-art results on FrameNet-style frame-
semantic analysis. Additionally, we report
strong results on PropBank-style semantic
role labeling in comparison to prior work.
1 Introduction
Distributed representations of words have proved
useful for a number of tasks. By providing richer
representations of meaning than what can be en-
compassed in a discrete representation, such ap-
proaches have successfully been applied to tasks
such as sentiment analysis (Socher et al, 2011),
topic classification (Klementiev et al, 2012) or
word-word similarity (Mitchell and Lapata, 2008).
We present a new technique for semantic frame
identification that leverages distributed word rep-
resentations. According to the theory of frame se-
mantics (Fillmore, 1982), a semantic frame rep-
resents an event or scenario, and possesses frame
elements (or semantic roles) that participate in the
?
The majority of this research was carried out during an
internship at Google.
event. Most work on frame-semantic parsing has
usually divided the task into two major subtasks:
frame identification, namely the disambiguation of
a given predicate to a frame, and argument iden-
tification (or semantic role labeling), the analysis
of words and phrases in the sentential context that
satisfy the frame?s semantic roles (Das et al, 2010;
Das et al, 2014).
1
Here, we focus on the first sub-
task of frame identification for given predicates;
we use our novel method (?3) in conjunction with
a standard argument identification model (?4) to
perform full frame-semantic parsing.
We present experiments on two tasks. First, we
show that for frame identification on the FrameNet
corpus (Baker et al, 1998; Fillmore et al, 2003),
we outperform the prior state of the art (Das et al,
2014). Moreover, for full frame-semantic parsing,
with the presented frame identification technique
followed by our argument identification method,
we report the best results on this task to date. Sec-
ond, we present results on PropBank-style seman-
tic role labeling (Palmer et al, 2005; Meyers et al,
2004; M`arquez et al, 2008), that approach strong
baselines, and are on par with prior state of the art
(Punyakanok et al, 2008).
2 Overview
Early work in frame-semantic analysis was pio-
neered by Gildea and Jurafsky (2002). Subsequent
work in this area focused on either the FrameNet
or PropBank frameworks, and research on the lat-
ter has been more popular. Since the CoNLL
2004-2005 shared tasks (Carreras and M`arquez,
1
There are exceptions, wherein the task has been modeled
using a pipeline of three classifiers that perform frame iden-
tification, a binary stage that classifies candidate arguments,
and argument identification on the filtered candidates (Baker
et al, 2007; Johansson and Nugues, 2007).
1448
John     bought    a   car   .
COMMERCE_BUY
buy.V
Buyer Goods
John     bought    a   car   .
buy.01
buy.V
A0 A1
Mary      sold        a   car   .
COMMERCE_BUY
sell.V
Seller Goods
Mary      sold        a   car   .
sell.01
sell.V
A0 A1
(a) (b)
Figure 1: Example sentences with frame-semantic analyses.
FrameNet annotation conventions are used in (a) while (b)
denotes PropBank conventions.
2004; Carreras and M`arquez, 2005) on PropBank
semantic role labeling (SRL), it has been treated
as an important NLP problem. However, research
has mostly focused on argument analysis, skipping
the frame disambiguation step, and its interaction
with argument identification.
2.1 Frame-Semantic Parsing
Closely related to SRL, frame-semantic parsing
consists of the resolution of predicate sense into
a frame, and the analysis of the frame?s argu-
ments. Work in this area exclusively uses the
FrameNet full text annotations. Johansson and
Nugues (2007) presented the best performing sys-
tem at SemEval 2007 (Baker et al, 2007), and Das
et al (2010) improved performance, and later set
the current state of the art on this task (Das et al,
2014). We briefly discuss FrameNet, and subse-
quently PropBank annotation conventions here.
FrameNet The FrameNet project (Baker et al,
1998) is a lexical database that contains informa-
tion about words and phrases (represented as lem-
mas conjoined with a coarse part-of-speech tag)
termed as lexical units, with a set of semantic
frames that they could evoke. For each frame,
there is a list of associated frame elements (or
roles, henceforth), that are also distinguished as
core or non-core.
2
Sentences are annotated us-
ing this universal frame inventory. For exam-
ple, consider the pair of sentences in Figure 1(a).
COMMERCE BUY is a frame that can be evoked by
morphological variants of the two example lexical
units buy.V and sell.V. Buyer, Seller and Goods are
some example roles for this frame.
2
Additional information such as finer distinction of the
coreness properties of roles, the relationship between frames,
and that of roles are also present, but we do not leverage that
information in this work.
PropBank The PropBank project (Palmer et al,
2005) is another popular resource related to se-
mantic role labeling. The PropBank corpus has
verbs annotated with sense frames and their ar-
guments. Like FrameNet, it also has a lexi-
cal database that stores type information about
verbs, in the form of sense frames and the possi-
ble semantic roles each frame could take. There
are modifier roles that are shared across verb
frames, somewhat similar to the non-core roles
in FrameNet. Figure 1(b) shows annotations for
two verbs ?bought? and ?sold?, with their lemmas
(akin to the lexical units in FrameNet) and their
verb frames buy.01 and sell.01. Generic core role
labels (of which there are seven, namely A0-A5 and
AA) for the verb frames are marked in the figure.
3
A key difference between the two annotation sys-
tems is that PropBank uses a local frame inven-
tory, where frames are predicate-specific. More-
over, role labels, although few in number, take spe-
cific meaning for each verb frame. Figure 1 high-
lights this difference: while both sell.V and buy.V
are members of the same frame in FrameNet, they
evoke different frames in PropBank. In spite of
this difference, nearly identical statistical models
could be employed for both frameworks.
Modeling In this paper, we model the frame-
semantic parsing problem in two stages: frame
identification and argument identification. As
mentioned in ?1, these correspond to a frame dis-
ambiguation stage,
4
and a stage that finds the var-
ious arguments that fulfill the frame?s semantic
roles within the sentence, respectively. This re-
sembles the framework of Das et al (2014), who
solely focus on FrameNet corpora, unlike this pa-
per. The novelty of this paper lies in the frame
identification stage (?3). Note that this two-stage
approach is unusual for the PropBank corpora
when compared to prior work, where the vast ma-
jority of published papers have not focused on the
verb frame disambiguation problem at all, only fo-
cusing on the role labeling stage (see the overview
paper of M`arquez et al (2008) for example).
3
NomBank (Meyers et al, 2004) is a similar resource for
nominal predicates, but we do not consider it in our experi-
ments.
4
For example in PropBank, the lexical unit buy.V has
three verb frames and in sentential context, we want to disam-
biguate its frame. (Although PropBank never formally uses
the term lexical unit, we adopt its usage from the frame se-
mantics literature.)
1449
2.2 Distributed Frame Identification
We present a model that takes word embeddings
as input and learns to identify semantic frames.
A word embedding is a distributed representa-
tion of meaning where each word is represented
as a vector in R
n
. Such representations allow a
model to share meaning between similar words,
and have been used to capture semantic, syntac-
tic and morphological content (Collobert and We-
ston, 2008; Turian et al, 2010, inter alia). We use
word embeddings to represent the syntactic con-
text of a particular predicate instance as a vector.
For example, consider the sentence ?He runs the
company.? The predicate runs has two syntac-
tic dependents ? a subject and direct object (but
no prepositional phrases or clausal complements).
We could represent the syntactic context of runs as
a vector with blocks for all the possible dependents
warranted by a syntactic parser; for example, we
could assume that positions 0 . . . n in the vector
correspond to the subject dependent, n+1 . . . 2n
correspond to the clausal complement dependent,
and so forth. Thus, the context is a vector in R
nk
with the embedding of He at the subject position,
the embedding of company in direct object posi-
tion and zeros everywhere else. Given input vec-
tors of this form for our training data, we learn a
matrix that maps this high dimensional and sparse
representation into a lower dimensional space. Si-
multaneously, the model learns an embedding for
all the possible labels (i.e. the frames in a given
lexicon). At inference time, the predicate-context
is mapped to the low dimensional space, and we
choose the nearest frame label as our classifica-
tion. We next describe this model in detail.
3 Frame Identification with Embeddings
We continue using the example sentence from
?2.2: ?He runs the company.? where we want to
disambiguate the frame of runs in context. First,
we extract the words in the syntactic context of
runs; next, we concatenate their word embeddings
as described in ?2.2 to create an initial vector space
representation. Subsequently, we learn a map-
ping from this initial representation into a low-
dimensional space; we also learn an embedding
for each possible frame label in the same low-
dimensional space. The goal of learning is to
make sure that the correct frame label is as close as
possible to the mapped context, while competing
frame labels are farther away.
Formally, let x represent the actual sentence
with a marked predicate, along with the associated
syntactic parse tree; let our initial representation
of the predicate context be g(x). Suppose that the
word embeddings we start with are of dimension
n. Then g is a function from a parsed sentence
x to R
nk
, where k is the number of possible syn-
tactic context types. For example g selects some
important positions relative to the predicate, and
reserves a block in its output space for the embed-
ding of words found at that position. Suppose g
considers clausal complements and direct objects.
Then g : X ? R
2n
and for the example sentence
it has zeros in positions 0 . . . n and the embedding
of the word company in positions n+1 . . . 2n.
g(x) = [0, . . . , 0, embedding of company].
Section 3.1 describes the context positions we use
in our experiments. Let the low dimensional space
we map to be R
m
and the learned mapping be M :
R
nk
? R
m
. The mapping M is a linear trans-
formation, and we learn it using the WSABIE algo-
rithm (Weston et al, 2011). WSABIE also learns an
embedding for each frame label (y, henceforth).
In our setting, this means that each frame corre-
sponds to a point in R
m
. If we have F possi-
ble frames we can store those parameters in an
F ?m matrix, one m-dimensional point for each
frame, which we will refer to as the linear map-
ping Y . Let the lexical unit (the lemma conjoined
with a coarse POS tag) for the marked predicate
be `. We denote the frames that associate with
` in the frame lexicon
5
and our training corpus
as F
`
. WSABIE performs gradient-based updates
on an objective that tries to minimize the distance
between M(g(x)) and the embedding of the cor-
rect label Y (y), while maintaining a large distance
between M(g(x)) and the other possible labels
Y (y?) in the confusion set F
`
. At disambiguation
time, we use a simple dot product similarity as our
distance metric, meaning that the model chooses
a label by computing the argmax
y
s(x, y) where
s(x, y) = M(g(x)) ?Y (y), where the argmax iter-
ates over the possible frames y ? F
`
if ` was seen
in the lexicon or the training data, or y ? F , if it
was unseen.
6
Model learning is performed using
the margin ranking loss function as described in
5
The frame lexicon stores the frames, corresponding se-
mantic roles and the lexical units associated with the frame.
6
This disambiguation scheme is similar to the one adopted
by Das et al (2014), but they use unlemmatized words to
define their confusion set.
1450
Figure 2: Context representation extraction for the
embedding model. Given a dependency parse (1)
the model extracts all words matching a set of paths
from the frame evoking predicate and its direct de-
pendents (2). The model computes a composed rep-
resentation of the predicate instance by using dis-
tributed vector representations for words (3) ? the
(red) vertical embedding vectors for each word are
concatenated into a long vector. Finally, we learn a
linear transformation function parametrized by the
context blocks (4).
Weston et al (2011), and in more detail in section
3.2.
Since WSABIE learns a single mapping from g(x)
to R
m
, parameters are shared between different
words and different frames. So for example ?He
runs the company? could help the model disam-
biguate ?He owns the company.? Moreover, since
g(x) relies on word embeddings rather than word
identities, information is shared between words.
For example ?He runs the company? could help
us to learn about ?She runs a corporation?.
3.1 Context Representation Extraction
In principle g(x) could be any feature function, but
we performed an initial investigation of two partic-
ular variants. In both variants, our representation
is a block vector where each block corresponds to
a syntactic position relative to the predicate, and
each block?s values correspond to the embedding
of the word at that position.
Direct Dependents The first context function we
considered corresponds to the examples in ?3. To
elaborate, the positions of interest are the labels of
the direct dependents of the predicate, so k is the
number of labels that the dependency parser can
produce. For example, if the label on the edge be-
tween runs and He is nsubj, we would put the em-
bedding of He in the block corresponding to nsubj.
If a label occurs multiple times, then the embed-
dings of the words below this label are averaged.
Unfortunately, using only the direct dependents
can miss a lot of useful information. For exam-
ple, topicalization can place discriminating infor-
mation farther from the predicate. Consider ?He
runs the company.? vs. ?It was the company that
he runs.? In the second sentence, the discrim-
inating word, company dominates the predicate
runs. Similarly, predicates in embedded clauses
may have a distant agent which cannot be captured
using direct dependents. Consider ?The athlete
ran the marathon.? vs. ?The athlete prepared him-
self for three months to run the marathon.? In the
second example, for the predicate run, the agent
The athlete is not a direct dependent, but is con-
nected via a longer dependency path.
Dependency Paths To capture more relevant
context, we developed a second context function
as follows. We scanned the training data for a
given task (either the PropBank or the FrameNet
domains) for the dependency paths that connected
the gold predicates to the gold semantic argu-
ments. This set of dependency paths were deemed
as possible positions in the initial vector space rep-
resentation. In addition, akin to the first context
function, we also added all dependency labels to
the context set. Thus for this context function, the
block cardinality k was the sum of the number of
scanned gold dependency path types and the num-
ber of dependency labels. Given a predicate in its
sentential context, we therefore extract only those
context words that appear in positions warranted
by the above set. See Figure 2 for an illustration
of this process.
We performed initial experiments using con-
text extracted from 1) direct dependents, 2) de-
pendency paths, and 3) both. For all our experi-
ments, setting 3) which concatenates the direct de-
pendents and dependency path always dominated
the other two, so we only report results for this
setting.
3.2 Learning
We model our objective function following We-
ston et al (2011), using a weighted approximate-
rank pairwise loss, learned with stochastic gradi-
ent descent. The mapping from g(x) to the low
dimensional space R
m
is a linear transformation,
so the model parameters to be learnt are the matrix
M ? R
nk?m
as well as the embedding of each
possible frame label, represented as another ma-
trix Y ? R
F?m
where there are F frames in total.
The training objective function minimizes:
?
x
?
y?
L
(
rank
y
(x)
)
max(0, ?+s(x, y)?s(x, y?)).
1451
where x, y are the training inputs and their cor-
responding correct frames, and y? are negative
frames, ? is the margin. Here, rank
y
(x) is the
rank of the positive frame y relative to all the neg-
ative frames:
rank
y
(x) =
?
y?
I(s(x, y) ? ? + s(x, y?)),
and L(?) converts the rank to a weight. Choos-
ing L(?) = C? for any positive constant C opti-
mizes the mean rank, whereas a weighting such as
L(?) =
?
?
i=1
1/i (adopted here) optimizes the
top of the ranked list, as described in (Usunier
et al, 2009). To train with such an objective,
stochastic gradient is employed. For speed the
computation of rank
y
(x) is then replaced with a
sampled approximation: sample N items y? until
a violation is found, i.e. max(0, ? + s(x, y?) ?
s(x, y))) > 0 and then approximate the rank with
(F ? 1)/N , see Weston et al (2011) for more
details on this procedure. For the choices of the
stochastic gradient learning rate, margin (?) and
dimensionality (m), please refer to ?5.4-?5.5.
Note that an alternative approach could learn
only the matrixM , and then use a k-nearest neigh-
bor classifier in R
m
, as in Weinberger and Saul
(2009). The advantage of learning an embedding
for the frame labels is that at inference time we
need to consider only the set of labels for classi-
fication rather than all training examples. Addi-
tionally, since we use a frame lexicon that gives
us the possible frames for a given predicate, we
usually only consider a handful of candidate la-
bels. If we used all training examples for a given
predicate for finding a nearest-neighbor match at
inference time, we would have to consider many
more candidates, making the process very slow.
4 Argument Identification
Here, we briefly describe the argument identifi-
cation model used in our frame-semantic parsing
experiments, post frame identification. Given x,
the sentence with a marked predicate, the argu-
ment identification model assumes that the pred-
icate frame y has been disambiguated. From a
frame lexicon, we look up the set of semantic roles
R
y
that associate with y. This set alo contains the
null role r
?
. From x, a rule-based candidate argu-
ment extraction algorithm extracts a set of spans
A that could potentially serve as the overt
7
argu-
7
By overtness, we mean the non-null instantiation of a
semantic role in a frame-semantic parse.
? starting word of a ? POS of the starting word of a
? ending word of a ? POS of the ending word of a
? head word of a ? POS of the head word of a
? bag of words in a ? bag of POS tags in a
? a bias feature ? voice of the predicate use
? word cluster of a?s head
? word cluster of a?s head conjoined with word cluster
of the predicate
?
? dependency path between a?s head and the predicate
? the set of dependency labels of the predicate?s children
? dependency path conjoined with the POS tag of a?s
head
? dependency path conjoined with the word cluster of
a?s head
? position of a with respect to the predicate (before, after,
overlap or identical)
? whether the subject of the predicate is missing (miss-
ingsubj)
? missingsubj, conjoined with the dependency path
? missingsubj, conjoined with the dependency path from
the verb dominating the predicate to a?s head
Table 1: Argument identification features. The span in con-
sideration is termed a. Every feature in this list has two ver-
sions, one conjoined with the given role r and the other con-
joined with both r and the frame y. The feature with a
?
su-
perscript is only conjoined with the role to reduce its sparsity.
mentsA
y
for y (see ?5.4-?5.5 for the details of the
candidate argument extraction algorithms).
Learning Given training data of the form
??x
(i)
, y
(i)
,M
(i)
??
N
i=1
, where,
M = {(r, a} : r ? R
y
, a ? A ?A
y
}, (1)
a set of tuples that associates each role r in R
y
with a span a according to the gold data. Note that
this mapping associates spans with the null role r
?
as well. We optimize the following log-likelihood
to train our model:
max
?
N
?
i=1
|M
(i)
|
?
j=1
log p?
(
(r, a)
j
|x, y,R
y
)
? C???
2
2
where p? is a log-linear model normalized over the
set R
y
, with features described in Table 1. We
set C = 1.0 and use L-BFGS (Liu and Nocedal,
1989) for training.
Inference Although our learning mechanism
uses a local log-linear model, we perform infer-
ence globally on a per-frame basis by applying
hard structural constraints. Following Das et al
(2014) and Punyakanok et al (2008) we use the
log-probability of the local classifiers as a score in
an integer linear program (ILP) to assign roles sub-
ject to hard constraints described in ?5.4 and ?5.5.
We use an off-the-shelf ILP solver for inference.
1452
5 Experiments
In this section, we present our experiments and
the results achieved. We evaluate our novel frame
identification approach in isolation and also con-
joined with argument identification resulting in
full frame-semantic structures; before presenting
our model?s performance we first focus on the
datasets, baselines and the experimental setup.
5.1 Data
We evaluate our models on both FrameNet- and
PropBank-style structures. For FrameNet, we use
the full-text annotations in the FrameNet 1.5 re-
lease
8
which was used by Das et al (2014, ?3.2).
We used the same test set as Das et al contain-
ing 23 documents with 4,458 predicates. Of the
remaining 55 documents, 16 documents were ran-
domly chosen for development.
9
For experiments with PropBank, we used the
Ontonotes corpus (Hovy et al, 2006), version 4.0,
and only made use of the Wall Street Journal doc-
uments; we used sections 2-21 for training, sec-
tion 24 for development and section 23 for testing.
This resembles the setup used by Punyakanok et
al. (2008). All the verb frame files in Ontonotes
were used for creating our frame lexicon.
5.2 Frame Identification Baselines
For comparison, we implemented a set of baseline
models, with varying feature configurations. The
baselines use a log-linear model that models the
following probability at training time:
p(y|x, `) =
e
??f(y,x,`)
?
y??F
`
e
??f(y?,x,`)
(2)
At test time, this model chooses the best frame as
argmax
y
? ? f(y, x, `) where argmax iterates over
the possible frames y ? F
`
if ` was seen in the
lexicon or the training data, or y ? F , if it was un-
seen, like the disambiguation scheme of ?3. We
train this model by maximizing L
2
regularized
log-likelihood, using L-BFGS; the regularization
constant was set to 0.1 in all experiments.
For comparison with our model from ?3, which
we call WSABIE EMBEDDING, we implemented two
baselines with the log-linear model. Both the
baselines use features very similar to the input rep-
resentations described in ?3.1. The first one com-
putes the direct dependents and dependency paths
8
See https://framenet.icsi.berkeley.edu.
9
These documents are listed in appendix A.
as described in ?3.1 but conjoins them with the
word identity rather than a word embedding. Ad-
ditionally, this model uses the un-conjoined words
as backoff features. This would be a standard NLP
approach for the frame identification problem, but
is surprisingly competitive with the state of the art.
We call this baseline LOG-LINEAR WORDS. The sec-
ond baseline, tries to decouple the WSABIE training
from the embedding input, and trains a log linear
model using the embeddings. So the second base-
line has the same input representation as WSABIE
EMBEDDING but uses a log-linear model instead of
WSABIE. We call this model LOG-LINEAR EMBED-
DING.
5.3 Common Experimental Setup
We process our PropBank and FrameNet training,
development and test corpora with a shift-reduce
dependency parser that uses the Stanford conven-
tions (de Marneffe and Manning, 2013) and uses
an arc-eager transition system with beam size of 8;
the parser and its features are described by Zhang
and Nivre (2011). Before parsing the data, it is
tagged with a POS tagger trained with a condi-
tional random field (Lafferty et al, 2001) with the
following emission features: word, the word clus-
ter, word suffixes of length 1, 2 and 3, capitaliza-
tion, whether it has a hyphen, digit and punctua-
tion. Beyond the bias transition feature, we have
two cluster features for the left and right words in
the transition. We use Brown clusters learned us-
ing the algorithm of Uszkoreit and Brants (2008)
on a large English newswire corpus for cluster fea-
tures. We use the same word clusters for the argu-
ment identification features in Table 1.
We learn the initial embedding representations
for our frame identification model (?3) using a
deep neural language model similar to the one pro-
posed by Bengio et al (2003). We use 3 hidden
layers each with 1024 neurons and learn a 128-
dimensional embedding from a large corpus con-
taining over 100 billion tokens. In order to speed
up learning, we use an unnormalized output layer
and a hinge-loss objective. The objective tries to
ensure that the correct word scores higher than a
random incorrect word, and we train with mini-
batch stochastic gradient descent.
5.4 Experimental Setup for FrameNet
Hyperparameters For our frame identification
model with embeddings, we search for the WSA-
BIE hyperparameters using the development data.
1453
SEMAFOR LEXICON FULL LEXICON
Development Data
Model All Ambiguous Rare All Ambiguous Rare
LOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07
LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70
WSABIE EMBEDDING (?3) 96.90 92.73 96.44 96.99 93.12 96.39
SEMAFOR LEXICON FULL LEXICON
Model All Ambiguous Rare Unseen All Ambiguous Rare
Test Data
Das et al (2014) supervised 82.97 69.27 80.97 23.08
Das et al (2014) best 83.60 69.19 82.31 42.67
LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10
LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03
WSABIE EMBEDDING (?3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38
Table 2: Frame identification results for FrameNet. See ?5.6.
SEMAFOR LEXICON FULL LEXICON
Model Precision Recall F
1
Precision Recall F
1
Development Data
LOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19
WSABIE EMBEDDING (?3) 89.89 76.40 82.59 89.94 76.27 82.54
Test Data
Das et al supervised 67.81 60.68 64.05
Das et al best 68.33 61.14 64.54
LOG-LINEAR WORDS 71.16 63.56 67.15 73.35 65.27 69.08
WSABIE EMBEDDING (?3) 72.79 64.95 68.64 74.44 66.17 70.06
Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We
skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
We search for the stochastic gradient learning
rate in {0.0001, 0.001, 0.01}, the margin ? ?
{0.001, 0.01, 0.1, 1} and the dimensionality of the
final vector space m ? {256, 512}, to maximize
the frame identification accuracy of ambiguous
lexical units; by ambiguous, we imply lexical units
that appear in the training data or the lexicon with
more than one semantic frame. The underlined
values are the chosen hyperparameters used to an-
alyze the test data.
Argument Candidates The candidate argument
extraction method used for the FrameNet data, (as
mentioned in ?4) was adapted from the algorithm
of Xue and Palmer (2004) applied to dependency
trees. Since the original algorithm was designed
for verbs, we added a few extra rules to handle
non-verbal predicates: we added 1) the predicate
itself as a candidate argument, 2) the span ranging
from the sentence position to the right of the pred-
icate to the rightmost index of the subtree headed
by the predicate?s head; this helped capture cases
like ?a few months? (where few is the predicate and
months is the argument), and 3) the span ranging
from the leftmost index of the subtree headed by
the predicate?s head to the position immediately
before the predicate, for cases like ?your gift to
Goodwill? (where to is the predicate and your gift
is the argument).
10
10
Note that Das et al (2014) describe the state of the art
in FrameNet-based analysis, but their argument identifica-
tion strategy considered all possible dependency subtrees in
Frame Lexicon In our experimental setup, we
scanned the XML files in the ?frames? directory
of the FrameNet 1.5 release, which lists all the
frames, the corresponding roles and the associ-
ated lexical units, and created a frame lexicon to
be used in our frame and argument identification
models. We noted that this renders every lexical
unit as seen; in other words, at frame disambigua-
tion time on our test set, for all instances, we only
had to score the frames in F
`
for a predicate with
lexical unit ` (see ?3 and ?5.2). We call this setup
FULL LEXICON. While comparing with prior state
of the art on the same corpus, we noted that Das et
al. (2014) found several unseen predicates at test
time.
11
For fair comparison, we took the lexical
units for the predicates that Das et al considered
as seen, and constructed a lexicon with only those;
training instances, if any, for the unseen predicates
under Das et al?s setup were thrown out as well.
We call this setup SEMAFOR LEXICON.
12
We also
experimented on the set of unseen instances used
by Das et al
ILP constraints For FrameNet, we used three
ILP constraints during argument identification
(?4). 1) each span could have only one role, 2)
each core role could be present only once, and 3)
all overt arguments had to be non-overlapping.
a parse, resulting in a much larger search space.
11
Instead of using the frame files, Das et al built a frame
lexicon from FrameNet?s exemplars and the training corpus.
12
We got Das et al?s seen predicates from the authors.
1454
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.21 90.54 93.33
LOG-LINEAR EMBEDDING 93.81 89.86 93.73
WSABIE EMBEDDING (?3) 94.79 91.52 92.55
Dev data ? ? Test data
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.74 92.07 91.32
LOG-LINEAR EMBEDDING 94.04 90.95 90.97
WSABIE EMBEDDING (?3) 94.56 91.82 90.62
Table 4: Frame identification accuracy results for PropBank.
The model and the column names have the same semantics
as Table 2.
Model P R F
1
LOG-LINEAR WORDS 80.02 75.58 77.74
WSABIE EMBEDDING (?3) 80.06 75.74 77.84
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 81.55 77.83 79.65
WSABIE EMBEDDING (?3) 81.32 77.97 79.61
Table 5: Full frame-structure prediction results for Propbank.
This is a metric that takes into account frames and arguments
together. See ?5.7 for more details.
5.5 Experimental Setup for PropBank
Hyperparameters As in ?5.4, we made a hyper-
parameter sweep in the same space. The chosen
learning rate was 0.01, while the other values were
? = 0.01 and m = 512. Ambiguous lexical units
were used for this selection process.
Argument Candidates For PropBank we use
the algorithm of Xue and Palmer (2004) applied
to dependency trees.
Frame Lexicon For the PropBank experiments
we scanned the frame files for propositions in
Ontonotes 4.0, and stored possible core roles for
each verb frame. The lexical units were simply
the verb associating with the verb frames. There
were no unseen verbs at test time.
ILP constraints We used the constraints of Pun-
yakanok et al (2008).
5.6 FrameNet Results
Table 2 presents accuracy results on frame iden-
tification.
13
We present results on all predicates,
ambiguous predicates seen in the lexicon or the
training data, and rare ambiguous predicates that
appear ? 11 times in the training data. The WS-
ABIE EMBEDDING model from ?3 performs signif-
icantly better than the LOG-LINEAR WORDS base-
line, while LOG-LINEAR EMBEDDING underperforms
in every metric. For the SEMAFOR LEXICON setup,
we also compare with the state of the art from Das
13
We do not report partial frame accuracy that has been
reported by prior work.
Model P R F
1
LOG-LINEAR WORDS 77.29 71.50 74.28
WSABIE EMBEDDING (?3) 77.13 71.32 74.11
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 79.47 75.11 77.23
WSABIE EMBEDDING (?3) 79.36 75.04 77.14
Punyakanok et al Collins 75.92 71.45 73.62
Punyakanok et al Charniak 77.09 75.51 76.29
Punyakanok et al Combined 80.53 76.94 78.69
Table 6: Argument only evaluation (semantic role labeling
metrics) using the CoNLL 2005 shared task evaluation script
(Carreras and M`arquez, 2005). Results from Punyakanok et
al. (2008) are taken from Table 11 of that paper.
et al (2014), who used a semi-supervised learn-
ing method to improve upon a supervised latent-
variable log-linear model. For unseen predicates
from the Das et al system, we perform better as
well. Finally, for the FULL LEXICON setting, the ab-
solute accuracy numbers are even better for our
best model. Table 3 presents results on the full
frame-semantic parsing task (measured by a reim-
plementation of the SemEval 2007 shared task
evaluation script) when our argument identifica-
tion model (?4) is used after frame identification.
We notice similar trends as in Table 2, and our re-
sults outperform the previously published best re-
sults, setting a new state of the art.
5.7 PropBank Results
Table 4 shows frame identification results on the
PropBank data. On the development set, our best
model performs with the highest accuracy on all
and ambiguous predicates, but performs worse on
rare ambiguous predicates. On the test set, the
LOG-LINEAR WORDS baseline performs best by a
very narrow margin. See ?6 for a discussion.
Table 5 presents results where we measure pre-
cision, recall and F
1
for frames and arguments to-
gether; this strict metric penalizes arguments for
mismatched frames, like in Table 3. We see the
same trend as in Table 4. Finally, Table 6 presents
SRL results that measures argument performance
only, irrespective of the frame; we use the eval-
uation script from CoNLL 2005 (Carreras and
M`arquez, 2005). We note that with a better frame
identification model, our performance on SRL im-
proves in general. Here, too, the embedding model
barely misses the performance of the best baseline,
but we are at par and sometimes better than the sin-
gle parser setting of a state-of-the-art SRL system
(Punyakanok et al, 2008).
14
14
The last row of Table 6 refers to a system which used the
1455
6 Discussion
For FrameNet, the WSABIE EMBEDDING model we
propose strongly outperforms the baselines on all
metrics, and sets a new state of the art. We be-
lieve that the WSABIE EMBEDDING model performs
better than the LOG-LINEAR EMBEDDING baseline
(that uses the same input representation) because
the former setting allows examples with differ-
ent labels and confusion sets to share informa-
tion; this is due to the fact that all labels live in
the same label space, and a single projection ma-
trix is shared across the examples to map the input
features to this space. Consequently, the WSABIE
EMBEDDING model can share more information be-
tween different examples in the training data than
the LOG-LINEAR EMBEDDING model. Since the LOG-
LINEAR WORDS model always performs better than
the LOG-LINEAR EMBEDDING model, we conclude
that the primary benefit does not come from the
input embedding representation.
15
On the PropBank data, we see that the LOG-
LINEAR WORDS baseline has roughly the same per-
formance as our model on most metrics: slightly
better on the test data and slightly worse on the
development data. This can be partially explained
with the significantly larger training set size for
PropBank, making features based on words more
useful. Another important distinction between
PropBank and FrameNet is that the latter shares
frames between multiple lexical units. The ef-
fect of this is clearly observable from the ?Rare?
column in Table 4. WSABIE EMBEDDING performs
poorly in this setting while LOG-LINEAR EMBEDDING
performs well. Part of the explanation has to do
with the specifics of WSABIE training. Recall that
the WSABIE EMBEDDING model needs to estimate
the label location in R
m
for each frame. In other
words, it must estimate 512 parameters based on
at most 10 training examples. However, since the
input representation is shared across all frames,
every other training example from all the lexical
units affects the optimal estimate, since they all
modify the joint parameter matrixM . By contrast,
in the log-linear models each label has its own
set of parameters, and they interact only via the
normalization constant. The LOG-LINEAR WORDS
model does not have this entanglement, but cannot
share information between words. For PropBank,
combination of two syntactic parsers as input.
15
One could imagine training a WSABIE model with word
features, but we did not perform this experiment.
these drawbacks and benefits balance out and we
see similar performance for LOG-LINEAR WORDS
and LOG-LINEAR EMBEDDING. For FrameNet, esti-
mating the label embedding is not as much of a
problem because even if a lexical unit is rare, the
potential frames can be frequent. For example, we
might have seen the SENDING frame many times,
even though telex.V is a rare lexical unit.
In comparison to prior work on FrameNet, even
our baseline models outperform the previous state
of the art. A particularly interesting comparison is
between our LOG-LINEAR WORDS baseline and the
supervised model of Das et al (2014). They also
use a log-linear model, but they incorporate a la-
tent variable that uses WordNet (Fellbaum, 1998)
to get lexical-semantic relationships and smooths
over frames for ambiguous lexical units. It is
possible that this reduces the model?s power and
causes it to over-generalize. Another difference is
that when training the log-linear model, they nor-
malize over all frames, while we normalize over
the allowed frames for the current lexical unit.
This would tend to encourage their model to ex-
pend more of its modeling power to rule out pos-
sibilities that will be pruned out at test time.
7 Conclusion
We have presented a simple model that outper-
forms the prior state of the art on FrameNet-
style frame-semantic parsing, and performs at par
with one of the previous-best single-parser sys-
tems on PropBank SRL. Unlike Das et al (2014),
our model does not rely on heuristics to con-
struct a similarity graph and leverage WordNet;
hence, in principle it is generalizable to varying
domains, and to other languages. Finally, we pre-
sented results on PropBank-style semantic role la-
beling with a system that included the task of au-
tomatic verb frame identification, in tune with the
FrameNet literature; we believe that such a sys-
tem produces more interpretable output, both from
the perspective of human understanding as well as
downstream applications, than pipelines that are
oblivious to the verb frame, only focusing on ar-
gument analysis.
Acknowledgments
We thank Emily Pitler for comments on an early
draft, and the anonymous reviewers for their valu-
able feedback.
1456
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceed-
ings of NAACL-HLT.
D. Das, D. Chen, A. F. T. Martins, N. Schneider, and
N. A. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics, 40(1):9?56.
M.-C. de Marneffe and C. D. Manning, 2013. Stanford
typed dependencies manual.
C. Fellbaum, editor. 1998. WordNet: an electronic
lexical database.
C. J. Fillmore, C. R. Johnson, and M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3):235?250.
C. J. Fillmore. 1982. Frame Semantics. In Linguis-
tics in the Morning Calm, pages 111?137. Hanshin
Publishing Co., Seoul, South Korea.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90 In Pro-
ceedings of NAACL-HLT.
R. Johansson and P. Nugues. 2007. LTH: semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503 ? 528.
L. M`arquez, X. Carreras, K. C. Litkowski, and
S. Stevenson. 2008. Semantic role labeling: an in-
troduction to the special issue. Computational Lin-
guistics, 34(2):145?159.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In Pro-
ceedings of NAACL/HLT Workshop on Frontiers in
Corpus Annotation.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
HLT.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL,
Stroudsburg, PA, USA.
N. Usunier, D. Buffoni, and P. Gallinari. 2009. Rank-
ing with ordered weighted pairwise classification. In
ICML.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. In Proceedings of
ACL-HLT.
K. Q. Weinberger and L. K. Saul. 2009. Distance met-
ric learning for large margin nearest neighbor clas-
sification. Journal of Machine Learning Research,
10:207?244.
J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie:
Scaling up to large vocabulary image annotation. In
Proceedings of IJCAI.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP
2004.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT.
1457
Number Filename
dev-1 LUCorpus-v0.3 20000420 xin eng-NEW.xml
dev-2 NTI SouthAfrica Introduction.xml
dev-3 LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xml
dev-4 LUCorpus-v0.3 AFGP-2002-600045-Trans.xml
dev-5 PropBank TicketSplitting.xml
dev-6 Miscellaneous Hijack.xml
dev-7 LUCorpus-v0.3 artb 004 A1 E1 NEW.xml
dev-8 NTI WMDNews 042106.xml
dev-9 C-4 C-4Text.xml
dev-10 ANC EntrepreneurAsMadonna.xml
dev-11 NTI LibyaCountry1.xml
dev-12 NTI NorthKorea NuclearOverview.xml
dev-13 LUCorpus-v0.3 20000424 nyt-NEW.xml
dev-14 NTI WMDNews 062606.xml
dev-15 ANC 110CYL070.xml
dev-16 LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xml
Table 7: List of files used as development set for the FrameNet 1.5 corpus.
A Development Data
Table 7 features a list of the 16 randomly selected
documents from the FrameNet 1.5 corpus, which
we used for development. The resultant develop-
ment set consists of roughly 4,500 predicates. We
use the same test set as in Das et al (2014), con-
taining 23 documents and 4,458 predicates.
1458
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224?229,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Bilingual Word Representations by Marginalizing Alignments
Tom
?
a
?
s Ko
?
cisk?y Karl Moritz Hermann
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{tomas.kocisky,karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
Phil Blunsom
Abstract
We present a probabilistic model that si-
multaneously learns alignments and dis-
tributed representations for bilingual data.
By marginalizing over word alignments
the model captures a larger semantic con-
text than prior work relying on hard align-
ments. The advantage of this approach is
demonstrated in a cross-lingual classifica-
tion task, where we outperform the prior
published state of the art.
1 Introduction
Distributed representations have become an in-
creasingly important tool in machine learning.
Such representations?typically continuous vec-
tors learned in an unsupervised setting?can fre-
quently be used in place of hand-crafted, and thus
expensive, features. By providing a richer rep-
resentation than what can be encoded in discrete
settings, distributed representations have been suc-
cessfully used in many areas. This includes AI and
reinforcement learning (Mnih et al, 2013), image
retrieval (Kiros et al, 2013), language modelling
(Bengio et al, 2003), sentiment analysis (Socher
et al, 2011; Hermann and Blunsom, 2013), frame-
semantic parsing (Hermann et al, 2014), and doc-
ument classification (Klementiev et al, 2012).
In Natural Language Processing (NLP), the use
of distributed representations is motivated by the
idea that they could capture semantics and/or syn-
tax, as well as encoding a continuous notion of
similarity, thereby enabling information sharing
between similar words and other units. The suc-
cess of distributed approaches to a number of
tasks, such as listed above, supports this notion
and its implied benefits (see also Turian et al
(2010) and Collobert and Weston (2008)).
While most work employing distributed repre-
sentations has focused on monolingual tasks, mul-
tilingual representations would also be useful for
several NLP-related tasks. Such problems include
document classification, machine translation, and
cross-lingual information retrieval, where multi-
lingual data is frequently the norm. Furthermore,
learning multilingual representations can also be
useful for cross-lingual information transfer, that
is exploiting resource-fortunate languages to gen-
erate supervised data in resource-poor ones.
We propose a probabilistic model that simulta-
neously learns word alignments and bilingual dis-
tributed word representations. As opposed to pre-
vious work in this field, which has relied on hard
alignments or bilingual lexica (Klementiev et al,
2012; Mikolov et al, 2013), we marginalize out
the alignments, thus capturing more bilingual se-
mantic context. Further, this results in our dis-
tributed word alignment (DWA) model being the
first probabilistic account of bilingual word repre-
sentations. This is desirable as it allows better rea-
soning about the derived representations and fur-
thermore, makes the model suitable for inclusion
in higher-level tasks such as machine translation.
The contributions of this paper are as follows.
We present a new probabilistic similarity measure
which is based on an alignment model and prior
language modeling work which learns and relates
word representations across languages. Subse-
quently, we apply these embeddings to a standard
document classification task and show that they
outperform the current published state of the art
(Hermann and Blunsom, 2014b). As a by-product
we develop a distributed version of FASTALIGN
(Dyer et al, 2013), which performs on par with
the original model, thereby demonstrating the ef-
ficacy of the learned bilingual representations.
2 Background
The IBM alignment models, introduced by Brown
et al (1993), form the basis of most statistical ma-
chine translation systems. In this paper we base
our alignment model on FASTALIGN (FA), a vari-
224
ation of IBM model 2 introduced by Dyer et al
(2013). This model is both fast and produces
alignments on par with the state of the art. Further,
to induce the distributed representations we incor-
porate ideas from the log-bilinear language model
presented by Mnih and Hinton (2007).
2.1 IBM Model 2
Given a parallel corpus with aligned sentences, an
alignment model can be used to discover matching
words and phrases across languages. Such mod-
els are an integral part of most machine translation
pipelines. An alignment model learns p(f ,a|e) (or
p(e,a
?
|f)) for the source and target sentences e
and f (sequences of words). a represents the word
alignment across these two sentences from source
to target. IBM model 2 (Brown et al, 1993) learns
alignment and translation probabilities in a gener-
ative style as follows:
p(f ,a|e) = p(J |I)
J
?
j=1
p(a
j
|j, I, J) p
(
f
j
|e
a
j
)
,
where p(J |I) captures the two sentence lengths;
p(a
j
|j, I, J) the alignment and p
(
f
j
|e
a
j
)
the
translation probability. Sentence likelihood is
given by marginalizing out the alignments, which
results in the following equation:
p(f |e) = p(J |I)
J
?
j=1
I
?
i=0
p(i|j, I, J) p(f
j
|e
i
) .
We use FASTALIGN (FA) (Dyer et al, 2013), a
log-linear reparametrization of IBM model 2. This
model uses an alignment distribution defined by
a single parameter that measures how close the
alignment is to the diagonal. This replaces the
original multinomial alignment distribution which
often suffered from sparse counts. This improved
model was shown to run an order of magnitude
faster than IBM model 4 and yet still outperformed
it in terms of the BLEU score and, on Chinese-
English data, in alignment error rate (AER).
2.2 Log-Bilinear Language Model
Language models assign a probability measure
to sequences of words. We use the log-bilinear
language model proposed by Mnih and Hinton
(2007). It is an n-gram based model defined in
terms of an energy function E(w
n
;w
1:n?1
). The
probability for predicting the next word w
n
given
its preceding context of n ? 1 words is expressed
using the energy function
E(w
n
;w
1:n?1
)=?
(
n?1
?
i=1
r
T
w
i
C
i
)
r
w
n
?b
T
r
r
w
n
?b
w
n
as p(w
n
|w
1:n?1
) =
1
Z
c
exp (?E(w
n
;w
1:n?1
))
where Z
c
=
?
w
n
exp (?E(w
n
;w
1:n?1
)) is the
normalizer, r
w
i
? R
d
are word representations,
C
i
? R
d?d
are context transformation matrices,
and b
r
? R
d
, b
w
n
? R are representation and word
biases respectively. Here, the sum of the trans-
formed context-word vectors endeavors to be close
to the word we want to predict, since the likelihood
in the model is maximized when the energy of the
observed data is minimized.
This model can be considered a variant of a
log-linear language model in which, instead of
defining binary n-gram features, the model learns
the features of the input and output words, and
a transformation between them. This provides a
vastly more compact parameterization of a lan-
guage model as n-gram features are not stored.
2.3 Multilingual Representation Learning
There is some recent prior work on multilin-
gual distributed representation learning. Simi-
lar to the model presented here, Klementiev et
al. (2012) and Zou et al (2013) learn bilingual
embeddings using word alignments. These two
models are non-probabilistic and conditioned on
the output of a separate alignment model, un-
like our model, which defines a probability dis-
tribution over translations and marginalizes over
all alignments. These models are also highly re-
lated to prior work on bilingual lexicon induc-
tion (Haghighi et al, 2008). Other recent ap-
proaches include Sarath Chandar et al (2013),
Lauly et al (2013) and Hermann and Blunsom
(2014a, 2014b). These models avoid word align-
ment by transferring information across languages
using a composed sentence-level representation.
While all of these approaches are related to the
model proposed in this paper, it is important to
note that our approach is novel by providing a
probabilistic account of these word embeddings.
Further, we learn word alignments and simultane-
ously use these alignments to guide the represen-
tation learning, which could be advantageous par-
ticularly for rare tokens, where a sentence based
approach might fail to transfer information.
Related work also includes Mikolov et al
(2013), who learn a transformation matrix to
225
reconcile monolingual embedding spaces, in an
l
2
norm sense, using dictionary entries instead of
alignments, as well as Schwenk et al (2007) and
Schwenk (2012), who also use distributed repre-
sentations for estimating translation probabilities.
Faruqui and Dyer (2014) use a technique based on
CCA and alignments to project monolingual word
representations to a common vector space.
3 Model
Here we describe our distributed word alignment
(DWA) model. The DWA model can be viewed
as a distributed extension of the FA model in that
it uses a similarity measure over distributed word
representations instead of the standard multino-
mial translation probability employed by FA. We
do this using a modified version of the log-bilinear
language model in place of the translation proba-
bilities p(f
j
|e
i
) at the heart of the FA model. This
allows us to learn word representations for both
languages, a translation matrix relating these vec-
tor spaces, as well as alignments at the same time.
Our modifications to the log-bilinear model are
as follows. Where the original log-bilinear lan-
guage model uses context words to predict the next
word?this is simply the distributed extension of
an n-gram language model?we use a word from
the source language in a parallel sentence to pre-
dict a target word. An additional aspect of our
model, which demonstrates its flexibility, is that it
is simple to include further context from the source
sentence, such as words around the aligned word
or syntactic and semantic annotations. In this pa-
per we experiment with a transformed sum over
k context words to each side of the aligned source
word. We evaluate different context sizes and re-
port the results in Section 5. We define the energy
function for the translation probabilities to be
E(f, e
i
) = ?
(
k
?
s=?k
r
T
e
i+s
T
s
)
r
f
?b
T
r
r
f
?b
f
(1)
where r
e
i
, r
f
? R
d
are vector representations for
source and target words e
i+s
? V
E
, f ? V
F
in
their respective vocabularies, T
s
? R
d?d
is the
transformation matrix for each surrounding con-
text position, b
r
? R
d
are the representation bi-
ases, and b
f
? R is a bias for each word f ? V
F
.
The translation probability is given by
p(f |e
i
) =
1
Z
e
i
exp (?E(f, e
i
)) , where
Z
e
i
=
?
f
exp (?E(f, e
i
)) is the normalizer.
In addition to these translation probabilities, we
have parameterized the translation probabilities
for the null word using a softmax over an addi-
tional weight vector.
3.1 Class Factorization
We improve training performance using a class
factorization strategy (Morin and Bengio, 2005)
as follows. We augment the translation probabil-
ity to be p(f |e) = p(c
f
|e) p(f |c
f
, e) where c
f
is a unique predetermined class of f ; the class
probability is modeled using a similar log-bilinear
model as above, but instead of predicting a word
representation r
f
we predict the class representa-
tion r
c
f
(which is learned with the model) and we
add respective new context matrices and biases.
Note that the probability of the word f depends
on both the class and the given context words: it is
normalized only over words in the class c
f
.
In our training we create classes based on word
frequencies in the corpus as follows. Considering
words in the order of their decreasing frequency,
we add word types into a class until the total fre-
quency of the word types in the currently consid-
ered class is less than
total tokens
?
|V
F
|
and the class size is
less than
?
|V
F
|. We have found that the maximal
class size affects the speed the most.
4 Learning
The original FA model optimizes the likelihood
using the expectation maximization (EM) algo-
rithm where, in the M-step, the parameter update
is analytically solvable, except for the ? parameter
(the diagonal tension), which is optimized using
gradient descent (Dyer et al, 2013). We modified
the implementations provided with CDEC (Dyer et
al., 2010), retaining its default parameters.
In our model, DWA, we optimize the likelihood
using the EM as well. However, while training we
fix the counts of the E-step to those computed by
FA, trained for the default 5 iterations, to aid the
convergence rate, and optimize the M-step only.
Let ? be the parameters for our model. Then the
gradient for each sentence is given by
?
??
log p(f |e) =
J
?
k=1
I
?
l=0
[
p(l|k, I, J) p(f
k
|e
l
)
?
I
i=0
p(i|k, I, J) p(f
k
|e
i
)
?
?
??
log(p(l|k, I, J) p(f
k
|e
l
))
]
226
where the first part are the counts from the FA
model and second part comes from our model.
We compute the gradient for the alignment
probabilities in the same way as in the FA model,
and the gradient for the translation probabilities
using back-propagation (Rumelhart et al, 1986).
For parameter update, we use ADAGRAD as the
gradient descent algorithm (Duchi et al, 2011).
5 Experiments
We first evaluate the alignment error rate of our
approach, which establishes the model?s ability to
both learn alignments as well as word representa-
tions that explain these alignments. Next, we use
a cross-lingual document classification task to ver-
ify that the representations are semantically useful.
We also inspect the embedding space qualitatively
to get some insight into the learned structure.
5.1 Alignment Evaluation
We compare the alignments learned here with
those of the FASTALIGN model which produces
very good alignments and translation BLEU
scores. We use the same language pairs and
datasets as in Dyer et al (2013), that is the FBIS
Chinese-English corpus, and the French-English
section of the Europarl corpus (Koehn, 2005). We
used the preprocessing tools from CDEC and fur-
ther replaced all unique tokens with UNK. We
trained our models with 100 dimensional repre-
sentations for up to 40 iterations, and the FA
model for 5 iterations as is the default.
Table 1 shows that our model learns alignments
on part with those of the FA model. This is in line
with expectation as our model was trained using
the FA expectations. However, it confirms that
the learned word representations are able to ex-
plain translation probabilities. Surprisingly, con-
text seems to have little impact on the alignment
error, suggesting that the model receives sufficient
information from the aligned words themselves.
5.2 Document Classification
A standard task for evaluating cross-lingual word
representations is document classification where
training is performed in one and evaluation in an-
other language. This tasks require semantically
plausible embeddings (for classification) which
are valid across two languages (for the semantic
transfer). Hence this task requires more of the
word embeddings than the previous task.
Languages Model
FA DWA DWA
k = 0 k = 3
ZH|EN 49.4 48.4 48.7
EN|ZH 44.9 45.3 45.9
FR|EN 17.1 17.2 17.0
EN|FR 16.6 16.3 16.1
Table 1: Alignment error rate (AER) compar-
ison, in both directions, between the FASTAL-
IGN (FA) alignment model and our model (DWA)
with k context words (see Equation 1). Lower
numbers indicate better performance.
We mainly follow the setup of Klementiev et al
(2012) and use the German-English parallel cor-
pus of the European Parliament proceedings to
train the word representations. We perform the
classification task on the Reuters RCV1/2 corpus.
Unlike Klementiev et al (2012), we do not use that
corpus during the representation learning phase.
We remove all words occurring less than five times
in the data and learn 40 dimensional word embed-
dings in line with prior work.
To train a classifier on English data and test it
on German documents we first project word rep-
resentations from English into German: we select
the most probable German word according to the
learned translation probabilities, and then compute
document representations by averaging the word
representations in each document. We use these
projected representations for training and subse-
quently test using the original German data and
representations. We use an averaged perceptron
classifier as in prior work, with the number of
epochs (3) tuned on a subset of the training set.
Table 2 shows baselines from previous work
and classification accuracies. Our model outper-
forms the model by Klementiev et al (2012), and
it also outperforms the most comparable models
by Hermann and Blunsom (2014b) when training
on German data and performs on par with it when
training on English data.
1
It seems that our model
learns more informative representations towards
document classification, even without additional
monolingual language models or context informa-
tion. Again the impact of context is inconclusive.
1
From Hermann and Blunsom (2014a, 2014b) we only
compare with models equivalent with respect to embedding
dimensionality and training data. They still achieve the state
of the art when using additional training data.
227
Model en? de de? en
Majority class 46.8 46.8
Glossed 65.1 68.6
MT 68.1 67.4
Klementiev et al 77.6 71.1
BiCVM ADD 83.7 71.4
BiCVM BI 83.4 69.2
DWA (k = 0) 82.8 76.0
DWA (k = 3) 83.1 75.4
Table 2: Document classification accuracy when
trained on 1,000 training examples of the RCV1/2
corpus (train?test). Baselines are the majority
class, glossed, and MT (Klementiev et al, 2012).
Further, we are comparing to Klementiev et al
(2012), BiCVM ADD (Hermann and Blunsom,
2014a), and BiCVM BI (Hermann and Blunsom,
2014b). k is the context size, see Equation 1.
5.3 Representation Visualization
Following the document classification task we
want to gain further insight into the types of fea-
tures our embeddings learn. For this we visu-
alize word representations using t-SNE projec-
tions (van der Maaten and Hinton, 2008). Fig-
ure 1 shows an extract from our projection of the
2,000 most frequent German words, together with
an expected representation of a translated English
word given translation probabilities. Here, it is
interesting to see that the model is able to learn
related representations for words chair and rat-
spr?asidentschaft (presidency) even though these
words were not aligned by our model. Figure 2
shows an extract from the visualization of the
10,000 most frequent English words trained on an-
other corpus. Here again, it is evident that the em-
beddings are semantically plausible with similar
words being closely aligned.
6 Conclusion
We presented a new probabilistic model for learn-
ing bilingual word representations. This dis-
tributed word alignment model (DWA) learns both
representations and alignments at the same time.
We have shown that the DWA model is able
to learn alignments on par with the FASTALIGN
alignment model which produces very good align-
ments, thereby determining the efficacy of the
learned representations which are used to calculate
Figure 1: A visualization of the expected represen-
tation of the translated English word chair among
the nearest German words: words never aligned
(green), and those seen aligned (blue) with it.
Figure 2: A cluster of English words from the
10,000 most frequent English words visualized us-
ing t-SNE. Word representations were optimized
for p(zh|en) (k = 0).
word translation probabilities for the alignment
task. Subsequently, we have demonstrated that
our model can effectively be used to project doc-
uments from one language to another. The word
representations our model learns as part of the
alignment process are semantically plausible and
useful. We highlighted this by applying these em-
beddings to a cross-lingual document classifica-
tion task where we outperform prior work, achieve
results on par with the current state of the art and
provide new state-of-the-art results on one of the
tasks. Having provided a probabilistic account of
word representations across multiple languages,
future work will focus on applying this model to
machine translation and related tasks, for which
previous approaches of learning such embeddings
are less suited. Another avenue for further study
is to combine this method with monolingual lan-
guage models, particularly in the context of se-
mantic transfer into resource-poor languages.
Acknowledgements
This work was supported by a Xerox Foundation
Award and EPSRC grant number EP/K036580/1.
We acknowledge the use of the Oxford ARC.
228
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155, February.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311, June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL-
HLT.
Manaal Faruqui and Chris Dyer. 2014. Improving Vec-
tor Space Word Representations Using Multilingual
Correlation. In Proceedings of EACL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
HLT.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Compo-
sitional Semantics. In Proceedings of ACL.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of ICLR.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of ACL.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of ACL.
Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdi-
nov. 2013. Multimodal neural language models. In
NIPS Deep Learning Workshop.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2013. Learning multilingual word representations
using a bag-of-words autoencoder. In NIPS Deep
Learning Workshop.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. CoRR, abs/1309.4168.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. In NIPS Deep Learn-
ing Workshop.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics, pages 246?252.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Learning representations by back-
propagating errors. Nature, 323:533?536, October.
A P Sarath Chandar, M Khapra Mitesh, B Ravindran,
Vikas Raykar, and Amrita Saha. 2013. Multilingual
deep learning. In Deep Learning Workshop at NIPS.
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram trans-
lation. In Proceedings of EMNLP-CoNLL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of COLING: Posters.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visual-
izing high-dimensional data using t-sne. Journal of
Machine Learning Research, 9:2579?2605.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. In
Proceedings of EMNLP.
229
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 8,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
New Directions in Vector Space Models of Meaning
Phil Blunsom, Edward Grefenstette
and Karl Moritz Hermann
?
University of Oxford
first.last@cs.ox.ac.uk
Georgiana Dinu
Center for Mind/Brain Sciences
University of Trento
georgiana.dinu@unitn.it
1 Abstract
Symbolic approaches have dominated NLP as a
means to model syntactic and semantic aspects of
natural language. While powerful inferential tools
exist for such models, they suffer from an inabil-
ity to capture correlation between words and to
provide a continuous model for word, phrase, and
document similarity. Distributed representations
are one mechanism to overcome these constraints.
This tutorial will supply NLP researchers with
the mathematical and conceptual background to
make use of vector-based models of meaning in
their own research. We will begin by motivating
the need for a transition from symbolic represen-
tations to distributed ones. We will briefly cover
how collocational (distributional) vectors can be
used and manipulated to model word meaning. We
will discuss the progress from distributional to dis-
tributed representations, and how neural networks
allow us to learn word vectors and condition them
on metadata such as parallel texts, topic labels, or
sentiment labels. Finally, we will present various
forms of semantic vector composition, and discuss
their relative strengths and weaknesses, and their
application to problems such as language mod-
elling, paraphrasing, machine translation and doc-
ument classification.
This tutorial aims to bring researchers up to
speed with recent developments in this fast-
moving field. It aims to strike a balance be-
tween providing a general introduction to vector-
based models of meaning, an analysis of diverg-
ing strands of research in the field, and also being
a hands-on tutorial to equip NLP researchers with
the necessary tools and background knowledge to
start working on such models. Attendees should
be comfortable with basic probability, linear alge-
bra, and continuous mathematics. No substantial
knowledge of machine learning is required.
?
Instructors listed in alphabetical order.
2 Outline
1. Motivation: Meaning in space
2. Learning distributional models for words
3. Neural language modelling and distributed
representations
(a) Neural language model fundamentals
(b) Recurrent neural language models
(c) Conditional neural language models
4. Semantic composition in vector spaces
(a) Algebraic and tensor-based composition
(b) The role of non-linearities
(c) Learning recursive neural models
(d) Convolutional maps and composition
3 Instructors
Phil Blunsom is an Associate Professor at the
University of Oxford?s Department of Computer
Science. His research centres on the probabilistic
modelling of natural languages, with a particular
interest in automating the discovery of structure
and meaning in text.
Georgiana Dinu is a postdoctoral researcher
at the University of Trento. Her research re-
volves around distributional semantics with a fo-
cus on compositionality within the distributional
paradigm.
Edward Grefenstette is a postdoctoral researcher
at Oxford?s Department of Computer Science. He
works on the relation between vector represen-
tations of language meaning and structured logi-
cal reasoning. His work in this area was recently
recognised by a best paper award at *SEM 2013.
Karl Moritz Hermann is a final-year DPhil stu-
dent at the Department of Computer Science in
Oxford. His research studies distributed and com-
positional semantics, with a particular emphasis
on mechanisms to reduce task-specific and mono-
lingual syntactic bias in such representations.
8
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70?74,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Learning Semantics and Selectional Preference of Adjective-Noun Pairs
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
karl.moritz.hermann@cs.ox.ac.uk
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cdyer@cs.cmu.edu
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
phil.blunsom@cs.ox.ac.uk
Stephen Pulman
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
stephen.pulman@cs.ox.ac.uk
Abstract
We investigate the semantic relationship be-
tween a noun and its adjectival modifiers.
We introduce a class of probabilistic mod-
els that enable us to to simultaneously cap-
ture both the semantic similarity of nouns
and modifiers, and adjective-noun selectional
preference. Through a combination of novel
and existing evaluations we test the degree to
which adjective-noun relationships can be cat-
egorised. We analyse the effect of lexical con-
text on these relationships, and the efficacy of
the latent semantic representation for disam-
biguating word meaning.
1 Introduction
Developing models of the meanings of words and
phrases is a key challenge for computational linguis-
tics. Distributed representations are useful in captur-
ing such meaning for individual words (Sato et al,
2008; Maas and Ng, 2010; Curran, 2005). How-
ever, finding a compelling account of semantic com-
positionality that utilises such representations has
proven more difficult and is an active research topic
(Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). It is in
this area that our paper makes its contribution.
The dominant approaches to distributional se-
mantics have relied on relatively simple frequency
counting techniques. However, such approaches fail
to generalise to the much sparser distributions en-
countered when modeling compositional processes
and provide no account of selectional preference.
We propose a probabilistic model of the semantic
representations for nouns and modifiers. The foun-
dation of this model is a latent variable representa-
tion of noun and adjective semantics together with
their compositional probabilities. We employ this
formulation to give a dual view of noun-modifier
semantics: the induced latent variables provide an
explicit account of selectional preference while the
marginal distributions of the latent variables for each
word implicitly produce a distributed representation.
Most related work on selectional preference uses
class-based probabilities to approximate (sparse)
individual probabilities. Relevant papers include
O? Se?aghdha (2010), who evaluates several topic
models adapted to learning selectional preference
using co-occurence and Baroni and Zamparelli
(2010), who represent nouns as vectors and adjec-
tives as matrices, thus treating them as functions
over noun meaning. Again, inference is achieved
using co-occurrence and dimensionality reduction.
2 Adjective-Noun Model
We hypothesize that semantic classes determine the
semantic characteristics of nouns and adjectives, and
that the distribution of either with respect to other
components of the sentences they occur in is also
mediated by these classes (i.e., not by the words
themselves). We assume that in general nouns select
for adjectives,1 and that this selection is dependent
on both their latent semantic classes. In the next sec-
tion, we describe a model encoding our hypotheses.
2.1 Generative Process
We model a corpus D of tuples of the form
(n,m, c1 . . . ck) consisting of a noun n, an adjective
m (modifier), and k words of context. The context
variables (c1 . . . ck) are treated as a bag of words and
1We evaluate this hypothesis as well as its inverse.
70
|N| |M|
|N|
N M
n m
c
k
|D|
?
N
?
N
?
M
?
M
?
c
?
c
|N|
?
n
?
m
?
n
?
m
Figure 1: Plate diagram illustrating our model of noun
and modifier semantic classes (designated N and M , re-
spectively), a modifier-noun pair (m,n), and its context.
include the words to the left and right of the noun,
its siblings and governing verbs. We designate the
vocabulary Vn for nouns, Vm for modifiers and Vc
for context. We use zi to refer to the ith tuple in D
and refer to variables within that tuple by subscript-
ing them with i, e.g., ni and c3,i are the noun and
the third context variable of zi. The latent noun and
adjective class variables are designated Ni and Mi.
The corpus D is generated according to the plate
diagram in figure 1. First, a set of parameters is
drawn. A multinomial ?N representing the distribu-
tion of noun semantic classes in the corpus is drawn
from a Dirichlet distribution with parameter ?N. For
each noun class i we have distributions ?Mi over
adjective classes, ?ni over Vn and ?
c
i over Vc, also
drawn from Dirichlet distributions. Finally, for each
adjective class j, we have distributions ?mj over Vm.
Next, the contents of the corpus are generated by
first drawing the length of the corpus (we do not
parametrise this since we never generate from this
model). Then, for each i, we generate noun class
Ni, adjective class Mi, and the tuple zi as follows:
Ni | ?
N ? Multi(?N)
Mi | ?
M
Ni? Multi(?
M
Ni)
ni | ?
n
Ni? Multi(?
n
Ni)
mi | ?
m
Mi? Multi(?
m
Mi)
?k: ck,i | ?
c
Ni? Multi(?
c
Ni)
2.2 Parameterization and Inference
We use Gibbs sampling to estimate the distributions
ofN andM , integrating out the multinomial param-
eters ?x (Griffiths and Steyvers, 2004). The Dirich-
let parameters ? are drawn independently from a
?(1, 1) distribution, and are resampled using slice
sampling at frequent intervals throughout the sam-
pling process (Johnson and Goldwater, 2009). This
?vague? prior encourages sparse draws from the
Dirichlet distribution. The number of noun and ad-
jective classes N and M was set to 50 each; other
sizes (100,150) did not significantly alter results.
3 Experiments
As our model was developed on the basis of several
hypotheses, we design the experiments and evalu-
ation so that these hypotheses can be examined on
their individual merit. We test the first hypothesis,
that nouns and adjectives can be represented by se-
mantic classes, recoverable using co-occurence, us-
ing a sense clustering evaluation by Ciaramita and
Johnson (2003). The second hypothesis, that the dis-
tribution with respect to context and to each other is
governed by these semantic classes is evaluated us-
ing pseudo-disambiguation (Clark and Weir, 2002;
Pereira et al, 1993; Rooth et al, 1999) and bigram
plausibility (Keller and Lapata, 2003) tests.
To test whether noun classes indeed select for ad-
jective classes, we also evaluate an inverse model
(Modi), where the adjective class is drawn first, in
turn generating both context and the noun class. In
addition, we evaluate copies of both models ignoring
context (Modnc and Modinc).
We use the British National Corpus (BNC), train-
ing on 90 percent and testing on 10 percent of the
corpus. Results are reported after 2,000 iterations
including a burn-in period of 200 iterations. Classes
are marginalised over every 10th iteration.
4 Evaluation
4.1 Supersense Tagging
Supersense tagging (Ciaramita and Johnson, 2003;
Curran, 2005) evaluates a model?s ability to clus-
ter words by their semantics. The task of this eval-
uation is to determine the WORDNET supersenses
of a given list of nouns. We report results on the
WN1.6 test set as defined by Ciaramita and John-
son (2003), who used 755 randomly selected nouns
with a unique supersense from the WORDNET 1.6
71
corpus. As their test set was random, results weren?t
exactly replicable. For a fair comparison, we select
all suitable nouns from the corpus that also appeared
in the training corpus. We report results on type and
token level (52314 tokens with 1119 types). The
baseline2 chooses the most common supersense.
k Token Type
Baseline .241 .210
Ciaramita & Johnson .523 .534
Curran - .680
Mod 10 .592 .517
Modnc 10 .473 .410
Table 1: Supersense evaluation results. Values are the
percentage of correctly assigned supersenses. k indicates
the number of nearest neighbours considered.
We use cosine-similarity on the marginal noun
class vectors to measure distance between nouns.
Each noun in the test set is then assigned a su-
persense by performing a distance-weighted voting
among its k nearest neighbours. Results of this eval-
uation are shown in Table 1, with Figure 2 showing
scores for model Mod across different values for k.
Figure 2: Scores of Mod on the supersense task. The up-
per line denotes token-, the lower type-level scores. The
y-axis is the percentage of correct assignments, the x-axis
denotes the number of neighbours included in the vote.
The results demonstrate that nouns can semanti-
cally be represented as members of latent classes,
while the superiority of Mod over Modnc supports
our hypothesis that context co-occurence is a key
feature for learning these classes.
4.2 Pseudo-Disambiguation
Pseudo-disambiguation was introduced by Clark
and Weir (2002) to evaluate models of selectional
preference. The task is to select the more probable
of two candidate arguments to associate with a given
2The baseline results are from Ciaramita and Johnson
(2003). Using the majority baseline on the full test set, we only
get .176 and .160 for token and type respectively.
predicate. For us, this is to decide which adjective,
a1 or a2, is more likely to modify a noun n.
We follow the approach by Clark and Weir (2002)
to create the test data. To improve the quality of
the data, we filtered using bigram counts from the
Web1T corpus, setting a lower bound on the proba-
ble bigram (a1, n) and chosing a2 from five candi-
dates, picking the lowest count for bigram (a2, n).
We report results for all variants of our model in
Table 2. As baseline we use unigram counts in our
training data, chosing the more frequent adjective.
L-bound 0 100 500 1000
Size 5714 5253 3741 2789
Baseline .543 .543 .539 .550
Mod .783 .792 .810 .816
Modi .781 .787 .800 .810
Modnc .720 .728 .746 .750
Modinc .722 .730 .747 .752
Table 2: Pseudo-disambiguation: Percentage of correct
choices made. L-bound denotes the Web1T lower bound
on the (a1, n) bigram, size the number of decisions made.
While all models decisively beat the baseline, the
models using context strongly outperform those that
do not. This supports our hypothesis regarding the
importance of context in semantic clustering.
The similarity between the normal and inverse
models implies that the direction of the noun-
adjective relationship has negligible impact for this
evaluation.
4.3 Bigram Plausibility
Bigram plausibility (Keller and Lapata, 2003) is a
second evaluation for selectional preference. Unlike
the frequency-based pseudo-disambiguation task, it
evaluates how well a model matches human judge-
ment of the plausibility of adjective-noun pairs.
Keller and Lapata (2003) demonstrated a correlation
between frequencies and plausibility, but this does
not sufficiently explain human judgement. An ex-
ample taken from their unseen data set illustrates the
dissociation between frequency and plausibility:
? Frequent, implausible: ?educational water?
? Infrequent, plausible: ?difficult foreigner?3
The plausibility evaluation has two data sets of 90
adjective-noun pairs each. The first set (seen) con-
tains random bigrams from the BNC. The second set
(unseen) are bigrams not contained in the BNC.
3At the time of writing, Google estimates 56,900 hits for
?educational water? and 575 hits for ?difficult foreigner?. ?Ed-
ucational water? ranks bottom in the gold standard of the unseen
set, ?difficult foreigner? ranks in the top ten.
72
Recent work (O? Se?aghdha, 2010; Erk et al,
2010) approximated plausibility with joint probabil-
ity (JP). We believe that for semantic plausibility
(not probability!) mutual information (MI), which
factors out acutal frequencies, is a better metric.4 We
report results using JP, MI and MI?2.
Seen Unseen
r ? r ?
AltaVista .650 ? .480 ?
BNC (Rasp) .543 .622 .135 .102
Pado? et al .479 .570 .120 .138
LDA .594 .558 .468 .459
ROOTH-LDA .575 .599 .501 .469
DUAL-LDA .460 .400 .334 .278
Mod (JP) .495 .413 .286 .276
Mod (MI) .394 .425 .471 .457
Mod (MI?2) .575 .501 .430 .408
Modnc (JP) .626 .505 .357 .369
Modnc (MI) .628 .574 .427 .385
Modnc (MI?2) .701 .623 .423 .394
Table 3: Results (Pearson r and Spearman ? correlations)
on the Keller and Lapata (2003) plausibility data. Bold
indicates best scores, underlining our best scores. High
values indicate high correlation with the gold standard.
Table 3 shows the performance of our models
compared to results reported in O? Se?aghdha (2010).
As before, results between the normal and the in-
verse model (omitted due to space) are very simi-
lar. Surprisingly, the no-context models consistently
outperform the models using context on the seen
data set. This suggests that the seen data set can
quite precisely be ranked using frequency estimates,
which the no-context models might be better at cap-
turing without the ?noise? introduced by context.
Standard Inverse (i)
r ? r ?
Mod (JP) .286 .276 .243 .245
Mod (MI) .471 .457 .409 .383
Mod (MI?2) .430 .408 .362 .347
Modnc (JP) .357 .369 .181 .161
Modnc (MI) .427 .385 .220 .209
Modnc (MI?2) .423 .394 .218 .185
Table 4: Results on the unseen plausibility dataset.
The results on the unseen data set (Table 4)
prove interesting as well. The inverse no-context
model is performing significantly poorer than any
of the other models. To understand this result we
must investigate the differences between the unseen
data set and the seen data set and to the pseudo-
disambiguation evaluation. The key difference to
pseudo-disambiguation is that we measure a human
4See (Evert, 2005) for a discussion of these metrics.
plausibility judgement, which ? as we have demon-
strated ? only partially correlates with bigram fre-
quencies. Our models were trained on the BNC,
hence they could only learn frequency estimates for
the seen data set, but not for the unseen data.
Based on our hypothesis about the role of con-
text, we expect Mod and Modi to learn semantic
classes based on the distribution of context. Without
the access to that context, we argued thatModnc and
Modinc would instead learn frequency estimates.5
The hypothesis that nouns generally select for ad-
jectives rather than vice versa further suggests that
Mod and Modnc would learn semantic properties
that Modi and Modinc could not learn so well.
In summary, we hence expected Mod to perform
best on the unseen data, learning semantics from
both context and noun-adjective selection. Also, as
supported by the results, we expected Modinc to
performs poorly, as it is the model least capable of
learning semantics according to our hypotheses.
5 Conclusion
We have presented a class of probabilistic mod-
els which successfully learn semantic clusterings of
nouns and a representation of adjective-noun selec-
tional preference. These models encoded our beliefs
about how adjective-noun pairs relate to each other
and to the other words in the sentence. The perfor-
mance of our models on estimating selectional pref-
erence strongly supported these initial hypotheses.
We discussed plausibility judgements from a the-
oretical perspective and argued that frequency esti-
mates and JP are imperfect approximations for plau-
sibility. While models can perform well on some
evaluations by using either frequency estimates or
semantic knowledge, we explained why this does
not apply to the unseen plausibility test. The perfor-
mance on that task demonstrates both the success of
our model and the shortcomings of frequency-based
approaches to human plausibility judgements.
Finally, this paper demonstrated that it is feasi-
ble to learn semantic representations of words while
concurrently learning how they relate to one another.
Future work will explore learning words from
broader classes of semantic relations and the role of
context in greater detail. Also, we will evaluate the
system applied to higher level tasks.
5This could also explain their weaker performance on
pseudo-disambiguation in the previous section, where the neg-
ative examples had zero frequency in the training corpus.
73
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 168?175, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Comput.
Linguist., 28:187?206, June.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?05, pages 26?33, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36:723?763.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1394?1404,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 317?325, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, pages 459?484.
Andrew L. Maas and Andrew Y. Ng. 2010. A probabilis-
tic model for semantic word vectors. In Workshop on
Deep Learning and Unsupervised Feature Learning,
NIPS ?10.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL-HLT?08,
pages 236 ? 244.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 435?444, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st annual meeting on Association for
Computational Linguistics, ACL ?93, pages 183?190,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th annual meeting of the Association
for Computational Linguistics on Computational Lin-
guistics, ACL ?99, pages 104?111, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa.
2008. Knowledge discovery of semantic relationships
between words using nonparametric bayesian graph
model. In Proceeding of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ?08, pages 587?595, New York,
NY, USA. ACM.
74
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 132?141,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Unsupervised Ranking Model for Noun-Noun Compositionality
Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman
Department of Computer Science
University of Oxford
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom,stephen.pulman}@cs.ox.ac.uk
Abstract
We propose an unsupervised system that
learns continuous degrees of lexicality for
noun-noun compounds, beating a strong base-
line on several tasks. We demonstrate that the
distributional representations of compounds
and their parts can be used to learn a fine-
grained representation of semantic contribu-
tion. Finally, we argue such a representation
captures compositionality better than the cur-
rent status-quo which treats compositionality
as a binary classification problem.
1 Introduction
A Multiword Expressions (MWE) can be defined as
a sequence of words whose meaning cannot nec-
essarily be derived from the meaning of the words
making up that sequence, for example:
Rat Race ? self-defeating or pointless pursuit1
MWEs are considered a ?key problem for the de-
velopment of large-scale, linguistically sound nat-
ural language processing technology? (Sag et al,
2002). The challenge posed by MWEs is three-
fold, consisting of MWE identification, classifica-
tion and interpretation. Following the identification
of a MWE, it needs to be established whether the
expression should be treated as lexical (idiomatic)
or as compositional. The final step, learning the se-
mantics of the MWE, strongly depends on this deci-
sion.
1Definition taken from Wikipedia, and clearly not recover-
able if one only knows the meaning of the words ?rat? and ?race?.
The problem posed by MWEs is considered hard,
but at the same time it is highly relevant and inter-
esting. MWEs occur frequently in language and in-
terpreting them correctly would directly improve re-
sults in a number of tasks in NLP such as translation
and parsing (Korkontzelos and Manandhar, 2010).
By extension this makes deciding the lexicality of
MWEs an important challenge for various fields in-
cluding machine translation, question answering and
information retrieval. In this paper we discuss com-
positionality with respect to noun-noun compounds.
Most Computational Linguistics literature treats
compositionality as a binary problem, classifying
compounds as either lexical or compositional. We
show that this approach is too simplistic and argue
for the real-valued treatment of compositionality.
We propose two unsupervised models that learn
compositionality rankings for compounds, placing
them on a scale between lexical and compositional
extremes. We develop a fine-grained representa-
tion of compositionality using a novel generative ap-
proach that models context as generated by com-
pound constituents. This representation differenti-
ates between the semantic contribution of both com-
pound constituents as well as the compound itself.
Comparing it with existing work in the field, we
demonstrate the competitiveness of our approach.
We evaluate on an existing corpus of noun com-
pounds with ranked compositionality data, as well
as on a large corpus with a binary annotation for lex-
ical and compositional compounds. We analyse the
impact of data sparsity and propose an interpolation
approximation which significantly reduces the effect
of sparsity on model performance.
132
2 Related Work
Interpreting MWEs is a difficult task as ?compound
nouns can be freely constructed? (Spa?rck Jones,
1985), and are thus able to proliferate infinitely. At
the same time, semantic composition can take many
different forms, making uniform interpretation of
compounds impossible (Zanzotto et al, 2010).
Most current work on MWEs focuses on inter-
preting compounds and sidesteps the task of deter-
mining whether a compound is compositional in the
first place (Butnariu et al, 2010; Kim and Baldwin,
2008). Such methods, aimed at learning the seman-
tics of compounds, can roughly be divided into two
major strands of research.
One group relies on data intensive methods to ex-
tract semantics vectors from large corpora (Baroni
and Zamparelli, 2010; Zanzotto et al, 2010; Gies-
brecht, 2009). The focus of these approaches is to
develop methods for composing the vectors of un-
igrams into a semantic vector representing a com-
pound. Some of the work in this area touches on the
issue of lexicality, as models learning distributional
representations of MWEs ideally would first estab-
lish whether a given MWE is compositional or not
(Mitchell and Lapata, 2010).
The other group are knowledge intensive ap-
proaches collecting linguistic features (Kim and
Baldwin, 2005; Korkontzelos and Manandhar,
2009). Tratz and Hovy (2010), for instance, train
a classifier for noun compound interpretation on a
large set of WORDNET and Thesaurus features.
Combined approaches include Kim and Baldwin
(2008), who interpret noun compounds by extrapo-
lating their semantics from observations where the
two nouns forming a compound are in an intransi-
tive relationship. For example extracting the phrase
?the family owns a car? from the training data would
help learn that the compound ?family car? describes
a POSSESSOR-OWNED/POSSESSED relationship.
Some of these supervised classifiers include lexi-
cality as a classification option, considering it jointly
with the actual compound interpretation.
Next to the work on MWE interpretation there has
been some work focused on determining lexicality
in its own right (Reddy et al, 2011; Bu et al, 2010;
Kim and Baldwin, 2007).
One possibility is to exploit special properties of
lexical MWEs such as high statistical association
of their constituents (Pedersen, 2011) or syntactic
rigidity (Fazly et al, 2009; McCarthy et al, 2007).
However, these approaches are limited in their ap-
plicability to compound nouns (Reddy et al, 2011).
Another method is to compare the semantics of
a compound and its constituents to decide com-
positionality. The approaches used to determine
those semantics can again be divided into knowl-
edge intensive and data-driven methods. Depending
on the chosen representation of semantics these ap-
proaches can either be used for supervised classifiers
or together with a distance metric comparing vector
space representations of semantics. In a binary set-
ting, a threshold would then be applied to the result
of that distance function (Korkontzelos and Man-
andhar, 2009). In a real-valued setting the distance
metric itself can be used as a measure for compo-
sitionality (Reddy et al, 2011). Related to the vec-
tor space based models, some research focuses on
improving the distance metrics used to compare in-
duced semantics (Bu et al, 2010).
3 Methodology
English noun-noun compounds are majority left-
branching (Lauer, 1995), with a head (the second
element), modified by an attributive noun (first el-
ement). For example:
Ground Floor ? The floor of a building at or near-
est ground level.2
In this paper, we will use the terms attributive noun
(AN) and head noun (HN) to refer to the first and
second noun in a noun compound.
3.1 Real-Valued Representation
Lexicality of MWEs is frequently treated as a bi-
nary property (Tratz and Hovy, 2010; O? Se?aghdha,
2007). We argue that lexicality should instead be
treated as a graded property, as most compound se-
mantics exhibit a mixture of compositional and lexi-
cal influences. For example, ?cocktail dress? derives
a large part of its semantics from ?dress?, but the
compound also contributes an idiosyncratic element
to its meaning.
2Definition from http://www.thefreedictionary.com
133
We define lexicality as the degree to which id-
iosyncrasy contributes to a compound?s semantics.
Inversely phrased, the compositionality of a com-
pound can be defined as the degree to which its sense
is related to the senses of its constituents.3
This graded representation follows Spa?rck Jones
(1985), who argued that ?it is not possible to main-
tain a principled distinction between lexicalised and
non-lexicalised compounds?. Some recent work
also supports this view (Reddy et al, 2011; Bu et
al., 2010; Baldwin, 2006). From a practical per-
spective, a real-valued representation of composi-
tionality should help improve interpretation of com-
pounds. This is especially true when factoring in the
respective semantic contributions of its parts.
3.2 Context Generation
According to the distributional hypothesis, the se-
mantics of a lexical item can be expressed by its
context. We apply this hypothesis to the problem of
noun compound compositionality by using a genera-
tive model on compound context. Our model allows
context to be generated by the compound itself or by
either one of its constituents. By learning which el-
ement of the compound generates which part of its
context we effectively determine the semantic con-
tribution of each element. This in turn gives us a
fine-grained, graded representation of a compound?s
lexicality.
4 Corpora for Evaluation
4.1 Ranked Corpus ? REDDY
As we want to evaluate our models? ability to learn
lexicality as a real-valued property, we require an
annotated data set of noun compounds ranked by
lexicality. To the best of our knowledge the only
such data set was developed by Reddy et al (2011).
This data set contains 90 distinct noun compounds
with real-valued gold standard scores ranking from
0 (lexical) to 5 (compositional). The compounds
are nearly linearly distributed across the [0;5] range,
with inter annotator agreement (Spearman?s ?) of
3For example, the meaning of ?gravy train? has hardly any
relation to either ?gravy? or ?train?. Its semantics are thus highly
dependent on the compound in its own right. On the other end
of the spectrum, ?climate change? is significantly related to both
?climate? and ?change?, contributing little inherent semantics to
its overall meaning.
0.522. We refer to this data set and evaluation as
REDDY throughout this paper.
4.2 Binary Corpora ? TRATZ
We also apply our models to a second, binary classi-
fication task. Tratz and Hovy (2010) compiled a data
set for noun compound interpretation, which classi-
fies noun compounds based on their internal struc-
ture. We use this corpus to extract lexical and com-
positional noun compounds.
After some pre-processing4 the data set contains
18,858 compositional and 118 lexical noun com-
pounds. We believe this to more accurately represent
the real world distribution of lexical and composi-
tional noun compounds: Tratz and Hovy (2010) ex-
tracted noun compounds from several large corpora
including the Wall Street Journal section of the Penn
Treebank, thus obtaining a reasonable approxima-
tion of real world occurrence. Other collections of
noun compounds (O? Se?aghdha, 2007) feature sim-
ilar proportions of lexical and compositional noun
compounds.
The large bias towards compositional noun com-
pounds does not support the status-quo of treating
compositionality as a binary property. As discussed
earlier, we assume that most compounds have a
compositional as well as a lexical element. While
the compositional aspect may be larger for most
compounds this alone does not suffice as a reason
to disregard the lexical element contained in these
compounds.
In order to evaluate our system on the TRATZ
data, we use receiving operator characteristic (ROC)
curves. ROC analysis enables us to evaluate a rank-
ing model without setting an artificial threshold for
the compositionality/lexicality decision.
5 Baseline Approach
We develop a set of advanced baselines related to
the semi-supervised models presented by Reddy et
al. (2011). We define the context K of a noun com-
pound as all words in all sentences the compound
appears in. From this we calculate distributional
representations of a compound (c = ?a, h?) and its
constituent elements a, h. We refer to these repre-
sentations as ~c for the compound and ~a, ~h for the
4We removed trigrams from the data set.
134
Name ? r ?
ADD w.Sac + (1? w).Shc .323 .567
MULT Sac.Shc .379 .551
MIN min(Sac, Shc) .343 .550
MAX max(Sac, Shc) .299 .505
COMB w1.Sac+w2.Shc+w3.Sac.Shc .366 .556
Table 1: Results of COSLEX with different operators on
the REDDY data set, reporting Pearson?s r and Spear-
man?s ? correlations. Weights for operators ADD (w =
0.3) and COMB (w = ?0.3, 0.1, 0.6?) are manually opti-
mised. Values range from -1 (negative correlation) to +1
(perfect correlation) with 0 describing random data.
attributive and head noun, respectively. We can cal-
culate the cosine similarity based lexicality score
(COSLEX) by combining the cosine similarity of the
compound?s distribution with each of its two con-
stituents (Reddy et al, 2011).
Sac = sim(~a,~c)
Shc = sim(~h,~c)
COSLEX(c) = Sac ? Shc
We evaluate a number of alternative operators ? for
combining Sac and Shc. Results for this baseline
on the REDDY corpus are in Table 1,5 with weights
wi on the combination operators manually optimised
for Spearman?s ? on that data set. In effect this
renders this baseline into a supervised approach, so
we would expect it to perform very well. We use
the best performing operators (ADD with w = 0.3,
MULT) as baselines for this paper.
6 Generative Models
We exploit the distributional hypothesis to model
the semantic contribution of the different elements
of a noun compound. For this, we require a sys-
tem that treats a noun compound as a vector of three
semantics-bearing units: the compound itself, its
head and its attributive noun. This system should
then model the relationship between the context of
the compound and these three units, deciding which
of them is responsible for each context element.
5Reddy et al (2011) report higher figures on our baseline
models. The differences are attributed to differences in training
data and parametrization.
6.1 3-way Compound Mixture
We model a corpus D of tuples d = {c, k1, ..., kn}.
Each tuple d contains a noun compound c = ?a, h?
and its context words K = (k1, ..., kn). We use vo-
cabularies Vc for noun compounds, Va for attributive
nouns, Vh for head nouns and Vk for context.
We condition our generative model on the noun
compounds. Given an observation d of a compound
c, we generate each context word in two steps. First,
we choose one of the compounds three elements6 to
generate the next context word. Second, we gener-
ate a new context word conditioned on that element.
Formally, the context is generated as follows.
We draw three multinomial parameters ?c, ?a
and ?h from Dirichlet distributions with parameters
?c, ?a and ?h. ?c represents the distribution over
context words Vk given compound c. ?a and ?h
are distributions over Vk given attributive noun a and
head noun h, respectively. These three distributions
form the mixture components of our model.
A fourth multinomial parameter ?z , drawn from
a Dirichlet distribution with parameter ?z , controls
the distribution over the mixture components. ?z is
specific to each compound c, so multiple observa-
tions of the same compound share this parameter.
For each context word we draw a mixture compo-
nent zc,i ? {c?, a?, h?} from the multinomial distribu-
tion with parameter ?z . zc,i determines which dis-
tribution the context word itself will be drawn from.
Finally, we draw the context word:
?i: ki | ?
{zc,i} ? Multi(?{zc,i})
Thus, for each observation of a compound noun we
have a vector zc = ?z1, ..., zn? detailing how its
context words were created either by the compound
itself or by one of its constituents. To determine lex-
icality, we are interested in learning the multinomial
parameter ?z , which describes to what extent the
compound and its constituents contribute to the gen-
eration of the context (i.e. semantics). We can ap-
proximate ?z from the vector zc.
We define the lexicality score Lex(c) for a com-
pound as the percentage of context words created by
6The compound itself, its attributive noun and its head noun
135
Figure 1: Plate diagram illustrating the MULT-CMPD
model with context words ki drawn from a mixture model
with three components controlled by zi.
the compound and not one of its constituents:
Lex(c) = p(z=c?|?a, h?), (1)
where c = ?a, h?
Figure 1 shows a plate diagram of this model, which
we will refer to as MULT-CMPD.
One hypothesis encoded in model MULT-CMPD
is that deciding which part of a compound (the com-
pound itself, the head or the attributive noun) gen-
erates context is a single decision. An alternative
representation could treat this as a two-step process,
which we encode in a second model BIN-CMPD.
The intuition behind the BIN-CMPD model is that
there are two distinct decisions. First, whether a
compound is compositional or not. Second, whether
(in the compositional case) its semantics stem from
its head or attributive noun
Where MULT-CMPD uses a three component mix-
ture to determine which multinomial distribution to
use, BIN-CMPD uses two cascaded binary mixtures
(see Figure 2). The BIN-CMPD model first chooses
whether to treat a compound as compositional or
lexical. If the compound is determined as composi-
tional, a second binary mixture determines whether
to generate a context word using the attributive (?a)
or head multinomial (?h). For the lexical case, the
model remains unchanged.
Figure 2: Schematic description of compositional-
ity/lexicality decision for models MULT-CMPD and BIN-
CMPD.
Model r ?
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
MULT-CMPD .141 .435
BIN-CMPD .168 .410
Table 2: Results on the REDDY data set, reporting Pear-
son?s r and Spearman?s ? correlations. Values range from
-1 (negative correlation) to +1 (perfect correlation).
6.1.1 Inference and Sampling
We use Gibbs sampling to learn the vectors z for
each instance d, integrating out the parameters ?x.
We train our models on the British National Corpus
(BNC), extracting all noun-noun compounds from a
parsed version of the corpus.
In order to speed up convergence of the sampler,
we use simulated annealing over the first 20 iter-
ations (Kirkpatrick et al, 1983), helping the ran-
domly initialised model reach a mode faster. We re-
port results using marginal distributions after a fur-
ther 130 iterations, excluding the counts of the an-
nealing stage.
6.1.2 Evaluation
We evaluate our two models on the REDDY data
set by comparing its scores for lexicality (Lex(c))
with the annotated gold standard. The aim of this
evaluation is to determine how accurately the mod-
els can capture gradual distinctions in lexicality. The
ROC analysis on the TRATZ data set furthermore in-
forms us how precise the models are at distinguish-
ing lexical from compositional compounds.
Results of the REDDY evaluation are in Table 2.
We use Spearman?s ? to measure the monotonic cor-
relation of our data to the gold standard. Pearson?s r
additionally captures the linear relationship between
the data, taking into account the relative differences
in Lex(c) scores among noun compounds.
136
Figure 3: ROC analysis of models MULT-CMPD and
BIN-CMPD versus the best COSLEX baseline (ADD) on
the TRATZ data set
While both models, BIN-CMPD and MULT-
CMPD, clearly learn a correlation with lexical-
ity rankings, they underperform the strong, semi-
supervised COSLEX baselines described earlier in
this paper. The second evaluation, on the binary
TRATZ data set shows a different picture (see Fig-
ure 3). The best COSLEX baseline (ADD with
w = 0.2) fails to outperform random choice on this
task. Both generative models clearly beat COSLEX
on this task, with MULT-CMPD in particular per-
forming very well for low sensitivity.
There is no clear distinction in performance be-
tween the two generative approaches. Further anal-
ysis might help us to separate the two more clearly,
and we will continue using both models throughout
this paper.
It is important to note the different performance of
the generative models vs. the cosine similarity ap-
proach on two tasks. The REDDY data set has a
nearly linear distribution of compositionality scores,
while the TRATZ data set is overwhelmingly com-
positional, which more closely represents the real
world distribution of compounds. The poor perfor-
mance of the cosine similarity approach (COSLEX)
on the TRATZ evaluation suggests the limitations
of this approach when applied to more realistic data
such as this data set. An additional explanation for
the semi-supervised baseline?s poorer result is that
the effect of parameter tuning decreases on larger
data.
Investigating the errors made by the models
MULT-CMPD and BIN-CMPD gives rise to a number
of possible explanations for their performance. The
most promising lead is related to data sparsity, with
many of the evaluated noun-noun compounds only
appearing once or twice in the corpus. This makes it
harder for our generative approach to learn sensible
context distributions for these instances.
We will next investigate how to reduce the effects
encountered by sparsity.
6.2 Interpolation
Working on problems related to non-unigram data,
sparsity is a frequently encountered problem. As al-
ready explored in the previous section, this is also
the case for our generative models of lexicality.
It would be possible to use an even larger training
corpus, but there are limitations as to what extent
this is possible. The BNC, containing 100 million
words, is already one of the largest corpora regu-
larly used in Computational Linguistics. However,
adding more data in an unsupervised sense is un-
likely to significantly improve results (Brants et al,
2007).
Alternatively, it would be possible to add spe-
cific training data that included the noun compounds
from the evaluation data sets. This would, how-
ever, compromise the unsupervised nature of our ap-
proach, and it thus not an option either.
In this paper, we will instead focus on extenuat-
ing the effects of data sparsity through other unsu-
pervised means. For this purpose we investigate in-
terpolating on a larger set of noun compounds.
Kim and Baldwin (2007) observed that seman-
tic similarity of verb-particle compounds correlates
with their lexicality. We extend this observation for
noun compounds, hypothesising that the lexicality
of similar words will be similar. We combine this
with the assumption that noun compounds sharing a
constituent are likely to be semantically similar (Ko-
rkontzelos and Manandhar, 2009).
Using this idea, we can approximate the lexical-
ity of a given compound with the lexicality scores of
all compounds sharing either of its constituents. So
far we have calculated the lexicality of a given com-
pound using the formula Lex(c) in Equation 1. The
formula Clex(c) in Equation 2 averages the lexical-
ity scores of a compound with those of its related
137
Function and Model r ?
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
Lex(c)
MULT-CMPD .141 .435
BIN-CMPD .168 .410
Clex(c)
MULT-CMPD .357 .596
BIN-CMPD .400 .592
Ilex(c)
MULT-CMPD .422 .621
BIN-CMPD .538 .623
Table 3: Results on the REDDY data set, reporting
Pearson?s r and Spearman?s ? correlations, comparing
Ilex(c) and Clec(c) interpolations with Lex(c).
compounds. As p(z=1|?a, h?) directly influences
both p(z=1|?a, ??) and p(z=1|??, h?), we can also
consider dropping it from the approximation such as
in Equation 3. This approach trades some specificity
in favour of reducing sparsity, as we observe more
instances of such related compounds than of a par-
ticular noun compound itself only.
Lex(c) ? Clex(c) (2)
Clex(c) =
p(z=1|?a, ??) + p(z=1|??, h?) + p(z=1|?a, h?)
3
,
where c = ?a, h?
Lex(c) ? Ilex(c) (3)
Ilex(c) =
p(z=1|?a, ??) + p(z=1|??, h?)
2
,
where c = ?a, h?
Both formulations enable us to better deal with
sparse data as decisions are made based on a wider
range of observations. At the same time, we avoid a
loss of specificity as the models and scores are still
highly dependent on the individual noun compound.
We avoid introducing additional degrees of free-
dom by using uniform weights only. However, it
would be simple to turn this approach into a semi-
supervised model by tuning the weights for the dif-
ferent probabilities involved in calculating Clex(c)
and Lex(c). That approach would be comparable to
the operators used on our COSLEX baselines.
Results on the REDDY data set using Clex(c)
and Ilex(c) are in Table 3. Figure 4 shows the im-
pact of these approximations on the Tratz data for
the BIN-CMPD model. These interpolations suggest
strong improvements in performance. It should es-
pecially be noted that Ilex(c) consistently outper-
forms Clex(c), which indicates the strength of the
Figure 4: ROC analysis of model BIN-CMPD on the
TRATZ data set, comparing Ilex(c) and Clec(c) inter-
polations with Lex(c).
related-compound probabilities over the individual
compound probabilities.
These results confirm our suspicion that sparsity
was a major factor affecting our models? perfor-
mance. Furthermore, they strengthen our hypothe-
sis about the relatedness of semantic similarity and
lexicality and demonstrate a sensible approach for
exploiting this relationship.
7 Analysis
We use this section for qualitative evaluation, com-
plementing the quantitative evaluation in the previ-
ous sections. The purpose of the qualitative evalu-
ation is to better understand exactly what it is our
models are learning.
Table 5 lists the compounds that model BIN-
CMPD considers the most lexical and the most com-
positional. The list of compounds with the high lex-
icality scores is dominated by proper nouns such as
countries, companies and persons. This is in line
with expectation as compounds of proper nouns are
fully lexical. Removing proper nouns (also in Table
5), we get a slightly more ambiguous list. For exam-
ple, ?study design? is not considered a lexical com-
pound, but rather a highly institutionalized, com-
positional MWE (Sag et al, 2002). Using Lex(c)
?study design? is ranked as such, so this appears to
be a case where interpolation has a negative impact.
In this paper we argued for a finer grained analysis
of compositionality, taking into account the differ-
138
Context of ?flea market? generated by
flea market flea market
canal, wall, incline,
campsite
stall, Paris, sale,
Saturday, week,
Sunday, quarter,
damage, change
barter, souvenir,
launderette,
Lamine, Canet,
Kouyate, Plage
Context of ?night owl? generated by
night owl night owl
court, fee, guest,
early, day, Baden,
membership, life,
game
waive, player,
Halikarnas, bar,
bird, unbooked,
Vienna
adventurous
Context of ?memory lane? generated by
memory lane memory lane
take, story, about,
tell, real, glimpse,
Britain, reminis-
cence
village, protection,
drive, catwalk,
plant
war, justify, bill,
Campbell, rude-
boys
Context of ?melting pot? generated by
melting pot melting pot
forest, racial,
caribbean, plan,
programme, real-
ity, arrangement
in, into, put, polit-
ical, community,
prepare
ethnic, greatest,
drawing, liaise,
pan-european,
myth
Table 4: Overview over context words generated by model BIN-CMPD. We list a selection of words predominately
generated by each of the mixture components of the given noun-noun compound.
Most Compositional
labour union, tax authority, health council,
market counterparty, employment policy
Most Lexical
study design, family motto, wood shaving,
avoidance behaviour, smash hit
Most Lexical (including Proper Nouns)
Vo Quy, Bonito Oliva, Mamur Zapt, Evander
Holyfield, Saudi Arabia
Table 5: Top lexical and compositional nouns for the
BIN-CMPD model using Ilex(c)
ent impact of both constituents. We tried to achieve
this by modelling a compound?s context as gener-
ated from its various semantic constituents. Table 4
highlights the impact of this method for a number
of noun compounds, showing which context words
were predominately generated by each constituent.
Due to the nature of the context used, some of
the links are semantically not obvious (e.g. the rela-
tionship between owls and Vienna). In some cases
the semantic contribution of the parts is more clearly
separated, such as the contributions of ?memory? and
?lane? to the semantics of ?memory lane?. In sum-
mary, these examples clearly suggest that our mod-
els learn to associate context with compound ele-
ments and that this association is an informed one.
8 Conclusion
We proposed a novel approach for learning lexicality
scores for noun compounds and empirically demon-
strated the feasiblity of this approach. Using a gen-
erative model we were able to beat a strong, semi-
supervised baseline with an unsupervised model.
We discussed the issue of data sparsity in depth
and proposed several approaches for overcoming
this problem. Focusing on unsupervised approaches,
we demonstrated how interpolation can be used to
tackle sparsity. The two interpolation methods that
we implemented helped us to strongly improve over-
all model performance. Our empirical evaluation of
interpolation metricsClex(c) and Ilex(c) also gives
credence to the hypothesis that lexicality is related to
semantic similarity.
On the theoretical side, we offered further support
to the real-valued treatment of lexicality.
Further work will include using larger training
corpora. While the BNC is a popular corpus in Com-
putational Linguistics, it proved to be too small to
learn sensible representations for a number of com-
pounds encountered in the test data. Using larger
corpora will also allow us to further study and re-
duce the sparsity issues encountered.
To study the relationship between constituent and
compound compositionality in greater depth, we
will also investigate alternative approaches for in-
terpolation. Similarity measures that consider the
semantic relevance of individual context elements
should also be considered as a next step.
Another obvious source of future work is to ap-
ply our approach to general collocations beyond the
special case of noun compounds only.
Acknowledgments
The authors would like to acknowledge the use of
the Oxford Supercomputing Centre (OSC) in carry-
ing out this work.
139
References
Timothy Baldwin. 2006. Compositionality and mul-
tiword expressions: Six of one, half a dozen of the
other? In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying
Properties, page 1, Sydney, Australia. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language Mod-
els in Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 116?
124, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O?. Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 39?
44, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Proceedings of the
17th International Conference on Conceptual Struc-
tures: Conceptual Structures: Leveraging Semantic
Technologies, ICCS ?09, pages 173?184, Berlin, Hei-
delberg. Springer-Verlag.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using wordnet simi-
larity. In In Proceedings of the 2nd International Joint
Conference on Natural Language Processing, Jeju Is-
land, South Korea, 1113, pages 945?956.
Su Nam Kim and Timothy Baldwin. 2007. Detect-
ing compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of the
7th Meeting of the Pacific Association for Computa-
tional Linguistics, PACLING ?07, pages 40?48.
Su Nam Kim and Timothy Baldwin. 2008. An unsu-
pervised approach to interpreting noun compounds. In
Natural Language Processing and Knowledge Engi-
neering, 2008. NLP-KE ?08. International Conference
on, pages 1?7.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983.
Optimization by simulated annealing. Science,
220(4598):671?680.
Ioannis Korkontzelos and Suresh Manandhar. 2009. De-
tecting compositionality in multi-word expressions.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, ACLShort ?09, pages 65?68, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 636?644, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL ?95, pages 47?54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379, Prague, Czech Republic. As-
sociation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Diarmuid O? Se?aghdha. 2007. Annotating and learning
compound noun semantics. In Proceedings of the 45th
Annual Meeting of the ACL: Student Research Work-
shop, ACL ?07, pages 73?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen. 2011. Identifying collocations to mea-
sure compositionality: shared task system description.
In Proceedings of the Workshop on Distributional Se-
mantics and Compositionality, DiSCo ?11, pages 33?
37, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
140
pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Chiang Mai, Thailand.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In In Proc.
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002, pages 1?15.
Karen Spa?rck Jones. 1985. Compound noun interpre-
tation problems. In Frank Fallside and William A.
Woods, editors, Computer speech processing, pages
363?381. Prentice Hall International (UK) Ltd., Hert-
fordshire, UK, UK.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 678?687, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10, pages 1263?1271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
141
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74?82,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
?Not not bad? is not ?bad?: A distributional account of negation
Karl Moritz Hermann Edward Grefenstette
University of Oxford Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, United Kingdom
firstname.lastname@cs.ox.ac.uk
Phil Blunsom
Abstract
With the increasing empirical success of
distributional models of compositional se-
mantics, it is timely to consider the types
of textual logic that such models are ca-
pable of capturing. In this paper, we ad-
dress shortcomings in the ability of cur-
rent models to capture logical operations
such as negation. As a solution we pro-
pose a tripartite formulation for a continu-
ous vector space representation of seman-
tics and subsequently use this representa-
tion to develop a formal compositional no-
tion of negation within such models.
1 Introduction
Distributional models of semantics characterize
the meanings of words as a function of the words
they co-occur with (Firth, 1957). These models,
mathematically instantiated as sets of vectors in
high dimensional vector spaces, have been applied
to tasks such as thesaurus extraction (Grefenstette,
1994; Curran, 2004), word-sense discrimination
(Schu?tze, 1998), automated essay marking (Lan-
dauer and Dumais, 1997), and so on.
During the past few years, research has shifted
from using distributional methods for modelling
the semantics of words to using them for mod-
elling the semantics of larger linguistic units such
as phrases or entire sentences. This move from
word to sentence has yielded models applied to
tasks such as paraphrase detection (Mitchell and
Lapata, 2008; Mitchell and Lapata, 2010; Grefen-
stette and Sadrzadeh, 2011; Blacoe and Lapata,
2012), sentiment analysis (Socher et al, 2012;
Hermann and Blunsom, 2013), and semantic re-
lation classification (ibid.). Most efforts approach
the problem of modelling phrase meaning through
vector composition using linear algebraic vector
operations (Mitchell and Lapata, 2008; Mitchell
and Lapata, 2010; Zanzotto et al, 2010), matrix
or tensor-based approaches (Baroni and Zampar-
elli, 2010; Coecke et al, 2010; Grefenstette et al,
2013; Kartsaklis et al, 2012), or through the use
of recursive auto-encoding (Socher et al, 2011;
Hermann and Blunsom, 2013) or neural-networks
(Socher et al, 2012). On the non-compositional
front, Erk and Pado? (2008) keep word vectors sep-
arate, using syntactic information from sentences
to disambiguate words in context; likewise Turney
(2012) treats the compositional aspect of phrases
and sentences as a matter of similarity measure
composition rather than vector composition.
These compositional distributional approaches
often portray themselves as attempts to recon-
cile the empirical aspects of distributional seman-
tics with the structured aspects of formal seman-
tics. However, they in fact only principally co-opt
the syntax-sensitivity of formal semantics, while
mostly eschewing the logical aspects.
Expressing the effect of logical operations in
high dimensional distributional semantic models
is a very different task than in boolean logic. For
example, whereas predicates such as ?red? are seen
in predicate calculi as functions mapping elements
of some set Mred to > (and all other domain ele-
ments to ?), in compositional distributional mod-
els we give the meaning of ?red? a vector-like
representation, and devise some combination op-
eration with noun representations to obtain the
representation for an adjective-noun pair. Under
the logical view, negation of a predicate therefore
yields a new truth-function mapping elements of
the complement of Mred to > (and all other do-
main elements to?), but the effect of negation and
other logical operations in distributional models is
not so sharp: we expect the representation for ?not
red? to remain close to other objects of the same
domain of discourse (i.e. other colours) while be-
ing sufficiently different from the representation of
?red? in some manner. Exactly how textual logic
74
would best be represented in a continuous vector
space model remains an open problem.
In this paper we propose one possible formu-
lation for a continuous vector space based repre-
sentation of semantics. We use this formulation
as the basis for providing an account of logical
operations for distributional models. In particu-
lar, we focus on the case of negation and how it
might work in higher dimensional distributional
models. Our formulation separates domain, value
and functional representation in such a way as to
allow negation to be handled naturally. We ex-
plain the linguistic and model-related impacts of
this mode of representation and discuss how this
approach could be generalised to other semantic
functions.
In Section 2, we provide an overview of work
relating to that presented in this paper, covering
the integration of logical elements in distributional
models, and the integration of distributional el-
ements in logical models. In Section 3, we in-
troduce and argue for a tripartite representation
in distributional semantics, and discuss the issues
relating to providing a linguistically sensible no-
tion of negation for such representations. In Sec-
tion 4, we present matrix-vector models similar to
that of Socher et al (2012) as a good candidate
for expressing this tripartite representation. We
argue for the elimination of non-linearities from
such models, and thus show that negation cannot
adequately be captured. In Section 5, we present
a short analysis of the limitation of these matrix-
vector models with regard to the task of modelling
non-boolean logical operations, and present an im-
proved model bypassing these limitations in Sec-
tion 6. Finally, in Section 7, we conclude by sug-
gesting future work which will extend and build
upon the theoretical foundations presented in this
paper.
2 Motivation and Related Work
The various approaches to combining logic with
distributional semantics can broadly be put into
three categories: those approaches which use
distributional models to enhance existing logical
tools; those which seek to replicate logic with the
mathematical constructs of distributional models;
and those which provide new mathematical defini-
tions of logical operations within distributional se-
mantics. The work presented in this paper is in the
third category, but in this section we will also pro-
vide a brief overview of related work in the other
two in order to better situate the work this paper
will describe in the literature.
Vector-assisted logic The first class of ap-
proaches seeks to use distributional models of
word semantics to enhance logic-based models of
textual inference. The work which best exempli-
fies this strand of research is found in the efforts of
Garrette et al (2011) and, more recently, Beltagy
et al (2013). This line of research converts logi-
cal representations obtained from syntactic parses
using Bos? Boxer (Bos, 2008) into Markov Logic
Networks (Richardson and Domingos, 2006), and
uses distributional semantics-based models such
as that of Erk and Pado? (2008) to deal with issues
polysemy and ambiguity.
As this class of approaches deals with improv-
ing logic-based models rather than giving a dis-
tributional account of logical function words, we
view such models as orthogonal to the effort pre-
sented in this paper.
Logic with vectors The second class of ap-
proaches seeks to integrate boolean-like logical
operations into distributional semantic models us-
ing existing mechanisms for representing and
composing semantic vectors. Coecke et al (2010)
postulate a mathematical framework generalising
the syntax-semantic passage of Montague Gram-
mar (Montague, 1974) to other forms of syntac-
tic and semantic representation. They show that
the parses yielded by syntactic calculi satisfying
certain structural constraints can be canonically
mapped to vector combination operations in dis-
tributional semantic models. They illustrate their
framework by demonstrating how the truth-value
of sentences can be obtained from the combina-
tion of vector representations of words and multi-
linear maps standing for logical predicates and re-
lations. They furthermore give a matrix interpre-
tation of negation as a ?swap? matrix which in-
verts the truth-value of vectorial sentence repre-
sentations, and show how it can be embedded in
sentence structure.
Recently, Grefenstette (2013) showed that the
examples from this framework could be extended
to model a full quantifier-free predicate logic using
tensors of rank 3 or lower. In parallel, Socher et
al. (2012) showed that propositional logic can be
learned using tensors of rank 2 or lower (i.e. only
matrices and vectors) through the use of non-linear
75
activation functions in recursive neural networks.
The work of Coecke et al (2010) and Grefen-
stette (2013) limits itself to defining, rather than
learning, distributional representations of logical
operators for distributional models that simulate
logic, and makes no pretense to the provision of
operations which generalise to higher-dimensional
distributional semantic representations. As for
the non-linear approach of Socher et al (2012),
we will discuss, in Section 4 below, the limita-
tions with this model with regard to the task of
modelling logic for higher dimensional represen-
tations.
Logic for vectors The third and final class of
approaches is the one the work presented here
belongs to. This class includes attempts to de-
fine representations for logical operators in high
dimensional semantic vector spaces. Such ap-
proaches do not seek to retrieve boolean logic and
truth values, but to define what logical operators
mean when applied to distributional representa-
tions. The seminal work in this area is found in the
work of Widdows and Peters (2003), who define
negation and other logical operators algebraically
for high dimensional semantic vectors. Negation,
under this approach, is effectively a binary rela-
tion rather than a unary relation: it expresses the
semantics of statements such as ?A NOT B? rather
than merely ?NOT B?, and does so by projecting
the vector for A into the orthogonal subspace of
the vector for B. This approach to negation is use-
ful for vector-based information retrieval models,
but does not necessarily capture all the aspects of
negation we wish to take into consideration, as
will be discussed in Section 3.
3 Logic in text
In order to model logical operations over semantic
vectors, we propose a tripartite meaning represen-
tation, which combines the separate and distinct
treatment of domain-related and value-related as-
pects of semantic vectors with a domain-driven
syntactic functional representation. This is a unifi-
cation of various recent approaches to the problem
of semantic representation in continuous distribu-
tional semantic modelling (Socher et al, 2012;
Turney, 2012; Hermann and Blunsom, 2013).
We borrow from Socher et al (2012) and oth-
ers (Baroni and Zamparelli, 2010; Coecke et al,
2010) the idea that the information words refer to
is of two sorts: first the semantic content of the
word, which can be seen as the sense or reference
to the concept the word stands for, and is typi-
cally modelled as a semantic vector; and second,
the function the word has, which models the effect
the word has on other words it combines with in
phrases and sentences, and is typically modelled
as a matrix or higher-order tensor. We borrow
from Turney (2012) the idea that the semantic as-
pect of a word should not be modelled as a single
vector where everything is equally important, but
ideally as two or more vectors (or, as we do here,
two or more regions of a vector) which stand for
the aspects of a word relating to its domain, and
those relating to its value.
We therefore effectively suggest a tripartite rep-
resentation of the semantics of words: a word?s
meaning is modelled by elements representing its
value, domain, and function, respectively.
The tripartite representation We argue that the
tripartite representation suggested above allows us
to explicitly capture several aspects of semantics.
Further, while there may be additional distinct as-
pects of semantics, we argue that this is a minimal
viable representation.
First of all, the differentiation between do-
main and value is useful for establishing similar-
ity within subspaces of meaning. For instance,
the words blue and red share a common domain
(colours) while having very different values. We
hypothesise that making this distinction explicit
will allow for the definition of more sophisticated
and fine-grained semantics of logical operations,
as discussed below. Although we will represent
domain and value as two regions of a vector, there
is no reason for these not to be treated as separate
vectors at the time of comparison, as done by Tur-
ney (2012).
Through the third part, the functional repre-
sentation, we capture the compositional aspect of
semantics: the functional representation governs
how a term interacts with its environment. In-
spired by the distributional interpretation (Baroni
and Zamparelli, 2010; Coecke et al, 2010) of
syntactically-paramatrized semantic composition
functions from Montogovian semantics (Mon-
tague, 1974), we will also assume the function part
of our representation to be parametrized princi-
pally by syntax and domain rather than value. The
intuition behind taking domain into account in ad-
dition to syntactic class being that all members of
a domain largely interact with their environment
76
in the same fashion.
Modeling negation The tripartite representation
proposed above allows us to define logical opera-
tions in more detail than competing approaches.
To exemplify this, we focus on the case of nega-
tion.
We define negation for semantic vectors to be
the absolute complement of a term in its domain.
This implies that negation will not affect the do-
main of a term but only its value. Thus, blue and
not blue are assumed to share a common domain.
We call this naive form of negation the inversion
of a term A, which we idealise as the partial inver-
sion Ainv of the region associated with the value
of the word in its vector representation A.
?
?
d
v
v
?
?
?
?
d
v
?v
?
?
?
?
d
v
??v
?
?
[
f
] [
f
] [
f
]
W Winv ?W
Figure 1: The semantic representations of a word
W , its inverse W inv and its negation ?W . The
domain part of the representation remains un-
changed, while the value part will partially be in-
verted (inverse), or inverted and scaled (negation)
with 0 < ? < 1. The (separate) functional repre-
sentation also remains unchanged.
Additionally, we expect negation to have a
diminutive effect. This diminutive effect is best
exemplified in the case of sentiment: good is more
positive than not bad, even though good and bad
are antonyms of each other. By extension not not
good and not not not bad end up somewhere in the
middle?qualitative statements still, but void of
any significant polarity. To reflect this diminutive
effect of negation and double negation commonly
found in language, we define the idealised diminu-
tive negation ?A of a semantic vectorA as a scalar
inversion over a segment of the value region of its
representation with the scalar ? : 0 < ? < 1, as
shown in Figure 1.
As we defined the functional part of our rep-
resentation to be predominately parametrized by
syntax and domain, it will remain constant under
negation and inversion.
4 A general matrix-vector model
Having discussed, above, how the vector compo-
nent of a word can be partitioned into domain and
value, we now turn to the partition between se-
mantic content and function. A good candidate for
modelling this partition would be a dual-space rep-
resentation similar to that of Socher et al (2012).
In this section, we show that this sort of represen-
tation is not well adapted to the modelling of nega-
tion.
Models using dual-space representations have
been proposed in several recent publications, no-
tably in Turney (2012) and Socher et al (2012).
We use the class of recursive matrix-vector mod-
els as the basis for our investigation; for a detailed
introduction see the MV-RNN model described in
Socher et al (2012).
We begin by describing composition for a gen-
eral dual-space model, and apply this model to the
notion of compositional logic in a tripartite repre-
sentation discussed earlier. We identify the short-
comings of the general model and subsequently
discuss alternative composition models and mod-
ifications that allow us to better capture logic in
vector space models of meaning.
Assume a basic model of compositionality for
such a tripartite representation as follows. Each
term is encoded by a semantic vector v captur-
ing its domain and value, as well as a matrix M
capturing its function. Thus, composition consists
of two separate operations to learn semantics and
function of the composed term:
vp = fv(va,vb,Ma,Mb) (1)
Mp = fM (Ma,Mb)
As we defined the functional representation to be
parametrized by syntax and domain, its compo-
sition function does not require va and vb as in-
puts, with all relevant information already being
contained in Ma,Mb. In the case of Socher et al
(2012) these functions are as follows:
Mp =WM
[
Ma
Mb
]
(2)
vp = g
(
Wv
[
Mavb
Mbva
])
(3)
where g is a non-linearity.
4.1 The question of non-linearities
While the non-linearity g could be equipped with
greater expressive power, such as in the boolean
77
logic experiment in Socher et al (2012)), the aim
of this paper is to place the burden of composition-
ality on the atomic representations instead. For
this reason we treat g as an identity function, and
WM , Wv as simple additive matrices in this inves-
tigation, by setting
g = I Wv =WM = [I I]
where I is an identity matrix. This simplification
is justified for several reasons.
A simple non-linearity such as the commonly
used hyperbolic tangent or sigmoid function will
not add sufficient power to overcome the issues
outlined in this paper. Only a highly complex non-
linear function would be able to satisfy the require-
ments for vector space based logic as discussed
above. Such a function would defeat the point
however, by pushing the ?heavy-lifting? from the
model structure into a separate function.
Furthermore, a non-linearity effectively en-
codes a scattergun approach: While it may have
the power to learn a desired behaviour, it similarly
has a lot of power to learn undesired behaviours
and side effects. From a formal perspective it
would therefore seem more prudent to explicitly
encode desired behaviour in a model?s structure
rather than relying on a non-linearity.
4.2 Negation
We have outlined our formal requirements for
negation in the previous section. From these re-
quirements we can deduce four equalities, con-
cerning the effect of negation and double nega-
tion on the semantic representation and function
of a term. The matrices J? and J? (illustrated in
?
?
?
?
?
?
?
?
?
?
1
. . . 0
1
??
0
. . .
??
?
?
?
?
?
?
?
?
?
?
Figure 2: A partially scaled and inverted identity
matrix J?. Such a matrix can be used to trans-
form a vector storing a domain and value repre-
sentation into one containing the same domain but
a partially inverted value, such as W and ?W de-
scribed in Figure 1.
Figure 2) describe a partially scaled and inverted
identity matrix, where 0 < ?, ? < 1.
fv(not, a) = J?va (4)
fM (not, a) ?Ma (5)
fv(not, fv(not, a)) = J?J?va (6)
fM (not, fM (not, a)) ?Ma (7)
Based on our assumption about the constant do-
main and interaction across negation, we can re-
place the approximate equality with a strict equal-
ity in Equations 5 and 7. Further, we assume that
both Ma 6= I and Ma 6= 0, i.e. that A has a spe-
cific and non-zero functional representation. We
make a similar assumption for the semantic repre-
sentation va 6= 0.
Thus, to satisfy the equalities in Equations 4
through 7, we can deduce the values of vnot and
Mnot as discussed below.
Value and Domain in Negation Under the sim-
plifications of the model explained earlier, we
know that the following is true:
fv(a, b) = g
(
Wv
[
Mavb
Mbva
])
= I
(
[
I I
]
[
Mavb
Mbva
])
=Mavb +Mbva
I.e. the domain and value representation of a par-
ent is the sum of the two Mv multiplications of
its children. The matrix Wv could re-weight this
addition, but that would not affect the rest of this
analysis.
Given the idea that the domain stays constant
under negation and that a part of the value is in-
verted and scaled, we further know that these two
equations hold:
?a ? A : fv(not, a) = J?va
?a ? A : fv(not, fv(not, a)) = J?J?va
Assuming that both semantic and functional
representation across all A varies and is non-zero,
these equalities imply the following conditions for
the representation of not:
Mnot = J? = J?
vnot = 0
These two equations suggest that the term not has
no inherent value (vnot = 0), but merely acts as a
function, inverting part of another terms semantic
representation (Mnot = J?).
78
Functional Representation in Negation We
can apply the same method to the functional rep-
resentation. Here, we know that:
fM (a, b) =WM
[
Ma
Mb
]
=
[
I I
]
[
Ma
Mb
]
=Ma +Mb
Further, as defined in our discussion of nega-
tion, we require the functional representation to
remain unchanged under negation:
?a ? A : fM (not, a) =Ma
?a ? A : fM (not, fM (not, a)) =Ma
These requirements combined leave us to con-
clude that Mnot = 0. Combined with the result
from the first part of the analysis, this causes a
contradiction:
Mnot = 0
Mnot = J?
=? J? = 0 
This demonstrates that the MV-RNN as de-
scribed in this paper is not capable of modelling
semantic logic according to the principles we out-
lined. The fact that we would require Mnot = 0
further supports the points made earlier about the
non-linearities and setting WM to
[
I I
]
. Even a
specific WM and non-linearity would not be able
to ensure that the functional representation stays
constant under negation given a non-zero Mnot.
Clearly, any other complex semantic represen-
tation would suffer from the same issue here?the
failure of double-negation to revert a representa-
tion to its (diminutive) original.
5 Analysis
The issue identified with the MV-RNN style mod-
els described above extends to a number of other
models of vector spaced compositionality. It can
be viewed as a problem of uninformed composi-
tion caused by a composition function that fails to
account for syntax and thus for scope.
Of course, identifying the scope of negation is a
hard problem in its own right?see e.g. the *SEM
2012 shared task (Morante and Blanco, 2012).
However, at least for simple cases, we can deduce
scope by considering the parse tree of a sentence:
S
VP
ADJP
JJ
blue
RB
not
VBZ
is
NP
N
car
Det
This
Figure 3: The parse tree for This car is not blue,
highlighting the limited scope of the negation.
If we consider the parse tree for this car is not blue,
it is clear that the scope of the negation expressed
includes the colour but not the car (Figure 3).
While the MV-RNN model in Socher et al
(2012) incorporates parse trees to guide the order
of its composition steps, it uses a single composi-
tion function across all steps. Thus, the functional
representation of not will to some extent propagate
outside of its scope, leading to a vector capturing
something that is not blue, but also not quite a car.
There are several possibilities for addressing
this issue. One possibility is to give greater weight
to syntax, for instance by parametrizing the com-
position functions fv and fM on the parse struc-
ture. This could be achieved by using specific
weight matrices Wv and WM for each possible
tag. While the power of this approach is limited
by the complexity of the parse structure, it would
be better able to capture effects such as the scoping
and propagation of functional representations.
Another approach, which we describe in greater
detail in the next section, pushes the issue of
propagation onto the word level. While both ap-
proaches could easily be combined, this second
option is more consistent with our aim of avoid-
ing the implicit encoding of logic into fixed model
parameters in favour of the explicit encoding in
model structure.
6 An improved model
As we outlined in this paper, a key requirement
for a compositional model motivated by formal se-
mantics is the ability to propagate functional rep-
resentations, but also to not propagate these repre-
sentations when doing so is not semantically ap-
propriate. Here, we propose a modification of the
MV-RNN class of models that can capture this dis-
79
tinction without the need to move the composition
logic into the non-linearity.
We add a parameter ? to the representation of
each word, controlling the degree to which its
functional representation propagates after having
been applied in its own composition step.
Thus, the composition step of the new model
requires three equations:
Mp =WM
[
?a
?a+?b
Ma
?b
?a+?b
Mb
]
(8)
vp = g
(
Wv
[
Mavb
Mbva
])
(9)
?p = max(?a, ?b) (10)
Going back to the discussion of negation, this
model has the clear advantage of being able to cap-
ture negation in the way we defined it. As fv(a, b)
is unchanged, these two equations still hold:
Mnot = J? = J?
vnot = 0
However, as fM (a, b) is changed, the second
set of equations changes. We use Z as the ?-
denominator (Z = ?a + ?B) for simplification:
fM (a, b) =WM
[?a
Z Ma
?b
Z Mb
]
=
[
I
I
] [?a
Z Ma
?b
Z Mb
]
=
?a
Z
Ma +
?b
Z
Mb
Further, we still require the functional representa-
tion to remain constant under negation:
?a ? A : fM (not, a) =Ma
?a ? A : fM (not, fM (not, a)) =Ma
Thus, we can infer the following two conditions
on the new model:
?not
Z
Mnot ? 0
?a
Z
Ma ?Ma
From our previous investigation we already know
that Mnot = J? 6= 0, i.e. that not has a non-
zero functional representation. While this caused
a contradiction for the original MV-RNN model,
the design of the improved model can resolve this
issue through the ?-parameter:
?not = 0
Thus, we can use this modified MV-RNN model
to represent negation according to the principles
outlined in this paper. The result ?not = 0 is in
accordance with our intuition about the propaga-
tion of functional aspects of a term: We commonly
expect negation to directly affect the things un-
der its scope (not blue) by choosing their semantic
complement. However, this behaviour should not
propagate outside of the scope of the negation. A
not blue car is still very much a car, and when a
film is not good, it is still very much a film.
7 Discussion and Further Work
In this paper, we investigated the capability of con-
tinuous vector space models to capture the seman-
tics of logical operations in non-boolean cases.
Recursive and recurrent vector models of meaning
have enjoyed a considerable amount of success in
recent years, and have been shown to work well on
a number of tasks. However, the complexity and
subsequent power of these models comes at the
price that it can be difficult to establish which as-
pect of a model is responsible for what behaviour.
This issue was recently highlighted by an inves-
tigation into recursive autoencoders for sentiment
analysis (Scheible and Schu?tze, 2013). Thus, one
of the key challenges in this area of research is the
question of how to control the power of these mod-
els. This challenge motivated the work in this pa-
per. By removing non-linearities and other param-
eters that could disguise model weaknesses, we fo-
cused our work on the basic model design. While
such features enhance model power, they should
not be used to compensate for inherently flawed
model designs.
As a prerequisite for our investigation we estab-
lished a suitable encoding of textual logic. Distri-
butional representations have been well explained
on the word level, but less clarity exists as to the
semantic content of compositional vectors. With
the tripartite meaning representation we proposed
one possible approach in that direction, which we
subsequently expanded by discussing how nega-
tion should be captured in this representation.
Having established a suitable and rigorous sys-
tem for encoding meaning in compositional vec-
tors, we were thus able to investigate the repre-
80
sentative power of the MV-RNN model. We fo-
cused this paper on the case of negation, which
has the advantage that it does not require many
additional assumptions about the underlying se-
mantics. Our investigation showed that the basic
MV-RNN model is incompatible with our notion
of negation and thus with any textual logic build-
ing on this proposal.
Subsequently, we analysed the reasons for this
failure. We explained how the issue of nega-
tion affects the general class of MV-RNN models.
Through the issue of double-negation we further
showed how this issue is largely independent on
the particular semantic encoding used. Based on
this analysis we proposed an improved model that
is able to capture such textual logic.
In summary, this paper has two key contribu-
tions. First, we developed a tripartite represen-
tation for vector space based models of seman-
tics, incorporating multiple previous approaches
to this topic. Based on this representation, the
second contribution of this paper was a modified
MV-RNN model that can capture effects such as
negation in its inherent structure.
In future work, we would like to build on the
proposals in this paper, both by extending our
work on textual logic to include formulations for
e.g. function words, quantifiers, or locative words.
Similarly, we plan to experimentally validate these
ideas. Possible tasks for this include sentiment
analysis and relation extraction tasks such as in
Socher et al (2012) but also more specific tasks
such as the *SEM shared task on negation scope
and reversal (Morante and Blanco, 2012).
Acknowledgements
The first author is supported by the UK Engineer-
ing and Physical Sciences Research Council (EP-
SRC). The second author is supported by EPSRC
Grant EP/I03808X/1.
References
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, E. Erk, and
R. Mooney. 2013. Montague meets markov: Deep
semantics with probabilistic logical form. June.
W. Blacoe and M. Lapata. 2012. A comparison of
vector-based representations for semantic composi-
tion. Proceedings of the 2012 Conference on Empir-
ical Methods in Natural Language Processing.
J. Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on
Semantics in Text Processing, pages 277?286. Asso-
ciation for Computational Linguistics.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for a Compositional Distribu-
tional Model of Meaning. March.
J. R. Curran. 2004. From distributional to semantic
similarity. Ph.D. thesis.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. Proceedings
of the Conference on Empirical Methods in Natural
Language Processing - EMNLP ?08, (October):897.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrating
logical representations with probabilistic informa-
tion using markov logic. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, pages 105?114. Association for Computational
Linguistics.
E. Grefenstette and M. Sadrzadeh. 2011. Experi-
mental support for a categorical compositional dis-
tributional model of meaning. In Proceedings of
EMNLP, pages 1394?1404.
E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning
for compositional distributional semantics. In Pro-
ceedings of the Tenth International Conference on
Computational Semantics. Association for Compu-
tational Linguistics.
E. Grefenstette. 2013. Towards a formal distributional
semantics: Simulating logical calculi with tensors.
Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery.
K. M. Hermann and P. Blunsom. 2013. The role of
syntax in vector space models of compositional se-
mantics. In Proceedings of ACL, Sofia, Bulgaria,
August. Association for Computational Linguistics.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2012. A
unified sentence space for categorical distributional-
compositional semantics: Theory and experiments.
In Proceedings of 24th International Conference
on Computational Linguistics (COLING 2012):
Posters, pages 549?558, Mumbai, India, December.
81
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
volume 8.
J. Mitchell and M. Lapata. 2010. Composition in Dis-
tributional Models of Semantics. Cognitive Science.
R. Montague. 1974. English as a Formal Language.
Formal Semantics: The Essential Readings.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
?12, pages 265?274, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine learning, 62(1-2):107?136.
C. Scheible and H. Schu?tze. 2013. Cutting recursive
autoencoder trees. In Proceedings of the Interna-
tional Conference on Learning Representations.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational linguistics, 24(1):97?123.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011. Dynamic pooling and un-
folding recursive autoencoders for paraphrase detec-
tion. Advances in Neural Information Processing
Systems, 24:801?809.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1201?1211.
P. D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
D. Widdows and S. Peters. 2003. Word vectors and
quantum logic: Experiments with negation and dis-
junction. Mathematics of language, 8(141-154).
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
S. Manandhar. 2010. Estimating linear models for
compositional distributional semantics. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 1263?1271. Associa-
tion for Computational Linguistics.
82
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22?27,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
A Deep Architecture for Semantic Parsing
Edward Grefenstette, Phil Blunsom, Nando de Freitas and Karl Moritz Hermann
Department of Computer Science
University of Oxford, UK
{edwgre, pblunsom, nando, karher}@cs.ox.ac.uk
Abstract
Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.
1 Introduction
The ubiquity of always-online computers in the
form of smartphones, tablets, and notebooks has
boosted the demand for effective question answer-
ing systems. This is exemplified by the grow-
ing popularity of products like Apple?s Siri or
Google?s Google Now services. In turn, this cre-
ates the need for increasingly sophisticated meth-
ods for semantic parsing. Recent work (Artzi and
Zettlemoyer, 2013; Kwiatkowski et al., 2013; Ma-
tuszek et al., 2012; Liang et al., 2011, inter alia)
has answered this call by progressively moving
away from strictly rule-based semantic parsing, to-
wards the use of distributed representations in con-
junction with traditional grammatically-motivated
re-write rules. This paper seeks to extend this line
of thinking to its logical conclusion, by provid-
ing the first (to our knowledge) entirely distributed
neural semantic generative parsing model. It does
so by adapting deep learning methods from related
work in sentiment analysis (Socher et al., 2012;
Hermann and Blunsom, 2013), document classifi-
cation (Yih et al., 2011; Lauly et al., 2014; Her-
mann and Blunsom, 2014a), frame-semantic pars-
ing (Hermann et al., 2014), and machine trans-
lation (Mikolov et al., 2010; Kalchbrenner and
Blunsom, 2013a), inter alia, combining two em-
pirically successful deep learning models to form
a new architecture for semantic parsing.
The structure of this short paper is as follows.
We first provide a brief overview of the back-
ground literature this model builds on in ?2. In ?3,
we begin by introducing two deep learning models
with different aims, namely the joint learning of
embeddings in parallel corpora, and the generation
of strings of a language conditioned on a latent
variable, respectively. We then discuss how both
models can be combined and jointly trained to
form a deep learning model supporting the gener-
ation of knowledgebase queries from natural lan-
guage questions. Finally, in ?4 we conclude by
discussing planned experiments and the data re-
quirements to effectively train this model.
2 Background
Semantic parsing describes a task within the larger
field of natural language understanding. Within
computational linguistics, semantic parsing is typ-
ically understood to be the task of mapping nat-
ural language sentences to formal representations
of their underlying meaning. This semantic rep-
resentation varies significantly depending on the
task context. For instance, semantic parsing has
been applied to interpreting movement instruc-
tions (Artzi and Zettlemoyer, 2013) or robot con-
trol (Matuszek et al., 2012), where the underlying
representation would consist of actions.
Within the context of question answering?the
focus of this paper?semantic parsing typically
aims to map natural language to database queries
that would answer a given question. Kwiatkowski
22
et al. (2013) approach this problem using a multi-
step model. First, they use a CCG-like parser
to convert natural language into an underspecified
logical form (ULF). Second, the ULF is converted
into a specified form (here a FreeBase query),
which can be used to lookup the answer to the
given natural language question.
3 Model Description
We describe a semantic-parsing model that learns
to derive quasi-logical database queries from nat-
ural language. The model follows the structure of
Kwiatkowski et al. (2013), but relies on a series of
neural networks and distributed representations in
lieu of the CCG and ?-Calculus based representa-
tions used in that paper.
The model described here borrows heavily from
two approaches in the deep learning literature.
First, a noise-contrastive neural network similar to
that of Hermann and Blunsom (2014a, 2014b) is
used to learn a joint latent representation for nat-
ural language and database queries (?3.1). Sec-
ond, we employ a structured conditional neural
language model in ?3.2 to generate queries given
such latent representations. Below we provide the
necessary background on these two components,
before introducing the combined model and de-
scribing its learning setup.
3.1 Bilingual Compositional Sentence Models
The bilingual compositional sentence model
(BiCVM) of Hermann and Blunsom (2014a) pro-
vides a state-of-the-art method for learning se-
mantically informative distributed representations
for sentences of language pairs from parallel cor-
pora. Through the joint production of a shared la-
tent representation for semantically aligned sen-
tence pairs, it optimises sentence embeddings
so that the respective representations of dissim-
ilar cross-lingual sentence pairs will be weakly
aligned, while those of similar sentence pairs will
be strongly aligned. Both the ability to jointly
learn sentence embeddings, and to produce latent
shared representations, will be relevant to our se-
mantic parsing pipeline.
The BiCVM model shown in Fig. 1 assumes
vector composition functions g and h, which map
an ordered set of vectors (here, word embed-
dings from D
A
,D
B
) onto a single vector in R
n
.
As stated above, for semantically equivalent sen-
tences a, b across languages L
A
,L
B
, the model
aims to minimise the distance between these com-
posed representations:
E
bi
(a, b) = ?g(a)? h(b)?
2
In order to avoid strong alignment between dis-
similar cross-lingual sentence pairs, this error
is combined with a noise-contrastive hinge loss,
where n ? L
B
is a randomly sampled sentence,
dissimilar to the parallel pair {a, b}, and m de-
notes some margin:
E
hl
(a, b, n) = [m+ E
bi
(a, b)? E
bi
(a, n)]
+
,
where [x]
+
= max(0, x). The resulting objective
function is as follows
J(?) =
?
(a,b)?C
(
k
?
i=1
E
hl
(a, b, n
i
) +
?
2
???
2
)
,
with
?
2
???
2
as the L
2
regularization term and
?={g, h,D
A
,D
B
} as the set of model variables.
...
L1 sentence embedding
L1 word embeddings
L2 sentence embedding
L2 word embeddings
contrastive estimation
g
h
Figure 1: Diagrammatic representation of a
BiCVM.
While Hermann and Blunsom (2014a) applied
this model only to parallel corpora of sentences,
it is important to note that the model is agnostic
concerning the inputs of functions g and h. In this
paper we will discuss how this model can be ap-
plied to non-sentential inputs.
23
3.2 Conditional Neural Language Models
Neural language models (Bengio et al., 2006) pro-
vide a distributed alternative to n-gram language
models, permitting the joint learning of a pre-
diction function for the next word in a sequence
given the distributed representations of a subset
of the last n?1 words alongside the representa-
tions themselves. Recent work in dialogue act la-
belling (Kalchbrenner and Blunsom, 2013b) and
in machine translation (Kalchbrenner and Blun-
som, 2013a) has demonstrated that a particular
kind of neural language model based on recurrent
neural networks (Mikolov et al., 2010; Sutskever
et al., 2011) could be extended so that the next
word in a sequence is jointly generated by the
word history and the distributed representation for
a conditioning element, such as the dialogue class
of a previous sentence, or the vector representation
of a source sentence. In this section, we briefly de-
scribe a general formulation of conditional neural
language models, based on the log-bilinear mod-
els of Mnih and Hinton (2007) due to their relative
simplicity.
A log-bilinear language model is a neural net-
work modelling a probability distribution over the
next word in a sequence given the previous n?1,
i.e. p(w
n
|w
1:n?1
). Let |V | be the size of our vo-
cabulary, and R be a |V | ? d vocabulary matrix
where the R
w
i
demnotes the row containing the
word embedding in R
d
of a word w
i
, with d be-
ing a hyper-parameter indicating embedding size.
Let C
i
be the context transform matrix in R
d?d
which modifies the representation of the ith word
in the word history. Let b
w
i
be a scalar bias as-
sociated with a word w
i
, and b
R
be a bias vector
in R
d
associated with the model. A log-bilinear
model expressed the probability of w
n
given a his-
tory of n?1 words as a function of the energy of
the network:
E(w
n
;w
1:n?1
) =
?
(
n?1
?
i=1
R
T
w
i
C
i
)
R
w
n
? b
T
R
R
w
n
? b
w
n
From this, the probability distribution over the
next word is obtained:
p(w
n
|w
1:n?1
) =
e
?E(w
n
;w
1:n?1
)
?
w
n
e
?E(w
n
;w
1:n?1
)
To reframe a log-bilinear language model as a
conditional language model (CNLM), illustrated
?
w
n
w
n-1
w
n-2
w
n-3
Figure 2: Diagrammatic representation of a Con-
ditional Neural Language Model.
in Fig. 2, let us suppose that we wish to jointly
condition the next word on its history and some
variable ?, for which an embedding r
?
has been
obtained through a previous step, in order to com-
pute p(w
n
|w
1:n?1
, ?). The simplest way to do this
additively, which allows us to treat the contribu-
tion of the embedding for ? as similar to that of an
extra word in the history. We define a new energy
function:
E(w
n
;w
1:n?1
, ?) =
?
((
n?1
?
i=1
R
T
w
i
C
i
)
+ r
T
?
C
?
)
R
w
n
? b
T
R
R
w
n
? b
w
n
to obtain the probability
p(w
n
|w
1:n?1
, ?) =
e
?E(w
n
;w
1:n?1
,?)
?
w
n
e
?E(w
n
;w
1:n?1
,?)
Log-bilinear language models and their condi-
tional variants alike are typically trained by max-
imising the log-probability of observed sequences.
3.3 A Combined Semantic Parsing Model
The models in ??3.1?3.2 can be combined to form
a model capable of jointly learning a shared la-
tent representation for question/query pairs using
a BiCVM, and using this latent representation to
learn a conditional log-bilinear CNLM. The full
model is shown in Fig. 3. Here, we explain the
final model architecture both for training and for
subsequent use as a generative model. The details
of the training procedure will be discussed in ?3.4.
The combination is fairly straightforward, and
happens in two steps at training time. For the
24
...
Knowledgebase query
Question
Latent
representation
Query embedding
Question embedding
Relation/object
embeddings
Word embeddings
Conditional
Log-bilinear
Language Model
g
h
Figure 3: Diagrammatic representation of the full
model. First the mappings for obtaining latent
forms of questions and queries are jointly learned
through a BiCVM. The latent form for questions
then serves as conditioning element in a log-
bilinear CNLM.
first step, shown in the left hand side of Fig. 3,
a BiCVM is trained against a parallel corpora
of natural language question and knowledgebase
query pairs. Optionally, the embeddings for the
query symbol representations and question words
are initialised and/or fine-tuned during training,
as discussed in ?3.4. For the natural language
side of the model, the composition function g can
be a simple additive model as in Hermann and
Blunsom (2014a), although the semantic informa-
tion required for the task proposed here would
probably benefit from a more complex composi-
tion function such as a convolution neural net-
work. Function h, which maps the knowledgebase
queries into the shared space could also rely on
convolution, although the structure of the database
queries might favour a setup relying primarily on
bi-gram composition.
Using function g and the original training data,
the training data for the second stage is created
by obtaining the latent representation for the ques-
tions of the original dataset. We thereby obtain
pairs of aligned latent question representations and
knowledgebase queries. This data allows us to
train a log-bilinear CNLM as shown on the right
side of Fig. 3.
Once trained, the models can be fully joined to
produce a generative neural network as shown in
Fig. 4. The network modelling g from the BiCVM
...
Question
Generated Query
g
Figure 4: Diagrammatic representation of the final
network. The question-compositional segment of
the BiCVM produces a latent representation, con-
ditioning a CNLM generating a query.
takes the distributed representations of question
words from unseen questions, and produces a la-
tent representation. The latent representation is
then passed to the log-bilinear CNLM, which con-
ditionally generates a knowledgebase query corre-
sponding to the question.
3.4 Learning Model Parameters
We propose training the model of ?3.3 in a two
stage process, in line with the symbolic model of
Kwiatkowski et al. (2013).
First, a BiCVM is trained on a parallel corpus
C of question-query pairs ?Q,R? ? C, using com-
position functions g for natural language questions
and h for database queries. While functions g and
hmay differ from those discussed in Hermann and
Blunsom (2014a), the basic noise-contrastive op-
timisation function remains the same. It is possi-
ble to initialise the model fully randomly, in which
25
case the model parameters ? learned at this stage
include the two distributed representation lexica
for questions and queries, D
Q
and D
R
respec-
tively, as well as all parameters for g and h.
Alternatively, word embeddings inD
Q
could be
initialised with representations learned separately,
for instance with a neural language model or a
similar system (Mikolov et al., 2010; Turian et al.,
2010; Collobert et al., 2011, inter alia). Likewise,
the relation and object embeddings inD
R
could be
initialised with representations learned from dis-
tributed relation extraction schemas such as that
of Riedel et al. (2013).
Having learned representations for queries in
D
R
as well as function g, the second training phase
of the model uses a new parallel corpus consisting
of pairs ?g(Q), R? ? C
?
to train the CNLM as pre-
sented in ?3.3.
The two training steps can be applied iteratively,
and further, it is trivial to modify the learning
procedure to use composition function h as an-
other input for the CNLM training phrase in an
autoencoder-like setup.
4 Experimental Requirements and
Further Work
The particular training procedure for the model
described in this paper requires aligned ques-
tion/knowledgebase query pairs. There exist some
small corpora that could be used for this task
(Zelle and Mooney, 1996; Cai and Yates, 2013). In
order to scale training beyond these small corpora,
we hypothesise that larger amounts of (potentially
noisy) training data could be obtained using a
boot-strapping technique similar to Kwiatkowski
et al. (2013).
To evaluate this model, we will follow the ex-
perimental setup of Kwiatkowski et al. (2013).
With the provisio that the model can generate
freebase queries correctly, further work will seek
to determine whether this architecture can gener-
ate other structured formal language expressions,
such as lambda expressions for use in textual en-
tailement tasks.
Acknowledgements
This work was supported by a Xerox Foundation
Award, EPSRC grants number EP/I03808X/1 and
EP/K036580/1, and the Canadian Institute for Ad-
vanced Research (CIFAR) Program on Adaptive
Perception and Neural Computation.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexi-
con Extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of the 2nd International
Conference on Learning Representations, Banff,
Canada, April.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Baltimore, USA,
June. Association for Computational Linguistics.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Baltimore, USA, June. Association
for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA. Association for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current convolutional neural networks for discourse
compositionality. arXiv preprint arXiv:1306.3584.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
26
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2014. Learning multilingual word representa-
tions using a bag-of-words autoencoder. CoRR,
abs/1401.1803.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 590?599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the 29th Inter-
national Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1,
2012.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of EMNLP-CoNLL, pages 1201?1211.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017?1024.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, Stroudsburg, PA, USA.
Wen-Tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning Discrimina-
tive Projections for Text Similarity Measures. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?11,
pages 247?256, Stroudsburg, PA, USA. Association
for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050?1055.
27
