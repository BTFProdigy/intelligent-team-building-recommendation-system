Squibs
Reliability Measurement without Limits
Dennis Reidsma?
University of Twente
Jean Carletta??
University of Edinburgh
In computational linguistics, a reliability measurement of 0.8 on some statistic such as ? is
widely thought to guarantee that hand-coded data is fit for purpose, with 0.67 to 0.8 tolerable,
and lower values suspect. We demonstrate that the main use of such data, machine learning, can
tolerate data with low reliability as long as any disagreement among human coders looks like
random noise. When the disagreement introduces patterns, however, the machine learner can
pick these up just like it picks up the real patterns in the data, making the performance figures
look better than they really are. For the range of reliability measures that the field currently
accepts, disagreement can appreciably inflate performance figures, and even a measure of 0.8 does
not guarantee that what looks like good performance really is. Although this is a commonsense
result, it has implications for how we work. At the very least, computational linguists should
look for any patterns in the disagreement among coders and assess what impact they will have.
1. Introduction
In computational linguistics, 0.8 is often regarded as some kind of magical reliability
cut-off guaranteeing the quality of hand-coded data (e.g., Reithinger and Kipp 1998;
Shriberg et al 1998; Galley et al 2004), with 0.67 to 0.8 tolerable?although it is as
often honored in the breech as in the observance. The argument for the meaning of
0.8 arises originally from Krippendorff (1980, page 147), in a comment about practice
in the field of content analysis. He states that correlations found between two variables
using their hand-coded values ?tend to be insignificant? when the hand-codings have
a reliability below 0.8. He uses a specific reliability statistic, ?, for his measurements,
but Carletta (1996) implicitly assumes kappa-like metrics are similar enough in practice
for the rule of thumb to apply to them as well. A detailed discussion on the differences
and similarities of these, and other, measures is provided by Krippendorff (2004); in this
article we will use Cohen?s ? (1960) to investigate the value of the 0.8 reliability cut-off
for computational linguistics.
Modern computational linguists use data in a completely different way from 1970s
content analysts. Rather than correlating two variables, we use hand-coded data as
? University of Twente, Human Media Interaction, Room ZI2067, PO Box 217, NL-7500 AE Enschede,
The Netherlands, D.Reidsma@utwente.nl.
?? University of Edinburgh, Human Communication Research Centre, J.Carletta@ed.ac.uk.
Submission received: 4 September 2007; revised submission received: 20 December 2007; accepted for
publication: 6 April 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
training and test material for automatic classifiers. The 0.8 rule of thumb is irrelevant
for this purpose, because classifiers will be affected by disagreement differently than
correlations. Furthermore, Krippendorff?s argument comes with a caveat: the disagree-
ment must be due to random noise. For his case of correlations, any patterns in the
disagreement could accidentally bolster the relationship perceived in the data, leading
to false results. To be sure that data is fit for the intended purpose, Krippendorff advises
the analyst to look for structure in the disagreement and consider how it might affect
data use. Although computational linguists have rarely followed this advice, it is just
as relevant to us. Machine-learning algorithms are designed specifically to look for, and
predict, patterns in noisy data. In theory, this makes random disagreement unimportant.
More data will yield more signal and the learner will ignore the noise. However, as
Craggs and McGee Wood (2005) suggest, this also makes systematic disagreement
dangerous, because it provides an unwanted pattern for the learner to detect. We
demonstrate that machine learning can tolerate data with a low reliability measurement
as long as the disagreement looks like random noise, and that when it does not, data
can have a reliability measure commonly held to be acceptable but produce misleading
results.
2. Method
To explain what is wrong with using 0.8 as a cut-off, we need to think about how data
is used for classification tasks. Consider Figure 1, which shows a relation between some
features A and a class label B. Learning labels from a set of features is a common task
in computational linguistics; for instance, in Shriberg et al (1998), which assumes a
pre-existing dialogue act segmentation, the labels are dialogue act types, and they are
learned from automatically derived prosodic features. In this way of using data, only
one of the variables?the output dialogue act label?is hand-coded. In the figure, the
real relationship between prosody and dialogue act label is shown on the left; R relates
the prosodic features A to the output act B.
Figure 1
Hand-coded target labels are used to train classifiers to automatically predict those labels
from features.
320
Reidsma and Carletta Reliability Measurement without Limits
In theory, there is one correct label for any given act. However, in practice hu-
man coders disagree, choosing different labels for the same act (sometimes even with
divergences that make one question whether there is one correct answer). The data
actually available for analysis is shown in the middle of the figure. Here, the automatic
features, A, are the same as before, but there are multiple, possibly differing labels
for the same act, Bobs, coming from different human annotators. Finally, on the right
the figure shows the classifier. It takes the same prosodic features A and uses them to
predict a dialogue act label Bpred on new data, using the relationship learned from the
observed data, RML. Projects vary in how they choose data from which to build the
classifier when coders disagree, but whatever they do is colored by the observations
they have available to them. We often think of reliability assessment as telling us how
much disagreement there is among the human coders, but the real issue is how their
individual interpretations of the coding scheme make RML differ from R.
There is a problem that arises for anyone using this methodology. Without the
?real? data, it is impossible to judge how well the learned relationship reflects the real
one. Classification performance for Bpred can only be calculated with respect to the ?ob-
served? data Bobs. In this article, we surmount this problem by simulating the real world
so that we can measure the differences between this ?observed? performance and the
?real? performance. Our simulation uses a Bayesian network (Pearl 1988) to create an
initial, ?real? data set with 3,000 samples of features (A) and their corresponding target
labels (B). For simplicity, we use a single five-valued feature and five possible labels.
The relative label frequencies vary between 17% and 25%. This gives us a small amount
of variation around what is essentially equally distributed data. We corrupt the labels
(B) to simulate the ?hand-coded? observed data (Bobs) corresponding to the output of
a human coder, and then train a neural network constructed using the WEKA toolkit
(Witten and Frank 2005) on 2,000 samples from Bobs. Finally, we calculate the neural
network?s performance twice, using as test data either the remaining 1,000 samples from
Bobs or the initial, ?real? versions of those same 1,000 samples.
There are three ways in which we need to vary our simulation in order to be sys-
tematic. The first is in the strength of the relationship between the features the machine
learner takes as input and the target labels, which we achieve simply by changing the
probabilities in the Bayesian network that creates the data set. In the simulation, we
vary the strength of the relationship in eight graded steps.1 The second is in the amount
of disagreement we introduce when we create the observed data (Bobs). We create 200
different versions of the hand-coded data that cover a range of values from ? = 0 to
1 We use Cramer?s phi to measure the strength of a relationship. Cramer?s phi is defined as
?c =
?
?2
(N) ? dfsmaller
with N the number of samples and dfsmaller the smallest degree of freedom of the two involved variables,
and is a measure of association for nominal variables with more than two values. It can be ?considered
like a correlation coefficient? (Aron and Aron 2003) that takes data set size into account and can easily
be derived for a Bayesian network from the priors and the conditional probability tables. We varied
the strength of the network between ?c = 0.06 and ?c = 0.45. Following Cohen (1988), for a five-way
distinction Aron and Aron (page 527) would consider 0.06 to represent a small real relationship?that
is, one with not much effect?and 0.3, a large one. Thus we describe 0.06 as ?weak,? 0.45 as ?very
strong,? and intermediate points as ?moderate? and ?strong.? It is an open question what strengths
of relationships actually occur in computational linguistics data, although there may be no point in
learning a relationship that?s too strong.
321
Computational Linguistics Volume 34, Number 3
Figure 2
Machine-learning performance obtained on annotations with noise-like disagreements for (a)
weak (?c = 0.06), (b) moderate (?c = 0.20), (c) strong (?c = 0.32), and (d) very strong (?c = 0.45)
relationships between the features and labels.
? = 1, by introducing a varying amount of observation errors in the simulated coding
process.2 The third is in the type of disagreement with which we degrade the real data
to create the observed data (Bobs), representing the types of coding errors the human
annotators make. Again for simplicity, we describe the effects of both random errors
and the overuse of a single coding label.
3. The Case of Noise
Figure 2 shows how a neural network performs when coders make random mistakes
in their coding task, that is, for noise-like disagreement, for the cases of (a) weak, (b)
moderate, (c) strong, and (d) very strong relationships between the features (A) and
labels (B). Here, the y axis shows ?accuracy,? or the percentage of samples in the test
2 To calculate ? for a specific simulated coding we generate two copies of additional ?real? data that has
not been used for training or testing, apply the same simulated human annotator to one copy, and a
second annotator who makes the same number of ?mistakes? to the other copy. This mimics the
common practice of having one annotator code the data, with a second annotator coding enough to
test the reliability.
322
Reidsma and Carletta Reliability Measurement without Limits
data for which the network chooses the correct label. The x axis varies the amount of
coder errors in the data to correspond to different ? values, with the two black lines
marking the values of ? = 0.67 and ? = 0.8.
Look first at the series depicted as a line. It shows accuracy measured by using
the ?observed? version of the test data, which is how testing is normally done. For each
relationship strength, as ? increases, so does accuracy. In all cases, at ? = 0 (that is, when
the coders fail to agree beyond what one would expect if they were all choosing their
labels randomly) accuracy is at 20%, which is what one would expect if the classifier
were choosing randomly as well. For any given ? value, the stronger the underlying
relationship, the more benefit the neural network can derive from the data. Now look
at the other of the two series, depicted as small squares. It shows accuracy measured
by using the ?real? version of the data. Interestingly, the ?real? performance, that is, the
power of the learned model to predict reality, is higher than performance as measured
against the observed data. This is because for some samples, the classifier?s predictions
are correct, but because the observations contain errors, the test data actually gets
them wrong. The stronger the relationship in the real data, the more marked this effect
becomes. The neural network is able to disregard noise-like coding errors at very low
? values simply because the errors contain no patterns for it to learn.
4. The Case of Overusing a Label
Now consider the case where instead of random coding errors, the coder over-uses
the least frequent one of the five labels for B. Figure 3 shows the results for this kind
of coding error. Remember that in the graphs, the series depicted as a line shows the
observed performance of the classifier?that is, performance as it is usually measured.
The two black lines again mark the ? values of interest (? = 0.67 and ? = 0.8).
The graphs show an entirely different effect from the one obtained for noise-like
coding errors: For lower values of ?, the observed performance is spuriously high. This
makes perfect sense?? is low when the pattern of label overuse is strong, and the neural
network picks it up. When the observed data is used to test performance, some of the
samples match not because the classifier gets the label right, but because it overuses
the same label as the human coder. For data with a very strong correlation between the
input features A and the output labels B, the turning point below which performance
is spuriously high occurs at around ? = 0.55 (Figure 3d), a value the community holds
to be pretty low but which is not unknown in published work. However, when the
underlying relationship to be learned is moderate or strong (Figures 3b and 3c), the
spuriously high results already occur for ? values commonly held to be tolerable. With
a weak relationship, the turning point can occur at ? > 0.8 (Figure 3a).
5. Discussion
Our simulation highlights a danger for current practice in computational linguistics,
among other fields. Overuse of a label is a realistic type of error for human annotators
to make. For instance, imagine a coding scheme for dialogue acts that distinguishes
backchannel utterances from utterances which indicate agreement. In data containing
many utterances where the speech consists of ?Yeah,? individual coders can easily have
a marked bias for either one of these two categories. Clearly, in actual coding, not all
disagreement will be of one type, but will contain a mix of different systematic and
noise-like errors. In addition, the underlying relationships that our systems attempt to
323
Computational Linguistics Volume 34, Number 3
Figure 3
Machine-learning performance obtained on annotations that suffered from over-coding for (a)
weak (?c = 0.06), (b) moderate (?c = 0.20), (c) strong (?c = 0.32), and (d) very strong (?c = 0.45)
relationships between the features and labels.
learn vary in strength. This makes discerning the degree of danger more difficult, but
does not change the substance of our argument.
Although the graphs we show are for a specific simulation, the general pattern we
describe is robust. In particular, using ? in place of ? does not markedly change the
results; neither does increasing or decreasing the data set size. Our simulations and
results are presented for a machine-learning context. However, that does not mean that
other types of data use are immune to the problems we describe here. Other statistical
uses of data will be affected in their own ways by the difference between structural and
noise-like disagreement.
6. Implications
At the moment, much of the effort we devote to reliability measurement as a community
is used to establish one or more overall reliability statistics for our data sets and to argue
about which reliability statistic is most appropriate. Methodological discussions focus
on questions such as how to force annotated data structures into the mathematical form
necessary to calculate ?, or what effects certain aspects of the annotation have on the
values of some metric rather than on possible uses of the resulting data (Marcu, Amorrortu,
324
Reidsma and Carletta Reliability Measurement without Limits
and Romera 1999; Di Eugenio and Glass 2004; Artstein and Poesio 2005). Computational
linguists are of course aware that no overall reliability measure can give a complete
story, but often fail to spend time analyzing coder disagreements further. Unfortunately,
our results suggest that current practice is insufficient, at least where the data is destined
to be input for a machine-learning process and quite possibly for other data uses as
well. This complements observations of Artstein and Poesio: Besides the fact that many
different ways of calculating reliability metrics lead to different values, which makes
comparing them to a threshold difficult (Artstein and Poesio in press), the very idea
of having any such single threshold in the first place turns out to be impossible to
hold. Instead of worrying about exactly how much disagreement there is in a data
set and how to measure it, we should be looking at the form the disagreement takes.
A headline measurement, no matter how it is expressed, will not show the difference
between noise-like and systematic disagreement, but this difference can be critical for
establishing whether or not a data set is fit for the purpose for which it is intended.
To tease out what sort of disagreement a data set contains, Krippendorff suggests
calculating odd-man-out and per-class reliability to find out which class distinctions are
problematic (1980, page 150). Bayerl and Paul (2007) discuss methods for determining
which factors (schema changes, coding team changes, etc.) were involved in causing
poor annotation quality. Wiebe, Bruce, and O?Hara (1999) suggest looking at the mar-
ginals and how they differ between coders to find indications of whether disagreements
are caused by systematic bias (as opposed to being random) and in which classes
they occur. Although clearly useful techniques, none of these diagnostics is specifically
designed to address the needs of machine learners which are designed to recognize pat-
terns. Overusing a label is just one simple example of a type of systematic disagreement
that adds unwanted patterns that a machine learner can find. Any spurious pattern
could be a problem. For this reason, we should be looking specifically for patterns in
the disagreement itself.
Our suggestion for one possible diagnostic technique is based on the following
observation: If the disagreements between two coders contain no pattern, any test for
association or correlation, when performed on only the disagreed items, should show
no relation between the labels assigned by the two coders. For certain patterns in the
disagreement, however, a correlation would show up. (To see this, consider the case
where one coder tends to label rhetorical questions as yes/no-questions and the other
coder assigns both labels correctly: If this happens often enough, tests for association
would come up with a relation between the labels for the two coders for the disagreed
items.) If the test shows a correlation, the disagreements add patterns for the machine
learner to find. Unfortunately, the converse does not necessarily hold: It is possible
that not all patterns that could be picked up by a machine learner will show up in
correlations between disagreed items, for example because the amount of multiply-
annotated data is too small. The computational linguistics community therefore needs
to develop additional diagnostics for patterns in the coder disagreements.
It should go without saying that analysts will benefit from keeping how they
intend to use the data firmly in mind at all times. As Krippendorff (2004, page 429)
recommends, one should test reliability for the ?distinctions that matter? and perform
?suitable experiments of the effects of unreliable data on the conclusions.? Patterns
found for an overall coding scheme will not always affect every possible data use. For
instance, we often build classifiers not for complete coding schemes, but for some subset
of the labels or some ?class map? that transforms the scheme into a smaller set of classes.
In these cases, what is important is disagreement for the subset or transformation, not
the entire scheme. Similarly, where classifier performance is reported per class, the
325
Computational Linguistics Volume 34, Number 3
reliability for that particular label will be the most important. Finally, different machine-
learning algorithms may react differently to different kinds of patterns in the data and
to combinations of patterns in different relative strengths. In complicated cases, perhaps
the safest way to assess whether or not there is a problem with systematic disagreement
is to run a simulation like the one we have reported but with the kind and scale of
disagreement suspected of the data, and to use that to estimate the possible effects of
unreliable data on the performance of machine-learning algorithms.
Acknowledgments
We thank Rieks op den Akker, Ron Artstein,
and Bonnie Webber for discussions that have
helped us frame this article, as well as the
anonymous reviewers for their thoughtful
comments. This work is supported by the
European IST Programme Project FP6-033812
(AMIDA, publication 36). This article only
reflects the authors? views and funding
agencies are not liable for any use that may
be made of the information contained herein.
References
Aron, Arthur and Elaine N. Aron. 2003.
Statistics for Psychology. Prentice Hall,
Upper Saddle River, NJ.
Artstein, Ron and Massimo Poesio. 2005.
Bias decreases in proportion to the number
of annotators. In Proceedings of FG-MoL
2005, pages 141?150, Edinburgh.
Artstein, Ron and Massimo Poesio. In press.
Inter-coder agreement for computational
linguistics. Computational Linguistics.
Bayerl, Petra Saskia and Karsten Ingmar
Paul. 2007. Identifying sources of
disagreement: Generalizability theory
in manual annotation studies.
Computational Linguistics, 33(1):3?8.
Carletta, Jean C. 1996. Assessing agreement
on classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Cohen, Jacob. 1988. Statistical power analysis
for the behavioral sciences, 2nd edition.
Lawrence Erlbaum, Hillsdale, NJ.
Craggs, Richard and Mary McGee Wood.
2005. Evaluating discourse and dialogue
coding schemes. Computational Linguistics,
31(3):289?296.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Galley, Michel, Kathleen McKeown, Julia
Hirschberg, and Elizabeth Shriberg.
2004. Identifying agreement and
disagreement in conversational speech:
Use of Bayesian networks to model
pragmatic dependencies. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04),
pages 669?676, Barcelona.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to its Methodology,
volume 5 of The Sage CommText Series.
Sage Publications, London.
Krippendorff, Klaus. 2004. Reliability
in content analysis. Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411?433.
Marcu, Daniel, Estibaliz Amorrortu, and
Magdalena Romera. 1999. Experiments in
constructing a corpus of discourse trees.
In Marilyn Walker, editor, Towards
Standards and Tools for Discourse Tagging:
Proceedings of the Workshop. Association
for Computational Linguistics, Somerset,
NJ, pages 48?57.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann Publishers
Inc., San Francisco, CA.
Reithinger, Norbert and Michael Kipp.
1998. Large scale dialogue annotation in
Verbmobil. In Workshop Proceedings of
ESSLLI 98, pages 1?6, Saarbru?cken.
Shriberg, Elizabeth, Rebecca Bates, Paul
Taylor, Andreas Stolcke, Daniel Jurafsky,
Klaus Ries, Noah Coccaro, Rachel
Martin, Marie Meteer, and Carol Van
Ess-Dykema. 1998. Can prosody aid the
automatic classification of dialog acts in
conversational speech? Language and
Speech, 41(3-4):443?492.
Wiebe, Janyce M., Rebecca F. Bruce, and
Thomas P. O?Hara. 1999. Development
and use of a gold-standard data set
for subjectivity classifications. In
Proceedings of the 37th Annual Meeting
of the Association for Computational
Linguistics, pages 246?253, Morristown, NJ.
Witten, Ian H. and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools and
Techniques, 2nd edition. Morgan Kaufmann,
San Francisco, CA.
326

This article has been cited by:
1. Ron Artstein, Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics.
Computational Linguistics 34:4, 555-596. [Abstract] [PDF] [PDF Plus]
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 8?16
Manchester, August 2008
Exploiting ?Subjective? Annotations
Dennis Reidsma
Human Media Interaction
University of Twente, PO Box 217
NL-7500 AE, Enschede, The Netherlands
dennisr@ewi.utwente.nl
Rieks op den Akker
Human Media Interaction
University of Twente, PO Box 217
NL-7500 AE, Enschede, The Netherlands
infrieks@ewi.utwente.nl
Abstract
Many interesting phenomena in conversa-
tion can only be annotated as a subjec-
tive task, requiring interpretative judge-
ments from annotators. This leads to
data which is annotated with lower lev-
els of agreement not only due to errors in
the annotation, but also due to the differ-
ences in how annotators interpret conver-
sations. This paper constitutes an attempt
to find out how subjective annotations with
a low level of agreement can profitably
be used for machine learning purposes.
We analyse the (dis)agreements between
annotators for two different cases in a
multimodal annotated corpus and explic-
itly relate the results to the way machine-
learning algorithms perform on the anno-
tated data. Finally we present two new
concepts, namely ?subjective entity? clas-
sifiers resp. ?consensus objective? classi-
fiers, and give recommendations for using
subjective data in machine-learning appli-
cations.
1 Introduction
Research that makes use of multimodal annotated
corpora is always presented with something of a
dilemma. One would prefer to have results which
are reproducible and independent of the particular
annotators that produced the corpus. One needs
data which is annotated with as few disagreements
between annotators as possible. But labeling a cor-
pus is a task which involves a judgement by the an-
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
notator and is therefore, in a sense, always a sub-
jective task. Of course, for some phenomena those
judgements can be expected to come out mostly
the same for different annotators. For other phe-
nomena the judgements can be more dependent on
the annotator interpreting the behavior being anno-
tated, leading to annotations which are more sub-
jective in nature. The amount of overlap or agree-
ment between annotations is then also influenced
by the amount of intersubjectivity in the judge-
ments of annotators.
This relates to the spectrum of content types
discussed extensively by Potter and Levine-
Donnerstein (1999). One of the major distinctions
that they make is a distinction in annotation of
manifest content (directly observable events), pat-
tern latent content (events that need to be inferred
indirectly from the observations), and projective
latent content (loosely said, events that require a
subjective interpretation from the annotator).
Manifest content is what is directly observable.
Some examples are annotation of instances where
somebody raises his hand or raises an eyebrow,
annotation of the words being said and indicating
whether there is a person in view of the camera.
Annotating manifest content can be a relatively
easy task. Although the annotation task involves
a judgement by the annotator, those judgements
should not diverge a lot for different annotators.
At the other end of the spectrum we find pro-
jective latent content. This is a type of content
for which the annotation schema does not spec-
ify in extreme detail the rules and surface forms
that determine the applicability of classes, but in
which the coding relies on the annotators? exist-
ing mental conception1 of the classes. Such an ap-
1Potter and Levine-Donnerstein use the word ?mental
scheme? for this. We will use ?mental conceptions? in this
8
proach is useful for everyday concepts that most
people understand and to a certain extent share a
common meaning for, but for which it is almost
impossible to provide adequately complete defini-
tions. Potter and Levine-Donnerstein use the ex-
ample ?chair? for everyday concepts that are dif-
ficult to define exhaustively. But this concept is
also especially relevant in an application context
that requires the end user of the data to agree with
the distinctions being made. This is very important
when machine learning classifiers are developed
to be used in everyday applications. For exam-
ple, one can make a highly circumscribed, etholog-
ically founded definition of the class ?dominant? to
guide annotation. This is good for, e.g., research
into social processes in multiparty conversations.
However, in a scenario where an automatic classi-
fier, trained to recognize this class, is to be used
in an application that gives a participant in a meet-
ing a quiet warning when he is being too dominant
(Rienks, 2007) one would instead prefer the class
rather to fit the mental conceptions of dominance
that a ?naive? user may have. When one designs
an annotation scheme for projective latent content,
the focus of the annotation guidelines is on instruc-
tions that trigger the appropriate existing mental
conceptions of the annotators rather than on writ-
ing exhaustive descriptions of how classes can be
distinguished from each other (Potter and Levine-
Donnerstein, 1999).
Interannotator agreement takes on different
roles for the two ends of the spectrum. For mani-
fest content the level of agreement tells you some-
thing about how accurate the measurement in-
strument (schema plus coders) is. Bakeman and
Gottman, in their text book observing interaction:
introduction to sequential analysis (1986, p 57),
say about this type of reliability measurement that
it is a matter of ?calibrating your observers?. For
projective content, we have additional problems;
the level of agreement may be influenced by the
level of intersubjectivity, too. Where Krippen-
dorff (1980) describes that annotators should be
interchangeable, annotations of projective latent
content can sometimes say as much about the men-
tal conceptions of the particular annotator as about
the person whose interactions are being annotated.
The personal interpretations of the data by the an-
notator should not necessarily be seen as ?errors?,
though, even if those interpretations lead to low in-
paper to avoid confusion with the term ?annotation scheme?.
terannotator agreement: they may simply be an un-
avoidable aspect of the interesting type of data one
works with.
Many different sources of low agreement levels,
and many different solutions, are discussed in the
literature. It is important to note that some types of
disagreement are more systematic and other types
are more noise like. For projective latent con-
tent one would expect more consistent structure in
the disagreements between annotators as they are
caused by the differences in the personal ways of
interpreting multimodal interaction. Such system-
atic disagreements are particularly problematic for
subsequent use of the data, more so than noise-
like disagreements. Therefore, an analysis of the
quality of an annotated corpus should not stop at
presenting the value of a reliability metric; instead
one should investigate the patterns in the disagree-
ments and discuss the possible impact they have on
the envisioned uses of the data (Reidsma and Car-
letta, 2008). Some sources of disagreements are
the following.
(1) ?Clerical errors? caused by a limited view
of the interactions being annotated (low quality
video, no audio, occlusions, etc) or by slipshod
work of the annotator or the annotator misunder-
standing the instructions. Some solutions are to
provide better instructions and training, using only
good annotators, and using high quality recordings
of the interaction being annotated.
(2) ?Invalid or imprecise annotation schemas?
that contain classes that are not relevant or do not
contain classes that are relevant, or force the anno-
tator to make choices that are not appropriate to the
data (e.g. to choose one label for a unit where more
labels are applicable). Solutions concern redesign-
ing the annotation schema, for example by merg-
ing difference classes, allowing annotators to use
multiple labels, removing classes, or adding new
classes.
(3) ?Genuinely ambiguous expressions? as de-
scribed by Poesio and Artstein (2005). They dis-
cuss that disagreements caused by ambiguity are
not so easily solved.
(4) ?A low level of intersubjectivity? for the in-
terpretative judgements of the annotators, caused
by the fact that there is less than perfect overlap
between the mental conceptions of the annotators.
The solutions mentioned above for issue (2) partly
also apply here. However, in this article we focus
on an additional, entirely different, way of coping
9
with disagreements resulting from a low level of
intersubjectivity that actively exploits the system-
atic differences in the annotations caused by this.
1.1 Useful results from data with low
agreement
Data with a low interannotator agreement may be
difficult to use, but there are other fields where
partial solutions have been found to the problem,
such as the information retrieval evaluation confer-
ences (TREC). Relevance judgements in TREC as-
sessments (and document relevance in general) are
quite subjective and it is well known that agree-
ment for relevance judgements is not very high
(Voorhees and Harman report 70% three-way per-
cent agreement on 15,000 documents for three
assessors (1997)). Quite early in the history of
the TREC, Voorhees investigated what the conse-
quences of this low level of agreement are for the
usefulness of results obtained on the TREC collec-
tion. It turns out that specifying a few constraints2
is enough to be able to use the TREC assessments
to obtain meaningful evaluation results (Voorhees,
2000). Inspired by this we try to find ways of look-
ing at subjective data that tells us what constraints
and restrictions on the use of it follow from the pat-
terns in the disagreements between annotators, as
also advised by Reidsma and Carletta (2008).
1.2 Related Work
In corpus research there is much work with anno-
tations that need subjective judgements of a more
subjective nature from an annotator about the be-
havior being annotated. This holds for Human
Computer Interaction topics such as affective com-
puting or the development of Embodied Conversa-
tional Agents with a personality, but also for work
in computational linguistics on topics such as emo-
tion (Craggs and McGee Wood, 2005), subjectivity
(Wiebe et al, 1999; Wilson, 2008) and agreement
and disagreement (Galley et al, 2004).
If we want to interpret the results of classifiers in
terms of the patterns of (dis)agreement found be-
tween annotators, we need to subject the classifiers
with respect to each other and to the ?ground truth
data? to the same analyses used to evaluate and
compare annotators to each other. Vieira (2002)
and Steidl et al (2005) similarily remark that it
2Only discuss relative performance differences on differ-
ent (variations of) algorithms/systems run on exactly the same
set of assessments using the same set of topics.
is not ?fair? to penalize machine learning perfor-
mance for errors made in situations where humans
would not agree either. Vieira however only looks
at the amount of disagreement and does not explic-
itly relate the classes where the system and coders
disagree to the classes where the coders disagree
with each other. Steidl et al?s approach is geared
to data which is multiply coded for the whole cor-
pus (very expensive) and for annotations that can
be seen as ?additive?, i.e., where judgements are
not mutually exclusive.
Passonneau et al (2008) present an extensive
analysis of the relation between per-class machine
learning performance and interannotator agree-
ment obtained on the task of labelling text frag-
ments with their function in the larger text. They
show that overall high agreement can indicate a
high learnability of a class in a multiply annotated
corpus, but that the interannotator agreement is not
necessarily predictive of the learnability of a la-
bel from a single annotator?s data, especially in the
context of what we call projective latent content.
1.3 This Paper
This paper constitutes an attempt to find out how
subjective annotations, annotated with a low level
of agreement, can profitably be used for machine
learning purposes. First we present the relevant
parts of the corpus. Subsequently, we analyse the
(dis)agreements between annotators, on more as-
pects than just the value of a reliability metric, and
explicitly relate the results to the way machine-
learning algorithms perform on the annotated data.
Finally we present two new concepts that can be
used to explain and exploit this relation (?subjec-
tive entity? classifiers resp. ?consensus objective?
classifiers) and give some recommendations for
using subjective data in machine-learning applica-
tions.
2 From Agreement to Machine Learning
Performance
We used the hand annotated face-to-face conversa-
tions from the 100 hour AMI meeting corpus (Car-
letta, 2007). In the scenario-based AMI meetings,
design project groups of four players have the task
to design a new remote TV control. Group mem-
bers have roles: project manager (PM), industrial
designer (ID), user interface design (UD), and mar-
keting expert (ME). Every group has four meetings
(20-40 min. each), dedicated to a subtask. Most of
10
the time the participants sit at a square table.
The meetings were recorded in a meeting room
stuffed with audio and video recording devices,
so that close facial views and overview video, as
well as high quality audio is available. Speech
was transcribed manually, and words were time
aligned. The corpus has several layers of anno-
tation for several modalities, such as dialogue acts,
topics, hand gestures, head gestures, subjectivity,
visual focus of attention (FOA), decision points,
and summaries, and is easily extendible with new
layers. The dialogue act (DA) layer segments
speaker turns into dialogue act segments, on top of
the word layer, and they are labeled with one of 15
dialogue act type labels, following an annotation
procedure.
In this section we will inspect (dis)agreements
and machine learning performance for two cor-
pus annotation layers: the addressing annotations
(Jovanovic? et al, 2006) and for a particular type
of utterances in the corpus, the ?Yeah-utterances?
(Heylen and op den Akker, 2007).
2.1 Contextual Addressing
A part of the AMI corpus is also annotated with ad-
dressee information. Real dialogue acts (i.e. all di-
alogue acts but backchannels, stalls and fragments)
were assigned a label indicating who the speaker
addresses his speech to (is talking to). In these
type of meetings most of the time the speaker ad-
dresses the whole group, but sometimes his dia-
logue act is particularly addressed to some indi-
vidual (about 2743 of the 6590 annotated real dia-
logue acts); for example because he wants to know
that individual?s opinion. The basis of the con-
cept of addressing underlying the addressee an-
notation in the AMI corpus originates from Goff-
man (Goffman, 1981). The addressee is the par-
ticipant ?oriented to by the speaker in a manner
to suggest that his words are particularly for them,
and that some answer is therefore anticipated from
them, more so than from the other ratified partic-
ipants?. Sub-group addressing hardly occurs and
was not annotated. Thus, DAs are either addressed
to the group (G-addressed) or to an individual (I-
addressed) (see Jovanovic et al (2006)).
Another layer of the corpus contains focus of at-
tention information derived from head, body and
gaze observations (Ba and Odobez, 2006), so that
for any moment it is known whether a person is
looking at the table, white board, or some other
participant. Gaze and focus of attention are impor-
tant elements of addressing behavior, and therefore
FOA is a strong cue for the annotator who needs to
determine the addressee of an utterance. However,
FOA is not the only cue. Other relevant cues are,
for example, proper names and the use of address-
ing terms such as ?you?. Even when the gaze is
drawn to a projection screen, or the meeting is held
as a telephone conference without visuals, people
are able to make the addressee of their utterances
clear.
From an extensive (dis)agreement analysis of
the addressing and FOA layers the following con-
clusions can be summarized: the visual focus of
attention was annotated with a very high level of
agreement (Jovanovic?, 2007); in the addressee an-
notation there is a large confusion between DAs
being G-addressed or I-addressed; if the annota-
tors agree on an utterance being I-addressed they
typically also agree on the particular individual be-
ing addressed; ?elicit? DAs were easier to annotate
with addressee than other types of dialog act; and
reliability of addressee annotation is dependent on
the FOA context (Reidsma et al, 2008). When the
speaker?s FOA is not directed to any participant the
annotators must rely on other cues to determine the
addressee and will disagree a lot more than when
they are helped by FOA related cues. Some of
these disagreements can be due to systematic sub-
jective differences, e.g. an annotator being biased
towards the ?Group? label for utterances that are
answers to some question. Other disagreements
may be caused by the annotator being forced to
choose an addressee label for utterances that were
not be clearly addressed in the first place.
In this section we will not so much focus on
the subjectivity of the addressee annotation as on
the multimodal context in which annotators agree
more. Specifically, we will look further at the way
the level of agreement with which addressee has
been annotated is dependent on the FOA context
of a set of utterances. We expect this will be re-
flected directly by the machine learning perfor-
mance in these two contexts: the low agreement
might indicate a context where addressee is in-
herently difficult to determine and furthermore the
context with high agreement will result in annota-
tions containing more consistent information that
machine learning can model.
To verify this assumption we experimented with
automatic detection of the addressee of an utter-
11
ance based on lexical and multimodal features.
Compared to Jovanovic? (2007), we use a limited
set of features that does not contain local context
features such as ?previous addressee? or ?previous
dialogue act type?. Besides several lexical fea-
tures we also used features for focus of attention
of the speaker and listeners during the utterance.
Below we describe two experiments with this task.
Roughly 1 out of every 3 utterances is performed
in a context where the speaker?s FOA is not di-
rected at any other participant. This gives us three
contexts to train and to test on: all utterances, all
utterances where the speaker?s FOA is not directed
at any other participant (1/3 of the data) and all
utterances during which the speaker?s FOA is di-
rected at least once at another participant (2/3 of
the data).
First Experiment For the first experiment we
trained a Bayesian Network adapted from Jo-
vanovic? (2007) on a mix of utterances from all
contexts, and tested its performance on utterances
from the three different contexts: (1) all data, (2)
all data in the context ?at least some person in
speaker?s FOA? and (3) all data in the context ?no
person in speaker?s FOA during utterance?. As was
to be expected, the performance in the second con-
text showed a clear gain compared to the first con-
text, and the performance in the third context was
clearly worse. The performance differences, for
different train/test splits, tend to be about five per-
cent.
Second Experiment Because the second con-
text showed such a better performance, we ran a
second experiment where we trained the network
on only data from the second context, to see if
we could improve the performance in that context
even more. In different train/test splits this gave us
another small performance increase.
Conclusions for Contextual Addressing The
performance increases can mostly be attributed
to the distinction between different individual ad-
dressees for I-addressed utterances. Precision and
recall for the G-addressed utterances does not
change so much for the different contexts. This
result is reminiscent of the fact that when the an-
notators agreed on an utterance being I-addressed
they typically also agreed on the particular individ-
ual being addressed.
These results are particularly interesting in the
light of the high accuracy with which FOA was an-
notated. If this accuracy points at the possibility to
also achieve a high automatic recognition rate for
FOA we can exploit these results in a practical ap-
plication context by defining a addressee detection
module which only assigns an addressee to an ut-
terance in the second FOA context (FOA at some
participants), and in all other cases labels an utter-
ance as ?addressee cannot be determined?. Such
a detection module achieves a much higher preci-
sion than a module that tries to assign an addressee
label regardless; of course this happens at the cost
of recall.
2.2 Interannotator Training and Testing
Classifiers behave as they are trained. When two
annotators differ in the way they annotate, i.e. have
different ?mental conceptions? of the phenomenon
being annotated, we can expect that a classifier
trained on the data annotated by one annotator
behaves different from a classifier trained on the
other annotator?s data. As Rienks describes, this
property allows us to use all data in the corpus, in-
stead of just the multiply annotated part of it, for
analyzing differences between annotators (Rienks,
2007, page 105). We can expect that a classifier A
trained on data annotated by A will perform bet-
ter when tested on data annotated by A, than when
tested on data annotated by B. In other words, clas-
sifier A is geared towards modelling the ?mental
conception? of annotator A. In this section we will
try to find out whether it is possible to explicitly
tease apart the overlap and the differences in the
mental conceptions of the annotators as mirrored
in the behavior of classifiers, on a subjective anno-
tation task. Suppose that we build a Voting Clas-
sifier, based on the votes of a number of classifiers
each trained on a different annotator?s data. The
Voting Classifier only makes a decision when all
voters agree on the class label. How good will
the Voting Classifier perform? Is there any rela-
tion between the (dis)agreement of the voters, and
the (dis)agreement of the annotators? Will the re-
sulting Voting Classifier in some way embody the
overlap between the ?mental conceptions? of the
different annotators?
As an illustration and a test case for such a
Voting Classifier, we consider the human annota-
tions and automatic classification of a particular
type of utterances in the AMI corpus, the ?Yeah-
utterances?, utterances that start with the word
?yeah?.
12
class train-tot test-tot DH-train/test S9-train/test VK-train/test
bc 3043 1347 1393/747 670/241 980/359
as 3724 1859 1536/1104 689/189 1499/566
in 782 377 340/229 207/60 235/88
ot 1289 596 316/209 187/38 786/349
Table 1: Sizes of train and test data sets used and the distribution of class labels over these data sets for
the different annotators.
The Data Response tokens like ?yeah?, ?okay?,
?right? and ?no? have the interest of linguists be-
cause they may give a clue about the stance that the
listener takes towards what is said by the speaker
(Gardner, 2004). Jefferson described the differ-
ence between ?yeah? and other backchannels in
terms of speaker recipiency, the willingness of
the speaker to take the floor (Jefferson, 1984).
Yeah utterances make up a substantial part of the
dialogue acts in the AMI meeting conversations
(about eight percent). ?Yeah? is the most ambigu-
ous utterance that occurs in discussion segments in
AMI meetings. In order to get information about
the stance that participants take with respect to-
wards the issue discussed it is important to be able
to tell utterances of ?Yeah? as a mere backchannel,
from Yeah utterances that express agreement with
the opinion of the speaker (see the work of Heylen
and Op den Akker (2007)).
The class variables for dialogue act types of
Yeah utterances that are distinguished are: Assess
(as), Backchannel (bc), Inform (in), and Other (ot).
Table 1 gives a distribution of the labels in our
train and test data sets. Note that for each annota-
tor, a disjunct train and test set have been defined.
The inter-annotator agreement on the Yeah utter-
ances is low. The pairwise alpha values for meet-
ing IS1003d, which was annotated by all three an-
notators, are (in brackets the number of agreed DA
segments that start with ?Yeah?): alpha(VK,DH)
= 0.36 (111), alpha (VK,S9) = 0.36 (132), al-
pha(DH,S9) = 0.45 (160).
Testing for Systematic Differences When one
suspects the annotations to have originated from
different mental conceptions of annotators, the first
step is to test whether these differences are system-
atic. Table 2 presents the intra and inter annota-
tor classification accuracy. There is a clear perfor-
mance drop between using the test data from the
same annotator from which the training data was
taken and using the test data of other annotators
or the mixed test data of all annotators. This sug-
gest that some of the disagreements in the annota-
tion stem from systematic differences in the mental
conceptions of the annotators.
TEST
TRAIN DH S9 VK Mixed
DH 69 64 52 63
S9 59 68 48 57
VK 63 57 66 63
Table 2: Performance of classifiers (in terms of ac-
curacy values ? i.e. percentage correct predictions)
trained and tested on various data sets. Results
were obtained with a decision tree classifier, J48
in the Weka toolkit.
Building the Voting Classifier Given the three
classifiers DH, S9 and VK, each trained on the
train data taken from one single annotator, we have
build a Voting Classifier that outputs a class label
when all three ?voters? (the classifiers DH, S9 and
VK) give the same label, and the label ?unknown?
otherwise. As was to be expected, the accuracy
for this Voting Classifier is much lower than the
accuracy of each of the single voters and than the
accuracy of a classifier trained on a mix of data
from all annotators (see Table 3), due to the many
times the Voting Classifier assigns the label ?un-
known? which is not present in the test data and is
always false. The precision of the Voting Classi-
fier however is higher than that of any of the other
classifiers, for each of the classes (see Table 4).
Conclusions for the Voting Classifier For the
data that we used in this experiment, building a
Voting Classifier as described above gave us a high
precision classifier. Based on our starting point,
this would relate to the classifier in some way em-
bodying the overlap in the mental conceptions of
each of the annotators. If that were true, the cases
in which the Voting Classifier returns an unani-
mous vote would be mostly those cases in which
the different annotators would also have agreed.
13
TRAIN Accuracy
train MIX(8838) 67
DH(3585) 63
S9(1753) 57
VK(3500) 63
VotingClassifier(8838) 43
Table 3: Performance of the MaxEnt classifiers (in
terms of accuracy values ? i.e. percentage cor-
rect predictions) tested on the whole test set, a mix
of three annotators data (4179 ?Yeah? utterances).
The first column between brackets the size of the
train sets.
Classifier
Class Voting DH S9 VK train MIX
BC 71 65 63 71 69
AS 73 62 64 61 66
IN 60 58 34 52 50
OT 86 59 32 57 80
Table 4: Precision values per class label for the
classifiers.
This can be tested quite simply using multiply an-
notated data. Note that not all data needs to be
annotated by more annotators: just enough to test
this hypothesis. Otherwise, it will suffice to have
enough data for each single annotator, be it over-
lapping or not. This is especially advantageous
when the corpus is really large, such as the 100h
AMI corpus. Another way to test the hypothesis
that the voting behavior relates to intersubjectivity
is to look at the type and context of the agreements
between annotators, found in the reliability analy-
sis, and see if that relates to the type and context
of the cases where the Voting Classifier renders an
unanimous judgement. That would be strong cir-
cumstantial evidence in support of the hypothesis.
Note that the gain in precision is obtained at
the cost of recall, because the Voting Classifier ap-
proach explicitly restricts judgements to the cases
where annotators would have agreed and, presum-
ably, therefore to the cases in which users of the
data are able to agree to the judgements as well. It
is possible that you ?lose? a class label in the clas-
sifier by having a high precision but a recall of less
than five percent, which in our example happened
for the ?other? class.
3 The Classifier as Subjective Entity vs
the Classifier as Embodiment of
Consensus Objectivity
Many annotation tasks are subjective to a larger de-
gree. When this is simply taken as a given, and the
systematic disagreements resulting from the differ-
ent mental conceptions of the annotators are not
taken into account while training a machine classi-
fier on the resulting data, there is no simple reason
to assume that the resulting classifier is any less
subjective in the judgements it makes. Without ad-
ditional analyses one cannot suppose the classifier
did not pick up idiosyncrasies from the annotators.
We have seen that machine classifiers can indeed
considered to be subjective in their judgements, a
property they have inherited from the annotations
they have been trained on. A judgement made by
such a classifier should be approached in a simi-
lar manner as a judgement made by another per-
son3. We will call the resulting classifier therefore
a ?subjective entity? classifier.
A careful analysis of the interannotator agree-
ments and disagreements might make it possible
to build classifiers that partly embody the intersub-
jective overlap between the mental conceptions of
the annotators. Because the classifier only tries to
give a judgement in situations where one can ex-
pect annotators or users to agree, one can approach
the judgements made by the classifier as a ?com-
mon sense? of judgements that people can agree
on, despite the subjective quality of the annotation
task. We will call the resulting classifier a ?consen-
sus objective? classifier.
4 Discussion
In the Introduction we distinguished several uses
of data annotation using human annotators. The
analyses and research in this paper mainly con-
cerns the use of annotated data for the training
and development of automatic machine classifiers.
Ideally the annotation schema and the class labels
that are distinguished reflect the use that is made
of the output of the machine classifiers in some
particular application in which the classifier op-
erates as a module. Imagine for example a sys-
tem that detects when meeting participants are too
dominant and signals the chairman of the meet-
3On a side note, letting the machine classifiers judgments
be presented through an embodied conversational agent can
be a way to present this human-like subjectivity for the user
(Reidsma et al, 2007).
14
ing to prevent some participants being dissatisfied
with the decision making processes. Or, a clas-
sifier for addressee detection that signals remote
participants that they are addressed by the speaker.
The way that users of the system interpret the sig-
nals output by the classifier should correspond to
the meanings that were used by the annotators and
that were implemented in the classifier.
When there is a lot of disagreement in the an-
notations this should be taken into account for
machine learning if one does not want to obtain
a ?subjective entity? classifier, the judgements of
which the user will often disagree with. In Sec-
tion 2 we presented two ways to exploit such data
for building machine classifiers. Here we elabo-
rate a bit on a difference between the two cases re-
lating to the different causes of the inter-annotator
disagreement.
For the addressing annotations, the annotators
sometimes had problems with choosing between
G-addressed and I-addressed. The participants
in the conversation usually did not seem to have
any problem with that. There are only a few in-
stances in the data where the participants explic-
itly requested clarification. It is reasonable to ex-
pect that in cases where it really matters ? for the
conversational partners ? who is being addressed,
outside observers will not have a problem to iden-
tify this. Thus, in those cases where the annotators
had problems to decide upon the type of address-
ing there maybe was no reason for the participants
in the conversation to make that clear because it
simply was not an issue. The annotators were then
tripped by the fact that they were forced by the an-
notation guidelines to choose one addressee label.
In the dialogue act classification task something
additional is going on. Here we see that annota-
tors also have problems because many utterances
themselves are ambiguous or poly-interpretable.
Some annotator may prefer to call this act an as-
sess where an other prefers to call it an inform, and
both may have good reason to back up their choice.
A similar situation occurs in the case of the clas-
sification of Yeah utterances. The disagreements
then seem to be caused more explicitly by differ-
ing judgements of a conversational situation.
5 Conclusions
We have argued that dis-agreements between dif-
ferent observers of ?subjective content? is unavoid-
able and an intrinsic quality of the interpretation
and classification process of such type of content.
Any subdivision of these type of phenomena into a
predefined set of disjunct classes suffers from be-
ing arbitrary. There are always cases that can be-
long to this but also to that class. Analysis of an-
notations of the same data by different annotators
may reveal that there are differences in the deci-
sions they make, such as some personal preference
for one class over another.
Instead of throwing away the data as not being
valuable at all for machine learning purposes, we
have shown two ways to exploit such data, both
leading to high precision / low recall classifiers
that in some cases refuse to give a judgement. The
first way was based on the identification of subsets
of the data that show higher inter-annotator agree-
ment. When the events in these subsets can be
identified computationally the way is open to use
classifiers trained on these subsets. We have illus-
trated this with several subsets of addressing events
in the AMI meeting corpus and we have shown that
this leads to an improvement in the accuracy of the
classifiers. Precision is raised in case the classi-
fier refrains from making a decision in those situa-
tion that fall outside the subsets. The second way
is to train a number of classifiers, one for each of
the annotators data part of the corpus, and build
a Voting Classifier that only makes a decision in
case all classifiers agree on the class label. This
approach was illustrated by the problem of classi-
fication of the dialogue act type of Yeah-utterances
in the AMI corpus. The results show that the ap-
proach indeed leads to the expected improvement
in precision, at the cost of a lower recall, because
of the cases in which the classifier doesn?t make a
decision.
Acknowledgements
The authors are in debt to many people for many
fruitful discussions, most prominently Jean Car-
letta, Ron Artstein, Arthur van Bunningen, Hen-
ning Rode and Dirk Heylen. This work is sup-
ported by the European IST Programme Project
FP6-033812 (AMIDA, publication 136). This ar-
ticle only reflects the authors? views and funding
agencies are not liable for any use that may be
made of the information contained herein.
References
Ba, S. O. and J.-M. Odobez. 2006. A study on visual
focus of attention recognition from head pose in a
15
meeting room. In Renals, S. and S. Bengio, editors,
Proc. of the MLMI 2006, volume 4299 of Lecture
Notes in Computer Science, pages 75?87. Springer.
Bakeman, R. and J. M. Gottman. 1986. Observing
Interaction: An Introduction to Sequential Analysis.
Cambridge University Press.
Carletta, J. C. 2007. Unleashing the killer corpus:
experiences in creating the multi-everything AMI
meeting corpus. Language Resources and Evalua-
tion, 41(2):181?190.
Craggs, R. and M. McGee Wood. 2005. Evaluating
discourse and dialogue coding schemes. Computa-
tional Linguistics, 31(3):289?296.
Galley, M., K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and
disagreement in conversational speech: Use of
Bayesian networks to model pragmatic dependen-
cies. In Proc. of the 42nd Meeting of the ACL, pages
669?676. ACL.
Gardner, R. 2004. Acknowledging strong ties between
utterances in talk: Connections through right as a re-
sponse token. In Proceedings of the 2004 Confer-
ence of the Australian Linguistic Society, pages 1?
12.
Goffman, E. 1981. Footing. In Forms of Talk, pages
124?159. Philadelphia: University of Pennsylvania
Press.
Heylen, D. and H. op den Akker. 2007. Comput-
ing backchannel distributions in multi-party conver-
sations. In Cassell, J. and D. Heylen, editors, Proc.
of the ACL Workshop on Embodied Language Pro-
cessing, Prague, pages 17?24. ACL.
Jefferson, G. 1984. Notes on a systematic deploy-
ment of the acknowledgement tokens ?yeah? and
?mm hm?. Papers in Linguistics, 17:197?206.
Jovanovic?, N., H. op den Akker, and A. Nijholt. 2006.
A corpus for studying addressing behaviour in multi-
party dialogues. Language Resources and Evalua-
tion, 40(1):5?23.
Jovanovic?, N. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Phd thesis, University of Twente.
Krippendorff, K. 1980. Content Analysis: An Intro-
duction to its Methodology, volume 5 of The Sage
CommText Series. Sage Publications, Beverly Hills,
London.
Passonneau, R. J., T. Yano, T. Lippincott, and J. Kla-
vans. 2008. Relation between agreement mea-
sures on human labeling and machine learning per-
formance: Results from an art history image index-
ing domain. In Proc. of the LREC 2008.
Poesio, M. and R. Artstein. 2005. The reliability of
anaphoric annotation, reconsidered: Taking ambigu-
ity into account. In Proc. of the Workshop on Fron-
tiers in Corpus Annotations II: Pie in the Sky, pages
76?83, Ann Arbor, Michigan. ACL.
Potter, J. W. and D. Levine-Donnerstein. 1999. Re-
thinking validity and reliability in content analy-
sis. Journal of applied communication research,
27(3):258?284.
Reidsma, D. and J. C. Carletta. 2008. Reliability mea-
surement without limits. Computational Linguistics,
34(3).
Reidsma, D., Z. M. Ruttkay, and A. Nijholt, 2007.
Challenges for Virtual Humans in Human Comput-
ing, chapter 16, pages 316?338. Number 4451 in
LNAI: State of the Art Surveys. Springer Verlag,
Berlin/Heidelberg.
Reidsma, D., D. Heylen, and H. op den Akker. 2008.
On the contextual analysis of agreement scores. In
Proc. of the LREC Workshop on Multimodal Cor-
pora.
Rienks, R. J. 2007. Meetings in Smart Environments:
Implications of progressing technology. Phd thesis,
SIKS Graduate School / University of Twente, En-
schede, NL.
Steidl, S., M. Levit, A. Batliner, E. No?th, and H. Nie-
mann. 2005. ?of all things the measure is man? auto-
matic classification of emotion and intra labeler con-
sistency. In ICASSP 2005, International Conference
on Acoustics, Speech, and Signal Processing.
Vieira, R. 2002. How to evaluate systems against hu-
man judgment on the presense of disagreement? In
Proc. workshop on joint evaluation of computational
processing of Portugese at PorTAL 2002.
Voorhees, E. M. and D. Harman. 1997. Overview of
the trec-5. In Proc. of the Fifth Text REtrieval Con-
ference (TREC-5), pages 1?28. NIST.
Voorhees, E. M. 2000. Variations in relevance
judgments and the measurement of retrieval effec-
tiveness. Information Processing & Management,
36(5):697?716.
Wiebe, J. M., R. F. Bruce, and T. P. O?Hara. 1999.
Development and use of a gold-standard data set for
subjectivity classifications. In Proc. of the 37th An-
nual Meeting of the ACL, pages 246?253. ACL.
Wilson, T. 2008. Annotating subjective content in
meetings. In Proc. of the Language Resources and
Evaluation Conference (LREC-2008).
16
