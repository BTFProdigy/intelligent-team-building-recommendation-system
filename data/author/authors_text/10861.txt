Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106?111,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
In this paper, we describe the SemEval-2010
shared task on ?Linking Events and Their Par-
ticipants in Discourse?. This task is a variant
of the classical semantic role labelling task.
The novel aspect is that we focus on linking
local semantic argument structures across sen-
tence boundaries. Specifically, the task aims at
linking locally uninstantiated roles to their co-
referents in the wider discourse context (if such
co-referents exist). This task is potentially ben-
eficial for a number of NLP applications and
we hope that it will not only attract researchers
from the semantic role labelling community
but also from co-reference resolution and infor-
mation extraction.
1 Introduction
Semantic role labelling (SRL) has been defined as
a sentence-level natural-language processing task in
which semantic roles are assigned to the syntactic
arguments of a predicate (Gildea and Jurafsky, 2002).
Semantic roles describe the function of the partici-
pants in an event. Identifying the semantic roles of
the predicates in a text allows knowing who did what
to whom when where how, etc.
SRL has attracted much attention in recent years,
as witnessed by several shared tasks in Sense-
val/SemEval (Ma`rquez et al, 2007; Litkowski, 2004;
Baker et al, 2007; Diab et al, 2007), and CoNLL
(Carreras and Ma`rquez, 2004; Carreras and Ma`rquez,
2005; Surdeanu et al, 2008). The state-of-the-art
in semantic role labelling has now advanced so
much that a number of studies have shown that au-
tomatically inferred semantic argument structures
can lead to tangible performance gains in NLP ap-
plications such as information extraction (Surdeanu
et al, 2003), question answering (Shen and Lapata,
2007) or recognising textual entailment (Burchardt
and Frank, 2006).
However, semantic role labelling as it is currently
defined also misses a lot of information that would
be beneficial for NLP applications that deal with
text understanding (in the broadest sense), such as
information extraction, summarisation, or question
answering. The reason for this is that SRL has tra-
ditionally been viewed as a sentence-internal task.
Hence, relations between different local semantic ar-
gument structures are disregarded and this leads to a
loss of important semantic information.
This view of SRL as a sentence-internal task is
partly due to the fact that large-scale manual anno-
tation projects such as FrameNet1 and PropBank2
typically present their annotations lexicographically
by lemma rather than by source text. Furthermore,
in the case of FrameNet, the annotation effort did
not start out with the goal of exhaustive corpus an-
notation but instead focused on isolated instances of
the target words sampled from a very large corpus,
which did not allow for a view of the data as ?full-text
annotation?.
It is clear that there is an interplay between local
argument structure and the surrounding discourse
(Fillmore, 1977). In early work, Palmer et al (1986)
discussed filling null complements from context by
using knowledge about individual predicates and ten-
1http://framenet.icsi.berkeley.edu/
2http://verbs.colorado.edu/?mpalmer/
projects/ace.html
106
dencies of referential chaining across sentences. But
so far there have been few attempts to find links
between argument structures across clause and sen-
tence boundaries explicitly on the basis of semantic
relations between the predicates involved. Two no-
table exceptions are Fillmore and Baker (2001) and
Burchardt et al (2005). Fillmore and Baker (2001)
analyse a short newspaper article and discuss how
frame semantics could benefit discourse processing
but without making concrete suggestions of how to
model this. Burchardt et al (2005) provide a detailed
analysis of the links between the local semantic argu-
ment structures in a short text; however their system
is not fully implemented either.
In the shared task, we intend to make a first step
towards taking SRL beyond the domain of individual
sentences by linking local semantic argument struc-
tures to the wider discourse context. In particular, we
address the problem of finding fillers for roles which
are neither instantiated as direct dependents of our
target predicates nor displaced through long-distance
dependency or coinstantatiation constructions. Of-
ten a referent for an uninstantiated role can be found
in the wider context, i.e. in preceding or following
sentences. An example is given in (1), where the
CHARGES role (ARG2 in PropBank) of cleared is left
empty but can be linked to murder in the previous
sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the object of
jealousy are not overtly expressed as syntactic depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
NIs are also very frequent in clinical reports.
For example, in (3) the EXPERIENCER role of
?cough?, ?tachypnea?, and ?breathing? can be linked
to ?twenty-two month old?. Text mining systems in
the biomedical domain focus on extracting relations
between biomedical entities and information about
patients. It is important that these systems extract
information as accurately as possible. Thus, finding
co-referents for NIs is also very relevant for improv-
ing results on mining relations in biomedical texts.
(3) Twenty-two month old with history of recur-
rent right middle lobe infiltrate. Increased
cough, tachypnea, and work of breathing.
In the following sections we describe the task in
more detail. We start by providing some background
on null instantiations (Section 2). Section 3 gives an
overview of the task, followed by a description of
how we intend to create the data (Section 4). Sec-
tion 5 provides a short description of how null in-
stantiations could be resolved automatically given
the provided data. Finally, Section 6 discusses the
evaluation measures and we wrap up in Section 7.
2 Background on Null Instantiation
The theory of null complementation used here is the
one adopted by FrameNet, which derives from the
work of Fillmore (1986).3 Briefly, omissions of core
arguments of predicates are categorised along two
dimensions, the licensor and the interpretation they
receive. The idea of a licensor refers to the fact that
either a particular lexical item or a particular gram-
matical construction must be present for the omission
of a frame element (FE) to occur. For instance, the
omission of the agent in (4) is licensed by the passive
construction.
(4) No doubt, mistakes were made 0Protagonist.
The omission is a constructional omission because
it can apply to any predicate with an appropriate
semantics that allows it to combine with the passive
construction. On the other hand, the omission in (5)
is lexically specific: the verb arrive allows the Goal
to be unspecified but the verb reach, also a member
of the Arriving frame, does not.
(5) We arrived 0Goal at 8pm.
The above two examples also illustrate the second
major dimension of variation. Whereas, in (4) the
protagonist making the mistake is only existentially
bound within the discourse (instance of indefinite null
3Palmer et al?s (1986) treatment of uninstantiated ?essential
roles? is very similar (see also Palmer (1990)).
107
instantiation, INI), the Goal location in (5) is an entity
that must be accessible to speaker and hearer from
the discourse or its context (definite null instantiation,
DNI). Finally note that the licensing construction or
lexical item fully and reliably determines the interpre-
tation. Missing by-phrases always have an indefinite
interpretation and whenever arrive omits the Goal
lexically, the Goal has to be interpreted as definite,
as it is in (5).
The import of this classification to the task here
is that we will concentrate on cases of DNI whether
they are licensed lexically or constructionally.
3 Task Description
We plan to run the task in the following two modes:
Full Task For the full task we supply a test set in
which the target words are marked and labelled with
the correct sense (i.e. frame).4 The participants then
have to:
1. find the overt semantic arguments of the target
(role recognition)
2. label them with the correct role (role labelling)
3. recognize definite null instantiations and find
links to antecedents in the wider context (NI
linking)
NIs only In the second mode, participants will be
supplied with a test set which is annotated with gold
standard local semantic argument structure.5 The
task is then restricted to recognizing that a core role
is missing, ascertaining that it must have a definite
interpretation and finding a filler for it (i.e., sub-task
3 from the full task).
The full task and the null instantiation linking task
will be evaluated separately. By setting up a SRL
task, we expect to attract participants from the es-
tablished SRL community. Furthermore, by allow-
ing participants to only address the second task, we
4We supply the correct sense to ensure that all systems use
the same role inventory for each target (i.e., the role inventory
associated with the gold standard sense). This makes it easier
to evaluate the systems consistently with respect to role assign-
ments and null instantiation linking, which is our main focus.
5The training set is identical for both set-ups and will contain
the full annotation, i.e., frames, semantic roles and their fillers,
and referents of null instantiations in the wider context (see
Section 4 for details).
hope to also attract researchers from areas such as co-
reference resolution or information extraction who do
not want to implement a complete SRL system. We
also plan to provide the data with both FrameNet and
PropBank style annotations to encourage researchers
from both areas to take part.
4 Data
The data will come from one of Arthur Conan
Doyle?s fiction works. We chose fiction rather than
news because we believe that fiction texts with
a linear narrative generally contain more context-
resolvable null instantiations. They also tend to be
longer and have a simpler structure than news texts
which typically revisit the same facts repeatedly at
different levels of detail (in the so-called ?inverted
pyramid? structure) and which mix event reports with
commentary and evaluation, thus sequencing mate-
rial that is understood as running in parallel. Fiction
texts should lend themselves more readily to a first at-
tempt at integrating discourse structure into semantic
role labeling. We chose Conan Doyle?s work because
most of his books are not subject to copyright restric-
tions anymore, which allows us to freely release the
annotated data.
We plan to make the data sets available with both
FrameNet and PropBank semantic argument anno-
tation, so that participants can choose which frame-
work they want to work in. The annotations will
originally be made using FrameNet-style and will
later be mapped semi-automatically to PropBank an-
notations. The data set for the FrameNet version of
the task will be built at Saarland University, in close
co-operation with the FrameNet team in Berkeley.
We aim for the same density of annotation as is ex-
hibited by FrameNet?s existing full-text annotation6
and are currently investigating whether the semantic
argument annotation can be done semi-automatically,
e.g., by starting the annotation with a run of the Shal-
maneser role labeller (Erk and Pado?, 2006), whose
output is then corrected and expanded manually. To
ensure a high annotation quality, at least part of the
data will be annotated by two annotators and then
manually adjudicated. We also provide detailed an-
notation guidelines (largely following the FrameNet
6http://framenet.icsi.berkeley.edu/
index.php?option=com_wrapper&Itemid=84
108
guidelines) and any open questions are discussed in
a weekly annotation meeting.
For the annotation of null instantiations and their
links to the surrounding discourse we have to create
new guidelines as this is a novel annotation task. We
will adopt ideas from the annotation of co-reference
information, linking locally unrealised roles to all
mentions of the referents in the surrounding dis-
course, where available. We will mark only identity
relations but not part-whole or bridging relations be-
tween referents. The set of unrealised roles under
consideration includes only the core arguments but
not adjuncts (peripheral or extra-thematic roles in
FrameNet?s terminology). Possible antecedents are
not restricted to noun phrases but include all con-
stituents that can be (local) role fillers for some pred-
icate plus complete sentences (which can sometimes
fill roles such as MESSAGE).
The data-set for PropBank will be created by map-
ping the FrameNet annotations onto PropBank and
NomBank labels. For verbal targets, we use the Sem-
link7 mappings. For nominal targets, there is no
existing hand-checked mapping between FrameNet
and NomBank but we will explore a way of build-
ing a FrameNet - NomBank mapping at least for
eventive nouns indirectly with the help of Semlink.
This would take advantage of the fact that PropBank
verbs and eventive NomBank nouns both have a map-
ping to VerbNet classes, which are referenced also by
Semlink. Time permitting, non-eventive nouns could
be mapped manually. For FrameNet targets of other
parts of speech, in particular adjectives and prepo-
sitions, no equivalent PropBank-style counterparts
will be available. The result of the automatic map-
pings will be partly hand-checked. The annotations
resolving null instantiations need no adjustment.
We intend to annotate at least two data sets of
around 4,000 words. One set for testing and one for
training. Because we realise that the training set will
not be large enough to train a semantic role labelling
system on it, we permit the participants to boost the
training data for the SRL task by making use of the
existing FrameNet and PropBank corpora.8
7http://verbs.colorado.edu/semlink/
8This may require some genre adaption but we believe this is
feasible.
5 Resolving Null Instantiations
We conceive of null instantiation resolution as a three
step problem. First, one needs to determine whether a
core role is missing. This involves looking up which
core roles are overtly expressed and which are not.
In the second step, one needs to determine what
licenses an omission and what its interpretation is.
To do this, one can use rules and heuristics based on
various syntactic and lexical facts of English. As an
example of a relevant syntactic fact, consider that sub-
jects in English can only be omitted when licensed by
a construction. One such construction is the impera-
tive (e.g. Please, sit down). Since this construction
also specifies that the missing referent must be the
addressee of the speaker of the imperative, it is clear
what referent one has to try to find.
As for using lexical knowledge, consider omis-
sions of the Goods FE of the verb steal in the Theft
frame. FrameNet annotation shows that whenever
the Goods FE of steal is missing it is interpreted in-
definitely, suggesting that a new instance of the FE
being missing should have the same interpretation.
More evidence to the same effect can be derived us-
ing Ruppenhofer?s (2004) observation that the inter-
pretation of a lexically licensed omission is definite
if the overt instances of the FE have mostly definite
form (i.e. have definite determiners such as that, the ,
this), and indefinite if they are mostly indefinite (i.e.
have bare or indefinite determiners such as a(n) or
some). The morphology of overt instances of an FE
could be inspected in the FrameNet data, or if the
predicate has only one sense or a very dominant one,
then the frequencies could even be estimated from
unannotated corpora.
The third step is linking definite omissions to ref-
erents in the context. This linking problem could be
modelled as a co-reference resolution task. While
the work of Palmer et al (1986) relied on special
lexicons, one might instead want to learn information
about the semantic content of different role fillers
and then assess for each of the potential referents in
the discourse context whether their semantic content
is close enough to the expected content of the null
instantiated role.
Information about the likely fillers of a role can
be obtained from annotated data sets (e.g., FrameNet
or PropBank). For instance, typical fillers of the
109
CHARGES role of clear might be murder, accusa-
tions, allegations, fraud etc. The semantic content of
the role could then be represented in a vector space
model, using additional unannotated data to build
meaning vectors for the attested role fillers. Meaning
vectors for potential role fillers in the context of the
null instantiation could be built in a similar fashion.
The likelihood of a potential filler filling the target
role can then be modelled as the distance between the
meaning vector of the filler and the role in the vec-
tor space model (see Pado? et al (2008) for a similar
approach for semi-automatic SRL).
We envisage that the manually annotated null in-
stantiated data can be used to learn additionally
heuristics for the filler resolution task, such as in-
formation about the average distance between a null
instantiation and its most recent co-referent.
6 Evaluation
As mentioned above we allow participants to address
either the full role recognition and labelling task plus
the linking of null instantiations or to make use of
the gold standard semantic argument structure and
look only at the null instantiations. We also permit
systems to perform either FrameNet or PropBank
style SRL. Hence, systems can be entered for four
subtasks which will be evaluated separately:
? full task, FrameNet
? null instantiations, FrameNet
? full task, PropBank
? null instantiations, PropBank
The focus for the proposed task is on the null in-
stantiation linking, however, for completeness, we
also evaluate the standard SRL task. For role recogni-
tion and labelling we use a standard evaluation set-up,
i.e., for role recognition we will evaluate the accuracy
with respect to the manually created gold standard,
for role labelling we will evaluate precision, recall,
and F-Score.
The null instantiation linkings are evaluated
slightly differently. In the gold standard, we will iden-
tify referents for null instantiations in the discourse
context. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system should be given
credit if the null instantiation is linked to any of these
expressions. To achieve this we create equivalence
sets for the referents of null instantiations. If the null
instantiation is linked to any item in the equivalence
set, the link is counted as a true positive. We can then
define NI linking precision as the number of all true
positive links divided by the number of links made by
a system, and NI linking recall as the number of true
positive links divided by the number of links between
a null instantiation and its equivalence set in the gold
standard. NI linking F-Score is then the harmonic
mean between NI linking precision and recall.
Since it may sometimes be difficult to determine
the correct extend of the filler of an NI, we score
an automatic annotation as correct if it includes the
head of the gold standard filler in the predicted filler.
However, in order to not favour systems which link
NIs to excessively large spans of text to maximise the
likelihood of linking to a correct referent, we intro-
duce a second evaluation measure, which computes
the overlap (Dice coefficient) between the words in
the predicted filler (P) of a null instantiation and the
words in the gold standard one (G):
NI linking overlap = 2|P ?G||P |+ |G| (6)
Example (7) illustrates this point. The verb won in
the second sentence evokes the Finish competition
frame whose COMPETITION role is null instantiated.
From the context it is clear that the competition role
is semantically filled by their first TV debate (head:
debate) and last night?s debate (head: debate) in
the previous sentences. These two expressions make
up the equivalence set for the COMPETITION role in
the last sentence. Any system that would predict a
linkage to a filler that covers the head of either of
these two expressions would score a true positive for
this NI. However, a system that linked to last night?s
debate would have an NI linking overlap of 1 (i.e.,
2*3/(3+3)) while a system linking the whole second
sentence Last night?s debate was eagerly anticipated
to the NI would have an NI linking overlap of 0.67
(i.e., 2*3/(6+3))
(7) US presidential rivals Republican John
McCain and Democrat Barack Obama have
yesterday evening attacked each other over
110
foreign policy and the economy, in [their
first TV debate]Competition. [Last night?s
debate]Competition was eagerly anticipated.
Two national flash polls suggest that
[Obama]Competitor wonFinish competition
0Competition.
7 Conclusion
In this paper, we described the SemEval-2010 shared
task on ?Linking Events and Their Participants in
Discourse?. With this task, we intend to take a first
step towards viewing semantic role labelling not as a
sentence internal problem but as a task which should
really take the discourse context into account. Specif-
ically, we focus on finding referents for roles which
are null instantiated in the local context. This is po-
tentially useful for various NLP applications. We
believe that the task is timely and interesting for a
number of researchers not only from the semantic
role labelling community but also from fields such as
co-reference resolution or information extraction.
While our task focuses specifically on finding links
between null instantiated roles and the discourse con-
text, we hope that in setting it up, we can stimulate re-
search on the interaction between discourse structure
and semantic argument structure in general. Possible
future editions of the task could then focus on addi-
tional connections between local semantic argument
structures (e.g., linking argument structures that refer
to the same event).
8 Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant
PI 154/9-3 and the Cluster of Excellence Multimodal
Computing and Interaction (MMCI), respectively). Roser
Morante?s research is funded by the GOA project BIO-
GRAPH of the University of Antwerp.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt and A. Frank. 2006. Approximating textual
entailment with LFG and framenet frames. In Pro-
ceedings of the Second Recognising Textual Entailment
Workshop.
A. Burchardt, A. Frank, and M. Pinkal. 2005. Building
text meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
X. Carreras and Ll. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-04, pages 89?97.
X. Carreras and Ll. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-05, pages 152?164.
M. Diab, M. Alkhalifa, S. ElKateb, C. Fellbaum, A. Man-
souri, and M. Palmer. 2007. SemEval-2007 Task 18:
Arabic semantic labeling. In Proc. of SemEval-07.
K. Erk and S. Pado?. 2006. Shalmaneser - a flexible
toolbox for semantic role assignment. In Proceedings
of LREC-06.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics for
text understanding. In Proc. of the NAACL-01 Work-
shop on WordNet and Other Lexical Resources.
C.J. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In Antonio Zampolli,
editor, Fundamental Studies in Computer Science, No.
59, pages 55?88. North Holland Publishing.
C.J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual Meet-
ing of the Berkeley Liguistics Society.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
K. Litkowski. 2004. SENSEVAL-3 Task: Automatic
labeling of semantic roles. In Proc. of SENSEVAL-3.
L. Ma`rquez, L. Villarejo, M. A. Mart?`, and M. Taule`. 2007.
SemEval-2007 Task 09: Multilevel semantic annotation
of Catalan and Spanish. In Proceedings of SemEval-07.
S. Pado?, M. Pennacchiotti, and C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations by
leveraging verbal data. In Proceedings of Coling-2008.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, and J. Dowding. 1986. Recovering
implicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer. 2004. The interaction of valence and
information structure. Ph.d., University of California,
Berkeley, CA.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In Proc. of EMNLP-07.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate arguments structures for infor-
mation extraction. In Proceedings of ACL-2003.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of CoNLL-2008, pages 159?177.
111
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19?26,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Assessing the benefits of partial automatic pre-labeling for frame-semantic
annotation
Ines Rehbein and Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{rehbein,josefr,csporled}@coli.uni-sb.de
Abstract
In this paper, we present the results of an
experiment in which we assess the useful-
ness of partial semi-automatic annotation
for frame labeling. While we found no con-
clusive evidence that it can speed up human
annotation, automatic pre-annotation does
increase its overall quality.
1 Introduction
Linguistically annotated resources play a crucial
role in natural language processing. Many recent
advances in areas such as part-of-speech tagging,
parsing, co-reference resolution, and semantic role
labeling have only been possible because of the cre-
ation of manually annotated corpora, which then
serve as training data for machine-learning based
NLP tools. However, human annotation of linguis-
tic categories is time-consuming and expensive.
While this is already a problem for major languages
like English, it is an even bigger problem for less-
used languages.
This data acquisition bottleneck is a well-known
problem and there have been numerous efforts to
address it on the algorithmic side. Examples in-
clude the development of weakly supervised learn-
ing methods such as co-training and active learning.
However, addressing only the algorithmic side is
not always possible and not always desirable in all
scenarios. First, some machine learning solutions
are not as generally applicable or widely re-usable
as one might think. It has been shown, for example,
that co-training does not work well for problems
which cannot easily be factorized into two indepen-
dent views (Mueller et al, 2002; Ng and Cardie,
2003). Some active learning studies suggest both
that the utility of the selected examples strongly
depends on the model used for classification and
that the example pool selected for one model can
turn out to be sub-optimal when another model is
trained on it at a later stage (Baldridge and Os-
borne, 2004). Furthermore, there are a number of
scenarios for which there is simply no alternative
to high-quality, manually annotated data; for exam-
ple, if the annotated corpus is used for empirical
research in linguistics (Meurers and Mu?ller, 2007;
Meurers, 2005).
In this paper, we look at this problem from the
data creation side. Specifically we explore whether
a semi-automatic annotation set-up in which a hu-
man expert corrects the output of an automatic sys-
tem can help to speed up the annotation process
without sacrificing annotation quality.
For our study, we explore the task of frame-
semantic argument structure annotation (Baker et
al., 1998). We chose this particular task because it
is a rather complex ? and therefore time-consuming
? undertaking, and it involves making a number of
different but interdependent annotation decisions
for each instance to be labeled (e.g. frame as-
signment and labeling of frame elements, see Sec-
tion 3.1). Semi-automatic support would thus be of
real benefit.
More specifically, we explore the usefulness of
automatic pre-annotation for the first step in the an-
notation process, namely frame assignment (word
sense disambiguation). Since the available inven-
tory of frame elements is dependent on the cho-
sen frame, this step is crucial for the whole anno-
tation process. Furthermore, semi-automatic an-
notation is more feasible for the frame labeling
sub-task. Most automatic semantic role labeling
systems (ASRL), including ours, tend to perform
much better on frame assignment than on frame
role labeling and correcting an erroneously chosen
19
frame typically also requires fewer physical opera-
tions from the annotator than correcting a number
of wrongly assigned frame elements.
We aim to answer three research questions in our
study: First, we explore whether pre-annotation of
frame labels can indeed speed up the annotation
process. This question is important because frame
assignment, in terms of physical operations of the
annotator, is a relatively minor effort compared to
frame role assignment and because checking a pre-
annotated frame still involves all the usual men-
tal operations that annotation from scratch does.
Our second major question is whether annotation
quality would remain acceptably high. Here the
concern is that annotators might tend to simply go
along with the pre-annotation, which would lead to
an overall lower annotation quality than they could
produce by annotating from scratch.1 Depending
on the purpose for which the annotations are to be
used, trading off accuracy for speed may or may
not be acceptable. Our third research question con-
cerns the required quality of pre-annotation for it
to have any positive effect. If the quality is too low,
the annotation process might actually be slowed
down because annotations by the automatic system
would have to be deleted before the new correct
one could be made. In fact, annotators might ig-
nore the pre-annotations completely. To determine
the effect of the pre-annotation quality, we not only
compared a null condition of providing no prior
annotation to one where we did, but we in fact com-
pared the null condition to two different quality
levels of pre-annotation, one that reflects the per-
formance of a state-of-the-art ASRL system and
an enhanced one that we artificially produced from
the gold standard.
2 Related Work
While semi-automatic annotation is frequently em-
ployed to create labeled data more quickly (see,
e.g., Brants and Plaehn (2000)), there are compar-
atively few studies which systematically look at
the benefits or limitations of this approach. One
of the earliest studies that investigated the advan-
tages of manually correcting automatic annotations
for linguistic data was carried out by Marcus et
al. (1993) in the context of the construction of the
Penn Treebank. Marcus et al (1993) employed
1This problem is also known in the context of resources
that are collaboratively constructed via the web (Kruschwitz
et al, 2009)
a post-correction set-up for both part-of-speech
and syntactic structure annotation. For pos-tagging
they compared the semi-automatic approach to a
fully manual annotation. They found that the semi-
automatic method resulted both in a significant
reduction of annotation time, effectively doubling
the word annotation rate, and in increased inter-
annotator agreement and accuracy.
Chiou et al (2001) explored the effect of au-
tomatic pre-annotation for treebank construction.
For the automatic step, they experimented with two
different parsers and found that both reduce over-
all annotation time significantly while preserving
accuracy. Later experiments by Xue et al (2002)
confirmed these findings.
Ganchev et al (2007) looked at semi-automatic
gene identification in the biomedical domain. They,
too, experimented with correcting the output of an
automatic annotation system. However, rather than
employing an off-the-shelf named entity tagger,
they trained a tagger maximized for recall. The
human annotators were then instructed to filter the
annotation, rejecting falsely labeled expressions.
Ganchev et al (2007) report a noticeable increase
in speed compared to a fully manual set-up.
The approach that is closest to ours is that of
Chou et al (2006) who investigate the effect of au-
tomatic pre-annotation for Propbank-style semantic
argument structure labeling. However that study
only looks into the properties of the semi-automatic
set-up; the authors did not carry out a control study
with a fully manual approach. Nevertheless Chou
et al (2006) provide an upper bound of the savings
obtained by the semi-automatic process in terms
of annotator operations. They report a reduction in
annotation effort of up to 46%.
3 Experimental setup
3.1 Frame-Semantic Annotation
The annotation scheme we use is that of FrameNet
(FN), a lexicographic project that produces a
database of frame-semantic descriptions of English
vocabulary. Frames are representations of proto-
typical events or states and their participants in the
sense of Fillmore (1982). In the FN database, both
frames and their participant roles are arranged in
various hierarchical relations (most prominently,
the is-a relation).
FrameNet links these descriptions of frames with
the words and multi-words (lexical units, LUs) that
evoke these conceptual structures. It also docu-
20
ments all the ways in which the semantic roles
(frame elements, FEs) can be realized as syntactic
arguments of each frame-evoking word by labeling
corpus attestations. As a small example, consider
the Collaboration frame, evoked in English by lexi-
cal units such as collaborate.v, conspire.v, collabo-
rator.n and others. The core set of frame-specific
roles that apply include Partner1, Partner2, Partners
and Undertaking. A labeled example sentence is
(1) [The two researchers Partners] COLLAB-
ORATED [on many papers Undertaking].
FrameNet uses two modes of annotation: full-
text, where the goal is to exhaustively annotate
the running text of a document with all the differ-
ent frames and roles that occur, and lexicographic,
where only instances of particular target words used
in particular frames are labeled.
3.2 Pilot Study
Prior to the present study we carried out a pilot
experiment comparing manual and semi-automatic
annotation of different segments of running text.
In this experiment we saw no significant effect
from pre-annotation. Instead we found that the
annotation speed and accuracy depended largely
on the order in which the texts were annotated and
on the difficulty of the segments. The influence
of order is due to the fact that FrameNet has more
than 825 frames and each frame has around two to
five core frame elements plus a number of non-core
elements. Therefore even experienced annotators
can benefit from the re-occuring of frames during
the ongoing annotation process.
Drawing on our experiences with the first exper-
iment, we chose a different experimental set-up for
the present study. To reduce the training effect, we
opted for annotation in lexicographic mode, restrict-
ing the number of lemmas (and thereby frames)
to annotate, and we started the experiment with
a training phase (see Section 3.5). Annotating in
lexicographic mode also gave us better control over
the difficulty of the different batches of data. Since
these now consist of unrelated sentences, we can
control the distribution of lemmas across the seg-
ments (see Section 3.4).
Furthermore, since the annotators in our pi-
lot study had often ignored the error-prone pre-
annotation, in particular for frame elements, we de-
cided not to pre-annotate frame elements and to ex-
periment with an enhanced level of pre-annotation
to explore the effect of pre-annotation quality.
3.3 Annotation Set-Up
The annotators included the authors and three com-
putational linguistics undergraduates who have
been performing frame-semantic annotation for at
least one year. While we use FrameNet data, our
annotation set-up is different. The annotation con-
sists of decorating automatically derived syntactic
constituency trees with semantic role labels using
the Salto tool (Burchardt et al, 2006) (see Figure 1).
By contrast, in FrameNet annotation a chunk parser
is used to provide phrase type and grammatical rela-
tions for the arguments of the target words. Further,
FrameNet annotators need to correct mistakes of
the automatic grammatical analysis, unlike in our
experiment. The first annotation step, frame as-
signment, involves choosing the correct frame for
the target lemma from a pull down menu; the sec-
ond step, role assignment, requires the annotators
to draw the available frame element links to the
appropriate syntactic constituent(s).
The annotators performed their annotation on
computers where access to the FrameNet website,
where gold annotations could have been found, was
blocked. They did, however, have access to local
copies of the frame descriptions needed for the
lexical units in our experiment. As the overall time
needed for the annotation was too long to do in
one sitting, the annotators did it over several days.
They were instructed to record the time (in minutes)
that they took for the annotation of each annotation
session.
Our ASRL system for state-of-the-art pre-
annotation was Shalmaneser (Erk and Pado, 2006).
The enhanced pre-annotation was created by man-
ually inserting errors into the gold standard.
3.4 Data
We annotated 360 sentences exemplifying all the
senses that were defined for six different lemmas in
FrameNet release 1.3. The lemmas were the verbs
rush, look, follow, throw, feel and scream. These
verbs were chosen for three reasons. First, they
have enough annotated instances in the FN release
that we could use some instances for testing and
still be left with a set of instances sufficiently large
to train our ASRL system. Second,we knew from
prior work with our automatic role labeler that it
had a reasonably good performance on these lem-
mas. Third, these LUs exhibit a range of difficulty
in terms of the number of senses they have in FN
(see Table 1) and the subtlety of the sense distinc-
21
Figure 1: The Salto Annotation Tool
Instances Senses
feel 134 6
follow 113 3
look 185 4
rush 168 2
scream 148 2
throw 155 2
Table 1: Lemmas used
tions ? e.g. the FrameNet senses of look are harder
to distinguish than those of rush. We randomly
grouped our sentences into three batches of equal
size and for each batch we produced three versions
corresponding to our three levels of annotation.
3.5 Study design
In line with the research questions that we want
to address and the annotators that we have avail-
able, we choose an experimental design that is
amenable to an analysis of variance. Specifically,
we randomly assign our 6 annotators (1-6) to three
groups of two (Groups I-III). Each annotator expe-
riences all three annotation conditions, namely no
pre-annotation (N), state-of-the-art pre-annotation
(S), and enhanced pre-annotation (E). This is the
within-subjects factor in our design, all other fac-
tors are between subjects. Namely, each group was
randomly matched to one of three different orders
in which the conditions can be experienced (see
Table 2). The orderings are designed to control
for the effects that increasing experience may have
on speed and quality. While all annotators end up
labeling all the same data, the groups also differ
as to which batch of data is presented in which
condition. This is intended as a check on any inher-
1st 2nd 3rd Annotators
Group I E S N 5, 6
Group II S N E 2, 4
Group III N E S 1, 3
Table 2: Annotation condition by order and group
ent differences in annotation difficulty that might
exist between the data sets. Finally, to rule out
difficulties with unfamiliar frames and frame el-
ements needed for the lexical units used in this
study, we provided some training to the annota-
tors. In the week prior to the experiment, they were
given 240 sentences exemplifying all 6 verbs in all
their senses to annotate and then met to discuss any
questions they might have about frame or FE dis-
tinctions etc. These 240 sentences were also used
to train the ASRL system.
4 Results
In addition to time, we measured precision, recall
and f-score for frame assignment and semantic role
assignment for each annotator. We then performed
an analysis of variance (ANOVA) on the outcomes
of our experiment. Our basic results are presented
in Table 3. As can be seen and as we expected,
our annotators differed in their performance both
with regard to annotation quality and speed. Below
we discuss our results with respect to the research
questions named above.
4.1 Can pre-annotation of frame assignment
speed up the annotation process?
Not surprisingly, there are considerable differences
in speed between the six annotators (Table 3),
22
Precision Recall F t p
Annotator 1
94/103 91.3 94/109 86.2 88.68 75 N
99/107 92.5 99/112 88.4 90.40 61 E
105/111 94.6 105/109 96.3 95.44 65 S
Annotator 2
93/105 88.6 93/112 83.0 85.71 135 S
86/98 87.8 86/112 76.8 81.93 103 N
98/106 92.5 98/113 86.7 89.51 69 E
Annotator 3
95/107 88.8 95/112 84.8 86.75 168 N
103/110 93.6 103/112 92.0 92.79 94 E
99/113 87.6 99/113 87.6 87.60 117 S
Annotator 4
106/111 95.5 106/112 94.6 95.05 80 S
99/108 91.7 99/113 87.6 89.60 59 N
105/112 93.8 105/113 92.9 93.35 52 E
Annotator 5
104/110 94.5 (104/112) 92.9 93.69 170 E
91/103 88.3 (91/113) 80.5 84.22 105 S
96/100 96.0 (96/113) 85.0 90.17 105 N
Annotator 6
102/106 96.2 102/112 91.1 93.58 124 E
94/105 89.5 94/112 83.9 86.61 125 S
93/100 93.0 93/113 82.3 87.32 135 N
Table 3: Results for frame assignment: precision,
recall, f-score (F), time (t) (frame and role as-
signment), pre-annotation (p): Non, Enhanced,
Shalmaneser
which are statistically significant with p ? 0.05.
Focussing on the order in which the text segments
were given to the annotators, we observe a sig-
nificant difference (p ? 0.05) in annotation time
needed for each of the segments. With one ex-
ception, all annotators took the most time on the
text segment given to them first, which hints at an
ongoing training effect.
The different conditions of pre-annotation (none,
state-of-the-art, enhanced) did not have a signifi-
cant effect on annotation time. However, all anno-
tators except one were in fact faster under the en-
hanced condition than under the unannotated con-
dition. The one annotator who was not faster anno-
tated the segment with the enhanced pre-annotation
before the other two segments; hence there might
have been an interaction between time savings from
pre-annotation and time savings due to a training
effect. This interaction between training effect and
degree of pre-annotation might be one reason why
we do not find a significant effect between anno-
tation time and pre-annotation condition. Another
reason might be that the pre-annotation only re-
duces the physical effort needed to annotate the
correct frame which is relatively minor compared
to the cognitive effort of determining (or verifying)
the right frame, which is required for all degrees of
pre-annotation.
4.2 Is annotation quality influenced by
automatic pre-annotation?
To answer the second question, we looked at the
relation between pre-annotation condition and f-
score. Even though the results in f-score for the
different annotators vary in extent (Table 4), there is
no significant difference between annotation qual-
ity for the six annotators.
Anot1 Anot2 Anot3 Anot4 Anot5 Anot6
91.5 85.7 89.0 92.7 89.4 89.2
Table 4: Average f-score for the 6 annotators
Next we performed a two-way ANOVA (Within-
Subjects design), and crossed the dependent vari-
able (f-score) with the two independent vari-
ables (order of text segments, condition of pre-
annotation). Here we found a significant effect
(p ? 0.05) for the impact of pre-annotation on an-
notation quality. All annotators achieved higher
f-scores for frame assignment on the enhanced pre-
annotated text segments than on the ones with no
pre-annotation. With one exception, all annotators
also improved on the already high baseline for the
enhanced pre-annotation (Table 5).
Seg. Precision Recall f-score
Shalmaneser
A (70/112) 62.5 (70/96) 72.9 67.30
B (75/113) 66.4 (75/101) 74.3 70.13
C (66/113) 58.4 (66/98) 67.3 62.53
Enhanced Pre-Annotation
A (104/112) 92.9 (104/111) 93.7 93.30
B (103/112) 92.0 (103/112) 92.0 92.00
C (99/113) 87.6 (99/113) 87.6 87.60
Table 5: Baselines for automatic pre-annotation
(Shalmaneser) and enhanced pre-annotation
The next issue concerns the question of whether
annotators make different types of errors when pro-
vided with the different styles of pre-annotation.
We would like to know if erroneous frame assign-
ment, as done by a state-of-the-art ASRL will tempt
annotators to accept errors they would not make in
the first place. To investigate this issue, we com-
pared f-scores for each of the frames for all three
pre-annotation conditions with f-scores for frame
assignment achieved by Shalmaneser. The boxplot
in Figure 2 shows the distribution of f-scores for
each frame for the different pre-annotation styles
and for Shalmaneser. We can see that the same
23
Figure 2: F-Scores per frame for human annotators on different levels of pre-annotation and for Shal-
maneser
error types are made by human annotators through-
out all three annotation trials, and that these errors
are different from the ones made by the ASRL.
Indicated by f-score, the most difficult frames
in our data set are Scrutiny, Fluidic motion, Seek-
ing, Make noise and Communication noise. This
shows that automatic pre-annotation, even if noisy
and of low quality, does not corrupt human anno-
tators on a grand scale. Furthermore, if the pre-
annotation is good it can even improve the overall
annotation quality. This is in line with previous
studies for other annotation tasks (Marcus et al,
1993).
4.3 How good does pre-annotation need to be
to have a positive effect?
Comparing annotation quality on the automatically
pre-annotated texts using Shalmaneser, four out of
six annotators achieved a higher f-score than on the
non-annotated sentences. The effect, however, is
not statistically significant. This means that pre-
annotation produced by a state-of-the-art ASRL
system is not yet good enough a) to significantly
speed up the annotation process, and b) to improve
the quality of the annotation itself. On the positive
side, we also found no evidence that the error-prone
pre-annotation decreases annotation quality.
Most interestingly, the two annotators who
showed a decrease in f-score on the text segments
pre-annotated by Shalmaneser (compared to the
text segments with no pre-annotation provided)
had been assigned to the same group (Group I).
Both had first annotated the enhanced, high-quality
pre-annotation, in the second trial the sentences
pre-annotated by Shalmaneser, and finally the texts
with no pre-annotation. It might be possible that
they benefitted from the ongoing training, resulting
in a higher f-score for the third text segment (no
pre-annotation). For this reason, we excluded their
annotation results from the data set and performed
another ANOVA, considering the remaining four
annotators only.
Figure 3 illustrates a noticeable trend for the in-
teraction between pre-annotation and annotation
quality: all four annotators show a decrease in
annotation quality on the text segments without
pre-annotation, while both types of pre-annotation
(Shalmaneser, Enhanced) increase f-scores for hu-
man annotation. There are, however, differences
between the impact of the two pre-annotation types
on human annotation quality: two annotators show
better results on the enhanced, high-quality pre-
24
Figure 3: Interaction between pre-annotation and
f-score
annotation, the other two perform better on the
texts pre-annotated by the state-of-the-art ASRL.
The interaction between pre-annotation and f-score
computed for the four annotators is weakly signifi-
cant with p ? 0.1.
Next we investigated the influence of pre-
annotation style on annotation time for the four
annotators. Again we can see an interesting pat-
tern: The two annotators (A1, A3) who annotated
in the order N-E-S, both take most time for the
texts without pre-annotation, getting faster on the
text pre-processed by Shalmaneser, while the least
amount of time was needed for the enhanced pre-
annotated texts (Figure 4). The two annotators (A2,
A4) who processed the texts in the order S-N-E,
showed a continuous reduction in annotation time,
probably caused by the interaction of training and
data quality. These observations, however, should
be taken with a grain of salt, as they outline trends,
but due to the low number of annotators, could not
be substantiated by statistical tests.
4.4 Semantic Role Assignment
As described in Section 3.5, we provided pre-
annotation for frame assignment only, therefore
we did not expect any significant effects of the dif-
ferent conditions of pre-annotation on the task of
semantic role labeling. To allow for a meaningful
Figure 4: Interaction between pre-annotation and
time
comparison, the evaluation of semantic role assign-
ment was done on the subset of frames annotated
correctly by all annotators.
As with frame assignment, there are consid-
erable differences in annotation quality between
the annotators. In contrast to frame assignment,
here the differences are statistically significant
(p ? 0.05). Table 6 shows the average f-score
for each annotator on the semantic role assignment
task.
Anot1 Anot2 Anot3 Anot4 Anot5 Anot6
85.2 80.1 87.7 89.2 82.5 84.3
Table 6: Average f-scores for the 6 annotators
As expected, neither the condition of pre-
annotation nor the order of text segments had any
significant effect on the quality of semantic role
assignment.2
5 Conclusion and future work
In the paper we presented experiments to assess
the benefits of partial automatic pre-annotation on
a frame assignment (word sense disambiguation)
task. We compared the impact of a) pre-annotations
2The annotation of frame and role assignment was done as
a combined task, therefore we do not report separate results
for annotation time for semantic role assignment.
25
provided by a state-of-the-art ASRL, and b) en-
hanced, high-quality pre-annotation on the annota-
tion process. We showed that pre-annotation has
a positive effect on the quality of human annota-
tion: the enhanced pre-annotation clearly increased
f-scores for all annotators, and even the noisy, error-
prone pre-annotations provided by the ASRL sys-
tem did not lower the quality of human annotation.
We suspect that there is a strong interaction
between the order in which the text segments
are given to the annotators and the three annota-
tion conditions, resulting in lower f-scores for the
group of annotators who processed the ASRL pre-
annotations in the first trial, where they could not
yet profit from the same amount of training as the
other two groups.
The same problem occurs with annotation time.
We have not been able to show that automatic
pre-annotation speeds up the annotation process.
However, we suspect that here, too, the interaction
between training effect and annotation condition
made it difficult to reach a significant improve-
ment. One way to avoid the problem would be a
further split of the test data, so that the different
types of pre-annotation could be presented to the
annotators at different stages of the annotation pro-
cess. This would allow us to control for the strong
bias through incremental training, which we can-
not avoid if one group of annotators is assigned
data of a given pre-annotation type in the first trial,
while another group encounters the same type of
data in the last trial. Due to the limited number
of annotators we had at our disposal as well as
the amount of time needed for the experiments we
could not sort out the interaction between order
and annotation conditions. We will take this issue
up in future work, which also needs to address the
question of how good the automatic pre-annotation
should be to support human annotation. F-scores
for the enhanced pre-annotation provided in our
experiments were quite high, but it is possible that
a similar effect could be reached with automatic
pre-annotations of somewhat lower quality.
The outcome of our experiments provides strong
motivation to improve ASRL systems, as automatic
pre-annotation of acceptable quality does increase
the quality of human annotation.
References
C. F. Baker, C. J. Fillmore, J. B. Lowe. 1998. The
berkeley framenet project. In Proceedings of the
17th international conference on Computational lin-
guistics, 86?90, Morristown, NJ, USA. Association
for Computational Linguistics.
J. Baldridge, M. Osborne. 2004. Active learning
and the total cost of annotation. In Proceedings of
EMNLP.
T. Brants, O. Plaehn. 2000. Interactive corpus annota-
tion. In Proceedings of LREC-2000.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?.
2006. SALTO ? a versatile multi-level annotation
tool. In Proceedings of LREC.
F.-D. Chiou, D. Chiang, M. Palmer. 2001. Facilitat-
ing treebank annotation using a statistical parser. In
Proceedings of HLT-2001.
W.-C. Chou, R. T.-H. Tsai, Y.-S. Su, W. Ku, T.-Y. Sung,
W.-L. Hsu. 2006. A semi-automatic method for an-
notation a biomedical proposition bank. In Proceed-
ings of FLAC-2006.
K. Erk, S. Pado. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC, Genoa, Italy.
C. J. F. C. J. Fillmore, 1982. Linguistics in the Morning
Calm, chapter Frame Semantics, 111?137. Hanshin
Publishing, Seoul, 1982.
K. Ganchev, F. Pereira, M. Mandel, S. Carroll, P. White.
2007. Semi-automated named entity annotation.
In Proceedings of the Linguistic Annotation Work-
shop, 53?56, Prague, Czech Republic. Association
for Computational Linguistics.
U. Kruschwitz, J. Chamberlain, M. Poesio. 2009. (lin-
guistic) science through web collaboration in the
ANAWIKI project. In Proceedings of WebSci?09.
M. P. Marcus, B. Santorini, M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
W. D. Meurers, S. Mu?ller. 2007. Corpora and syntax
(article 44). In A. Lu?deling, M. Kyto?, eds., Corpus
linguistics. Mouton de Gruyter, Berlin.
W. D. Meurers. 2005. On the use of electronic cor-
pora for theoretical linguistics. case studies from
the syntax of german. Lingua, 115(11):1619?
1639. http://purl.org/net/dm/papers/
meurers-03.html.
C. Mueller, S. Rapp, M. Strube. 2002. Applying co-
training to reference resolution. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, 352?359, Philadelphia, Penn-
sylvania, USA. Association for Computational Lin-
guistics.
V. Ng, C. Cardie. 2003. Bootstrapping corefer-
ence classifiers with multiple machine learning algo-
rithms. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003).
N. Xue, F.-D. Chiou, M. Palmer. 2002. Building a
large-scale annotated chinese corpus. In Proceed-
ings of Coling-2002.
26
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801?808
Manchester, August 2008
Discourse Level Opinion Interpretation ?
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Josef Ruppenhofer
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
josefr@cs.pitt.edu
Abstract
This work proposes opinion frames as a
representation of discourse-level associa-
tions which arise from related opinion top-
ics. We illustrate how opinion frames help
gather more information and also assist
disambiguation. Finally we present the re-
sults of our experiments to detect these as-
sociations.
1 Introduction
Opinions have been investigated at the phrase, sen-
tence, and document levels. However, little work
has been carried out regarding interpreting opin-
ions at the level of the discourse.
Consider the following excerpt from a dialog
about designing a remote control for a television
(the opinion targets ? what the opinions are about
? are shown in italics).
(1) D :: And I thought not too edgy and like a box, more
kind of hand-held not as computery, yeah, more or-
ganic shape I think. Simple designs, like the last one
we just saw, not too many buttons.
Speaker D expresses an opinion in favor of a
design that is simple and organic in shape, and
against an alternative design which is not. Several
individual opinions are expressed in this passage.
The first is a negative opinion about the design be-
ing too edgy and box-like, the next is a positive
opinion toward a hand-held design, followed by a
negative opinion toward a computery shape, and
so on. While recognizing individual expressions
?This research was supported in part by the Department of
Homeland Security under grant N000140710152.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of opinions and their properties is important, dis-
course interpretation is needed as well. It is by un-
derstanding the passage as a discourse that we see
edgy, like a box, computery, and many buttons as
descriptions of the type of design D does not pre-
fer, and hand-held, organic shape, and simple de-
signs as descriptions of the type he does. These de-
scriptions are not in general synonyms/antonyms
of one another; for example, there are hand-held
?computery? devices and simple designs that are
edgy. The unison/opposition among the descrip-
tions is due to how they are used in the discourse.
This paper focuses on such relations between
the targets of opinions in discourse. Specifically, in
this work, we propose a scheme of opinion frames,
which consist of two opinions that are related by
virtue of having united or opposed targets. We
argue that recognizing opinion frames will pro-
vide more opinion information for NLP applica-
tions than recognizing individual opinions alone.
Further, if there is uncertainty about any one of the
components, we believe opinion frames are an ef-
fective representation incorporating discourse in-
formation to make an overall coherent interpreta-
tion (Hobbs et al, 1993). Finally, we also report
the first results of experiments in recognizing the
presence of these opinion frames.
We introduce our data in Section 2, present
opinion frames in Section 3 and illustrate their util-
ity in Section 4. Our experiments are in Section 5,
related work is discussed in Section 6, and conclu-
sions are in Section 7.
2 Data
The data used in this work is the AMI meet-
ing corpus (Carletta et al, 2005) which con-
tains multi-modal recordings of group meetings.
Each meeting has rich transcription and seg-
801
ment (turn/utterance) information for each speaker.
Each utterance consists of one or more sentences.
We also use some of the accompanying manual an-
notations (like adjacency pairs) as features in our
machine learning experiments.
3 Opinion Frames
In this section, we lay out definitions relating to
opinion frames, illustrate with examples how these
are manifested in our data, and consider them in
the context of discourse relations.
3.1 Definitions
The components of opinion frames are individual
opinions and the relationships between their tar-
gets. Following (Wilson and Wiebe, 2005; So-
masundaran et al, 2007), we address two types of
opinions, sentiment and arguing.
Sentiment includes positive and negative eval-
uations, emotions, and judgments. Arguing in-
cludes arguing for or against something, and argu-
ing that something should or should not be done.
Opinions have a polarity that can be positive or
negative. 1 The target of an opinion is the entity or
proposition that the opinion is about. We establish
relations between targets, in the process relating
their respective opinions. We address two types of
relations, same and alternative.
The same relation holds between targets that
refer to the same entity, property, or proposi-
tion. Observing the relations marked by an-
notators, we found that same covers not only
identity, but also part-whole, synonymy, gener-
alization, specialization, entity-attribute, instan-
tiation, cause-effect, epithets and implicit back-
ground topic, i.e., relations that have been studied
by many researchers in the context of anaphora and
co-reference (e.g. (Clark, 1975; Vieira and Poe-
sio, 2000; Mueller and Strube, 2001)). Actually,
same relations holding between entities often in-
volve co-reference (where co-reference is broadly
conceived to include relations such as part-whole
listed above). However, there are no morpho-
syntactic constraints on what targets may be. Thus,
same relations may also hold between adjective
phrases, verb phrases, and clauses. An instance of
this is Example 1, where the same target relation
holds between the adjectives edgy and computery.
1Polarity can also be neutral or both (Wilson and Wiebe,
2005), but these values are not significant for our opinion
frames.
SPSPsame, SNSNsame, APAPsame, ANANsame,
SPAPsame, APSPsame, SNANsame, ANSNsame,
SPSNalt, SNSPalt, APANalt, ANAPalt,
SPANalt, SNAPalt, APSNalt, ANSPalt
SPSNsame, SNSPsame, APANsame, ANAPsame,
SPANsame, APSNsame, SNAPsame, ANSPsame,
SPSPalt, SNSNalt, APAPalt, ANANalt,
SPAPalt, SNANalt, APSPalt, ANSNalt
Table 1: Opinion Frames
The alternative relation holds between targets
that are related by virtue of being opposing (mu-
tually exclusive) options in the context of the dis-
course. For example, in the domain of TV remote
controls, the set of all shapes are alternatives to
one another, since a remote control may have only
one shape at a time. In such scenarios, a positive
opinion regarding one choice may imply a nega-
tive opinion toward competing choices, and vice
versa. Objects appear as alternatives via world and
domain knowledge (for example, shapes of a re-
mote); the context of the discourse (for example,
Hillary Clinton and Barak Obama are alternatives
in discussions of the primaries, but not in discus-
sions of the general election); and the way the ob-
jects are juxtaposed while expressing opinions (for
instance hand-held and computery in Example 1).
While same and alternative are not the only pos-
sible relations between targets, they commonly oc-
cur in task-oriented dialogs such as those in the
data we use.
Now that we have all the ingredients, we can
define opinion frames. An opinion frame is de-
fined as a structure composed of two opinions and
their respective targets connected via their target
relations. With four opinion type/polarity pairs
(SN,SP,AN,AP), for each of two opinion slots, and
two possible target relations, we have 4 * 4 * 2 =
32 types of frame, listed in Table 1.
3.2 Examples
We will now illustrate how the frames are applied
with the following meeting snippets from the AMI
meeting corpus. In our examples, the lexical an-
chors revealing the opinion type (as the words are
interpreted in context) are indicated in bold face.
The text span capturing the target of the opinion
(as interpreted in context) is indicated in italics. To
make it easier to understand the opinion frames,
we separately list each opinion, followed by the
major relation between the targets and, in paren-
theses, the relevant subtype of the major relation.
In the passage below, the speaker D expresses
802
his preferences about the material for the TV re-
mote.
(2) D:: ... this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot.
A bit more durable and that 2 can also be ergonomic
and it kind of feels a bit different from all the other
remote controls.
Opinion Span - target Span Type
O1 bit more bouncy - it?s [t1] SP
O2 bit more durable - ellipsis [t2] SP
O3 ergonomic - that [t3] SP
O4 a bit different from all the other remote - it [t4] SP
Target - target Rel
t1 - t2 same (ellipsis)
t3 - t4 same (identity)
t1 - t3 same (identity)
The speaker?s positive sentiment regarding the
rubbery material is apparent from the text spans
bit more bouncy (Sentiment Positive or SP), bit
more durable (SP), ergonomic (SP) and a bit dif-
ferent from all the other remote controls (SP).
As shown, the targets of these opinions (it?s [t1],
that [t3], and it [t4]) are related by the same rela-
tion. The ellipsis occurs with bit more durable.
Target [t2] represents the (implicit) target of that
opinion, and [t2] has a same relation to [t1], the
target of the bit more bouncy opinion. The opin-
ion frames occurring throughout this passage are
all SPSPsame denoting that both the opinion com-
ponents are sentiments with positive polarity with
a same relation between their targets. One frame
occurs between O1 and O2, another between O3
and O4, and so on.
Example 2 illustrates relatively simple same re-
lations between targets. Now let us consider the
more involved passage below, in which a meeting
participant analyzes two leading remotes on the
market.
(3) D:: These are two leading remote controls at the mo-
ment. You know they?re grey, this one?s got loads of
buttons, it?s hard to tell from here what they actually
do, and they don?t look very exciting at all.
Opinion Span - target Span Rel
O1 leading - remote controls [t1] SP
O2 grey - they [t2] SN
O3 loads of buttons - this one [t3] SN
O4 hard to tell - they [t4] SN
O5 don?t look very exciting at all - they [t5] SN
Target - target Rel
t1 - t2 same (identity)
t2 - t3 same (t3 subset of t2)
2Note that the ?that? refers to the property of being
durable; however, as our annotation scheme is not hierarchi-
cal, we connect it to the entity the opinion is about ? in this
case the rubbery material.
t3 - t4 same (t4 partof t3)
t5 - t1 same (identity)
Target [t2] is the set of two leading remotes; [t3],
which is in a same relation with [t2], is one of those
remotes. Target [t4], which is also in a same rela-
tion with [t3], is a part of that remote, namely its
buttons. Thus, opinion O3 is directly about one of
the remotes, and indirectly about the set of both re-
motes. Similarly, O4 is directly about the buttons
of one of the remotes, and indirectly about that re-
mote itself. The assessments at different levels ac-
crue toward the analysis of the main topic under
consideration.
Moving on to alternative (alt) relations, con-
sider the passage below, where the speaker is ar-
guing for the curved shape.
(4) C:: . . . shapes should be curved, so round shapes.
Nothing square-like.
.
.
.
C:: . . . So we shouldn?t have too square corners and
that kind of thing.
B:: Yeah okay. Not the old box look.
Opinion Span - target Span Rel
O1 should be - curved [t1] AP
O2 Nothing - square-like [t2] AN
O3 shouldn?t have - square corners [t3] AN
O4 too - square corners [t3] SN
O5 Not - the old box look [t4] AN
O6 the old box look - the old box look [t4] SN
Target - target Rel
t1 -t2 alternatives
t2 - t3 same (specification)
t3 - t4 same (epithet)
Opinion O1 argues for a curved shape, O2 ar-
gues against a square shape, and O3 argues against
square corners. Note that square corners is also
the target of a negative sentiment, O4, expressed
here by too. Opinion O5 argues against the old
box look. In addition, the wording old box look
implies a negative sentiment ? O6 (we list the tar-
get span as ?old box look,? which refers to the look
of having square corners).
There is an alt relation between [t1] and [t2].
Thus, we have an opinion frame of type APANalt
between O1 and O2. From this frame, we are able
to understand that a positive opinion is expressed
toward something and a negative opinion is ex-
pressed toward its alternative.
3.3 Link Transitivity
When individual targets are linked, they form a
chain-like structure. Due to this, a connecting path
may exist between targets that were not directly
803
linked by the human annotators. This path can be
traversed to create links between new pairs of tar-
gets, which in turn results in new opinion frame
relations.
Let us illustrate this idea with Example 4. The
frames with direct relations are O1O2 APANalt.
By following the alt link from [t1] to [t2] and the
same link from [t2] to [t3], we have an alt link
between [t1] and [t3], and the additional frames
O1O3 APANalt and O1O4 APSNalt. Repeating
this process would finally link speaker C?s opinion
O1 with B?s opinion O6 via a APSNalt frame.
Simple recipes such as this can be used by ap-
plications such as QA to gather more information
from the discourse.
3.4 Frame Types
In our corpus, we found that the 32 frames of Ta-
ble 1 can be categorized into two functional types:
reinforcing frames and non-reinforcing frames.
The set of frames that occur in scenarios where
the speaker intends to fortify or reinforce his opin-
ion/stance are called reinforcing frames. These
are the ones in the top row of the Table 1. Note that
these frames cover all opinion types, polarities and
target relations. It is the particular combination of
these frame components that bring about the rein-
forcement of the opinion in the discourse.
On the other hand, the frames at the bottom row
of the table are non-reinforcing. In our corpus,
these frames occur when a speaker is ambivalent
or weighing pros and cons.
Example 2 is characterized by opinion frames
in which the opinions reinforce one another ? that
is, individual positive sentiments (SP) occurring
throughout the passage fortify the positive regard
for the rubbery material via the same target rela-
tions and the resulting SPSPsame frames.
Interestingly, interplays among different opin-
ion types may show the same type of reinforce-
ment. For instance, Example 4 is characterized by
mixtures of opinion types, polarities, and target re-
lations. However, the opinions are still unified in
the intention to argue for a particular type of shape.
3.5 Discourse Relations and Opinion Frames
Opinion-frame recognition and discourse interpre-
tation go hand in hand; together, they provide
richer overall interpretations. For example, con-
sider the opinion frames and the Penn Discourse
Treebank relations (Prasad et al, 2007) for Ex-
ample 2. PDTB would see a list or conjunction
relation between the clauses containing opinions
bit more durable (O2) and ergonomic (O3), as
well as between the clauses containing opinions
ergonomic (O3) and a bit different from all the
other remote controls (O4). All of our opinion
frames for this passage are of type SPSPsame, a
reinforcing frame type. This passage illustrates
the case in which discourse relations nicely corre-
spond to opinion frames. The opinion frames flesh
out the discourse relations: we have lists specifi-
cally of positive sentiments toward related objects.
However, opinion-frame and discourse-relation
schemes are not redundant. Consider the following
three passages.
(e1) Non-reinforcing opinion frame (SNSPsame); Con-
trast discourse relation
D:: . . . I draw for you this schema that can be maybe
too technical for you but is very important for me
. . ..
(e2) Reinforcing opinion frame (SNAPalt); Contrast
discourse relation
D:: not too edgy and like a box, more kind of hand-
held
(e3) Reinforcing opinion frame (SPSPsame); no dis-
course relation
. . . they want something that?s easier to use straight
away, more intuitive perhaps.
In both e1 and e2, the discourse relation be-
tween the two opinions is contrast (?too technical?
is contrasted with ?very important?, and ?not too
edgy and like a box? is contrasted with ?more kind
of hand-held?). However, the opinion frame in e1
is SNSPsame, which is a non-reinforcing frame,
while the opinion frame in e2 is SNAPalt, which
is a reinforcing frame. In e3, the opinion frame
holds between targets within a subordinated clause
(easier to use and more intuitive are two desired
targets); most discourse theories don?t predict any
discourse relation in this situation.
Generally speaking, we find that there are not
definitive mappings between opinion frames and
the relations of popular discourse theories. For ex-
ample, Hobbs? (Hobbs et al, 1993) contrast cov-
ers at least four of our frames (SPSPalt, APAPalt,
APANsame, SPSNsame), while, for instance, our
SPSPsame frame can map to both the elaboration
and explanation relations.
4 Benefits of Discourse Opinion Frames
This section argues for two motivations for opinion
frames: they may unearth additional information
over and above the individual opinions stated in
the text, and they may contribute toward arriving
804
Positive Negative
Counting only individual opinions
Accepted Items 120 20
Rejected Items 9 12
individual + opinions via Reinforcing Opinion frames
Accepted Items 252 63
Rejected Items 22 26
Table 2: Opinion Polarity Distribution for Ac-
cepted/Rejected Items
at a coherent interpretation (Hobbs et al, 1993) of
the opinions in the discourse.
4.1 Gathering More Information
Frame relations provide a mechanism to relate
opinions expressed in non-local contexts - the
opinion may occur elsewhere in the discourse, but
will become relevant to a given target due to a re-
lation between its target and the given target. For
instance, in Example 3, there is one direct eval-
uation of the leading remotes (O1) and two eval-
uations via identity (O2, O5). Following frames
constructed via t2-t3 and t3-t4, we get two more
opinions (O3 and O4) for the leading remotes.
Furthermore, opinions regarding something not
lexically or even anaphorically related can be-
come relevant, providing more opinion informa-
tion. This is particularly interesting when alt re-
lations are involved, as opinions towards one alter-
native imply opinions of opposite polarity toward
the competing options. For instance in Example 4,
if we consider only the explicitly stated opinions,
there is only one (positive) opinion, O1, about the
curved shape. However, the speaker expresses sev-
eral other opinions which reinforce his positivity
toward the curved shape. Thus, by using the frame
information, it is possible to gather more opinions
regarding curved shapes for TV remotes.
As a simple proof of concept, we counted the
number of positive and negative opinions towards
the items that were accepted or rejected in the
meetings (information about accepted and rejected
items is obtained from the manual abstractive sum-
maries provided by the AMI corpus). Counts are
obtained, over opinions manually annotated in the
data, for two conditions: with and without frame
information. The items in our meeting data are
mainly options for the new TV remote, which in-
clude attributes and features like different shapes,
materials, designs, and functionalities. We ob-
served that for the accepted items, the number of
positive opinions is higher and, for rejected items,
the number of negative opinions is higher. The
top section of Table 2 shows a contingency ta-
ble of counts of positive/negative opinions for ac-
cepted/rejected items for 5 AMI meetings.
Then we counted the number of reinforc-
ing opinions that were expressed regarding these
items. This meant also counting additional opin-
ions that were related via reinforcing frames. The
bottom section of Table 2 shows the counts when
the reinforcing frames are considered. Compared
to the counts of only individual opinions, we see
that the numbers in each cell have increased, while
maintaining the same pattern of distribution.
Thus, in effect we have procured more instances
of opinions for the items. We believe this added
information would help applications like meeting
summarizers and QA systems to make more in-
formed decisions.
4.2 Interdependent Interpretation
We believe that our opinion frames, anaphoric re-
lations and discourse relations can symbiotically
help disambiguate each other in the discourse. In
particular, suppose that some aspect of an individ-
ual opinion, such as polarity, is unclear. If the dis-
course suggests certain opinion frames, this may in
turn resolve the underlying ambiguity.
Revisiting Example 2 from above, we see that
out of context, the polarities of bouncy and dif-
ferent from other remotes are unclear (bounci-
ness and being different may be negative attributes
for another type of object). However, the polari-
ties of two of the opinions are clear (durable and
ergonomic). There is evidence in this passage of
discourse continuity and same relations such as the
pronouns, the lack of contrastive cue phrases, and
so on. This evidence suggests that the speaker ex-
presses similar opinions throughout the passage,
making the opinion frame SPSPsame more likely
throughout. Recognizing the frames would resolve
the polarity ambiguities of bouncy and different.
In the following example (5), the positive senti-
ment (SP) towards the this and the positive arguing
(AP) for the it are clear. These two individual opin-
ions can be related by a same/alt target relation, be
unrelated, or have some other relation not covered
by our scheme (in which case we would not have
a relation between them). There is evidence in the
discourse that makes one interpretation more likely
than others. The ?so? indicates that the two clauses
are highly likely to be related by a cause discourse
805
relation (PDTB). This information confirms a dis-
course continuity, as well as makes a reinforcing
scenario likely, which makes the reinforcing frame
SPAPsame highly probable. This increase in like-
lihood will in turn help a coreference system to in-
crease its confidence that the ?that? and the ?it?
co-refer.
(5) B :: ... and this will definitely enhance our market
sales, so we should take it into consideration also.
Opinion Span - target Span Rel
O1 definitely enhance our market sales - this [t1] SP
O2 so we should - it [t2] AP
Target - target Rel
t1 -t2 same (identity)
5 Experiments
There has been much work on recognizing indi-
vidual aspects of opinions like extracting individ-
ual opinions from phrases or sentences and recog-
nizing opinion type and polarity. Accordingly, in
our machine learning experiments we assume ora-
cle opinion and polarity information. Our experi-
ments thus focus on the new question: ?Given two
opinion sentences, determine if they participate in
any frame relation.? Here, an opinion sentence is a
sentence containing one or more sentiment or ar-
guing expression. In this work, we consider frame
detection only between sentence pairs belonging to
the same speaker.
5.1 Annotation of Gold Standard
Creating gold-standard opinion-frame data is ac-
complished by annotating frame components and
then building the frames from those underlying an-
notations.
We began with annotations created by Soma-
sundaran et al (2007), namely four meetings
of the AMI meeting corpus annotated for senti-
ment and arguing opinions (text anchor and type).
Following that annotation scheme, we annotated
an additional meeting. This gave us a corpus of
4436 sentences or 2942 segments (utterances). We
added attributes to the existing opinion annota-
tions, namely polarity and target-id. The target-
id attribute links the opinion to its local target
span. Relations between targets were then anno-
tated. When a newly annotated target is similar (or
opposed) to a set of targets already participating in
same relations, then the same (or alt) link is made
only to one of them - the one that seems most natu-
ral. This is often the one that is physically closest.
Content Word overlap between the sentence pair
Focus space overlap between the sentence pair
Anaphoric indicator in the second sentence
Time difference between the sentence pair
Number of intervening sentences
Existence of adjacency pair between the sentence pair
Bag of words for each sentence
Table 3: Features for Opinion Frame detection
Link transitivity is then used to connect targets that
are not explicitly linked by the annotators.
All annotations were performed by two of the
co-authors of this paper by consensus labeling.
The details of our annotation scheme and inter-
annotator agreement studies are presented in (So-
masundaran et al, 2008).
Once the individual frame components are an-
notated, conceptually, a frame exists for a pair of
opinions if their polarities are either positive or
negative and their targets are in a same or alt rela-
tion. For our experiments, if a path exists between
two targets, then their opinions are considered to
be participating in an opinion-frame relation.
The experimental data consists of pairs of opin-
ion sentences and the gold-standard information
whether there exists a frame between them. We
approximate continuous discourse by only pair-
ing sentences that are not more than 10 sentences
apart. We also filter out sentences that are less than
two words in length in order to handle data skew-
ness. This filters out very small sentences (e.g.,
?Cool.?) which rarely participate in frames. The
experiments were performed on a total of 2539
sentence pairs, of which 551 are positive instances.
5.2 Features
The factor that determines if two opinions are
related is primarily the target relations between
them. Instead of first finding the target span for
each opinion sentence and then inferring if they
should be related, we directly try to encode target
relation information in our features. By this ap-
proach, even in the absence of explicit target-span
information, we are able to determine if the opin-
ion sentence pairs are related.
We explored a number of features to incorpo-
rate this. The set that give the best performance
are listed in Table 3. The content word overlap
feature captures the degree of topic overlap be-
tween the sentence pair, and looks for target re-
lations via identity. The focus space overlap fea-
ture is motivated by our observation that partici-
806
Acc. Prec. Recall F-measure
False 78.3% - 0% -
Distribution 66% 21.7% 21.7% 21.4%
Random 50.0% 21.5% 49.4% 29.8 %
True 21.7% 21.6% 100% 35.5 %
System 67.6% 36.8% 64.9% 46%
Table 4: Automatic Detection of Opinion Frames
pants refer to an established discourse topic with-
out explicitly referring to it. Thus, we construct a
focus space for each sentence containing recently
used NP chunks. The feature is the percent over-
lap between the focus spaces of the two opinion
sentences. The anaphoric indicator feature checks
for the presence of pronouns such as it and that
in the second sentence to account for target rela-
tions via anaphora. The time difference between
the sentences and the number of intervening sen-
tences are useful features to capture the idea that
topics shift with time. The existence of an adja-
cency pair 3 between the sentences can clue the
system that the opinions in the sentences are re-
lated too. Finally, standard bag of words features
are included for each sentence.
5.3 Results
We performed 5-fold cross validation experiments,
using the standard SVMperf package (Joachims,
2005), an implementation of SVMs designed
for optimizing multivariate performance measures.
We found that, on our skewed data, optimizing on
F-measure obtains the best results.
Our system is compared to four baselines in Ta-
ble 4. The majority class baseline which always
guesses false (False) has good accuracy but zero
recall. The baseline that always guesses true (True)
has 100% recall and the best f-measure among the
baselines, but poor accuracy. We also constructed
a baseline that guesses true/false over the test set
based on the distribution in the training data (Dis-
tribution). This baseline is smarter than the other
baselines, as it does not indiscriminately guess any
one of the class. The last baseline Random guesses
true 50% of the time.
The bottom row of Table 4 shows the perfor-
mance of our system (System). The skewness of
the data affects the baselines as well as our sys-
tem. Our system beats the best baseline f-measure
by over 10 percentage points, and the best base-
line precision by 14 percentage points. Comparing
3Adjacency Pairs are manual dialog annotations available
in the AMI corpus.
it to the baseline which has comparable accuracy,
namely Distribution, we see that our system im-
proves in f-measure by 24 percentage points.
Our results are encouraging - even using simple
features to capture target relations achieves consid-
erable improvement over the baselines. However,
there is much room for improvement. Using more
detailed target and discourse information promises
to further improve system performance. These are
avenues for future work.
6 Related work
Evidence from the surrounding context has been
used previously to determine if the current sen-
tence should be subjective/objective (Riloff et al,
2003; Pang and Lee, 2004) and adjacency pair in-
formation has been used to predict congressional
votes (Thomas et al, 2006). However, these meth-
ods do not explicitly model the relations between
opinions. An application of the idea of alterna-
tive targets can be seen in Kim and Hovy?s (2007)
work on election prediction. They assume that if
a speaker expresses support for one party, all men-
tions of the competing parties have negative po-
larity, thus creating automatically labeled training
data.
In the field of product review mining, sentiments
and features (aspects) have been mined (Popescu
and Etzioni, 2005), where the aspects correspond
to our definition of targets. However, the aspects
themselves are not related to each other in any
fashion.
Polanyi and Zaenen (2006), in their discussion
on contextual valence shifters, have also observed
the phenomena described in this work - namely
that a central topic may be divided into subtopics
in order to perform evaluations, and that discourse
structure can influence the overall interpretation of
valence. Snyder and Barzilay (2007) combine an
agreement model based on contrastive RST rela-
tions with a local aspect model to make a more
informed overall decision for sentiment classifi-
cation. In our scheme, their aspects would be
related as same and their high contrast relations
would correspond to the non-reinforcing frames
SPSNsame, SNSPsame. Additionally, our frame
relations would link the sentiments across non-
adjacent clauses, and make connections via alt tar-
get relations.
With regard to meetings, the most closely re-
lated work includes the dialog-related annotation
807
schemes for various available corpora of conver-
sation (e.g., Carletta et al (2005) for AMI). As
shown by Somasundaran et al (2007), dialog
structure information and opinions are in fact com-
plementary. We believe that, like the discourse
relations, the dialog information will additionally
help in arriving at an overall coherent interpreta-
tion.
7 Conclusions
In this paper, we described the idea of opin-
ion frames as a representation capturing discourse
level relations that arise from related opinion tar-
gets and which are common in task-oriented di-
alogs. We introduced the alternative relations that
hold between targets by virtue of being opposing
in the discourse context. We discussed how our
opinion-frame scheme and discourse relations go
hand in hand to provide a richer overall interpreta-
tion. We also illustrated that such discourse level
opinion associations have useful benefits, namely
they help gather more opinion information and
help interdependent interpretation. Finally, we
showed via our machine learning experiments that
the presence of opinion frames can be automati-
cally detected.
References
Carletta, J., S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meetings Corpus. In
Proceedings of Measuring Behavior Symposium on
?Annotating and measuring Meeting Behavior?.
Clark, H. H. 1975. Bridging. Theoretical issues in
natural language processing . New York: ACM.
Hobbs, J., M. Stickel, D. Appelt, and P. Martin. 1993.
Interpretation as abduction. AI, 63.
Joachims, T. 2005. A support vector method for multi-
variate performance measures. In ICML 2005.
Kim, Soo-Min and Eduard Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In EMNLP-
CoNLL 2007.
Mueller, C. and M. Strube. 2001. Annotating
anaphoric and bridging relations with mmax. In 2nd
SIGdial Workshop on Discourse and Dialogue.
Pang, B. and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
Polanyi, L. and A. Zaenen, 2006. Contextual Valence
Shifters, chapter 1. Computing Attitude and Affect
in Text: Theory and Applications. Springer.
Popescu, A.-M. and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
Prasad, R., E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber, 2007. PDTB 2.0 Anno-
tation Manual.
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning
subjective nouns using extraction pattern bootstrap-
ping. In CoNLL 2003.
Snyder, B. and R. Barzilay. 2007. Multiple aspect
ranking using the good grief algorithm. In HLT
2007: NAACL.
Somasundaran, S., J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
Somasundaran, S, J Ruppenhofer, and J Wiebe. 2008.
Discourse level opinion relations: An annotation
study. In SIGdial Workshop on Discourse and Di-
alogue. ACL.
Thomas, M., B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
Vieira, R. and M. Poesio. 2000. An empirically based
system for processing definite descriptions. Comput.
Linguist., 26(4).
Wilson, T. and J. Wiebe. 2005. Annotating attributions
and private states. In Proceedings of ACL Workshop
on Frontiers in Corpus Annotation II: Pie in the Sky.
808
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Discourse Level Opinion Relations: An Annotation Study
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Josef Ruppenhofer
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
josefr@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work proposes opinion frames as a repre-
sentation of discourse-level associations that
arise from related opinion targets and which
are common in task-oriented meeting dialogs.
We define the opinion frames and explain their
interpretation. Additionally we present an
annotation scheme that realizes the opinion
frames and via human annotation studies, we
show that these can be reliably identified.
1 Introduction
There has been a great deal of research in recent
years on opinions and subjectivity. Opinions have
been investigated at the phrase, sentence, and docu-
ment levels. However, little work has been carried
out at the level of discourse.
Consider the following excerpt from a dialog
about designing a remote control for a television (the
opinion targets - what the opinions are about - are
shown in italics).
(1) D:: And I thought not too edgy and like a box, more
kind of hand-held not as computery, yeah, more or-
ganic shape I think. Simple designs, like the last one
we just saw, not too many buttons . . .
Speaker D expresses an opinion in favor of a de-
sign that is simple and organic in shape, and against
an alternative design which is not. Several individ-
ual opinions are expressed in this passage. The first
is a negative opinion about the design being too edgy
and box-like, the next is a positive opinion toward
a hand-held design, followed by a negative opin-
ion toward a computery shape, and so on. While
we believe that recognizing individual expressions
of opinions, their properties, and components is im-
portant, we believe that discourse interpretation is
needed as well. It is by understanding the passage
as a discourse that we see edgy, like a box, com-
putery, and many buttons as descriptions of the type
of design D does not prefer, and hand-held, organic
shape, and simple designs as descriptions of the type
he does. These descriptions are not in general syn-
onyms/antonyms of one another; for example, there
are hand-held ?computery? devices and simple de-
signs that are edgy. The unison/opposition among
the descriptions is due to how they are used in the
discourse.
This paper focuses on such relations between the
targets of opinions in discourse. Specifically, we
propose opinion frames, which consist of two opin-
ions which are related by virtue of having united
or opposed targets. We believe that recognizing
opinion frames will provide more information for
NLP applications than recognizing their individual
components alone. Further, if there is uncertainty
about any one of the components, we believe opin-
ion frames are an effective representation incorpo-
rating discourse information to make an overall co-
herent interpretation (Hobbs, 1979; Hobbs, 1983).
To our knowledge, this is the first work to ex-
tend a manual annotation scheme to relate opinions
in the discourse. In this paper, we present opin-
ion frames, and motivate their usefulness through
examples. Then we provide an annotation scheme
for capturing these opinion frames. Finally we per-
form fine-grained annotation studies to measure the
human reliability in recognizing of these opinion
frames.
129
Opinion frames are presented in Section 2, our an-
notation scheme is described in Section 3, the inter-
annotator agreement studies are presented in Section
4, related work is discussed in Section 5, and conclu-
sions are in Section 6.
2 Opinion Frames
2.1 Introduction
The components of opinion frames are individual
opinions and the relationships between their targets.
We address two types of opinions, sentiment and
arguing. Following (Wilson and Wiebe, 2005; So-
masundaran et al, 2007), sentiment includes posi-
tive and negative evaluations, emotions, and judg-
ments, while arguing includes arguing for or against
something, and arguing that something should or
should not be done. In our examples, the lexical an-
chors revealing the opinion type (as the words are
interpreted in context) are indicated in bold face.
In addition, the text span capturing the target of the
opinion (again, as interpreted in context) is indicated
in italics.
(2) D:: . . . this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot. A
bit more durable and that can also be ergonomic and it
kind of feels a bit different from all the other remote
controls.
Speaker D expresses his preference for the rub-
bery material for the remote. He reiterates his opin-
ion with a number of positive evaluations like bit
more bouncy, bit more durable, ergonomic and
so on.
All opinions in this example are related to the oth-
ers via opinion frames by virtue of having the same
targets, i.e., the opinions are essentially about the
same things (the rubbery material for the remote).
For example, the opinions ergonomic and a bit dif-
ferent from all the other remote controls are re-
lated in a frame of type SPSPsame, meaning the first
opinion is a S(entiment) with polarity P(ositive); the
second also is a S(entiment) with polarity P(ositive);
and the targets of the opinions are in a same (target)
relation.
The specific target relations addressed in this pa-
per are the relations of either being the same or being
alternatives to one another. While these are not the
only possible relations, they are not infrequent, and
SPSPsame, SNSNsame, APAPsame, ANANsame,
SPAPsame, APSPsame, SNANsame, ANSNsame,
SPSNalt, SNSPalt, APANalt, ANAPalt,
SPANalt, SNAPalt, APSNalt, ANSPalt
SPSNsame, SNSPsame, APANsame, ANAPsame,
SPANsame, APSNsame, SNAPsame, ANSPsame,
SPSPalt, SNSNalt, APAPalt, ANANalt,
SPAPalt, SNANalt, APSPalt, ANSNalt
Table 1: Opinion Frames
they commonly occur in task-oriented dialogs such
as those in our data.
With four opinion type - polarity pairs (SN, SP,
AN, AP), for each of two opinion slots, and two pos-
sible target relations, we have 4 * 4 * 2 = 32 types
of frame, listed in Table 1.
In the remainder of this section, we elaborate fur-
ther on the same target relation (in 2.2) the alter-
native target relation (in 2.3) and explain a method
by which these relationships can be propagated (in
2.4). Finally, we illustrate the usefulness of opinion
frames in discourse interpretation (in 2.5).
2.2 Same Targets
Our notion of sameness for targets includes cases
of anaphora and ellipses, lexically similar items, as
well as less direct relations such as part-whole, sub-
set, inferable, and instance-class.
Looking at the opinion frames for Example 2 in
more detail, we separately list the opinions, followed
by the relations between targets.
Opinion Span - target Span Type
O1 bit more bouncy - it?s [t1] SP
O2 bit more durable - ellipsis [t2] SP
O3 ergonomic - that [t3] SP
O4 a bit different from all the other remote - it [t4] SP
Target - target Rel
t1 - t2 same
t1 - t3 same
t3 - t4 same
Ellipsis occurs with bit more durable. [t2] rep-
resents the (implicit) target of that opinion, and [t2]
has a same relation to [t1], the target of the bit more
bouncy opinion. (Note that the interpretation of the
first target, [t1], would require anaphora resolution
of its target span with a previous noun phrase, rub-
bery material.)
Let us now consider the following passage, in
which a meeting participant analyzes two leading re-
130
motes on the market.1
(3) D:: These are two leading remote controls at the mo-
ment. You know they?re grey, this one?s got loads of
buttons, it?s hard to tell from here what they actually
do, and they don?t look very exciting at all.
Opinion Span - target Span Rel
O1 leading - remote controls [t1] SP
O2 grey - they [t2] SN
O3 loads of buttons - this one [t3] SN
O4 hard to tell - they [t4] SN
O5 don?t look very exciting at all - they [t5] SN
Target - target Rel
t1 - t2 same
t2 - t3 same
t3 - t4 same
t5 - t1 same
Target [t2] is the set of two leading remotes, and [t3],
which is in a same relation with [t2], is one of those
remotes. Target [t4], which is also in a same rela-
tion with [t3], is an aspect of that remote, namely
its buttons. Thus, opinion O3 is directly about one
of the remotes, and indirectly about the set of both
remotes. Similarly, opinion O4 is directly about the
buttons of one of the remotes, and indirectly about
that remote itself.
2.3 Alternative Targets
The alt(ernative) target relation arises when multiple
choices are available, and only one can be selected.
For example, in the domain of TV remote controls,
the set of all shapes are alternatives to one another,
since a remote control may have only one shape at a
time. In such scenarios, a positive opinion regarding
one choice may imply a negative opinion toward the
rest of the choices, and vice versa.
As an example, let us now consider the follow-
ing passage (some intervening utterances have been
removed for clarity).
(4) C:: . . . shapes should be curved, so round shapes2
Nothing square-like.
C:: . . . So we shouldn?t have too square corners and
that kind of thing.
B:: Yeah okay. Not the old box look.
1In the other examples in this paper, the source (holder) of
the opinions is the speaker. The leading opinion in this example
is an exception: its source is implicit; it is a consensus opinion
that is not necessarily shared by the speaker (i.e., it is a nested
source (Wiebe et al, 2005)).
2In the context of the dialogs, the annotators read the ?so
round shapes? as a summary statement. Had the ?so? been inter-
preted as Arguing, the round shapes would have been annotated
as a target (and linked to curved).
Opinion Span - target Span Rel
O1 should be - curved [t1] AP
O2 Nothing - square-like [t2] AN
O3 shouldn?t have - square corners [t3] AN
O4 too - square corners [t3] SN
O5 Not - the old box look [t4] AN
O6 the old box look - the old box look [t4] SN
Target - target Rel
t1 -t2 alternatives
t2 - t3 same
t3 - t4 same
There is an alt relation between, for example,
[t1] and [t2]. Thus, we have an opinion frame be-
tween O1 and O2, whose type is APANalt. From
this frame, we understand that a positive opinion is
expressed toward something and a negative opinion
is expressed toward its alternative.
2.4 Link Transitivity
When individual targets are linked, they form a
chain-like structure. Due to this, a connecting path
may exist between targets that were not directly
linked by the human annotators. This path may be
traversed to create links between new pairs of tar-
gets - which in turn results in new opinion frame re-
lations. For instance, in Example 4, the frame with
direct relation is O1O2 APANalt. By following the
alt link from [t1] to [t2] and the same link from [t2]
to [t3], we have an alt link between [t1] and [t3],
and the additional frames O1O3 APANalt and O1O4
APSNalt. Repeating this process would finally link
speaker C?s opinion O1 with B?s opinion O6, yield-
ing a APSNalt frame.
2.5 Interpretation
This section illustrates two motivations for opinion
frames: they may unearth additional information
over and above the individual opinions stated in the
text, and they may contribute toward arriving at a
coherent interpretation (Hobbs, 1979; Hobbs, 1983)
of the opinions in the discourse.
Through opinion frames, opinions regarding
something not explicitly mentioned in the local con-
text and not even lexically related can become rel-
evant, providing more information about someone?s
opinions. This is particularly interesting when alt
relations are involved, as opinions towards one al-
ternative imply opinions of opposite polarity toward
the remaining options. For instance in Example 4
131
above, if we consider only the explicitly stated opin-
ions, there is only one (positive) opinion about the
curved shape, namely O1. However, the speaker ex-
presses several other opinions which reinforce his
positivity toward the curved shape. These are in
fact opinion frames in which the other opinion has
the opposite polarity as O1 and the target relation is
alt (for example frames such as O1O3 APANalt and
O1O4 APSNalt).
In the dialog, notice that speaker B agrees with
C and exhibits his own reinforcing opinions. These
would be similarly linked via targets resulting in
frames like O1O6 APSNalt.
Turning to our second point, arriving at a coher-
ent interpretation obviously involves disambigua-
tion. Suppose that some aspect of an individual
opinion, such as polarity, is unclear. If the discourse
suggests certain opinion frames, this may in turn re-
solve the underlying ambiguity. For instance in Ex-
ample 2, we see that out of context, the polarities of
bouncy and different from other remotes are un-
clear (bounciness and being different may be neg-
ative attributes for another type of object). How-
ever, the polarities of two of the opinions are clear
(durable and ergonomic). There is evidence in this
passage of discourse continuity and same relations,
such as the pronouns, the lack of contrastive cue
phrases, and so on. This evidence suggests that the
speaker expresses similar opinions throughout the
passage, making the opinion frame SPSPsame more
likely throughout. Recognizing the frames would re-
solve the polarity ambiguities of bouncy and differ-
ent.
Example 2 is characterized by opinion frames in
which the opinions reinforce one other. Interest-
ingly, interplays among different opinion types may
show the same type of reinforcement. As we an-
alyzed above, Example 4 is characterized by mix-
tures of opinion types, polarities, and target rela-
tions. However, the opinions are still unified in
the intention to argue for a particular type of shape.
There is evidence in this passage suggesting rein-
forcing frames: the negations are applied to targets
that are alternative to the desired option, and the pas-
sage is without contrastive discourse cues. If we
are able to recognize the best overall set of opinion
frames for the passage, the polarity ambiguities will
be resolved.
On the other hand, evidence for non-reinforcing
opinions would suggest other frames, potentially re-
sulting in different interpretations of polarity and re-
lations among targets. Such non-reinforcing associ-
ations between opinions and often occur when the
speaker is ambivalent or weighing pros and cons.
Table 1 lists the frames that occur in reinforcing sce-
narios in the top row, and the frames that occur in
non-reinforcing scenarios in the bottom row.
3 Annotation Scheme
Our annotation scheme began with the definition
and basics of the opinion annotation from previ-
ous work (Wilson and Wiebe, 2005; Somasundaran
et al, 2007). We then add to it the attributes and
components that are necessary to make an Opinion
Frame.
First, the text span that reveals the opinion expres-
sion is identified. Then, the text spans corresponding
to the targets are marked, if there exist any (we also
allow span-less targets). Then, the type and polar-
ity of the opinion in the context of the discourse is
marked. Finally the targets that are related (again
in the context of the discourse) are linked. Specif-
ically, the components that form the Annotation of
the frame are as follows:
Opinion Span: This is a span of text that reveals
the opinion.
Type: This attribute specifies the opinion type as ei-
ther Arguing or Sentiment.
Polarity: This attribute identifies the valence of an
opinion and can be one of: positive, negative,
neutral, both, unknown.
Target Span: This is a span of text that captures
what an opinion is about. This can be a propo-
sition or an entity.
Target Link: This is an attribute of a target and
records all the targets in the discourse that the
target is related to.
Link Type: The link between two targets is speci-
fied by this attribute as either same or alterna-
tive.
132
In addition to these definitions, our annotation man-
ual has guidelines detailing how to deal with gram-
matical issues, disfluencies, etc. Appendix A illus-
trates how this annotation scheme is applied to the
utterances of Example 4.
Links between targets can be followed in either
direction to construct chains. In this work, we
consider target relations to be commutative, i.e.,
Link(t1,t2) => Link(t2,t1). When a newly anno-
tated target is similar (or opposed) to a set of tar-
gets already participating in same relations, then the
same (or alt) link is made only to one of them - the
one that looks most natural. This is often the one
that is closest.
4 Annotation Studies
Construction of an opinion frame is a stepwise pro-
cess where first the text spans revealing the opinions
and their targets are selected, the opinion text spans
are classified by type and polarity and finally the
targets are linked via one of the possible relations.
We split our annotation process into these 3 intuitive
stages and use an evaluation that is most applicable
for the task at that stage.
Two annotators (both co-authors on the paper) un-
derwent training at each stage, and the annotation
manual was revised after each round of training. In
order to prevent errors incurred at earlier stages from
affecting the evaluation of later stages, the anno-
tators produced a consensus version at the end of
each stage, and used that consensus annotation as
the starting point for the next annotation stage. In
producing these consensus files, one annotator first
annotated a document, and the other annotator re-
viewed the annotations, making changes if needed.
This prevented any discussion between the annota-
tors from influencing the tagging task of the next
stage.
In the following subsections, we first introduce
the data and then present our results for annotation
studies for each stage, ending with discussion.
4.1 Data
The data used in this work is the AMI meeting cor-
pus (Carletta et al, 2005) which contains multi-
modal recordings of group meetings. We annotated
meetings from the scenario based meetings, where
Gold Exact Lenient Subset
ANN-1 53 89 87
ANN-2 44 76 74
Table 2: Inter-Annotator agreement on Opinion Spans
four participants collaborate to design a new TV
remote control in a series of four meetings. The
meetings represent different project phases, namely
project kick-off, functional design, conceptual de-
sign, and detailed design. Each meeting has rich
transcription and segment (turn/utterance) informa-
tion for each speaker. Each utterance consists of
one or more sentences. At each agreement stage we
used approximately 250 utterances from a meeting
for evaluation. The annotators also used the audio
and video recordings in the annotation of meetings.
4.2 Opinion Spans and Target Spans
In this step, the annotators selected text spans and
labeled them as opinion or target We calculated our
agreement for text span retrieval similar to Wiebe et
al. (2005). This agreement metric corresponds to
the Precision metric in information retrieval, where
annotations from one annotator are considered the
gold standard, and the other annotator?s annotations
are evaluated against it.
Table 2 shows the inter-annotator agreement (in
percentages). For the first row, the annotations pro-
duced by Annotator-1 (ANN-1) are taken as the gold
standard and, for the second row, the annotations
from annotator-2 form the gold standard. The ?Ex-
act? column reports the agreement when two text
spans have to match exactly to be considered cor-
rect. The ?Lenient? column shows the results if
an overlap relation between the two annotators? re-
trieved spans is also considered to be a hit. Wiebe
et al (2005) use this approach to measure agree-
ment for a (somewhat) similar task of subjectivity
span retrieval in the news corpus. Our agreement
numbers for this column is comparable to theirs. Fi-
nally, the third column, ?Subset?, shows the agree-
ment for a more strict constraint, namely, that one
of the spans must be a subset of the other to be con-
sidered a match. Two opinion spans that satisfy this
relation are ensured to share all the opinion words of
the smaller span.
The numbers indicate that, while the annotators
133
Gold Exact Lenient Subset
ANN-1 54 73 71
ANN-2 54 75 74
Table 3: Inter-Annotator agreement on Target Spans
Gold Exact Lenient Subset
ANN-1 74 87 87
ANN-2 76 90 90
Table 4: Inter-Annotator agreement on Targets with Per-
fect Opinion spans
do not often retrieve the exact same span, they
reliably retrieve approximate spans. Interestingly,
the agreement numbers between Lenient and Sub-
set columns are close. This implies that, in the cases
of inexact matches, the spans retrieved by the two
annotators are still close. They agree on the opinion
words and differ mostly on the inclusion of func-
tion words (e.g. articles) and observation of syntac-
tic boundaries.
In similar fashion, Table 3 gives the inter-
annotator agreement for target span retrieval. Ad-
ditionally, Table 4 shows the inter-annotator agree-
ment for target span retrieval when opinions that do
not have an exact match are filtered out. That is, Ta-
ble 4 shows results only for targets of the opinions
on which the annotators perfectly agree. As targets
are annotated with respect to the opinions, this sec-
ond evaluation removes any effects of disagreements
in the opinion detection task. As seen in Table 4, this
improves the inter-coder agreement.
4.3 Opinion Type and Polarity
In this step, the annotators began with the consensus
opinion span and target span annotations. We hy-
pothesized that given the opinion expression, deter-
mining whether it is Arguing or Sentiment would not
be difficult. Similarly, we hypothesized that target
information would make the polarity labeling task
clearer.
As every opinion instance is tagged with a type
Type Tagging Polarity Tagging
Accuracy 97.8% 98.5%
? 0.95 0.952
Table 5: Inter-Annotator agreement on Opinion Types
and Polarity
and polarity, we use Accuracy and Cohen?s Kappa
(?) metric (Cohen, 1960). The ? metric measures
the inter-annotator agreement above chance agree-
ment. The results, in Table 5, show that ? both for
type and polarity tagging is very high. This con-
firms our hypothesis that Sentiment and Arguing can
be reliably distinguished once the opinion spans are
known. Our polarity detection task shows an im-
provement in ? over a similar polarity assignment
task by Wilson et al (2005) for the news corpus (?
of 0.72). We believe this improvement can partly be
attributed to the target information available to our
annotators.
4.4 Target Linking
As an intuitive first step in evaluating target link-
ing, we treat target links in the discourse similarly to
anaphoric chains and apply methods developed for
co-reference resolution (Passonneau, 2004) for our
evaluation. Passonneau?s method is based on Krip-
pendorf?s ? metric (Krippendorff, 2004) and allows
for partial matches between anaphoric chains. In ad-
dition to this, we evaluate links identified by both
annotators for the type (same / alternative) labeling
task with the help of the ? metric.
Passonneau (2004) reports that in her co-reference
task on spoken monologs, ? varies with the diffi-
culty of the corpus (from 0.46 to 0.74). This is true
in our case too. Table 6 shows our agreement for
the four types of meetings in the AMI corpus: the
kickoff meeting (a), the functional design (b), the
conceptual design (c) and the detailed design (d).
Of the meetings, the kickoff meeting (a) we use
has relatively clear discussions. The conceptual de-
sign meeting (c) is the toughest, as as participants
are expressing opinions about a hypothetical (desir-
able) remote. In our detailed design meeting (d),
there are two final designs being evaluated. On an-
alyzing the chains from the two annotators, we dis-
covered that one annotator had maintained two sepa-
rate chains for the two remotes as there is no explicit
linguistic indication (within the 250 utterances) that
these two are alternatives. The second annotator, on
the other hand, used the knowledge that the goal
of the meeting is to design a single TV remote to
link them as alternatives. Thus by changing just
two links in the second annotator?s file to account
for this, our ? for this meeting went up from 0.52
134
Meeting: a b c d
Target linking (?) 0.79 0.74 0.59 0.52
Relation Labeling (?) 1 1 0.91 1
Table 6: Inter-Annotator agreement on Target relation
identification
to 0.70. We plan to further explore other evalua-
tion methodologies that account for severity of dif-
ferences in linking and are more relevant for our
task. Nonetheless, the resulting numbers indicate
that there is sufficient information in the discourse
to provide for reliable linking of targets.
The high ? for the relation type identification
shows that once the presence of a link is detected,
it is not difficult to determine if the targets are simi-
lar or alternatives to each other.
4.5 Discussion
Our agreement studies help to identify the aspects of
opinion frames that are straightforward, and those
that need complex reasoning. Our results indicate
that while the labeling tasks such as opinion type,
opinion polarity and target relation type are rel-
atively reliable for humans, retrieval of opinions
spans, target spans and target links is more difficult.
A common cause of annotation disagreement is
different interpretation of the utterance, particularly
in the presence of disfluencies and restarts. For ex-
ample consider the following utterance where a par-
ticipant is evaluating the drawing of another partici-
pant on the white board.
(5) It?s a baby shark , it looks to me, . . .
One annotator interpreted this ?it looks to me? as
an arguing for the belief that it was indeed a draw-
ing of a baby shark (positive Arguing). The sec-
ond annotator on the other hand looked at it as a
neutral viewpoint/evaluation (Sentiment) being ex-
pressed regarding the drawing. Thus even though
both annotators felt an opinion is being expressed,
they differed on its type and polarity.
There are some opinions that are inherently on the
borderline of Sentiment and Arguing. For example,
consider the following utterance where there is an
appeal to importance:
(6) Also important for you all is um the production cost
must be maximal twelve Euro and fifty cents.
Here, ?also important? might be taken as an assess-
ment of the high value of adhering to the budget (rel-
ative to other constraints), or simply as an argument
for adhering to the budget.
One potential source of problems to the target-
linking process consists of cases where the same
item becomes involved in more than one opposition.
For instance, in the example below, speaker D ini-
tially sets up an alternative between speech recog-
nition and buttons as a possible interface for navi-
gation. But later, speaker A re-frames the choice as
between having speech recognition only and having
both options. Connecting up all references to speech
recognition as a target respects the co-reference but
it also results in incorrect conclusions: the speech
recognition is an alternative to having both speech
recognition and buttons.
(7) A:: One thing is interesting is talking about speech
recognition in a remote control...
D:: ... So that we don?t need any button on the remote
control it would be all based on speech.
A:: ... I think that would not work so well. You wanna
have both options.
5 Related Work
Evidence from the surrounding context has been
used previously to determine if the current sentence
should be subjective/objective (Riloff et al, 2003;
Pang and Lee, 2004)) and adjacency pair informa-
tion has been used to predict congressional votes
(Thomas et al, 2006). However, these methods do
not explicitly model the relations between opinions.
Additionally, in our scheme opinions that are not
in the immediate context may be allowed to influ-
ence the interpretation of a given opinion via target
chains.
Polanyi and Zaenen (2006), in their discussion on
contextual valence shifters, have also observed the
phenomena described in this work - namely that a
central topic may be divided into subtopics in order
to perform evaluations, and that discourse structure
can influence the overall interpretation of valence.
Snyder and Barzilay (2007) combine an agree-
ment model based on contrastive RST relations with
a local aspect (or target) model to make a more in-
formed overall decision for sentiment classification.
The contrastive cue indicates a change in the senti-
ment polarity. In our scheme, their aspects would
be related as same and their high contrast relations
would result in frames such as SPSNsame, SNSP-
same. Additionally, our frame relations would link
sentiments across non-adjacent clauses, and make
connections via alt target relations.
135
Considering the discourse relation annotations in
the PDTB (Prasad et al, 2006), there can be align-
ment between discourse relations (like contrast) and
our opinion frames when the frames represent dom-
inant relations between two clauses. However, when
the relation between opinions is not the most promi-
nent one between two clauses, the discourse relation
may not align with the opinion frames. And when an
opinion frame is between two opinions in the same
clause, there would be no discourse relation counter-
part at all. Further, opinion frames assume particular
intentions that are not necessary for the establish-
ment of ostensibly similar discourse relations. For
example, we may not impose an opinion frame even
if there are contrastive cues. (Please refer to Ap-
pendix B for examples)
With regard to meetings, the most closely re-
lated work includes the dialog-related annotation
schemes for various available corpora of conversa-
tion (Dhillon et al (2003) for ICSI MRDA; Car-
letta et al (2005) for AMI ) As shown by Soma-
sundaran et al (2007), dialog structure information
and opinions are in fact complementary. We believe
that, like discourse relations, dialog information will
additionally help in arriving at an overall coherent
interpretation.
6 Conclusion and Future work
This is the first work that extends an opinion annota-
tion scheme to relate opinions via target relations.
We first introduced the idea of opinion frames as
a representation capturing discourse level relations
that arise from related opinion targets and which are
common in task-oriented dialogs such as our data.
We built an annotation scheme that would capture
these relationships. Finally, we performed extensive
inter-annotator agreement studies in order to find the
reliability of human judgment in recognizing frame
components. Our results and analysis provide in-
sights into the complexities involved in recognizing
discourse level relations between opinions.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152.
References
J. Carletta, S. Ashby, and et al 2005. The AMI Meetings
Corpus. In Proceedings of Measuring Behavior Sym-
posium on ?Annotating and measuring Meeting Be-
havior?.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg. 2003.
Meeting recorder project: Dialog act labeling guide.
Technical report, ICSI Tech Report TR-04-002.
J. Hobbs. 1979. Coherence and coreference. Cognitive
Science, 3:67?90.
J. Hobbs, 1983. Why is Discourse Coherent?, pages 29?
70. Buske Verlag.
K. Krippendorff. 2004. Content Analysis: An Introduc-
tion to Its Methodology, 2nd Edition. Sage Publica-
tions, Thousand Oaks, California.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
R. J. Passonneau. 2004. Computing reliability for coref-
erence annotation. In LREC.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters, chapter 1. Computing Attitude and Affect in
Text: Theory and Applications. Springer.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, and B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. In Workshop on Sentiment and Subjectivity
in Text. ACL.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In CoNLL 2003.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In HLT 2007:
NAACL.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In SIG-
dial Workshop on Discourse and Dialogue 2007.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In EMNLP 2006.
J. Wiebe, T. Wilson, and C Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, pages 164?210.
T. Wilson and J. Wiebe. 2005. Annotating attributions
and private states. In Proceedings of ACL Workshop
on Frontiers in Corpus Annotation II: Pie in the Sky.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT-EMNLP 2005.
136
A Annotation Example
C:: . . . shapes should be curved, so round shapes. Nothing
square-like.
C:: . . . So we shouldn?t have too square corners and that kind
of thing.
B:: Yeah okay. Not the old box look.
Span Attributes
O1 should be type=Arguing; Polarity=pos; target=t1
t1 curved Link,type=(t2,alt)
O2 Nothing type=Arguing; Polarity=neg; target=t2
t2 square-like Link,type=(t1,alt),(t3,same)
O3 shouldn?t have type=Arguing; Polarity=neg; target=t3
O4 too type=Sentiment; Polarity=neg; target=t3
t3 square corners Link,type=(t2,same),(t4,same)
O5 Not type=Arguing; Polarity=neg; target=t4
t4 the old box look Link,type=(t3,same)
O6 the old box look type=Sentiment; Polarity=neg; target=t4
B Comparison between Opinion Frames
and Discourse Relations
Opinion frames can align with discourse relations
between clauses only when the frames represent the
dominant relation between two clauses (1); but not
when the opinions occur in the same clause (2); or
when the relation between opinions is not the most
prominent (3); or when two distinct targets are nei-
ther same nor alternatives (4).
(1) Non-reinforcing opinion frame (SNSP-
same); Contrast discourse relation
D :: And so what I have found and after a lot
of work actually I draw for you this schema
that can be maybe too technical for you but
is very important for me you know.
(2) Reinforcing opinion frame (SPSPsame); no
discourse relation
Thirty four percent said it takes too long
to learn to use a remote control, they want
something that?s easier to use straight away,
more intuitive perhaps.
(3) Reinforcing opinion frame (SPSPsame);
Reason discourse relation
She even likes my manga, actually the quote
is: ?I like it, because you like it, honey.?
(source: web)
(4) Unrelated opinions; Contrast discourse re-
lation
A :: Yeah, what I have to say about means.
The smart board is okay. Digital pen is hor-
rible. I dunno if you use it. But if you want
to download it to your computer, it?s doesn?t
work. No.
137
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 949?957,
Beijing, August 2010
Bringing Active Learning to Life
Ines Rehbein
Computational Linguistics
Saarland University
Josef Ruppenhofer
Computational Linguistics
Saarland University
{rehbein|josefr|apalmer}@coli.uni-sb.de
Alexis Palmer
Computational Linguistics
Saarland University
Abstract
Active learning has been applied to dif-
ferent NLP tasks, with the aim of limit-
ing the amount of time and cost for human
annotation. Most studies on active learn-
ing have only simulated the annotation
scenario, using prelabelled gold standard
data. We present the first active learning
experiment for Word Sense Disambigua-
tion with human annotators in a realistic
environment, using fine-grained sense dis-
tinctions, and investigate whether AL can
reduce annotation cost and boost classifier
performance when applied to a real-world
task.
1 Introduction
Active learning has recently attracted attention as
having the potential to overcome the knowledge
acquisition bottleneck by limiting the amount of
human annotation needed to create training data
for statistical classifiers. Active learning has been
shown, for a number of different NLP tasks, to re-
duce the number of manually annotated instances
needed for obtaining a consistent classifier perfor-
mance (Hwa, 2004; Chen et al, 2006; Tomanek et
al., 2007; Reichart et al, 2008).
The majority of such results have been achieved
by simulating the annotation scenario using prela-
belled gold standard annotations as a stand-in for
real-time human annotation. Simulating annota-
tion allows one to test different parameter set-
tings without incurring the cost of human anno-
tation. There is, however, a major drawback: we
do not know whether the results of experiments
performed using hand-corrected data carry over to
real-world scenarios in which individual human
annotators produce noisy annotations. In addi-
tion, we do not know to what extent error-prone
annotations mislead the learning process. A sys-
tematic study of the impact of erroneous annota-
tion on classifier performance in an active learn-
ing (AL) setting is overdue. We need to know a)
whether the AL approach can really improve clas-
sifier performance and save annotation time when
applied in a real-world scenario with noisy data,
and b) whether AL works for classification tasks
with fine-grained or complex annotation schemes
and a low inter-annotator agreement.
In this paper we bring active learning to life in
the context of frame semantic annotation of Ger-
man texts within the SALSA project (Burchardt
et al, 2006). Specifically, we apply AL methods
for learning to assign semantic frames to predi-
cates, following Erk (2005) in treating frame as-
signment as a Word Sense Disambiguation task.
Under our fine-grained annotation scheme, anno-
tators have to deal with a high level of ambigu-
ity, resulting in low inter-annotator agreement for
some word senses. This fact, along with the po-
tential for wrong annotation decisions or possi-
ble biases from individual annotators, results in
an annotation environment in which we get noisy
data which might mislead the classifier. A sec-
ond characteristic of our scenario is that there is no
gold standard for the newly annotated data, which
means that evaluation is not straightforward. Fi-
nally, we have multiple annotators whose deci-
949
sions on particular instances may diverge, raising
the question of which annotations should be used
to guide the AL process. This paper thus investi-
gates whether active learning can be successfully
applied in a real-world scenario with the particular
challenges described above.
Section 2 of the paper gives a short overview
of the AL paradigm and some related work, and
Section 3 discusses the multi-annotator scenario.
In Section 4 we present our experimental design
and describe the data we use. Section 5 presents
results, and Section 6 concludes.
2 Active Learning
The active learning approach aims to reduce the
amount of manual annotation needed to create
training data sufficient for developing a classifier
with a given performance. At each iteration of
the AL cycle, the actual knowledge state of the
learner guides the learning process by determin-
ing which instances are chosen next for annota-
tion. The main goal is to advance the learning
process by selecting instances which provide im-
portant information for the machine learner.
In a typical active learning scenario, a small set
of manually labelled seed data serves as the ini-
tial training set for the classifier (learner). Based
on the predictions of the classifier, a large pool
of unannotated instances is queried for the next
instance (or batch of instances) to be presented
to the human annotator (sometimes called the or-
acle). The underlying active learning algorithm
controlling the learning process tries to select the
most informative instances in order to get a strong
boost in classifier performance. Different meth-
ods can be used for determining informativity of
instances. We use uncertainty sampling (Cohn et
al., 1995) in which ?most informative? instances
are those for which the classifier has the lowest
confidence in its label predictions. The rough in-
tuition behind this selection method is that it iden-
tifies instance types which have yet to be encoun-
tered by the classifier. The learning process pro-
ceeds by presenting the selected instances to the
human annotator, who assigns the correct label.
The newly-annotated instances are added to the
seed data and the classifier is re-trained on the new
data set. The newly trained classifier now picks
the next instances, based on its updated knowl-
edge, and the process repeats. If the learning pro-
cess can provide precisely that information which
the classifier still needs to learn, a smaller number
of instances should suffice to achieve the same ac-
curacy as on a larger training set of randomly se-
lected training examples.
Active learning has been applied to a num-
ber of natural language processing tasks like
POS tagging (Ringger et al, 2007), NER (Laws
and Schu?tze, 2008; Tomanek and Hahn, 2009),
syntactic parsing (Osborne and Baldridge, 2004;
Hwa, 2004), Word Sense Disambiguation (Chen
et al, 2006; Chan and Ng, 2007; Zhu and Hovy,
2007; Zhu et al, 2008) and morpheme gloss-
ing for language documentation (Baldridge and
Palmer, 2009). While most of these studies suc-
cessfully show that the same classification accu-
racy can be achieved with a substantially smaller
data set, these findings are mostly based on simu-
lations using gold standard data.
For our task of Word Sense Disambiguation
(WSD), mixed results have been achieved. AL
seems to improve results in a WSD task with
coarse-grained sense distinctions (Chan and Ng,
2007), but the results of (Dang, 2004) raise doubts
as to whether AL can successfully be applied to
a fine-grained annotation scheme, where Inter-
Annotator Agreement (IAA) is low and thus the
consistency of the human annotations decreases.
In general, AL has been shown to reduce the cost
of annotation when applied to classification tasks
where a single human annotator predicts labels for
new data points with a reasonable consistency and
accuracy. It is not clear whether the same settings
can be applied to a multi-annotator environment
where IAA is low.
3 Active Learning in a realistic task
including multiple annotators
Another possible difference between active learn-
ing simulations and real-world scenarios is the
multi-annotator environment. In such a setting,
two or more annotators assign labels to the same
instances, which are then merged to check for con-
flicting decisions from different annotators. This
is standard practise in many annotation projects
doing fine-grained semantic annotation with a
950
high level of ambiguity, and it necessitates that all
annotators work on the same data set.
Replicating an active learning simulation on
hand-corrected data, starting with a fixed set of
seed data and fixed parameter settings, using the
same algorithm, will always result in the same
training set selected from the pool. Human anno-
tators, however, will assign different labels to the
same instances, thus influencing the selection of
the next instance from the pool. This means that
individual annotators might end up with very dif-
ferent sets of annotated data, depending on factors
like their interpretation of the annotation guide-
lines, an implicit bias towards a particular label,
or simply errors made during annotation.
There is not much work addressing this prob-
lem. (Donmez and Carbonell, 2008) consider
modifications of active learning to accommodate
variability of annotators. (Baldridge and Palmer,
2009) present a real-world study with human an-
notators in the context of language documenta-
tion. The task consists of producing interlin-
ear glossed text, including morphological and
grammatical analysis, and can be described as
a sequence labelling task. Annotation cost is
measured as the actual time needed for annota-
tion. Among other settings, the authors compare
the performance of two annotators with different
grades of expertise. The classifier trained on the
data set created by the expert annotator in an ac-
tive learning setting does obtain a higher accuracy
on the gold standard. For the non-expert annota-
tor, however, the active learning setting resulted
in a lower accuracy than for a classifier trained on
a randomly selected data set. This finding sug-
gests that the quality of annotation needs to be
high enough for active learning to actually work,
and that annotation noise is a problem for AL.
There are two problems arising from this:
1. It is not clear whether active learning will
work when applied to noisy data
2. It is not straightforward to apply active learn-
ing to a real-world scenario, where low IAA
asks for multiple annotators
In our experiment we address these questions
by systematically investigating the impact of an-
notation noise on classifier performance and on
the composition of the training set. The next sec-
tion presents the experimental design and the data
used in our experiment.
4 Experimental Design
In the experiment we annotated 8 German cau-
sation nouns, namely Ausgang, Anlass, Ergeb-
nis, Resultat, Grund, Konsequenz, Motiv, Quelle
(outcome, occasion, effect, result, reason, con-
sequence, motive, source of experience). These
nouns were chosen because they exhibit a range
of difficulty in terms of the number of senses they
have in our annotation scheme. They all encode
subtle distinctions between different word senses,
but some of them are clearly easier to disam-
biguate than others. For instance, although Aus-
gang has 9 senses, they are easier to distinguish
for humans than the 4 senses of Konsequenz.
Six annotators participated in the experiment.
While all annotators were trained, having at least
one year experience in frame-semantic annota-
tion, one of the annotators is an expert with several
years of training and working experience in the
Berkeley FrameNet Project. This annotator also
defined the frames (word senses) used in our ex-
periment.
Prior to the experiment, all annotators were
given 100 randomly chosen sentences. After
annotating the training data, problematic cases
were discussed to make sure that the annotators
were familiar with the fine-grained distinctions
between word senses in the annotation scheme.
The data sets used for training were adjudicated
by two of the annotators (one of them being the
expert) and then used as a gold standard to test
classifier performance in the active learning pro-
cess.
4.1 Data and Setup
For each lemma we extracted sentences from the
Wahrig corpus1 containing this particular lemma.
The annotators had to assign word senses to 300
instances for each target word, split into 6 pack-
ages of 50 sentences each. This resulted in 2,400
annotated instances per annotator (14,400 anno-
tated instances in total). The annotation was done
1The Wahrig corpus includes more than 113 mio. sen-
tences from German newspapers and magazines covering
topics such as politics, science, fashion, and others.
951
Anlass Motiv Konsequenz Quelle Ergebnis / Resultat Ausgang Grund
Occasion (37) Motif (47) Causation (32) Relational nat feat.(3) Causation (4/10) Outcome (67) Causation (24)
Reason (63) Reason(53) Level of det.(6) Source of getting (14) Competitive score(12/36) Have leave (4) Reason (58)
Response (61) Source of exp. (14) Decision (11/6) Portal (21) Death (1)
MWE1 (1) Source of info. (56) Efficacy (2/3) Outgoing goods (4) Part orientation. (0)
Well (6) Finding out (24/23) Ostomy (0) Locale by owner(3)
Emissions source (7) Mathematics (1/0) Origin (5) Surface earth (0)
Operating result (36/5) Tech output (7) Bottom layer (0)
Outcome (10/17) Process end (2) Soil (1)
Departing (1) CXN1 (0)
CXN2 (0)
MWE1 (0)
MWE2 (10)
MWE3 (0)
MWE4 (3)
MWE5 (0)
MWE6 (0)
Fleiss? kappa for the 6 annotators for the 150 instances annotated in the random setting
0.67 0.79 0.55 0.77 0.63 / 0.59 0.82 0.43
Table 1: 8 causation nouns and their word senses (numbers in brackets give the distribution of word
senses in the gold standard (100 sentences); CXN: constructions, MWE: multi-word expressions; note
that Ergebnis and Resultat are synonyms and therefore share the same set of frames.)
using a Graphical User Interface where the sen-
tence was presented to the annotator, who could
choose between all possible word senses listed in
the GUI. The annotators could either select the
frame by mouse click or use keyboard shortcuts.
For each instance we recorded the time it took
the annotator to assign an appropriate label. To
ease the reading process the target word was high-
lighted.
As we want to compare time requirements
needed for annotating random samples and sen-
tences selected by active learning, we had to con-
trol for training effects which might speed up the
annotation. Therefore we changed the annotation
setting after each package, meaning that the first
annotator started with 50 sentences randomly se-
lected from the pool, then annotated 50 sentences
selected by AL, followed by another 50 randomly
chosen sentences, and so on. We divided the an-
notators into two groups of three annotators each.
The first group started annotating in the random
setting, the second group in the AL setting. The
composition of the groups was changed for each
lemma, so that each annotator experienced all dif-
ferent settings during the annotation process. The
annotators were not aware of which setting they
were in.
Pool data For the random setting we randomly
selected three sets of sentences from the Wahrig
corpus which were presented for annotation to all
six annotators. This allows us to compare annota-
tion time and inter-annotator agreement between
the annotators. For the active learning setting we
randomly selected three sets of 2000 sentences
each, from which the classifier could pick new in-
stances during the annotation process. This means
that for each trial the algorithm could select 50 in-
stances out of a pool of 2000 sentences. On any
given AL trial each annotator uses the same pool
as all the other annotators. In an AL simulation
with fixed settings and gold standard labels this
would result in the same subset of sentences se-
lected by the classifier. For our human annotators,
however, due to different annotation decisions the
resulting set of sentences is expected to differ.
Sampling method Uncertainty sampling is a
standard sampling method for AL where new in-
stances are selected based on the confidence of the
classifier for predicting the appropriate label. Dur-
ing early stages of the learning process when the
classifier is trained on a very small seed data set,
it is not beneficial to add the instances with the
lowest classifier confidence. Instead, we use a dy-
namic version of uncertainty sampling (Rehbein
and Ruppenhofer, 2010), based on the confidence
of a maximum entropy classifier2, taking into ac-
count how much the classifier has learned so far.
In each iteration one new instance is selected from
the pool and presented to the oracle. After anno-
tation the classifier is retrained on the new data
set. The modified uncertainty sampling results in
a more robust classifier performance during early
stages of the learning process.
2http://maxent.sourceforge.net
952
Anlass Motiv Konsequenz Quelle Ergebnis Resultat Ausgang Grund
R U R U R U R U R U R U R U R U
A1 8.6 9.6 5.9 6.6 10.7 10.5 6.0 4.8 10.5 7.4 10.1 9.6 6.4 10.0 10.2 11.1
A2 4.4 5.7 4.8 5.9 8.2 9.2 4.9 4.9 6.4 4.4 11.7 8.5 5.1 7.7 9.0 9.3
A3 9.9 9.2 6.8 6.7 6.8 8.3 7.4 6.1 9.4 7.6 9.0 12.3 7.5 8.5 11.7 10.2
A4 5.8 4.9 3.6 3.6 9.9 11.3 4.8 3.5 7.9 7.1 9.7 11.1 3.6 4.1 9.9 9.4
A5 3.0 3.5 3.0 2.6 4.8 4.9 3.8 3.0 6.8 4.8 6.7 6.1 3.1 3.5 6.3 6.0
A6 5.4 6.3 5.3 4.7 6.7 8.6 5.4 4.6 7.8 6.1 8.7 9.0 6.9 6.6 9.3 8.5
? 6.2 6.5 4.9 5.0 7.8 8.8 5.4 4.5 8.1 6.2 9.3 9.4 5.4 6.7 9.4 9.1
sl 25.8 27.8 27.8 26.0 24.2 25.8 24.9 26.5 25.7 25.2 29.0 35.9 25.5 27.9 26.8 29.7
Table 2: Annotation time (sec/instance) per target/annotator/setting and average sentence length (sl)
5 Results
The basic idea behind active learning is to se-
lect the most informative instances for annotation.
The intuition behind ?more informative? is that
these instances support the learning process, so we
might need fewer annotated instances to achieve
a comparable classifier performance, which could
decrease the cost of annotation. On the other
hand, ?more informative? also means that these
instances might be more difficult to annotate, so it
is only fair to assume that they might need more
time for annotation, which increases annotation
cost. To answer the question of whether AL re-
duces annotation cost or not we have to check a)
how long it took the annotators to assign labels
to the AL samples compared to the randomly se-
lected instances, and b) how many instances we
need to achieve the best (or a sufficient) perfor-
mance in each setting. Furthermore, we want to
investigate the impact of active learning on the
distribution of the resulting training sets and study
the correlation between the performance of the
classifier trained on the annotated data and these
factors: the difficulty of the annotation task (as-
sessed by IAA), expertise and individual proper-
ties of the annotators.
5.1 Does AL speed up the annotation process
when working with noisy data?
Table 2 reports annotation times for each annota-
tor and target for random sampling (R) and uncer-
tainty sampling (U). For 5 out of 8 targets the time
needed for annotating in the AL setting (averaged
over all annotators) was higher than for annotat-
ing the random samples. To investigate whether
this might be due to the length of the sentences
in the samples, Table 2 shows the average sen-
tence length for random samples and AL samples
for each target lemma. Overall, the sentences se-
lected by the classifier during AL are longer (26.2
vs. 28.1 token per sentence), and thus may take
the annotators more time to read.3 However, we
could not find a significant correlation (Spearman
rank correlation test) between sentence length and
annotation time, nor between sentence length and
classifier confidence.
The three target lemmas which took longer to
annotate in the random setting are Ergebnis (re-
sult), Grund (reason) and Quelle (source of expe-
rience). This observation cannot be explained by
sentence length. While sentence length for Ergeb-
nis is nearly the same in both settings, for Grund
and Quelle the sentences picked by the classi-
fier in the AL setting are significantly longer and
therefore should have taken more time to anno-
tate. To understand the underlying reason for this
we have to take a closer look at the distribution of
word senses in the data.
5.2 Distribution of word senses in the data
In the literature it has been stated that AL implic-
itly alleviates the class imbalance problem by ex-
tracting more balanced data sets, while random
sampling tends to preserve the sense distribution
present in the data (Ertekin et al, 2007). We could
not replicate this finding when using noisy data
to guide the learning process. Table 3 shows the
distribution of word senses for the target lemma
Ergebnis a) in the gold standard, b) in the random
samples, and c) in the AL samples.
The variance in the distribution of word senses
in the random samples and the gold standard can
3The correlation between sentence length and annotation
time is not obvious, as the annotators only have to label one
target in each sentence. For ambiguous sentences, however,
reading time may be longer, while for the clear cases we do
not expect a strong effect.
953
Ergebnis
Frame gold (%) R (%) U (%)
Causation 4.0 4.8 3.7
Outcome 10.0 17.8 10.5
Finding out 24.0 26.2 8.2
Efficacy 2.0 0.8 0.1
Decision 11.0 5.1 3.2
Mathematics 1.0 1.6 0.4
Operating result 36.0 24.5 66.7
Competitive score 12.0 19.2 7.2
Table 3: Distribution of frames (word senses) for
the lemma Ergebnis in the gold standard (100 sen-
tences), in the random samples (R) and AL sam-
ples (U) (150 sentences each)
be explained by low inter-annotator agreement
caused by the high level of ambiguity for the tar-
get lemmas. The frame distribution in the data
selected by uncertainty sampling, however, cru-
cially deviates from those of the gold standard
and the random samples. A disproportionately
high 66% of the instances selected by the classi-
fier have been assigned the label Operating result
by the human annotators. This is the more sur-
prising as this frame is fairly easy for humans to
distinguish.
The classifier, however, proved to have seri-
ous problems learning this particular word sense
and thus repeatedly selected more instances of this
frame for annotation. As a result, the distribution
of word senses in the training set for the uncer-
tainty samples is highly skewed, having a nega-
tive effect on the overall classifier performance.
The high percentage of instances of the ?easy-to-
decide? frame Operating result explains why the
instances for Ergebnis took less time to annotate
in the AL setting. Thus we can conclude that an-
notating the same number of instances on average
takes more time in the AL setting, and that this
effect is not due to sentence length.
5.3 What works, what doesn?t, and why
For half of the target lemmas (Motiv, Konsequenz,
Quelle, Ausgang), we did obtain best results in
the AL setting (Table 4). For Ausgang and Mo-
tiv AL gives a substantial boost in classifier per-
formance of 5% and 7% accuracy, while the gains
for Konsequenz and Quelle are somewhat smaller
with 2% and 1%, and for Grund the highest accu-
racy was reached on both the AL and the random
Random Uncertainty
50 100 150 50 100 150
Anlass 0.85 0.86 0.85 0.84 0.85 0.84
Motiv 0.57 0.62 0.63 0.64 0.67 0.70
Konseq. 0.55 0.59 0.60 0.61 0.62 0.62
Quelle 0.56 0.53 0.54 0.52 0.52 0.57
Ergebnis 0.39 0.42 0.41 0.39 0.37 0.38
Resultat 0.31 0.35 0.37 0.32 0.34 0.34
Ausgang 0.67 0.69 0.69 0.68 0.72 0.74
Grund 0.48 0.47 0.47 0.47 0.44 0.48
Table 4: Avg. classifier performance (acc.) over
all annotators for the 8 target lemmas when train-
ing on 50, 100 and 150 annotated instances for
random samples and uncertainty samples
sample.
Figure 1 (top row) shows the learning curves
for Resultat, our worst-performing lemma, for the
classifier trained on the manually annotated sam-
ples for each individual annotator. The solid black
line represents the majority baseline, obtained by
assigning the most frequent word sense in the gold
standard to all instances. For both random and AL
settings, results are only slightly above the base-
line. The curves for the AL setting show how erro-
neous decisions can mislead the classifier, result-
ing in classifier accuracy below the baseline for
two of the annotators, while the learning curves
for these two annotators on the random samples
show the same trend as for the other 4 annotators.
For Konsequenz (Figure 1, middle), the classi-
fier trained on the AL samples yields results over
the baseline after around 25 iterations, while in
the random sampling setting it takes at least 100
iterations to beat the baseline. For Motiv (Figure
1, bottom row), again we observe far higher re-
sults in the AL setting. A possible explanation for
why AL seems to work for Ausgang, Motiv and
Quelle might be the higher IAA4 (? 0.825, 0.789,
0.768) as compared to the other target lemmas.
This, however, does not explain the good results
achieved on the AL samples for Konsequenz, for
which IAA was quite low with ? 0.554.
Also startling is the fact that AL seems to work
particularly well for one of the annotators (A6,
Figure 1) but not for others. Different possible ex-
planations come to mind: (a) the accuracy of the
annotations for this particular annotator, (b) the
4IAA was computed on the random samples, as the AL
samples do not include the same instances.
954
0 50 100 150
0.
10
0.
20
0.
30
0.
40
Resultat (Random Sampling)
no. of iterations
a
cc
u
ra
cy
Annotator
A1
A2
A3
A4
A5
A6
0 50 100 150
0.
10
0.
20
0.
30
0.
40
Resultat (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz (Random Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
50
0.
60
0.
70
Motiv (Random Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
50
0.
60
0.
70
Motiv (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
Figure 1: Active learning curves for Resultat, Konsequenz and Motiv (random sampling versus uncer-
tainty sampling; the straight black line shows the majority baseline)
955
Konsequenz A1 A2 A3 A4 A5 A6
human 0.80 0.72 0.89 0.73 0.89 0.76
maxent 0.60 0.63 0.67 0.60 0.63 0.64
Table 5: Acc. for human annotators against the
adjudicated random samples and for the classifier
instances selected by the classifier based on the
annotation decisions of the individual annotators,
and (c) the distribution of frames in the annotated
training sets for the different annotators.
To test (a) we evaluated the annotated ran-
dom samples for Konsequenz for each annotator
against the adjudicated gold standard. Results
showed that there is no strong correlation between
the accuracy of the human annotations and the
performance of the classifier trained on these an-
notations. The annotator for whom AL worked
best had a medium score of 0.76 only, while the
annotator whose annotations were least helpful
for the classifier showed a good accuracy of 0.80
against the gold standard.
Next we tested (b) the impact of the particu-
lar instances in the AL samples for the individ-
ual annotators on classifier performance. We took
all instances in the AL data set from A6, whose
annotations gave the greatest boost to the clas-
sifier, removed the frame labels and gave them
to the remaining annotators for re-annotation.
Then we trained the classifier on each of the re-
annotated samples and compared classifier perfor-
mance. Results for 3 of the remaining annotators
were in the same range or even higher than the
ones for A6 (Figure 2). For 2 annotators, however,
results remained far below the baseline.
This again shows that the AL effect is not di-
rectly dependent on the accuracy of the individual
annotators, but that particular instances are more
informative for the classifier than others. Another
crucial point is (c) the distribution of frames in
the samples. In the annotated samples for A1 and
A2 the majority frame for Konsequenz is Causa-
tion, while in the samples for the other annotators
Response was more frequent. In our test set Re-
sponse also is the most frequent frame, therefore it
is not surprising that the classifiers trained on the
samples of A3 to A6 show a higher performance.
This means that high-quality annotations (identi-
fied by IAA) do not necessarily provide the in-
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz: Re?annotated samples
no. of iterations
ac
cu
ra
cy
Annotator
A1
A2
A3
A4
A5
A6
Figure 2: Re-annotated instances for Konsequenz
(AL samples from annotator A6)
formation from which the classifier benefits most,
and that in a realistic annotation task address-
ing the class imbalance problem (Zhu and Hovy,
2007) is crucial.
6 Conclusions
We presented the first experiment applying AL in
a real-world scenario by integrating the approach
in an ongoing annotation project. The task and
annotation environment pose specific challenges
to the AL paradigm. We showed that annotation
noise caused by biased annotators as well as erro-
neous annotations mislead the classifier and result
in skewed data sets, and that for this particular task
no time savings are to be expected when applied
to a realistic scenario. Under certain conditions,
however, classifier performance can improve over
the random sampling baseline even on noisy data
and thus yield higher accuracy in the active learn-
ing setting. Critical features which seem to influ-
cence the outcome of AL are the amount of noise
in the data as well as the distribution of frames
in training- and test sets. Therefore, addressing
the class imbalance problem is crucial for apply-
ing AL to a real annotation task.
956
Acknowledgments
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3 and the MMCI
Cluster of Excellence).
References
Baldridge, Jason and Alexis Palmer. 2009. How well
does active learning actually work?: Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP 2009.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC-
2006.
Chan, Yee Seng and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of ACL-2007.
Chen, Jinying, Andrew Schein, Lyle Ungar, and
Martha Palmer. 2006. An empirical study of the
behavior of active learning for word sense disam-
biguation. In Proceedings of NAACL-2006, New
York, NY.
Cohn, David A., Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In Tesauro, G., D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705?712. The MIT Press.
Dang, Hoa Trang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
PhD dissertation, University of Pennsylvania, Penn-
sylvania, PA.
Donmez, Pinar and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of
CIKM08.
Erk, Katrin. 2005. Frame assignment as word sense
disambiguation. In Proceedings of the IWCS-6.
Ertekin, S?eyda, Jian Huang, L?eon Bottou, and Lee
Giles. 2007. Learning on the border: active learn-
ing in imbalanced data classification. In Proceed-
ings of CIKM ?07.
Hwa, Rebecca. 2004. Sample selection for statisti-
cal parsing. Computational Linguistics, 30(3):253?
276.
Laws, Florian and Heinrich Schu?tze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of Coling 2008.
Osborne, Miles and Jason Baldridge. 2004.
Ensemble-based active learning for parse selection.
In Proceedings of HLT-NAACL 2004.
Rehbein, Ines and Josef Ruppenhofer. 2010. Theres
no data like more data? revisiting the impact of
data size on a classification task. In Proceedings
of LREC-07, 2010.
Reichart, Roi, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for
linguistic annotations. In Proceedings of ACL-08:
HLT.
Ringger, Eric, Peter Mcclanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, and
Deryle Lonsdale. 2007. Active learning for part-
of-speech tagging: Accelerating corpus annotation.
In Proceedings of ACL Linguistic Annotation Work-
shop.
Tomanek, Katrin and Udo Hahn. 2009. Reducing
class imbalance during active learning for named
entity annotation. In Proceedings of the 5th Interna-
tional Conference on Knowledge Capture, Redondo
Beach, CA.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In Proceedings of
EMNLP-CoNLL 2007.
Zhu, Jingbo and Ed Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proceed-
ings of EMNLP-CoNLL 2007.
Zhu, Jingbo, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sam-
pling by uncertainty and density for word sense dis-
ambiguation and text classification. In Proceedings
of Coling 2008.
957
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 117?122,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Comparing methods for deriving intensity scores for adjectives
Josef Ruppenhofer?, Michael Wiegand?, Jasper Brandes?
?Hildesheim University
Hildesheim, Germany
{ruppenho|brandesj}@uni-hildesheim.de
?Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Abstract
We compare several different corpus-
based and lexicon-based methods for the
scalar ordering of adjectives. Among
them, we examine for the first time a low-
resource approach based on distinctive-
collexeme analysis that just requires a
small predefined set of adverbial modi-
fiers. While previous work on adjective in-
tensity mostly assumes one single scale for
all adjectives, we group adjectives into dif-
ferent scales which is more faithful to hu-
man perception. We also apply the meth-
ods to both polar and non-polar adjectives,
showing that not all methods are equally
suitable for both types of adjectives.
1 Introduction
Ordering adjectives by strength (e.g. good < great
< excellent) is a task that has recently received
much attention due to the central role of intensity
classification in sentiment analysis. However, the
need to assess the relative strength of adjectives
also applies to non-polar adjectives. We are thus
interested in establishing prior or lexical intensity
scores and rankings for arbitrary sets of adjectives
that evoke the same scale.1 We do not address con-
textualized intensity, i.e. the fact that e.g. negation
and adverbs such as very or slightly impact the per-
ceived intensity of adjectives.
We work with four scales of adjectives (cf. Ta-
ble 1). Our polar adjectives include 29 adjectives
referring to quality and 18 adjectives relating to
intelligence. Our non-polar adjectives include 8
dimensional adjectives denoting size and 22 de-
noting duration. The adjectives are taken, in part,
from FrameNet?s (Baker et al., 1998) frames for
1As there has been previous work on how to group adjec-
tives into scales (Hatzivassiloglou and McKeown, 1993), we
consider this grouping as given.
DESIRABILITY, MENTAL PROPERTY, SIZE and
DURATION DESCRIPTION. These scales are used
because they are prototypical and have multiple
members on the positive and negative half-scales.
We evaluate several corpus- and resource-based
methods that have been used to assign intensity
scores to adjectives. We compare them to a new
corpus-based method that is robust and of low
complexity, and which directly uses information
related to degree modification of the adjectives to
be orderered. It rests on the observation that ad-
jectives with different types of intensities co-occur
with different types of adverbial modifiers.2
POLAR ADJECTIVES
Intelligence Adjs. Intensity Level
brilliant very high positive
ingenious high positive
brainy, intelligent medium positive
smart low positive
bright very low positive
daft very low negative
foolish low negative
inane lower medium negative
dim upper medium negative
dim-witted, dumb, mindless high negative
brainless, idiotic, imbecillic, moronic, stupid very high negative
Quality Adjs. Intensity Level
excellent, extraordinary, first-rate, great, outstand-
ing, super, superb, superlative, tip-top, top-notch
very high positive
good high positive
decent upper medium positive
fine, fair lower medium positive
okay, average low positive
so-so very low positive
mediocre very low negative
second-rate, substandard low negative
inferior lower medium negative
bad, crappy, lousy, poor, third-rate medium negative
rotten upper medium negative
awful high negative
shitty very high negative
DIMENSIONAL ADJECTIVES
Size Adjs. Intensity Level
colossal, enormous, gargantuan, giant, gigantic, gi-
normous, humongous
high positive
big, huge, immense, large, oversize, oversized, vast medium positive
outsize, outsized low positive
diminutive, little, puny, small low negative
tiny medium negative
microscopic high negative
Duration Adjs. Intensity Level
long high positive
lengthy medium positive
extended low positive
momentaneous low negative
brief, fleeting, momentary medium negative
short high negative
Table 1: Adjectives used grouped by human gold
standard intensity classes
2The ratings we collected and our scripts are avail-
able at www.uni-hildesheim.de/ruppenhofer/
data/DISA_data.zip.
117
2 Data and resources
Table 2 gives an overview of the different corpora
and resources that we use to produce the different
scores and rankings that we want to compare. The
corpora and ratings will be discussed alongside the
associated experimental methods in ?4.1 and ?4.2.
Corpora Tokens Reference
BNC ?112 M (Burnard, 2007)
LIU reviews ?1.06 B (Jindal and Liu, 2008)
ukWaC ?2.25 B (Baroni et al., 2009)
Resources Entries Reference
Affective norms ?14 K (Warriner et al., 2013)
SoCAL ? 6.5 K (Taboada et al., 2011)
SentiStrength ? 2.5 K (Thelwall et al., 2010)
Table 2: Corpora and resources used
3 Gold standard
We collected human ratings for our four sets of ad-
jectives. All items were rated individually, in ran-
domized order, under conditions that minimized
bias. Participants were asked to use a horizontal
slider, dragging it in the desired direction, repre-
senting polarity, and releasing the mouse at the de-
sired intensity, ranging from ?100 to +100 .
Through Amazon Mechanical Turk (AMT), we
recruited subjects with the following qualifica-
tions: US residency, a HIT-approval rate of at least
96% (following Akkaya et al. (2010)), and 500
prior completed HITs. We collected 20 ratings for
each item but had to exclude some participants?
answers as unusable, which reduced our sample to
17 subjects for some items. In the raw data, all ad-
jectives had different mean ratings and their stan-
dard deviations overlapped. We therefore trans-
formed the data into sets of equally strong adjec-
tives as follows. For a given pair of adjectives of
identical polarity, we counted how many partici-
pants rated adjective A more intense than adjective
B; B more intense than A; or A as intense as B.
Whenever a simple majority existed for one of the
two unequal relations, we adopted that as our rela-
tive ranking for the two adjectives.3 The resulting
rankings (intensity levels) are shown in Table 1.
4 Methods
Our methods to determine the intensity of adjec-
tives are either corpus- or lexicon-based.
3In our data, there was no need to break circular rankings,
so we do not consider this issue here.
4.1 Corpus-based methods
Our first method, distinctive-collexeme analysis
(Collex) (Gries and Stefanowitsch, 2004) assumes
that adjectives with different types of intensities
co-occur with different types of adverbial modi-
fiers (Table 3). End-of-scale modifiers such as ex-
tremely or absolutely target adjectives with a par-
tially or fully closed scale, such as brilliant or out-
standing, which occupy extreme positions on the
intensity scale. ?Normal? degree modifiers such
as very or rather target adjectives with an open
scale structure (in the sense of Kennedy and Mc-
Nally (2005)), such as good or decent, which oc-
cupy non-extreme positions.
To determine an adjective?s preference for one
of the two constructions, the Fisher exact test
(Pedersen, 1996) is used. It makes no distribu-
tional assumptions and does not require a min-
imum sample size. The direction in which ob-
served values differ from expected ones indicates a
preference for one construction over the other and
the p-values are taken as a measure of the prefer-
ence strength. Our hypothesis is that e.g. an adjec-
tive A with greater preference for the end-of-scale
construction than adjective B has a greater inher-
ent intensity than B. We ran distinctive-collexeme
analysis on both the ukWaC and the BNC. We re-
fer to the output as Collex
ukWaC
and Collex
BNC
.
Note that this kind of method has not yet been ex-
amined for automatic intensity classification.
end-of-scale ?normal?
100%, fully, totally, absolutely,
completely, perfectly, entirely,
utterly, almost, partially, half,
mostly
all, as, awfully, enough, extremely,
fairly, highly, how, least, less, much,
pretty, quite, rather, so, somewhat,
sort of, terribly, too, very, well
Table 3: Domain independent degree modifiers (3
most freq. terms in the BNC; 3 most freq. terms
in the ukWaC)
Another corpus-based method we consider em-
ploys Mean star ratings (MeanStar) from prod-
uct reviews as described by Rill et al. (2012). Un-
like Collex, this method uses no linguistic prop-
erties of the adjectives themselves. Instead, it de-
rives intensity from the star rating scores that re-
viewers (manually) assign to reviews. We count
how many instances of each adjective i (of the set
of adjectives to classify) occur in review titles with
a given star rating (score) S
j
within a review cor-
pus. The intensity score is defined as the weighted
mean of the star ratings SR
i
=
?
n
j=1
S
i
j
n
.
Horn (1976) proposes pattern-based diagnos-
118
Pattern Any Int. Qual. Size Dur.
X or even Y 4118 1 34 9 3
X if not Y 3115 1 0 29 0
be X but not Y 2815 0 74 3 1
not only X but Y 1114 0 3 0 0
X and in fact Y 45 0 0 0 0
not X, let alone Y 4 0 0 0 0
not Y, not even X 4 0 1 0 0
Table 4: Phrasal patterns in the ukWaC
tics for acquiring information about the scalar
structure of adjectives. This was validated on ac-
tual data by Sheinman and Tokunaga (2009). A
pattern such as not just/only X but Y implies that
[Y] must always be stronger than [X] (as in It?s
not just good but great.).
The pattern-based approach has a severe cover-
age problem. Table 4 shows the results for 7 com-
mon phrasal patterns in the larger of our two cor-
pora, the ukWaC. The slots in the patterns are typ-
ically not filled by adjectives from the same scale.
For example, the most frequent pattern X or even
Y has 4118 instances in the ukWaC. Only 34 of
these have quality adjectives in both slots. Though
de Melo and Bansal (2013) have shown that the
coverage problems can be overcome and state-of-
the-art results obtained using web scale data in the
form of Google n-grams, we still set aside this
method here because of its great resource need.
4.2 Manually compiled lexical resources
In addition to the corpus methods, we also con-
sider some manually compiled resources. We want
to know if the polarity and intensity information in
them can be used for ordering polar adjectives.
One resource we consider are the affective rat-
ings (elicited with AMT) for almost 14,000 En-
glish words collected by Warriner et al. (2013).
They include scores of valence (unhappy to
happy), arousal (calm to aroused) and dominance
(in control to controlled) for each word in the list.
This scoring system follows the dimensional the-
ory of emotion by Osgood et al. (1957). We will
interpret each of these dimensions as a separate in-
tensity score, i.e. War
V al
, War
Aro
and War
Dom
.
Beyond Warriner?s ratings, we consider the two
polarity lexicons SentiStrength (Thelwall et al.,
2010) and SoCAL (Taboada et al., 2011) which
also assign intensity scores to polar expressions.
5 Experiments
For our evaluation, we compute the similarity be-
tween the gold standard and every other ranking
we are interested in in terms of Spearman?s rank
correlation coefficient (Spearman?s ?).
Polar Dimensional
Data set Intelligence Quality Duration Size
MeanStar 0.886 0.935 0.148 -0.058
SoCAL 0.848 0.953 NA 0.776
SentiStrength 0.874 0.880 NA NA
Collex
ukWaC
0.837 0.806 0.732 0.808
Collex
ukWaC
? 0.845 0.753 0.732 0.940
Collex
BNC
0.834 0.790 0.732 0.733
Collex
BNC
? 0.705 0.643 0.834 0.700
War
V al
0.779 0.916 -0.632 -0.031
War
Aro
0.504 -0.452 0.316 0.717
War
Dom
0.790 0.891 0.632 0.285
Table 5: Spearman rank correlations with the hu-
man gold standard (?: only the 3 most frequent
modifiers are used (see Table 3))
5.1 Data transformation
For the word lists with numeric scores (MeanStar
(?4.1); SentiStrength, SoCAL, War
V al
, War
Aro
and War
Dom
(?4.2)) we did as follows: Adjectives
not covered by the word lists were ignored. Ad-
jectives with equal scores were given tied ranks.
For the experiments involving distinctive
collexeme analysis in our two corpora (?4.1) we
proceeded as follows: The adjectives classified
as distinctive for the end-of-scale modification
constructions were put at the top and bottom of
the ranking according to polarity; the greater the
collostructional strength for the adjective as de-
noted by the p-value, the nearer it is placed to the
top or bottom of the ranking. The adjectives that
are distinctive for the normal degree modification
construction are placed between those adjectives
distinctive for the end-of-scale modification
construction, again taking polarity and collostruc-
tional strength into account. This time, the least
distinctive lemmas for the normal modification
construction come to directly join up with the
least distinctive lemmas for the end-of-scale
construction. In between the normal modifiers,
we place adjectives that have no preference for
one or the other construction, which may result
from non-occurrence in small data sets (see ?5.2).
5.2 Results
The results of the pairwise correlations between
the human-elicited gold standard and the rankings
derived from various methods and resources are
shown in Table 5. For polar adjectives, most rank-
ings correlate fairly well with human judgments.
Warriner?s arousal list, however, performs poorly
on quality adjectives, whereas MeanStar and War-
riner?s dominance and valence lists perform bet-
ter on quality than on intelligence adjectives. For
MeanStar, this does not come as a surprise as qual-
ity adjectives are much more frequent in prod-
119
uct reviews than intelligence adjectives. Overall,
it seems that MeanStar most closely matches the
human judgments that we elicited for the intel-
ligence adjectives. SentiStrength also produces
high scores. However, we do not have full confi-
dence in that result since SentiStrength lacks many
of our adjectives, thus leading to a possibly higher
correlation than would have been achieved if ranks
(scores) had been available for all adjectives.
The picture is very different for the dimensional
(non-polar) adjectives. While Collex still gives
very good results, especially on the ukWaC, the
MeanStar method and most Warriner lists produce
very low positive or even negative correlations.
This shows that estimating the intensity of non-
polar adjectives from metadata or ratings elicited
in terms of affect is not useful. It is much better to
consider their actual linguistic behavior in degree
constructions, which Collex does. SentiStrength
has no coverage for size or duration adjectives.
SoCAL covers 14 of the 22 size adjectives.
Although it never gives the best result, Collex
produces stable results across both corpora and
the four scales. It also requires the least human
effort by far. While all other rankings are pro-
duced with the help of heavy human annotation
(even MeanStar is completely dependent on manu-
ally assigned review scores), one has only to spec-
ify some domain-independent degree and end-of-
scale modifiers. Table 5 also shows that normally
a larger set of modifiers is necessary: only consid-
ering the 3 most frequent terms (Table 3) results in
a notably reduced correlation. As there is no con-
sistent significant difference between Collex
BNC
and Collex
ukWaC
even though the ukWaC is 20
times larger than the BNC (Table 2), we may
conclude that the smaller size of the BNC is al-
ready sufficient. This, however, raises the question
whether even smaller amounts of data than the full
BNC could already produce a reasonable intensity
ranking. Figure 1 plots the Spearman correlation
for our adjectives using various sizes of the BNC
corpus.4 It shows that further reducing the size of
the corpus causes some deterioration, most signifi-
cantly on the intelligence adjectives. The counter-
intuitive curve for duration adjectives is explained
as follows. Collex produces ties in the middle of
the scale when data is lacking (see ?5.1). Because
the smallest corpus slices contain no or very few
instances and because the gold standard does in-
4For each size, we average across 10 samples.
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  20  40  60  80  100
Sp
ea
rm
an
?s 
rh
o
% Size of BNC
Intelligence
Quality
Size
Duration
Figure 1: Reducing the size of the BNC
clude several ties, the results for duration adjec-
tives are inflated initially, when data is lacking.
6 Related work
Sentiment analysis on adjectives has been exten-
sively explored in previous work, however, most
work focussed on the extraction of subjective ad-
jectives (Wiebe, 2000; Vegnaduzzo, 2004; Wie-
gand et al., 2013) or on the detection of polar ori-
entation (Hatzivassiloglou and McKeown, 1997;
Kamps et al., 2004; Fahrni and Klenner, 2008).
Intensity can be considered in two ways, as a
contextual strength analysis (Wilson et al., 2004)
or as an out-of-context analysis, as in this paper.
Our main contribution is that we compare sev-
eral classification methods that include a new
effective method based on distinctive-collexeme
analysis requiring hardly any human guidance and
which moreover can solve the problem of intensity
assignment for all, not only polar adjectives.
7 Conclusion
We compared diverse corpus-based and lexicon-
based methods for the intensity classification of
adjectives. Among them, we examined for the first
time an approach based on distinctive-collexeme
analysis. It requires only a small predefined set
of adverbial modifiers and relies only on infor-
mation about individual adjectives rather than co-
occurrences of adjectives within patterns. As a re-
sult, it can be used with far less data than e.g. the
Google n-grams provide. Unlike the mean star ap-
proach, it needs no extrinsic meta-data and it can
handle both polar and non-polar adjectives. Ac-
cordingly, it appears to be very promising for cases
where only few resources are available and as a
source of evidence to be used in hybrid methods.
120
Acknowledgments
Michael Wiegand was funded by the German Fed-
eral Ministry of Education and Research (BMBF)
under grant no. 01IC12SO1X. The authors would
like to thank Maite Taboada for providing her sen-
timent lexicon (SoCAL) to be used for the experi-
ments presented in this paper.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In
NAACL-HLT 2010 Workshop on Creating Speech
and Language Data With Amazon?s Mechanical
Turk, pages 195?203, Los Angeles, CA, USA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley Framenet Project.
In Proceedings of the International Conference
on Computational Linguistics and Annual Meeting
of the Association for Computational Linguistics
(COLING/ACL), pages 86?90, Montre?al, Quebec,
Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetti. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Lou Burnard, 2007. Reference Guide for the British
National Corpus. Research Technologies Service
at Oxford University Computing Services, Oxford,
UK.
Gerard de Melo and Mohit Bansal. 2013. Good, Great,
Excellent: Global Inference of Semantic Intensities.
Transactions of the Association for Computational
Linguistics, 1:279?290.
Angela Fahrni and Manfred Klenner. 2008. Old Wine
or Warm Beer: Target Specific Sentiment Analysis
of Adjectives. In Proceedings of the Symposium on
Affective Language in Human and Machine, pages
60?63, Aberdeen, Scotland, UK.
Stefan Th. Gries and Anatol Stefanowitsch. 2004.
Extending collostructional analysis: a corpus-based
perspective on ?alternations?. International Journal
of Corpus Linguistics, 9(1):97?129.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1993. Towards the Automatic Identification of Ad-
jectival Scales: Clustering Adjectives According to
Meaning. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 172?182, Columbus, OH, USA.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. In Proceedings of the Conference on Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 174?181, Madrid, Spain.
Laurence Robert Horn. 1976. On the Semantic Prop-
erties of Logical Operators in English. Indiana Uni-
versity Linguistics Club.
Nitin Jindal and Bing Liu. 2008. Opinion Spam
and Analysis. In Proceedings of the international
conference on Web search and web data mining
(WSDM), pages 219?230, Palo Alto, USA.
Jaap Kamps, M.J. Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using Wordnet to Mea-
sure Semantic Orientations of Adjectives. In Pro-
ceedings of the Conference on Language Resources
and Evaluation (LREC), pages 1115?1118, Lisbon,
Portugal.
Christopher Kennedy and Louise McNally. 2005.
Scale Structure, Degree Modification, and the
Semantics of Gradable Predicates. Language,
81(2):345?338.
Charles E. Osgood, George Suci, and Percy Tannen-
baum. 1957. The Measurement of Meaning. Uni-
versity of Illinois Press.
Ted Pedersen. 1996. Fishing for exactness. In
Proceedings of the South-Central SAS Users Group
Conference, Austin, TX, USA.
Sven Rill, Johannes Drescher, Dirk Reinel, Joerg
Scheidt, Oliver Schuetz, Florian Wogenstein, and
Daniel Simon. 2012. A Generic Approach to Gen-
erate Opinion Lists of Phrases for Opinion Mining
Applications. In Proceedings of the KDD-Workshop
on Issues of Sentiment Discovery and Opinion Min-
ing (WISDOM), Beijing, China.
Vera Sheinman and Takenobu Tokunaga. 2009. Ad-
jScales: Differentiating between Similar Adjectives
for Language Learners. CSEDU, 1:229?235.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
Based Methods for Sentiment Analysis. Computa-
tional Linguistics, 37(2):267 ? 307.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
and Di Cai. 2010. Sentiment Strength Detec-
tion in Short Informal Text. Journal of the Ameri-
can Society for Information Science and Technology,
61(12):2544?2558.
Stefano Vegnaduzzo. 2004. Acquisition of Subjective
Adjectives with Limited Resources. In Proceedings
of the AAAI Spring Symposium on Exploring Atti-
tude and Affect in Text: Theories and Applications,
Stanford, CA, USA.
Amy Warriner, Victor Kuperman, and Marc Brysbaert.
2013. Norms of valence, arousal, and dominance for
13,915 english lemmas. Behavior Research Meth-
ods, Online First:1?17.
Janyce M. Wiebe. 2000. Learning Subjective Adjec-
tives from Corpora. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), pages
735?740, Austin, TX, USA.
121
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative Adjectives: An Unsu-
pervised Criterion to Extract Subjective Adjectives.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT/NAACL), pages 534?539, Atlanta, GA,
USA.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. In Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI),
pages 761?767, San Jose, CA, USA.
122
Proceedings of NAACL-HLT 2013, pages 534?539,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Predicative Adjectives: An Unsupervised Criterion to Extract Subjective
Adjectives
Michael Wiegand
Spoken Language Systems
Saarland University
michael.wiegand@lsv.uni-saarland.de
Josef Ruppenhofer
Dept. of Information Science
and Language Technology
Hildesheim University
ruppenho@uni-hildesheim.de
Dietrich Klakow
Spoken Language Systems
Saarland University
dietrich.klakow@lsv.uni-saarland.de
Abstract
We examine predicative adjectives as an unsu-
pervised criterion to extract subjective adjec-
tives. We do not only compare this criterion
with a weakly supervised extraction method
but also with gradable adjectives, i.e. another
highly subjective subset of adjectives that can
be extracted in an unsupervised fashion. In or-
der to prove the robustness of this extraction
method, we will evaluate the extraction with
the help of two different state-of-the-art senti-
ment lexicons (as a gold standard).
1 Introduction
Since the early work on sentiment analysis, it has
been established that the part of speech with the
highest proportion of subjective words are adjec-
tives (Wiebe et al, 2004) (see Sentence (1)). How-
ever, not all adjectives are subjective (2).
(1) A grumpy guest made some impolite remarks
to the insecure and inexperienced waitress.
(2) The old man wearing a yellow pullover sat on a
plastic chair.
This justifies the exploration of criteria to automati-
cally separate the subjective adjectives from the non-
subjective adjectives.
In this work, we are interested in an out-of-
context assessment of adjectives and therefore eval-
uate them with the help of sentiment lexicons. We
examine the property of being a predicative adjec-
tive as an extraction criterion. Predicative adjectives
are adjectives that do not modify the head of a noun
phrase, but which predicate a property of the refer-
ent of a noun phrase to which they are linked via a
copula or a control predicate (3).
We show that adjectives that frequently occur as
predicative adjectives are more likely to convey sub-
jectivity (in general) than adjectives that occur non-
predicatively, such as the pre-nominal (attributive)
adjectives (4). A subjective adjective may occur
both as a predicative (3) and a non-predicative (5)
adjective and also convey subjectivity in both con-
texts. However, a large fraction of non-subjective
adjectives do not occur as predicative adjectives (6).
(3) Her idea was brilliant.
(4) This is a financial problem.
(5) She came up with a brilliant idea.
(6) ?The problem is financial.
2 Related Work
The extraction of subjective adjectives has already
attracted some considerable attention in previous re-
search. Hatzivassiloglou and McKeown (1997) ex-
tract polar adjectives by a weakly supervised method
in which subjective adjectives are found by search-
ing for adjectives that are conjuncts of a pre-defined
set of polar seed adjectives. Wiebe (2000) in-
duces subjective adjectives with the help of distribu-
tional similarity. Hatzivassiloglou and Wiebe (2000)
examine the properties of dynamic, gradable and
polar adjectives as a means to detect subjectivity.
Vegnaduzzo (2004) presents another bootstrapping
method of extracting subjective adjectives with the
help of head nouns of the subjective candidates and
distributional similarity. Baroni and Vegnaduzzo
534
(2004) employ Web-based Mutual information for
this task and largely outperform the results produced
by Vegnaduzzo (2004).
3 Method
In the following, we present different features with
the help of which subjective adjectives can be ex-
tracted. For all resulting lists, the adjectives will be
ranked according to their frequency of co-occurring
with a particular feature.
3.1 Extracting Predicative Adjectives (PRD)
For the extraction of predicative adjectives, we ex-
clusively rely on the output of a dependency parser.
Predicative adjectives are usually connected to the
subject of the sentence via the dependency label
nsubj (Example (7) would correspond to Sen-
tence (3)).
(7) nsubj(brilliant, idea)
3.2 Extracting Gradable Adjectives (GRD)
As an alternative extraction method, we consider
morpho-syntactically gradable adjectives. Gradable
adjectives, such as nice or small, are adjectives ?that
can be inflected to specify the degree or grade of
something? (Wiktionary1). It has been stated in pre-
vious work that if some adjective can build a com-
parative (e.g. nicer) or a superlative (e.g. nicest),
then this adjective tends to be subjective (Hatzivas-
siloglou and Wiebe, 2000).
We employ the property of gradability, since,
firstly, it is very predictive towards subjectivity and,
secondly, it is the only other unsupervised criterion
currently known to extract subjective adjectives. For
the extraction of gradable adjectives, we rely, on the
one hand, on the part-of-speech labels JJR (com-
parative) and JJS (superlative). On the other hand,
we also consider adjectives being modified by ei-
ther more or most. For the former case, we need
to normalize the comparative (e.g. nicer) or superla-
tive (e.g. nicest) word form to the canonical positive
word form (e.g. nice) that is commonly used in sen-
timent lexicons.
1http://en.wiktionary.org/wiki/gradable
3.3 Weakly-Supervised Extraction (WKS)
We also consider a weakly supervised extraction
method in this paper, even though it is not strictly
fair to compare such a method with our two pre-
vious extraction methods which are completely un-
supervised. WKS considers an adjective subjective,
if it co-occurs as a conjunct of a previously defined
highly subjective (seed) adjective (8). In order to de-
tect such conjunctions, we employ the dependency
relation conj. By just relying on surface patterns,
we would not be able to exclude spurious conjunc-
tions in which other constituents than the two adjec-
tives are coordinated, such as Sentence (10).
(8) This approach is ill-conceived and ineffective.
(9) conj(ill-conceived,
ineffective)
(10) [Evil witches are stereotypically dressed in
black] and [good fairies in white].
We also experimented with other related weakly-
supervised extraction methods, such as mutual in-
formation of two adjectives at the sentence level (or
even smaller window sizes). However, using con-
junctions largely outperformed these alternative ap-
proaches so we only pursue conjunctions here.
4 Experiments
As a large unlabeled (training) corpus, we chose the
North American News Text Corpus (LDC95T21)
comprising approximately 350 million words of
news text. For syntactic analysis we use the Stan-
ford Parser (Finkel et al, 2005). In order to decide
whether an extracted adjective is subjective or not,
we employ two sentiment lexicons, namely the Sub-
jectivity Lexicon (SUB) (Wilson et al, 2005) and
SO-CAL (SOC) (Taboada et al, 2011). According to
the recent in-depth evaluation presented in Taboada
et al (2011), these two sentiment lexicons are the
most effective resources for English sentiment anal-
ysis. By taking into account two different lexicons,
which have also been built independently of each
other, we want to provide evidence that our pro-
posed criterion to extract subjective adjectives is not
sensitive towards a particular gold standard (which
would challenge the general validity of the proposed
method).
535
ALL other new last many first such next political federal own sev-
eral few good? former same economic public major recent
American second big? foreign high small local military fi-
nancial little? national
PRD able? likely available clear? difficult? important? ready?
willing? hard? good? due possible? sure? interested un-
likely necessary? high responsible? easy? strong? unable?
different enough open aware happy impossible? right?
wrong? confident?
Table 2: The 30 most frequent adjectives (ALL) and pred-
icative adjectives (PRD); ? marks matches with both sen-
timent lexicons SUB and SOC.
In order to produce the subjective seed adjec-
tives for the weakly supervised extraction, we col-
lect from the sentiment lexicon that we evaluate the
n most frequent subjective adjectives according to
our corpus. In order to further improve the quality
of the seed set, we only consider strong subjective
expressions from SUB and expressions with the in-
tensity strength ?5 from SOC.
Table 1 lists the size of the different sentiment lex-
icons and the rankings produced by the different ex-
traction methods. Of course, the list of all adjectives
from the corpus (ALL) is the largest list2 while PRD
is the second largest and GRD the third largest. The
rankings produced by WKS are fairly sparse, in par-
ticular the ones induced with the help of SOC; ap-
parently there are more frequently occurring strong
subjective adjectives in SUB than there are high in-
tensity adjectives in SOC.
4.1 Frequent Adjectives vs. Frequent
Predicative Adjectives
Table 2 compares the 30 most frequent adjectives
(ALL) and predicative adjectives (PRD). Not only
does this table show that the proportion of subjective
adjectives is much larger among the predicative ad-
jectives but we may also gain some insight into what
non-subjective adjectives are excluded. Among the
high frequent adjectives are many quantifiers (many,
few and several) and ordinal expressions (first, next
and last). In principle, most of these expressions
are not subjective. One may argue that these adjec-
tives behave like function words. Since they occur
2It will also contain many words erroneously tagged as ad-
jectives, however, this is unlikely to affect our experiments since
we only focus on the highly ranked (i.e. most frequent) words.
The misclassifications rather concern infrequent words.
very frequently, one might exclude some of them
by just ignoring the most frequent adjectives. How-
ever, there are also other types of adjectives, espe-
cially pertainyms (political, federal, economic, pub-
lic, American, foreign, local, military, financial and
national) that appear on this list which could not be
excluded by that heuristic. We found that these non-
subjective content adjectives are present throughout
the entire ranking and they are fairly frequent (on
the ranking). On the list of predicative adjectives all
these previous types of adjectives are much less fre-
quent. Many of them only occur on lower ranks (and
we assume that several of them only got on the list
due to parsing errors).
4.2 Comparison of the Different Extraction
Methods
Table 3 compares the precision of the different ex-
traction methods at different cut-off values. It is in-
teresting to see that for ALL in particular the higher
ranks are worse than the lower ranks (e.g. rank
1000). We assume that this is due to the high-
frequency adjectives which are similar to function
words (see Section 4.1). At all cut-off values, how-
ever, this baseline is beaten by every other method,
including our proposed method PRD. The two unsu-
pervised methods PRD and GRD perform on a par
with each other. On SUB, PRD even mostly out-
performs GRD. The precision achieved by WKS is
quite good. However, the coverage of this method
is low. It would require more seed expressions to
increase it, however, this would also mean consider-
ably more manual guidance.
Table 3 also shows that the precision of all ex-
traction methods largely drops on the lower ranks.
However, one should not conclude from that the ex-
traction methods proposed only work well for highly
frequent words. The drop can be mostly explained
by the fact that the two sentiment lexicons we use
for evaluation are finite (i.e. SUB: 4396 words/SOC:
2827 words (Table 1)), and that neither of these lexi-
cons (nor their union) represents the complete set of
all English subjective adjectives. Both lexicons will
have a bias towards frequently occurring subjective
expressions.
Inspecting the ranks 3001-3020 produced by PRD
as displayed in Table 4, for example, actually reveals
that there are still many more subjective adjectives
536
Lexicons Extraction Methods
WKS-5 WKS-10 WKS-25 WKS-50
SUB SOC ALL PRD GRD SUB SOC SUB SOC SUB SOC SUB SOC
4396 2827 212287 20793 7942 292 81 440 131 772 319 1035 385
Table 1: Statistics regarding the size (i.e. number of adjectives) of the different sentiment lexicons and rankings.
artistic? appealable airtight adjustable? activist? accommodat-
ing acclimated well-meaning weakest upsetting? unsurpassed
unsatisfying? unopposed unobtrusive? unobjectionable unem-
ployable understanding? uncharacteristic submerged speechless
Table 4: A set of entries PRD produces on lower ranks
(ranks 3001-3020); ? marks matches with either of the
sentiment lexicons SUB or SOC.
than the matches with our sentiment lexicons sug-
gest (e.g. appealable, accomodating, well-meaning,
weakest, unsurpassed, unopposed, unobjectionable,
unemployable, uncharacteristic or speechless). In
other words, these are less frequent words; many
of them are actually subjective even though they are
not listed in the sentiment lexicons. Moreover, irre-
spective of the drop in precision on the lower ranks,
PRD and GRD still outperform ALL on both senti-
ment lexicons (Table 3). Despite the sparseness of
our two gold standards on the lower ranks, we thus
have some indication that PRD and GRD are more
effective than ALL.
The problem of the evaluation of less-frequent
words could not be solved by an extrinsic evaluation,
either, e.g. by using the extracted lists for some text
classification task (at the sentence/document level).
The evaluation on contextual classification on cor-
pora would also be biased towards high-frequency
words (as the word distribution is typically Zipfian).
For instance, on the MPQA-corpus (Wiebe et al,
2005), i.e. the standard dataset for (fine-grained)
sentiment analysis, there is not a single mention of
the subjective words appealable, accommodating,
unsurpassed, unopposed, unobtrusive or speechless,
which were found among the lower ranks 3001-
3020.
4.3 How Different Are Gradable and
Predicative Adjectives?
Since in the previous experiments the proportion of
subjective adjectives was similar among the grad-
able adjectives and the predicative adjectives, we
may wonder whether these two extraction methods
produce the same adjectives. In principle, the set of
gradable adjectives extracted is much smaller than
the list of extracted predicative adjectives (see Ta-
ble 1). We found that the gradable adjectives are
a proper subset of predicative adjectives, which is
in line with the observation by (Bolinger, 1972,
21) that gradable adjectives (which he calls degree
words) readily occur predicatively whereas non-
gradable ones tend not to.
However, while gradability implies compatibility
with predicative use, the reverse is not true. Ac-
cordingly, we found adjectives that are definitely not
gradable among the predicative adjectives that are
subjective, for instance endless, insolvent, nonexis-
tent, stagnant, unavailable or untrue. This means
that with the criterion of predicative adjectives one
is able to extract relevant subjective adjectives that
cannot be caught by the gradability criterion alone,
namely complementary adjectives that refer to a sim-
ple binary opposition (Cruse, 1986, 198-99).
4.4 Intersecting the Different Unsupervised
Criteria
In this section, we want to find out whether we can
increase the precision by considering intersections
of the two different unsupervised extraction crite-
ria. (Due to the sparsity of WKS, it does not make
sense to include that method in this experiment.) In
our previous experiments it turned out that as far as
precision is concerned, our new proposed extraction
criterion was similar to the gradability criterion. If,
however, the intersection of these two criteria pro-
duces better results, then we have provided some
further proof of the effectiveness of our proposed
criterion (even though we may sacrifice some exclu-
sive subjective adjectives in PRD as pointed out in
Section 4.3). It would mean that this criterion is also
beneficial in the presence of the gradability criterion.
Figure 1 shows the corresponding results. We
computed the intersection of PRD and GRD at var-
537
ALL PRD GRD WKS-5 WKS-10 WKS-25 WKS-50
Rank n SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC
10 10.00 30.00 90.00 90.00 80.00 60.00 80.00 90.00 80.00 90.00 90.00 70.00 90.00 70.00
25 20.00 32.00 88.00 60.00 64.00 60.00 92.00 80.00 91.00 80.00 92.00 80.00 92.00 84.00
50 30.00 34.00 88.00 64.00 70.00 68.00 82.00 78.00 92.00 78.00 92.00 84.00 90.00 86.00
100 37.00 38.00 81.00 68.00 79.00 75.00 80.00 N/A 82.00 72.00 89.00 78.00 92.00 77.00
250 45.60 43.20 79.60 75.60 84.80 76.00 70.80 N/A 74.40 N/A 80.40 67.50 82.04 67.20
500 48.00 49.20 77.20 70.00 82.20 74.00 N/A N/A N/A N/A 72.60 N/A 75.20 N/A
1000 48.70 48.10 75.50 65.60 72.60 65.00 N/A N/A N/A N/A N/A N/A 64.30 N/A
1500 49.07 46.53 68.60 59.07 66.27 58.60 N/A N/A N/A N/A N/A N/A N/A N/A
2000 48.00 43.85 64.55 55.40 61.55 54.25 N/A N/A N/A N/A N/A N/A N/A N/A
2500 46.08 40.96 59.52 51.28 56.36 50.00 N/A N/A N/A N/A N/A N/A N/A N/A
3000 44.20 39.17 54.63 47.13 51.47 46.03 N/A N/A N/A N/A N/A N/A N/A N/A
Table 3: Precision at rank n of the different extraction methods; WKS-m denotes that for the extraction the m most
frequent subjective adjectives from the respective sentiment lexicon were considered as seed expressions.
ious cut-off values of n. The resulting intersection
comprises m ranks with m < n. The precision of
the intersection was consequently compared against
the precision of PRD and GRD at rank m. The figure
shows that with the exception of the higher ranks on
SUB (< 200) there is indeed a systematic increase
in precision when the intersection of PRD and GRD
is considered.
5 Conclusion
We examined predicative adjectives as a criterion
to extract subjective adjectives. As this extraction
method is completely unsupervised, it is preferable
to weakly supervised extraction methods since we
are not dependent on a manually designed high qual-
ity seed set and we obtain a much larger set of ad-
jectives. This extraction method is competitive if
not slightly better than gradable adjectives. In ad-
dition, combining these two unsupervised methods
by assessing their intersection results mostly in an
increase in precision.
Acknowledgements
This work was performed in the context of the Software-
Cluster project EMERGENT. Michael Wiegand was
funded by the German Federal Ministry of Education and
Research (BMBF) under grant no. ?01IC10S01?. The
authors would like to thank Maite Taboada for providing
her sentiment lexicon (SO-CAL) to be used for the exper-
iments presented in this paper.
 55
 60
 65
 70
 75
 80
 85
 500  1000  1500  2000  2500
Pr
ec
is
io
n
Top N Ranked Adjectives
Predicative Adjectives (PRD)
Gradable Adjectives (GRD)
Intersection of PRD and GRD
(a) Evaluation on SUB lexicon
 50
 55
 60
 65
 70
 75
 80
 500  1000  1500  2000  2500
Pr
ec
is
io
n
Top N Ranked Adjectives
Predicative Adjectives (PRD)
Gradable Adjectives (GRD)
Intersection of PRD and GRD
(b) Evaluation on SOC lexicon
Figure 1: Comparison of the individual rankings of GRD
and PRD with their intersection.
538
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identify-
ing Subjective Adjectives through Web-based Mutual
Information. In Proceedings of KONVENS, pages 17?
24, Vienna, Austria.
Dwight Bolinger. 1972. Degree words. Mouton, The
Hague.
David Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge, UK.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
363?370, Ann Arbor, MI, USA.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 174?181, Madrid, Spain.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of Adjective Orientation and Gradability on Sen-
tence Subjectivity. In Proceedings of the International
Conference on Computational Linguistics (COLING),
pages 299?305, Saarbru?cken, Germany.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267 ? 307.
Stefano Vegnaduzzo. 2004. Acquisition of Subjective
Adjectives with Limited Resources. In Proceedings of
the AAAI Spring Symposium on Exploring Attitude and
Affect in Text: Theories and Applications, Stanford,
CA, USA.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning Subjective
Language. Computational Linguistics, 30(3).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions
in Language. Language Resources and Evaluation,
39(2/3):164?210.
Janyce M. Wiebe. 2000. Learning Subjective Adjectives
from Corpora. In Proceedings of the National Confer-
ence on Artificial Intelligence (AAAI), pages 735?740,
Austin, TX, USA.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-level
Sentiment Analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 347?354, Vancouver, BC, Canada.
539
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43?51,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Evaluating the Impact of Coder Errors on Active Learning
Ines Rehbein
Computational Linguistics
Saarland University
rehbein@coli.uni-sb.de
Josef Ruppenhofer
Computational Linguistics
Saarland University
josefr@coli.uni-sb.de
Abstract
Active Learning (AL) has been proposed as a
technique to reduce the amount of annotated
data needed in the context of supervised clas-
sification. While various simulation studies
for a number of NLP tasks have shown that
AL works well on goldstandard data, there is
some doubt whether the approach can be suc-
cessful when applied to noisy, real-world data
sets. This paper presents a thorough evalua-
tion of the impact of annotation noise on AL
and shows that systematic noise resulting from
biased coder decisions can seriously harm the
AL process. We present a method to filter out
inconsistent annotations during AL and show
that this makes AL far more robust when ap-
plied to noisy data.
1 Introduction
Supervised machine learning techniques are still the
mainstay for many NLP tasks. There is, how-
ever, a well-known bottleneck for these approaches:
the amount of high-quality data needed for train-
ing, mostly obtained by human annotation. Active
Learning (AL) has been proposed as a promising ap-
proach to reduce the amount of time and cost for hu-
man annotation. The idea behind active learning is
quite intuitive: instead of annotating a large number
of randomly picked instances we carefully select a
small number of instances that are maximally infor-
mative for the machine learning classifier. Thus a
smaller set of data points is able to boost classifier
performance and to yield an accuracy comparable to
the one obtained when training the same system on
a larger set of randomly chosen data.
Active learning has been applied to several NLP
tasks like part-of-speech tagging (Ringger et al,
2007), chunking (Ngai and Yarowsky, 2000), syn-
tactic parsing (Osborne and Baldridge, 2004; Hwa,
2004), Named Entity Recognition (Shen et al,
2004; Laws and Schu?tze, 2008; Tomanek and Hahn,
2009), Word Sense Disambiguation (Chen et al,
2006; Zhu and Hovy, 2007; Chan and Ng, 2007),
text classification (Tong and Koller, 1998) or statis-
tical machine translation (Haffari and Sarkar, 2009),
and has been shown to reduce the amount of anno-
tated data needed to achieve a certain classifier per-
formance, sometimes by as much as half. Most of
these studies, however, have only simulated the ac-
tive learning process using goldstandard data. This
setting is crucially different from a real world sce-
nario where we have to deal with erroneous data
and inconsistent annotation decisions made by the
human annotators. While simulations are an indis-
pensable instrument to test different parameters and
settings, it has been shown that when applying AL
to highly ambiguous tasks like e.g. Word Sense
Disambiguation (WSD) with fine-grained sense dis-
tinctions, AL can actually harm the learning process
(Dang, 2004; Rehbein et al, 2010). Dang suggests
that the lack of a positive effect of AL might be due
to inconsistencies in the human annotations and that
AL cannot efficiently be applied to tasks which need
double blind annotation with adjudication to insure
a sufficient data quality. Even if we take a more opti-
mistic view and assume that AL might still be useful
even for tasks featuring a high degree of ambiguity,
it remains crucial to address the problem of annota-
tion noise and its impact on AL.
43
In this paper we present a thorough evaluation of
the impact of annotation noise on AL. We simulate
different types of coder errors and assess the effect
on the learning process. We propose a method to de-
tect inconsistencies and remove them from the train-
ing data, and show that our method does alleviate the
problem of annotation noise in our experiments.
The paper is structured as follows. Section 2 re-
ports on recent research on the impact of annota-
tion noise in the context of supervised classification.
Section 3 describes the experimental setup of our
simulation study and presents results. In Section 4
we present our filtering approach and show its im-
pact on AL performance. Section 5 concludes and
outlines future work.
2 Related Work
We are interested in the question whether or not AL
can be successfully applied to a supervised classifi-
cation task where we have to deal with a consider-
able amount of inconsistencies and noise in the data,
which is the case for many NLP tasks (e.g. sen-
timent analysis, the detection of metaphors, WSD
with fine-grained word senses, to name but a few).
Therefore we do not consider part-of-speech tag-
ging or syntactic parsing, where coders are expected
to agree on most annotation decisions. Instead,
we focus on work on AL for WSD, where inter-
coder agreement (at least for fine-grained annotation
schemes) usually is much lower than for the former
tasks.
2.1 Annotation Noise
Studies on active learning for WSD have been lim-
ited to running simulations of AL using gold stan-
dard data and a coarse-grained annotation scheme
(Chen et al, 2006; Chan and Ng, 2007; Zhu and
Hovy, 2007). Two exceptions are Dang (2004) and
Rehbein et al (2010) who both were not able to
replicate the positive findings obtained for AL for
WSD on coarse-grained sense distinctions. A pos-
sible reason for this failure is the amount of annota-
tion noise in the training data which might mislead
the classifier during the AL process. Recent work on
the impact of annotation noise on a machine learning
task (Reidsma and Carletta, 2008) has shown that
random noise can be tolerated in supervised learn-
ing, while systematic errors (as caused by biased an-
notators) can seriously impair the performance of a
supervised classifier even if the observed accuracy
of the classifier on a test set coming from the same
population as the training data is as high as 0.8.
Related work (Beigman Klebanov et al, 2008;
Beigman Klebanov and Beigman, 2009) has been
studying annotation noise in a multi-annotator set-
ting, distinguishing between hard cases (unreliably
annotated due to genuine ambiguity) and easy cases
(reliably annotated data). The authors argue that
even for those data points where the annotators
agreed on one particular class, a proportion of the
agreement might be merely due to chance. Fol-
lowing this assumption, the authors propose a mea-
sure to estimate the amount of annotation noise in
the data after removing all hard cases. Klebanov
et al (2008; 2009) show that, according to their
model, high inter-annotator agreement (?) achieved
in an annotation scenario with two annotators is no
guarantee for a high-quality data set. Their model,
however, assumes that a) all instances where anno-
tators disagreed are in fact hard cases, and b) that for
the hard cases the annotators decisions are obtained
by coin-flips. In our experience, some amount of
disagreement can also be observed for easy cases,
caused by attention slips or by a deviant interpre-
tation of some class(es) by one of the annotators,
and the annotation decision of an individual annota-
tor cannot so much be described as random choice
(coin-flip) but as systematically biased selection,
causing the types of errors which have been shown
to be problematic for supervised classification (Rei-
dsma and Carletta, 2008).
Further problems arise in the AL scenario where
the instances to be annotated are selected as a func-
tion of the sampling method and the annotation
judgements made before. Therefore, Beigman and
Klebanov Beigman (2009)?s approach of identify-
ing unreliably annotated instances by disagreement
is not applicable to AL, as most instances are anno-
tated only once.
2.2 Annotation Noise and Active Learning
For AL to be succesful, we need to remove system-
atic noise in the training data. The challenge we face
is that we only have a small set of seed data and no
information about the reliability of the annotations
44
assigned by the human coders.
Zhu et al (2008) present a method for detecting
outliers in the pool of unannotated data to prevent
these instances from becoming part of the training
data. This approach is different from ours, where
we focus on detecting annotation noise in the man-
ually labelled training data produced by the human
coders.
Schein and Ungar (2007) provide a systematic in-
vestigation of 8 different sampling methods for AL
and their ability to handle different types of noise
in the data. The types of noise investigated are a)
prediction residual error (the portion of squared er-
ror that is independent of training set size), and b)
different levels of confusion among the categories.
Type a) models the presence of unknown features
that influence the true probabilities of an outcome: a
form of noise that will increase residual error. Type
b) models categories in the data set which are intrin-
sically hard to disambiguate, while others are not.
Therefore, type b) errors are of greater interest to us,
as it is safe to assume that intrinsically ambiguous
categories will lead to biased coder decisions and
result in the systematic annotation noise we are in-
terested in.
Schein and Ungar observe that none of the 8
sampling methods investigated in their experiment
achieved a significant improvement over the random
sampling baseline on type b) errors. In fact, en-
tropy sampling and margin sampling even showed a
decrease in performance compared to random sam-
pling. For AL to work well on noisy data, we need
to identify and remove this type of annotation noise
during the AL process. To the best of our knowl-
edge, there is no work on detecting and removing
annotation noise by human coders during AL.
3 Experimental Setup
To make sure that the data we use in our simula-
tion is as close to real-world data as possible, we do
not create an artificial data set as done in (Schein
and Ungar, 2007; Reidsma and Carletta, 2008) but
use real data from a WSD task for the German verb
drohen (threaten).1 Drohen has three different word
senses which can be disambiguated by humans with
1The data has been provided by the SALSA project:
http://www.coli.uni-saarland.de/projects/salsa
a high accuracy.2 This point is crucial to our setup.
To control the amount of noise in the data, we need
to be sure that the initial data set is noise-free.
For classification we use a maximum entropy
classifier.3 Our sampling method is uncertainty sam-
pling (Lewis and Gale, 1994), a standard sampling
heuristic for AL where new instances are selected
based on the confidence of the classifier for predict-
ing the appropriate label. As a measure of uncer-
tainty we use Shannon entropy (1) (Zhang and Chen,
2002) and the margin metric (2) (Schein and Ungar,
2007). The first measure considers the model?s pre-
dictions q for each class c and selects those instances
from the pool where the Shannon entropy is highest.
?
?
c
qc log qc (1)
The second measure looks at the difference be-
tween the largest two values in the prediciton vector
q, namely the two predicted classes c, c? which are,
according to our model, the most likely ones for in-
stance xn, and selects those instances where the dif-
ference (margin) between the two predicted proba-
bilities is the smallest. We discuss some details of
this metric in Section 4.
Mn = |P (c|xn) ? P (c?|xn)| (2)
The features we use for WSD are a combination
of context features (word token with window size 11
and POS context with window size 7), syntactic fea-
tures based on the output of a dependency parser4
and semantic features based on GermaNet hyper-
onyms. These settings were tuned to the target verb
by (Rehbein et al, 2009). All results reported below
are averages over a 5-fold cross validation.
3.1 Simulating Coder Errors in AL
Before starting the AL trials we automatically sepa-
rate the 2,500 sentences into test set (498 sentences)
and pool (2,002 sentences),5 retaining the overall
distribution of word senses in the data set. We in-
sert a varying amount of noise into the pool data,
2In a pilot study where two human coders assigned labels to
a set of 100 sentences, the coders agreed on 99% of the data.
3http://maxent.sourceforge.net
4The MaltParser: http://maltparser.org
5The split has been made automatically, the unusual num-
bers are caused by rounding errors.
45
test pool
ALrand ALbias
% errors 0% 0% 30% 30%
drohen1-salsa 126 506 524 514
Comittment 129 520 522 327
Run risk 243 976 956 1161
Total 498 2002 2002 2002
Table 1: Distribution of word senses in pool and test sets
starting from 0% up to 30% of noise, increasing by
2% in each trial.
We assess the impact of annotation noise on ac-
tive learning in three different settings. In the first
setting, we randomly select new instances from the
pool (random sampling; rand). In the second setting,
we randomly replace n percent of all labels (from 0
to 30) in the pool by another label before starting
the active learning trial, but retain the distribution of
the different labels in the pool data (active learning
with random errors); (Table 1, ALrand, 30%). In
the third setting we simulate biased decisions by a
human annotator. For a certain fraction (0 to 30%)
of instances of a particular non-majority class, we
substitute the majority class label for the gold label,
thereby producing a more skewed distribution than
in the original pool (active learning with biased er-
rors); (Table 1, ALbias, 30%).
For all three settings (rand, ALrand, ALbias) and
each degree of noise (0-30%), we run active learning
simulations on the already annotated data, simulat-
ing the annotation process by selecting one new, pre-
labelled instance per trial from the pool and, instead
of handing them over to a human coder, assigning
the known (possibly erroneous) label to the instance
and adding it to the training set. We use the same
split (test, pool) for all three settings and all degrees
of noise, with identical test sets for all trials.
3.2 Results
Figure 1 shows active learning curves for the differ-
ent settings and varying degrees of noise. The hori-
zontal black line slightly below 0.5 accuracy shows
the majority baseline (the performance obtained
when always assigning the majority class). For all
degrees of randomly inserted noise, active learning
(ALrand) outperforms random sampling (rand) at an
early stage in the learning process. Looking at the
biased errors (ALbias), we see a different picture.
With a low degree of noise, the curves for ALrand
and ALbias are very similar. When inserting more
noise, performance for ALbias decreases, and with
around 20% of biased errors in the pool AL performs
worse than our random sampling baseline. In the
random noise setting (ALrand), even after inserting
30% of errors AL clearly outperforms random sam-
pling. Increasing the size of the seed data reduces
the effect slightly, but does not prevent it (not shown
here due to space limitations). This confirms the
findings that under certain circumstances AL per-
forms worse than random sampling (Dang, 2004;
Schein and Ungar, 2007; Rehbein et al, 2010). We
could also confirm Schein and Ungar (2007)?s obser-
vation that margin sampling is less sensitive to cer-
tain types of noise than entropy sampling (Table 2).
Because of space limitations we only show curves
for margin sampling. For entropy sampling, the gen-
eral trend is the same, with results being slightly
lower than for margin sampling.
4 Detecting Annotation Noise
Uncertainty sampling using the margin metric se-
lects instances for which the difference between
classifier predictions for the two most probable
classes c, c? is very small (Section 3, Equation 2).
When selecting unlabelled instances from the pool,
this metric picks examples which represent regions
of uncertainty between classes which have yet to be
learned by the classifier and thus will advance the
learning process. Our human coder, however, is not
the perfect oracle assumed in most AL simulations,
and might also assign incorrect labels. The filter ap-
proach has two objectives: a) to detect incorrect la-
bels assigned by human coders, and b) to prevent
the hard cases (following the terminology of Kle-
banov et al (2008)) from becoming part of the train-
ing data.
We proceed as follows. Our approach makes use
of the limited set of seed data S and uses heuris-
tics to detect unreliably annotated instances. We
assume that the instances in S have been validated
thoroughly. We train an ensemble of classifiers E
on subsets of S, and use E to decide whether or not
a newly annotated instance should be added to the
seed.
46
error=2%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
al_rand
al_bias
error=6%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=10%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=14%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=18%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
al_rand
al_bias
error=22%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=26%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=30%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
Figure 1: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to
1200 instances (solid circle (black): random sampling; filled triangle point-up (red): AL with random errors; cross
(green): AL with biased errors)
47
filter % error 0 4 8 12 16 20 24 28 30
- rand 0.763 0.752 0.736 0.741 0.726 0.708 0.707 0.677 0.678
entropy - ALrand 0.806 0.786 0.779 0.743 0.752 0.762 0.731 0.724 0.729
entropy y ALrand 0.792 0.786 0.777 0.760 0.771 0.748 0.730 0.729 0.727
margin - ALrand 0.795 0.795 0.782 0.771 0.758 0.755 0.737 0.719 0.708
margin y ALrand 0.800 0.785 0.773 0.777 0.765 0.766 0.734 0.735 0.718
entropy - ALbias 0.806 0.793 0.759 0.748 0.702 0.651 0.625 0.630 0.622
entropy y ALbias 0.802 0.781 0.777 0.735 0.702 0.678 0.687 0.624 0.616
margin - ALbias 0.795 0.789 0.770 0.753 0.706 0.684 0.656 0.634 0.624
margin y ALbias 0.787 0.781 0.787 0.768 0.739 0.700 0.671 0.653 0.651
Table 2: Accuracy for the different sampling methods without and with filtering after adding 500 instances to the seed
data
There are a number of problems with this ap-
proach. First, there is the risk of overfitting S. Sec-
ond, we know that classifier accuracy in the early
phase of AL is low. Therefore, using classifier pre-
dictions at this stage to accept or reject new in-
stances could result in poor choices that might harm
the learning proceess. To avoid this and to gener-
alise over S to prevent overfitting, we do not directly
train our ensemble on instances from S. Instead, we
create new feature vectors Fgen on the basis of the
feature vectors Fseed in S. For each class in S, we
extract all attribute-value pairs from the feature vec-
tors for this particular class. For each class, we ran-
domly select features (with replacement) from Fseed
and combine them into a new feature vector Fgen,
retaining the distribution of the different classes in
the data. As a result, we obtain a more general set of
feature vectors Fgen with characteristic features be-
ing distributed more evenly over the different feature
vectors.
In the next step we train n = 5 maximum en-
tropy classifiers on subsets of Fgen, excluding the
instances last annotated by the oracle. Each subset
is half the size of the current S. We use the ensemble
to predict the labels for the new instances and, based
on the predictions, accept or reject these, following
the two heuristics below (also see Figure 2).
1. If all n ensemble classifiers agree on one label
but disagree with the oracle ? reject.
2. If the sum of the margins predicted by the en-
semble classifiers is below a particular theshold
tmargin ? reject.
The threshold tmargin was set to 0.01, based on a
qualitative data analysis.
AL with Filtering:
Input: annotated seed data S,
unannotated pool P
AL loop:
? train classifier C on S
? let C predict labels for data in P
? select new instances from P according to
sampling method, hand over to oracle for
annotation
Repeat: after every c new instances
annotated by the oracle
? for each class in S, extract sets of
features Fseed
? create new, more general feature vectors
Fgen from this set (with replacement)
? train an ensemble E of n classifiers on
different subsets of Fgen
Filtering Heuristics:
? if all n classifier in E agree on label
but disagree with oracle:
? remove instance from seed
? if margin is less than threshold tmargin:
? remove instance from seed
Until done
Figure 2: Heuristics for filtering unreliable data points
(parameters used: initial seed size: 9 sentences, c = 10,
n = 5, tmargin = 0.01)
48
In each iteration of the AL process, one new in-
stance is selected using margin sampling. The in-
stance is presented to the oracle who assigns a label.
Then the instance is added to the seed data, thus in-
fluencing the selection of the next data point to be
annotated. After 10 new instances have been added,
we apply the filter technique which finally decides
whether the newly added instances will remain in
the seed data or will be removed.
Figure 3 shows learning curves for the filter ap-
proach. With increasing amount of errors in the
pool, a clear pattern emerges. For both sampling
methods (ALrand, ALbias), the filtering step clearly
improves results. Even for the noisier data sets with
up to 26% of errors, ALbias with filtering performs
at least as well as random sampling.
4.1 Error Analysis
Next we want to find out what kind of errors the
system could detect. We want to know whether the
approach is able to detect the errors previously in-
serted into the data, and whether it manages to iden-
tify hard cases representing true ambiguities.
To answer these questions we look at one fold of
the ALbias data with 10% of noise. In 1,200 AL it-
erations the system rejected 116 instances (Table 3).
The major part of the rejections was due to the ma-
jority vote of the ensemble classifiers (first heuris-
tic, H1) which rejects all instances where the en-
semble classifiers agree with each other but disagree
with the human judgement. Out of the 105 instances
rejected by H1, 41 were labelled incorrectly. This
means that we were able to detect around half of the
incorrect labels inserted in the pool.
11 instances were filtered out by the margin
threshold (H2). None of these contained an incor-
errors inserted in pool 173
err. instances selected by AL 93
instances rejected by H1+H2 116
instances rejected by H1 105
true errors rejected by H1 41
instances rejected by H2 11
true errors rejected by H2 0
Table 3: Error analysis of the instances rejected by the
filtering approach
rect label. On first glance H2 seems to be more le-
nient than H1, considering the number of rejected
sentences. This, however, could also be an effect of
the order in which we apply the filters.
The different word senses are evenly distributed
over the rejected instances (H1: Commitment 30,
drohen1-salsa 38, Run risk 36; H2: Commitment 3,
drohen1-salsa 4, Run risk 4). This shows that there
is less uncertainty about the majority word sense,
Run risk.
It is hard to decide whether the correctly labelled
instances rejected by the filtering method would
have helped or hurt the learning process. Simply
adding them to the seed data after the conclusion
of AL would not answer this question, as it would
merely tell us whether they improve classification
accuracy further, but we still would not know what
impact these instances would have had on the selec-
tion of instances during the AL process.
5 Conclusions
This paper shows that certain types of annotation
noise cause serious problems for active learning ap-
proaches. We showed how biased coder decisions
can result in an accuracy for AL approaches which
is below the one for random sampling. In this case,
it is necessary to apply an additional filtering step
to remove the noisy data from the training set. We
presented an approach based on a resampling of the
features in the seed data and guided by an ensemble
of classifiers trained on the resampled feature vec-
tors. We showed that our approach is able to detect
a certain amount of noise in the data.
Future work should focus on finding optimal pa-
rameter settings to make the filtering method more
robust even for noisier data sets. We also plan to im-
prove the filtering heuristics and to explore further
ways of detecting human coder errors. Finally, we
plan to test our method in a real-world annotation
scenario.
6 Acknowledgments
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3). We would like
to thank the anonymous reviewers for their helpful
comments and suggestions.
49
error=2%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
ALrand
ALrand_f
ALbias
ALbias_f
error=6%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=10%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=14%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=18%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
rand
ALrand
ALrand_f
ALbias
ALbias_f
error=22%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=26%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
error=30%
Training size
Ac
cu
ra
cy
0 250 600 950
0.
4
0.
5
0.
6
0.
7
0.
8
Figure 3: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to
1200 instances (solid circle (black): random sampling; open circle (red): ALrand; cross (green): ALrand with filtering;
filled triangle point-up (black): ALbias; plus (blue): ALbias with filtering)
50
References
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Compu-
tational Linguistics, 35:495?503, December.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In Pro-
ceedings of the Workshop on Human Judgements in
Computational Linguistics, HumanJudge ?08, pages
2?7, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of ACL-2007.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior of
active learning for word sense disambiguation. In Pro-
ceedings of NAACL-2006, New York, NY.
Hoa Trang Dang. 2004. Investigations into the role of
lexical semantics in word sense disambiguation. PhD
dissertation, University of Pennsylvania, Pennsylva-
nia, PA.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, pages 181?
189. Association for Computational Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Florian Laws and H. Schu?tze. 2008. Stopping crite-
ria for active learning of named entity recognition.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), Manch-
ester, UK, August.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of ACM-SIGIR, Dublin, Ireland.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 117?125, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In Proceed-
ings of HLT-NAACL 2004.
Ines Rehbein, Josef Ruppenhofer, and Jonas Sunde.
2009. Majo - a toolkit for supervised word sense dis-
ambiguation and active learning. In Proceedings of
the 8th Workshop on Treebanks and Linguistic Theo-
ries (TLT-8), Milano, Italy.
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), Beijing, China.
Dennis Reidsma and Jean Carletta. 2008. Reliability
measurement without limits. Computational Linguis-
tics, 34:319?326.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop,
Prague.
Andrew I. Schein and Lyle H. Ungar. 2007. Active learn-
ing for logistic regression: an evaluation. Machine
Learning, 68:235?265.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Tomanek and Udo Hahn. 2009. Reducing class
imbalance during active learning for named entity an-
notation. In Proceedings of the 5th International Con-
ference on Knowledge Capture, Redondo Beach, CA.
Simon Tong and Daphne Koller. 1998. Support vector
machine active learning with applications to text clas-
sification. In Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning (ICML-00),
pages 287?295.
Cha Zhang and Tsuhan Chen. 2002. An active learn-
ing framework for content-based information retrieval.
IEEE Transactions on Multimedia, 4(2):260?268.
Jingbo Zhu and Edward Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, Prague, Czech Republic.
Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sampling
by uncertainty and density for word sense disambigua-
tion and text classification. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), Manchester, UK.
51
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 45?50,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
We describe the SemEval-2010 shared
task on ?Linking Events and Their Partic-
ipants in Discourse?. This task is an ex-
tension to the classical semantic role label-
ing task. While semantic role labeling is
traditionally viewed as a sentence-internal
task, local semantic argument structures
clearly interact with each other in a larger
context, e.g., by sharing references to spe-
cific discourse entities or events. In the
shared task we looked at one particular as-
pect of cross-sentence links between ar-
gument structures, namely linking locally
uninstantiated roles to their co-referents
in the wider discourse context (if such
co-referents exist). This task is poten-
tially beneficial for a number of NLP ap-
plications, such as information extraction,
question answering or text summarization.
1 Introduction
Semantic role labeling (SRL) has been defined as
a sentence-level natural-language processing task
in which semantic roles are assigned to the syntac-
tic arguments of a predicate (Gildea and Jurafsky,
2002). Semantic roles describe the function of the
participants in an event. Identifying the seman-
tic roles of the predicates in a text allows knowing
who did what to whom when where how, etc.
However, semantic role labeling as it is cur-
rently defined misses a lot of information due to
the fact that it is viewed as a sentence-internal
task. Hence, relations between different local se-
mantic argument structures are disregarded. This
view of SRL as a sentence-internal task is partly
due to the fact that large-scale manual annotation
projects such as FrameNet
1
and PropBank
2
typ-
ically present their annotations lexicographically
by lemma rather than by source text.
It is clear that there is an interplay between lo-
cal argument structure and the surrounding dis-
course (Fillmore, 1977). In early work, Palmer et
al. (1986) discussed filling null complements from
context by using knowledge about individual pred-
icates and tendencies of referential chaining across
sentences. But so far there have been few attempts
to find links between argument structures across
clause and sentence boundaries explicitly on the
basis of semantic relations between the predicates
involved. Two notable exceptions are Fillmore and
Baker (2001) and Burchardt et al (2005). Fillmore
and Baker (2001) analyse a short newspaper arti-
cle and discuss how frame semantics could benefit
discourse processing but without making concrete
suggestions of how to model this. Burchardt et al
(2005) provide a detailed analysis of the links be-
tween the local semantic argument structures in a
short text; however their system is not fully imple-
mented either.
With the shared task, we aimed to make a first
step towards taking SRL beyond the domain of
individual sentences by linking local semantic ar-
gument structures to the wider discourse context.
The task addresses the problem of finding fillers
for roles which are neither instantiated as direct
dependents of our target predicates nor displaced
through long-distance dependency or coinstantia-
tion constructions. Often a referent for an unin-
stantiated role can be found in the wider context,
i.e. in preceding or following sentences. An ex-
ample is given in (1), where the CHARGES role
1
http://framenet.icsi.berkeley.edu/
2
http://verbs.colorado.edu/
?
mpalmer/
projects/ace.html
45
(ARG2 in PropBank) of cleared is left empty but
can be linked to murder in the previous sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was
cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the ob-
ject of jealousy are not overtly expressed as depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
This paper is organized as follows. In Section 2
we define how the concept of Null Instantiation
is understood in the task. Section 3 describes the
tasks to be performed, and Section 4, how they
are evaluated. Section 5 presents the participant
systems, and Section 6, their results. Finally, in
Section 7, we put forward some conclusions.
2 Null Instantiations
The theory of null complementation used here is
the one adopted by FrameNet, which derives from
the work of Fillmore (1986).
3
Briefly, omissions
of core arguments of predicates are categorized
along two dimensions, the licensor and the in-
terpretation they receive. The idea of a licensor
refers to the fact that either a particular lexical item
or a particular grammatical construction must be
present for the omission of a frame element (FE)
to occur. For instance, the omission of the agent in
(3) is licensed by the passive construction.
(3) No doubt, mistakes were made
0
Protagonist
.
The omission is a constructional omission be-
cause it can apply to any predicate with an appro-
priate semantics that allows it to combine with the
passive construction. On the other hand, the omis-
sion in (4) is lexically specific: the verb arrive al-
lows the Goal to be unspecified but the verb reach,
also a member of the Arriving frame, does not.
(4) We arrived 0
Goal
at 8pm.
3
Palmer et al?s (1986) treatment of uninstantiated ?essen-
tial roles? is very similar (see also Palmer (1990)).
The above two examples also illustrate the sec-
ond major dimension of variation. Whereas, in (3)
the protagonist making the mistake is only existen-
tially bound within the discourse (instance of in-
definite null instantiation, INI), the Goal location
in (4) is an entity that must be accessible to speaker
and hearer from the discourse or its context (def-
inite null instantiation, DNI). Finally, note that
the licensing construction or lexical item fully and
reliably determines the interpretation. Whereas
missing by-phrases have always an indefinite in-
terpretation, whenever arrive omits the Goal lexi-
cally, the Goal has to be interpreted as definite, as
it is in (4).
The import of this classification to the task here
is that we will concentrate on cases of DNI, be
they licensed lexically or constructionally.
3 Description of the Task
3.1 Tasks
We originally intended to offer the participants a
choice of two different tasks: a full task, in which
the test set was only annotated with gold stan-
dard word senses (i.e., frames) for the target words
and the participants had to perform role recogni-
tion/labeling and null instantiation linking, and a
NI only task, in which the test set was already
annotated with gold standard semantic argument
structures and the participants only had to recog-
nize definite null instantiations and find links to
antecedents in the wider context (NI linking).
However, it turned out that the basic semantic
role labeling task was already quite challenging
for our data set. Previous shared tasks have shown
that frame-semantic SRL of running text is a hard
problem (Baker et al, 2007), partly due to the fact
that running text is bound to contain many frames
for which no or little annotated training data are
available. In our case the difficulty was increased
because our data came from a new genre and do-
main (i.e., crime fiction, see Section 3.2). Hence,
we decided to add standard SRL, i.e., role recogni-
tion and labeling, as a third task (SRL only). This
task did not involve NI linking.
3.2 Data
The participants were allowed to make use of a va-
riety of data sources. We provided a training set
annotated with semantic argument structure and
null instantiation information. The annotations
were originally made using FrameNet-style and
46
later mapped semi-automatically to PropBank an-
notations, so that participants could choose which
framework they wanted to work in. The data for-
mats we used were TIGER/SALSA XML (Erk
and Pad?o, 2004) (FrameNet-style) and a modified
CoNLL-format (PropBank-style). As it turned
out, all participants chose to work on FrameNet-
style annotations, so we will not describe the Prop-
Bank annotation in this paper (see Ruppenhofer et
al. (2009) for more details).
FrameNet-style annotation of full text is ex-
tremely time-consuming. Since we also had to an-
notate null instantiations and co-reference chains
(for evaluation purposes, see Section 4), we could
only make available a limited amount of data.
Hence, we allowed participants to make use of ad-
ditional data, in particular the FrameNet and Prop-
Bank releases.
4
We envisaged that the participants
would want to use these additional data sets to
train SRL systems for the full task and to learn
something about typical fillers for different roles
in order to solve the NI linking task. The anno-
tated data sets we made available were meant to
provide additional information, e.g., about the typ-
ical distance between an NI and its filler and about
how to distinguish DNIs and INIs.
We annotated texts from two of Arthur Conan
Doyle?s fiction works. The text that served as
training data was taken from ?The Adventure of
Wisteria Lodge?. Of this lengthy, two-part story
we annotated the second part, titled ?The Tiger of
San Pedro?. The test set was made up of the last
two chapters of ?The Hound of the Baskervilles?.
We chose fiction rather than news because we be-
lieve that fiction texts with a linear narrative gen-
erally contain more context-resolvable NIs. They
also tend to be longer and have a simpler structure
than news texts, which typically revisit the same
facts repeatedly at different levels of detail (in the
so-called ?inverted pyramid? structure) and which
mix event reports with commentary and evalua-
tion, thus sequencing material that is understood
as running in parallel. Fiction texts should lend
themselves more readily to a first attempt at inte-
grating discourse structure into semantic role la-
beling. We chose Conan Doyle?s work because
most of his books are not subject to copyright any-
more, which allows us to freely release the anno-
tated data. Note, however, that this choice of data
4
For FrameNet we provided an intermediate release,
FrameNet 1.4 alpha, which contained more frames and lexi-
cal units than release 1.3.
means that our texts come from a different domain
and genre than many of the examples in FrameNet
and PropBank as well as making use of a some-
what older variety of English.
5
Table 1 provides basic statistics of the data sets.
The training data had 3.1 frames per sentence and
the test data 3.2, which is lower than the 8.8 frames
per sentence in the test data of the 2007 SemEval
task on Frame Semantic Structure Extraction.
6
We
think this is mainly the result of switching to a do-
main different from the bulk of what FrameNet
has made available in the way of full-text anno-
tation. In doing so, we encountered many new
frames and lexical units for which we could not
ourselves create the necessary frames and pro-
vide lexicographic annotations. The statistics also
show that null-instantiation is relatively common:
in the training data, about 18.7% of all FEs are
omitted, and in the test set, about 18.4%. Of the
DNIs, 80.9% had an antecedent in the training
data, and 74.2% in the test data.
To ensure a high quality of the annotations, both
data sets were annotated by more than one person
and then adjudicated. The training set was an-
notated independently by two experienced anno-
tators and then adjudicated by the same two peo-
ple. The test set was annotated by three annota-
tors and then adjudicated by the two experienced
annotators. Throughout the annotation and adju-
dication process, we discussed difficult cases and
also maintained a wiki. Additionally, we created a
software tool that checked the consistency of our
annotations against the frame, frame element and
FE-relation specifications of FrameNet and alerted
annotators to problems with their annotations. The
average agreement (F-score) for frame assignment
for pairs of annotators on the two chapters in the
test set ranges from 0.7385 to 0.7870. The agree-
ment of individual annotators with the adjudicated
gold standard ranges from 0.666 to 0.798. Given
that the gold standard for the two chapters features
228 and 229 different frame types, respectively,
this level of agreement seems quite good.
5
While PropBank provides annotations for the Penn Tree-
bank and is thus news-based, the lexicographic annotations
in FrameNet are extracted from the BNC, a balanced cor-
pus. The FrameNet full-text annotations, however, only cover
three domains: news, travel guides, and nuclear proliferation
reports.
6
The statistics in Table 1 and all our discussion of the
data includes only instances of semantic frames and ignores
the instances of the Coreference, Support, and Relativization
frames, which we labeled on the data as auxiliary informa-
tion.
47
data set sentences tokens frame inst. frame types overt FEs DNIs (resolved) INIs
train 438 7,941 1,370 317 2,526 303 (245) 277
test 525 9,131 1,703 452 3,141 349 (259) 361
Table 1: Statistics for the provided data sets
For the annotation of NIs and their links to the
surrounding discourse we created new guidelines
as this was a novel annotation task. We adopted
ideas from the annotation of co-reference informa-
tion, linking locally unrealized roles to all men-
tions of the referents in the surrounding discourse,
where available. We marked only identity rela-
tions but not part-whole or bridging relations be-
tween referents. The set of unrealized roles un-
der consideration includes only the core arguments
but not adjuncts (peripheral or extra-thematic roles
in FrameNet?s terminology). Possible antecedents
are not restricted to noun phrases but include all
constituents that can be (local) role fillers for
some predicate plus complete sentences (which
can sometimes fill roles such as MESSAGE).
4 Evaluation
As noted above, we allowed participants to ad-
dress three different tasks: SRL only, NI only,
full task. For role recognition and labeling we
used a standard evaluation set-up, i.e., accuracy for
role labeling and precision, recall, F-Score for role
recognition.
The NI linkings were evaluated slightly differ-
ently. In the gold standard, we identified refer-
ents for null instantiations in the discourse con-
text. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system is given credit
if the NI is linked to any of these expressions. To
achieve this we create equivalence sets for the ref-
erents of NIs (by annotating coreference chains).
If the NI is linked to any item in the equivalence
set, the link is counted as a true positive. We can
then define NI linking precision as the number
of all true positive links divided by the number of
links made by a system, and NI linking recall as
the number of true positive links divided by the
number of links between an NI and its equivalence
set in the gold standard. NI linking F-Score is
then the harmonic mean between NI linking preci-
sion and recall.
Since it may sometimes be difficult to deter-
mine the correct extent of the filler of an NI, we
score an automatic annotation as correct if it in-
cludes the head of the gold standard filler in the
predicted filler. However, in order to not favor sys-
tems which link NIs to very large spans of text to
maximize the likelihood of linking to a correct ref-
erent, we introduce a second evaluation measure,
which computes the overlap (Dice coefficient) be-
tween the words in the predicted filler (P) of an NI
and the words in the gold standard one (G):
NI linking overlap =
2|P ?G|
|P | + |G|
(5)
Example (6) illustrates this point. The verb
won in the second sentence evokes the Fin-
ish competition frame whose COMPETITION role
is omitted. From the context it is clear that the
competition role is semantically filled by their first
TV debate (head: debate) and last night?s debate
(head: debate) in the previous sentences. These
two expressions form the equivalence set for the
COMPETITION role in the last sentence. Any sys-
tem that would predict a linkage to a filler that
covers the head of either of these two expressions
would score a true positive for this NI. However,
a system that linked to last night?s debate would
have an NI linking overlap of 1 (i.e., 2*3/(3+3))
while a system linking the whole second sentence
Last night?s debate was eagerly anticipated to the
NI would have an overlap of 0.67 (i.e., 2*3/(6+3))
(6) US presidential rivals Republican John
McCain and Democrat Barack Obama
have yesterday evening attacked each
other over foreign policy and the econ-
omy, in [their first TV debate]
Competition
.
[Last night?s debate]
Competition
was ea-
gerly anticipated. Two national flash
polls suggest that [Obama]
Competitor
won
Finish competition
0
Competition
.
5 Participating Systems
While a fair number of people expressed an inter-
est in the task and 26 groups or individuals down-
loaded the data sets, only three groups submitted
48
results for evaluation. Feedback from the teams
that downloaded the data suggests that this was
due to coinciding deadlines and to the difficulty
and novelty of the task. Only the SEMAFOR
group addressed the full task, using a pipeline of
argument recognition followed by NI identifica-
tion and resolution. Two groups (GETARUNS++
and SEMAFOR) tackled the NI only task, and
also two groups, the SRL only task (CLR and SE-
MAFOR
7
).
All participating systems were built upon ex-
isting systems for semantic processing which
were modified for the task. Two of the groups,
GETARUNS++ and CLR, employed relatively
deep semantic processing, while the third, SE-
MAFOR, employed a shallower probabilistic sys-
tem. Different approaches were taken for NI link-
ing. The SEMAFOR group modeled NI linking as
a variant of role recognition and labeling by ex-
tending the set of potential arguments beyond the
locally available arguments to also include noun
phrases from the previous sentence. The system
then uses, among other information, distributional
semantic similarity between the heads of potential
arguments and role fillers in the training data. The
GETARUNS++ group applied an existing system
for deep semantic processing, anaphora resolution
and recognition of textual entailment, to the task.
The system analyzes the sentences and assigns its
own set of labels, which are subsequently mapped
to frame semantic categories. For more details of
the participating systems please consult the sepa-
rate system papers.
6 Results and Analysis
6.1 SRL Task
Argument Recognition Label
Prec. Rec. F1 Acc.
SHA 0.6332 0.3884 0.4812 0.3471
SEM 0.6528 0.4674 0.5448 0.4184
CLR 0.6702 0.1121 0.1921 0.1093
Table 2: Shalmaneser (SHA), SEMAFOR (SEM)
and CLR performance on the SRL task (across
both chapters)
The results on the SRL task are shown in Table
2. To get a better sense of how good the perfor-
mance of the submitted systems was on this task,
7
For SEMAFOR, this was the first step of their pipeline.
we applied the Shalmaneser statistical semantic
parser (Erk and Pad?o, 2006) to our test data and
report the results. Note, however, that we used a
Shalmaneser trained only on FrameNet version 1.3
which is different from the version 1.4 alpha that
was used in the task, so its results are lower than
what can be expected with release 1.4 alpha.
We observe that although the SEMAFOR and
the CLR systems score a higher precision than
Shalmaneser for argument recognition, the SE-
MAFOR system scores considerably higher recall
than Shalmaneser, whereas the CLR system scores
a much lower recall.
6.2 NI Task
Tackling the resolution of NIs proved to be a dif-
ficult problem due to a variety of factors. First,
the NI sub-task was completely new and involves
several steps of linguistic processing. It also is
inherently difficult in that a given FE is not al-
ways omitted with the same interpretation. For
instance, the Content FE of the Awareness frame
evoked by know is interpreted as indefinite in
the blog headline More babbling about what it
means to know but as definite in a discourse
like Don?t tell me you didn?t know!. Second,
prior to this SemEval task there was no full-text
training data available that contained annotations
with all the kinds of information that is relevant
to the task, namely overt FEs, null-instantiated
FEs, resolutions of null-instantiations, and coref-
erence. Third, the data we used also represented
a switch to a new domain compared to existing
FrameNet full-text annotation, which comes from
newspapers, travel guides, and the nuclear pro-
liferation domain. Our most frequent frame was
Observable bodyparts, whereas it is Weapons in
FrameNet full-text. Fourth, it was not well un-
derstood at the beginning of the task that, in cer-
tain cases, FrameNet?s null-instantiation annota-
tions for a given FE cannot be treated in isolation
of the annotations of other FEs. Specifically, null-
instantiation annotations interact with the set of re-
lations between core FEs that FrameNet uses in its
analyses. As an example, consider the CoreSet re-
lation, which specifies that from a set of core FEs
at least one must be instantiated overtly, though
more of them can be. As long as one of the FEs
in the set is expressed overtly, null-instantiation is
not annotated for the other FEs in the set. For
instance, in the Statement frame, the two FEs
49
Topic and Message are in one CoreSet and the
two FEs Speaker and Medium are in another. If
a frame instance occurs with an overt Speaker and
an overt Topic, the Medium and Message FEs are
not marked as null-instantiated. Automatic sys-
tems that treat each core FE separately, may pro-
pose DNI annotations for Medium and Message,
resulting in false positives.
Therefore, we think that the evaluation that we
initially defined was too demanding for a novel
task. It would have been better to give sepa-
rate scores for 1) ability to recognize when a core
FE has to be treated as null-instantiated; 2) abil-
ity to distinguish INI and DNI; and 3) ability to
find antecedents. The systems did have to tackle
these steps anyway and an analysis of the sys-
tem output shows that they did so with different
success. The two chapters of our test data con-
tained a total of 710 null instantiations, of which
349 were DNI and 361 INI. The SEMAFOR sys-
tem recognized 63.4% (450/710) of the cases of
NI, while the GETARUNS++ system found only
8.0% (57/710). The distinction between DNI and
INI proved very difficult, too. Of the NIs that
the SEMAFOR system correctly identified, 54.7%
(246/450) received the correct interpretation type
(DNI or INI). For GETARUNS++, the percentage
is higher at 64.2% (35/57), but also based on fewer
proposed classifications. A simple majority-class
baseline gives a 50.8% accuracy. Interestingly, the
SEMAFOR system labeled many more INIs than
DNIs, thus often misclassifying DNIs as INI. The
GETARUNS++ system applied both labels about
equally often.
7 Conclusion
In this paper we described the SemEval-2010
shared task on ?Linking Events and Their Partic-
ipants in Discourse?. The task is novel, in that it
tackles a semantic cross-clausal phenomenon that
has not been treated before in a task, namely, link-
ing locally uninstantiated roles to their coreferents
at the text level. In that sense the task represents
a first step towards taking SRL beyond the sen-
tence level. A new corpus of fiction texts has been
annotated for the task with several types of seman-
tic information: semantic argument structure, co-
reference chains and NIs. The results scored by
the systems in the NI task and the feedback from
participant teams shows that the task was more dif-
ficult than initially estimated and that the evalua-
tion should have focused on more specific aspects
of the NI phenomenon, rather than on the com-
pleteness of the task. Future work will focus on
modeling the task taking this into account.
Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant PI
154/9-3 and the Cluster of Excellence Multimodal Comput-
ing and Interaction (MMCI), respectively). Roser Morante?s
research is funded by the GOA project BIOGRAPH of the
University of Antwerp. We would like to thank Jinho Choi,
Markus Dr?ager, Lisa Fuchs, Philip John Gorinski, Russell
Lee-Goldman, Ines Rehbein, and Corinna Schorr for their
help with preparing the data and/or implementing software
for the task. Thanks also to the SemEval-2010 Chairs Katrin
Erk and Carlo Strapparava for their support during the task
organization period.
References
C. Baker, M. Ellsworth, K. Erk. 2007. SemEval-2007
Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt, A. Frank, M. Pinkal. 2005. Building text
meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
K. Erk, S. Pad?o. 2004. A powerful and versatile XML
format for representing role-semantic annotation. In
Proceedings of LREC-2004.
K. Erk, S. Pad?o. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC-06.
C. Fillmore, C. Baker. 2001. Frame semantics for text
understanding. In Proc. of the NAACL-01 Workshop
on WordNet and Other Lexical Resources.
C. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In A. Zampolli, ed.,
Fundamental Studies in Computer Science, No. 59,
55?88. North Holland Publishing.
C. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual
Meeting of the Berkeley Liguistics Society.
D. Gildea, D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
M. Palmer. 2009. Semeval-2010 task 10: Linking
events and their participants in discourse. In The
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-09).
50
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 104?109,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Semantic frames as an anchor representation for sentiment analysis
Josef Ruppenhofer
Department of Information Science
and Natural Language Processing
University of Hildesheim, Germany
ruppenho@uni-hildesheim.de
Ines Rehbein
SFB 632: Information Structure
German Department
Potsdam University, Germany
irehbein@uni-potsdam.de
Abstract
Current work on sentiment analysis is char-
acterized by approaches with a pragmatic fo-
cus, which use shallow techniques in the inter-
est of robustness but often rely on ad-hoc cre-
ation of data sets and methods. We argue that
progress towards deep analysis depends on
a) enriching shallow representations with lin-
guistically motivated, rich information, and b)
focussing different branches of research and
combining ressources to create synergies with
related work in NLP. In the paper, we propose
SentiFrameNet, an extension to FrameNet, as
a novel representation for sentiment analysis
that is tailored to these aims.
1 Introduction
Sentiment analysis has made a lot of progress on
more coarse-grained analysis levels using shallow
techniques. However, recent years have seen a trend
towards more fine-grained and ambitious analyses
requiring more linguistic knowledge and more com-
plex statistical models. Recent work has tried to pro-
duce relatively detailed summaries of opinions ex-
pressed in news texts (Stoyanov and Cardie, 2011);
to assess the impact of quotations from business
leaders on stock prices (Drury et al, 2011); to detect
implicit sentiment (Balahur et al, 2011); etc. Ac-
cordingly, we can expect that greater demands will
be made on the amount of linguistic knowledge, its
representation, and the evaluation of systems.
Against this background, we argue that it is
worthwhile to complement the existing shallow
and pragmatic approaches with a deep, lexical-
semantics based one in order to enable deeper analy-
sis. We report on ongoing work in constructing Sen-
tiFrameNet, an extension of FrameNet (Baker et al,
1998) offering a novel representation for sentiment
analysis based on frame semantics.
2 Shallow and pragmatic approaches
Current approaches to sentiment analysis are mainly
pragmatically oriented, without giving equal weight
to semantics. One aspect concerns the identifica-
tion of sentiment-bearing expressions. The anno-
tations in the MPQA corpus (Wiebe et al, 2005),
for instance, were created without limiting what an-
notators can annotate in terms of syntax or lexicon.
While this serves the spirit of discovering the variety
of opinion expressions in actual contexts, it makes
it difficult to match opinion expressions when us-
ing the corpus as an evaluation dataset as the same
or similar structures may be treated differently. A
similar challenge lies in distinguishing so-called po-
lar facts from inherently sentiment-bearing expres-
sions. For example, out of context, one would not
associate any of the words in the sentence Wages
are high in Switzerland with a particular evaluative
meaning. In specific contexts, however, we may
take the sentence as reason to either think positively
or negatively of Switzerland: employees receiving
wages may be drawn to Switzerland, while employ-
ers paying wages may view this state of affairs neg-
atively. As shown by the inter-annotator agreement
results reported by (Toprak et al, 2010), agreement
on distinguishing polar facts from inherently eval-
uative language is low. Unsurprisingly, many ef-
forts at automatically building up sentiment lexica
simply harvest expressions that frequently occur as
part of polar facts without resolving whether the sub-
jectivity clues extracted are inherently evaluative or
104
merely associated with statements of polar fact.
Pragmatic considerations also lead to certain ex-
pressions of sentiment or opinion being excluded
from analysis. (Seki, 2007), for instance, annotated
sentences as ?not opinionated? if they contain indi-
rect hearsay evidence or widely held opinions.
In the case of targets, the work by (Stoyanov and
Cardie, 2008) exhibits a pragmatic focus as well.
These authors distinguish between (a) the topic of
a fine-grained opinion, defined as the real-world ob-
ject, event or abstract entity that is the subject of the
opinion as intended by the opinion holder; (b) the
topic span associated with an opinion expression is
the closest, minimal span of text that mentions the
topic; and (c) the target span defined as the span
of text that covers the syntactic surface form com-
prising the contents of the opinion. As the defini-
tions show, (Stoyanov and Cardie, 2008) focus on
text-level, pragmatic relevance by paying attention
to what the author intends, rather than concentrat-
ing on the explicit syntactic dependent (their target
span) as the topic. This pragmatic focus is also in
evidence in (Wilson, 2008)?s work on contextual po-
larity classification, which uses features in the clas-
sification that are syntactically independent of the
opinion expression such as the number of subjectiv-
ity clues in adjoining sentences.
Among lexicon-driven approaches, we find that
despite arguments that word sense distinctions are
important to sentiment analysis (Wiebe and Mihal-
cea, 2006), often-used resources do not take them
into account and new resources are still being cre-
ated which operate on the more shallow lemma-level
(e.g. (Neviarouskaya et al, 2009)). Further, most
lexical resources do not adequately represent cases
where multiple opinions are tied to one expression
and where presuppositions and temporal structure
come into play. An example is the verb despoil:
there is a positive opinion by the reporter about the
despoiled entity in its former state, a negative opin-
ion about its present state, and (inferrable) negative
sentiment towards the despoiler. In most resources,
the positive opinion will not be represented.
The most common approach to the task is an in-
formation extraction-like pipeline. Expressions of
opinion, sources and targets are often dealt with sep-
arately, possibly using separate resources. Some
work such as (Kim and Hovy, 2006) has explored
the connection to role labeling. One reason not to
pursue this is that ?in many practical situations, the
annotation beyond opinion holder labeling is too ex-
pensive? (Wiegand, 2010, p.121). (Shaikh et al,
2007) use semantic dependencies and composition
rules for sentence-level sentiment scoring but do not
deal with source and target extraction. The focus on
robust partial solutions, however, prevents the cre-
ation of an integrated high-quality resource.
3 The extended frame-semantic approach
We now sketch a view of sentiment analysis on the
basis of an appropriately extended model of frame
semantic representation.1
Link to semantic frames and roles Since the pos-
sible sources and targets of opinion are usually iden-
tical to a predicate?s semantic roles, we add opinion
frames with slots for Source, Target, Polarity and
Intensity to the FrameNet database. We map the
Source and Target opinion roles to semantic roles
as appropriate, which enables us to use semantic
role labeling systems in the identification of opinion
roles (Ruppenhofer et al, 2008).
In SentiFrameNet al lexical units (LUs) that are
inherently evaluative are associated with opinion
frames. The language of polar facts is not associ-
ated with opinion frames. However, we show in the
longer version of this paper (cf. footnote 1) how we
support certain types of inferred sentiment. With re-
gard to targets, our representation selects as targets
of opinion the target spans of (Stoyanov and Cardie,
2008) rather than their opinion topics (see Section
2). For us, opinion topics that do not coincide with
target spans are inferential opinion targets.
Formal diversity of opinion expressions For fine-
grained sentiment-analysis, handling the full vari-
ety of opinion expressions is indispensable. While
adjectives in particular have often been found to
be very useful cues for automatic sentiment anal-
ysis (Wiebe, 2000; Benamara et al, 2007), eval-
uative meaning pervades all major lexical classes.
There are many subjective multi-words and idioms
such as give away the store and evaluative mean-
ing also attaches to grammatical constructions, even
ones without obligatory lexical material. An exam-
1We present a fuller account of our ideas in an unpublished
longer version of this paper, available from the authors? web-
sites.
105
ple is the construction exemplified by Him be a doc-
tor? The so-called What, me worry?-construction
(Fillmore, 1989) consists only of an NP and an in-
finitive phrase. Its rhetorical effect is to express the
speaker?s surprise or incredulity about the proposi-
tion under consideration. The FrameNet database
schema accommodates not only single and multi-
words but also handles data for a constructicon (Fill-
more et al, to appear) that pairs grammatical con-
structions with meanings.
Multiple opinions We need to accommodate multi-
ple opinions relating to the same predicate as in the
case of despoil mentioned above. Predicates with
multiple opinions are not uncommon: in a 100-item
random sample taken from the Pittsburgh subjectiv-
ity clues, 17 involved multiple opinions.
The use of opinion frames as described above en-
ables us to readily represent multiple opinions. For
instance, the verb brag in the modified Bragging
frame has two opinion frames. The first one has pos-
itive polarity and represents the frame-internal point
of view. The SPEAKER is the Source relative to the
TOPIC as the Target. The second opinion frame has
negative polarity, representing the reporter?s point of
view. The SPEAKER is the Target but the Source is
unspecified, indicating that it needs to be resolved
to an embedded source. For a similar representation
of multiple opinions in a Dutch lexical resource, see
(Maks and Vossen, 2011).
Event structure and presuppositions A complete
representation of subjectivity needs to include event
and presuppositional structure. This is necessary,
for instance, for predicates like come around (on) in
(1), which involve changes of opinion relative to the
same target by the same source. Without the pos-
sibility of distinguishing between attitudes held at
different times, the sentiment associated with these
predicates cannot be modeled adequately.
(1) Newsom is still against extending weekday me-
tering to evenings, but has COME AROUND on
Sunday enforcement.
For come around (on), we want to to distinguish
its semantics from that of predicates such as ambiva-
lent and conflicted, where a COGNIZER simultane-
ously holds opposing valuations of (aspects of) a tar-
get. Following FrameNet?s practice, we model pre-
supposed knowledge explicitly in SentiFrameNet by
Figure 1: Frame analysis for "Come around"
using additional frames and frame relations. A par-
tial analysis of come around is sketched in Figure 1.
We use the newly added Come around scenario
frame as a background frame that ties together all
the information we have about instances of coming
around. Indicated by the dashed lines are the SUB-
FRAMES of the scenario. Among them are three
instances of the Deciding frame (solid lines), all
related temporally (dashed-dotted) and in terms of
content to an ongoing Discussion. The initial dif-
ference of opinion is encoded by the fact that De-
ciding1 and Deciding2 share the same POSSIBILI-
TIES but differ in the DECISION. The occurrence
of Come_around leads to Deciding3, which has the
same COGNIZER as Deciding1 but its DECISION is
now identical to that in Deciding2, which has been
unchanged. The sentiment information we need is
encoded by simply stating that there is a sentiment
of positive polarity of the COGNIZER (as source)
towards the DECISION (as target) in the Deciding
frame. (This opinion frame is not displayed in the
graphic.) The Come around frame itself is not as-
106
sociated with sentiment information, which seems
right given that it does not include a DECISION as a
frame element but only includes the ISSUE.
For a discussion of how SentiFrameNet captures
factuality presuppositions by building on (Saur?,
2008)?s work on event factuality, we refer the inter-
ested reader to the longer version of the paper.
Modulation, coercion and composition Speakers
can shift the valence or polarity of sentiment-bearing
expressions through some kind of negation operator,
or intensify or attenuate the impact of an expression.
Despite these interacting influences, it is desirable to
have at least a partial ordering among predicates re-
lated to the same semantic scale; we want to be able
to find out from our resource that good is less pos-
itive than excellent, while there may be no ordering
between terrific and excellent. In SentiFrameNet, an
ordering between the polarity strength values of dif-
ferent lexical units is added on the level of frames.
The frame semantic approach also offers new per-
spectives on sentiment composition. We can, for in-
stance, recognize cases of presupposed sentiment,
as in the case of the noun revenge, which are not
amenable to shifting by negation: She did not take
revenge does not imply that there is no negative eval-
uation of some injury inflicted by an offender.
Further, many cases of what has been called va-
lence shifting for us are cases where the evaluation
is wholly contained in a predicate.
(2) Just barely AVOIDED an accident today.
(3) I had served the bank for 22 years and had
AVOIDED a promotion since I feared that I
would be transferred out of Chennai city.
If we viewed avoid as a polarity shifter and fur-
ther treated nouns like promotion and accident as
sentiment-bearing (rather than treating them as de-
noting events that affect somebody positively or neg-
atively) we should expect that while (2) has positive
sentiment, (3) has negative sentiment. But that is not
so: accomplished intentional avoiding is always pos-
itive for the avoider. Also, the reversal analysis for
avoid cannot deal with complements that have no in-
herent polarity. It readily follows from the coercion
analysis that I avoid running into her is negative but
that cannot be derived in e.g. (Moilanen and Pul-
man, 2007)?s compositional model which takes into
account inherent lexical polarity, which run (into)
lacks. The fact that avoid imposes a negative evalu-
ation by its subject on its object can easily be mod-
eled using opinion frames.
4 Impact and Conclusions
Deep analysis Tying sentiment analysis to frame se-
mantics enables immediate access to a deeper lexical
semantics. Given particular application-interests,
for instance, identifying statements of uncertainty,
frames and lexical units relevant to the task can
be pulled out easily from the general resource. A
frame-based treatment also improves over resources
such as SentiWordNet (Baccianella et al, 2008),
which, while representing word meanings, lacks any
representation of semantic roles.
Theoretical insights New research questions await,
among them: whether predicates with multiple opin-
ions can be distinguished automatically from ones
with only one, and whether predicates carrying fac-
tivity or other sentiment-related presuppositions can
be discovered automatically. Further, our approach
lets us ask how contextual sentiment is, and how
much of the analysis of pragmatic annotations can
be derived from lexical and syntactic knowledge.
Evaluation With a frame-based representation,
the units of annotation are pre-defined by a gen-
eral frame semantic inventory and systems can read-
ily know what kind of units to target as potential
opinion-bearing expressions. Once inherent seman-
tics and pragmatics are distinguished, the correct-
ness of inferred (pragmatic) targets and the polarity
towards them can be weighted differently from that
of immediate (semantic) targets and their polarity.
Synergy On our approach, lexically inherent sen-
timent information need not be annotated, it can be
imported automatically once the semantic frame?s
roles are annotated. Only pragmatic information
needs to be labeled manually. By expanding the
FrameNet inventory and creating annotations, we
improve a lexical resource and create role-semantic
annotationsas well as doing sentiment analysis.
We have proposed SentiFrameNet as a linguisti-
cally sound, deep representation for sentiment anal-
ysis, extending an existing resource. Our approach
complements pragmatic approaches, allows us to
join forces with related work in NLP (e.g. role label-
ing, event factuality) and enables new insights into
the theoretical foundations of sentiment analysis.
107
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2008. SEN-
TIWORDNET 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation LREC10, pages
2200?2204. European Language Resources Associa-
tion (ELRA).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet Project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics-Volume 1,
pages 86?90. Association for Computational Linguis-
tics.
Alexandra Balahur, Jes?s M. Hermida, and Andr?s Mon-
toyo. 2011. Detecting implicit expressions of senti-
ment in text based on commonsense knowledge. In
Proceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis
(WASSA 2.011), pages 53?60, Portland, Oregon, June.
Association for Computational Linguistics.
Farah Benamara, Sabatier Irit, Carmine Cesarano, Napoli
Federico, and Diego Reforgiato. 2007. Sentiment
analysis : Adjectives and adverbs are better than ad-
jectives alone. In Proc of Int Conf on Weblogs and
Social Media, pages 1?4.
Brett Drury, Ga?l Dias, and Lu?s Torgo. 2011. A con-
textual classification strategy for polarity analysis of
direct quotations from financial news. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing 2011, pages 434?440,
Hissar, Bulgaria, September. RANLP 2011 Organising
Committee.
Charles J. Fillmore, Russell Lee-Goldman, and Russell
Rhodes, to appear. Sign-based Construction Gram-
mar, chapter The FrameNet Constructicon. CSLI,
Stanford, CA.
Charles J. Fillmore. 1989. Grammatical construction
theory and the familiar dichotomies. In R. Dietrich
and C.F. Graumann, editors, Language processing in
social context, pages 17?38. North-Holland/Elsevier,
Amsterdam.
S.M. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news media
text. In Proceedings of the Workshop on Sentiment and
Subjectivity in Text, pages 1?8. Association for Com-
putational Linguistics.
Isa Maks and Piek Vossen. 2011. A verb lexicon model
for deep sentiment analysis and opinion mining appli-
cations. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 10?18, Portland, Ore-
gon, June. Association for Computational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Sentiful: Generating a reliable lexicon for senti-
ment analysis. In Affective Computing and Intelligent
Interaction and Workshops, 2009. ACII 2009. 3rd In-
ternational Conference on, pages 1?6. Ieee.
J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008.
Finding the sources and targets of subjective expres-
sions. In LREC, Marrakech, Morocco.
Roser Saur?. 2008. A Factuality Profiler for Eventualities
in Text. Ph.d., Brandeis University.
Yohei Seki. 2007. Crosslingual opinion extraction from
author and authority viewpoints at ntcir-6. In Proceed-
ings of NTCIR-6 Workshop Meeting, Tokyo, Japan.
Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mit-
suru. 2007. Assessing sentiment of text by semantic
dependency and contextual valence analysis. Affec-
tive Computing and Intelligent Interaction, pages 191?
202.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Veselin Stoyanov and Claire Cardie. 2011. Auto-
matically creating general-purpose opinion summaries
from text. In Proceedings of RANLP 2011, pages 202?
209, Hissar, Bulgaria, September.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of ACL-10, the 48th Annual Meeting of the Association
for Computational Linguistics, Portland. Association
for Computational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 1065?1072, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence (AAAI-
2000), pages 735?740, Austin, Texas.
108
Michael Wiegand. 2010. Hybrid approaches to senti-
ment analysis. Ph.D. thesis, Saarland University, Saar-
br?cken.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity
and Sentiment Analysis: Recognizing the Intensity, Po-
larity, and Attitudes of Private States. Ph.D. thesis,
University of Pittsburgh.
109
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 166?173,
Dublin, Ireland, August 23, 2014.
Dimensions of Metaphorical Meaning
Andrew Gargett
?
, Josef Ruppenhofer
?
and John Barnden
?
?
University of Birmingham
United Kingdom
{a.d.gargett|j.a.barnden}@cs.bham.ac.uk
?
Hildesheim University
Germany
ruppenho@uni-hildesheim.de
Abstract
Recent work suggests that concreteness and imageability play an important role in the mean-
ings of figurative expressions. We investigate this idea in several ways. First, we try to define
more precisely the context within which a figurative expression may occur, by parsing a corpus
annotated for metaphor. Next, we add both concreteness and imageability as ?features? to the
parsed metaphor corpus, by marking up words in this corpus using a psycholinguistic database of
scores for concreteness and imageability. Finally, we carry out detailed statistical analyses of the
augmented version of the original metaphor corpus, cross-matching the features of concreteness
and imageability with others in the corpus such as parts of speech and dependency relations, in
order to investigate in detail the use of such features in predicting whether a given expression is
metaphorical or not.
1 Introduction
Figurative language plays an important role in ?grounding? our communication in the world around us.
Being able to talk metaphorically about ?the journey of life?, ?getting into a relationship?, whether there
are ?strings attached? to a contract, or even just ?surfing the internet?, are important and useful aspects of
everyday discourse. Recent work on such phenomena has pursued this kind of grounding in interesting
directions, in particular, treating it as a way of injecting meanings that are somehow more ?concrete?
into daily discourse (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2013), or else as a way
of expressing abstract ideas in terms of concepts that are more ?imageable?, where imageability can be
defined as how easily a word can evoke mental imagery, (Cacciari and Glucksberg, 1995; Gibbs, 2006;
Urena and Faber, 2010). It should be noted that while it is generally accepted that imageability and
concreteness are highly correlated, recent work has shown they are contrastive, in particular, in their
interaction with additional cognitive dimensions such as affective states, so that they ?can no longer be
considered interchangeable constructs? (Dellantonio et al., 2014).
When someone describes love as a journey, or life as a test, one possible way of thinking about what
they are doing is that they are trying to cast a fairly abstract idea or concept, such as love or life, in
terms of more concrete or imageable experiences or concepts, such as a journey or a test. More formally,
metaphor can be characterized as the mapping of properties from a ?source? domain concept (typically
more concrete) on to a ?target? domain concept (typically more abstract). However, despite the ease
with which people understand both established metaphors such as these, or even more novel ones
1
,
and despite well-established findings about the ubiquity of metaphor in everyday discourse (Lakoff and
Johnson, 1980), explicit and testable proposals for the mechanisms underlying such forms of expression
remain elusive.
When looking for such mechanisms, it seems natural to start with the patterns of language that so
effectively convey metaphorical meanings. Along these lines, Deignan (2006) argues that:
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
Consider how readily one can make sense of a novel, yet metaphorical utterance, such as ?life is a box of chocolates? (from
a recent film), despite never having heard it before.
166
[M]etaphorical uses of words show differences in their grammatical behavior, or even their
word class, when compared to their literal use. In addition, it shows that metaphorical uses of
a word commonly appear in distinctive and relatively fixed syntactic patterns.
Focusing on word class of figurative expressions, so-called content words, such as nouns, adjectives
and verbs, have long been considered to more strongly convey figurative meanings than so-called func-
tion words, such as prepositions (Neuman et al., 2013; Tsvetkov et al., 2013). Yet, Steen et al. (2010)
find prepositions within figurative expressions to be as prevalent as content words such as nouns and
verbs, and indeed, for particular genres (such as academic texts) prepositions are the most frequently
attested part of speech for figurative expressions.
Further, there has been work on the interaction between metaphorical expressions and syntactically
defined contexts (e.g. phrase, clause, sentence). For example, Neuman et al. (2013) investigate how
metaphorical expressions apparently pattern by syntactically definable types, specifically: Type I, where
?a subject noun is associated with an object noun via a form of the copula verb to be? (e.g. ?God is a
king?), Type II having the verb as ?the focus of the metaphorical use representing the act of a subject
noun on an object noun? (e.g. ?The war absorbed his energy?), and Type III ?involve an adjective-
noun phrase? (e.g. ?sweet girl?). While such work yields a useful typology of figurative expressions,
such investigations into the syntactic patterns of figurative forms of expression is far from exhaustive. It
would be useful to take this further somewhat, with a more rigorous, syntactically precise definition of
the context of occurrence of figurative language.
Motivated by the above considerations, we have begun investigating the interaction of concreteness
and imageability with figurative meanings in several ways. This paper reports the initial stages of this
ongoing work into the dimensions of meaning of figurative language such as metaphor. As part of this
work, we have attempted to define more precisely the context within which a figurative expression may
occur, by parsing a corpus annotated for metaphor, the Vrije University Amsterdam Metaphor Corpus
(VUAMC) (Steen et al., 2010), using an off the shelf dependency parser, the Mate parser (Bohnet, 2010).
In addition, we add both concreteness and imageability as ?features? to the dependency parsed metaphor
corpus, by marking up words in this corpus using a psycholinguistic database of scores for concreteness
and imageability, the MRC Psycholinguistic Database (Wilson, 1988). In this paper, we report detailed
statistical analyses we have carried out of the resulting data set, cross-matching the features of concrete-
ness and imageability with others in the corpus such as parts of speech (PsOS) and dependency relations,
in order to investigate in detail the use of such features in determining whether a given expression is
metaphorical or not.
2 Method
2.1 Data
Our data comes from the Vrije University Amsterdam Metaphor Corpus (VUAMC), consisting of ap-
proximately 188,000 words selected from the British National Corpus-Baby (BNC-Baby), and annotated
for metaphor using the Metaphor Identification Procedure (MIP) (Steen et al., 2010). The corpus has four
registers, of between 44,000 and 50,000 words each: academic texts, news texts, fiction, and conversa-
tions. We have chosen this corpus because of its broad coverage and its rich metaphorical annotation.
2.2 Procedure
PRE-PROCESSING. We have enriched the VUAMC in several ways. First, we have parsed the corpus
using the graph-based version of the Mate tools dependency parser (Bohnet, 2010), adding rich syntactic
information.
2
Second, we have incorporated the MRC Psycholinguistic Database
3
(Wilson, 1988), a
dictionary of 150,837 words, with different subsets of these words having been rated by human subjects
in psycholinguistic experiments. Of special note, the database includes 4,295 words rated with degrees of
abstractness, these ratings ranging from 158 (meaning highly abstract) to 670 (meaning highly concrete),
2
https://code.google.com/p/mate-tools/
3
http://ota.oucs.ox.ac.uk/headers/1054.xml
167
and also 9,240 words rated for degrees of imageability, which can be defined as how easily a word can
evoke mental imagery, these ratings also ranging between 100 and 700 (a higher score indicating greater
imageability). It should be noted that it has long been known that the concreteness and imageability
scores are highly correlated (Paivio et al., 1968), however, there are interesting differences between
these sets of scores (Dellantonio et al., 2014), and we are currently investigating these differences in
further studies (see Section (4) below). These scores have been used extensively for work that is similar
to ours, e.g. (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2013), and while our work is also
largely computational in approach, a significant component of our research is devoted to investigating in
some detail the cognitive aspects of figurative meanings.
EXPERIMENTAL DESIGN. We carried out five studies, all beginning with pre-processing tasks to pre-
pare the data (additional to those listed immediately above, undertaken to prepare the entire corpus for
these studies). We list the aims, details of pre-processing, and hypotheses below.
Study 1. This study initiated the investigation, and guided the setting up of the computational frame-
work for our broader research activities. The VUAMC was extended with dependency information from
the Mate dependency parser, enabling extraction of both dependency information and metaphorical anno-
tation for each VUAMC word.
4
Hypotheses: H
1
= nouns are more prevalent in metaphorical expressions
than verbs, verbs more than adjectives, adjectives more than prepositions; H
2
= metaphorical expressions
are more likely to occur in sentences in which other metaphorical expressions occur.
Study 2. This study aimed to evaluate claims about syntactically-defined metaphor types (Neuman et
al., 2013), and search for other types. The structure of a sentence revealed by a dependency parse is
based on the relation between a word, known as a head, and its dependents. This extended VUAMC data
provided variables for metaphor types I, II and III, respectively, Noun-BE-Noun, Noun-ActiveVerb-Noun,
and Adjective-Noun, as well as the discovery of additional metaphor types.
Study 3. Going further than Studies 1 and 2, this study extended the VUAMC data with MRC con-
creteness and imageability scores, plus further processing of the VUAMC corpus, assigning MRC scores
to each item in this corpus. Note here that the VUAMC data was examined word-by-word (rather than
sentence-by-sentence, as for Study 2). However, the VUAMC data set is much larger than the MRC
data set, so that many VUAMC words have no MRC scores. To smooth this discrepancy, for this initial
stage of our investigations, we have implemented the fairly rudimentary approach of calculating global
MRC scores by: first, from VUAMC words with MRC scores, a global average MRC score for each part
of speech of the VUAMC data was calculated, and second, those VUAMC words without MRC scores
(i.e. missing from the MRC database) were assigned a global score based on their part of speech. Of
course, a range of possible smoothing strategies are available, and while at this stage we are employing
a rather crude averaging of the score, this is an area we intend to investigate further in follow-up studies,
inspired by the more sophisticated methods that have been implemented by others, e.g. (Feng et al.,
2011; Tsvetkov et al., 2013).
5
For this study, we sought to answer the following two questions: Do
concreteness and imageability scores correlate with metaphoricity of expressions? Do concreteness and
imageability scores correlate with parts of speech of metaphorical expressions?
Study 4. This study replicated Study 3, but also considered the data sentence-by-sentence (cf. Study
2), to integrate syntactic information and MRC score. Examining MRC scores across syntactically fine-
grained contexts, enabled collecting information about heads, their dependent/s, as well as the depen-
dency relation/s, and this information could then be examined to see if it helped to distinguish between
literal and nonliteral items. This approach enables us to investigate in detail the contexts in which con-
creteness and imageability with figurative meanings, a key aim of our work, as pointed out in Section (1).
Hypotheses: H
3
= metaphorical expressions are more likely to occur in sentences where the head is more
4
For more details on the VUAMC categories, see: http://www.natcorp.ox.ac.uk/docs.
5
This work is part of a larger project, http://www.cs.bham.ac.uk/
?
gargetad/genmeta-about.html,
which aims to annotate larger web-based corpora of discourse on illness and political conflict.
168
Figure 1: Plots of concreteness vs. imageability scores for literal vs. nonliteral words in the VUAMC
(Conc=concreteness, Imag=imageability, NL=nonliteral, L=literal)
concrete than the dependent/s; H
4
= metaphorical expressions are more likely to occur in sentences where
the head is more imageable than the dependent/s.
Study 5. Finally, this study finished by examining the relative importance of the variables identified
so far, for predicting literal vs. nonliteral expressions, another key aim of our work (as mentioned in
Section (1)). We implemented this study through building and evaluating a series of logistic regression
models.
3 Results
3.1 Study 1
The first hypothesis listed for this study above has not been refuted, with the percentage of all non-
literal sentences in our collection having only one nonliteral item being 27%, while the percentage
of all nonliteral sentences having more than one nonliteral item is 73%: so after finding one nonlit-
eral item in a sentence, we can expect to find more. Regarding the second hypothesis, our data set
had the following proportions of occurrence of nonliteral items according to parts of speech: Adjec-
tives=10.8%, Prepositions=28%, Nouns=22.5%, Verbs=27%, Adverbs=5%, Pronouns=0.2%, Conjunc-
tions=0.5%, Other=6%. Consistent with Steen et al. (2010), that function words can occur more fre-
quently than content words in metaphorical expressions, we found prepositions to be far more prevalent
than adjectives in such expressions, and occur about as frequently as verbs.
3.2 Study 2
We found the following percentages of metaphor types (across all metaphors): Type I = 3.06%, Type
II = 33.53%, Type III = 7.56% (note the reversal for Type II vs. Type III, contrary to (Neuman et al.,
2013)). Such differences may be due to differences in data sets, as well as different syntactic models.
6
Additionally, we found a pattern of expression we have dubbed ?Type IV? metaphors, consisting of
preposition as head, together with noun phrase dependents (e.g. ?at the end of the decade?, ?after the
break-up?): these account for 35.53% of the total occurrence of metaphors.
3.3 Study 3
The boxplots in Figure (1) compare concreteness and imageability scores for nonliteral vs. literal items,
suggesting nonliteral and literal items are indistinguishable from one another with respect to their con-
creteness and imageability scores. Next, we further categorise our data according to parts of speech, the
boxplots in Figure (2) showing results for concreteness, and the boxplots Figure (3) presenting results for
imageability ? these figures suggest literal and nonliteral items can be better distinguished, with respect
to their concreteness and imageability scores, by increasing the granularity of annotation of the context
(e.g. by including parts of speech). Note that imageability scores for prepositions seem to show the
6
Neuman et al. (2013) used the Stanford Dependency Parser (De Marneffe and Manning, 2008).
169
Figure 2: Plots of concreteness scores for literal vs. nonliteral/metaphorical words in the VUAMC,
grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition)
Figure 3: Plots of imageability scores for literal vs. nonliteral/metaphorical words in the VUAMC,
grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition)
clearest distinction between literal vs. nonliteral items. But can we do better? What further categories
in the data should we focus on in order to achieve even clearer distinctions between literal vs. nonliteral
items?
3.4 Study 4
Figures (4) and (5) show the variation that can be achieved by making a more fine-grained distinction
within our data set between heads and their dependents, plus MRC scores of each. Figure (4) shows that
concreteness scores enable distinguishing between literal and nonliteral items for some parts of speech,
such as nouns, where nonliteral heads have higher MRC scores than their dependents, distinct from
literal head nouns (verbs appear to make no such a distinction). While literal and nonliteral head prepo-
sitions both seem indistinguishable from their dependents in terms of concreteness scores, nonliteral
head prepositions seem to have imageability scores quite distinct from their dependents.
3.5 Study 5
Based on our previous studies, we here examine the following 5 independent variables: POS = part
of speech of the head, C Head = concreteness score of the head, I Head = imageability score of the
head, C Dep = average concreteness score of the dependents, I Dep = average imageability score of
the dependents. Table (1) sets out the results for 7 logistic regression models we tested, and formulas
representing these models M1 to M7 are as follows (Nonliteral of course being the dependent variable,
its values being either ?yes, this is nonliteral? or ?no, this is not nonliteral?):
170
Figure 4: Plots of concreteness scores for literal vs. nonliteral/metaphorical heads vs. their dependents,
in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition,
h=head, d=dependents)
Figure 5: Plots of imageability scores for literal vs. nonliteral/metaphorical heads vs. their dependents,
in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition,
h=head, d=dependents)
M1: Nonliteral ? POS + C Head + I Head + C Dep + I Dep
M2: Nonliteral ? C Head + I Head
M3: Nonliteral ? POS + C Head + I Head
M4: Nonliteral ? POS + C Head + C Dep + I Dep
M5: Nonliteral ? POS + I Head + C Dep + I Dep
M6: Nonliteral ? POS + C Head + C Dep
M7: Nonliteral ? POS + I Head + I Dep
In Table (1), p-values have three categories, p < .0001, p < .001, or p < .01: this value represents a
test of the null hypothesis that the coefficient of the variable being considered is zero, i.e., the variable
has no effect on the model (a lower p-value is stronger evidence for rejecting the null hypothesis). Where
variables have significantly low p-values, Table (1) in effect presents optimal combinations of variables
for specific models, with low p-values indicating variables likely to have a greater effect on the model
and so more directly reflecting changes in the independent variable. For example, Table (1) shows that
models selecting MRC scores for heads (e.g. C Head) with the same kinds of scores for their dependents
(e.g.C Dep) seem most successful, which is perhaps to be expected, in light of studies 3 and 4.
It should be noted that no single variable models are reported here, since (1) while models such as
Nonliteral ? I Head and Nonliteral ? C Head indeed achieve significant p-values, others such
as Nonliteral ? I Dep and Nonliteral ? C Dep do not, (2) single variable models do not explain
Figure (1), nor indeed the variation for multiple variable contexts as exhibited by Figures (4) and (5).
We are currently comparing single vs. multiple variables, and early machine learning results suggest
multiple variable models are superior compared to single variable models as predictive tools.
171
Variables M1 M2 M3 M4 M5 M6 M7
Intercept -7.534*** -2.609* -9.088*** -7.836*** -7.522*** -7.816*** -7.614***
POS 9.265*** 8.884*** 9.330*** 9.163*** 9.316*** 9.082***
C Head 1.555 0.288 1.382 4.844*** 4.876***
I Head 0.459 -1.312 0.513 4.611*** 4.660***
C Dep -1.964 -1.982 -1.919 -3.799***
I Dep 0.682 0.699 0.660 -3.325**
Table 1: Results (t scores) of logistic regression model for predicting non/literal items
from the VUAMC, n=1855 (nb. p-values are shown by asterisks, ***=p<.0001, **=p<.001, *=p<.01)
4 Discussion
This paper reports results from ongoing work we are carrying out toward building a tool for identi-
fying metaphorical expressions in everyday discourse, through fine-grained analysis of the dimensions
of meaning of such expressions. We have presented evidence that detecting metaphor can usefully be
pursued as the problem of modeling how conceptual meanings such as concreteness and imageability,
interact with syntactically definable linguistic contexts. We increase the granularity of our analyses by
incorporating detailed syntactic information about the context in which metaphorical expressions occur.
By increasing the granularity of context, we were able to distinguish between metaphorical expressions
according to different parts of speech, and further, according to heads and their dependents.
We were able to show that for the purpose of determining whether a specific linguistic expression is
metaphorical or not, the most successful approach seems to be to combine information about parts of
speech with either concreteness scores for both heads and their dependents, or else with imageability
scores for both heads and their dependents. Note that this result is in part a direct consequence of the
high correlation between concreteness and imageability, whereby their combination will typically not
result in an optimal regression model. Such high correlation between concreteness and imageability has
been understood for some time (Paivio et al., 1968), yet, of course, there is good reason to think that
concreteness and imageability do not in fact pattern identically, and that they are at some level distinct
phenomena. Indeed, concreteness and imageability are likely related to distinct cognitive systems, and
we are currently undertaking further investigations in this direction.
Finally, we should note that while our results are likely to be language-specific, it is reasonable to
assume the general approach could be replicated across languages. We are currently planning such
cross-linguistic research for future work.
Acknowledgements
We acknowledge financial support through a Marie Curie International Incoming Fellowship (project
330569) awarded to two of the authors (Gargett as fellow, Barnden as P.I.). We would also like to
sincerely thank the reviewers for many very useful comments; of course, we assume full responsibility
for the final version.
References
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In The 23rd Interna-
tional Conference on Computational Linguistics (COLING 2010), Beijing, China.
Christina Cacciari and Sam Glucksberg. 1995. Imaging idiomatic expressions: literal or figurative meanings. In
Martin Everaert, Erik-Jan van der Linden, Andr Schenk, and Rober Schreuder, editors, Idioms: Structural and
psychological perspectives, pages 43?56. Lawrence Erlbaum.
Marie-Catherine De Marneffe and Christopher D Manning. 2008. The stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages
1?8. Association for Computational Linguistics.
172
Alice Deignan. 2006. The grammar of linguistic metaphors. In Anatol Stefanowitsch and Stefan Gries, editors,
Corpus-based approaches to metaphor and metonymy, pages 106?122. Walter de Gruyter.
Sara Dellantonio, Claudio Mulatti, Luigi Pastore, and Remo Job. 2014. Measuring inconsistencies can lead you
forward: The case of imageability and concreteness ratings. Frontiers in Psychology, 5(708).
Shi Feng, Zhiqiang Cai, Scott A Crossley, and Danielle S McNamara. 2011. Simulating human ratings on word
concreteness. In FLAIRS Conference.
Raymond W Gibbs. 2006. Metaphor interpretation as embodied simulation. Mind & Language, 21(3):434?458.
George Lakoff and Mark Johnson. 1980. Metaphors We Live By. University of Chicago.
Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last, Shlomo Argamon, Newton Howard, and Ophir Frieder. 2013.
Metaphor identification in large texts corpora. PloS one, 8(4):e62343.
Allan Paivio, John C Yuille, and Stephen A Madigan. 1968. Concreteness, imagery, and meaningfulness values
for 925 nouns. Journal of experimental psychology, 76(1, pt.2):1?25.
G.J. Steen, A.G. Dorst, J.B. Herrmann, A.A. Kaal, and T. Krennmayr. 2010. A Method for Linguistic Metaphor
Identification: From MIP to MIPVU. Converging Evidence in Language and Communication Research. John
Benjamins Publishing Company.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gershman. 2013. Cross-lingual metaphor detection using common
semantic features. In Proceedings of the First Workshop on Metaphor in NLP, pages 45?51, Atlanta, Georgia,
June. Association for Computational Linguistics.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings of the 2011 Conference on the Empirical Methods in
Natural Language Processing, pages 680?690.
Jose Manuel Urena and Pamela Faber. 2010. Reviewing imagery in resemblance and non-resemblance metaphors.
Cognitive Linguistics, 21(1):123?149.
Michael Wilson. 1988. MRC psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior
Research Methods, Instruments, & Computers, 20(1):6?10.
173
