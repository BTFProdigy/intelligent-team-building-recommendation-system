Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 17?24, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Semantic Relations to Refine Coreference Decisions 
 
 
Heng Ji David Westbrook Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu westbroo@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
We present a novel mechanism for im-
proving reference resolution by using the 
output of a relation tagger to rescore 
coreference hypotheses. Experiments 
show that this new framework can im-
prove performance on two quite different 
languages -- English and Chinese. 
1 Introduction 
Reference resolution has proven to be a major 
obstacle in building robust systems for information 
extraction, question answering, text summarization 
and a number of other natural language processing 
tasks.  
Most reference resolution systems use represen-
tations built out of the lexical and syntactic attrib-
utes of the noun phrases (or ?mentions?) for which 
reference is to be established. These attributes may 
involve string matching, agreement, syntactic dis-
tance, and positional information, and they tend to 
rely primarily on the immediate context of the 
noun phrases (with the possible exception of sen-
tence-spanning distance measures such as Hobbs 
distance). Though gains have been made with such 
methods (Tetreault 2001; Mitkov 2000; Soon et al 
2001; Ng and Cardie 2002), there are clearly cases 
where this sort of local information will not be suf-
ficient to resolve coreference correctly. 
Coreference is by definition a semantic 
relationship: two noun phrases corefer if they both 
refer to the same real-world entity. We should 
therefore expect a successful coreference system to 
exploit world knowledge, inference, and other 
forms of semantic information in order to resolve 
hard cases. If, for example, two nouns refer to 
people who work for two different organizations, 
we want our system to infer that these noun 
phrases cannot corefer. Further progress will likely 
be aided by flexible frameworks for representing 
and using the information provided by this kind of 
semantic relation between noun phrases.  
This paper tries to make a small step in that di-
rection. It describes a robust reference resolver that 
incorporates a broad range of semantic information 
in a general news domain. Using an ontology that 
describes relations between entities (the Auto-
mated Content Extraction program1 relation ontol-
ogy) along with a training corpus annotated for 
relations under this ontology, we first train a classi-
fier for identifying relations. We then apply the 
output of this relation tagger to the task of refer-
ence resolution.  
The rest of this paper is structured as follows. 
Section 2 briefly describes the efforts made by 
previous researchers to use semantic information in 
reference resolution.  Section 3 describes our own 
method for incorporating document-level semantic 
context into coreference decisions. We propose a 
representation of semantic context that isolates a 
particularly informative structure of interaction 
between semantic relations and coreference. Sec-
tion 4 explains in detail our strategies for using 
relation information to modify coreference deci-
sions, and the linguistic intuitions behind these 
strategies. Section 5 then presents the system archi-
tectures and algorithms we use to incorporate rela-
tional information into reference resolution. 
                                                          
1
 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/  and the ACE guidelines at 
http://www.ldc.upenn.edu/Projects/ACE/ 
17
Section 6 presents the results of experiments on 
both English and Chinese test data. Section 7 pre-
sents our conclusions and directions for future 
work.  
2 Prior Work 
Much of the earlier work in anaphora resolution 
(from the 1970?s and 1980?s, in particular) relied 
heavily on deep semantic analysis and inference 
procedures (Charniak 1972; Wilensky 1983; 
Carbonell and Brown 1988; Hobbs et al 1993).  
Using these methods, researchers were able to give 
accounts of some difficult examples, often by 
encoding quite elaborate world knowledge.  
Capturing sufficient knowledge to provide 
adequate coverage of even a limited but realistic 
domain was very difficult. Applying these 
reference resolution methods to a broad domain 
would require a large scale knowledge-engineering 
effort. 
The focus for the last decade has been primarily 
on broad coverage systems using relatively shallow 
knowledge, and in particular on corpus-trained sta-
tistical models.  Some of these systems attempt to 
apply shallow semantic information. (Ge et al 
1998) incorporate gender, number, and animaticity 
information into a statistical model for anaphora 
resolution by gathering coreference statistics on 
particular nominal-pronoun pairs. (Tetreault and 
Allen 2004) use a semantic parser to add semantic 
constraints to the syntactic and agreement con-
straints in their Left-Right Centering algorithm. 
(Soon et al 2001) use WordNet to test the seman-
tic compatibility of individual noun phrase pairs. In 
general these approaches do not explore the possi-
bility of exploiting the global semantic context 
provided by the document as a whole. 
Recently Bean and Riloff (2004) have sought to 
acquire automatically some semantic patterns that 
can be used as contextual information to improve 
reference resolution, using techniques adapted 
from information extraction.  Their experiments 
were conducted on collections of texts in two topic 
areas (terrorism and natural disasters). 
3 Relational Model of Semantic Context 
Our central goal is to model semantic and corefer-
ence structures in such a way that we can take ad-
vantage of a semantic context larger than the 
individual noun phrase when making coreference 
decisions. Ideally, this model should make it possi-
ble to pick out important features in the context 
and to distinguish useful signals from background 
noise. It should, for example, be able to represent 
such basic relational facts as whether the (possibly 
identical) people referenced by two noun phrases 
work in the same organization, whether they own 
the same car, etc.  And it should be able to use this 
information to resolve references even when sur-
face features such as lexical or grammatical attrib-
utes are imperfect or fail altogether.  
In this paper we present a Relational Corefer-
ence Model (abbreviated as RCM) that makes pro-
gress toward these goals.  To represent semantic 
relations, we use an ontology (the ACE 2004 rela-
tion ontology) that describes 7 main types of rela-
tions between entities and 23 subtypes (Table 1).2 
These relations prove to be more reliable guides 
for coreference than simple lexical context or even 
tests for the semantic compatibility of heads and 
modifiers. The process of tagging relations implic-
itly selects relevant items of context and abstracts 
raw lists of modifiers into a representation that is 
deeper, but still relatively lightweight.  
 
Relation Type Example 
Agent-Artifact 
(ART) 
Rubin Military Design, the 
makers of the Kursk 
Discourse (DISC) each of whom 
Employment/ 
Membership 
(EMP-ORG) 
Mr. Smith, a senior pro-
grammer at Microsoft 
Place-Affiliation 
(GPE-AFF) 
Salzburg Red Cross offi-
cials 
Person-Social  
(PER-SOC) 
relatives of the dead 
 
Physical 
(PHYS) 
a town some 50 miles south 
of Salzburg 
Other-Affiliation 
(Other-AFF) 
Republican senators 
 
Table 1. Examples of the ACE Relation Types 
 
Given these relations we can define a semantic 
context for a candidate mention coreference pair 
(Mention 1b and Mention 2b) using the structure 
                                                          
2
 See http://www.ldc.upenn.edu/Projects/ACE/docs/Eng-
lishRDCV4-3-2.PDF for a more complete description of ACE 
2004 relations. 
18
depicted in Figure 1. If both mentions participate 
in relations, we examine the types and directions of 
their respective relations as well as whether or not 
their relation partners (Mention 1a and Mention 
2a) corefer. These values (which correspond to the 
edge labels in Figure 1) can then be factored into a 
coreference prediction. This RCM structure 
assimilates relation information into a coherent 
model of semantic context. 
 
 
 
 
 
 
 
 
Figure 1. The RCM structure 
4 Incorporating Relations into Reference 
Resolution 
Given an instance of the RCM structure, we need 
to convert it into semantic knowledge that can be 
applied to a coreference decision. We approach 
this problem by constructing a set of RCM patterns 
and evaluating the accuracy of each pattern as 
positive or negative evidence for coreference. The 
resulting knowledge sources fall into two catego-
ries: rules that improve precision by pruning incor-
rect coreference links between mentions, and rules 
that improve recall by recovering missed links.  
To formalize these relation patterns, based on 
Figure 1, we define the following clauses: 
 
A: RelationType1 = RelationType2 
B: RelationSubType1 = RelationSubType2 
C: Two Relations have the same direction 
Same_Relation: CBA ??  
CorefA: Mention1a and Mention2a corefer 
CorefBMoreLikely: Mention1b and Mention2b are 
more likely to corefer 
CorefBLessLikely: Mention1b and Mention2b are 
less likely to corefer 
 
From these clauses we can construct the follow-
ing plausible inferences: 
 
Rule (1) 
LikelyCorefBLessCorefAlationSame ???Re_  
Rule (2) 
LikelyCorefBLessCorefAlationSame ??? Re_  
Rule (3) 
LikelyCorefBMoreCorefAlationSame ??Re_   
 
Rule (1) and (2) can be used to prune corefer-
ence links that simple string matching might incor-
rectly assert; and (3) can be used to recover missed 
mention pairs.  
The accuracy of Rules (1) and (3) varies depend-
ing on the type and direction of the particular rela-
tion shared by the two noun phrases. For example, 
if Mention1a and Mention 2a both refer to the 
same nation, and Mentions 1b and 2b participate in 
citizenship relations (GPE-AFF) with Mentions 1a 
and 2a respectively, we should not necessarily 
conclude that 1b and 2b refer to the same person.  
If 1a and 2a refer to the same person, however, and 
1b and 2b are nations in citizenship relations with 
1a and 2a, then it would indeed be the rare case in 
which 1b and 2b refer to two different nations. In 
other words, the relation of a nation to its citizens 
is one-to-many.  
Our system learns broad restrictions like these 
by evaluating the accuracy of Rules (1) and (3) 
when they are instantiated with each possible rela-
tion type and direction and used as weak classifi-
ers. For each such instantiation we use cross-
validation on our training data to calculate a reli-
ability weight defined as: 
 
| Correct decisions by rule for given instance | 
 
| Total applicable cases for given instance | 
 
  We count the number of correct decisions for a 
rule instance by taking the rule instance as the only 
source of information for coreference resolution 
and making only those decisions suggested by the 
rule?s implication (interpreting CorefBMoreLikely 
as an assertion that mention 1b and mention 2b do 
in fact corefer, and interpreting CorefBLessLikely 
as an assertion that they do not corefer). 
Every rule instance with a reliability weight of 
70% or greater is retained for inclusion in the final 
system. Rule (2) cannot be instantiated with a 
single type because it requires that the two relation 
types be different, and so we do not perform this 
filtering for Rule (2) (Rule (2) has 97% accuracy 
across all relation types). 
This procedure yields 58 reliable (reliability 
weight > 70%) type instantiations of Rule (1) and 
(3), in addition to the reliable Rule 2. We can 
Relation? 
Type2/Subtype2 
Mention1a 
Mention2a 
 
 
 
Candidate 
Mention1b 
Mention2b 
Relation? 
Type1/Subtype1
  Contexts: Corefer?  
19
recover an additional 24 reliable rules by 
conjoining additional boolean tests to less reliable 
rules. Tests include equality of mention heads, 
substring matching, absence of temporal key words 
such as ?current? and ?former,? number 
agreement, and high confidence for original 
coreference decisions (Mention1b and Mention2b). 
For each rule below the reliability threshold, we 
search for combinations of 3 or fewer of these 
restrictions until we achieve reliability of 70% or 
we have exhausted the search space.  
We give some examples of particular rule 
instances below. 
 
Example for Rule (1) 
 
Bush campaign officials ... decided to tone down a 
post-debate rally, and were even considering can-
celing it. 
? 
The Bush and Gore campaigns did not talk to each 
other directly about the possibility of postpone-
ment, but went through the debate commission's di-
rector, Janet Brown...Eventually, Brown 
recommended that the debate should go on, and 
neither side objected, according to campaign offi-
cials. 
 
Two mentions that do not corefer share the same 
nominal head (?officials?). We can prune the 
coreference link by noting that both occurrences of 
?officials? participate in an Employee-
Organization (EMP-ORG) relation, while the Or-
ganization arguments of these two relation in-
stances do not corefer (because the second 
occurrence refers to officials from both cam-
paigns). 
 
Example for Rule (2) 
 
Despite the increases, college remains affordable 
and a good investment, said College Board Presi-
dent Gaston Caperton in a statement with the sur-
veys. ? 
A majority of students need grants or loans -- or 
both -- but their exact numbers are unknown, a 
College Board spokesman said. 
 
  ?Gaston Caperton? stands in relation EMP-
ORG/Employ-Executive with ?College Board?, 
while "a College Board spokesman" is in relation 
EMP-ORG/Employ-Staff with the same organiza-
tion. We conclude that ?Gaston Caperton? does not 
corefer with "spokesman." 
 
Example for Rule (3) 
 
In his foreign policy debut for Syria, this Sunday 
Bashar Assad met Sunday with Egyptian President 
Hosni Mubarak in talks on Mideast peace and the 
escalating violence in the Palestinian territories. 
? 
The Syrian leader's visit came on a fourth day of 
clashes that have raged in the West Bank, Gaza 
Strip and Jerusalem??  
 
  If we have detected a coreference link between 
?Syria? and ?Syrian,? as well as EMP-ORG/ 
Employ-Executive relations between this country 
and two noun phrases ?Bashar Assad? and 
?leader?, it is likely that the two mentions both 
refer to the same person. Without this inference, a 
resolver might have difficulty detecting this 
coreference link. 
5 Algorithms 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  System Pipeline (Test Procedure) 
 
 
 
Coreference 
Rules 
Baseline Maxent 
Coref Classifiers 
Relation 
Tagger 
Final coreference decisions 
Entities 
Relation Features 
Rescoring Coreference Decisions 
 
Mentions 
20
In this section we will describe our algorithm for 
incorporating semantic relation information from 
the RCM into the reference resolver. In a nutshell, 
the system applies a baseline statistical resolver to 
generate multiple coreference hypotheses, applies a 
relation tagger to acquire relation information, and 
uses the relation information to rescore the 
coreference hypotheses. This general system archi-
tecture is shown in Figure 2.  
In section 5.1 below we present our baseline 
coreference system. In Section 5.2 we describe a 
system that combines the output of this baseline 
system with relation information to improve per-
formance. 
5.1 Baseline System 
Baseline reference resolver 
As the first stage in the resolution process we 
apply a baseline reference resolver that uses no 
relation information at all. This baseline resolver 
goes through two successive stages.  
First, high-precision heuristic rules make some 
positive and negative reference decisions. Rules 
include simple string matching (e.g., names that 
match exactly are resolved), agreement constraints 
(e.g., a nominal will never be resolved with an en-
tity that doesn't agree in number), and reliable syn-
tactic cues (e.g., mentions in apposition are 
resolved). When such a rule applies, it assigns a 
confidence value of 1 or 0 to a candidate mention-
antecedent pair. 
The remaining pairs are assigned confidence 
values by a collection of maximum entropy mod-
els. Since different mention types have different 
coreference problems, we separate the system into 
different models for names, nominals, and pro-
nouns. Each model uses a distinct feature set, and 
for each instance only one of these three models is 
used to produce a probability that the instance 
represents a correct resolution of the mention. 
When the baseline is used as a standalone system, 
we apply a threshold to this probability: if some 
resolution has a confidence above the  threshold, 
the highest confidence resolution will be made. 
Otherwise the mention is assumed to be the first 
mention of an entity. When the baseline is used as 
a component of the system depicted in figure 2, the 
confidence value is passed on to the rescoring 
stage described in 5.2 below. 
Both the English and the Chinese coreference 
models incorporate features representing agree-
ment of various kinds between noun phrases 
(number, gender, humanness), degree of string 
similarity, synonymy between noun phrase heads, 
measures of distance between noun phrases (such 
as the number of intervening sentences), the pres-
ence or absence of determiners or quantifiers, and 
a wide variety of other properties. 
Relation tagger 
The relation tagger uses a K-nearest-neighbor algo-
rithm. We consider a mention pair as a possible 
instance of a relation only when: (1) there is at 
most one other mention between their heads, and 
(2) the coreference probability produced for the 
pair by the baseline resolver is lower than a thresh-
old.  Each training / test example consists of the 
pair of mentions and the sequence of intervening 
words. We defined a distance metric between two 
examples based on: 
 whether the heads of the mentions match 
 whether the ACE types of the heads of the 
mentions match (for example, both are people 
or both are organizations) 
 whether the intervening words match 
To tag a test example, we find the k nearest 
training examples, use the distance to weight each 
neighbor, and then select the most heavily 
weighted class in the weighted neighbor set. 
Name tagger and noun phrase chunker  
Our baseline name tagger consists of a HMM 
tagger augmented with a set of post-processing 
rules.  The HMM tagger generally follows the 
Nymble model (Bikel et al 1997), but with a larger 
number of states (12 for Chinese, 30 for English) 
to handle name prefixes and suffixes, and, for 
Chinese, transliterated foreign names separately.  
For Chinese it operates on the output of a word 
segmenter from Tsinghua University. Our nominal 
mention tagger (noun phrase chunker) is a 
maximum entropy tagger trained on treebanks 
from the University of Pennsylvania. 
5.2 Rescoring stage 
To incorporate information from the relation tagger 
into the final coreference decision, we split the 
maxent classification into two stages. The first 
21
stage simply applies the baseline maxent models, 
without any relation information, and produces a 
probability of coreference. This probability 
becomes a feature in the second (rescoring) stage 
of maxent classification, together with features 
representing the relation knowledge sources. If a 
high reliability instantiation of one of the RCM 
rules (as defined in section 4 above) applies to a 
given mention-antecedent pair, we include the 
following features for that pair: the type of the 
RCM rule, the reliability of the rule instantiation, 
the relation type and subtype, the direction of the 
relation, and the tokens for the two mentions. 
The second stage helps to increase the margin 
between correct and incorrect links and so effects 
better disambiguation. See figure 3 below for a 
more detailed description of the training and test-
ing processes. 
 
 
 Training  
1. Calculate reliability weights of relation knowl-
edge sources using cross-validation (for each of k 
divisions of training data, train relation tagger on k 
? 1 divisions, tag relations in remaining division 
and compute reliability of each relation knowledge 
source using this division). 
2. Use high reliability relation knowledge sources 
to generate relation features for 2nd stage Maxent 
training data. 
3. Apply baseline coreference resolver to 2nd stage 
training data. 
4. Using output of both 2 and 3 as features, train 
2nd stage Maxent resolver. 
 
Test 
1. Tag relations. 
2. Convert relation knowledge sources into fea-
tures for second stage Maxent models. 
3. Use baseline Maxent models to get coreference 
probabilities for use as features in second stage 
Maxent models. 
4. Using output of 2 and 3 as features for 2nd stage 
Maxent model, apply 2nd stage resolver to make 
final coreference decisions. 
 
Figure 3.  Training and Testing Processes 
 
 
 
6 Evaluation Results 
6.1 Corpora 
We evaluated our system on two languages: 
English and Chinese. The following are the 
training corpora used for the components in these 
two languages. 
English 
For English, we trained the baseline maxent 
coreference model on 311 newswire and 
newspaper texts from the ACE 2002 and ACE 
2003 training corpora. We trained the relation 
tagger on 328 ACE 2004 texts. We used 126 
newswire texts from the ACE 2004 data to train the 
English second-stage model, and 65 newswire 
texts from the ACE 2004 evaluation set as a test set 
for the English system.  
Chinese 
For Chinese, the baseline reference resolver was 
trained on 767 texts from ACE 2003 and ACE 
2004 training data. Both the baseline relation 
tagger and the rescoring model were trained on 646 
texts from ACE 2004 training data. We used 100 
ACE texts for a final blind test. 
6.2 Experiments 
We used the MUC coreference scoring metric 
(Vilain et al1995) to evaluate3 our systems.  
To establish an upper limit for the possible 
improvement offered by our models, we first did 
experiments using perfect (hand-tagged) mentions 
and perfect relations as inputs. The algorithms for 
                                                          
3
 In our scoring, we use the ACE keys and only score mentions which appear in 
both the key and system response.  This therefore includes only mentions identi-
fied as being in the ACE semantic categories by both the key and the system 
response.  Thus these scores cannot be directly compared against coreference 
scores involving all noun phrases. (Ng 2005) applies another variation on the 
MUC metric to several systems tested on the ACE data by scoring all response 
mentions against all key mentions. For coreference systems that don?t restrict 
themselves to mentions in the ACE categories (or that don?t succeed in so re-
stricting themselves), this scoring method could lead to some odd effects. For 
example, systems that recover more correct links could be penalized for this 
greater recall because all links involving non-ACE mentions will be incorrect 
according to the ACE key. For the sake of comparison, however, we present 
here English system results measured according to this metric: On newswire 
data, our baseline had an F of 62.8 and the rescoring method had an F of 64.2. 
Ng?s best F score (on newspaper data) is 69.3. The best F score of  the (Ng and 
Cardie 2002)  system (also on newspaper data) is 62.1. On newswire data the 
(Ng 2005) system had an F score of 54.7 and the (Ng and Cardie 2002) system 
had an F score of 50.1. Note that Ng trained and tested these systems on differ-
ent ACE data sets than those we used for our experiments. 
22
these experiments are identical to those described 
above except for the omission of the relation tagger 
training. Tables 2 and 3 show the performance of 
the system for English and Chinese.  
 
Performance Recall Precision F-measure 
Baseline 74.5 86.6 80.1 
Rescoring 78.3 87.0 82.4 
 
Table 2. Performance of English system 
with perfect mentions and perfect relations 
 
 
Performance Recall Precision F-measure 
Baseline 87.5 83.2 85.3 
Rescoring 88.8 84.7 86.7 
 
Table 3. Performance of Chinese system 
with perfect mentions and perfect relations 
 
We can see that the relation information 
provided some improvements for both languages. 
Relation information increased both recall and 
precision in both cases. 
We then performed experiments to evaluate the 
impact of coreference rescoring when used with 
mentions and relations produced by the system. 
Table 4 and Table 5 list the results.4 
 
 
Performance Recall Precision F-measure 
Baseline 77.2 87.3 81.9 
Rescoring 80.3 87.5 83.7 
 
Table 4. Performance of English system 
with system mentions and system relations 
 
 
Performance Recall Precision F-measure 
Baseline 75.0 76.3 75.6 
Rescoring 76.1 76.5 76.3 
 
Table 5. Chinese system performance with 
system mentions and system relations 
 
                                                          
4
 Note that, while English shows slightly less relative gain from rescoring when 
using system relations and mentions, all of these scores are higher than the 
perfect mention/perfect relation scores. This increase may be a byproduct of the 
fact that the system mention tagger output contains almost 8% fewer scoreable 
mentions than the perfect mention set (see footnote 3). With a difference of this 
magnitude, the particular mention set selected can be expected to have a sizable 
impact on the final scores. 
The improvement provided by rescoring in trials 
using mentions and relations detected by the 
system is considerably less than the improvement 
in trials using perfect mentions and relations, 
particularly for Chinese. The performance of our 
relation tagger is the most likely cause for this 
difference. We would expect further gain after 
improving the relation tagger. 
A sign test applied to a 5-way split of each of the 
test corpora indicated that for both languages, for 
both perfect and system mentions/relations, the 
system that exploited relation information signifi-
cantly outperformed the baseline (at the 95% con-
fidence level, judged by F measure). 
6.3 Error Analysis 
Errors made by the RCM rules reveal both the 
drawbacks of using a lightweight semantic 
representation and the inherent difficulty of 
semantic analysis. Consider the following instance: 
 
Card's interest in politics began when he became 
president of the class of 1965 at Holbrook High 
School?In 1993, he became president and chief 
executive of the American Automobile Manufac-
turers Association, where he oversaw the lobbying 
against tighter fuel-economy and air pollution regu-
lations for automobiles? 
 
The two occurrences of ?president? should core-
fer even though they have EMP-ORG/Employ-
Executive relations with two different organiza-
tions. The relation rule (Rule 1) fails here because 
it doesn't take into account the fact that relations 
change over time (in this case, the same person 
filling different positions at different times). In 
these and other cases, a little knowledge is a dan-
gerous thing: a more complete schema might be 
able to deal more thoroughly with temporal and 
other essential semantic dimensions. 
Nevertheless, performance improvements indi-
cate that the rewards of the RCM?s simple seman-
tic representation outweigh the risks. 
7 Conclusion and Future Work 
We have outlined an approach to improving refer-
ence resolution through the use of semantic rela-
tions, and have described a system which can 
exploit these semantic relations effectively. Our 
experiments on English and Chinese data showed 
23
that these small inroads into semantic territory do 
indeed offer performance improvements. Further-
more, the method is low-cost and not domain-
specific. 
  These experiments also suggest that some gains 
can be made through the exploration of new archi-
tectures for information extraction applications. 
The ?resolve coreference, tag relations, resolve 
coreference? procedure described above could be 
seen as one and a half iterations of a ?resolve 
coreference then tag relations? loop. Seen in this 
way, the system poses the question of whether fur-
ther gains could be made by pushing the iterative 
approach further. Perhaps by substituting an itera-
tive procedure for the pipeline architecture?s linear 
sequence of stages we can begin to address the 
knotty, mutually determining nature of the interac-
tion between semantic relations and coreference 
relations. This approach could be applied more 
broadly, to different NLP tasks, and also more 
deeply, going beyond the simple one-and-a-half-
iteration procedure we present here. Ultimately, we 
would want this framework to boost the perform-
ance of each component automatically and signifi-
cantly. 
We also intend to extend our method both to 
cross-document relation detection and to event de-
tection. 
Acknowledgements  
This research was supported by the Defense Ad-
vanced Research Projects Agency under Grant 
N66001-04-1-8920 from SPAWAR San Diego, 
and by the National Science Foundation under 
Grant 03-25657. This paper does not necessarily 
reflect the position or the policy of the U.S. Gov-
ernment. 
References  
David Bean, Ellen Riloff. 2004. Unsupervised learning 
of contextual role knowledge for coreference resolu-
tion. Proc. HLT-NAACL 2004, pp. 297-304. 
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. Proc. Fifth Conf. 
on Applied Natural Language Processing, Washing-
ton, D.C., pp. 194-201. 
Carbonell, Jaime and Ralf Brown. 1988. Anaphora reso-
lution: A multi-strategy approach. Proc. COLING 
1988, pp.96-101 
Eugene Charniak. 1972. Toward a model of children's 
story comprehension. Ph.D. thesis, Massachusetts Insti-
tute of Technology, Cambridge, MA. 
Niyu Ge, John Hale and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. Proc. the 
Sixth Workshop on Very Large Corpora. 
Jerry Hobbs, Mark Stickel, Douglas Appelt and Paul 
Martin. 1993.  Interpretation as abduction.  Artificial 
Intelligence, 63, pp. 69-142. 
Ruslan Mitkov. 2000. Towards a more consistent and 
comprehensive evaluation of anaphora resolution al-
gorithms and systems. Proc. 2nd Discourse Anaph-
ora and Anaphora Resolution Colloquium, pp. 96-
107 
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution. 
Proc. ACL 2002, pp.104-111 
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, Volume 27, Number 4, pp. 521-
544 
Joel R. Tetreault. 2001. A corpus-based evaluation of 
centering and pronoun resolution. Computational 
Linguistics, Volume 27, Number 4, pp. 507-520 
Joel  R. Tetreault and James Allen. 2004. Semantics, 
Dialogue, and Pronoun Resolution. Proc. CATALOG 
'04 Barcelona, Spain. 
Marc Vilain, John Burger, John Aberdeen, Dennis Con- 
     nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. Proc. the 6th 
Message Understanding Conference (MUC-6). San 
Mateo, Cal. Morgan Kaufmann. 
Robert Wilensky.  1983.  Planning and Understanding.  
Addison-Wesley. 
24
Proceedings of the 43rd Annual Meeting of the ACL, pages 411?418,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Improving Name Tagging by  
Reference Resolution and Relation Detection 
 
 
Heng Ji Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Information extraction systems incorpo-
rate multiple stages of linguistic analysis.  
Although errors are typically compounded 
from stage to stage, it is possible to re-
duce the errors in one stage by harnessing 
the results of the other stages.  We dem-
onstrate this by using the results of 
coreference analysis and relation extrac-
tion to reduce the errors produced by a 
Chinese name tagger.  We use an N-best 
approach to generate multiple hypotheses 
and have them re-ranked by subsequent 
stages of processing.  We obtained 
thereby a reduction of 24% in spurious 
and incorrect name tags, and a reduction 
of 14% in missed tags. 
1 Introduction 
Systems which extract relations or events from a 
document typically perform a number of types of 
linguistic analysis in preparation for information 
extraction.  These include name identification and 
classification, parsing (or partial parsing), semantic 
classification of noun phrases, and coreference 
analysis.  These tasks are reflected in the evalua-
tion tasks introduced for MUC-6 (named entity, 
coreference, template element) and MUC-7 (tem-
plate relation). 
In most extraction systems, these stages of 
analysis are arranged sequentially, with each stage 
using the results of prior stages and generating a 
single analysis that gets enriched by each stage.  
This provides a simple modular organization for 
the extraction system.  
Unfortunately, each stage also introduces a cer-
tain level of error into the analysis.  Furthermore, 
these errors are compounded ? for example, errors 
in name recognition may lead to errors in parsing.  
The net result is that the final output (relations or 
events) may be quite inaccurate. 
This paper considers how interactions between 
the stages can be exploited to reduce the error rate. 
For example, the results of coreference analysis or 
relation identification may be helpful in name clas-
sification, and the results of relation or event ex-
traction may be helpful in coreference. 
Such interactions are not easily exploited in a 
simple sequential model ? if name classification 
is performed at the beginning of the pipeline, it 
cannot make use of the results of subsequent stages. 
It may even be difficult to use this information im-
plicitly, by using features which are also used in 
later stages, because the representation used in the 
initial stages is too limited. 
To address these limitations, some recent sys-
tems have used more parallel designs, in which a 
single classifier (incorporating a wide range of fea-
tures) encompasses what were previously several 
separate stages (Kambhatla, 2004; Zelenko et al, 
2004).  This can reduce the compounding of errors 
of the sequential design.  However, it leads to a 
very large feature space and makes it difficult to 
select linguistically appropriate features for par-
ticular analysis tasks.  Furthermore, because these 
decisions are being made in parallel, it becomes 
much harder to express interactions between the 
levels of analysis based on linguistic intuitions. 
411
In order to capture these interactions more ex-
plicitly, we have employed a sequential design in 
which multiple hypotheses are forwarded from 
each stage to the next, with hypotheses being rer-
anked and pruned using the information from later 
stages. We shall apply this design to show how 
named entity classification can be improved by 
?feedback? from coreference analysis and relation 
extraction.  We shall show that this approach can 
capture these interactions in a natural and efficient 
manner, yielding a substantial improvement in 
name identification and classification. 
2 Prior Work 
A wide variety of trainable models have been ap-
plied to the name tagging task, including HMMs 
(Bikel et al, 1997), maximum entropy models 
(Borthwick, 1999), support vector machines 
(SVMs), and conditional random fields.  People 
have spent considerable effort in engineering ap-
propriate features to improve performance; most of 
these involve internal name structure or the imme-
diate local context of the name. 
Some other named entity systems have explored 
global information for name tagging. (Borthwick,  
1999) made a second tagging pass which uses in-
formation on token sequences tagged in the first 
pass; (Chieu and Ng, 2002) used as features infor-
mation about features assigned to other instances 
of the same token. 
Recently, in (Ji and Grishman, 2004) we pro-
posed a name tagging method which applied an 
SVM based on coreference information to filter the 
names with low confidence, and used coreference 
rules to correct and recover some names. One limi-
tation of this method is that in the process of dis-
carding many incorrect names, it also discarded 
some correct names. We attempted to recover 
some of these names by heuristic rules which are 
quite language specific. In addition, this single-
hypothesis method placed an upper bound on recall. 
Traditional statistical name tagging methods 
have generated a single name hypothesis. BBN 
proposed the N-Best algorithm for speech recogni-
tion in (Chow and Schwartz, 1989). Since then N-
Best methods have been widely used by other re-
searchers (Collins, 2002; Zhai et al, 2004). 
In this paper, we tried to combine the advan-
tages of the prior work, and incorporate broader 
knowledge into a more general re-ranking model. 
3 Task and Terminology 
Our experiments were conducted in the context of 
the ACE Information Extraction evaluations, and 
we will use the terminology of these evaluations: 
entity:  an object or a set of objects in one of the 
semantic categories of interest 
mention:  a reference to an entity (typically, a noun 
phrase) 
name mention:  a reference by name to an entity 
nominal mention:  a reference by a common noun 
or noun phrase to an entity 
relation:  one of a specified set of relationships be-
tween a pair of entities 
The 2004 ACE evaluation had 7 types of entities, 
of which the most common were PER (persons), 
ORG (organizations), and GPE (?geo-political enti-
ties? ? locations which are also political units, such 
as countries, counties, and cities).  There were 7 
types of relations, with 23 subtypes.  Examples of 
these relations are ?the CEO of Microsoft? (an em-
ploy-exec relation), ?Fred?s wife? (a family rela-
tion), and ?a military base in Germany? (a located 
relation). 
In this paper we look at the problem of identify-
ing name mentions in Chinese text and classifying 
them as persons, organizations, or GPEs.  Because 
Chinese has neither capitalization nor overt word 
boundaries, it poses particular problems for name 
identification. 
4 Baseline System 
4.1 Baseline Name Tagger 
Our baseline name tagger consists of a HMM tag-
ger augmented with a set of post-processing rules.  
The HMM tagger generally follows the Nymble 
model (Bikel et al 1997), but with multiple hy-
potheses as output and a larger number of states 
(12) to handle name prefixes and suffixes, and 
transliterated foreign names separately.  It operates 
on the output of a word segmenter from Tsinghua 
University.   
Within each of the name class states, a statistical 
bigram model is employed, with the usual one-
word-per-state emission. The various probabilities 
involve word co-occurrence, word features, and 
class probabilities. Then it uses A* search decod-
ing to generate multiple hypotheses. Since these 
probabilities are estimated based on observations 
412
seen in a corpus, ?back-off models? are used to 
reflect the strength of support for a given statistic, 
as for the Nymble system. 
We also add post-processing rules to correct 
some omissions and systematic errors using name 
lists (for example, a list of all Chinese last names; 
lists of organization and location suffixes) and par-
ticular contextual patterns (for example, verbs oc-
curring with people?s names).  They also deal with 
abbreviations and nested organization names. 
The HMM tagger also computes the margin ? 
the difference between the log probabilities of the 
top two hypotheses.  This is used as a rough meas-
ure of confidence in the top hypothesis (see sec-
tions 5.3 and 6.2, below). 
The name tagger used for these experiments 
identifies the three main ACE entity types: Person 
(PER), Organization (ORG), and GPE (names of 
the other ACE types are identified by a separate 
component of our system, not involved in the ex-
periments reported here). 
4.2 Nominal Mention Tagger 
Our nominal mention tagger (noun group recog-
nizer) is a maximum entropy tagger trained on the 
Chinese TreeBank from the University of Pennsyl-
vania, supplemented by list matching. 
4.3  Reference Resolver  
Our baseline reference resolver goes through two 
successive stages: first, coreference rules will iden-
tify some high-confidence positive and negative 
mention pairs, in training data and test data; then 
the remaining samples will be used as input of a 
maximum entropy tagger. The features used in this 
tagger involve distance, string matching, lexical 
information, position, semantics, etc. We separate 
the task into different classifiers for different men-
tion types (name / noun / pronoun). Then we in-
corporate the results from the relation tagger to 
adjust the probabilities from the classifiers. Finally 
we apply a clustering algorithm to combine them 
into entities (sets of coreferring mentions). 
4.4 Relation Tagger 
The relation tagger uses a k-nearest-neighbor algo-
rithm. For both training and test, we consider all 
pairs of entity mentions where there is at most one 
other mention between the heads of the two men-
tions of interest1.  Each training / test example con-
sists of the pair of mentions and the sequence of 
intervening words. Associated with each training 
example is either one of the ACE relation types or 
no relation at all. We defined a distance metric be-
tween two examples based on 
? whether the heads of the mentions match 
? whether the ACE types of the heads of the mentions 
match (for example, both are people or both are or-
ganizations) 
? whether the intervening words match 
To tag a test example, we find the k nearest 
training examples (where k = 3) and use the dis-
tance to weight each neighbor, then select the most 
common class in the weighted neighbor set. 
To provide a crude measure of the confidence of 
our relation tagger, we define two thresholds, Dnear 
and Dfar.  If the average distance d to the nearest 
neighbors d < Dnear, we consider this a definite re-
lation.  If Dnear < d < Dfar, we consider this a possi-
ble relation.  If d > Dfar, the tagger assumes that no 
relation exists (regardless of the class of the nearest 
neighbor). 
5 Information from Coreference and Re-
lations 
Our system is processing a document consisting of 
multiple sentences.  For each sentence, the name 
recognizer generates multiple hypotheses, each of 
which is an NE tagging of the entire sentence. The 
names in the hypothesis, plus the nouns in the 
categories of interest constitute the mention set for 
that hypothesis. Coreference resolution links these 
mentions, assigning each to an entity.  In symbols: 
 Si  is the i-th sentence in the document. 
Hi  is the hypotheses set for Si  
 hij  is the j-th hypothesis in Si  
Mij  is the mention set for hij  
mijk  is the k-th mention in Mij  
eijk  is the entity which mijk belongs to according to 
the current reference resolution results 
5.1 Coreference Features 
For each mention we compute seven quantities 
based on the results of name tagging and reference 
resolution: 
                                                          
1 This constraint is relaxed for parallel structures such as ?mention1, mention2, 
[and] mention3?.?; in such cases there can be more than one intervening men-
tion. 
413
CorefNumijk  is the number of mentions in eijk  
WeightSumijk  is the sum of all the link weights be-
tween mijk and other mentions in eijk , 0.8 for 
name-name coreference; 0.5 for apposition;  
0.3 for other name-nominal coreference 
FirstMentionijk  is 1 if mijk is the first name mention 
in the entity; otherwise 0 
Headijk  is 1 if mijk includes the head word of name; 
otherwise 0 
Withoutidiomijk  is 1 if mijk is not part of an idiom; 
otherwise 0 
PERContextijk  is the number of PER context words 
around a PER name such as a title or an ac-
tion verb involving a PER 
ORGSuffixijk  is 1 if ORG mijk includes a suffix word; 
otherwise 0 
The first three capture evidence of the correct-
ness of a name provided by reference resolution; 
for example, a name which is coreferenced with 
more other mentions is more likely to be correct.  
The last four capture local or name-internal evi-
dence; for instance, that an organization name in-
cludes an explicit, organization-indicating suffix. 
We then compute, for each of these seven quan-
tities, the sum over all mentions k in a sentence, 
obtaining values for CorefNumij, WeightSumij, etc.: 
CorefNum CorefNumij ijk
k
= ?   etc. 
Finally, we determine, for a given sentence and 
hypothesis, for each of these seven quantities, 
whether this quantity achieves the maximum of its 
values for this hypothesis: 
BestCorefNumij ?  
 CorefNumij = maxq CorefNumiq   etc. 
We will use these properties of the hypothesis as 
features in assessing the quality of a hypothesis.  
5.2 Relation Word Clusters 
In addition to using relation information for 
reranking name hypotheses, we used the relation 
training corpus to build word clusters which could 
more directly improve name tagging.  Name tag-
gers rely heavily on words in the immediate con-
text to identify and classify names; for example, 
specific job titles, occupations, or family relations 
can be used to identify people names.  Such words 
are learned individually from the name tagger?s 
training corpus.  If we can provide the name tagger 
with clusters of related words, the tagger will be 
able to generalize from the examples in the training 
corpus to other words in the cluster. 
The set of ACE relations includes several in-
volving employment, social, and family relations.  
We gathered the words appearing as an argument 
of one of these relations in the training corpus, 
eliminated low-frequency terms and manually ed-
ited the ten resulting clusters to remove inappro-
priate terms.  These were then combined with lists 
(of titles, organization name suffixes, location suf-
fixes) used in the baseline tagger. 
5.3 Relation Features 
Because the performance of our relation tagger 
is not as good as our coreference resolver, we have 
used the results of relation detection in a relatively 
simple way to enhance name detection.  The basic 
intuition is that a name which has been correctly 
identified is more likely to participate in a relation 
than one which has been erroneously identified. 
For a given range of margins (from the HMM), 
the probability that a name in the first hypothesis is 
correct is shown in the following table, for names 
participating and not participating in a relation: 
 
Margin In Relation(%) Not in Relation(%)
<4 90.7 55.3 
<3 89.0 50.1 
<2 86.9 42.2 
<1.5 81.3 28.9 
<1.2 78.8 23.1 
<1 75.7 19.0 
<0.5 66.5 14.3 
Table 1 Probability of a name being correct 
 
Table 1 confirms that names participating in re-
lations are much more likely to be correct than 
names that do not participate in relations.  We also 
see, not surprisingly, that these probabilities are 
strongly affected by the HMM hypothesis margin 
(the difference in log probabilities) between the 
first hypothesis and the second hypothesis.  So it is 
natural to use participation in a relation (coupled 
with a margin value) as a valuable feature for re-
ranking name hypotheses. 
Let mijk be the k-th name mention for hypothe-
sis hij of sentence; then we define: 
414
Inrelationijk  = 1 if mijk  is in a definite relation 
   = 0 if mijk is in a possible relation 
   = -1 if mijk is not in a relation  
 Inrelation Inrelationij ijk
k
= ?  
Mostrelated Inrelation Inrelationij ij q iq? =( max )
  Finally, to capture the interaction with the margin, 
we let zi  = the margin for sentence Si and divide 
the range of values of zi into six intervals Mar1, ? 
Mar6.  And we define the hypothesis ranking in-
formation: FirstHypothesisij = 1 if j =1; otherwise 0. 
We will use as features for ranking hij the con-
junction of Mostrelatedij, zi ? Marp (p = 1, ?, 6), 
and FirstHypothesisij . 
6 Using the Information from Corefer-
ence and Relations 
6.1 Word Clustering based on Relations 
As we described in section 5.2, we can generate 
word clusters based on relation information. If a 
word is not part of a relation cluster, we consider it 
an independent (1-word) cluster.  
The Nymble name tagger (Bikel et al, 1999) re-
lies on a multi-level linear interpolation model for 
backoff. We extended this model by adding a level 
from word to cluster, so as to estimate more reli-
able probabilities for words in these clusters. Table 
2 shows the extended backoff model for each of 
the three probabilities used by Nymble.  
 
Transition  
Probability 
First-Word 
Emission  
Probability 
Non-First-Word
Emission  
Probability 
P(NC2|NC1, 
 <w1, f1>) 
P(<w2,f2>| 
NC1, NC2) 
P(<w2,f2>| 
<w1,f1>, NC2) 
 P(<Cluster2,f2>| 
NC1, NC2) 
P(<Cluster2,f2>|
<w1,f1>, NC2) 
P(NC2|NC1,  
<Cluster1, 
f1>) 
P(<Cluster2,f2>| 
<+begin+, other>, 
NC2) 
P(<Cluster2,f2>|
<Cluster1,f1>, 
NC2) 
P(NC2|NC1) P(<Cluster2, f2>|NC2) 
P(NC2)  P(Cluster2|NC2) * P(f2|NC2) 
1/#(name 
classes) 
1/#(cluster)  *  1/#(word features) 
Table2 Extended Backoff Model 
 
6.2 Pre-pruning by Margin 
The HMM tagger produces the N best hypotheses 
for each sentence.2  In order to decide when we 
need to rely on global (coreference and relation) 
information for name tagging, we want to have 
some assessment of the confidence that the name 
tagger has in the first hypothesis.  In this paper, we 
use the margin for this purpose. A large margin 
indicates greater confidence that the first hypothe-
sis is correct.3  So if the margin of a sentence is 
above a threshold, we select the first hypothesis, 
dropping the others and by-passing the reranking. 
6.3 Re-ranking based on Coreference 
We described in section 5.1, above, the coreference 
features which will be used for reranking the hy-
potheses after pre-pruning. A maximum entropy 
model for re-ranking these hypotheses is then 
trained and applied as follows: 
 
Training 
1. Use K-fold cross-validation to generate multi-
ple name tagging hypotheses for each docu-
ment in the training data Dtrain (in each of the K 
iterations, we use K-1 subsets to train the 
HMM and then generate hypotheses from the 
Kth subset). 
2. For each document d in Dtrain, where d includes 
n sentences S1?Sn 
For i = 1?n, let m = the number of hy-
potheses for Si 
(1) Pre-prune the candidate hypotheses us-
ing the HMM margin 
(2) For each hypothesis hij, j = 1?m 
(a) Compare hij with the key, set the 
prediction Valueij ?Best? or ?Not 
Best? 
(b) Run the Coreference Resolver on 
hij and the best hypothesis for each 
of the other sentences, generate 
entity results for each candidate 
name in hij 
(c) Generate a coreference feature vec-
tor Vij for hij 
(d) Output Vij and Valueij 
                                                          
2 We set different N = 5, 10, 20 or 30 for different margin ranges, by cross-
validation checking the training data about the ranking position of the best 
hypothesis for each sentence.  With this N, optimal reranking (selecting the best 
hypothesis among the N best) would yield Precision = 96.9 Recall = 94.5 F = 
95.7 on our test corpus. 
3 Similar methods based on HMM margins were used by (Scheffer et al, 2001). 
415
3. Train Maxent Re-ranking system on all Vij and 
Valueij 
 
Test 
1. Run the baseline name tagger to generate mul-
tiple name tagging hypotheses for each docu-
ment in the test data Dtest 
2. For each document d in Dtest, where d includes 
n sentences S1?Sn 
(1) Initialize: Dynamic input of coreference re-
solver H = {hi-best | i = 1?n, hi-best is the 
current best hypothesis for Si} 
(2) For i = 1?n, assume m = the number of 
hypotheses  for Si 
(a) Pre-prune the candidate hypotheses us-
ing the HMM margin 
(b) For each hypothesis hij, j = 1?m 
? hi-best = hij  
? Run the Coreference Resolver on H, 
generate entity results for each name 
candidate in hij 
? Generate a coreference feature vec-
tor Vij for hij 
? Run Maxent Re-ranking system on 
Vij, produce Probij of ?Best? value 
(c) hi-best = the hypothesis with highest 
Probij of ?Best? value, update H and 
output hi-best 
6.4 Re-ranking based on Relations 
From the above first-stage re-ranking by corefer-
ence, for each hypothesis we got the probability of 
its being the best one. By using these results and 
relation information we proceed to a second-stage 
re-ranking. As we described in section 5.3, the in-
formation of ?in relation or not? can be used to-
gether with margin as another important measure 
of confidence. 
  In addition, we apply the mechanism of weighted 
voting among hypotheses (Zhai et al, 2004) as an 
additional feature in this second-stage re-ranking. 
This approach allows all hypotheses to vote on a 
possible name output. A recognized name is con-
sidered correct only when it occurs in more than 30 
percent of the hypotheses (weighted by their prob-
ability).  
In our experiments we use the probability pro-
duced by the HMM, probij , for hypothesis hij . We 
normalize this probability weight as: 
W
prob
probij
ij
iq
q
= ?
exp( )
exp( )
 
For each name mention mijk in hij , we define:  
Occur mq ijk( )  = 1 if mijk occurs in hq  
   = 0 otherwise 
Then we count its voting value as follows: 
Votingijk  is 1 if W Occur miq q ijk
q
?? ( ) >0.3;  
  otherwise 0. 
The voting value of hij is:  
Voting Votingij ijk
k
= ?  
Finally we define the following voting feature: 
BestVoting Voting Votingij ij q iq? =( max )  
This feature is used, together with the features 
described at the end of section 5.3 and the prob-
ability score from the first stage, for the second-
stage maxent re-ranking model. 
One appeal of the above two re-ranking algo-
rithms is its flexibility in incorporating features 
into a learning model: essentially any coreference 
or relation features which might be useful in dis-
criminating good from bad structures can be in-
cluded.  
7 System Pipeline 
Combining all the methods presented above, the 
flow of our final system is shown in figure 1.  
8 Evaluation Results 
8.1 Training and Test Data 
We took 346 documents from the 2004 ACE train-
ing corpus and official test set, including both 
broadcast news and newswire, as our blind test set. 
To train our name tagger, we used the Beijing Uni-
versity Insititute of Computational Linguistics cor-
pus ? 2978 documents from the People?s Daily in 
1998 ? and 667 texts in the training corpus for the 
2003 & 2004 ACE evaluation. Our reference re-
solver is trained on these 667 ACE texts. The rela-
tion tagger is trained on 546 ACE 2004 texts, from 
which we also extracted the relation clusters. The 
test set included 11715 names: 3551 persons, 5100 
GPEs and 3064 organizations. 
 
416
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1  System Flow 
8.2 Overall Performance Comparison 
Table 3 shows the performance of the baseline sys-
tem; Table 4 is the system with relation word clus-
ters; Table 5 is the system with both relation 
clusters and re-ranking based on coreference fea-
tures; and Table 6 is the whole system with sec-
ond-stage re-ranking using relations. 
The results indicate that relation word clusters 
help to improve the precision and recall of most 
name types. Although the overall gain in F-score is 
small (0.7%), we believe further gain can be 
achieved if the relation corpus is enlarged in the 
future. The re-ranking using the coreference fea-
tures had the largest impact, improving precision 
and recall consistently for all types. Compared to 
our system in (Ji and Grishman, 2004), it helps to 
distinguish the good and bad hypotheses without 
any loss of recall. The second-stage re-ranking us-
ing the relation participation feature yielded a 
small further gain in F score for each type, improv-
ing precision at a slight cost in recall. 
The overall system achieves a 24.1% relative re-
duction on the spurious and incorrect tags, and 
14.3% reduction in the missing rate over a state-of-
the-art baseline HMM trained on the same material. 
Furthermore, it helps to disambiguate many name 
type errors: the number of cases of type confusion 
in name classification was reduced from 191 to 
102. 
 
Name Precision Recall F 
PER 88.6 89.2 88.9 
GPE 88.1 84.9 86.5 
ORG 88.8 87.3 88.0 
ALL 88.4 86.7 87.5 
Table 3 Baseline Name Tagger 
 
Name Precision Recall F 
PER 89.4 90.1 89.7 
GPE 88.9 85.8 89.4 
ORG 88.7 87.4 88.0 
ALL 89.0 87.4 88.2 
Table 4 Baseline + Word Clustering by Relation 
 
Name Precision Recall F 
PER 90.1 91.2 90.5 
GPE 89.7 86.8 88.2 
ORG 90.6 89.8 90.2 
ALL 90.0 88.8 89.4 
Table 5 Baseline + Word Clustering by Relation + 
Re-ranking by Coreference 
 
Name Precision Recall F 
PER 90.7 91.0 90.8 
GPE 91.2 86.9 89.0 
ORG 91.7 89.1 90.4 
ALL 91.2 88.6 89.9 
Table 6 Baseline + Word Clustering by Relation +   
Re-ranking by Coreference +  
Re-ranking by Relation 
 
In order to check how robust these methods are, 
we conducted significance testing (sign test) on the 
346 documents. We split them into 5 folders, 70 
documents in each of the first four folders and 66 
in the fifth folder. We found that each enhance-
ment (word clusters, coreference reranking, rela-
tion reranking) produced an improvement in F 
score for each folder, allowing us to reject the hy-
pothesis that these improvements were random at a 
95% confidence level. The overall F-measure im-
provements (using all enhancements) for the 5 
folders were: 2.3%, 1.6%, 2.1%, 3.5%, and 2.1%. 
 
HMM Name Tagger, word 
clustering based on rela-
tions, pruned by margin 
Multiple name 
hypotheses 
Maxent Re-ranking
by coreference 
Single name
 hypothesis 
Post-processing  
by heuristic rules
Input 
Nominal 
Mention 
Tagger 
Nominal 
Mentions
Relation 
Tagger 
Maxent Re-ranking
by relation 
Coreference 
Resolver 
417
9 Conclusion 
This paper explored methods for exploiting the 
interaction of analysis components in an informa-
tion extraction system to reduce the error rate of 
individual components.  The ACE task hierarchy 
provided a good opportunity to explore these inter-
actions, including the one presented here between 
reference resolution/relation detection and name 
tagging. We demonstrated its effectiveness for 
Chinese name tagging, obtaining an absolute im-
provement of 2.4% in F-measure (a reduction of 
19% in the (1 ? F) error rate). These methods are 
quite low-cost because we don?t need any extra 
resources or components compared to the baseline 
information extraction system. 
Because no language-specific rules are involved 
and no additional training resources are required, 
we expect that the approach described here can be 
straightforwardly applied to other languages.  It 
should also be possible to extend this re-ranking 
framework to other levels of analysis in informa-
tion extraction ?- for example, to use event detec-
tion to improve name tagging; to incorporate 
subtype tagging results to improve name tagging; 
and to combine name tagging, reference resolution 
and relation detection to improve nominal mention 
tagging.  For Chinese (and other languages without 
overt word segmentation) it could also be extended 
to do character-based name tagging, keeping mul-
tiple segmentations among the N-Best hypotheses.  
Also, as information extraction is extended to cap-
ture cross-document information, we should expect 
further improvements in performance of the earlier 
stages of analysis, including in particular name 
identification. 
For some levels of analysis, such as name tag-
ging, it will be natural to apply lattice techniques to 
organize the multiple hypotheses, at some gain in 
efficiency. 
Acknowledgements 
This research was supported by the Defense Ad-
vanced Research Projects Agency under Grant 
N66001-04-1-8920 from SPAWAR San Diego, 
and by the National Science Foundation under 
Grant 03-25657. This paper does not necessarily 
reflect the position or the policy of the U.S. Gov-
ernment. 
References 
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder.  Proc. Fifth 
Conf. on Applied Natural Language Processing, 
Washington, D.C. 
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition.  Ph.D. Disser-
tation, Dept. of Computer Science, New York 
University. 
Hai Leong Chieu and Hwee Tou Ng. 2002.  Named En-
tity Recognition: A Maximum Entropy Approach Us-
ing Global Information.  Proc.: 17th Int?l Conf. on 
Computational Linguistics (COLING 2002), Taipei, 
Taiwan. 
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best 
Algorithm: An efficient Procedure for Finding Top N 
Sentence Hypotheses. Proc. DARPA Speech and 
Natural Language Workshop 
Michael Collins. 2002. Ranking Algorithms for Named-
Entity Extraction: Boosting and the Voted Percep-
tron. Proc. ACL 2002 
Heng Ji and Ralph Grishman. 2004. Applying Corefer-
ence to Improve Name Recognition. Proc. ACL 2004 
Workshop on Reference Resolution and Its Applica-
tions, Barcelona, Spain 
N. Kambhatla. 2004. Combining Lexical, Syntactic, and 
Semantic Features with Maximum Entropy Models 
for Extracting Relations. Proc. ACL 2004. 
Tobias Scheffer, Christian Decomain, and Stefan 
Wrobel. 2001. Active Hidden Markov Models for In-
formation Extraction. Proc. Int?l Symposium on In-
telligent Data Analysis (IDA-2001). 
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets. 
2004.  Binary Integer Programming for Information 
Extraction.  ACE Evaluation Meeting, September 
2004, Alexandria, VA. 
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine 
Carpuat, and Dekai Wu. 2004. Using N-best Lists for 
Named Entity Recognition from Chinese Speech. 
Proc. NAACL 2004 (Short Papers) 
418
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 420?427,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Analysis and Repair of Name Tagger Errors 
 
 
Heng Ji Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu grishman@cs.nyu.edu 
 
  
Abstract 
Name tagging is a critical early stage in 
many natural language processing pipe-
lines. In this paper we analyze the types 
of errors produced by a tagger, distin-
guishing name classification and various 
types of name identification errors.  We 
present a joint inference model to im-
prove Chinese name tagging by incorpo-
rating feedback from subsequent stages in 
an information extraction pipeline: name 
structure parsing, cross-document 
coreference, semantic relation extraction 
and event extraction. We show through 
examples and performance measurement 
how different stages can correct different 
types of errors.  The resulting accuracy 
approaches that of individual human an-
notators.   
1 Introduction 
High-performance named entity (NE) tagging is 
crucial in many natural language processing tasks, 
such as information extraction and machine 
translation. In 'traditional' pipelined system archi-
tectures, NE tagging is one of the first steps in 
the pipeline. NE errors adversely affect subse-
quent stages, and error rates are often com-
pounded by later stages. 
However, (Roth and Yi 2002, 2004) and our 
recent work have focused on incorporating richer 
linguistic analysis, using the ?feedback? from 
later stages to improve name taggers. We ex-
panded our last year?s model (Ji and Grishman, 
2005) that used the results of coreference analy-
sis and relation extraction, by adding ?feedback? 
from more information extraction components ? 
name structure parsing, cross-document corefer-
ence, and event extraction ? to incrementally re-
rank the multiple hypotheses from a baseline 
name tagger.  
While together these components produced a 
further improvement on last year?s model, our 
goal in this paper is to look behind the overall 
performance figures in order to understand how 
these varied components contribute to the im-
provement, and compare the remaining system 
errors with the human annotator?s performance. 
To this end, we shall decompose the task of name 
tagging into two subtasks 
? Name Identification ? The process of iden-
tifying name boundaries in the sentence. 
? Name Classification ? Given the correct 
name boundaries, assigning the appropri-
ate name types to them. 
and observe the effects that different components 
have on errors of each type.  Errors of identifica-
tion will be further subdivided by type (missing 
names, spurious names, and boundary errors).  
We believe such detailed understanding of the 
benefits of joint inference is a prerequisite for 
further improvements in name tagging perform-
ance. 
After summarizing some prior work in this 
area, describing our baseline NE tagger, and ana-
lyzing its errors, we shall illustrate, through a 
series of examples, the potential for feedback to 
improve NE performance. We then present some 
details on how this improvement can be achieved 
through hypothesis reranking in the extraction 
pipeline, and analyze the results in terms of dif-
ferent types of identification and classification 
errors. 
2 Prior Work 
Some recent work has incorporated global infor-
mation to improve the performance of name tag- 
gers.  
For mixed case English data, name identifica-
tion is relatively easy. Thus some researchers 
have focused on the more challenging task ? 
classifying names into correct types. In (Roth and 
420
Yi 2002, 2004), given name boundaries in the 
text, separate classifiers are first trained for name 
classification and semantic relation detection. 
Then, the output of the classifiers is used as a 
conditional distribution given the observed data. 
This information, along with the constraints 
among the relations and entities (specific rela-
tions require specific classes of names), is used to 
make global inferences by linear programming 
for the most probable assignment. They obtained 
significant improvements in both name classifi-
cation and relation detection. 
In (Ji and Grishman 2005) we generated N-
best NE hypotheses and re-ranked them after 
coreference and semantic relation identification; 
we obtained a significant improvement in Chi-
nese name tagging performance. In this paper we 
shall use a wider range of linguistic knowledge 
sources, and integrate cross-document techniques. 
3 Baseline Name Tagger 
We apply a multi-lingual (English / Chinese) 
bigram HMM tagger to identify four named 
entity types: Person, Organization, GPE (?geo-
political entities? ? locations which are also 
political units, such as countries, counties, and 
cities) and Location. The HMM tagger generally 
follows the Nymble model (Bikel et al 1997), 
and uses best-first search to generate N-Best 
hypotheses for each input sentence. 
In mixed-case English texts, most proper 
names are capitalized. So capitalization provides 
a crucial clue for name boundaries.  
In contrast, a Chinese sentence is composed of 
a string of characters without any word bounda-
ries or capitalization. Even after word segmenta-
tion there are still no obvious clues for the name 
boundaries. However, we can apply the following 
coarse ?usable-character? restrictions to reduce 
the search space. 
Standard Chinese family names are generally 
single characters drawn from a set of 437 family 
names (there are also 9 two-character family 
names, although they are quite infrequent) and 
given names can be one or two characters (Gao et 
al., 2005). Transliterated Chinese person names 
usually consist of characters in three relatively 
fixed character lists (Begin character list, Middle 
character list and End character list). Person ab-
breviation names and names including title words 
match a few patterns. The suffix words (if there 
are any) of Organization and GPE names belong 
to relatively fixed lists too. 
However, this ?usable-character? restriction is 
not as reliable as the capitalization information 
for English, since each of these special characters 
can also be part of common words. 
3.1 Identification and Classification Errors 
We begin our error analysis with an investigation 
of the English and Chinese baseline taggers, de-
composing the errors into identification and clas-
sification errors. In Figure 1 we report the 
identification F-Measure for the baseline (the 
first hypothesis), and the N-best upper bound, the 
best of the N hypotheses1, using different models: 
English MonoCase (EN-Mono, without capitali-
zation), English Mixed Case (EN-Mix, with capi-
talization), Chinese without the usable character 
restriction (CH-NoRes) and Chinese with the 
usable character restriction (CH-WithRes). 
 
Figure 1. Baseline and Upper Bound of 
Name Identification 
 
Figure 1 shows that capitalization is a crucial 
clue in English name identification (increasing 
the F measure by 7.6% over the monocase score). 
We can also see that the best of the top N (N <= 
30) hypotheses is very good, so reranking a small 
number of hypotheses has the potential of pro-
ducing a very good tagger. 
The ?usable? character restriction plays a ma-
jor role in Chinese name identification, increas-
ing the F-measure 4%.  With this restriction, the 
performance of the best-of-N-best is again very 
good. However, it is evident that, even with this 
restriction, identification is more challenging for 
Chinese, due to the absence of capitalization and 
word boundaries. 
Figure 2 shows the classification accuracy of 
the above four models. We can see that capitali-
zation does not help English name classification; 
                                                          
1 These figures were obtained using training and test corpora 
described later in this paper, and a value of N ranging from 
1 to 30 depending on the margin of the HMM tagger, as also 
described below.  All figures are with respect to the official 
ACE keys prepared by the Linguistic Data Consortium. 
421
and the difficulty of classification is similar for 
the two languages. 
 
Figure 2. Baseline and Upper Bound of 
Name Classification 
3.2 Identification Errors in Chinese 
For the remainder of this paper we shall focus on 
the more difficult problems of Chinese tagging, 
using the HMM system with character restric-
tions as our baseline.  The name identification 
errors of this system can be divided into missed 
names (21%), spurious names (29%), and bound-
ary errors, where there is a partial overlap be-
tween the names in the key and the system 
response (50%).  Confusion between names and 
nominals (phrases headed by a common noun) is 
a major source of both missed and spurious 
names (56% of missed, 24% of spurious).  In a 
language without capitalization, this is a hard 
task even for people; one must rely largely on 
world knowledge to decide whether a phrase 
(such as the "criminal-processing team") is an 
organization name or merely a description of an 
organization.  The other major source of missed 
names is words not seen in the training data, gen-
erally representing minor cities or other locations 
in China (28%).  For spurious names, the largest 
source of error is names of a type not included in 
the key (44%) which are mistakenly tagged as 
one of the known name types.2  As we shall see, 
different types of knowledge are required for cor-
recting different types of errors. 
4 Mutual Inferences between Informa-
tion Extraction Stages  
4.1 Extraction Pipeline 
Name tagging is typically one of the first stages 
                                                          
2 If the key included an 'other' class of names, these would 
be classification errors; since it does not -- since these names 
are not tagged in the key -- the automatic scorer treats them 
as spurious names. 
in an information extraction pipeline. Specifically, 
we will consider a system which was developed 
for the ACE (Automatic Content Extraction) 
task 3  and includes the following stages: name 
structure parsing, coreference, semantic relation 
extraction and event extraction (Ji et al, 2006). 
All these stages are performed after name tag-
ging since they take names as input ?objects?. 
However, the inferences from these subsequent 
stages can also provide valuable constraints to 
identify and classify names.  
Each of these stages connects the name candi-
date to other linguistic elements in the sentence, 
document, or corpus, as shown in Figure 3.   
 
                                                       Sentence    Document 
                                                             Boundary  Boundary 
 
 
 
 
 
 
Name        Local    Related   Event              Coreferring  
Candidate Context Mention  trigger&arg     Mentions 
 
                  Linguistic Elements Supporting Inference 
 
Figure 3. Name candidate and its global context 
 
The baseline name tagger (HMM) uses very 
local information; feedback from later extraction 
stages allows us to draw from a wider context in 
making final name tagging decisions. 
In the following we use two related (translated) 
texts as examples, to give some intuition of how 
these different types of linguistic evidence im-
prove name tagging.4 
 
Document 1: Yugoslav election 
 
[?] More than 300,000 people rushed the <bei 
er ge le>0 congress building, forcing <yugo-
slav>1 president <mi lo se vi c>2 to admit 
frankly that in the Sept. 24 election he was 
beaten by his opponent <ke shi tu ni cha>3. 
    <mi lo se vi c>4 was forced to flee <bei er ge 
le>5; the winning opposition party's <sai er wei 
ya>6 <anti-democracy committee>7 on the 
morning of the 6th formed a <crisis-handling 
                                                          
3 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/  and the ACE 
guidelines at http://www.ldc.upenn.edu/Projects/ACE/ 
4 Rather than offer the most fluent translation, we have pro-
vided one that more closely corresponds to the Chinese text 
in order to more clearly illustrate the linguistic issues.  
Transliterated names are rendered phonetically, character by 
character. 
supporting  inference 
information 
422
committee>8, to deal with transfer-of-power is-
sues. 
        This crisis committee includes police, supply,  
economics and other important departments. 
In such a crisis, people cannot think through 
this question: has the <yugoslav>9 president <mi 
lo se vi c>10 used up his skills? 
        According to the official voting results in the 
first round of elections, <mi lo se vi c>11 was 
beaten by <18 party opposition committee>12 
candidate <ke shi tu ni cha>13. [?] 
 
Document 2: Biography of these two leaders 
 
[?]<ke shi tu ni cha>14 used to pursue an aca-
demic career, until 1974, when due to his opposi-
tion position he was fired by <bei er ge le>15 
<law school>16 and left the academic community. 
    <ke shi tu ni cha>17 also at the beginning of the 
1990s joined the opposition activity, and in 1992 
founded <sai er wei ya>18 <opposition party>19. 
This famous new leader and his previous 
classmate at law school, namely his wife <zuo li 
ka>20 live in an apartment in <bei er ge le>21. 
The vanished <mi lo se vi c>22 was born in 
<sai er wei ya>23 ?s central industrial city. [?] 
 
4.1 Inferences for Correcting Name Errors 
4.2.1 Internal Name Structure 
Constraints and preferences on the structure of 
individual names can capture local information 
missed by the baseline name tagger. They can 
correct several types of identification errors, in-
cluding in particular boundary errors.  For exam-
ple, ?<ke shi tu ni cha>3? is more likely to be 
correct than ?<shi tu ni cha>3? since ?shi? (?) 
cannot be the first character of a transliterated 
name. 
Name structures help to classify names too. 
For example, ?anti-democracy committee7? is 
parsed as ?[Org-Modifier anti-democracy] [Org-
Suffix committee]?, and the first character is not 
a person last name or the first character of a 
transliterated person name, so it is more likely to 
be an organization than a person name.  
4.2.2 Patterns 
Information about expected sequences of con-
stituents surrounding a name can be used to cor-
rect name boundary errors.  In particular, event 
extraction is performed by matching patterns in-
volving a "trigger word" (typically, the main verb 
or nominalization representing the event) and a 
set of arguments.  When a name candidate is in-
volved in an event, the trigger word and other 
arguments of the event can help to determine the 
name boundaries.  For example, in the sentence 
?The vanished mi lo se vi c was born in sai er wei 
ya ?s central industrial city?, ?mi lo se vi c? is 
more likely to be a name than ?mi lo se?, ?sai er 
wei ya? is more likely be a name than ?er wei?, 
because these boundaries will allow us to match 
the event pattern ?[Adj] [PER-NAME] [Trigger 
word for 'born' event] in [GPE-NAME]?s [GPE-
Nominal]?. 
4.2.3 Selection 
Any context which can provide selectional con-
straints or preferences for a name can be used to 
correct name classification errors.  Both semantic 
relations and events carry selectional constraints 
and so can be used in this way. 
For instance, if the ?Personal-Social/Business? 
relation (?opponent?) between ?his? and ?<ke shi 
tu ni cha>3? is correctly identified, it can help to 
classify ?<ke shi tu ni cha>3? as a person name. 
Relation information is sometimes crucial to 
classifying names. ?<mi lo se vi c>10? and ?<ke 
shi tu ni cha>13? are likely person names because 
they are ?employees? of ?<yugoslav>9? and 
?<18 party opponent committee>12?. Also the 
?Personal-Social/Family? relation (?wife?) be-
tween ?his? and ?<zuo li ka>20? helps to classify 
<zuo li ka>20 as a person name.   
Events, like relations, can provide effective se-
lectional preferences to correctly classify names. 
For example, ?<mi lo se vi c>2,4,10,11,22? are likely 
person names because they are involved in the 
following events: ?claim?, ?escape?, ?built?, 
?beat?, ?born?, while ?<sai er wei ya>23?can be 
easily tagged as GPE because it?s a ?birth-place? 
in the event ?born?.  
4.2.4 Coreference 
Names which are introduced in an article are 
likely to be referred to again, either by repeating 
the same name or describing it with nominal 
mentions (phrases headed by common nouns).  
These mentions will have the same spelling 
(though if a name has several parts, some may be 
dropped) and same semantic type.  So if the 
boundary or type of one mention can be deter-
mined with some confidence, coreference can be 
used to disambiguate other mentions.  
For example, if ?< mi lo se vi c>2? is con-
firmed as a name, then ?< mi lo se vi c>10? is 
more likely to be a name than ?< mi lo se>10?, by 
423
refering to ?< mi lo se vi c>2?. Also ?This crisis 
committee? supports the analysis of ?<crisis-
handling committee>8? as an organization name 
in preference to the alternative name candidate 
?<crisis-handling>8?. 
For a name candidate, high-confidence infor-
mation about the type of one mention can be used 
to determine the type of other mentions. For ex-
ample, for the repeated person name ?< mi lo se 
vi c>2,4,10,11,22? type information based on the 
event context of one mention can be used to clas-
sify or confirm the type of the others. The person 
nominal ?This famous new leader? confirms 
?<ke shi tu ni cha>17? as a person name.  
5 Incremental Re-Ranking Algorithm 
5.1 Overall Architecture 
In this section we will present the algorithms to 
capture the intuitions described in Section 4. The 
overall system pipeline is presented in Figure 4.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4.  System Architecture 
 
 
 
The baseline name tagger generates N-Best 
multiple hypotheses for each sentence, and also 
computes the margin ? the difference between 
the log probabilities of the top two hypotheses.  
This is used as a rough measure of confidence in 
the top hypothesis. A large margin indicates 
greater confidence that the first hypothesis is cor-
rect.5 It generates name structure parsing results 
too, such as the family name and given name of 
person, the prefixes of the abbreviation names, 
the modifiers and suffixes of organization names. 
Then the results from subsequent components 
are exploited in four incremental re-rankers. 
From each re-ranking step we output the best 
name hypothesis directly if the re-ranker has high 
confidence in its decisions. Otherwise the sen-
tence is forwarded to the next re-ranker, based on 
other features. In this way we can adjust the rank-
ing of multiple hypotheses and select the best 
tagging for each sentence gradually. 
The nominal mention tagger (noun phrase 
chunker) uses a maximum entropy model. Entity 
type assignment for the nominal heads is done by 
table look-up. The coreference resolver is a com-
bination of high-precision heuristic rules and 
maximum entropy models. In order to incorpo-
rate wider context we use cross-document 
coreference for the test set. We cluster the docu-
ments using a cross-entropy metric and then treat 
the entire cluster as a single document. 
The relation tagger uses a K-nearest-neighbor 
algorithm. 
   We extract event patterns from the ACE05 
training corpus for personnel, contact, life, busi-
ness, and conflict events. We also collect addi-
tional event trigger words that appear frequently 
in name contexts, from a syntactic dictionary, a 
synonym dictionary and Chinese PropBank V1.0. 
Then the patterns are generalized and tested 
semi-automatically. 
5.2 Supervised Re-Ranking Model 
In our name re-ranking model, each hypothesis is 
an NE tagging of the entire sentence, for example, 
?The vanished <PER>mi lo se vi c</PER> was 
born in <GPE>sai er wei ya</GPE>?s central 
industrial city?; and each pair of hypotheses (hi, 
hj) is called a ?sample?.  
 
                                                          
5 The margin also determines the number of hypotheses (N) 
generated by the baseline tagger.  Using cross-validation on 
the training data, we determine the value of N required to 
include the best hypothesis, as a function of the margin.  We 
then divide the margin into ranges of values, and set a value 
of N for each range, with a maximum of 30. 
High-
Confidence 
Ranking 
Best Name 
Hypothesis 
Event based 
Re-Ranking 
Cross-document 
Coreference based 
Re-Ranking 
Coref  
Resolver
Event 
Patterns
Raw Sentence 
HMM Name 
Tagger and Name 
Structure Parser 
Multiple name 
hypotheses 
Name Structure 
based Re-Ranking 
Relation
Tagger
Mentions
Relation based 
Re-Ranking 
Nominal 
Tagger
424
Re-Ranker Property for comparing names Nik and Njk 
HMMMargin scaled margin value from HMM 
Idiomik -1 if Nik is part of an idiom; otherwise 0 
PERContextik the number of PER context words if Nik and Njk  are both PER; otherwise 0 
ORGSuffixik 1 if Nik is tagged as ORG and it includes a suffix word; otherwise 0 
PERCharac-
terik 
-1 if Nik is tagged as PER without family name, and it does not consist entirely of 
transliterated person name characters; otherwise 0 
Titlestructureik -1 if Nik = title word + family name while Njk = title word + family name + given 
name; otherwise 0 
Digitik -1 if Nik is  PER or GPE and it includes digits or punctuation; otherwise 0 
AbbPERik -1 if Nik = little/old + family name + given name while Njk = little/old + family 
name; otherwise 0 
SegmentPERik -1 if Nik is GPE (PER)* GPE , while Njk is PER*; otherwise 0 
Votingik the voting rate among all the candidate hypotheses6 
 
 
 
 
Name  
Structure 
Based 
Famous-
Nameik 
1 if Nik is tagged as the same type in one of the famous name lists7; otherwise 0 
Probability1i scaled ranking probability for (hi, hj) from name structure based re-ranker 
Relation 
Constraintik 
If Nik is in relation R (Nik = EntityType1, M2 = EntityType2), compute 
Prob(EntityType1|EntityType2, R) from training data and scale it; otherwise 0 
 
Relation 
Based 
 Conjunction of 
InRelation i & 
Probability1i 
Inrelationik is 1 if Nik and Njk  have different name types, and Nik is in a definite re-
lation while Njk  is not; otherwise 0. ?
k
iki InrelationInrelation?  
Probability2i scaled ranking probability for (hi, hj) from relation based re-ranker 
Event 
Constrainti 
1 if all entity types in hi match event pattern, -1 if some do not match, and 0 if the 
argument slots are empty 
Event 
Based 
EventSubType Event subtype if the patterns are extracted from ACE data, otherwise?None? 
Probability3i scaled ranking probability for (hi, hj) from event based re-ranker 
Headik 1 if ikN includes the head word of name; otherwise 0 
CorefNumik the number of mentions corefered to Nik  
WeightNumik the sum of all link weights between Nik and its corefered mentions, 0.8 for name-
name coreference; 0.5 for apposition;  0.3 for other name-nominal coreference 
Cross- 
document 
Corefer-
ence 
Based 
NumHigh-
Corefi 
the number of mentions which corefer to Nik and output by previous re-rankers with 
high confidence 
 
Table 3. Re-Ranking Properties 
 
 
Component Data 
Baseline name tagger 2978 texts from the People?s Daily in 1998 and 1300 texts from 
ACE03, 04, 05 training data 
Nominal tagger Chinese Penn TreeBank V5.1 
Coreference resolver 1300 texts from ACE03, 04, 05 training data 
Relation tagger 633 ACE 05 texts, and 546 ACE 04 texts with types/subtypes 
mapped into 05 set 
Event pattern 376 trigger words, 661 patterns 
Name structure, coreference 
and relation based re-rankers 
1,071,285 samples (pairs of hypotheses) from ACE 03, 04 and 
05 training data 
 
 
 
 
 
Training 
Event based re-ranker 325,126 samples from ACE sentences including event trigger 
words 
Test 100 texts from ACE 04 training corpus, includes 2813 names: 
1126 persons, 712 GPEs, 785 organizations and 190 locations. 
 
Table 4. Data Description 
                                                          
6 The method of counting the voting rate refers to (Zhai, 04) and (Ji and Grishman, 05) 
7 Extracted from the high-frequency name lists from the training corpus, and country/province/state/ city lists from Chinese 
wikipedia. 
  
425
The goal of each re-ranker is to learn a ranking 
function f of the following form: for each pair of 
hypotheses (hi, hj), f : H ? H ? {-1, 1}, such that 
f(hi, hj) = 1 if hi is better than hj; f (hi, hj) = -1 if hi 
is worse than hj. In this way we are able to con-
vert ranking into a classification problem. And 
then a maximum entropy model for re-ranking 
these hypotheses can be trained and applied.  
During training we use F-measure to measure 
the quality of each name hypothesis against the 
key. During test we get from the MaxEnt classi-
fier the probability (ranking confidence) for each 
pair: Prob (f (hi, hj) = 1). Then we apply a dy-
namic decoding algorithm to output the best hy-
pothesis. More details about the re-ranking 
algorithm are presented in (Ji et al, 2006). 
5.3 Re-Ranking Features 
For each sample (hi, hj), we construct a feature 
set for assessing the ranking of hi and hj. Based 
on the information obtained from inferences, we 
compute (for each property) the property score 
PSik for each individual name candidate Nik in hi; 
some of these properties depend also on the cor-
responding name tags in hj.  Then we sum over 
all names in each hypothesis hi: ?=
k
iki PSPS  
Finally we use the quantity (PSi?PSj) as the 
feature value for the sample (hi, hj).  Table 3 
summarizes the property scores PSik used in the 
different re-rankers; space limitations prevent us 
from describing them in further detail. 
6 Experimental Results and Analysis 
Table 4 shows the data used to train each stage, 
drawn from the ACE training data and other 
sources. The training samples of the re-rankers 
are obtained by running the name tagger in cross-
validation. 100 ACE 04 documents were held out 
for use as test data. 
In the following we evaluate the contributions 
of re-rankers in name identification and classifi-
cation separately.   
 
Identification Model 
Precision Recall F-Measure
Baseline 93.2 93.4 93.3 
+name structure 94.0 93.5 93.7 
+relation 93.9 93.7 93.8 
+event 94.1 93.8 93.9 
+cross-doc  
coreference 
95.1 93.9 94.5 
 
Table 5. Name Identification 
Identification 
+Classification 
 
Model 
Classifi-
cation 
Accuracy P R F 
Baseline 93.8 87.4 87.6 87.5
+name structure 94.3 88.7 88.2 88.4
+relation 95.2 89.4 89.2 89.3
+event 95.7 90.1 89.8 89.9
+cross-doc 
coreference 
96.5 91.8 90.6 91.2
 
Table 6. Name Classification 
 
Tables 5 and 6 show the performance on iden-
tification, classification, and the combined task as 
we add each re-ranker to the system.  
The gain is greater for classification (2.7%) 
than for identification (1.2%).  Furthermore, we 
can see that the gain in identification is produced 
primarily by the name structure and coreference 
components. As we noted earlier, the name struc-
ture analysis can correct boundary errors by pre-
ferring names with complete internal components, 
while coreference can resolve a boundary ambi-
guity for one mention of a name if another men-
tion is unambiguous. The greatest gains were 
therefore obtained in boundary errors: the stages 
together eliminated over 1/3 of boundary errors 
and about 10% of spurious names; only a few 
missing names were corrected, and some correct 
names were deleted. 
Both relations and events contribute substan-
tially to classification performance through their 
selectional constraints.  The lesser contribution of 
events is related to their lower frequency.  Only 
11% of the sentences in the test data contain in-
stances of the original ACE event types.  To in-
crease the impact of the event patterns, we 
broadened their coverage to include additional 
frequent event types, so that finally 35% of sen-
tences contain event "trigger words".  
We used a simple cross-document coreference 
method in which the test documents were clus-
tered based on their cross-entropy and documents 
in the same cluster were treated as a single 
document for coreference. This produced small 
gains in both identification (0.6% vs. 0.4%) and 
classification (0.8% vs. 0.4%) over single- 
document coreference. 
7 Discussion 
The use of 'feedback' from subsequent stages of 
analysis has yielded substantial improvements in 
name tagging accuracy, from F=87.5 with the 
baseline HMM to F=91.2. This performance 
compares quite favorably with the performance 
of the human annotators who prepared the ACE 
426
2005 training data.  The annotator scores (when 
measured against a final key produced by review 
and adjudication of the two annotations) were 
F=92.5 for one annotator and F=92.7 for the 
other. 
As in the case of the automatic tagger, human 
classification accuracy (97.2 - 97.6%) was better 
than identification accuracy (F = 95.0 - 95.2%).   
In Figure 5 we summarize the error rates for 
the baseline system, the improved system without 
coreference based re-ranker, the final system 
with re-ranking, and a single annotator.8 
 
 
 
Figure 5.  Error Distribution 
 
Figure 5 shows that the performance im-
provement reflects a reduction in classification 
and boundary errors. Compared to the system, 
the human annotator?s identification accuracy 
was much more skewed (52.3% missing, 13.5% 
spurious), suggesting that a major source of iden-
tification error was not difference in judgement 
but rather names which were simply overlooked 
by one annotator and picked up by the other.  
This further suggests that through an extension of 
our joint inference approach we may soon be able 
to exceed the performance of a single manual 
annotator. 
Our analysis of the types of errors, and the per-
formance of our knowledge sources, gives some 
indication of how these further gains may be 
achieved.  The selectional force of event extrac-
tion was limited by the frequency of event pat-
terns ? only about 1/3 of sentences had a pattern 
                                                          
8  Here spurious errors are names in the system response 
which do not overlap names in the key; missing errors are 
names in the key which do not overlap names in the system 
response; and boundary errors are names in the system re-
sponse which partially overlap names in the key plus names 
in the key which partially overlap names in the system re-
sponse. 
instance.  Even with this limitation, we obtained 
a gain of 0.5% in name classification.  Capturing 
a broader range of selectional patterns should 
yield further improvements.  Nearly 70% of the 
spurious names remaining in the final output 
were in fact instances of 'other' types of names, 
such as book titles and building names; creating 
explicit models of such names should improve 
performance. Finally, our cross-document 
coreference is currently performed only within 
the (small) test corpus.  Retrieving related articles 
from a large collection should increase the likeli-
hood of finding a name instance with a disam-
biguating context. 
Acknowledgment 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023, and the 
National Science Foundation under Grant IIS-
00325657.  Any opinions, findings and conclu-
sions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of the U. S. Government. 
References  
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder. Proc. 
ANLP1997. pp. 194-201., Washington, D.C.  
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang. 2005. Chinese Word Segmentation and 
Named Entity Recognition: A Pragmatic Approach. 
Computational Linguistics 31(4). pp. 531-574 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Heng Ji, Cynthia Rudin and Ralph Grishman. 2006. 
Re-Ranking Algorithms for Name Tagging. Proc. 
HLT/NAACL 06 Workshop on Computationally 
Hard Problems and Joint Inference in Speech and 
Language Processing. New York, NY, USA 
Dan Roth and Wen-tau Yih. 2004. A Linear Pro-
gramming Formulation for Global Inference in 
Natural Language Tasks. Proc. CONLL2004. 
Dan Roth and Wen-tau Yih. 2002. Probabilistic Rea-
soning for Entity & Relation Recognition. Proc. 
COLING2002. 
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine 
Carpuat, and Dekai Wu. 2004. Using N-best Lists 
for Named Entity Recognition from Chinese 
Speech. Proc. NAACL 2004 (Short Papers) 
427
Proceedings of ACL-08: HLT, pages 254?262,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Refining Event Extraction through Cross-document Inference 
 
 
Heng Ji Ralph Grishman 
Computer Science Department 
New York University 
New York, NY 10003, USA 
(hengji, grishman)@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
We apply the hypothesis of ?One Sense Per 
Discourse? (Yarowsky, 1995) to information 
extraction (IE), and extend the scope of ?dis-
course? from one single document to a cluster 
of topically-related documents. We employ a 
similar approach to propagate consistent event 
arguments across sentences and documents. 
Combining global evidence from related doc-
uments with local decisions, we design a sim-
ple scheme to conduct cross-document 
inference for improving the ACE event ex-
traction task1. Without using any additional 
labeled data this new approach obtained 7.6% 
higher F-Measure in trigger labeling and 6% 
higher F-Measure in argument labeling over a 
state-of-the-art IE system which extracts 
events independently for each sentence. 
1 Introduction 
Identifying events of a particular type within indi-
vidual documents ? ?classical? information extrac-
tion ? remains a difficult task. Recognizing the 
different forms in which an event may be ex-
pressed, distinguishing events of different types, 
and finding the arguments of an event are all chal-
lenging tasks. 
Fortunately, many of these events will be re-
ported multiple times, in different forms, both 
within the same document and within topically- 
related documents (i.e. a collection of documents 
sharing participants in potential events). We can 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
take advantage of these alternate descriptions to 
improve event extraction in the original document, 
by favoring consistency of interpretation across 
sentences and documents. Several recent studies 
involving specific event types have stressed the 
benefits of going beyond traditional single-
document extraction; in particular, Yangarber 
(2006) has emphasized this potential in his work 
on medical information extraction. In this paper we 
demonstrate that appreciable improvements are 
possible over the variety of event types in the ACE 
(Automatic Content Extraction) evaluation through 
the use of cross-sentence and cross-document evi-
dence. 
As we shall describe below, we can make use of 
consistency at several levels: consistency of word 
sense across different instances of the same word 
in related documents, and consistency of argu-
ments and roles across different mentions of the 
same or related events. Such methods allow us to 
build dynamic background knowledge as required 
to interpret a document and can compensate for the 
limited annotated training data which can be pro-
vided for each event type. 
2 Task and Baseline System 
2.1 ACE Event Extraction Task 
The event extraction task we are addressing is that 
of the Automatic Content Extraction (ACE) evalu-
ations2. ACE defines the following terminology: 
                                                          
2 In this paper we don?t consider event mention coreference 
resolution and so don?t distinguish event mentions and events. 
254
entity: an object or a set of objects in one of the 
semantic categories of interest 
mention: a reference to an entity (typically, a 
noun phrase) 
event trigger: the main word which most clearly 
expresses an event occurrence 
event arguments: the mentions that are in-
volved in an event (participants) 
event mention: a phrase or sentence within 
which an event is described, including trigger 
and arguments 
 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 distinct event types. 
For example, for a sentence: 
 
Barry Diller on Wednesday quit as chief of Vivendi 
Universal Entertainment. 
 
the event extractor should detect a ?Person-
nel_End-Position? event mention, with the trigger 
word, the position, the person who quit the posi-
tion, the organization, and the time during which 
the event happened: 
 
Trigger Quit 
 
 
Arguments 
Role = Person Barry Diller 
Role =  
Organization 
Vivendi Universal 
Entertainment 
Role = Position Chief 
Role =  
Time-within Wednesday 
 
Table 1. Event Extraction Example 
 
We define the following standards to determine 
the correctness of an event mention: 
? A trigger is correctly labeled if its event type 
and offsets match a reference trigger. 
? An argument is correctly identified if its event 
type and offsets match any of the reference ar-
gument mentions. 
? An argument is correctly identified and classi-
fied if its event type, offsets, and role match 
any of the reference argument mentions. 
2.2 A Baseline Within-Sentence Event Tagger 
We use a state-of-the-art English IE system as our 
baseline (Grishman et al, 2005). This system ex-
tracts events independently for each sentence. Its 
training and test procedures are as follows.  
The system combines pattern matching with sta-
tistical models. For every event mention in the 
ACE training corpus, patterns are constructed 
based on the sequences of constituent heads sepa-
rating the trigger and arguments. In addition, a set 
of Maximum Entropy based classifiers are trained: 
? Trigger Labeling: to distinguish event men-
tions from non-event-mentions, to classify 
event mentions by type;  
? Argument Classifier: to distinguish arguments 
from non-arguments; 
? Role Classifier: to classify arguments by ar-
gument role.  
? Reportable-Event Classifier: Given a trigger, 
an event type, and a set of arguments, to de-
termine whether there is a reportable event 
mention. 
In the test procedure, each document is scanned 
for instances of triggers from the training corpus. 
When an instance is found, the system tries to 
match the environment of the trigger against the set 
of patterns associated with that trigger. This pat-
tern-matching process, if successful, will assign 
some of the mentions in the sentence as arguments 
of a potential event mention. The argument clas-
sifier is applied to the remaining mentions in the 
sentence; for any argument passing that classifier, 
the role classifier is used to assign a role to it. Fi-
nally, once all arguments have been assigned, the 
reportable-event classifier is applied to the poten-
tial event mention; if the result is successful, this 
event mention is reported. 
3 Motivations 
In this section we shall present our motivations 
based on error analysis for the baseline event tag-
ger. 
3.1 One Trigger Sense Per Cluster 
Across a heterogeneous document corpus, a partic-
ular verb can sometimes be trigger and sometimes 
not, and can represent different event types. How-
ever, for a collection of topically-related docu-
ments, the distribution may be much more 
convergent. We investigate this hypothesis by au-
tomatically obtaining 25 related documents for 
each test text. The statistics of some trigger exam-
ples are presented in table 2. 
255
 
Candidate Triggers 
 
Event Type 
Perc./Freq. as 
trigger in ACE 
training corpora 
Perc./Freq. as 
trigger in test  
document 
Perc./Freq. as 
trigger in test + 
related  
documents 
 
Correct 
Event 
Triggers 
advance Movement_Transport 31% of 16 50% of 2 88.9% of 27 
fire Personnel_End-Position 7% of 81 100% of 2 100% of 10 
fire Conflict_Attack 54% of 81 100% of 3 100% of 19 
replace Personnel_End-Position 5% of 20 100% of 1 83.3% of 6 
form Business_Start-Org 12% of 8 100% of 2 100% of 23 
talk Contact_Meet 59% of 74 100% of 4 100% of 26 
Incorrect 
Event 
Triggers 
hurt Life_Injure 24% of 33 0% of 2 0% of 7 
execution Life_Die 12% of 8 0% of 4 4% of 24 
 
Table 2. Examples: Percentage of a Word as Event Trigger in Different Data Collections 
 
As we can see from the table, the likelihood of a 
candidate word being an event trigger in the test 
document is closer to its distribution in the collec-
tion of related documents than the uniform training 
corpora. So if we can determine the sense (event 
type) of a word in the related documents, this will 
allow us to infer its sense in the test document. In 
this way related documents can help recover event 
mentions missed by within-sentence extraction.  
For example, in a document about ?the advance 
into Baghdad?: 
 
Example 1:  
[Test Sentence]  
Most US army commanders believe it is critical to 
pause the breakneck advance towards Baghdad to se-
cure the supply lines and make sure weapons are oper-
able and troops resupplied?. 
[Sentences from Related Documents]  
British and US forces report gains in the advance on 
Baghdad and take control of Umm Qasr, despite a 
fierce sandstorm which slows another flank. 
? 
 
The baseline event tagger is not able to detect 
?advance? as a ?Movement_Transport? event trig-
ger because there is no pattern ?advance towards 
[Place]? in the ACE training corpora (?advance? 
by itself is too ambiguous). The training data, 
however, does include the pattern ?advance on 
[Place]?, which allows the instance of ?advance? in 
the related documents to be successfully identified 
with high confidence by pattern matching as an 
event. This provides us much stronger ?feedback? 
confidence in tagging ?advance? in the test sen-
tence as a correct trigger. 
On the other hand, if a word is not tagged as an 
event trigger in most related documents, then it?s 
less likely to be correct in the test sentence despite 
its high local confidence. For example, in a docu-
ment about ?assessment of Russian president Pu-
tin?: 
 
Example 2:  
[Test Sentence]  
But few at the Kremlin forum suggested that Putin's 
own standing among voters will be hurt by Russia's 
apparent diplomacy failures. 
[Sentences from Related Documents]  
Putin boosted ties with the United States by throwing 
his support behind its war on terrorism after the Sept. 
11 attacks, but the Iraq war has hurt the relationship. 
? 
 
The word ?hurt? in the test sentence is mistaken-
ly identified as a ?Life_Injure? trigger with high 
local confidence (because the within-sentence ex-
tractor misanalyzes ?voters? as the object of ?hurt? 
and so matches the pattern ?[Person] be hurt?). 
Based on the fact that many other instances of 
?hurt? are not ?Life_Injure? triggers in the related 
documents, we can successfully remove this wrong 
event mention in the test document. 
3.2 One Argument Role Per Cluster 
Inspired by the observation about trigger distribu-
tion, we propose a similar hypothesis ? one argu-
ment role per cluster for event arguments. In other 
words, each entity plays the same argument role, or 
no role, for events with the same type in a collec-
tion of related documents. For example, 
 
256
Example 3:  
[Test Sentence]  
Vivendi earlier this week confirmed months of press 
speculation that it planned to shed its entertainment 
assets by the end of the year. 
[Sentences from Related Documents]  
Vivendi has been trying to sell assets to pay off huge 
debt, estimated at the end of last month at more than 
$13 billion. 
Under the reported plans, Blackstone Group would 
buy Vivendi's theme park division, including Universal 
Studios Hollywood, Universal Orlando in Florida... 
? 
   
The above test sentence doesn?t include an ex-
plicit trigger word to indicate ?Vivendi? as a ?sel-
ler? of a ?Transaction_Transfer-Ownership? event 
mention, but ?Vivendi? is correctly identified as 
?seller? in many other related sentences (by match-
ing patterns ?[Seller] sell? and ?buy [Seller]?s?). 
So we can incorporate such additional information 
to enhance the confidence of ?Vivendi? as a ?sel-
ler? in the test sentence. 
  On the other hand, we can remove spurious ar-
guments with low cross-document frequency and 
confidence. In the following example,  
 
Example 4:  
[Test Sentence]  
The Davao Medical Center, a regional government 
hospital, recorded 19 deaths with 50 wounded. 
 
?the Davao Medical Center? is mistakenly 
tagged as ?Place? for a ?Life_Die? event mention. 
But the same annotation for this mention doesn?t 
appear again in the related documents, so we can 
determine it?s a spurious argument. 
4 System Approach Overview 
Based on the above motivations we propose to in-
corporate global evidence from a cluster of related 
documents to refine local decisions. This section 
gives more details about the baseline within-
sentence event tagger, and the information retrieval 
system we use to obtain related documents. In the 
next section we shall focus on describing the infe-
rence procedure. 
4.1 System Pipeline 
Figure 1 depicts the general procedure of our ap-
proach. EMSet represents a set of event mentions 
which is gradually updated. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Cross-doc Inference for Event Extraction 
4.2 Within-Sentence Event Extraction 
For each event mention in a test document t , the 
baseline Maximum Entropy based classifiers pro-
duce three types of confidence values: 
 
? LConf(trigger,etype): The probability of a 
string trigger indicating an event mention with 
type etype; if the event mention is produced by 
pattern matching then assign confidence 1. 
? LConf(arg, etype): The probability that a men-
tion arg is an argument of some particular 
event type etype. 
? LConf(arg, etype, role): If arg is an argument 
with event type etype, the probability of arg 
having some particular role. 
 
We apply within-sentence event extraction to get 
an initial set of event mentions 0
tEMSet , and con-
duct cross-sentence inference (details will be pre-
sented in section 5) to get an updated set of event 
mentions 1
tEMSet . 
4.3 Information Retrieval 
We then use the INDRI retrieval system (Strohman 
et al, 2005) to obtain the top N (N=25 in this pa-
Test doc
Within-sent 
Event Extraction
Query 
Construction 
Cross-sent 
Inference
Query 
Unlabeled 
Corpora 
Information 
Retrieval 
Related 
docs
Within-sent 
Event Extraction
Cross-sent 
Inference
1
rEMSet
Cross-doc 
Inference
0
tEMSet
0
rEMSet
1
tEMSet
2
tEMSet
257
per3) related documents. We construct an INDRI 
query from the triggers and arguments, each 
weighted by local confidence and frequency in the 
test document. For each argument we also add oth-
er names coreferential with or bearing some ACE 
relation to the argument. 
For each related document r returned by INDRI, 
we repeat the within-sentence event extraction and 
cross-sentence inference procedure, and get an ex-
panded event mention set 1
t rEMSet + . Then we apply 
cross-document inference to 1
t rEMSet +  and get the 
final event mention output 2
tEMSet . 
5 Global Inference 
The central idea of inference is to obtain docu-
ment-wide and cluster-wide statistics about the 
frequency with which triggers and arguments are 
associated with particular types of events, and then 
use this information to correct event and argument 
identification and classification.  
For a set of event mentions we tabulate the fol-
lowing document-wide and cluster-wide confi-
dence-weighted frequencies: 
? for each trigger string, the frequency with 
which it appears as the trigger of an event of a 
particular type; 
? for each event argument string and the names 
coreferential with or related to the argument, 
the frequency of the event type; 
? for each event argument string and the names 
coreferential with or related to the argument, 
the frequency of the event type and role. 
Besides these frequencies, we also define the 
following margin metric to compute the confi-
dence of the best (most frequent) event type or role: 
 
Margin = 
   (WeightedFrequency (most frequent value) 
    ? WeightedFrequency (second most freq value))/ 
   WeightedFrequency (second most freq value) 
A large margin indicates greater confidence in 
the most frequent value. We summarize the fre-
quency and confidence metrics in Table 3. 
Based on these confidence metrics, we designed 
the inference rules in Table 4. These rules are ap-
plied in the order (1) to (9) based on the principle 
of improving ?local? information before global 
                                                          
3 We tested different N ? [10, 75] on dev set; and N=25 
achieved best gains. 
propagation. Although the rules may seem com-
plex, they basically serve two functions:    
? to remove triggers and arguments with low 
(local or cluster-wide) confidence; 
? to adjust trigger and argument identification 
and classification to achieve (document-wide 
or cluster-wide) consistency. 
6 Experimental Results and Analysis 
In this section we present the results of applying 
this inference method to improve ACE event ex-
traction. 
6.1 Data 
We used 10 newswire texts from ACE 2005 train-
ing corpora (from March to May of 2003) as our 
development set, and then conduct blind test on a 
separate set of 40 ACE 2005 newswire texts. For 
each test text we retrieved 25 related texts from 
English TDT5 corpus which in total consists of 
278,108 texts (from April to September of 2003). 
6.2 Confidence Metric Thresholding 
We select the thresholds (?k with k=1~13) for vari-
ous confidence metrics by optimizing the F-
measure score of each rule on the development set, 
as shown in Figure 2 and 3 as follows. 
Each curve in Figure 2 and 3 shows the effect on 
precision and recall of varying the threshold for an 
individual rule.  
 
 
Figure 2. Trigger Labeling Performance with  
Confidence Thresholding on Dev Set 
258
 
 
Figure 3. Argument Labeling Performance with 
Confidence Thresholding on Dev Set 
 
The labeled point on each curve shows the best 
F-measure that can be obtained on the develop-
ment set by adjusting the threshold for that rule. 
The gain obtained by applying successive rules can 
be seen in the progression of successive points to-
wards higher recall and, for argument labeling, 
precision4. 
6.3 Overall Performance 
Table 5 shows the overall Precision (P), Recall (R) 
and F-Measure (F) scores for the blind test set. In 
addition, we also measured the performance of two 
human annotators who prepared the ACE 2005 
training data on 28 newswire texts (a subset of the 
blind test set). The final key was produced by re-
view and adjudication of the two annotations. 
Both cross-sentence and cross-document infe-
rences provided significant improvement over the 
baseline with local confidence thresholds con-
trolled. 
We conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on a document basis. The re-
sults show that the improvement using cross-
sentence inference is significant at a 99.9% confi-
dence level for both trigger and argument labeling; 
adding cross-document inference is significant at a 
99.9% confidence level for trigger labeling and 
93.4% confidence level for argument labeling. 
                                                          
4 We didn?t show the classification adjusting rules (2), (6) and 
(8) here because of their relatively small impact on dev set. 
6.4 Discussion 
From table 5 we can see that for trigger labeling 
our approach dramatically enhanced recall (22.9% 
improvement) with some loss (7.4%) in precision. 
This precision loss was much larger than that for 
the development set (0.3%). This indicates that the 
trigger propagation thresholds optimized on the 
development set were too low for the blind test set 
and thus more spurious triggers got propagated. 
The improved trigger labeling is better than one 
human annotator and only 4.7% worse than anoth-
er. 
For argument labeling we can see that cross-
sentence inference improved both identification 
(3.7% higher F-Measure) and classification (6.1% 
higher accuracy); and cross-document inference 
mainly provided further gains (1.9%) in classifica-
tion. This shows that identification consistency 
may be achieved within a narrower context while 
the classification task favors more global back-
ground knowledge in order to solve some difficult 
cases. This matches the situation of human annota-
tion as well: we may decide whether a mention is 
involved in some particular event or not by reading 
and analyzing the target sentence itself; but in or-
der to decide the argument?s role we may need to 
frequently refer to wider discourse in order to infer 
and confirm our decision. In fact sometimes it re-
quires us to check more similar web pages or even 
wikipedia databases. This was exactly the intuition 
of our approach. We should also note that human 
annotators label arguments based on perfect entity 
mentions, but our system used the output from the 
IE system. So the gap was also partially due to 
worse entity detection. 
Error analysis on the inference procedure shows 
that the propagation rules (3), (4), (7) and (9) pro-
duced a few extra false alarms. For trigger labe-
ling, most of these errors appear for support verbs 
such as ?take? and ?get? which can only represent 
an event mention together with other verbs or 
nouns. Some other errors happen on nouns and 
adjectives. These are difficult tasks even for human 
annotators. As shown in table 5 the inter-annotator 
agreement on trigger identification is only about 
40%. Besides some obvious overlooked cases (it?s 
probably difficult for a human to remember 33 dif-
ferent event types during annotation), most diffi-
culties were caused by judging generic verbs, 
nouns and adjectives.
259
             Performance 
 
System/Human 
Trigger  
Identification 
+Classification
Argument  
Identification 
Argument 
Classification 
Accuracy 
Argument  
Identification 
+Classification
P R F P R F P R F 
Within-Sentence IE with  
Rule (1) (Baseline) 67.6 53.5 59.7 47.8 38.3 42.5 86.0 41.2 32.9 36.6 
Cross-sentence Inference 64.3 59.4 61.8 54.6 38.5 45.1 90.2 49.2 34.7 40.7 
Cross-sentence+ 
Cross-doc Inference 60.2 76.4 67.3 55.7 39.5 46.2 92.1 51.3 36.4 42.6 
Human Annotator1 59.2 59.4 59.3 60.0 69.4 64.4 85.8 51.6 59.5 55.3 
Human Annotator2 69.2 75.0 72.0 62.7 85.4 72.3 86.3 54.1 73.7 62.4 
Inter-Annotator Agreement 41.9 38.8 40.3 55.2 46.7 50.6 91.7 50.6 42.9 46.4 
 
Table 5. Overall Performance on Blind Test Set (%) 
 
In fact, compared to a statistical tagger trained on 
the corpus after expert adjudication, a human an-
notator tends to make more mistakes in trigger 
classification. For example it?s hard to decide 
whether ?named? represents a ?Person-
nel_Nominate? or ?Personnel_Start-Position? 
event mention; ?hacked to death? represents a 
?Life_Die? or ?Conflict_Attack? event mention 
without following more specific annotation guide-
lines. 
7 Related Work 
The trigger labeling task described in this paper is 
in part a task of word sense disambiguation 
(WSD), so we have used the idea of sense consis-
tency introduced in (Yarowsky, 1995), extending 
it to operate across related documents.  
Almost all the current event extraction systems 
focus on processing single documents and, except 
for coreference resolution, operate a sentence at a 
time (Grishman et al, 2005; Ahn, 2006; Hardy et 
al., 2006).  
We share the view of using global inference to 
improve event extraction with some recent re-
search. Yangarber et al (Yangarber and Jokipii, 
2005; Yangarber, 2006; Yangarber et al, 2007) 
applied cross-document inference to correct local 
extraction results for disease name, location and 
start/end time. Mann (2007) encoded specific infe-
rence rules to improve extraction of CEO (name, 
start year, end year) in the MUC management 
succession task. In addition, Patwardhan and Ri-
loff (2007) also demonstrated that selectively ap-
plying event patterns to relevant regions can 
improve MUC event extraction. We expand the 
idea to more general event types and use informa-
tion retrieval techniques to obtain wider back-
ground knowledge from related documents. 
8 Conclusion and Future Work 
One of the initial goals for IE was to create a da-
tabase of relations and events from the entire input 
corpus, and allow further logical reasoning on the 
database. The artificial constraint that extraction 
should be done independently for each document 
was introduced in part to simplify the task and its 
evaluation. In this paper we propose a new ap-
proach to break down the document boundaries 
for event extraction. We gather together event ex-
traction results from a set of related documents, 
and then apply inference and constraints to en-
hance IE performance. 
In the short term, the approach provides a plat-
form for many byproducts. For example, we can 
naturally get an event-driven summary for the col-
lection of related documents; the sentences includ-
ing high-confidence events can be used as 
additional training data to bootstrap the event tag-
ger; from related events in different timeframes 
we can derive entailment rules; the refined consis-
tent events can serve better for other NLP tasks 
such as template based question-answering. The 
aggregation approach described here can be easily 
extended to improve relation detection and corefe-
rence resolution (two argument mentions referring 
to the same role of related events are likely to 
corefer). Ultimately we would like to extend the 
system to perform essential, although probably 
lightweight, event prediction. 
 
260
XSent-Trigger-Freq(trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all sentences within a document 
XDoc-Trigger-Freq (trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all documents in a cluster  
XDoc-Trigger-BestFreq (trigger) Maximum over all etypes of XDoc-Trigger-Freq (trigger, etype) 
XDoc-Arg-Freq(arg, etype) The weighted frequency of arg appearing as an argument of an event of type etype across all documents in a cluster 
XDoc-Role-Freq(arg, etype, role)  The weighted frequency of arg appearing as an argument of an event of type etype with role role across all documents in a cluster 
XDoc-Role-BestFreq(arg)  Maximum over all etypes and roles of XDoc-Role-Freq(arg, etype, role) 
XSent-Trigger-Margin(trigger) The margin value of trigger in XSent-Trigger-Freq 
XDoc-Trigger-Margin(trigger) The margin value of trigger in XDoc-Trigger-Freq 
XDoc-Role-Margin(arg) The margin value of arg in XDoc-Role-Freq 
 
Table 3. Global Frequency and Confidence Metrics 
 
Rule (1): Remove Triggers and Arguments with Low Local Confidence 
If LConf(trigger, etype) < ?1, then delete the whole event mention EM; 
If LConf(arg, etype) < ?2 or LConf(arg, etype, role) < ?3, then delete arg. 
Rule (2): Adjust Trigger Classification to Achieve Document-wide Consistency 
If XSent-Trigger-Margin(trigger) >?4, then propagate the most frequent etype to all event mentions with  trigger in 
the document; and correct roles for corresponding arguments. 
Rule (3): Adjust Trigger Identification to Achieve Document-wide Consistency 
If LConf(trigger, etype) > ?5, then propagate etype to all unlabeled strings trigger in the document. 
Rule (4): Adjust Argument Identification to Achieve Document-wide Consistency 
If LConf(arg, etype) > ?6, then in the document, for each sentence containing an event mention EM with etype, add 
any unlabeled mention in that sentence with the same head as arg as an argument of EM with role. 
Rule (5): Remove Triggers and Arguments with Low Cluster-wide Confidence 
If XDoc-Trigger-Freq (trigger, etype) < ?7, then delete EM;  
If XDoc-Arg-Freq(arg, etype) < ?8 or XDoc-Role-Freq(arg, etype, role) < ?9, then delete arg. 
Rule (6): Adjust Trigger Classification to Achieve Cluster-wide Consistency 
If XDoc-Trigger-Margin(trigger) >?10, then propagate most frequent etype to all event mentions with trigger in the 
cluster; and correct roles for corresponding arguments. 
Rule (7): Adjust Trigger Identification to Achieve Cluster-wide Consistency 
If XDoc-Trigger-BestFreq (trigger) >?11, then propagate etype to all unlabeled strings trigger in the cluster, override 
the results of Rule (3) if conflict. 
Rule (8): Adjust Argument Classification to Achieve Cluster-wide Consistency 
If XDoc-Role-Margin(arg) >?12, then propagate the most frequent etype and role to all arguments with the same 
head as arg in the entire cluster. 
Rule (9): Adjust Argument Identification to Achieve Cluster-wide Consistency 
If XDoc-Role-BestFreq(arg) > ?13, then in the cluster, for each sentence containing an event mention EM with etype, 
add any unlabeled mention in that sentence with the same head as arg as an argument of EM with role. 
 
Table 4. Probabilistic Inference Rule 
 
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657. Any opinions, findings and conclusions 
expressed in this material are those of the authors 
and do not necessarily reflect the views of the U. S. 
Government. 
261
References  
David Ahn. 2006. The stages of event extraction. Proc. 
COLING/ACL 2006 Workshop on Annotating and 
Reasoning about Time and Events. Sydney, Aus-
tralia. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Descrip-
tion. Proc. ACE 2005 Evaluation Workshop. Wash-
ington, US. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Us-
ing Surface Text Features. Proc. AAAI06 Workshop 
on Event Extraction and Synthesis. Boston, Massa-
chusetts. US. 
Gideon Mann. 2007. Multi-document Relationship Fu-
sion via Constraints on Probabilistic Databases. 
Proc. HLT/NAACL 2007. Rochester, NY, US. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. Proc. EMNLP 2007. 
Prague, Czech Republic. 
Trevor Strohman, Donald Metzler, Howard Turtle and 
W. Bruce Croft. 2005. Indri: A Language-model 
based Search Engine for Complex Queries (ex-
tended version). Technical Report IR-407, CIIR, 
Umass Amherst, US. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. Proc. RANLP 2007 work-
shop on Multi-source, Multilingual Information Ex-
traction and Summarization. Borovets, Bulgaria. 
Roman Yangarber. 2006. Verification of Facts across 
Document Boundaries. Proc. International Work-
shop on Intelligent Information Access. Helsinki, 
Finland. 
Roman Yangarber and Lauri Jokipii. 2005.   Redundan-
cy-based Correction of Automatically Extracted 
Facts. Proc. HLT/EMNLP 2005. Vancouver, Cana-
da. 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Proc. 
ACL 1995. Cambridge, MA, US. 
 
 
262
Applying Coreference to Improve Name Recognition 
Heng JI and Ralph GRISHMAN 
Department of Computer Science 
New York University 
715 Broadway, 7th Floor 
New York, NY 10003, U.S.A. 
   hengji@cs.nyu.edu,  grishman@cs.nyu.edu 
 
Abstract 
We present a novel method of applying the 
results of coreference resolution to improve 
Name Recognition for Chinese.  We consider 
first some methods for gauging the confidence 
of individual tags assigned by a statistical 
name tagger.  For names with low confidence, 
we show how these names can be filtered 
using coreference features to improve 
accuracy.  In addition, we present rules which 
use coreference information to correct some 
name tagging errors.  Finally, we show how 
these gains can be magnified by clustering 
documents and using cross-document 
coreference in these clusters.  These combined 
methods yield an absolute improvement of 
about 3.1% in tagger F score. 
 
1 Introduction 
The problem of name recognition and 
classification has been intensively studied since 
1995, when it was introduced as part of the MUC-
6 Evaluation (Grishman and Sundheim, 1996).  A 
wide variety of machine learning methods have 
been applied to this problem, including Hidden 
Markov Models (Bikel et al 1997), Maximum 
Entropy methods (Borthwick et al 1998, Chieu 
and Ng 2002), Decision Trees (Sekine et al 1998), 
Conditional Random Fields (McCallum and Li 
2003), Class-based Language Model (Sun et al 
2002), Agent-based Approach (Ye et al 2002) and 
Support Vector Machines. However, the 
performance of even the best of these models1 has 
been limited by the amount of labeled training data 
available to them and the range of features which 
they employ.  In particular, most of these methods 
classify an instance of a name based on the 
information about that instance alone, and very 
local context of that instance ? typically, one or 
                                                   
1
  The best results reported for Chinese named entity 
recognition, on the MET-2 test corpus, are 0.92 to 0.95   
F-measure for the different name types (Ye et al 2002). 
two words preceding and following the name.  If a 
name has not been seen before, and appears in a 
relatively uninformative context, it becomes very 
hard to classify. 
We propose to use more global information to 
improve the performance of name recognition.  
Some name taggers have incorporated a name 
cache or similar mechanism which makes use of 
names previously recognized in the document.  In 
our approach, we perform coreference analysis and 
then use detailed evidence from other phrases in 
the document which are co-referential with this 
name in order to disambiguate the name.  This 
allows us to perform a richer set of corrections 
than with a name cache.  We then go one step 
further and process similar documents containing 
instances of the same name, and combine the 
evidence from these additional instances.  At each 
step we are able to demonstrate a small but 
consistent improvement in named entity 
recognition. 
The rest of the paper is organized as follows. 
Section 2 briefly describes the baseline name 
tagger and coreference resolver used in this paper. 
Section 3 considers methods for assessing the 
confidence of name tagging decisions.  Section 4 
examines the distribution of name errors, as a 
motivation for using coreference information. 
Section 5 shows the coreference features we use 
and how they are incorporated into a statistical 
name filter.  Section 6 describes additional rules 
using coreference to improve name recognition. 
Section 7 provides the flow graph of the improved 
system.  Section 8 reports and discusses the 
experimental results while Section 9 summarizes 
the conclusions. 
2 Baseline Systems 
The task we consider in this paper is to identify 
three classes of names in Chinese text:  persons 
(PER), organizations (ORG), and geo-political 
entities (GPE).  Geo-political entities are locations 
which have an associated government, such as 
cities, states, and countries.2  Name recognition in 
Chinese poses extra challenges because neither 
capitalization nor word segmentation clues are 
explicitly provided, although most of the 
techniques we describe are more generally 
applicable. 
Our study builds on an extraction system 
developed for the ACE evaluation, a multi-site 
evaluation of information extraction organized by 
the U.S. Government.  Following ACE 
terminology, we will use the term mention to refer 
to a name or noun phrase of one of the types of 
interest, and the term entity for a set of coreferring 
mentions.  We briefly describe in this section the 
baseline Chinese named entity tagger, as well as 
the coreference system, used in our experiments. 
2.1 Chinese Name Tagger 
Our baseline name tagger consists of an HMM 
tagger augmented with a set of post-processing 
rules.  The HMM tagger generally follows the 
NYMBLE model (Bikel et al 1997), but with a 
larger number of states (12) to handle name 
prefixes and suffixes, and transliterated foreign 
names separately.  It operates on the output of a 
word segmenter from Tsinghua University.  It uses 
a trigram model with dynamic backoff.  The post-
processing rules correct some omissions and 
systematic errors using name lists (for example, a 
list of all Chinese last names; lists of organization 
and location suffixes) and particular contextual 
patterns (for example, verbs occurring with 
people?s names).  They also deal with 
abbreviations and nested organization names. 
 
2.2 Chinese Coreference Resolver 
For this study we have used a rule-based 
coreference resolver.  Table 1 lists the main rules 
and patterns used.  We have extensive rules for 
name-name coreference, including rules specific to 
the particular name types.  For these experiments, 
we do not attempt to resolve pronouns, and we 
only resolve names with nominals when the name 
and nominal appear in close proximity in a specific 
structure, as listed in Table 1. 
We have used the MUC coreference scoring 
metric (Vilain et al 1995) to evaluate this resolver, 
excluding all pronouns and limiting ourselves to 
noun phrases of semantic type PER, ORG, and 
GPE.  Using a perfect (hand-generated) set of 
mentions, we obtain a recall of 82.7% and 
precision of 95.1%, for an F score of 88.47%.  
                                                   
2
 This class is used in the U.S. Government?s ACE 
evaluations;  it excludes locations without governments, 
such as bodies of water and mountains. 
Using the mentions generated by our extraction 
system, we obtain a recall of 74.3%, a precision of 
84.5%, and an F score of 79.07%.3  
3 Confidence Measures 
In order to decide when we need to rely on 
global (coreference) information for name tagging, 
we want to have some assessment of the 
confidence that the name tagger has in individual 
tagging decisions.  In this paper, we use two tools 
to reach this goal.  The first method is to use three 
manually built proper name lists which include 
common names of each type (selected from the 
high frequency names in the user query blog of 
COMPASS, a Chinese search engine, and name 
lists provided by Linguistic Data Consortium; the 
PER list includes 147 names, the GPE list 226 
names, and the ORG list 130 names).  Names on 
these lists are accepted without further review. 
The second method is to have the HMM tagger 
compute a probability margin for the identification 
of a particular name as being of a particular type.  
Scheffer et al (2001) used a similar method to 
identify good candidates for tagging in an active 
learner.  During decoding, the HMM tagger seeks 
the path of maximal probability through the Viterbi 
lattice.  Suppose we wish to evaluate the 
confidence with which words wi, ?, wj are 
identified as a name of type T.  We compute 
 
Margin (wi,?, wj; T) =  log P1 ? log P2 
 
Here P1 is the maximum path probability and P2 is 
the maximum probability among all paths for 
which some word in wi, ?, wj is assigned a tag 
other than T. 
A large margin indicates greater confidence in 
the tag assignment.  If we exclude names tagged 
with a margin below a threshold, we can increase 
the precision of name tagging at some cost in recall.  
Figure 1 shows the trade-off between margin 
threshold and name recognition performance.  
Names with a margin over 3.0 are accepted on this 
basis. 
                                                   
3
 In our scoring, we use the ACE keys and only score 
mentions which appear in both the key and system 
response.  This therefore includes only mentions 
identified as being in the ACE semantic categories by 
both the key and the system response.  Thus these 
scores cannot be directly compared against coreference 
scores involving all noun phrases. 
85
87
89
91
93
95
97
99
0 1 2 3 4 5 6 7 8 9 10 11 12
Threshold
Pr
ec
isi
o
n
(%
)
 
Figure 1: Tradeoff between Margin Threshold and 
name recognition performance 
 
4   Distribution of Name Errors 
We consider now names which did not pass the 
confidence measure tests: names not on the 
common name list, which were tagged with a 
margin below the threshold.  We counted the 
accuracy of these ?obscure? names as a function of 
the number of mentions in an entity; the results are 
shown in Table 2. 
The table shows that the accuracy of name 
recognition increases as the entity includes more 
mentions.  In other words, if a name has more 
coref-ed mentions, it is more likely to be correct. 
This also provides us a linguistic intuition: if 
people mention an obscure name in a text, they 
tend to emphasize it later by repeating the same 
name or describe it with nominal mentions. 
The table also indicates that the accuracy of 
single name entities (singletons) is much lower 
than the overall accuracy.  So, although they 
constitute only about 10% of all names, increasing 
their accuracy can significantly improve overall 
performance.  Coreference information can play a 
great role here.  Take the 157 PER singletons as an 
example; 56% are incorrect names. Among these 
incorrect names, 73% actually belong to the other 
two name types.  Many of these can be easily fixed 
by searching for coreference to other mentions 
without type restriction.  Among the correct names, 
71% can be confirmed by the presence of a title 
word or a Chinese last name.  From these 
observations we can conclude that without strong 
confirmation features, singletons are much less 
likely to be correct names. 
5 Incorporating Coreference Information 
into Name Recognition 
We make use of several features of the 
coreference relations a name is involved in; the 
features are listed in Table 3.  Using these features, 
we built an independent classifier to predict if a 
name identified by the baseline name tagger is 
correct or not. (Note that this classifier is trained 
on all name mentions, but during test only 
?obscure? names which failed the tests in section 3 
are processed by this classifier.) Each name 
corresponds to a feature vector which consists of 
the factors described in Table 3.  The PER context 
words are generated from the context patterns 
described in  (Ji and Luo, 2001).  We used a 
Support Vector Machine to implement the 
classifier, because of its state-of-the-art 
performance and good generalization ability.  We 
used a polynomial kernel of degree 3. 
6 Name Rules based on Coreference 
Besides the factors in the above statistical model, 
additional coreference information can be used to 
filter and in some cases correct the tagging 
produced by the HMM.  We developed the 
following rules to correct names generated by the 
baseline tagger. 
6.1 Name Structure Errors 
Sometimes the Name tagger outputs names 
which are too short (incomplete) or too long.  We 
can make use of the relation among mentions in 
the same entity to fix them.  For example, nested 
ORGs are traditionally difficult to recognize 
correctly.  Errors in ORG names can take the 
following forms: 
 
(1) Head Missed. Examples: ????????/ 
Chinese Art (Group)?, ????????/ Chinese 
Student (Union)?, ??????????/ Russian 
Nuclear Power (Instituition)? 
 
Rule 1: If an ORG name x is coref-ed with other 
mentions with head y (an ORG suffix), and in the 
original text x is immediately followed by y, then 
tag xy instead of x; otherwise discard x. 
 
(2) Modifier Missed. Rule 1 can also be used to 
restore missed modifiers. For example, ????
???? / (Edinburgh) University?; ??????
??? / (Peng Cheng) Limited Corporation?, and 
some incomplete translated PER names such as 
??????? / (Pa)lestine?. 
 
(3) Name Too Long 
Rule 2: If a name x has no coref-ed mentions 
but part of it, x', is identical to a name in another 
entity y, and y includes at least two mentions; then 
tag x' instead of x. 
.
 Rule Type Rule Description 
Ident(i, j) Mentioni and Mentionj are identical 
Abbrev(i, j) Mentioni is an abbreviation of Mentionj 
Modifier(i, j) Mentionj = Modifier + ?de? + Mentioni 
 
 
All 
Formal(i, j) Formal and informal ways of referring to the same entity 
(Ex. ?????? / American Defense Dept. &  
????/ Pentagon?) 
Substring(i, j) Mentioni is a substring of Mentionj   
PER Title(i, j) Mentionj = Mentioni + title word; or 
Mentionj = LastName + title word 
ORG Head(i, j) Mentioni and Mentionj have the same head 
Head(i, j) Mentioni and Mentionj have the same head 
Capital(i, j) Mentioni: country name; 
Mentionj: name of the capital of this country 
Applied in restricted context. 
 
 
 
 
 
 
 
 
Name & 
Name 
 
 
 
GPE 
Country(i, j) Mentioni and Mentionj are different names referring to the same 
country.  
(Ex. ??? / China & ?? / Huaxia & ??? / Republic?) 
RSub(i, j) Namei is a right substring of Nominalj 
Apposition(i, j) Nominalj is the apposite of Namei 
 
All 
Modifier2(i, j) Nominalj = Determiner/Modifier + Namei/ head 
 
 
Name & 
Nominal 
 
GPE 
Ref(i, j) Nominalj = Namei + GPE Ref Word  
(examples of GPE Ref Word: ??? / Side?, ???/Government?, 
???? / Republic?, ?????/ Municipality?) 
IdentN(i, j) Nominali and Nominalj are identical Nominal& 
Nominal 
All 
Modifier3(i, j) Nominalj = Determiner/Modifier + Nominali 
 
Table1: Main rules used in the Coreference Resolver 
 
Number of mentions 
per entity 
Name Type 
1 
 
2 3 4 5 6 7 8 >8 
PER 43.94 87.07 91.23 87.95 91.57 91.92 94.74 92.31 97.36 
GPE 55.81 88.8 96.07 100 100 100 100 95.83 97.46 
ORG 64.71 80.59 89.47 94.29 100 100 -- -- 100 
 
Table 2 Accuracy(%) of ?obscure? name recognition 
 
 Factor Description 
Coreference Type 
Weight 
Average of weights of coreference relations for which this mention 
is antecedent:  0.8 for name-name coreference; 0.5 for apposition;  
0.3 for other name-nominal coreference 
First 
Mention 
Is first name mention in the entity 
Head Includes head word of name 
Idiom Name is part of an idiom 
PER context For PER Name, has context word in text 
PER title For PER Name, includes title word 
 
 
Mention 
Weight 
ORG suffix For ORG Name, includes suffix word 
Entity Weight Number of mentions in entity / total number of mentions in all 
entities in document which include a name mention  
 
Table 3 Coreference factors for name recognition 
6.2 Name Type Errors 
Some names are mistakenly recognized as other 
name types.  For example, the name tagger has 
difficulty in distinguishing transliterated PER 
name and transliterated GPE names. 
To solve this problem we designed the 
following rules based on the relation among 
entities. 
Rule 3: If namei is recognized as type1, the 
entity it belongs to has only one mention; and 
namej is recognized as type2, the entity it belongs 
to has at least two mentions; and namei is identical 
with namej or namei is a substring of namej, then 
correct type1 to type2. 
 
For example, if ????? / Kremlin? is 
mistakenly identified as PER, while ?????? 
/ Kremlin Palace? is correctly identified as ORG, 
and in coreference results, ????? / Kremlin? 
belongs to a singleton entity, while ?????? / 
Kremlin Palace? has coref-ed mentions, then we 
correct the type of ????? / Kremlin? to ORG.  
 
Another common mistake gives rise to the 
sequence ?PER+title+PER?, because our name 
tagger uses the title word as an important context 
feature for a person name (either preceding or 
following the title).  But this is an impossible 
structure in Chinese.  We can also use coreference 
information to fix it. 
Rule 4: If ?PER+title+PER? appears in the 
name tagger?s output,  then we discard the PER 
name with lower coref certainty; and check 
whether it is coref-ed to other mentions in a GPE 
entity or ORG entity; if it is, correct the type. 
Using this rule we can correctly identify ?[??
?? / Sri Lanka GPE] ?? / Premier [????
? / Bandaranaike PER]?, instead of ?[???? / 
Sri Lanka PER] ?? / Premier [????? / 
Bandaranaike PER]?. 
 
6.3 Name Abbreviation Errors 
Name abbreviations are difficult to recognize 
correctly due to a lack of training data.  Usually 
people adopt a separate list of abbreviations or 
design separate rules (Sun et al 2002) to identify 
them.  But many wrong abbreviation names might 
be produced.  We find that coreference 
information helps to select abbreviations. 
Rule 5: If an abbreviation name has no coref-ed 
mentions and it is not adjacent to another 
abbreviation (ex. ??/China ? /America?), then 
we discard it. 
 
7 System Flow 
Combining all the methods presented above, the 
flow of our final system is shown in Figure 2: 
 
 
Figure 2  System Flow 
 
8 Experiments 
8.1 Training and Test Data 
For our experiments, we used the Beijing 
University Insititute of Computational Linguistics 
corpus ? 2978 documents from the People?s Daily 
in 1998, one million words with name tags ? and 
the training corpus for  the 2003 ACE evaluation, 
223 documents.  153 of our ACE documents were 
used as our test set. 4   The 153 documents 
contained 1614 names.  Of the system-tagged 
names, 959 were considered ?obscure?:  were not 
on a name list and had a margin below the 
threshold.  These were the names to which the 
rules and classifier were applied.  We ran all the 
following experiments using the MUC scorer.  
                                                   
4
 The test set was divided into two parts, of 95 
documents and 58 documents.  We trained two name 
tagger and classifier models, each time using one part 
of the test set alng with all the other documents, and 
evaluated on the other part of the test set.  The results 
reported here are the combined results for the entire 
test set. 
Input 
 
Name 
tagger 
Nominal 
mention 
tagger 
Coreference 
Resolver 
Coreference 
Rules to fix  name 
errors 
SVM classifier to select 
correct names using 
coreference features 
Output 
8.2 Overall Performance Comparison 
Table 4 shows the performance of the baseline 
system; Table 5 the system with rule-based 
corrections; and Table 6 the system with both 
rules and the SVM classifier. 
 
Name Precision Recall F 
PER 90.9 88.2 89.5 
GPE 82.3 90.8 86.3 
ORG 92.1 91.8 91.9 
ALL 87.8 90.5 89.1 
 
Table 4 Baseline Name Tagger 
 
Name Precision Recall F 
PER 93.3 87.5 90.3 
GPE 83.5 90.4 86.8 
ORG 90.9 92.1 91.5 
ALL 88.5 90.3 89.4 
 
Table 5 Results with Coref Rules Alone 
 
Name Precision Recall F 
PER 95.7 84.4 89.7 
GPE 88.0 91.7 89.8 
ORG 94.5 91.2 92.8 
ALL 92.2 89.6 90.9 
 
Table 6 Results for Single Document System 
 
The gains we observed from coreference within 
single documents suggested that further 
improvement might be possible by gathering 
evidence from several related documents. 5  We 
did this in two stages.  First, we clustered the 153 
documents in the test set into 38 topical clusters.  
Most (29) of the clusters had only two documents;  
the largest had 28 documents.  We then applied 
the same procedures, treating the entire cluster as 
a single document.  This yielded another 1.0% 
improvement in overall F score (Table 7). 
The improvement in F score was consistent for 
the larger clusters (3 or more documents):  the F 
score improved for 8 of those clusters and 
remained the same for the 9th.  To heighten the 
multi-document benefit, we took 11 of the small 
                                                   
5
 Borthwick (1999) did use some cross-document 
information across the entire test corpus, maintaining 
in effect a name cache for the corpus, in addition to one 
for the document.  No attempt was made to select or 
cluster documents. 
(2 document clusters) and enlarged them by 
retrieving related documents from 
sina.com.cn.  In total, we added 52 texts to 
these 11 clusters.  The net result was a further 
improvement of 0.3% in F score (Table 8).6 
 
 
Name Precision Recall F 
PER 93.3 86.8 90.5 
GPE 95.2 90.0 92.5 
ORG 92.9 91.7 92.3 
ALL 93.8 90.1 91.9 
 
Table 7 Results for Mutiple Document System 
 
Name Precision Recall F 
PER 94.7 87.1 90.7 
GPE 95.6 89.6 92.5 
ORG 95.8 90.3 93.0 
ALL 95.4 89.2 92.2 
 
Table 8 Results for Mutiple Document System 
               with additional retrieved texts 
 
8.3 Contribution of Coreference Features 
Since feature selection is crucial to SVMs, we 
did experiments to determine how precision 
increased as each feature was added.  The results 
are shown in Figure 3.  We can see that each 
feature in the SVM helps to select correct names 
from the output of the baseline name tagger, 
although some (like FirstMention) are more 
crucial than others. 
 
87
88
89
90
91
92
93
B as
elin
e
Edg
eT
yp e
Fir
stM
en
tion He
ad
Id io
m
Per
C o
nte
x t
Per
Tit
le
Org
Suf
fix
Ent
ityW
eig
ht
Feature
Pr
ec
is
io
n
(%
)
 
Figure 3  Contributions of features 
 
                                                   
6
 Scores are still computed on the 153 test 
documents ;  the retrieved documents are excluded 
from the scoring. 
8.4 Comparison to Cache Model 
Some named entity systems use a name cache, 
in which tokens or complete names which have 
been previously assigned a tag are available as 
features in tagging the remainder of a document.  
Other systems have made a second tagging pass 
which uses information on token sequences 
tagged in the first pass (Borthwick 1999), or have 
used as features information about features 
assigned to other instances of the same token 
(Chieu and Ng 2002).  Our system, while more 
complex, makes use of a richer set of global 
features, involving the detailed structure of 
individual mentions, and in particular makes use 
of both name ? name and name ? nominal 
relations. 
 
We have compared the performance of our 
method (applied to single documents) with a 
voted cache model, which takes into account the 
number of times a particular name has been 
previously assigned each type of tag: 
 
System Precision Recall F 
baseline 88.8 90.5 89.1 
 voted cache 87.6 92.8 90.1 
current 92.2 89.6 90.9 
 
Table 9.  Comparison with voted cache 
 
Compared to a simple voted cache model, our 
model provides a greater improvement in name 
recognition F score; in particular, it can 
substantially increase the precision of name 
recognition.  The voted cache model can recover 
some missed names, but at some loss in precision. 
9 Conclusions and Future Work 
In this paper, we presented a novel idea of 
applying coreference information to improve 
name recognition.  We used both a statistical filter 
based on a set of coreference features and rules 
for correcting specific errors in name recognition.  
Overall, we obtained an absolute improvement of 
3.1% in F score.  Put another way, we were able 
to eliminate about 60% of erroneous name tags 
with only a small loss in recall. 
The methods were tested on a Chinese name 
tagger, but most of the techniques should be 
applicable to other languages.  More generally, it 
offers an example of using global and cross-
document information to improve local decisions 
for information extraction.  Such methods will be 
important for breaking the ?performance ceiling? 
in many areas of information extraction. 
In the future, we plan to experiment with 
improvements in coreference resolution (in 
particular, adding pronoun resolution) to see if we 
can obtain further gains in name recognition.  We 
also intend to explore the production of multiple 
tagging hypotheses by our statistical name tagger, 
with the alternative hypotheses then reranked 
using global information.  This may allow us to 
replace some of our hand-coded error-correction 
rules with corpus-trained methods. 
10 Acknowledgements 
This research was supported by the Defense 
Advanced Research Projects Agency as part of the 
Translingual Information Detection, Extraction 
and Summarization (TIDES) program, under 
Grant N66001-001-1-8917 from the Space and 
Naval Warfare Systems Center San Diego, and by 
the National Science Foundation under Grants 
IIS-0081962 and 0325657.  This paper does not 
necessarily reflect the position or the policy of the 
U.S. Government. 
References  
Daniel M. Bikel, Scott Miller, Richard Schuartz, 
and Ralph Weischedel. 1999. Nymble: a high-
performance Learning Name-finder.  Proc. Fifth 
Conf. On Applied Natural Language Processing, 
Washington, D.C. 
Andrew Borthwick.  1999. A Maximum Entropy 
Approach to Named Entity Recognition.  Ph.D. 
Dissertation, Dept. of Computer Science, New 
York University. 
Andrew Borthwick, John Sterling, Eugene 
Agichtein, and Ralph Grishman. 1998. 
Exploiting Diverse Knowledge Sources via 
Maximum Entropy in Named Entity 
Recognition. Proc. Sixth Workshop on Very 
Large Corpora, Montreal, Canada. 
Hai Leong Chieu and Hwee Tou Ng. 2002.  
Named Entity Recognition: A Maximum 
Entropy Approach Using Global Information.  
Proc.: 17th Int?l Conf. on Computational 
Linguistics (COLING 2002), Taipei, Taiwan. 
Ralph Grishman and Beth Sundheim.  1996. 
Message understanding conference - 6: A brief 
history. Proc. 16th Int?l Conference on 
Computational Linguistics (COLING 96), 
Copenhagen. 
Heng Ji, Zhensheng Luo, 2001. A Chinese Name 
Identifying System Based on Inverse Name 
Frequency Model and Rules. Natural Language 
Processing and Knowledge Engineering 
(NLPKE) Mini Symposium of 2001 IEEE 
International Conference on Systems, Man, and 
Cybernetics (SMC2001) 
Andrew McCallum and Wei Li.  2003.  Early 
results for Named Entity Recognition With 
Conditional Random Fields, Feature Induction, 
and Web-Enhanced Lexicons. Proc. Seventh 
Conf. on Computational Natural Language 
Learning (CONLL-2003), Edmonton, Canada. 
Tobias Scheffer, Christian Decomain, and Stefan 
Wrobel. 2001. Active Hidden Markov Models 
for Information Extraction. Proc. Int?l 
Symposium on Intelligent Data Analysis (IDA-
2001). 
Satoshi Sekine, Ralph Grishman and Hiroyuki 
Shinnou. 1998. A Decision Tree Method for 
Finding and Classifying Names in Japanese 
Texts.  Proc. Sixth Workshop on Very Large 
Corpora; Montreal, Canada. 
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou 
and Changning Huang. 2002. Chinese Named 
Entity Identification Using Class-based 
Language Model.  Coling 2002. 
Marc Vilain, John Burger, John Aberdeen, Dennis 
Connelly, Lynette Hirschman. 1995. A model --
Theoretic Coreference Scoring Scheme. MUC-6 
Proceedings, Nov. 1995. 
Shiren Ye, Tat-Seng Chua, Liu Jimin. 2002. An 
Agent-based Approach to Chinese Named 
Entity Recognition. Coling 2002. 
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48?55,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Data Selection in Semi-supervised Learning for Name Tagging 
Heng Ji Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
We present two semi-supervised learning 
techniques to improve a state-of-the-art 
multi-lingual name tagger. For English 
and Chinese, the overall system obtains 
1.7% - 2.1% improvement in F-measure, 
representing a 13.5% - 17.4% relative re-
duction in the spurious, missing, and in-
correct tags. We also conclude that 
simply relying upon large corpora is not 
in itself sufficient: we must pay attention 
to unlabeled data selection too. We de-
scribe effective measures to automatically 
select documents and sentences. 
1 Introduction 
When applying machine learning approaches to 
natural language processing tasks, it is time-
consuming and expensive to hand-label the large 
amounts of training data necessary for good per-
formance. Unlabeled data can be collected in 
much larger quantities. Therefore, a natural ques-
tion is whether we can use unlabeled data to 
build a more accurate learner, given the same 
amount of labeled data. This problem is often 
referred to as semi-supervised learning. It signifi-
cantly reduces the effort needed to develop a 
training set. It has shown promise in improving 
the performance of many tasks such as name tag-
ging (Miller et al, 2004), semantic class extrac-
tion (Lin et al, 2003), chunking (Ando and 
Zhang, 2005), coreference resolution (Bean and 
Riloff, 2004) and text classification (Blum and 
Mitchell, 1998).  
However, it is not clear, when semi-supervised 
learning is applied to improve a learner, how the 
system should effectively select unlabeled data, 
and how the size and relevance of data impact the 
performance. 
In this paper we apply two semi-supervised 
learning algorithms to improve a state-of-the-art 
name tagger. We run the baseline name tagger on 
a large unlabeled corpus (bootstrapping) and the 
test set (self-training), and automatically generate 
high-confidence machine-labeled sentences as 
additional ?training data?. We then iteratively re-
train the model on the increased ?training data?. 
We first investigated whether we can improve 
the system by simply using a lot of unlabeled 
data. By dramatically increasing the size of the 
corpus with unlabeled data, we did get a signifi-
cant improvement compared to the baseline sys-
tem. But we found that adding off-topic 
unlabeled data sometimes makes the performance 
worse. Then we tried to select relevant docu-
ments from the unlabeled data in advance, and 
got clear further improvements. We also obtained 
significant improvement by self-training (boot-
strapping on the test data) without any additional 
unlabeled data.  
Therefore, in contrast to the claim in (Banko 
and Brill, 2001), we concluded that, for some 
applications, effective use of large unlabeled cor-
pora demands good data selection measures. We 
propose and quantify some effective measures to 
select documents and sentences in this paper. 
The rest of this paper is structured as follows. 
Section 2 briefly describes the efforts made by 
previous researchers to use semi-supervised 
learning as well as the work of (Banko and Brill, 
2001).  Section 3 presents our baseline name tag-
ger. Section 4 describes the motivation for our 
approach while Section 5 presents the details of 
two semi-supervised learning methods. Section 6 
presents and discusses the experimental results 
on both English and Chinese. Section 7 presents 
our conclusions and directions for future work. 
2 Prior Work 
This work presented here extends a substantial 
body of previous work (Blum and Mitchell, 1998; 
Riloff and Jones, 1999; Ando and Zhang, 2005) 
48
that all focus on reducing annotation require-
ments. For the specific task of named entity an-
notation, some researchers have emphasized the 
creation of taggers from minimal seed sets 
(Strzalkowski and Wang, 1996; Collins and 
Singer, 1999; Lin et al, 2003) while another line 
of inquiry (which we are pursuing) has sought to 
improve on high-performance baseline taggers 
(Miller et al, 2004). 
Banko and Brill (2001) suggested that the de-
velopment of very large training corpora may be 
most effective for progress in empirical natural 
language processing. Their experiments show a 
logarithmic trend in performance as corpus size 
increases without performance reaching an upper 
bound. Recent work has replicated their work on 
thesaurus extraction (Curran and Moens, 2002) 
and is-a relation extraction (Ravichandran et al, 
2004), showing that collecting data over a very 
large corpus significantly improves system per-
formance. However, (Curran, 2002) and (Curran 
and Osborne, 2002) claimed that the choice of 
statistical model is more important than relying 
upon large corpora. 
3 Motivation 
The performance of name taggers has been lim-
ited in part by the amount of labeled training data 
available.  How can an unlabeled corpus help to 
address this problem?  Based on its original train-
ing (on the labeled corpus), there will be some 
tags (in the unlabeled corpus) that the tagger will 
be very sure about.  For example, there will be 
contexts that were always followed by a person 
name (e.g., "Capt.") in the training corpus.  If we 
find a new token T in this context in the unla-
beled corpus, we can be quite certain it is a per-
son name.  If the tagger can learn this fact about 
T, it can successfully tag T when it appears in the 
test corpus without any indicative context.  In the 
same way, if a previously-unseen context appears 
consistently in the unlabeled corpus before 
known person names, the tagger should learn that 
this is a predictive context. 
We have adopted a simple learning approach:  
we take the unlabeled text about which the tagger 
has greatest confidence in its decisions, tag it, 
add it to the training set, and retrain the tagger.  
This process is performed repeatedly to bootstrap 
ourselves to higher performance.  This approach 
can be used with any supervised-learning tagger 
that can produce some reliable measure of confi-
dence in its decisions. 
4 Baseline Multi-lingual Name Tagger 
Our baseline name tagger is based on an HMM 
that generally follows the Nymble model (Bikel 
et al 1997). Then it uses best-first search to gen-
erate NBest hypotheses, and also computes the 
margin ? the difference between the log prob-
abilities of the top two hypotheses.  This is used 
as a rough measure of confidence in our name 
tagging.1 
In processing Chinese, to take advantage of 
name structures, we do name structure parsing 
using an extended HMM which includes a larger 
number of states (14). This new HMM can han-
dle name prefixes and suffixes, and transliterated 
foreign names separately. We also augmented the 
HMM model with a set of post-processing rules 
to correct some omissions and systematic errors. 
The name tagger identifies three name types: 
Person (PER), Organization (ORG) and Geo-
political (GPE) entities (locations which are also 
political units, such as countries, counties, and 
cities).  
5 Two Semi-Supervised Learning Meth-
ods for Name Tagging 
We have applied this bootstrapping approach to 
two sources of data: first, to a large corpus of 
unlabeled data and second, to the test set.  To 
distinguish the two, we shall label the first "boot-
strapping" and the second "self-training". 
We begin (Sections 5.1 and 5.2) by describing 
the basic algorithms used for these two processes.  
We expected that these basic methods would 
provide a substantial performance boost, but our 
experiments showed that, for best gain, the addi-
tional training data should be related to the target 
problem, namely, our test set.  We present meas-
ures to select documents (Section 5.3) and sen-
tences (Section 5.4), and show (in Section 6) the 
effectiveness of these measures. 
5.1 Bootstrapping 
We divided the large unlabeled corpus into seg-
ments based on news sources and dates in order 
to: 1) create segments of manageable size; 2) 
separately evaluate the contribution of each seg-
ment (using a labeled development test set) and 
reject those which do not help; and 3) apply the 
latest updated best model to each subsequent 
                                                          
1 We have also used this metric in the context of rescoring of 
name hypotheses (Ji and Grishman, 2005); Scheffer et al 
(2001) used a similar metric for active learning of name tags. 
49
segment. The procedure can be formalized as 
follows. 
 
1. Select a related set RelatedC from a large cor-
pus of unlabeled data with respect to the test set 
TestT, using the document selection method de-
scribed in section 5.3. 
 
2. Split RelatedC into n subsets and mark them 
C1, C2?Cn. Call the updated HMM name tagger 
NameM (initially the baseline tagger), and a de-
velopment test set DevT.  
 
3. For i=1 to n 
(1)  Run NameM on Ci;  
 
    (2) For each tagged sentence S in Ci, if S is 
tagged with high confidence, then keep S; 
otherwise remove S; 
 
    (3) Relabel the current name tagger (NameM) 
as OldNameM, add Ci to the training data, 
and retrain the name tagger, producing an 
updated model NameM; 
 
    (4) Run NameM on DevT; if the performance 
gets worse, don?t use Ci and reset NameM 
= OldNameM; 
5.2 Self-training 
An analogous approach can be used to tag the 
test set. The basic intuition is that the sentences 
in which the learner has low confidence may get 
support from those sentences previously labeled 
with high confidence. 
Initially, we build the baseline name tagger 
from the labeled examples, then gradually add 
the most confidently tagged test sentences into 
the training corpus, and reuse them for the next 
iteration, until all sentences are labeled. The pro-
cedure can be formalized as follows. 
 
1. Cluster the test set TestT into n clusters T1, 
T2, ?,Tn, by collecting document pairs with low 
cross entropy (described in section 5.3.2) into the 
same cluster.  
 
2. For i=1 to n 
(1) NameM = baseline HMM name tagger; 
 
(2) While (there are new sentences tagged with 
confidence higher than a threshold) 
          a. Run NameM on Ti;  
          b. Set an appropriate threshold for margin; 
c. For each tagged sentence S in Ti, if S is 
tagged with high confidence, add S to the 
training data; 
d. Retrain the name tagger NameM with 
augmented training data. 
 
At each iteration, we lower the threshold so 
that about 5% of the sentences (with the largest 
margin) are added to the training corpus.2  As an 
example, this yielded the following gradually 
improving performance for one English cluster 
including 7 documents and 190 sentences. 
 
No. of  
iterations
No. of  
sentences 
added 
No. of 
tags 
changed 
F-Measure
0 0 0 91.4 
1 37 28 91.9 
2 69 22 92.1 
3 107 21 92.4 
4 128 11 92.6 
5 146 9 92.7 
6 163 8 92.8 
7 178 6 92.8 
8 190 0 92.8 
 
Table 1. Incremental Improvement from  
Self-training (English) 
 
Self-training can be considered a cache model 
variant, operating across the entire test collection.  
But it uses confidence measures as weights for 
each name candidate, and relies on names tagged 
with high confidence to re-adjust the prediction 
of the remaining names, while in a cache model, 
all name candidates are equally weighted for vot-
ing (independent of the learner?s confidence). 
5.3 Unlabeled Document Selection 
To further investigate the benefits of using very 
large corpora in bootstrapping, and also inspired 
by the gain from the ?essence? of self-training, 
which aims to gradually emphasize the predic-
tions from related sentences within the test set, 
we reconsidered the assumptions of our approach. 
The bootstrapping method implicitly assumes 
that the unlabeled data is reliable (not noisy) and 
uniformly useful, namely: 
 
                                                          
2 To be precise, we repeatedly reduce the threshold by 0.1 
until an additional 5% or more of the sentences are included;  
however, if more than an additional 20% of the sentences 
are captured because many sentences have the same margin, 
we add back 0.1 to the threshold. 
50
? The unlabeled data supports the acquisition 
of new names and contexts, to provide new 
evidence to be incorporated in HMM and re-
duce the sparse data problem; 
? The unlabeled data won?t make the old esti-
mates worse by adding too many names 
whose tags are incorrect, or at least are incor-
rect in the context of the labeled training data 
and the test data. 
 
If the unlabeled data is noisy or unrelated to 
the test data, it can hurt rather than improve the 
learner?s performance on the test set.  So it is 
necessary to coarsely measure the relevance of 
the unlabeled data to our target test set. We de-
fine an IR (information retrieval) - style rele-
vance measure between the test set TestT and an 
unlabeled document d as follows. 
5.3.1 ?Query set? construction 
We model the information expected from the 
unlabeled data by a 'bag of words' technique.  We 
construct a query term set from the test corpus 
TestT to check whether each unlabeled document 
d is useful or not. 
 
? We prefer not to use all the words in TestT  
as key words, since we are only concerned 
about the distribution of name candidates. 
(Adding off-topic documents may in fact in-
troduce noise into the model). For example, 
if one document in TestT talks about the 
presidential election in France while d talks 
about the presidential election in the US, they 
may share many common words such as 
'election', ?voting?, 'poll', and ?camp?, but we 
would expect more gain from other unlabeled 
documents talking about the French election, 
since they may share many name candidates. 
 
? On the other hand it is insufficient to only 
take the name candidates in the top one hy-
pothesis for each sentence (since we are par-
ticularly concerned with tokens which might 
be names but are not so labeled in the top 
hypothesis).  
 
So our solution is to take all the name candi-
dates in the top N best hypotheses for each sen-
tence to construct a query set Q. 
5.3.2 Cross-entropy Measure 
Using Q, we compute the cross entropy H(TestT, 
d) between TestT and d by: 
?
?
??=
Qx
dxprobTestTxprobdTestTH )|(log)|(),( 2
 
 where x is a name candidate in Q, and 
prob(x|TestT) is the probability (frequency) of x 
appearing in TestT while prob(x|d) is the prob-
ability of x in d. If H(T, d) is smaller than a 
threshold then we consider d a useful unlabeled 
document3. 
5.4 Sentence Selection 
We don?t want to add all the tagged sentences in 
a relevant document to the training corpus be-
cause incorrectly tagged or irrelevant sentences 
can lead to degradation in model performance. 
The value of larger corpora is partly dependent 
on how much new information is extracted from 
each sentence of the unlabeled data compared to 
the training corpus that we already have.  
The following confidence measures were ap-
plied to assist the semi-supervised learning algo-
rithm in selecting useful sentences for re-training 
the model.  
5.4.1 Margin to find reliable sentences  
For each sentence, we compute the HMM hy-
pothesis margin (the difference in log probabili-
ties) between the first hypothesis and the second 
hypothesis. We select the sentences with margins 
larger than a threshold4 to be added to the train-
ing data. 
Unfortunately, the margin often comes down 
to whether a specific word has previously been 
observed in training; if the system has seen the 
word, it is certain, if not, it is uncertain. There-
fore the sentences with high margins are a mix of 
interesting and uninteresting samples. We need to 
apply additional measures to remove the uninter-
esting ones.  On the other hand, we may have 
confidence in a tagging due to evidence external 
to the HMM, so we explored measures beyond 
the HMM margin in order to recover additional 
sentences. 
 
 
                                                          
3 We also tried a single match method, using the query set to 
find all the relevant documents that include any names be-
longing to Q, and got approximately the same result as 
cross-entropy. In addition to this relevance selection, we 
used one other simple filter: we removed a document if it 
includes fewer than five names, because it is unlikely to be 
news. 
4 In bootstrapping, this margin threshold is selected by test-
ing on the development set, to achieve more than 93% F-
Measure. 
51
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Bootstrapping for Name Tagging 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
Figure 2.  Self-Training for Name Tagging 
 
 
Data English Chinese 
Baseline 
Training data 
ACE02,03,04 989,003 words Beijing Corpus +ACE03,04,05 
1,460,648 words 
Total 196,494 docs in Mar-Jun of 2003  
(69M words) from ACE05 unlabeled data
41061 docs in Nov,Dec of 2000, and Jan 
of 2001 (25M words) from ACE05 and 
TDT4 transcripts 
Selected 
Docs 
62584 docs (1,314,148 Sentences) 14,537 docs (222,359 sentences) 
Unlabeled 
Data 
Selected 
Sentences 
290,973 sentences (6,049,378 words) 55,385 sentences (1,128,505 words) 
Dev Set 20 ACE04 texts in Oct of 2000 90 ACE05 texts in Oct of 2000 
Test Set 20 ACE04 texts in Oct of 2000 
and 80 ACE05 texts in Mar-May of 2003 
(3093 names, 1205 PERs, 1021GPEs, 867 
ORGs) 
90 ACE05 texts in Oct of 2000 
(3093 names, 1013 PERs, 695 GPEs, 769 
ORGs) 
 
Table 2. Data Description 
 
C1 Ci ? ? 
Unlabeled Data 
Cross-entropy based Document Selection
i=i+1
Save Ti? as  
system output 
T1 Ti ? ? 
Test Set 
Cross-entropy based Document Clustering
Ti?? Ti tagged 
with NameM
Yes 
Add Ti? to training corpus 
Retrain NameM 
Ti? Empty?
Ti?? sentences 
selected from Ti?
No 
NameM ? baseline tagger 
i?1
i < n?Yes 
i?1 
NameM performs 
better on dev set?
Yes 
No 
OldNameM ? NameM 
Ci??Ci tagged with NameM 
Add Ci? to training corpus 
Retrain NameM
i=i+1
i < n?
Ci?? sentences selected from Ci? 
Yes 
NameM ? baseline tagger 
Cn Tn
NameM ? OldNameM 
Set margin threshold 
52
5.4.2 Name coreference to find more reliable 
sentences 
Names introduced in an article are likely to be 
referred to again, so a name coreferred to by 
more other names is more likely to have been 
correctly tagged. In this paper, we use simple 
coreference resolution between names such as 
substring matching and name abbreviation reso-
lution.  
In the bootstrapping method we apply single-
document coreference for each individual unla-
beled text. In self-training, in order to further 
benefit from global contexts, we consider each 
cluster of relevant texts as one single big docu-
ment, and then apply cross-document coreference. 
Assume S is one sentence in the document, and 
there are k names tagged in S: {N1, N2 .?.. Nk}, 
which are coreferred to by {CorefNum1, Coref-
Num2, ?CorefNumk} other names separately. 
Then we use the following average name 
coreference count AveCoref as a confidence 
measure for tagging S:5 
?
=
=
k
i
i kCorefNumAveCoref
1
/)(  
5.4.3 Name count and sentence length to re-
move uninteresting sentences 
In bootstrapping on unlabeled data, the margin 
criterion often selects some sentences which are 
too short or don?t include any names. Although 
they are tagged with high confidence, they may 
make the model worse if added into the training 
data (for example, by artificially increasing the 
probability of non-names). In our experiments we 
don?t use a sentence if it includes fewer than six 
words, or doesn?t include any names. 
5.5 Data Flow 
We depict the above two semi-supervised learn-
ing methods in Figure 1 and Figure 2. 
6 Evaluation Results and Discussions 
6.1 Data 
We evaluated our system on two languages: En- 
glish and Chinese. Table 2 shows the data used in 
our experiments. 
                                                          
5 For the experiments reported here, sentences were selected 
if AveCoref > 3.1 (or 3.1?number of documents for cross-
document coreference) or the sentence margin exceeded the 
margin threshold. 
We present in section 6.2 ? 6.4 the overall per-
formance of precision (P), recall (R) and F-
measure (F) for both languages, and also some 
diagnostic experiment results. For significance 
testing (using the sign test), we split the test set 
into 5 folders, 20 texts in each folder of English, 
and 18 texts in each folder of Chinese. 
6.2 Overall Performance 
Table 3 and Table 4 present the overall perform-
ance6 by applying the two semi-supervised learn-
ing methods, separately and in combination, to 
our baseline name tagger. 
 
Learner P R F 
Baseline 87.3 87.6 87.4
Bootstrapping  
with data selection 
88.2 88.6 88.4
Self-training 88.1 88.4 88.2
Bootstrapping with data 
selection + Self-training 
 
89.0 
 
89.2 
 
89.1
 
Table 3. English Name Tagger 
 
Learner P R F 
Baseline 88.2 87.6 87.9
Bootstrapping  
with data selection 
89.8 89.5 89.6
Self-training 89.5 88.3 88.9
Bootstrapping with data 
selection + Self-training 
 
90.2 
 
89.7 
 
90.0
 
Table 4. Chinese Name Tagger 
 
For English, the overall system achieves a 
13.4% relative reduction on the spurious and in-
correct tags, and 12.9% reduction in the missing 
rate. For Chinese, it achieves a 16.9% relative 
reduction on the spurious and incorrect tags, and 
16.9% reduction in the missing rate.7 For each of 
the five folders, we found that both bootstrapping 
and self-training produced an improvement in F 
score for each folder, and the combination of two 
methods is always better than each method alone. 
This allows us to reject the hypothesis that these 
                                                          
6 Only names which exactly match the key in both extent 
and type are counted as correct; unlike MUC scoring, no 
partial credit is given. 
7 The performance achieved should be considered in light of 
human performance on this task.  The ACE keys used for 
the evaluations were obtained by dual annotation and adju-
dication.  A single annotator, evaluated against the key, 
scored F=93.6% to 94.1% for English and 92.5% to 92.7% 
for Chinese.  A second key, created independently by dual 
annotation and adjudication for a small amount of the Eng-
lish data, scored F=96.5% against the original key. 
53
improvements were random at a 95% confidence 
level. 
6.3 Analysis of Bootstrapping 
6.3.1 Impact of Data Size 
Figure 3 and 4 below show the results as each 
segment of the unlabeled data is added to the 
training corpus.  
 
 
Figure 3. Impact of Data Size (English) 
 
 
 
Figure 4. Impact of Data Size (Chinese) 
 
We can see some flattening of the gain at the 
end, particularly for the larger English corpus, 
and that some segments do not help to boost the 
performance (reflected as dips in the Dev Set 
curve and gaps in the Test Set curve). 
6.3.2 Impact of Data Selection 
In order to investigate the contribution of docu-
ment selection in bootstrapping, we performed 
diagnostic experiments for Chinese, whose re-
sults are shown in Table 5. All the bootstrapping 
tests (rows 2 - 4) use margin for sentence selec-
tion; row 4 augments this with the selection 
methods described in sections 5.4.2 and 5.4.3. 
Learner P R F 
(1) Baseline 88.2 87.6 87.9
(2) (1) + Bootstrapping 88.9 88.7 88.8
(3) (2) + Document  
Selection 
89.3 88.9 89.1
(4) (3) + Sentence  
Selection 
89.8  89.5 89.6
 
Table 5. Impact of Data Selection (Chinese) 
 
Comparing row 2 with row 3, we find that not 
using document selection, even though it multi-
plies the size of the corpus, results in 0.3% lower 
performance (0.3-0.4% loss for each folder). This 
leads us to conclude that simply relying upon 
large corpora is not in itself sufficient. Effective 
use of large corpora demands good confidence 
measures for document selection to remove off-
topic material. By adding sentence selection (re-
sults in row 4) the system obtained 0.5% further 
improvement in F-Measure (0.4-0.7% for each 
folder). All improvements are statistically sig-
nificant at the 95% confidence level. 
6.4 Analysis of Self-training  
We have applied and evaluated different meas-
ures to extract high-confidence sentences in self-
training. The contributions of these confidence 
measures to F-Measure are presented in Table 6. 
 
Confidence Measure English Chinese
Baseline 87.4 87.9 
Margin 87.8 88.3 
Margin + single-doc  
name coreference 
88.0 88.7 
Margin + cross-doc  
name coreference 
88.2 88.9 
 
Table 6. Impact of Confidence Measures 
 
It shows that Chinese benefits more from add-
ing name coreference, mainly because there are 
more coreference links between name abbrevia-
tions and full names. And we also can see that 
the margin is an important measure for both lan-
guages. All differences are statistically signifi-
cant at the 95% confidence level except for the 
gain using cross-document information for the 
Chinese name tagging. 
7 Conclusions and Future Work 
This paper demonstrates the effectiveness of two 
straightforward semi-supervised learning meth-
ods for improving a state-of-art name tagger, and 
54
investigates the importance of data selection for 
this application. 
Banko and Brill (2001) suggested that the de-
velopment of very large training corpora may be 
central to progress in empirical natural language 
processing. When using large amounts of unla-
beled data, as expected, we did get improvement 
by using unsupervised bootstrapping. However, 
exploiting a very large corpus did not by itself 
produce the greatest performance gain. Rather, 
we observed that good measures to select rele-
vant unlabeled documents and useful labeled sen-
tences are important.  
The work described here complements the ac-
tive learning research described by (Scheffer et 
al., 2001). They presented an effective active 
learning approach that selects ?difficult? (small 
margin) sentences to label by hand and then add 
to the training set. Our approach selects ?easy? 
sentences ? those with large margins ? to add 
automatically to the training set. Combining 
these methods can magnify the gains possible 
with active learning. 
In the future we plan to try topic identification 
techniques to select relevant unlabeled docu-
ments, and use the downstream information ex-
traction components such as coreference 
resolution and relation detection to measure the 
confidence of the tagging for sentences. We are 
also interested in applying clustering as a pre-
processing step for bootstrapping. 
Acknowledgment 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023, and the 
National Science Foundation under Grant IIS-
00325657.  Any opinions, findings and conclu-
sions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of the U. S. Government. 
References  
Rie Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Methods 
for Text Chunking. Proc.  ACL2005. pp. 1-8. Ann 
Arbor, USA 
Michele Banko and Eric Brill. 2001. Scaling to very 
very large corpora for natural language disam-
biguation. Proc.  ACL2001. pp. 26-33. Toulouse, 
France 
David Bean and Ellen Riloff. 2004. Unsupervised 
Learning of Contextual Role Knowledge for 
Coreference Resolution. Proc.  HLT-NAACL2004. 
pp. 297-304. Boston, USA 
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder. Proc. Fifth 
Conf. on Applied Natural Language Processing. 
pp.194-201. Washington D.C., USA 
Avrim Blum and Tom Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training. Proc. 
of the Workshop on Computational Learning The-
ory. Morgan Kaufmann Publishers 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. Proc. 
of EMNLP/VLC-99. 
James R. Curran and Marc Moens. 2002. Scaling con-
text space. Proc. ACL 2002. Philadelphia, USA 
James R. Curran. 2002. Ensemble Methods for Auto-
matic Thesaurus Extraction. Proc. EMNLP 2002. 
Philadelphia, USA 
James R. Curran and Miles Osborne. 2002. A very 
very large corpus doesn?t always yield reliable es-
timates. Proc. ACL 2002 Workshop on Effective 
Tools and Methodologies for Teaching Natural 
Language Processing and Computational Linguis-
tics. Philadelphia, USA 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Winston Lin, Roman Yangarber and Ralph Grishman. 
2003. Bootstrapping Learning of Semantic Classes 
from Positive and Negative Examples. Proc.  
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data. Washington, D.C. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and Dis-
criminative Training. Proc. HLT-NAACL2004. pp. 
337-342. Boston, USA 
Deepak Ravichandran, Patrick Pantel, and Eduard 
Hovy. 2004. The Terascale Challenge. Proc. KDD 
Workshop on Mining for and from the Semantic 
Web (MSW-04). pp. 1-11. Seattle, WA, USA 
Ellen Riloff and Rosie Jones. 1999. Learning Diction-
aries for Information Extraction by Multi-Level 
Bootstrapping. Proc. AAAI/IAAI 
Tobias Scheffer, Christian Decomain, and Stefan 
Wrobel. 2001. Active Hidden Markov Models for 
Information Extraction. Proc. Int?l Symposium on 
Intelligent Data Analysis (IDA-2001). 
Tomek Strzalkowski and Jin Wang. 1996. A Self-
Learning Universal Concept Spotter. Proc. 
COLING. 
55
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49?56,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Re-Ranking Algorithms for Name Tagging 
 
Heng Ji Cynthia Rudin Ralph Grishman 
Dept. of Computer Science Center for Neural Science and Courant 
Institute of Mathematical Sciences 
Dept. of Computer Science 
New York University 
New York, N.Y. 10003 
hengji@cs.nyu.edu rudin@nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Integrating information from different 
stages of an NLP processing pipeline can 
yield significant error reduction. We dem-
onstrate how re-ranking can improve name 
tagging in a Chinese information extrac-
tion system by incorporating information 
from relation extraction, event extraction, 
and coreference. We evaluate three state-
of-the-art re-ranking algorithms (MaxEnt-
Rank, SVMRank, and p-Norm Push Rank-
ing), and show the benefit of multi-stage 
re-ranking for cross-sentence and cross-
document inference. 
1 Introduction 
In recent years, re-ranking techniques have been 
successfully applied to enhance the performance 
of NLP analysis components based on generative 
models. A baseline generative model produces N-
best candidates, which are then re-ranked using a 
rich set of local and global features in order to 
select the best analysis. Various supervised learn-
ing algorithms have been adapted to the task of re-
ranking for NLP systems, such as MaxEnt-Rank 
(Charniak and Johnson, 2005; Ji and Grishman, 
2005), SVMRank (Shen and Joshi, 2003), Voted 
Perceptron (Collins, 2002; Collins and Duffy, 
2002; Shen and Joshi, 2004), Kernel Based Meth-
ods (Henderson and Titov, 2005), and RankBoost 
(Collins, 2002; Collins and Koo, 2003; Kudo et al, 
2005). 
These algorithms have been used primarily 
within the context of a single NLP analysis com-
ponent, with the most intensive study devoted to 
improving parsing performance. The re-ranking 
models for parsing, for example, normally rely on 
structures generated within the baseline parser 
itself. Achieving really high performance for some 
analysis components, however, requires that we 
take a broader view, one that looks outside a sin-
gle component in order to bring to bear knowl-
edge from the entire NL analysis process.  In this 
paper we will demonstrate the potential of this 
approach in enhancing the performance of Chi-
nese name tagging within an information extrac-
tion application.  
Combining information from other stages in the 
analysis pipeline allows us to incorporate informa-
tion from a much wider context, spanning the en-
tire document and even going across documents.  
This will give rise to new design issues; we will 
examine and compare different re-ranking algo-
rithms when applied to this task.  
We shall first describe the general setting and 
the special characteristics of re-ranking for name 
tagging. Then we present and evaluate three re-
ranking algorithms ? MaxEnt-Rank, SVMRank 
and a new algorithm, p-Norm Push Ranking ? for 
this problem, and show how an approach based on 
multi-stage re-ranking can effectively handle fea-
tures across sentence and document boundaries. 
2 Prior Work 
2.1 Ranking 
We will describe the three state-of-the-art super-
vised ranking techniques considered in this work. 
Later we shall apply and evaluate these algorithms 
for re-ranking in the context of name tagging. 
Maximum Entropy modeling (MaxEnt) has 
been extremely successful for many NLP classifi-
49
cation tasks, so it is natural to apply it to re-
ranking problems. (Charniak and Johnson, 2005) 
applied MaxEnt to improve the performance of a 
state-of-art parser; also in (Ji and Grishman, 2005) 
we used it to improve a Chinese name tagger.  
Using SVMRank, (Shen and Joshi, 2003) 
achieved significant improvement on parse re-
ranking. They compared two different sample 
creation methods, and presented an efficient train-
ing method by separating the training samples into 
subsets.  
The last approach we consider is a boosting-
style approach. We implement a new algorithm 
called p-Norm Push Ranking (Rudin, 2006). This 
algorithm is a generalization of RankBoost 
(Freund et al 1998) which concentrates specifi-
cally on the top portion of a ranked list. The pa-
rameter ?p? determines how much the algorithm 
concentrates at the top.  
2.2 Enhancing Named Entity Taggers 
There have been a very large number of NE tagger 
implementations since this task was introduced at 
MUC-6 (Grishman and Sundheim, 1996).  Most 
implementations use local features and a unifying 
learning algorithm based on, e.g., an HMM, Max-
Ent, or SVM. Collins (2002) augmented a baseline 
NE tagger with a re-ranker that used only local, 
NE-oriented features.  Roth and Yih (2002) com-
bined NE and semantic relation tagging, but 
within a quite different framework (using a linear 
programming model for joint inference). 
3 A Framework for Name Re-Ranking 
3.1 The Information Extraction Pipeline 
The extraction task we are addressing is that of the 
Automatic Content Extraction (ACE)1 evaluations. 
The 2005 ACE evaluation had 7 types of entities, 
of which the most common were PER (persons), 
ORG (organizations), LOC (natural locations) and 
GPE (?geo-political entities? ? locations which are 
also political units, such as countries, counties, 
and cities).  There were 6 types of semantic rela-
tions, with 18 subtypes.  Examples of these rela-
tions are ?the CEO of Microsoft? (an 
organization-affiliation relation), ?Fred?s wife? (a 
                                                          
1 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/ 
personal-social relation), and ?a military base in 
Germany? (a located relation). And there were 8 
types of events, with 33 subtypes, such as ?Kurt 
Schork died in Sierra Leone yesterday? (a Die 
event), and ?Schweitzer founded a hospital in 
1913? (a Start-Org event). 
  To extract these elements we have developed a 
Chinese information extraction pipeline that con-
sists of the following stages: 
? Name tagging and name structure parsing 
(which identifies the internal structure of some 
names); 
? Coreference resolution, which links "men-
tions" (referring phrases of selected semantic 
types) into "entities": this stage is a combina-
tion of high-precision heuristic rules and 
maximum entropy models; 
? Relation tagging, using a K-nearest-neighbor 
algorithm to identify relation types and sub-
types; 
? Event patterns, semi-automatically extracted 
from ACE training corpora. 
3.2 Hypothesis Representation and Genera-
tion 
Again, the central idea is to apply the baseline 
name tagger to generate N-Best multiple hypothe-
ses for each sentence; the results from subsequent 
components are then exploited to re-rank these 
hypotheses and the new top hypothesis is output 
as the final result. 
In our name re-ranking model, each hypothesis 
is an NE tagging of the entire sentence. For ex-
ample, ?<PER>John</PER> was born in 
<GPE>New York</GPE>.? is one hypothesis 
for the sentence ?John was born in New York?. 
We apply a HMM tagger to identify four named 
entity types: Person, GPE, Organization and Loca-
tion. The HMM tagger generally follows the 
Nymble model (Bikel et al 1997), and uses best-
first search to generate N-Best hypotheses. It also 
computes the ?margin?, which is the difference 
between the log probabilities of the top two hy-
potheses.  This is used as a rough measure of con-
fidence in the top hypothesis. A large margin 
indicates greater confidence that the first hypothe-
sis is correct. The margin also determines the 
number of hypotheses (N) that we will store. Us-
ing cross-validation on the training data, we de-
termine the value of N required to include the best 
50
hypothesis, as a function of the margin.  We then 
divide the margin into ranges of values, and set a 
value of N for each range, with a maximum of 30. 
To obtain the training data for the re-ranking 
algorithm, we separate the name tagging training 
corpus into k folders, and train the HMM name 
tagger on k-1 folders. We then use the HMM to 
generate N-Best hypotheses H = {h1, h2,?,hN} for 
each sentence in the remaining folder.  Each hi in 
H is then paired with its NE F-measure, measured 
against the key in the annotated corpus. 
We define a ?crucial pair? as a pair of hypothe-
ses such that, according to F-Measure, the first 
hypothesis in the pair should be more highly 
ranked than the second. That is, if for a sentence, 
the F-Measure of hypothesis hi is larger than that 
of hj, then (hi, hj) is a crucial pair. 
3.3 Re-Ranking Functions 
We investigated the following three different for-
mulations of the re-ranking problem: 
? Direct Re-Ranking by Score 
For each hypothesis hi, we attempt to learn a scor-
ing function f : H ? R, such that f(hi) > f(hj) if the 
F-Measure of hi is higher than the F-measure of hj. 
? Direct Re-Ranking by Classification 
For each hypothesis hi, we attempt to learn f : H 
? {-1, 1}, such that f(hi) = 1 if hi has the top F-
Measure among H; otherwise f(hi) = -1. This can 
be considered a special case of re-ranking by 
score. 
? Indirect Re-Ranking Function 
For each ?crucial? pair of hypotheses (hi, hj), we 
learn f : H ? H ? {-1, 1}, such that f(hi, hj) = 1 if 
hi is better than hj; f (hi, hj) = -1 if hi is worse than 
hj. We call this ?indirect? ranking because we 
need to apply an additional decoding step to pick 
the best hypothesis from these pair-wise compari-
son results. 
4 Features for Re-Ranking 
4.1 Inferences From Subsequent Stages 
Information extraction is a potentially symbiotic 
pipeline with strong dependencies between stages 
(Roth and Yih, 2002&2004; Ji and Grishman, 
2005). Thus, we use features based on the output 
of four subsequent stages ? name structure parsing, 
relation extraction, event patterns, and coreference 
analysis ? to seek the best hypothesis.  
We included ten features based on name struc-
ture parsing to capture the local information 
missed by the baseline name tagger such as details 
of the structure of Chinese person names. 
The relation and event re-ranking features are 
based on matching patterns of words or constitu-
ents.  They serve to correct name boundary errors 
(because such errors would prevent some patterns 
from matching).  They also exert selectional pref-
erences on their arguments, and so serve to correct 
name type errors.  For each relation argument, we 
included a feature whose value is the likelihood 
that relation appears with an argument of that se-
mantic type (these probabilities are obtained from 
the training corpus and binned).  For each event 
pattern, a feature records whether the types of the 
arguments match those required by the pattern. 
Coreference can link multiple mentions of 
names provided they have the same spelling 
(though if a name has several parts, some may be 
dropped) and same semantic type. So if the 
boundary or type of one mention can be deter-
mined with some confidence, coreference can be 
used to disambiguate other mentions, by favoring 
hypotheses which support more coreference. To 
this end, we incorporate several features based on 
coreference, such as the number of mentions re-
ferring to a name candidate.  
Each of these features is defined for individual 
name candidates; the value of the feature for a 
hypothesis is the sum of its values over all names 
in the hypothesis. The complete set of detailed 
features is listed in (Ji and Grishman, 2006). 
4.2 Handling Cross-Sentence Features by 
Multi-Stage Re-Ranking 
Coreference is potentially a powerful contributor 
for enhancing NE recognition, because it provides 
information from other sentences and even docu-
ments, and it applies to all sentences that include 
names. For a name candidate, 62% of its corefer-
ence relations span sentence boundaries.  How-
ever, this breadth poses a problem because it 
means that the score of a hypothesis for a given 
51
sentence may depend on the tags assigned to the 
same names in other sentences.2 
Ideally, when we re-rank the hypotheses for one 
sentence S, the other sentences that include men-
tions of the same name should already have been 
re-ranked, but this is not possible because of the 
mutual dependence. Repeated re-ranking of a sen-
tence would be time-consuming, so we have 
adopted an alternative approach. Instead of incor-
porating coreference evidence with all other in-
formation in one re-ranker, we apply two re-
rankers in succession.  
   In the first re-ranking step, we generate new 
rankings for all sentences based on name structure, 
relation and event features, which are all sentence-
internal evidence.  Then in a second pass, we ap-
ply a re-ranker based on coreference between the 
names in each hypothesis of sentence S and the 
mentions in the top-ranking hypothesis (from the 
first re-ranker) of all other sentences.3  In this way, 
the coreference re-ranker can propagate globally 
(across sentences and documents) high-confidence 
decisions based on the other evidence. In our final 
MaxEnt Ranker we obtained a small additional 
gain by further splitting the first re-ranker into 
three separate steps: a name structure based re-
ranker, a relation based re-ranker and an event 
based re-ranker; these were incorporated in an 
incremental structure.   
4.3 Adding Cross-Document Information 
The idea in coreference is to link a name mention 
whose tag is locally ambiguous to another men-
tion that is unambiguously tagged based on local 
evidence.  The wider a net we can cast, the greater 
the chance of success.  To cast the widest net pos-
sible, we have used cross-document coreference 
for the test set. We cluster the documents using a 
cross-entropy metric and then treat the entire clus-
ter as a single document.      
We take all the name candidates in the top N 
hypotheses for each sentence in each cluster T to 
construct a ?query set? Q. The metric used for the 
clustering is the cross entropy H(T, d) between the 
distribution of the name candidates in T and 
                                                          
2 For in-document coreference, this problem could be avoided if the tagging of 
an entire document constituted a hypothesis, but that would be impractical ? a 
very large N would be required to capture sufficient alternative taggings in an 
N-best framework. 
3 This second pass is skipped for sentences for which the confidence in the top 
hypothesis produced by the first re-ranker is above a threshold. 
document d. If H(T, d) is smaller than a threshold 
then we add d to T. H(T, d) is defined by: 
?
?
??=
Qx
xdprobxTprobdTH ),(log),(),( . 
We built these clusters two ways: first, just 
clustering the test documents; second, by aug-
menting these clusters with related documents 
retrieved from a large unlabeled corpus (with 
document relevance measured using cross-
entropy). 
5 Re-Ranking Algorithms 
We have been focusing on selecting appropriate 
ranking algorithms to fit our application. We 
choose three state-of-the-art ranking algorithms 
that have good generalization ability. We now 
describe these algorithms. 
5.1 MaxEnt-Rank 
5.1.1  Sampling and Pruning 
 
Maximum Entropy models are useful for the task 
of ranking because they compute a reliable rank-
ing probability for each hypothesis.  We have tried 
two different sampling methods ? single sampling 
and pairwise sampling.  
The first approach is to use each single hy-
pothesis hi as a sample. Only the best hypothesis 
of each sentence is regarded as a positive sample; 
all the rest are regarded as negative samples. In 
general, absolute values of features are not good 
indicators of whether a hypothesis will be the best 
hypothesis for a sentence; for example, a co-
referring mention count of 7 may be excellent for 
one sentence and poor for another.  Consequently, 
in this single-hypothesis-sampling approach, we 
convert each feature to a Boolean value, which is 
true if the original feature takes on its maximum 
value (among all hypotheses) for this hypothesis.  
This does, however, lose some of the detail about 
the differences between hypotheses. 
In pairwise sampling we used each pair of hy-
potheses (hi, hj) as a sample. The value of a fea-
ture for a sample is the difference between its 
values for the two hypotheses.  However, consid-
ering all pairs causes the number of samples to 
grow quadratically (O(N2)) with the number of 
hypotheses, compared to the linear growth with 
best/non-best sampling. To make the training and 
52
test procedures more efficient, we prune the data 
in several ways.  
We perform pruning by beam setting, removing 
candidate hypotheses that possess very low prob-
abilities from the HMM, and during training we 
discard the hypotheses with very low F-measure 
scores. Additionally, we incorporate the pruning 
techniques used in (Chiang 2005), by which any 
hypothesis with a probability lower than?times 
the highest probability for one sentence is dis-
carded. We also discard the pairs very close in 
performance or probability. 
 
5.1.2 Decoding 
 
If f is the ranking function, the MaxEnt model 
produces a probability for each un-pruned ?cru-
cial? pair: prob(f(hi, hj) = 1), i.e., the probability 
that for the given sentence, hi is a better hypothe-
sis than hj. We need an additional decoding step to 
select the best hypothesis. Inspired by the caching 
idea and the multi-class solution proposed by 
(Platt et al 2000), we use a dynamic decoding 
algorithm with complexity O(n) as follows. 
We scale the probability values into three types: 
CompareResult (hi, hj) = ?better? if prob(f(hi, hj) = 
1) >?1, ?worse? if prob(f(hi, hj) = 1) <?2, and 
?unsure? otherwise, where ?1??2. 4  
 
Prune 
for i = 1 to n 
Num = 0; 
for j = 1 to n and j?i 
If CompareResult(hi, hj) = ?worse? 
Num++; 
    if Num>?then discard hi from H 
 
Select 
Initialize: i = 1, j = n 
while (i<j) 
if CompareResult(hi, hj) = ?better? 
discard hj from H; 
j--; 
else if CompareResult(hi, hj) = ?worse? 
discard hi from H; 
i++; 
else break; 
 
                                                          
4 In the final stage re-ranker we use?1=?2 so that we don?t generate the 
output of ?unsure?, and one hypothesis is finally selected. 
Output 
If the number of remaining hypotheses in H is 1, 
then output it as the best hypothesis; else propa-
gate all hypothesis pairs into the next re-ranker. 
5.2 SVMRank 
We implemented an SVM-based model, which 
can theoretically achieve very low generalization 
error. We use the SVMLight package (Joachims, 
1998), with the pairwise sampling scheme as for 
MaxEnt-Rank. In addition we made the following 
adaptations: we calibrated the SVM outputs, and 
separated the data into subsets. 
To speed up training, we divided our training 
samples into k subsets. Each subset contains N(N-
1)/k pairs of hypotheses of each sentence.  
In order to combine the results from these dif-
ferent SVMs, we must calibrate the function val-
ues; the output of an SVM yields a distance to the 
separating hyperplane, but not a probability. We 
have applied the method described in (Shen and 
Joshi, 2003), to map SVM?s results to probabili-
ties via a sigmoid. Thus from the kth SVM, we get 
the probability for each pair of hypotheses: 
)1),(( =jik hhfprob , 
namely the probability of hi being better than hj. 
Then combining all k SVMs? results we get: 
       ? ==
k
jikji hhfprobhhZ )1),((),( . 
So the hypothesis hi with maximal value is cho-
sen as the top hypothesis:  
?
j
ji
h
hhZ
i
)),((maxarg . 
5.3 P-Norm Push Ranking 
The third algorithm we have tried is a general 
boosting-style supervised ranking algorithm called 
p-Norm Push Ranking (Rudin, 2006). We de-
scribe this algorithm in more detail since it is quite 
new and we do not expect many readers to be fa-
miliar with it.  
The parameter ?p? determines how much em-
phasis (or ?push?) is placed closer to the top of the 
ranked list, where p?1. The p-Norm Push Ranking 
algorithm generalizes RankBoost (take p=1 for 
RankBoost). When p is set at a large value, the 
rankings at the top of the list are given higher pri-
ority (a large ?push?), at the expense of possibly 
making misranks towards the bottom of the list. 
53
Since for our application, we do not care about the 
rankings at the bottom of the list (i.e., we do not 
care about the exact rank ordering of the bad hy-
potheses), this algorithm is suitable for our prob-
lem. There is a tradeoff for the choice of p; larger 
p yields more accurate results at the very top of 
the list for the training data. If we want to consider 
more than simply the very top of the list, we may 
desire a smaller value of p. Note that larger values 
of p also require more training data in order to 
maintain generalization ability (as shown both by 
theoretical generalization bounds and experi-
ments). If we want large p, we must aim to choose 
the largest value of p that allows generalization, 
given our amount of training data. When we are 
working on the first stage of re-ranking, we con-
sider the whole top portion of the ranked list, be-
cause we use the rank in the list as a feature for 
the next stage. Thus, we have chosen the value 
p1=4 (a small ?push?) for the first re-ranker. For 
the second re-ranker we choose p2=16 (a large 
?push?). 
The objective of the p-Norm Push Ranking al-
gorithm is to create a scoring function f: H?R 
such that for each crucial pair (hi, hj), we shall 
have f(hi) > f(hj). The form of the scoring function 
is f(hi) = ??kgk(hi), where gk is called a weak 
ranker: gk : H ? [0,1]. The values of ?k are de-
termined by the p-Norm Push algorithm in an it-
erative way.  
The weak rankers gk are the features described 
in Section 4. Note that we sometimes allow the 
algorithm to use both gk and g?k(hi)=1-gk(hi) as 
weak rankers, namely when gk has low accuracy 
on the training set; this way the algorithm itself 
can decide which to use.  
As in the style of boosting algorithms, real-
valued weights are placed on each of the training 
crucial pairs, and these weights are successively 
updated by the algorithm. Higher weights are 
given to those crucial pairs that were misranked at 
the previous iteration, especially taking into ac-
count the pairs near the top of the list. At each 
iteration, one weak ranker gk is chosen by the al-
gorithm, based on the weights. The coefficient ?k 
is then updated accordingly.  
6 Experiment Results 
6.1 Data and Resources 
We use 100 texts from the ACE 04 training corpus 
for a blind test. The test set included 2813 names: 
1126 persons, 712 GPEs, 785 organizations and 
190 locations. The performance is measured via 
Precision (P), Recall (R) and F-Measure (F). 
The baseline name tagger is trained from 2978 
texts from the People?s Daily news in 1998 and 
also 1300 texts from ACE training data.  
The 1,071,285 training samples (pairs of hy-
potheses) for the re-rankers are obtained from the 
name tagger applied on the ACE training data, in 
the manner described in Section 3.2. 
We use OpenNLP5 for the MaxEnt-Rank ex-
periments. We use SVMlight (Joachims, 1998) for 
SVMRank, with a linear kernel and the soft mar-
gin parameter set to the default value. For the p-
Norm Push Ranking, we apply 33 weak rankers, 
i.e., features described in Section 4. The number 
of iterations was fixed at 110, this number was 
chosen by optimizing the performance on a devel-
opment set of 100 documents. 
6.2 Effect of Pairwise Sampling 
We have tried both single-hypothesis and pairwise 
sampling (described in section 5.1.1) in MaxEnt-
Rank and p-Norm Push Ranking. Table 1 shows 
that pairwise sampling helps both algorithms. 
MaxEnt-Rank benefited more from it, with preci-
sion and recall increased 2.2% and 0.4% respec-
tively. 
 
Model P R F 
Single Sampling 89.6 90.2 89.9MaxEnt-
Rank Pairwise Sampling 91.8 90.6 91.2
Single Sampling 91.4 89.6 90.5p-Norm 
Push Pairwise Sampling 91.2 90.8 91.0
 
Table 1. Effect of Pairwise Sampling 
6.3 Overall Performance 
In Table 2 we report the overall performance for 
these three algorithms. All of them achieved im-
provements on the baseline name tagger. MaxEnt 
yields the highest precision, while p-Norm Push 
Ranking with p2 = 16 yields the highest recall. 
A larger value of ?p? encourages the p-Norm 
Push Ranking algorithm to perform better near the 
top of the ranked list. As we discussed in section 
                                                          
5 http://maxent.sourceforge.net/index.html 
54
5.3, we use p1 = 4 (a small ?push?) for the first re-
ranker and p2 = 16 (a big ?push?) for the second 
re-ranker. From Table 2 we can see that p2 = 16 
obviously performed better than p2 = 1. In general, 
we have observed that for p2 ?16, larger p2 corre-
lates with better results. 
 
Model P R F 
Baseline  87.4 87.6 87.5
MaxEnt-Rank 91.8 90.6 91.2
SVMRank 89.5 90.1 89.8
p-Norm Push Ranking (p2 =16) 91.2 90.8 91.0
p-Norm Push Ranking  
(p2 =1, RankBoost) 
89.3 89.7 89.5
 
Table 2. Overall Performance  
The improved NE results brought better per-
formance for the subsequent stages of information 
extraction too. We use the NE outputs from Max-
Ent-Ranker as inputs for coreference resolver and 
relation tagger. The ACE value6 of entity detec-
tion (mention detection + coreference resolution) 
is increased from 73.2 to 76.5; the ACE value of 
relation detection is increased from 34.2 to 34.8. 
6.4 Effect of Cross-document Information 
As described in Section 4.3, our algorithm incor-
porates cross-document coreference information. 
The 100 texts in the test set were first clustered 
into 28 topics (clusters). We then apply cross-
document coreference on each cluster. Compared 
to single document coreference, cross-document 
coreference obtained 0.5% higher F-Measure, us-
ing MaxEnt-Ranker, improving performance for 
15 of these 28 clusters. 
These clusters were then extended by selecting 
84 additional related texts from a corpus of 15,000 
unlabeled Chinese news articles (using a cross-
entropy metric to select texts). 24 clusters gave 
further improvement, and an overall 0.2% further 
improvement on F-Measure was obtained.  
6.5 Efficiency 
Model Training Test 
MaxEnt-Rank 7 hours 55 minutes
SVMRank 48 hours 2 hours 
p-Norm Push Ranking 3.2 hours 10 minutes
 
Table 3. Efficiency Comparison 
                                                          
6 The ACE04 value scoring metric can be found at: 
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-v7.pdf 
In Table 3 we summarize the running time of 
these three algorithms in our application. 
7 Discussion 
We have shown that the other components of an 
IE pipeline can provide information which can 
substantially improve the performance of an NE 
tagger, and that these improvements can be real-
ized through a variety of re-ranking algorithms.  
MaxEnt re-ranking using binary sampling and p-
Norm Push Ranking proved about equally effec-
tive.7  p-Norm Push Ranking was particularly ef-
ficient for decoding (about 10 documents / 
minute), although no great effort was invested in 
tuning these procedures for speed. 
We presented methods to handle cross-sentence 
inference using staged re-ranking and to incorpo-
rate additional evidence through document clus-
tering. 
An N-best / re-ranking strategy has proven ef-
fective for this task because with relatively small 
values of N we are already able to include highly-
rated hypotheses for most sentences.  Using the 
values of N we have used throughout (dependent 
on the margin of the baseline HMM, but never 
above 30), the upper bound of N-best performance 
(if we always picked the top-scoring hypothesis) 
is 97.4% recall, 96.2% precision, F=96.8%. 
Collins (2002) also applied re-ranking to im-
prove name tagging. Our work has addressed both 
name identification and classification, while his 
only evaluated name identification.  Our re-ranker 
used features from other pipeline stages, while his 
were limited to local features involving lexical 
information and 'word-shape' in a 5-token window.  
Since these feature sets are essentially disjoint, it 
is quite possible that a combination of the two 
could yield even further improvements. His boost-
ing algorithm is a modification of the method in 
(Freund et al, 1998), an adaptation of AdaBoost, 
whereas our p-Norm Push Ranking algorithm can 
emphasize the hypotheses near the top, matching 
our objective. 
Roth and Yih (2004) combined information 
from named entities and semantic relation tagging, 
adopting a similar overall goal but using a quite 
different approach based on linear programming.  
                                                          
7 The features were initially developed and tested using the MaxEnt re-ranker, 
so it is encouraging that they worked equally well with the p-Norm Push 
Ranker without further tuning. 
55
They limited themselves to name classification, 
assuming the identification given.  This may be a 
natural subtask for English, where capitalization is 
a strong indicator of a name, but is much less use-
ful for Chinese, where there is no capitalization or 
word segmentation, and boundary errors on name 
identification are frequent. Expanding their ap-
proach to cover identification would have greatly 
increased the number of hypotheses and made 
their approach slower.  In contrast, we adjust the 
number of hypotheses based on the margin in or-
der to maintain efficiency while minimizing the 
chance of losing a high-quality hypothesis. 
In addition we were able to capture selectional 
preferences (probabilities of semantic types as 
arguments of particular semantic relations as 
computed from the corpus), whereas Roth and Yih 
limited themselves to hard (boolean) type con-
straints. 
Acknowledgment 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657 and a postdoctoral research fellowship.  
Any opinions, findings and conclusions expressed 
in this material are those of the authors and do not 
necessarily reflect the views of the U. S. Govern-
ment. 
References  
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder.  Proc. 
ANLP1997. pp. 194-201. Washington, D.C.  
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
Fine N-Best Parsing and MaxEnt Discriminative 
Reranking. Proc. ACL2005. pp. 173-180. Ann Arbor, 
USA 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. Proc. 
ACL2005. pp. 263-270. Ann Arbor, USA 
Michael Collins. 2002. Ranking Algorithms for 
Named-Entity Extraction: Boosting and the Voted 
Perceptron. Proc. ACL 2002. pp. 489-496 
Michael Collins and Nigel Duffy. 2002. New Ranking 
Algorithms for Parsing and Tagging: Kernels over 
Discrete Structures, and the Voted Perceptron. Proc. 
ACL2002. pp. 263-270. Philadelphia, USA 
Michael Collins and Terry Koo. 2003. Discriminative 
Reranking for Natural Language Parsing. Journal of 
Association for Computational Linguistics. pp. 175-
182. 
Yoav Freund, Raj Iyer, Robert E. Schapire and Yoram 
Singer. 1998. An efficient boosting algorithm for 
combining  preferences. Machine Learning: Pro-
ceedings of the Fifteenth International Conference. 
pp. 170-178 
Ralph Grishman and Beth Sundheim.  1996. Message 
understanding conference - 6: A brief history. Proc. 
COLING1996,. pp. 466-471. Copenhagen. 
James Henderson and Ivan Titov. 2005. Data-Defined 
Kernels for Parse Reranking Derived from Probabil-
istic Models. Proc. ACL2005. pp. 181-188. Ann Ar-
bor, USA. 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Heng Ji and Ralph Grishman. 2006. Analysis and Re-
pair of Name Tagger Errors. Proc. ACL2006 
(POSTER). Sydney, Australia. 
Thorsten Joachims. 1998. Making large-scale support 
vector machine learning practical. Advances in Ker-
nel Methods: Support Vector Machine. MIT Press. 
Taku Kudo, Jun Suzuki and Hideki Isozaki. 2005. 
Boosting-based Parse Reranking Derived from 
Probabilistic Models. Proc. ACL2005. pp. 189-196. 
Ann Arbor, USA. 
John Platt, Nello Cristianini, and John Shawe-Taylor. 
2000.  Large margin dags for multiclass classifica-
tion. Advances in Neural Information Processing 
Systems 12. pp. 547-553 
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural 
Language Tasks. Proc. CONLL2004. pp. 1-8 
Dan Roth and Wen-tau Yih. 2002. Probabilistic Rea-
soning for Entity & Relation Recognition. Proc. 
COLING2002. pp. 835-841 
Cynthia Rudin. 2006. Ranking with a p-Norm Push. 
Proc. Nineteenth Annual  Conference on Computa-
tional Learning Theory (CoLT 2006), Pittsburgh, 
Pennsylvania. 
Libin Shen and Aravind K. Joshi. 2003. An SVM 
Based Voting Algorithm with Application to Parse 
ReRanking. Proc. HLT-NAACL 2003 workshop on 
Analysis of Geographic References. pp. 9-16 
Libin Shen and Aravind K. Joshi. 2004. Flexible Mar-
gin Selection for Reranking with Full Pairwise Sam-
ples. Proc.IJCNLP2004. pp. 446-455. Hainan Island, 
China. 
56
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 369?372,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Predicting Unknown Time Arguments  
based on Cross-Event Propagation 
Prashant Gupta Heng Ji 
Indian Institute of Information  
Technology Allahabad 
Computer Science Department, Queens College and  
the Graduate Center, City University of New York 
Allahabad, India, 211012 New York, NY, 11367, USA 
greatprach@gmail.com hengji@cs.qc.cuny.edu 
 
  
 
Abstract 
Many events in news articles don?t include 
time arguments. This paper describes two 
methods, one based on rules and the other 
based on statistical learning, to predict the un-
known time argument for an event by the 
propagation from its related events. The re-
sults are promising ? the rule based approach 
was able to correctly predict 74% of the un-
known event time arguments with 70% preci-
sion. 
1 Introduction 
Event time argument detection is important to 
many NLP applications such as textual inference 
(Baral et al, 2005), multi-document text summa-
rization (e.g. Barzilay e al., 2002), temporal 
event linking (e.g. Bethard et al, 2007; Cham-
bers et al, 2007; Ji and Chen, 2009) and template 
based question answering (Ahn et al, 2006). It?s 
a challenging task in particular because about 
half of the event instances don?t include explicit 
time arguments. Various methods have been ex-
ploited to identify or infer the implicit time ar-
guments (e.g. Filatova and Hovy, 2001; Mani et 
al., 2003; Lapata and Lascarides, 2006; Eidelman, 
2008).  
Most of the prior work focused on the sen-
tence level by clustering sentences into topics 
and ordering sentences on a time line. However, 
many sentences in news articles include multiple 
events with different time arguments. And it was 
not clear how the errors of topic clustering tech-
niques affected the inference scheme. Therefore 
it will be valuable to design inference methods 
for more fine-grained events.  
In addition, in the previous approaches the lin-
guistic evidences such as verb tense were mainly 
applied for inferring the exact dates of implicit 
time expressions. In this paper we are interested 
in those more challenging cases in which an 
event mention and all of its coreferential event 
mentions do not include any explicit or implicit 
time expressions; and therefore its time argument 
can only be predicted based on other related e-
vents even if they have different event types. 
2 Terminology and Task 
In this paper we will follow the terminology de-
fined in the Automatic Content Extraction 
(ACE)1 program: 
entity: an object or a set of objects in one of the 
semantic categories of interest: persons, locations, 
organizations, facilities, vehicles and weapons. 
event: a specific occurrence involving participants. 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 distinct event types. In 
contrast to ACE event extraction, we exclude ge-
neric, negative, and hypothetical events. 
event mention: a phrase or sentence within which 
an event is described. 
event argument: an entity involved in an event 
with some specific role. 
event time: an exact date normalized from time ex-
pressions and a role to indicate that an event occurs 
before/after/within the date. 
For any pair of event mentions <EMi, EMj>, if: 
? EMi includes a time argument time-arg; 
? EMj and its coreferential event mentions 
don?t include any time arguments; 
The goal of our task is to determine whether 
time-arg can be propagated into EMj or not. 
3 Motivation 
The events in a news document may contain a 
temporal or locative dimension, typical about an 
unfolding situation. Various situations are evolv-
ing, updated, repeated and corrected in different 
event mentions. Here later information may 
override earlier more tentative or incomplete 
                                                 
1 http://www.nist.gov/speech/tests/ace/ 
369
events. As a result, different events with particu-
lar types tend to occur together frequently, for 
example, the chains of ?Conflict?Life-Die/Life-
Injure? and ?Justice-Convict ? Justice-Charge-
Indict/Justice-Trial-Hearing? often appear within 
one document. To avoid redundancy, the news 
writers rarely provide time arguments for all of 
these events. Therefore, it?s possible to recover 
the time argument of an event by gleaning 
knowledge from its related events, especially if 
they are involved in a pre-cursor/consequence or 
causal relation. We present two examples as fol-
lows. 
? Example 1 
For example, we can propagate the time ?Sunday 
(normalized into ?2003-04-06?)? from a ?Con-
flict-Attack? EMi to a ?Life-Die? EMj because 
they both involve ?Kurdish/Kurds?: 
[Sentence including EMi]  
Injured Russian diplomats and a convoy of Amer-
ica's Kurdish comrades in arms were among unin-
tended victims caught in crossfire and friendly fire 
Sunday. 
[Sentence including EMj]  
Kurds said 18 of their own died in the mistaken 
U.S. air strike. 
? Example 2 
This kind of propagation can also be applied be-
tween two events with similar event types. For 
example, in the following we can propagate 
?Saturday? from a ?Justice-Convict? event to a 
?Justice-Sentence? event because they both in-
volve arguments ?A state security court/state? 
and ?newspaper/Monitor?: 
[Sentence including EMi]  
A state security court suspended a newspaper criti-
cal of the government Saturday after convicting it 
of publishing religiously inflammatory material. 
[Sentence including EMj]  
The sentence was the latest in a series of state ac-
tions against the Monitor, the only English lan-
guage daily in Sudan and a leading critic of condi-
tions in the south of the country, where a civil war 
has been waged for 20 years. 
4 Approaches 
Based on these motivations we have developed 
two approaches to conduct cross-event propaga-
tion.  Section 4.1 below will describe the rule-
based approach and section 4.2 will present the 
statistical learning framework respectively. 
4.1 Rule based Prediction 
The easiest solution is to encode rules based on 
constraints from event arguments and positions 
of two events. We design three types of rules in 
this paper. 
If  EMi has an event type typei and includes an 
argument argi with role rolei, while EMj has an 
event type typej and includes an argument argj 
with role rolej, they are not from two temporally 
separate groups of Justice events {Release-Parole, 
Appeal, Execute, Extradite, Acquit, Pardon} and 
{Arrest-Jail, Trial-Hearing, Charge-Indict, Sue, 
Convict, Sentence, Fine}2, and they match one of 
the following rules, then we propagate the time 
argument between them. 
? Rule1: Same-Sentence Propagation 
EMi and EMj are in the same sentence and 
only one time expression exists in the sen-
tence; This follows the within-sentence infer-
ence idea in (Lapata and Lascarides, 2006). 
? Rule2: Relevant-Type Propagation 
argi is coreferential with argj;  
typei= ?Conflict?, typej= ?Life-Die/Life-
Injure?; 
rolei=?Target? and rolej=?Victim?, or 
rolei=rolej=?Instrument?. 
? Rule3: Same-Type Propagation 
argi is coreferential with argj, typei= typej, 
rolei= rolej, and they match one of the Time-
Cue event type and argument role combina-
tions in Table 1. 
 
Event Typei Argument Rolei 
Conflict Target/Attacker/Crime 
Justice Defendant/Crime/Plantiff
Life-Die/Life-Injure Victim 
Life-Be-Born/Life-
Marry/Life-Divorce 
Person/Entity 
Movement-Transport Destination/Origin 
Transaction Buyer/Seller/Giver/ 
Recipient 
Contact Person/Entity 
Personnel Person/Entity 
Business Organization/Entity   
Table 1. Time-Cue Event Types and  
Argument Roles   
 
The combinations shown in Table 1 above are 
those informative arguments that are specific 
enough to indicate the event time, thus they are 
                                                 
2 Statistically there is often a time gap between these 
two groups of events. 
370
called ?Time-Cue? roles. For example, in a 
?Conflict-Attack? event, ?Attacker? and ?Tar-
get? are more important than ?Person? to indi-
cate the event time. The general idea is similar to 
extracting the cue phrases for text summarization 
(Edmundson, 1969). 
4.2 Statistical Learning based Prediction 
In addition, we take a more general statistical 
approach to capture the cross-event relations and 
predict unknown time arguments. We manually 
labeled some ACE data and trained a Maximum 
Entropy classifier to determine whether to 
propagate the time argument of EMi to EMj or 
not. The features in this classifier are most de-
rived from the rules in the above section 4.1. 
Following Rule 1, we build the following two 
features: 
? Feature1: Same-Sentence 
F_SameSentence: whether EMi and EMj are 
located in the same sentence or not. 
? Feature2: Number of Time Arguments 
F_TimeNum: if F_SameSentence = true, then 
assign the number of time arguments in the 
sentence, otherwise assign the feature value as 
?Empty?. 
For all the Time-Cue argument role pairs in 
Rule 2 and Rule 3, we construct a set of features: 
? Feature Set3: Time-Cue Argument Role 
Matching 
F_CueRoleij: Construct a feature for any pair 
of Time-Cue role types Rolei and Rolej in Rule 
2 and 3, assign the feature value as follows: 
if the argument argi in EMi has a role Rolei 
and the argument argj has a role Rolej: 
            if argi and argj are coreferential then 
F_CueRoleij = Coreferential,   
           else F_CueRoleij = Non-Coreferential. 
else F_CueRoleij = Empty. 
5 Experimental Results 
In this section we present the results of applying 
these two approaches to predict unknown event 
time arguments. 
5.1 Data and Answer-Key Annotation 
We used 47 newswire texts from ACE 2005 
training corpora to train the Maximum Entropy 
classifier, and conduct blind test on a separate set 
of 10 ACE 2005 newswire texts. For each docu-
ment we constructed any pair of event mentions 
<EMi, EMj> as a candidate sample if EMi in-
cludes a time argument while EMj and its 
coreferential event mentions don?t include any 
time arguments. We then manually labeled 
?Propagate/Not-Propagate? for each sample. The 
annotation for both training and test sets took one 
human annotator about 10 hours. We asked an-
other annotator to label the 10 test texts sepa-
rately and the inter-annotator agreement is above 
95%. There are 485 ?Propagate? samples and 
617 ?Not-Propagate? samples in the training set; 
and in total 212 samples in the test set. 
5.2 Overall Performance 
Table 2 presents the overall Precision (P), Recall 
(R) and F-Measure (F) of using these two differ-
ent approaches. 
 
     Method P (%) R (%) F(%)
Rule-based 70.40 74.06 72.18
Statistical Learning 72.48 50.94 59.83
 
Table 2. Overall Performance 
 
The results of the rule-based approach are 
promising: we are able to correctly predict 74% 
of the unknown event time arguments at about 
30% error rate. The most common correctly 
propagated pairs are:  
? From Conflict-Attack to Life-Die/Life-Injure 
? From Justice Convict to Justice-Sentence/ 
Justice-Charge-Indict 
? From Movement-Transport  to Contact-Meet 
? From Justice-Charge-Indict  to Justice-
Convict 
5.3 Discussion 
From Table 2 we can see that the rule-based ap-
proach achieved 23% higher recall than the sta-
tistical classifier, with only 2% lower precision. 
The reason is that we don?t have enough training 
data to capture all the evidences from different 
Time-cue roles. For instance, for the Example 2 
in section 3, Rule 3 is able to predict the time 
argument of the ?Justice-Sentence? event as 
?Saturday (normalized as 2003-05-10)? because 
these two events share the coreferential Time-cue 
?Defendant? arguments ?newspaper? and ?Moni-
tor?. However, there is only one positive sample 
matching these conditions in the training corpora, 
and thus the Maximum Entropy classifier as-
signed a very low confidence score for propaga-
tion.  We have also tried to combine these two 
approaches in a self-training framework ? adding 
the results from the propagation rules as addi-
tional training data and re-train the Maximum 
371
Entropy classifier, but it did not provide further 
improvement. 
The spurious errors made by the prediction 
rules reveal both the shortcomings of ignoring 
event reporting order and the restricted matching 
on event arguments. 
For example, in the following sentences: 
 [Context Sentence] 
American troops stormed a presidential palace and 
other key buildings in Baghdad as U.S. tanks rum-
bled into the heart of the battered Iraqi capital on 
Monday amid the thunder of gunfire and explo-
sions? 
[Sentence including EMj] 
At the palace compound, Iraqis shot <instru-
ment>small arms</instrument> fire from a clock 
tower, which the U.S. tanks quickly destroyed. 
[Sentence including EMi]  
The first one was on Saturday and triggered in-
tense <instrument>gun</instrument> battles, 
which according to some U.S. accounts, left at least 
2,000 Iraqi fighters dead. 
 
The time argument ?Saturday? was mistakenly 
propagated from the ?Conflict-Attack? event 
?battles? to ?shot? because they share the same 
Time-cue role ?instrument? (?small arms/gun?). 
However, the correct time argument for the 
?shot? event should be ?Monday? as indicated in 
the ?gunfire/explosions? event in the previous 
context sentence. But since the ?shot? event 
doesn?t share any arguments with ?gun-
fire/explosions?, our approach failed to obtain 
any evidence for propagating ?Monday?. In the 
future we plan to incorporate the distance and 
event reporting order as additional features and 
constraints. 
Nevertheless, as Table 2 indicates, the rewards 
of using propagation rules outweigh the risks 
because it can successfully predict a lot of un-
known time arguments which were not possible 
using the traditional time argument extraction 
techniques. 
6 Conclusion and Future Work 
In this paper we described two approaches to 
predict unknown time arguments based on the 
inference and propagation between related events. 
In the future we shall improve the confidence 
estimation of the Maximum Entropy classifier so 
that we could incorporate dynamic features from 
the high-confidence time arguments which have 
already been predicted. We also plan to test the 
effectiveness of this system in textual inference, 
temporal event linking and event coreference 
resolution. We are also interested in extending 
these approaches to the setting of cross-
document, so that we can predict more time ar-
guments based on the background knowledge 
from related documents.  
Acknowledgments 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program. 
References  
David Ahn, Steven Schockaert, Martine De Cock and 
Etienne Kerre. 2006. Supporting Temporal Ques-
tion Answering: Strategies for Offline Data Collec-
tion. Proc. 5th International Workshop on Infer-
ence in Computational Semantics (ICoS-5). 
Regina Barzilay, Noemie Elhadad and Kathleen 
McKeown. 2002. Inferring Strategies for Sentence 
Ordering in Multidocument Summarization. JAIR, 
17:35-55. 
Chitta Baral, Gregory Gelfond, Michael Gelfond and 
Richard B. Scherl. 2005. Proc. AAAI'05 Workshop 
on Inference for Textual Question Answering. 
Steven Bethard, James H. Martin and Sara Klingen-
stein. 2007. Finding Temporal Structure in Text: 
Machine Learning of Syntactic Temporal Relations. 
International Journal of Semantic Computing 
(IJSC), 1(4), December 2007. 
Nathanael Chambers, Shan Wang and Dan Jurafsky. 
2007. Classifying Temporal Relations Between 
Events. Proc. ACL2007. 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM. 16(2):264-285. 
Vladimir Eidelman. 2008. Inferring Activity Time in 
News through Event Modeling. Proc. ACL-HLT 
2008.  
Elena Filatova and Eduard Hovy. 2001. Assigning 
Time-Stamps to Event-Clauses. Proc. ACL 2001 
Workshop on Temporal and Spatial Information 
Processing. 
Heng Ji and Zheng Chen. 2009. Cross-document 
Temporal and Spatial Person Tracking System 
Demonstration. Proc. HLT-NAACL 2009. 
Mirella Lapata and Alex Lascarides. 2006. Learning 
Sentence-internal Temporal Relations. Journal of 
Artificial Intelligence Research 27. pp. 85-117. 
Inderjeet Mani, Barry Schiffman and Jianping Zhang. 
2003. Inferring Temporal Ordering of Events in 
News. Proc. HLT-NAACL 2003. 
372
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of NAACL HLT 2009: Short Papers, pages 209?212,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Language Specific Issue and Feature Exploration in  
Chinese Event Extraction 
 
Zheng Chen Heng Ji 
Department of Computer Science 
The Graduate Center Queens College and The Graduate Center 
The City University of New York 
365 Fifth Avenue, New York, NY 10016, USA 
zchen1@gc.cuny.edu hengji@cs.qc.cuny.edu 
 
 
Abstract 
In this paper, we present a Chinese event ex-
traction system. We point out a language spe-
cific issue in Chinese trigger labeling, and 
then commit to discussing the contributions of 
lexical, syntactic and semantic features ap-
plied in trigger labeling and argument labeling. 
As a result, we achieved competitive perform-
ance, specifically, F-measure of 59.9 in trigger 
labeling and F-measure of 43.8 in argument 
labeling. 
1 Introduction 
In this paper we address the event extraction task 
defined in Automatic Content Extraction (ACE)1 
program. The ACE program defines the following 
terminology for event extraction task: 
z Trigger: the word that most clearly expresses 
an event?s occurrence 
z Argument:  an entity, or a temporal expression 
or a value that plays a certain role in the event 
instance 
z Event mention: a phrase or sentence with a 
distinguished trigger and participant argu-
ments  
Some English event extraction systems based on 
supervised learning have been reported by re-
searchers (Ahn, 2006; Ji and Grishman, 2008). In 
this paper we developed a modularized Chinese 
event extraction system. We nicely handled the 
language specific issue in trigger labeling and ex-
plored effective lexical, syntactic and semantic 
features that were applied in trigger labeling and 
argument labeling. Tan et al (2008) addressed the 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
same task as we did in this paper. However, to our 
knowledge, the language specific issue and feature 
contributions for Chinese event extraction have not 
been reported by earlier researchers. 
The remainder of the paper is organized as fol-
lows. Section 2 points out a language specific issue 
in Chinese trigger labeling and discusses two 
strategies of trigger labeling: word-based and char-
acter-based. Section 3 presents argument labeling. 
Section 4 discusses the experimental results. Sec-
tion 5 concludes the paper. 
2 Trigger Labeling 
We split trigger labeling into two steps: 1) trigger 
identification: to recognize the event trigger 2) 
trigger classification: to assign an event type for 
the trigger. The two strategies we will discuss in 
trigger labeling (word-based and character-based) 
only differ in the first step. 
2.1 A Language-Specific Issue 
Chinese, and some other languages, e.g., Japanese 
do not have delimiters between words. Thus, seg-
mentation is usually an indispensible step for fur-
ther processing, e.g., Part-of-Speech tagging, 
parsing, etc. However, the segmentation may cause 
a problem in some tasks, e.g., name entity recogni-
tion (Jing et al, 2003) and event trigger identifica-
tion. For a specific example, ???? (shoot and kill) 
is segmented as a Chinese word. However, there 
are two triggers in the word, one is ???(shoot) 
with the event type of Attack, and the other is 
???(kill) with the event type of Die. The trigger 
may also cross two or more words, e.g., the trigger 
is ???? ? (public letter) which crosses two 
words, ???? (public) and ???(letter).  
In the ACE Chinese corpus, 2902 triggers ex-
actly one-to-one match their corresponding words, 
209
meanwhile, 431 triggers are inconsistent with the 
words (either within the word, or across words). 
The inconsistency rate is as high as 13%.  
We then discuss two strategies of trigger label-
ing, one is word-based in which we use a global 
errata table to alleviate the inconsistency problem, 
and the other is character-based which solves the 
inconsistency problem. 
2.2 Word-based Trigger Labeling 
We apply Maximum-Entropy based classifiers for 
trigger identification and trigger classification. 
The two classifiers share the same set of features: 
z Lexical features: word, POS of the word, pre-
vious word + word, word + next word, previous 
POS + POS, and POS + next POS. 
z Syntactic features: 1) depth: the depth of the 
trigger in the parse tree 2) path to root: the path 
from the leaf node of the trigger to the root in the 
parse tree 3) sub-categorization : the phrase struc-
ture expanded by the father of the trigger 4) 
phrase type: the phrase type of the trigger 
z Semantic dictionaries: 1) predicate existence: 
a boolean value indicating the existence of trigger 
in a predicate list which is produced from Chi-
nese Propbank (Xue and Palmer, 2008) 2) syno-
nym entry: the entry number of the trigger in a 
Chinese synonym dictionary  
z Nearest entity information: 1) the entity type 
of the syntactically nearest entity to the trigger in 
the parse tree 2) the entity type of the physically 
nearest entity to the trigger in the sentence 
 
To deal with the language-specific issue in trig-
ger identification, we construct a global errata table 
to record the inconsistencies existing in the train-
ing set. In the test procedure, if the scanned word 
has an entry in the errata table, we select the possi-
ble triggers in the entry as candidate triggers. 
2.3 Character-based Trigger Labeling  
Although the error table significantly helps to re-
duce segmentation inconsistencies, it is not a per-
fect solution since it only recognizes the 
inconsistencies existing in the training data. 
To take a further step we build a separate char-
acter-based trigger identification classifier for 
comparison. We use a MEMM (Maximum En-
tropy Markov Model) to label each character with 
a tag indicating whether it is out of the trigger (O), 
or is the beginning of the trigger (B) or is a part of 
the trigger except the beginning (I).  Our MEMM 
classifier performs sequential classification by as-
signing each character one of the three tags. We 
then apply Viterbi algorithm to decode the tag se-
quence and identify the triggers in the sequence. 
Features used in our MEMM classifier include: 
the character, previous character, next character, 
previous tag and word-based features that the char-
acter carries. We apply the same set of features for 
trigger classification as used in word-based trigger 
labeling. 
3 Argument Labeling 
We also split argument labeling into two steps: 1) 
argument identification: to recognize an entity or a 
temporal expression or a value as an argument 2) 
role classification: to assign a role to the argument. 
We apply Maximum-Entropy based classifiers for 
the two steps and they share the same set of fea-
tures:  
z Basic features: trigger, event subtype of the 
event mention, type of the ACE entity mention, 
head word of the entity mention, combined value 
of event subtype and head word, combined value 
of event subtype and entity subtype. 
z Neighbor words: 1) left neighbor word of the 
entity, temporal expression, or value 2) right 
neighbor word of the entity, temporal expression, 
or value 
z Syntactic features: 1) sub-categorization: the 
phrase structure expanding the parent of the trig-
ger 2) position: the relative position of the entity 
regarding to the trigger (before or after) 3) path: 
the minimal path from the entity to the trigger 4) 
distance: the shortest length from the entity to the 
trigger in the parse tree 
4 Experimental Results 
4.1 Data and Scoring Metric 
We used 2005 ACE training corpus for our ex-
periments. The corpus contains 633 Chinese docu-
ments. In this paper we follow the setting of ACE 
diagnostic tasks and use the ground truth entities, 
times and values for our training and testing.  
We randomly selected 558 documents as train-
ing set and 66 documents as test set. For the train-
ing set, we reserved 33 documents as development 
set. 
We define the following standards to determine 
the correctness of an event mention: 
210
z A trigger is correctly labeled if its event type 
and offsets exactly match a reference trigger. 
z An argument is correctly labeled if its event 
type, offsets, and role match the reference argu-
ment mention.  
4.2 Overall System Performance 
Table 1 shows the overall Precision (P), Recall (R) 
and F-Measure (F) scores of our baseline system 
(word-based system with only lexical features in 
trigger labeling and basic features in argument la-
beling), word-based system with full integrated 
features and character-based system with full inte-
grated features.  
Comparing to the Chinese event extraction sys-
tem reported by (Tan et al, 2008), our scores are 
much lower. However, we argue that we apply 
much more strict evaluation metrics. 
4.3 Comparison between Word-based and 
Character-based Trigger Labeling 
Table 1 lists the comparison results between char-
acter-based and word-based trigger labeling. It in-
dicates that the character-based method 
outperforms the word-based method, mostly due to 
the better performance in the step of trigger identi-
fication (3.3% improvement in F-Measure) with 
precision as high as 82.4% (14.3% improvement), 
and a little loss in recall (2.1%).  
4.4 Feature Contributions for Trigger Label-
ing 
Table 2 presents the feature contributions for 
word-based trigger labeling, and we observe simi-
lar feature contributions for character-based since 
it only differs from word-based in trigger identifi-
cation and works similarly in trigger classification 
(we omit the results here). Table 2 shows that 
maintaining an errata table is an effective strategy 
for word-based trigger identification and diction-
ary resources improve the performance. 
It is worth noting that the performance drops 
when integrating the syntactic features. Our expla-
nation might be that the trigger, unlike the predi-
cate in the semantic role labeling task, can not only 
be a verb, but also can be a noun or other types. 
Thus the syntactic position for the trigger in the 
parse tree is much more flexible than the predicate 
in Semantic Role Labeling. For this reason, syntac-
tic features are not so discriminative in trigger la-
beling. Furthermore, the syntactic features cannot 
discriminate the word senses of a candidate trigger. 
In the following example, 
S1:??? ?? ?? ?? ?? ?? ?? ? ?? 
The players are entering the stadium to prepare for 
the coming game. 
S2:?? ??? ? ?? ?? ?? ? ??? 
Many farm products have been rotted before entering 
the market. 
The word ???? (entering) indicates a ?Trans-
port? event in sentence 1 but not in sentence 2. The 
phrase structures around the word ???? in both 
sentences are exactly the same (VP?VP-NP). 
However, if an entity of ?PERSON? appears ahead 
of ????, the word ???? is much more likely to 
be a trigger. Hence the features of nearby entity 
information could be effective.  
4.5 Feature Contributions for Argument La-
beling 
Table 3 shows feature contributions for argument 
labeling after word-based trigger labeling and we 
also observe the same feature contributions for ar-
gument labeling after character-based trigger label-
ing (results are omitted). It shows that the two 
neighbor word features are fairly effective. We 
observe that in some patterns of event description, 
the left word is informative to tell the followed 
entity mention is an argument. For example, ??
[Entity]???(killed by [Entity]) is a common pat-
tern to describe an attack event, and the left 
neighbor word of the entity ??? (by) can strongly 
imply that the entity is an argument with a role of 
?Attacker?. Meanwhile, the right word can help 
reduce the spurious arguments. For example, in the 
Chinese ??? (of) structure, the word ??? (of) 
strongly suggests that the entity on the left side of 
??? is not an argument.  
The sub-categorization feature contributes little 
since it is a feature shared by all the arguments in 
the parse tree.  Table 3 also shows that Path and 
Distance are two effective features. It is obvious 
that in the parse tree, each argument attached to the 
trigger is in a certain syntactic configuration. For 
example, the path ?NP VP VV? ? ? implies that it 
might be a Subject-Verb structure and thus the en-
tity in NP is highly likely to be an argument of the 
trigger (VV). The Position feature is helpful to dis-
criminate argument roles in syntactically identical 
structure, e.g., ?Subject Verb Object? structure.  
211
Trigger Identification Trigger Labeling Argument 
Identification 
Argument Labeling Performance 
 
System P R F P R F P R F P R F 
Baseline 61.0 50.0 54.9 58.7 48.2 52.9 49.5 38.2 43.1 44.6 34.4 38.9 
Word-based  68.1 52.7 59.4 65.7 50.9 57.4 56.1 38.2 45.4 53.1 36.2 43.1 
Character-based  82.4 50.6 62.7 78.8 48.3 59.9 64.4 36.4 46.5 60.6 34.3 43.8 
Table 1. Overall system performance (%) 
 
Trigger Identification Trigger Labeling  
P R F P R F 
Lexical features : (1) 61.0 50.0 54.9 58.7 48.2 52.9 
(1) + Errata table: (2) 64.0 52.0 57.4 61.3 49.8 54.9 
(2) + Dictionaries: (3) 64.9 53.5 58.6 62.7 51.6 56.6 
(3)+ Syntactic features: (4) 64.3 51.8 57.4 60.6 48.9 54.1 
(3) + Entity information: (5) 68.1 52.7 59.4 65.7 50.9 57.4 
Table 2.  Feature contributions for word-based trigger labeling (%) 
 
Argument Identification Argument Labeling  
P R F P R F 
Basic feature set: (1) 40.5 32.8 36.2 37.7 30.5 33.7 
(1)+Left word: (2) 45.2 35.4 39.7 41.6 32.5 36.5 
(1)+Right word: (3) 47.7 35.6 40.8 44.1 32.9 37.7 
Feature set 2: (2)+(3) 49.0 35.7 41.3 46.1 33.6 38.9 
(1)+Sub-categorization: (4) 41.9 33.1 37.0 38.7 30.5 34.1 
(1)+Path: (5) 46.6 36.2 40.7 43.4 33.7 38.0 
(1)+Distance: (6) 49.5 37.0 42.3 45.0 33.6 38.5 
(1)+Position:(7) 43.8 35.3 39.1 41.0 33.1 36.6 
Feature set 3 (from 4 to 7) 56.2 36.1 43.9 51.2 32.9 40.0 
Total 56.1 38.2 45.4 53.1 36.2 43.1 
Table 3.  Feature contributions for argument labeling after word-based trigger labeling (%) 
5 Conclusions and Future Work 
In this paper, we took a close look at language spe-
cific issue in Chinese event extraction and ex-
plored effective features for Chinese event 
extraction task. All our work contributes to setting 
up a high performance Chinese event extraction 
system.  
For future work, we intend to explore an ap-
proach to conducting cross-lingual event extraction 
and investigate whether the cross-lingual inference 
can bootstrap either side when running two lan-
guage event extraction systems in parallel.  
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program. 
 
References  
D. Ahn. 2006. The stages of event extraction. Proc. 
COLING/ACL 2006 Workshop on Annotating and 
Reasoning about Time and Events. Sydney, Austra-
lia. 
H. Ji and R. Grishman. 2008. Refining Event Extraction 
Through Cross-document Inference. Proc. ACL 
2008. Ohio, USA.  
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Ittyche-
riah. 2003. HowtogetaChineseName(Entity): Seg-
mentation and combination issues. Proc. EMNLP 
2003.  
H. Tan; T. Zhao; J. Zheng. 2008. Identification of Chi-
nese Event and Their Argument Roles. Proc. of the 
2008 IEEE 8th International Conference on Com-
puter and Information Technology Workshops. 
N. Xue and M. Palmer. 2008. Adding Semantic Role to 
the Chinese Treebank. Natural Language Engineer-
ing. Combridge University Press. 
212
Proceedings of NAACL HLT 2009: Demonstrations, pages 1?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Cross-document Temporal and Spatial Person Tracking  
System Demonstration 
 
Heng Ji Zheng Chen 
Queens College and the Graduate Center The Graduate Center 
The City University of New York 
New York, NY, 11367 
hengji@cs.qc.cuny.edu zchen1@gc.cuny.edu 
 
  
 
Abstract 
Traditional Information Extraction (IE) sys-
tems identify many unconnected facts. The 
objective of this paper is to define a new 
cross-document information extraction task 
and demonstrate a system which can extract, 
rank and track events in two dimensions: tem-
poral and spatial. The system can automati-
cally label the person entities involved in 
significant events as 'centroid arguments', and 
then present the events involving the same 
centroid on a time line and on a geographical 
map. 
1 Introduction 
Information Extraction (IE) systems can identify 
?facts? (entities, relations and events) of particular 
types within individual documents, and so can 
unleash the knowledge embedded in texts for many 
domains, such as military monitoring, daily news, 
financial analysis and biomedical reports. However, 
most current IE systems focus on processing single 
documents and, except for coreference resolution, 
operate a sentence at a time. The result are large 
databases containing many unconnected, unranked, 
redundant (and some erroneous) facts. 
McNamara (2001) proved that a high-coherence 
text has fewer conceptual gaps and thus requires 
fewer inferences and less prior knowledge, render-
ing the text easier to understand. In our task text 
coherence is the extent to which the relationships 
between events in a text can be made explicit. We 
noted that linking all events in temporal and spatial 
directions for the entire corpus was not feasible 
because of the large number of event arguments. 
Grosz et al (1995) claimed that certain entities are 
more central than others and that this property im-
posed constraints on discourse coherence. There-
fore we have developed a system which can extract 
globally salient and novel arguments as centroid 
arguments, and link all events involving each cen-
troid argument on a time line and on a geographi-
cal map.    
Beyond extracting isolated facts from individual 
sentences, we provide coherent event chains so that 
the users can save time in connecting relevant 
events and conducting reasoning, such as tracking 
a person?s movement activities and an organiza-
tion?s personnel changes. This will provide a richer 
set of views than is possible with document clus-
tering for summarization or with topic tracking. In 
addition, such cross-document extraction results 
are indexed and allow a fast entity searching 
mechanism. Beyond traditional search, the system 
can correlate and organize information across dif-
ferent time series by temporal tracking, and deliver 
to users in different geographies by spatial tracking. 
The rest of this paper is structured as follows. 
Section 2 presents the overall system architecture 
including the baseline system and the detailed ap-
proaches to extract event chains. Section 3 then 
presents the experimental results compared to tra-
ditional IE. Section 4 demonstrates the system out-
put. Section 5 compares our approach with related 
work and Section 6 then concludes the paper and 
sketches our future work. 
2 System Overview 
In this section we will present the overall proce-
dure of our system.  
 
 
 
1
2.1 Within-document IE 
We first apply a state-of-the-art English IE system 
(Ji and Grishman, 2008) to extract events from 
each single document. The IE system includes en-
tity extraction, time expression extraction and 
normalization, relation extraction and event extrac-
tion. Entities include persons, locations, organiza-
tions, facilities, vehicles and weapons; Events 
include the 33 distinct event types defined in 
Automatic Content Extraction (ACE05)1.  
The event extraction system combines pattern 
matching with statistical models. For every event 
instance in the ACE training corpus, patterns are 
constructed based on the sequences of constituent 
heads separating the trigger and arguments. In ad-
dition, a set of Maximum Entropy classifiers are 
trained: to distinguish events from non-events; to 
classify events by type and subtype; to distinguish 
arguments from non-arguments; to classify argu-
ments by argument role; and given a trigger, an 
event type, and a set of arguments, to determine 
whether there is a reportable event mention. In ad-
dition, the global evidence from related documents 
is combined with local decisions to conduct cross-
document inference for improving the extraction 
performance as described in (Ji and Grishman, 
2008). 
2.2 Centroid Argument Detection 
After we harvest a large repository of events we 
can label those important person entities which are 
involved frequently in events as ?centroid argu-
ments?. Not only are such arguments central to the 
information in a collection (high-frequency), they 
also should have higher accuracy (high-
confidence). In this project we exploit global con-
fidence metrics to reach both of these two goals. 
For an event mention, the within-document 
event classifiers produce the following local confi-
dences values: 
 
? LConf(trigger,etype): The probability of a 
string trigger indicating an event mention with 
type etype. 
? LConf(arg, etype): The probability that a men-
tion arg is an argument of some particular 
event type etype. 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
? LConf(arg, etype, role): If arg is an argument 
with event type etype, the probability of arg 
having some particular role. 
 
We use the INDRI information retrieval system 
(Strohman et al, 2005) to obtain the top N related 
documents for each test document to form a topi-
cally-related cluster. The intuition is that if an ar-
gument appears frequently as well as with high 
extraction confidence in a cluster, it is more salient. 
For each argument arg we also added other person 
names coreferential with or bearing some ACE 
relation to the argument as argset.  
In addition we developed a cross-document per-
son name disambiguation component based on 
heuristic rules to resolve ambiguities among cen-
troid arguments. Then we define the following global 
metric weighted with the local confidence values to 
measure salience, and generate the top-ranked entities 
as centroid arguments. 
 
? Global-Confidence(arg): The frequency of 
argset appearing as an event argument in a 
cluster, weighted by local confidence values: 
LConf(trigger,etype)*LConf(arg, etype)*  
 LConf(arg, etype, role). 
2.3 Cross-document Event Aggregation and 
Global Time Discovery 
If two events involve the same centroid argument, 
we order them along a time line according to their 
time arguments and group them into specific geo-
graphical locations based on their place arguments. 
When ordering a pair of entity arguments, we re-
place pronouns with their coreferential names or 
nominals, and replace nominals with their corefer-
ential names, if applicable. If the normalized dates 
are the same for two events, we further compare 
them based on their time roles (e.g. ?time-end? 
should be ordered after ?time-beginning?).  
We start from aggregating events by merging 
coreferential event mentions using the within-
document coreference resolution component in the 
IE system. However, the degree of similarity 
among events contained in a group of topically-
related documents is much higher than within a 
document, as each document is apt to describe the 
main point as well as necessary shared background.
2
Relation Eventi Arguments Eventj Arguments Centroid Event Type Event Time
Coreference Entity[Ariel Sharon] 
Place [Jerusalem] 
Entity[Sharon] 
Place[Jerusalem] 
Powell Contact-
Meet 
2003-06-20
Subset Entity[Bush] Entity[Bush] 
Place[Camp David]
Blair Contact-
Meet 
2003-03-27
Subsumption Destination[Mideast] Destination[Egypt] Bush Movement-
Transport 
2003-06-02
 
Complement 
Sentence 
[nine-year jail] 
Crime[corruption] 
Adjudicator[court] 
Place[Malaysia] 
Sentence 
[nine-year prison] 
Anwar 
Ibrahim 
Justice- 
Sentence 
2003-04-18
 
Table 1. Cross-document Event Aggregation Examples 
 
Therefore in order to maximize diversity, we 
merge any pair of events that have the same event 
type and involve the same centroid argument, via 
one of the operations in Table 1.  
3 Experimental Results 
We used 10 newswire texts from ACE 2005 train-
ing corpora as our test. For each test text we re-
trieved 25 related texts from English Topic 
Detection and Tracking (TDT-5)2 corpus which in 
total consists of 278,108 texts. The IE system ex-
tracted 179 event mentions including 140 Name 
arguments. We define an argument is correctly 
extracted if its event type, offsets, and role match 
any of the reference argument mentions. 
We found that after ranking with the global con-
fidence metrics, the top-ranked event arguments 
are substantially more accurate than the arguments 
as a whole: the overall accuracy without ranking is 
about 53%; but after ranking the top 85 arguments 
(61% of total) get accuracy above 70% and the top 
116 arguments (83% of total) are above 60% accu-
racy. It suggests that aggregating and ranking 
events according to global evidence can enable 
users to access salient and accurate information 
rapidly.  
4 Demonstration 
In this section we will demonstrate the results on 
all the documents in the English TDT5 corpus. In 
total 7962 person entities are identified as centroid 
arguments. The offline processing takes about 
three hours on a single PC. The real time browsing 
only takes one second in a standard web browser. 
                                                          
2 http://projects.ldc.upenn.edu/TDT5/ 
Figure 1 and Figure 2 present the temporal and 
spatial event chains involving the top 5 centroid 
arguments: ?Bush?, ?Arafat?, ?Taylor?, ?Saddam? 
and ?Abbas?. The events involving each centroid 
are ordered on a time line (Figure 1) and associated 
with their corresponding geographical codes in a 
map (Figure 2).  
The users can drag the timeline and map to 
browse the events. In addition, the aggregated 
event arguments are indexed and allow fast cen-
troid searching. Each argument is also labeled by 
its global confidence, language sources, and linked 
to its context sentences and other event chains it is 
involved. We omit these details in these screen-
shots. 
 
 
 
Figure 1. Temporal Person Tracking 
 
3
 
Figure 2. Spatial Person Tracking 
5 Related Work 
Recently there has been heightened interest in dis-
covering temporal event chains. For example, 
Bethard and Martin (2008) applied supervised 
learning to classify temporal and causal relations 
simultaneously. Chambers and Jurafsky (2008) 
extracted narrative event chains based on common 
protagonists. In this paper we import these ideas 
into IE while take into account some major differ-
ences. Following the original idea of centering 
(Grosz et al, 1995) and the approach of centering 
events involving protagonists (Chambers and Ju-
rafsky, 2008), we introduce a new concept of ?cen-
troid arguments? to represent those entities which 
are involved in all kinds of salient events fre-
quently. We operate cross-document instead of 
within-document, which requires us to resolve 
more conflicts and ambiguities. In addition, we 
study the temporal and spatial linking task on top 
of IE results. In this way we extend the representa-
tion of each node in the chains to a structured ag-
gregated event including fine-grained information 
such as event types, arguments and their roles. 
6 Conclusion and Future Work  
In this paper we described several new modes for 
browsing and searching a large collection of news 
articles, and demonstrated a system implementing 
these modes. We introduced ranking methods into 
IE, so that the extracted events are connected into 
temporal and spatial chains and presented to the 
user in an order of salience. We believe these new 
forms of presentation are likely to be highly bene-
ficial, especially to users whose native language is 
not English, by distilling the information landscape 
contained in the large collection of daily news arti-
cles ? making more information sources accessible 
and useful to them.  
On the other hand, for the users searching news 
about particular person entities, our system can 
suggest a list of centroid event arguments as key 
words, and provide a brief story by presenting all 
connected events. We believe this will signifi-
cantly speed up text comprehension. In this paper 
we only demonstrated the results for person enti-
ties, but this system can be naturally extended to 
other entity types, such as company names to track 
their start/end/acquire/merge activities. In addition, 
we plan to automatically adjust cross-document 
event aggregation operations according to specific 
compression ratios provided by the users.  
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program.  
References  
Steven Bethard and James H. Martin. 2008. Learning 
semantic links from a corpus of parallel temporal 
and causal relations. Proc. ACL-HLT 2008. 
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised Learning of Narrative Event Chains. Proc. 
ACL 2008. 
Barbara Grosz, Aravind Joshi, and Scott Weinstein. 
1995. Centering: A Framework for Modelling the 
Local Coherence of Discourse. Computational Lin-
guistics, 2(21), 1995. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction Through Unsupervised Cross-document 
Inference. Proc. ACL 2008. 
Danielle S McNamara. 2001. Reading both High-
coherence and Low-coherence Texts: Effects of Text 
Sequence and Prior Knowledge. Canadian Journal 
of Experimental Psychology. 
Trevor Strohman, Donald Metzler, Howard Turtle and 
W. Bruce Croft. 2005. Indri: A Language-model 
based Search Engine for Complex Queries (ex-
tended version). Technical Report IR-407, CIIR, 
Umass Amherst, US. 
4
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 27?35,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Cross-lingual Predicate Cluster Acquisition to Improve  
Bilingual Event Extraction by Inductive Learning 
 
Heng Ji 
Computer Science Department 
Queens College and The Graduate Center 
The City University of New York 
hengji@cs.qc.cuny.edu 
 
  
 
 
Abstract 
In this paper we present two approaches to 
automatically extract cross-lingual predi-
cate clusters, based on bilingual parallel 
corpora and cross-lingual information ex-
traction. We demonstrate how these clus-
ters can be used to improve the NIST 
Automatic Content Extraction (ACE) event 
extraction task1. We propose a new induc-
tive learning framework to automatically 
augment background data for low-
confidence events and then conduct global 
inference. Without using any additional 
data or accessing the baseline algorithms 
this approach obtained significant im-
provement over a state-of-the-art bilingual 
(English and Chinese) event extraction sys-
tem. 
1 Introduction 
Event extraction, the ?classical? information extrac-
tion (IE) task, has progressed from Message Un-
derstanding Conference (MUC)-style single 
template extraction to the more comprehensive 
multi-lingual Automatic Content Extraction (ACE) 
extraction including more fine-grained types. This 
extension has made event extraction more widely 
applicable in many NLP tasks including cross-
lingual document retrieval (Hakkani-Tur et al, 
2007) and question answering (Schiffman et al, 
2007). Various supervised learning approaches 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
have been explored for ACE multi-lingual event 
extraction (e.g. Grishman et al, 2005; Ahn, 2006; 
Hardy et al, 2006; Tan et al, 2008; Chen and Ji, 
2009). All of these previous literatures showed that 
one main bottleneck of event extraction lies in low 
recall. It?s a challenging task to recognize the dif-
ferent forms in which an event may be expressed, 
given the limited amount of training data. The goal 
of this paper is to improve the performance of a 
bilingual (English and Chinese) state-of-the-art 
event extraction system without accessing its inter-
nal algorithms or annotating additional data. 
As for a separate research theme, extensive 
techniques have been used to produce word clus-
ters or paraphrases from large unlabeled corpora 
(Brown et al, 1990; Pereira et al, 1993; Lee and 
Pereira, 1999, Barzilay and McKeown, 2001; Lin 
and Pantel, 2001; Ibrahim et al, 2003; Pang et al, 
2003). For example, (Bannard and Callison-Burch, 
2005) and (Callison-Burch, 2008) described a 
method to extract paraphrases from largely avail-
able bilingual corpora. The resulting clusters con-
tain words with similar semantic information and 
therefore can be useful to augment a small amount 
of annotated data. We will automatically extract 
cross-lingual predicate clusters using two different 
approaches based on bilingual parallel corpora and 
cross-lingual IE respectively; and then use the de-
rived clusters to improve event extraction. 
We propose a new learning method called in-
ductive learning to exploit the derived predicate 
clusters. For each test document, a background 
document is constructed by gradually replacing the 
low-confidence events with the predicates in the 
same cluster. Then we conduct cross-document 
inference technique as described in (Ji and Grish-
27
man, 2008) to improve the performance of event 
extraction. This inductive learning approach 
matches the procedure of human knowledge acqui-
sition and foreign language education: analyze in-
formation from specific examples and then 
discover a pattern or draw a conclusion; attempt 
synonyms to convey/learn the meaning of an intri-
cate word.  
The rest of this paper is structured as follows. 
Section 2 describes the terminology used in this 
paper. Section 3 presents the overall system archi-
tecture and the baseline system. Section 4 then de-
scribes in detail the approaches of extracting cross-
lingual predicate clusters. Section 5 describes the 
motivations of using cross-lingual clusters to im-
prove event extraction. Section 6 presents an over-
view of the inductive learning algorithm. Section 7 
presents the experimental results. Section 8 com-
pares our approach with related work and Section 9 
then concludes the paper and sketches our future 
work. 
2 Terminology 
The event extraction task we are addressing is that 
of ACE evaluations. ACE defines the following 
terminology: 
 
entity: an object or a set of objects in one of the 
semantic categories of interest 
mention: a reference to an entity (typically, a 
noun phrase) 
event trigger: the main word which most clearly 
expresses an event occurrence 
event arguments: the mentions that are in-
volved in an event (participants) 
event mention: a phrase or sentence within 
which an event is described, including trigger 
and arguments 
 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 distinct event types. 
For example, for a sentence ?Barry Diller on 
Wednesday quit as chief of Vivendi Universal En-
tertainment?, the event extractor should detect all 
the following information: a ?Personnel_End-
Position? event mention, with ?quit? as the trigger 
word, ?chief? as an argument with a role of  ?posi-
tion?, ?Barry Diller? as the person who quit the 
position, ?Vivendi Universal Entertainment? as the 
organization, and the time during which the event 
happened is ?Wednesday?. 
3 Approach Overview 
3.1 System Pipeline 
Figure 1 depicts the general procedure of our ap-
proach. The set of test event mentions is improved 
by exploiting cross-lingual predicate clusters.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. System Overview 
 
The following section 3.2 will give more details 
about the baseline bilingual event tagger. Then we 
will present the predicate cluster acquisition algo-
rithm in section 4 and the method of exploiting 
clusters for event extraction in section 6. 
3.2 A Baseline Bilingual Event Extraction 
System 
We use a state-of-the-art bi-lingual event extrac-
tion system (Grishman et al, 2005; Chen and Ji, 
2009) as our baseline. The system combines pat-
tern matching with a set of Maximum Entropy 
classifiers: to distinguish events from non-events; 
Inductive Learning 
Cross-lingual Predicate 
Cluster Acquisition 
Test 
Document
Baseline 
Event Extraction
Predicate Clusters 
Unlabeled 
Corpora
Cross-lingual 
IE 
Background
Document
Low-confidence 
Event  
Replacement 
Cross-document
Inference
Test Events
Parallel 
Corpora
Alignment 
Based Clustering
Baseline 
Event Extraction
Background 
Events 
Improved
Test Events
28
to classify events by type and subtype; to distin-
guish arguments from non-arguments; to classify 
arguments by argument role; and given a trigger, 
an event type, and a set of arguments, to determine 
whether there is a reportable event mention. In ad-
dition, the Chinese system incorporates some lan-
guage-specific features to address the problem of 
word segmentation (Chen and Ji, 2009). 
4 Cross-lingual Predicate Cluster Acqui-
sition 
We start from two different approaches to extract 
cross-lingual predicate clusters, based on parallel 
corpora and cross-lingual IE techniques respec-
tively. 
4.1 Acquisition from Bilingual Parallel Cor-
pora 
In the first approach, we take use of the 852 Chi-
nese event trigger words in ACE05 training cor-
pora as our ?anchor set?. For each Chinese trigger, 
we search its automatically aligned English words 
from a Chinese-English parallel corpus including 
50,000 sentence pairs (part of Global Autonomous 
Language Exploitation Y3 Machine Translation 
training corpora) to construct an English predicate 
cluster. The word alignment was obtained by run-
ning Giza++ (Och and Ney, 2003). In each cluster 
we record the frequency of each unique English 
word. Then we conduct the same procedure in the 
other direction to construct Chinese predicate clus-
ters anchored by English triggers. 
State-of-the-art Chinese-English word alignment 
error rate is about 40% (Deng and Byrne, 2005). 
Therefore the resulting cross-lingual clusters in-
clude a lot of word alignment errors. In order to 
address this problem, we filter the clusters by only 
keeping those predicates including the original 
predicate forms in ACE training data or Eng-
lish/Chinese Propbank (Palmer et al, 2005; Xue 
and Palmer, 2009).  
4.2 Acquisition from Cross-lingual IE 
Based on the intuition that Machine Translation 
(MT) may translate a Chinese trigger word into 
different English words in different contexts, we 
employ the second approach using cross-lingual IE 
techniques (Hakkani-Tur et al, 2007) on TDT5 
Chinese corpus to generate more clusters.  We ap-
ply the following two cross-lingual IE pipelines: 
 
Chinese IE_MT: Apply Chinese IE on the Chinese 
texts to get a set of Chinese triggers ch-trigger-set1, 
and then use word alignments to translate (project) 
ch-trigger-set1 into a set of English triggers en-
trigger-set1; 
 
MT_English IE: Translate Chinese texts into Eng-
lish, and then apply English IE on the translated 
texts to get a set of English triggers en-trigger-set2. 
 
For any Chinese trigger ch-trigger in ch-trigger-
set1, if its corresponding translation en-trigger in 
en-trigger-set1 is the same as that in en-trigger-
set2, then we add en-trigger into the cluster an-
chored by ch-trigger.  
  We apply the English and Chinese IE systems 
as described in (Grishman et al, 2005; Chen and Ji, 
2009). Both cross-lingual IE pipelines need ma-
chine translation to translate Chinese documents 
(for English IE) or project the extraction results 
from Chinese IE into English. We use the RWTH 
Aachen Chinese-to-English statistical phrase-based 
machine translation system (Zens and Ney, 2004) 
for these purposes.  
4.3 Derived Cross-lingual Predicate Clusters 
Applying the above two approaches we obtained 
438 English predicate clusters and 543 Chinese 
predicate clusters. 
For example, for a trigger ??(injure)?, we can 
get the following two predicate clusters with their 
frequency in the parallel corpora:  
 
?? {injured:99 injuries:96 injury:76 
 wounded:38 wounding:28 injuring:14 wounds:7 
killed:4 died:2 mutilated:1 casualties:1 chop:1 kill-
ing:1 shot:1}.  
 
injured ? {??:1624 ??:102 ?:99 ??:29 ?
?:23 ?:12 ??:10 ??:6 ??:3 ??:2 ??:1
?:1 ??:1 ??:1 ??:1 ??:1 ??:1 ??:1 } 
 
We can see that the predicates in the same clus-
ter are not restrictedly synonyms, but they were 
generated as alternative translations for the same 
word and therefore represent similar meanings. 
More importantly, these triggers vary from very 
common ones such as ?injured? to rare words such 
as ?mutilate?. This indicates how these clusters can 
aid extracting low-confidence events: when decid-
ing whether a word ?mutilate? indicates a ?Life-
29
Injure? event in a certain context, we can replace it 
with other predicates in the same cluster and may 
provide us more reliable overall evidence. 
Figure 2 presents the distribution of clusters 
which include more than one predicate.  
 
 
 
Figure 2. Cluster Size Distribution 
 
We can see that most clusters include 2-9 predi-
cates in both English and Chinese. However on 
average English clusters include more predicates. 
In addition, there are many more singletons in 
Chinese (232) than in English (101). This indicates 
that Chinese event triggers are more ambiguous. 
5 Motivation of Using Cross-lingual Clus-
ters for Event Extraction 
After extracting cross-lingual predicate clusters, 
we can combine the evidence from all the predi-
cates in each cluster to adjust the probabilities of 
event labeling. In the following we present some 
examples in both languages to demonstrate this 
motivation. 
5.1 Improve Rare Trigger Labeling  
Due to the limited training data, many trigger 
words only appear a few times as a particular type 
of event. This data sparse problem directly leads to 
the low recall of trigger labeling. But exploiting 
the evidence from other predicates in the same 
cluster may boost the confidence score of the can-
didate event. We present two examples as follows. 
 
 
 
(1) English Example 1 
 
For example, ?blown up? doesn?t appear in the 
training data as a ?Conflict-Attack? event, and so it 
cannot be identified in the following test sentence. 
However, if we replace it with other predicates in 
the same cluster, the system can easily identify 
?Conflict-Attack? events in the new sentences with 
high confidence values: 
 
(a) Test Sentence:  
Identified as  ?Conflict-Attack? Event with Confi-
dence=0: 
 
He told AFP that Israeli intelligence had been deal-
ing with at least 40 tip-offs of impending attacks 
when the Haifa bus was blown up. 
 
(b) Cross-lingual Cluster 
?? ? { blown up:4 bombing:3 blew:2 destroying:1 
destroyed:1 } 
 
(c) Replaced Sentences 
Identified as ?Conflict-Attack? Event with Confi-
dence=0.799: 
 
He told AFP that Israeli intelligence had been deal-
ing with at least 40 tip-offs of impending attacks 
when the Haifa bus was destroyed. 
? 
 
(2) Chinese Example 1 
 
Chinese predicate clusters anchored by English 
words can also provide external evidence for event 
identification. For example, the trigger word ??? 
(release/parole)? appears rarely in the Chinese 
training data but in most cases it can be replaced 
by a more frequent trigger ???(release)? to rep-
resent the same meaning. Therefore by combining 
the evidence from ???? we can enhance the con-
fidence value of identifying ???? as a  ?Justice-
Release_Parole? event. For example, 
 
(a) Test Sentence:  
Identified as ?Justice-Release_Parole? Event with 
Confidence=0: 
 
????????????????????
?. ? (This suspect was released because of the vio-
lation case but committed a felony again.) 
 
 
30
(b) Cross-lingual Cluster 
releasing ? {??:4 ??:1 } 
 
(c) Replaced Sentences 
Identified as ?Justice-Release_Parole? Event with 
Confidence=0.964: 
 
?????????????????????. 
? 
 
5.2 Improve Frequent Trigger Labeling  
On the other hand, some common words are highly 
ambiguous in particular contexts. But the other 
less-ambiguous predicates in the clusters can help 
classify event types more accurately. 
 
(1) English Example 2 
 
For example, in the following sentence the ?Per-
sonnel-End_Position? event is missing because 
?step? doesn?t indicate any ACE events in the 
training data. However, after replacing ?step? with 
other prediates such as ?quit?, the system can iden-
tify the event more easily: 
 
(a) Test Sentence:  
Identified as ?Personnel-End_Position? Event 
with Confidence=0: 
 
Barry Diller on Wednesday step from chief of Vivendi 
Universal Entertainment, the entertainment unit of 
French giant Vivendi Universal. 
 
(b) Cross-lingual Cluster 
?? ? { resign:6 step:5 quit:3} 
 
(c) Replaced Sentences 
Classified as ?Personnel-End_Position? Event 
with Confidence=0.564: 
 
Barry Diller on Wednesday quit from chief of Vivendi 
Universal Entertainment, the entertainment unit of 
French giant Vivendi Universal. 
? 
 
(2) Chinese Example 2 
 
Some single-character Chinese predicates can rep-
resent many different event types in different con-
texts. For example, the word ??? appears in 27 
different predicate clusters, representing the mean-
ing of hit/call/strike/form/take/draw etc. Therefore 
we can take use of other less ambiguous predicates 
in these clusters to adjust the likelihood of event 
classification.  
For example, in the following test sentence, the 
word ??? indicates two different event types. If 
we replace these words with other predicates, we 
can classify them into different event types more 
accurately based on the evidence from replaced 
predicates and contexts. 
 
(a) Test Sentence:  
Event Classification for trigger word ???: 
 
?????????? (?call?, Phone-Write event 
with confidence 0) ?????????????
??? 10 ???????? (?attacked/killed?, 
Conflict-Attack event with confidence 0.528)???
?????(?attacked?, Conflict-Attack event with 
confidence 0.946)???????????(Several 
days ago the Captain called  urgent telegraphs to ask 
for help, expressing that the boat pilot Cai Mingzhi 
was already killed by mainland fishermen and he 
himself was assaulted and duressed to the mainland.) 
 
 
(b) Cross-lingual Cluster 
 
call? {???:6 ??:6 ?:1 ??:1 } 
 
attack?{??:564 ??:110 ??:114 ??:24 ?
?:15 ??:15 ??:15 ?:8 ?:6 ??:6 ??:5 ?
?:4 ??:3 ??:3 ??:2 ??:2 ?:2 ??:2 ?
?:2 ??:2 ??:1 ??:1 ??:1 ??:1 ??:1 
?:1 ??:1 ?:1 } 
 
(c) Replaced Sentences 
Event Classification for trigger word ??? with 
higher confidence: 
 
???????????  (?call?, Phone-Write 
event with confidence 0.938) ?????????
? ? ? ? ? ? ? 10 ? ? ? ? ? ? ? ?
(?attacked/killed?, Conflict-Attack event with confi-
dence 0.583)????????(?attacked?, Con-
flict-Attack event with confidence 0.987)?????
?????? 
? 
 
Based on the above motivations we propose to 
incorporate cross-lingual predicate clusters to re-
fine event identification and classification. In order 
31
to exploit these clusters effectively, we shall gen-
erate additional background data and conduct 
global confidence. The sections below will present 
the detailed algorithms. 
6 Inductive Learning 
We design a framework of inductive learning to 
incorporate the derived predicate clusters. The 
general idea of inductive learning is to analyze in-
formation from all kinds of specific examples until 
we can draw a conclusion. Since the main goal of 
our approach is to improve the recall of event ex-
traction, we shall focus on those events generated 
by the baseline tagger with low confidence. For 
those events we automatically generate back-
ground documents using the predicate clusters (de-
tails in section 6.1) and then conduct global 
inference between each test document and its 
background documents (section 6.2).  
6.1 Background Document Generation 
For each event mention in a test document, the 
baseline event tagger produces the following local  
confidence value: 
 
? LConf(trigger, etype): The probability of a 
string trigger indicating an event mention with 
type etype in a context sentence S; 
 
If LConf(trigger, etype) is lower than a threshold, 
and it belongs to a predicate cluster C,  we create 
an additional background document BD by: 
 
? For each predicatei ? C, we replace trigger 
with predicatei in S to generate new sentence 
S?, and add S? into BD.    
6.2 Global Inference 
For each background document BD, we apply the 
baseline event extraction and get a set of back-
ground events. We then apply the cross-document 
inference techniques as described in (Ji and 
Grishman, 2008) to improve trigger and argument 
labeling performance by favoring interpretation 
consistency across the test events and background 
events. 
This approach is based on the premise that many 
events will be reported multiple times from differ-
ent sources in different forms. This naturally oc-
curs in the test document and the background 
document because they include triggers from the 
same predicate cluster. 
By aggregating events across each pair of test 
document TD and background document BD, we 
conduct the following statistical global inference: 
 
? to remove triggers and arguments with low 
confidence in TD and BD; 
? to adjust trigger and argument identification 
and classification to achieve consistency across 
TD and BD. 
 
In this way we can propagate highly consistent 
and frequent triggers and arguments with high 
global confidence to override other, lower confi-
dence, extraction results.  
7 Experimental Results 
7.1 Data and Scoring Metric 
We used ACE2005 English and Chinese training 
corpora to evaluate our approach. Table 1 shows 
the number of documents used for training, devel-
opment and blind testing. 
 
Language Training 
Set 
Development 
Set 
Test Set
English 525 33 66 
Chinese 500 10 40 
 
Table 1. Number of Documents 
 
We define the following standards to determine 
the correctness of an event mention: 
 
? A trigger is correctly identified if its position 
in the document matches a reference trigger. 
? A trigger is correctly identified and classified 
if its event type and position in the document 
match a reference trigger. 
? An argument is correctly identified if its event 
type and position in the document match any 
of the reference argument mentions. 
? An argument is correctly identified and classi-
fied if its event type, position in the document, 
and role match any of the reference argument 
mentions. 
 
 
 
  
32
Trigger  
Identification 
+Classification 
Argument  
Identification 
Argument  
Identification 
+Classification 
             Performance 
 
Language/System 
P R F P R F 
Argument 
Classification 
Accuracy 
P R F 
Baseline 67.8 53.5 59.8 49.3 31.4 38.3 88.2 43.5 27.7 33.9 
English After Using  
Cross-lingual 
Predicate Clusters 
69.2 59.4 63.9 51.7 32.7 40.1 89.6 46.3 29.3 35.9
Baseline 58.1 47.2 52.1 46.2 33.7 39.0 95.0 43.9 32.0 37.0 
Chinese After Using  
Cross-lingual  
Predicate Clusters 
60.2 52.6 56.1 46.8 36.7 41.1 95.6 44.7 35.1 39.3
 
Table 2. Overall Performance on Blind Test Set (%) 
 
7.2 Confidence Metric Thresholding 
Before blind testing we select the thresholds for the 
trigger confidence LConf(trigger, etype) as defined 
in section 6.1 by optimizing the F-measure score of 
on the development set. Figure 3 shows the effect 
on precision and recall of varying the threshold for 
inductive learning using cross-lingual predicate 
clusters. 
 
 
 
Figure 3. Trigger Labeling Performance with 
Inductive Learning Confidence Thresholding on 
English Development Set 
 
We can see that the best performance on the de-
velopment set can be obtained by selecting thresh-
old 0.6, achieving 9.4% better recall with a little 
loss in precision (0.26%) compared to the baseline 
(with threshold=0) . Then we apply this threshold 
value directly for blind test. This optimizing pro-
cedure is repeated for Chinese as well. 
7.3 Overall Performance 
Table 2 shows the overall Precision (P), Recall (R) 
and F-Measure (F) scores for the blind test set.  
For both English and Chinese, the inductive 
learning approach using cross-lingual predicate 
clusters provided significant improvement over the 
baseline event extraction system (about 4% abso-
lute improvement on trigger labeling and 2%-2.3% 
on argument labeling). The most significant gain 
was provided for the recall of trigger labeling ? 
5.9% absolute improvement for English and 5.4% 
absolute improvement for Chinese. 
Surprisingly this approach didn?t cause any loss 
in precision. In fact small gains were obtained on 
precision for both languages. This indicates that 
cross-lingual predicate clusters are effective at ad-
justing the confidence values so that the events 
were not over-generated. The refined event trigger 
labeling also directly yields better performance in 
argument labeling. 
We conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on a document basis. The re-
sults show that for both languages the improve-
ment using cross-lingual predicate clusters is 
significant at a 99.7% confidence level for trigger 
labeling and a 96.4% confidence level for argu-
ment labeling. 
7.4 Discussion 
For comparison we attempted a self-training ap-
proach: adding high-confidence events in the test 
set back as additional training data and re-train the 
event tagger. This produced 1.7% worse F-measure 
score for the English development set. It further 
33
proves that using the test set itself is not enough, 
we need to explore new predicates to serve as 
background evidence. 
In addition we also applied a bootstrapping ap-
proach using relevant unlabeled data and obtained 
limited improvement ? about 1.6% F-measure gain 
for English. As Ji and Grishman (2006) pointed out, 
both self-training and bootstrapping methods re-
quire good data selection scheme. But not for any 
test set we can easily find relevant unlabeled data. 
Therefore the approach presented in this paper is 
less expensive ? we can automatically generate 
background data while introducing new evidence. 
An alternative way of incorporating the cross-
lingual predicate clusters would follow (Miller et 
al., 2004), namely encoding the cluster member-
ship as an additional feature in the supervised-
learning procedure of the baseline event tagger. 
However in the situation where we cannot directly 
change the algorithms of the baseline system, our 
approach of inductive learning is more flexible. 
8 Related Work 
Our approach of extracting predicate clusters is 
related to some prior work on paraphrase or word 
cluster discovery, either from mono-lingual paral-
lel corpora (e.g. Barzilay and McKeown, 2001; Lin 
and Pantel, 2001; Ibrahim et al, 2003; Pang et al, 
2003) or cross-lingual parallel corpora (e.g. Ban-
nard and Callison-Burch, 2005; Callison-Burch, 
2008). Shinyama and Sekine (2003) presented an 
approach of extracting paraphrases using names, 
dates and numbers as anchors. Hasegawa et al 
(2004) described a paraphrase discovery approach 
based on clustering concurrent name pairs.  
Several recent studies have stressed the benefits 
of using paraphrases or word clusters to improve 
IE components. For example, (Miller et al, 2004) 
proved that word clusters can significantly improve 
English name tagging. The idea of using predicates 
in the same cluster for candidate trigger replace-
ment is similar to Ge et al(1998) who used local 
context replacement for pronoun resolution. To the 
best of our knowledge, our work presented the first 
experiment of using cross-lingual predicate para-
phrases for the ACE event extraction task.  
9 Conclusion and Future Work 
In this paper we described two approaches to ex-
tract cross-lingual predicate clusters, and designed 
a new inductive learning framework to effectively 
incorporate these clusters for event extraction. 
Without using any additional data or changing the 
baseline algorithms, we demonstrated that this 
method can significantly enhance the performance 
of a state-of-the-art bilingual event tagger. 
We have noticed that the current filtering 
scheme based on Propbank may be too restricted to 
keep enough informative predicates. In the future 
we will attempt incorporating POS tagging results 
and frequency information.  
In addition we will extend this framework to ex-
tract cross-lingual relation and name clusters to 
improve other IE tasks such as name tagging, rela-
tion extraction, event coreference and event trans-
lation. We are also interested in automatically 
discovering new event types (non-ACE event types) 
or more fine-grained subtypes/attributes for exist-
ing ACE event types from the derived predicate 
clusters. 
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program.  
References  
David Ahn. 2006. The stages of event extraction. Proc. 
COLING/ACL 2006 Workshop on Annotating and 
Reasoning about Time and Events. Sydney, Australia. 
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proc. ACL 
2005.  
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. Proc. 
ACL 2001. 
Peter F. Brown, Vinvent J. Della pietra, Peter V. 
deSouza, Jenifer C. Lai, Robert L. Mercer. 1990. 
Class-based N-gram Models of Natural Language. 
Computational Linguistics.  
Chris Callison-Burch. 2008. Syntactic Constraints on 
Paraphrases Extracted from Parallel Corpora. Proc. 
EMNLP 2008. Honolulu, USA. 
Zheng Chen and Heng Ji. 2009. Language Specific Is-
sue and Feature Exploration in Chinese Event Extrac-
tion. Proc. HLT-NAACL 2009. Boulder, Co.  
34
Yonggang Deng and William Byrne. 2005. HMM Word 
and Phrase Alignment for Statistical Machine Trans-
lation. Proc. HLT-EMNLP 2005. Vancouver, Can-
anda. 
Niyu Ge, John Hale and Eugene Charniak. 1998. A Sta-
tistical Approach to Anaphora Resolution. Proc. 
Sixth Workshop on Very Large Corpora 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
Proc. ACE 2005 Evaluation Workshop. Washington, 
US. 
Dilek Hakkani-Tur, Heng Ji and Ralph Grishman. 2007. 
Using Information Extraction to Improve Cross-
lingual Document Retrieval. Proc. RANLP2007 
workshop on Multi-source, Multilingual Information 
Extraction and Summarization. 
Hilda Hardy, Vika Kanchakouskaya and Tomek 
Strzalkowski. 2006. Automatic Event Classification 
Using Surface Text Features. Proc. AAAI06 Work-
shop on Event Extraction and Synthesis. Boston, 
Massachusetts. US. 
Takaaki Hasegawa, Satoshi Sekine and Ralph Grishman. 
2004. Discovering Relations among Named Entities 
from Large Corpora. Proc. ACL 2004. Barcelona, 
Spain.  
Ali Ibrahim, Boris Katz and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. Proc. ACL 2003. 
Heng Ji and Ralph Grishman. 2006. Data Selection in 
Semi-supervised Learning for Name Tagging. Proc.  
ACL 2006 Workshop on Information Extraction Be-
yond the Document. Sydney, Australia. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction Through Cross-document Inference. Proc. 
ACL 2008. Ohio, USA 
Lillian Lee and Fernando Pereira. 1999. Distributional 
Similarity Models: Clustering vs. Nearest Neighbors. 
Proc. ACL1999. pp. 33-40. 
Dekang Lin and Patrick Pantel. 2001. DIRT-Discovery 
of Inference Rules from Text. Proc. ACM SIGDD 
Conference on Knowledge Discovery and Data Min-
ing. 
Scott Miller, Jethran Guinness and Alex Zamanian.2004. 
Name Tagging with Word Clusters and Discrimina-
tive Training. Proc. HLT-NAACL2004. pp. 337-342. 
Boston, USA. 
Franz Josef Och and Hermann Ney. 2003. "A System-
atic Comparison of Various Statistical Alignment 
Models", Computational Linguistics, volume 29, 
number 1, pp. 19-51. 
Martha Palmer, Daniel Gildea and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics. Vol-
ume 31, Issue 1. pp. 71-106. 
Bo Pang, Kevin Knight and Daniel Marcu. 2003. Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences. 
Proc. HLT/NAACL 2003. 
Fernando Pereira, Naftali Tishby and Lillian Lee. 1993. 
Distributional Clustering of English Words. Proc. 
ACL1993. pp. 183-190. 
Barry Schiffman, Kathleen R. McKeown, Ralph Grish-
man and James Allan. 2007. Question Answering us-
ing Integrated Information Retrieval and Information 
Extraction. Proc. HLT-NAACL 2007. Rochester, US. 
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase 
Acquisition for Information Extraction. Proc. ACL 
2003 workshop on Paraphrasing (IWP 2003). 
Hongye Tan, Tiejun Zhao and Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their Argument 
Roles. Proc. Computer and Information Technology 
Workshops. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language 
Engineering, 15(1):143-172. 
Richard Zens and Hermann Ney. 2004. Improvements 
in Phrase-Based Statistical Machine Translation. In 
HLT/NAACL 2004. New York City, NY, US 
 
35
66 
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 66?74, 
Boulder, Colorado, June 2009. ? 2009 Association for Computational Linguistics 
Can One Language Bootstrap the Other:  
A Case Study on Event Extraction 
 
  
Zheng Chen Heng Ji 
The Graduate Center Queens College and The Graduate Center 
The City University of New York The City University of New York 
zchen1@gc.cuny.edu hengji@cs.qc.cuny.edu 
 
 
 
Abstract 
This paper proposes a new bootstrapping 
framework using cross-lingual information pro-
jection. We demonstrate that this framework is 
particularly effective for a challenging NLP task 
which is situated at the end of a pipeline and 
thus suffers from the errors propagated from up-
stream processing and has low-performance 
baseline. Using Chinese event extraction as a 
case study and bitexts as a new source of infor-
mation, we present three bootstrapping tech-
niques. We first conclude that the standard 
mono-lingual bootstrapping approach is not so 
effective. Then we exploit a second approach 
that potentially benefits from the extra informa-
tion captured by an English event extraction sys-
tem and projected into Chinese. Such a cross-
lingual scheme produces significant performance 
gain. Finally we show that the combination of 
mono-lingual and cross-lingual information in 
bootstrapping can further enhance the perfor-
mance. Ultimately this new framework obtained 
10.1% relative improvement in trigger labeling 
(F-measure) and 9.5% relative improvement in 
argument-labeling. 
1 Introduction 
Bootstrapping methods can reduce the efforts 
needed to develop a training set and have shown 
promise in improving the performance of many 
tasks such as name tagging (Miller et al, 2004; Ji 
and Grishman, 2006), semantic class extraction 
(Lin et al, 2003), chunking (Ando and Zhang, 
2005), coreference resolution (Bean and Riloff, 
2004) and text classification (Blum and Mitchell, 
1998). Most of these bootstrapping methods impli-
citly assume that: 
? There exists a high-accuracy ?seed set? or ?seed 
model? as the baseline; 
? There exists unlabeled data which is reliable 
and relevant to the test set in some aspects, e.g. 
from similar time frames and news sources; 
and therefore the unlabeled data supports the 
acquisition of new information, to provide new 
evidence to be incorporated to bootstrap the 
model and reduce the sparse data problem. 
? The seeds and unlabeled data won?t make the 
old estimates worse by adding too many incor-
rect instances. 
However, for some more comprehensive and 
challenging tasks such as event extraction, the per-
formance of the seed model suffers from the li-
mited annotated training data and also from the 
errors propagated from upstream processing such 
as part-of-speech tagging and parsing. In addition, 
simply relying upon large unlabeled corpora can-
not compensate for these limitations because more 
errors can be propagated from upstream processing 
such as entity extraction and temporal expression 
identification. 
Inspired from the idea of co-training (Blum and 
Mitchell, 1998), in this paper we intend to boot-
strap an event extraction system in one language 
(Chinese) by exploring new evidences from the 
event extraction system in another language (Eng-
lish) via cross-lingual projection. We conjecture 
that the cross-lingual bootstrapping for event ex-
traction can naturally fit the co-training model:  a 
same event is represented in two ?views? (de-
scribed in two languages). Furthermore, the cross-
lingual bootstrapping can benefit from the different 
sources of training data.  For example, the Chinese 
training corpus includes articles from Chinese new 
agencies in 2000 while most of English training 
data are from the US news agencies in 2003, thus
67 
 
English and Chinese event extraction systems have 
the nature of generating different results on parallel 
documents and may complement each other. In this 
paper, we explore approaches of exploiting the 
increasingly available bilingual parallel texts (bi-
texts).  
We first investigate whether we can improve a 
Chinese event extraction system by simply using 
the Chinese side of bitexts in a regular monolin-
gual bootstrapping framework. By gradually in-
creasing the size of the corpus with unlabeled data, 
we did not get much improvement for trigger labe-
ling and even observed performance deterioration 
for argument labeling. But then by aligning the 
texts at the word level, we found that the English 
event extraction results can be projected into Chi-
nese for bootstrapping and lead to significant im-
provement. We also obtained clear further 
improvement by combining mono-lingual and 
cross-lingual bootstrapping. 
The main contributions of this paper are two- 
fold. We formulate a new algorithm of cross-
lingual bootstrapping, and demonstrate its effec-
tiveness in a challenging task of event extraction; 
and we conclude that, for some applications be-
sides machine translation, effective use of bitexts 
can be beneficial.  
The remainder of the paper is organized as fol-
lows. Section 2 formalizes the event extraction task 
addressed in this paper. Section 3 discusses event 
extraction bootstrapping techniques. Section 4 re-
ports our experimental results. Section 5 presents 
related work. Section 6 concludes this paper and 
points out future directions. 
2 Event Extraction  
2.1 Task Definition and Terminology 
The event extraction that we address in this paper 
is specified in the Automatic Content Extraction 
(ACE) 1
? event trigger: the word that most clearly ex-
presses an event?s occurrence 
 program. The ACE 2005 Evaluation de-
fines the following terminology for the event ex-
traction task: 
? event argument:  an entity, a temporal expres-
sion or a value that plays a certain role in the 
event instance 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
? event mention: a phrase or sentence with a 
distinguished trigger and participant arguments 
 
The event extraction task in our paper is to 
detect certain types of event mentions that are indi-
cated by event triggers (trigger labeling), recog-
nize the event participants e.g., who, when, where, 
how (argument labeling) and merge the co-
referenced event mentions into a unified event 
(post-processing). In this paper, we focus on dis-
cussing trigger labeling and argument labeling. 
In the following example,  
Mike got married in 2008. 
The event extraction system should identify 
?married? as the event trigger which indicates the 
event type of ?Life? and subtype of ?Marry?. Fur-
thermore, it should detect ?Mike? and ?2008? as 
arguments in which ?Mike? has a role of ?Person? 
and ?2008? has a role of ?Time-Within?. 
2.2 A Pipeline of Event Extraction 
Our pipeline framework of event extraction in-
cludes trigger labeling, argument labeling and 
post-processing, similar to (Grishman et al, 2005), 
(Ahn, 2006) and (Chen and Ji, 2009). We depict 
the framework as Figure 1.  
 
 
Figure 1. Pipeline of Event Extraction 
 
The event extraction system takes raw docu-
ments as input and conducts some pre-processing 
steps. The texts are automatically annotated with
Argument labeling 
 
Trigger labeling 
Trigger classification 
Trigger identification 
Argument identification 
Pre-processing 
Argument classification 
Post-processing 
68 
 
word segmentation, Part-of-Speech tags, parsing 
structures, entities, time expressions, and relations. 
The annotated documents are then sent to the fol-
lowing four components. Each component is a 
classifier and produces confidence values; 
? Trigger identification: the classifier recognizes 
a word or a phrase as the event trigger. 
? Trigger classification: the classifier assigns an 
event type to an identified trigger.  
? Argument identification: the classifier recog-
nizes whether an entity, temporal expression or 
value is an argument associated with a particu-
lar trigger in the same sentence. 
? Argument classification: the classifier assigns 
a role to the argument.  
The post-processing merges co-referenced event 
mentions into a unified representation of event. 
2.3 Two Monolingual Event Extraction Sys-
tems 
We use two monolingual event extraction systems, 
one for English, and the other for Chinese. Both 
systems employ the above framework and use 
Maximum Entropy based classifiers. The corres-
ponding classifiers in both systems also share some 
language-independent features, for example, in 
trigger identification, both classifiers use the ?pre-
vious word? and ?next word? as features, however, 
there are some language-dependent features that 
only work well for one monolingual system, for 
example, in argument identification, the next word 
of the candidate argument is a good feature for 
Chinese system but not for English system. To il-
lustrate this, in the Chinese ??? (of) structure, the 
word ??? (of) strongly suggests that the entity on 
the left side of ??? is not an argument. For a spe-
cific example, in ???????? (The mayor of 
New York City), ????? (New York City) on the 
left side of ??? (of) cannot be considered as an 
argument because it is a modifier of the noun ??
??(mayor). Unlike Chinese, ?of? (???) appears 
ahead of the entity in the English phrase. 
Table 1 lists the overall Precision (P), Recall (R) 
and F-Measure (F) scores for trigger labeling and 
argument labeling in our two monolingual event 
extraction systems.  For comparison, we also list 
the performance of an English human annotator 
and a Chinese human annotator.   
Table 1 shows that event extraction is a difficult 
NLP task because even human annotators cannot 
achieve satisfying performance. Both monolingual 
systems relied on expensive human labeled data 
(much more expensive than other NLP tasks due to 
the extra tagging tasks of entities and temporal ex-
pressions), thus a natural question arises: can the  
monolingual system benefit from bootstrapping 
techniques with a relative small set of training data? 
The other question is: can a monolingual system 
benefit from the other monolingual system by 
cross-lingual bootstrapping? 
Performance 
 
System/ 
Human 
Trigger  
Labeling 
Argument 
Labeling 
P R F P R F 
English  
System 
64.3 59.4 61.8 49.2 34.7 40.7 
Chinese  
System 
78.8 48.3 59.9 60.6 34.3 43.8 
English  
Annotator 
59.2 59.4 59.3 51.6 59.5 55.3 
Chinese  
Annotator 
75.2 74.6 74.9 58.6 60.9 59.7 
Table 1.Performance of Two Monolingual Event 
Extraction Systems and Human Annotators 
3 Bootstrapping Event Extraction 
3.1 General Bootstrapping Algorithm 
Bootstrapping algorithms have attracted much at-
tention from researchers because a large number of 
unlabeled examples are available and can be uti-
lized to boost the performance of a system trained 
on a small set of labeled examples. The general 
bootstrapping algorithm is depicted in Figure 2, 
similar to (Mihalcea, 2004). 
Self-training and Co-training are two most 
commonly used bootstrapping methods. 
A typical self-training process is described as 
follows: it starts with a set of training examples 
and builds a classifier with the full integrated fea-
ture set. The classifier is then used to label an addi-
tional portion of the unlabeled examples. Among 
the resulting labeled examples, put the most confi-
dent ones into the training set, and re-train the clas-
sifier. This iterates until a certain condition is 
satisfied (e.g., all the unlabeled examples have 
been labeled, or it reaches a certain number of ite-
rations). 
Co-training(Blum and Mitchell, 1998) differs 
from self-training in that it assumes that the data 
can be represented using two or more separate
69 
 
?views? (thus the whole feature set is split into dis-
joint feature subsets) and each classifier can be 
trained on one view of the data. For each iteration, 
both classifiers label an additional portion of the 
unlabeled examples and put the most confident 
ones to the training set. Then the two classifiers are 
retrained on the new training set and iterate until a 
certain condition is satisfied. 
Both self-training and co-training can fit in the 
general bootstrapping process. If the number of 
classifiers is set to one, it is a self-training process, 
and it is a co-training process if there are two dif-
ferent classifiers that interact in the bootstrapping 
process.  
 
Figure 2. General Bootstrapping Algorithm. 
In the following sections, we adapt the boot-
strapping techniques discussed in this section to a 
larger scale (system level). In other words, we aim 
to bootstrap the overall performance of the system 
which may include multiple classifiers, rather than 
just improve the performance of a single classifier 
in the system. It is worth noting that for the pipe-
line event extraction depicted in Section 2.2, there 
are two major steps that determine the overall sys-
tem performance: trigger labeling and argument 
labeling. Furthermore, the performance of trigger 
labeling can directly affect the performance of ar-
gument labeling because the involving arguments 
are constructed according to the trigger. If a trigger 
is wrongly recognized, all the involving arguments 
will be considered as wrong arguments. If a trigger 
is missing, all the attached arguments will be con-
sidered as missing arguments. 
3.2 Monolingual Self-training 
It is rather smooth to adapt the idea of traditional 
self-training to monolingual self-training if we 
consider our monolingual event extraction system 
as a black box or even a single classifier that de-
termines whether an event combining the result of 
trigger labeling and argument labeling is a reporta-
ble event.  
Thus the monolingual self-training procedure for 
event extraction is quite similar with the one de-
scribed in Section 3.1. The monolingual event ex-
traction system is first trained on a starting set of 
labeled documents, and then tag on an additional 
portion of unlabeled documents. Note that in each 
labeled document, multiple events could be tagged 
and confidence score is assigned to each event. 
Then the labeled documents are added into the 
training set and the system is retrained based on 
the events with high confidence. This iterates until 
all the unlabeled documents have been tagged. 
3.3 Cross-lingual Co-Training 
We extend the idea of co-training to cross-lingual 
co-training. The intuition behind cross-lingual co-
training is that the same event has different ?views? 
described in different languages, because the lexi-
cal unit, the grammar and sentence construction 
differ from one language to the other. Thus one 
monolingual event extraction system probably uti-
lizes the language dependent features that cannot 
work well for the other monolingual event extrac-
tion systems. Blum and Mitchell (1998) derived 
PAC-like guarantees on learning under two as-
sumptions: 1) the two views are individually suffi-
cient for classification and 2) the two views are 
conditionally independent given the class. Ob-
viously, the first assumption can be satisfied in 
cross-lingual co-training for event extraction, since 
each monolingual event extraction system is suffi-
cient for event extraction task. However, we re-
serve our opinion on the second assumption. 
Although the two monolingual event extraction 
systems may apply the same language-independent 
features such as the part-of-speech, the next word 
and the previous word, the features are exhibited in 
their own context of language, thus it is too subjec-
tive to conclude that the two feature sets are or are
Input:  
L : a set of labeled examples,  
U : a set of unlabeled examples   
{ iC }: a set of classifiers 
Initialization:  
Create a pool U ? of examples by choosing P
random examples from U  
Loop until a condition is satisfied (e.g., U ? ? , or 
iteration counter reaches a preset number I ) 
? Train each classifier iC  on L , and label 
the examples in U ?  
? For each classifier iC ,select the most con-
fidently labeled examples (e.g., the confi-
dence score is above a preset threshold ?
or the top  K ) and add them to L  
? Refill U ?with examples from U , and keep 
the size of U ?  as constant P  
70 
 
not conditionally independent. It is left to be an 
unsolved issue which needs further strict analysis 
and supporting experiments. 
The cross-lingual co-training differs from tradi-
tional co-training in that the two systems in cross-
lingual co-training are not initially trained from the 
same labeled data. Furthermore, in the bootstrap-
ping phase, each system only labels half portion of 
the bitexts in its own language. In order to utilize 
the labeling result by the other system, we need to 
conduct an extra step named cross-lingual projec-
tion that transforms tagged events from one lan-
guage to the other. 
3.3.1 A Cross-lingual Co-training Algorithm 
The algorithm for cross-lingual co-training is de-
picted in Figure 3. 
 
 
Figure 3. Cross-lingual Co-training Algorithm 
3.3.2 Cross-lingual Semi-co-training 
Cross-lingual semi-co-training is a variation of 
cross-lingual co-training, and it differs from cross-
lingual co-training in that it tries to bootstrap only 
one system by the other fine-trained system. This 
technique is helpful when we have relatively large 
amount of training data in one language while we 
have scarce data in the other language.  
Thus we only need to make a small modification 
in the cross-lingual co-training algorithm so that it 
can soon be adapted to cross-lingual semi-co-
training, i.e., we retrain one system and do not re-
train the other. In this paper, we will conduct expe-
riments to investigate whether a fine-trained 
English event extraction system can bootstrap the 
Chinese event extraction system, starting from a 
small set of training data. 
3.3.3 Cross-lingual Projection 
Cross-lingual projection is a key operation in the 
cross-lingual co-training algorithm. In the case of 
event extraction, we need to project the triggers 
and the participant arguments from one language 
into the other language according to the alignment 
information provided by bitexts. Figure 4 shows an 
example of projecting an English event into the 
corresponding Chinese event. 
Before projection: 
<event ID="chtb_282-EV1" TYPE="Contact" SUBTYPE="Meet"> 
<event_mention ID="chtb_282-EV1-1" p="1.000"> 
       <trigger> 
            <charseq START="2259" END="2265">meeting</charseq> 
       </trigger> 
       <event_mention_argument ROLE="Entity" p="0.704" 
pRole="0.924"/> 
          <extent> 
             <charseq START="2238" END="2244">Gan Luo</charseq> 
          </extent> 
       </event_mention_argument> 
     </event_mention> 
</event> 
 
After projection: 
<event ID="chtb_282-EV1" TYPE="Contact" SUBTYPE="Meet"> 
<event_mention ID="chtb_282-EV1-1" p="1.000"> 
       <trigger> 
            <charseq START="454" END="455">??</charseq> 
       </trigger> 
       <event_mention_argument ROLE="Entity" p="0.704" 
pRole="0.924"/> 
          <extent> 
             <charseq START="449" END="450">??</charseq> 
          </extent> 
</event_mention_argument> 
    </event_mention> 
</event> 
Figure 4. An Example of Cross-lingual Projection
Input:  
1L : a set of labeled examples in language A 
2L : a set of labeled examples in language B 
U : a set of unlabeled bilingual examples  (bi-
texts) with alignment information 
{ 1 2,S S }: two monolingual systems, one for 
language A and the other for language B. 
Initialization:  
Create a pool U ? of examples by choosing P
random examples from U  
Loop until a condition is satisfied (e.g., U ? ? , 
or iteration counter reaches a preset number I ) 
? Train 1S on 1L and 2S on 2L  
? Use 1S to label the examples in U ?  (the por-
tion in Language A) and use 2S to label the 
examples in U ? (the portion in Language B) 
? For 1S , select the most confidently labeled 
examples (e.g., the confidence score is 
above a preset threshold ? or the top K ) , 
apply the operation of cross-lingual projec-
tion, transform the selected examples from 
Language A to Language B, and put them 
into 2L . The same procedure applies to 2S . 
? Refill U ?with examples from U , and keep 
the size of U ?  as constant P  
71 
 
4 Experiments and Results 
4.1 Data and Scoring Metric 
We used the ACE 2005 corpus to set up two mono-
lingual event extraction systems, one for English, 
the other for Chinese. 
The ACE 2005 corpus contains 560 English 
documents from 6 sources: newswire, broadcast 
news, broadcast conversations, weblogs, new-
sgroups and conversational telephone speech; 
meanwhile the corpus contains 633 Chinese docu-
ments from 3 sources: newswire, broadcast news 
and weblogs.  
We then use 159 texts from the LDC Chinese 
Treebank English Parallel corpus with manual 
alignment for our cross-lingual bootstrapping ex-
periments. 
We define the following standards to determine 
the correctness of an event mention: 
? A trigger is correctly labeled if its event type 
and offsets match a reference trigger. 
? An argument is correctly labeled if its event 
type, offsets, and role match any of the refer-
ence argument mentions. 
4.2 Monolingual Self-training on ACE 2005 
Data 
We first investigate whether our Chinese event 
extraction system can benefit from monolingual 
self-training on ACE data. We reserve 66 Chinese 
documents for testing purpose and set the size of 
seed training set to 100. For a single trial of the 
experiment, we randomly select 100 documents as 
training set and use the remaining documents as 
self-training data. For each iteration of the self-
training, we keep the pool size as 50, in other 
words, we always pick another 50 ACE documents 
to self-train the system. The iteration continues 
until all the unlabeled ACE documents have been 
tagged and thus it completes one trial of the expe-
riment. We conduct the same experiment for 100 
trials and compute the average scores.  
The most important motivation for us to conduct 
self-training experiments on ACE data is that the 
ACE data provide ground-truth entities and tem-
poral expressions so that we do not have to take 
into account the effects of propagated errors from 
upstream processing such as entity extraction and 
temporal expression identification. 
For one setting of the experiments, we set the 
confidence threshold to 0, in other words, we keep 
all the labeling results for retraining. The results 
are given in Figure 5 (trigger labeling) and Figure 
6 (argument labeling). It shows that when the 
number of self-trained ACE documents reaches 
450, we obtain a gain of 3.4% (F-Measure) above 
the baseline for trigger labeling and a gain of 1.4% 
for argument labeling. 
For the other setting of the experiments, we set 
the confidence threshold to 0.8, and the results are 
presented in Figure 7 and Figure 8. Surprisingly, 
retraining on the high confidence examples does 
not lead to much improvement. We obtain a gain 
of 3.7% above the baseline for trigger labeling and 
1.5% for argument labeling when the number of 
self-trained documents reaches 450. 
 
 
 
Figure 5. Self-training for trigger labeling  
(confidence threshold = 0) 
 
 
Figure 6. Self-training for argument labeling  
(confidence threshold= 0)
72 
 
 
Figure 7. Self-training for trigger labeling  
(confidence threshold = 0.8) 
 
 
Figure 8. Self-training for argument labeling  
(confidence threshold = 0.8)  
4.3 Cross-lingual Semi-co-training on Bitexts 
The experiments in Section 4.2 show that we can 
obtain gain in performance by monolingual self-
training on data with ground-truth entities and 
temporal expressions, but what if we do not have 
such ground-truth data, then how the errors propa-
gated from entity extraction and temporal expres-
sion identification will affect the overall 
performance of our event extraction system? And 
if these errors are compounded in event extraction, 
can the cross-lingual semi-co-training alleviate the 
impact? 
To investigate all these issues, we use 159 texts 
from LDC Chinese Treebank English Parallel cor-
pus to conduct cross-lingual semi-co-training.  The 
experimental results are summarized in Figure 9 
and Figure 10.  
 For monolingual self-training on the bitexts, we 
conduct experiments exactly as section 4.2 except 
that the entities are tagged by the IE system and the 
labeling pool size is set to 20. When the number of 
bitexts reaches 159, we obtain a little gain of 0.4% 
above the baseline for trigger labeling and a loss of 
0.1% below the baseline for argument labeling. 
The deterioration tendency of the self-training 
curve in Figure 10 indicates that entity extraction 
errors do have counteractive impacts on argument 
labeling. 
We then conduct the cross-lingual semi-co-
training experiments as follows: we set up an Eng-
lish event extraction system trained on a relative 
large training set (500 documents). For each trial 
of the experiment, we randomly select 100 ACE 
Chinese document as seed training set, and then it 
enters a cross-lingual semi-co-training process: for 
each iteration, the English system labels the Eng-
lish portions of the 20 bitexts and by cross-lingual 
projection, the labeled results are transformed into 
Chinese and put into the training set of Chinese 
system. From Figure 9 and Figure 10 we can see 
that when the number of bitexts reaches 159, we 
obtain a gain of 1.7% for trigger labeling and 0.7% 
for argument labeling. 
We then apply a third approach to bootstrap our 
Chinese system: during each iteration, the Chinese 
system also labels the Chinese portions of the 20 
bitexts. Then we combine the results from both 
monolingual systems using the following rules:  
? If the event labeled by English system is not 
labeled by Chinese system, add the event to 
Chinese system 
? If the event labeled by Chinese system is not 
labeled by English system, keep the event in 
the Chinese system 
? If both systems label the same event but with 
different event types and arguments, select the 
one with higher confidence 
From Figure 9 and Figure 10 we can see that this 
approach leads to even further improvement in per-
formance, shown as the ?Combined-labeled? 
curves. When the number of bitexts reaches 159, 
we obtain a gain of 3.1% for trigger labeling and 
2.1% for argument labeling.  
In order to check how robust our approach  
is, we conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on F-measures for all these 100 
trials. The results show that we can reject  
the hypotheses that the improvements using Cross-
lingual Semi-co-training were random at a 99.99% 
confidence level, for both trigger labeling and ar-
gument labeling. 
73 
 
 
Figure 9. Self-training, and Semi-co-training  
(English- labeled & Combined-labeled)  
for Trigger Labeling 
 
 
Figure 10. Self-training, and Semi-co-training  
(English- labeled & Combined-labeled)  
for Argument Labeling 
5 Related Work 
There is a huge literature on utilizing parallel cor-
pus for monolingual improvement. To our know-
ledge, it can retrace to (Dagan et.al 1991). We 
apologize to those whose work is not cited due to 
space constraints. The work described here com-
plements some recent research using bitexts or 
translation techniques as feedback to improve enti-
ty extraction. Huang and Vogel (2002) presented 
an effective integrated approach that can improve 
the extracted named entity translation dictionary 
and the entity annotation in a bilingual training 
corpus. Ji and Grishman (2007) expanded this idea 
of alignment consistency to the task of entity ex-
traction in a monolingual test corpus without refer-
ence translations, and applied sophisticated infe-
inference rules to enhance both entity extraction 
and translation. Zitouni and Florian (2008) applied 
English mention detection on translated texts and 
added the results as additional features to improve 
mention detection in other languages.  
In this paper we share the similar idea of import-
ing evidences from English with richer resources 
to improve extraction in other languages. However, 
to the best of our knowledge this is the first work 
of incorporating cross-lingual feedback to improve 
the event extraction task. More importantly, it is 
the first attempt of combining cross-lingual projec-
tion with bootstrapping methods, which can avoid 
the efforts of designing sophisticated inference 
rules or features. 
6 Conclusions and Future Work 
Event extraction remains a difficult task not only 
because it is situated at the end of an IE pipeline 
and thus suffers from the errors propagated from 
upstream processing, but also because the labeled 
data are expensive and thus suffers from data scar-
city. In this paper, we proposed a new co-training 
framework using cross-lingual information projec-
tion and demonstrate that the additional informa-
tion from English system can be used to bootstrap 
a Chinese event extraction system.  
To move a step forward, we would like to con-
duct experiments on cross-lingual co-training and 
investigate whether the two systems on both sides 
can benefit from each other. A main issue existing 
in cross-lingual co-training is that the cross-lingual 
projection may not be perfect due to the word 
alignment problem. In this paper, we used a corpus 
with manual alignment, but in the future we intend 
to investigate the effect of automatic alignment 
errors.  
We believe that the proposed cross-lingual boot-
strapping framework can also be applied to many 
other challenging NLP tasks such as relation ex-
traction. However, we still need to provide a theo-
retical analysis of the framework. 
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program.  
74 
 
References  
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-training. Proc. of 
the Workshop on Computational Learning Theory. 
Morgan Kaufmann Publishers. 
David Ahn. 2006. The stages of event extraction. Proc. 
COLING/ACL 2006 Workshop on Annotating and 
Reasoning about Time and Events. Sydney, Aus-
tralia. 
David Bean and Ellen Riloff. 2004. Unsupervised 
Learning of Contextual Role Knowledge for Corefe-
rence Resolution. Proc.  HLT-NAACL2004. pp. 297-
304. Boston, USA. 
Fei Huang and Stephan Vogel. 2002. Improved Named 
Entity Translation and Bilingual Named Entity Ex-
traction. Proc. ICMI 2002. Pittsburgh, PA, US. 
Heng Ji and Ralph Grishman. 2006. Data Selection in 
Semi-supervised Learning for Name Tagging. In 
ACL 2006 Workshop on Information Extraction 
Beyond the Document:48-55. Sydney, Australia.  
Heng Ji and Ralph Grishman. 2007. Collaborative Enti-
ty Extraction and Translation. Proc. International 
Conference on Recent Advances in Natural Lan-
guage Processing 2007. Borovets, Bulgaria. 
Ido Dagan, Alon Itai and Ulrike Schwall. 1991. Two 
languages are more informative than one. Proc. ACL 
1991. 
Imed Zitouni and Radu Florian. 2008. Mention Detec-
tion Crossing the Language Barrier. Proc. EMNLP. 
Honolulu, Hawaii. 
Michael Collins and Yoram Singer. 1999. Unsupervised 
Models for Named Entity Classification. Proc. of 
EMNLP/VLC-99. 
Rada Mihalcea. 2004. Co-training and self-training for 
word sense disambiguation. In Proceedings of the 
Conference on Computational Natural Language 
Learning (CoNLL-2004). 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Descrip-
tion. Proc. ACE 2005 Evaluation Workshop. Wash-
ington, US. 
Rie Ando and Tong Zhang. 2005. A High-Performance 
Semi-Supervised Learning Methods for Text Chunk-
ing. Proc.  ACL2005. pp. 1-8. Ann Arbor, USA 
Scott Miller, Jethran Guinness and Alex Zamanian.2004. 
Name Tagging with Word Clusters and Discrimina-
tive Training. Proc. HLT-NAACL2004. pp. 337-342. 
Boston, USA 
Winston Lin, Roman Yangarber and Ralph Grishman. 
2003. Bootstrapping Learning of Semantic Classes 
from Positive and Negative Examples. Proc.  ICML-
2003 Workshop on The Continuum from Labeled to 
Unlabeled Data. Washington, D.C. 
Zheng Chen and Heng Ji. 2009. Language Specific Is-
sue and Feature Exploration in Chinese Event Extrac-
tion. Proc. HLT-NAACL 2009. Boulder, Co.  
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 34?37,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Mining Name Translations from Comparable Corpora  
by Creating Bilingual Information Networks 
Heng Ji 
Computer Science Department,  Queens College and the Graduate Center 
The City University of New York,  New York, NY, 11367, USA 
hengji@cs.qc.cuny.edu 
 
 
 
Abstract 
This paper describes a new task to extract and 
align information networks from comparable 
corpora. As a case study we demonstrate the 
effectiveness of this task on automatically 
mining name translation pairs. Starting from a 
small set of seeds, we design a novel approach 
to acquire name translation pairs in a boot-
strapping framework. The experimental results 
show this approach can generate highly accu-
rate name translation pairs for persons, geo-
political and organization entities. 
1 Introduction 
Accurate name translation is crucial to many 
cross-lingual information processing tasks such 
as information retrieval (e.g. Ji et al, 2008). Re-
cently there has been heightened interest in dis-
covering name pairs from comparable corpora 
(e.g. Sproat et al, 2006; Klementiev and Roth, 
2006). By comparable corpora we mean texts 
that are about similar topics, but are not in gen-
eral translations of each other. These corpora are 
naturally available, for example, many news 
agencies release multi-lingual news articles on 
the same day.  There are no document-level or 
sentence-level alignments across languages, but 
important facts such as names, relations and 
events in one language in such corpora tend to 
co-occur with their counterparts in the other. 
However, most of the previous approaches 
used a phonetic similarity based name translitera-
tion module as baseline to generate translation 
hypotheses, and then exploit the distribution evi-
dence from comparable corpora to re-score these 
hypotheses. As a result, these approaches are 
limited to names which are phonetically translit-
erated (e.g. translate Chinese name ???? (You 
shen ke)? to ?Yushchenko? in English). But many 
other types of names such as organizations are 
often rendered semantically, for example, the 
Chinese name ????? (jie fang zhi hu)? is 
translated into ?Liberation Tiger? in English. 
Furthermore, many name translations are context 
dependent. For example, a person name ???
?????? ? should be translated into ?Yasser 
Arafat (PLO Chairman)? or ?Yasir Arafat 
(Cricketer)? based on different contexts.    
Information extraction (IE) techniques ? iden-
tifying important entities, relations and events ? 
are currently available for some non-English lan-
guages. In this paper we define a new notion ?bi-
lingual information networks? which can be ex-
tracted from comparable corpora. An information 
network is a set of directed graphs, in which each 
node is a named entity and the nodes are linked 
by various ?attributes? such as hometown, em-
ployer, spouse etc. Then we align the informa-
tion networks in two languages automatically in 
a bootstrapping way to discover name translation 
pairs. For example, after we extract bilingual 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. An example for Bilingual Information Networks 
??? 
2. ????? 
Sibling 
3. ??? 
Arrest/2001-06-25
Located 4. ?? 
1. ?????Leader
Located
Leader ??
Capital
?? Birth-Place
Leader 
Arrest/2001-06-25 
2. Montesinos
3. Callao
Jorge Chavez 
Intl. Airport
Located
4. Peru
Located
Located 
Birth-Place 
Arequipa
1. National
Intelligence 
Service
34
information networks as shown in Figure 1, we 
can start from  a common name translation ???
??? -National Intelligence Service (1)?, to 
align its leader as ?????? - Montesinos 
(2)?, align the arrest place of Montesinos as ??
??-Callao (3)?, and then align the location of 
Callao as ???-Peru (4)?. Using this approach 
we can discover name pairs of various types 
(person, organization and location) while mini-
mizing using supervised name transliteration 
techniques. At the same time, we can provide 
links among names for entity disambiguation. 
2 General Approach 
Figure 2 depicts the general procedure of our 
approach. The language pair that we are consid-
ering in this paper is Chinese and English. We 
apply IE techniques to extract information net-
works (more details in section 3), then use a 
bootstrapping algorithm to align them and dis-
cover name pairs (section 4). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Name Translation Mining Overview 
3 Information Network Creation 
3.1 Bilingual Information Extraction 
We apply a state-of-the-art bilingual information 
extraction system (Chen and Ji, 2009; Ji and 
Grishman, 2008) to extract ACE1 types of enti-
ties, relations and events from the comparable 
corpora. Both systems include name tagging, 
                                                 
1 http://www.itl.nist.gov/iad/mig//tests/ace/ 
nominal mention tagging, coreference resolution, 
time expression extraction and normalization, 
relation extraction and event extraction. Entities 
include persons, geo-political (GPE) and organi-
zations; Relations include 18 types (e.g. ?a town 
some 50 miles south of Salzburg? indicates a 
located relation.); Events include the 33 distinct 
event types defined in ACE05 (e.g.  ?Barry Dil-
ler on Wednesday quit as chief of Vivendi? indi-
cates that ?Barry Diller? is the person argument 
of a quit event occurred on Wednesday). The re-
lation extraction and event extraction compo-
nents produce confidence values. 
3.2 Attribute Conversion 
Then we construct a set of directed graphs for 
each language { }( , )i i iG G V E=  , where iV  is 
the collection of named entities, and iE  is the 
edges linking one name to the other, labeled by 
the attributes derived from the following two 
sources: (1) We select the relations with more 
static types to form specific attributes in Table 22, 
according to the entity types of a linked name 
pair. (2) For each extracted event we compose an 
attribute by combining its type and time argu-
ment (e.g. the ?Arrest/2001-06-25? link in Figure 
1). As we will see in the next section, these at-
tributes are the key to discover name translations 
from the information networks because they are 
language-independent. 
4 Information Network Alignment 
After creating the information networks from 
each language, we automatically align them to 
discover name translation pairs. The general idea 
is that starting from a small seed set of common 
name pairs, we can rely on the link attributes to 
align their related names. Then the new name 
translations are added to the seed set for the next 
iteration. We repeat this bootstrapping procedure 
until no new translations are produced. We start 
from names which are frequently linked to others 
so that we can traverse through the information 
networks efficiently. For example, the seed set in 
processing ACE newswire data includes famous 
names such as ?Indonesia?, ?China?, ?Palestine?, 
?Sharon? and ?Yugoslavia?.  
For each name pair <CHName, EName>, we 
search for all its related pairs <CHName?,
                                                 
2 Many of these attributes are consistent with the definitions 
in NIST TAC-KBP task: http://apl.jhu.edu/~paulmac/kbp/ 
090220-KBPTaskGuidelines.pdf 
Information Network Alignment 
Information  
Network Creation 
Chinese Corpora 
Chinese IE 
English Corpora
English IE
Chinese Infor-
mation Network 
English Infor-
mation Network
Attribute Conversion Attribute Conversion
High-Confidence
Name Pairs 
Graph Traverse based on 
Confidence Estimation
Seed  
Name Pairs 
bootstrapping 
35
Name? 
Name 
Person Geo-political Organization 
Person Spouse, Parent, Child, Sibling Birth-Place, Death-Place, 
Resides-Place, Nationality
Schools-Attended, Employer 
Geo-political Leader Located-Country, Capital - 
Organization Leader Location - 
 
Table 2. Relation-driven Attributes (Name ? Name?) in Information Network 
 
Language 
Corpus 
Chinese English 
ACE CHSet1: XIN Oct-Dec 2000: 150 
documents 
ENSet1: APW Oct-Dec 2000: 150 documents 
ENSet2: AFP&APW Mar-June 2003: 150 documents
TDT-5 CHSet3: XIN Apr-Aug 2003: 
30,000 documents 
ENSet3: XIN Apr-Aug 2003: 30,000 documents 
ENSet4: AFP Apr-Aug 2003: 30,000 documents 
 
Table 3. Number of Documents 
 
ENName?>. Assuming CHName is linked to 
CHName? by an edge CHEdge, and ENName is 
linked to ENName? by ENEdge, then if the fol-
lowing conditions are satisfied, we align 
CHName? and ENName? and add them as seeds 
for the next iteration: 
? CHEdge and ENEdge are generated by IE systems  
with confidence values higher than thresholds; 
? CHEdge and ENEdge have the same attributes; 
? CHName? and ENName? have the same entity type; 
? If CHName? and ENName? are persons, the Dam-
erau?Levenshtein edit distance between the pin-
yin form of CHName? and ENName? is lower 
than a threshold.  
It?s worth noting that although we exploit the 
pinyin information as essential constraints, this 
approach differs from the standard transliteration 
models which convert pinyin into English by 
adding/deleting/replacing certain phonemes.  
5 Experimental Results  
5.1 Data 
We use some documents from the ACE (2004, 
2005) training corpora and TDT-5 corpora to 
manually evaluate our approach. Table 3 shows 
the number of documents from different news 
agencies and time frames. We hold out 20 ACE 
texts from each language to optimize the thresh-
olds of confidence values in section 4. A name 
pair <CHName, EName> is judged as correct if 
both of them are correctly extracted and one is 
the correct translation of the other in the certain 
contexts of the original documents.  
5.2 Overall Performance 
Table 4 shows the number and accuracy of name 
translation pairs discovered from CH-Set3 and 
EN-Set3, using 100 name pairs as seeds. After 
four iterations we discovered 968 new name 
translation pairs with accuracy 82.9%. Among 
them there are 361 persons (accuracy 76.4%), 
384 geo-political names (accuracy 87.5%) and 
223 organization names (accuracy 85.2%). 
 
Iteration 1 2 3 4 
Number of Name Pairs 205 533 787 968
Accuracy (%) 91.8 88.5 85.8 82.9
 
Table 4. Overall Performance 
5.3 Impact of Time Frame and News 
Source Similarity 
One major evidence exploited in the prior work 
is that the bilingual comparable corpora should 
be weakly temporally aligned. For example, 
Klementiev and Roth (2006) used the time dis-
tribution of names to re-score name translitera-
tion. In order to verify this observation, we in-
vestigated how well our new approach can per-
form on comparable corpora with different time 
frames. Table 5 presents the performance of two 
combinations: CHSet1-ENSet1 (from the same 
time frame) and CHSet1-ENSet2 (from different 
time frames) with a seed set of 10 name pairs 
after 5 iterations. 
 
Corpora CHSet1-ENSet1 CHSet1-ENSet2
Number of 
Name Pairs 
42 17 
Accuracy (%) 81.0 76.5 
 
Table 5. Impact of Time Frame Similarity     
In addition, in order to measure the impact of 
news source similarity, we apply our approach to 
the combination of CHSet3 and ENSet4 which 
are from different news agencies. In total 815 
name pairs are discovered after 4 iterations with 
overall accuracy 78.7%, which is worse than the 
results from the corpora of the same news source 
as shown in Table 4. Therefore we can clearly 
see that time and news source similarities are 
36
important to the performance of name translation 
pair mining. 
5.4 Impact of IE Errors 
Since in our approach we used the fully auto-
matic IE pipeline to create the information net-
works, the errors from each component will be 
propagated into the alignment step and thus limit 
the performance of name translation discovery. 
For example, Chinese name boundary detection 
errors caused about 30% of the incorrect name 
pairs. As a diagnostic analysis, we tried to dis-
cover name pairs from CHSet1 and ENSet1 but 
with perfect IE annotations. We obtained 63 
name pairs with a much higher accuracy 90.5%.  
6 Related Work  
Most of the previous name translation work 
combined supervised transliteration approaches 
with Language Model based re-scoring (e.g. Al-
Onaizan and Knight, 2002; Huang et al, 2004). 
Ji et al (2009) described various approaches to 
automatically mine name translation pairs from 
aligned phrases (e.g. cross-lingual Wikipedia 
title links) or aligned sentences (bi-texts). Our 
approach of extracting and aligning information 
network from comparable corpora is related to 
some prior work using comparable corpora to re-
score name transliterations (Sproat et al, 2006; 
Klementiev and Roth, 2006).  
In this paper we extend the target names from 
persons to geo-political and organization names, 
and extract relations links among names simulta-
neously. And we use a bootstrapping approach to 
discover name translations from the bilingual 
information networks of comparable corpora. In 
this way we don?t need to have a name translit-
eration module to serve as baseline, or compute 
document-wise temporal distributions.  
7 Conclusion and Future Work  
We have described a simple approach to create 
bilingual information networks and then discover 
name pairs from comparable corpora. The ex-
periments on Chinese and English have shown 
that this method can generate name translation 
pairs with high accuracy by using a small seed 
set. In the short term, our approach will provide a 
framework for many byproducts and directly 
benefit other NLP tasks. For example, the 
aligned sub-graphs with names, relations and 
events can be used to improve information re-
dundancy in cross-lingual question answering; 
the outlier (mis-aligned) sub-graphs can be used 
to detect the novel or local information described 
in one language but not in the other. 
In the future we plan to import more efficient 
graph mining and alignment algorithms which 
have been widely used for protein-protein inter-
action detection (Kelley et al, 2003). In addition, 
we will attempt using unsupervised relation ex-
traction based on lexical semantics to replace the 
supervised IE pipeline. More importantly, we 
will investigate the tradeoff between coverage 
and accuracy by applying the generated name 
pairs to cross-lingual name search and machine 
translation tasks. 
Acknowledgments 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program. 
References  
Y. Al-Onaizan and K. Knight. 2002. Translating 
Named Entities Using Monolingual and Bilingual 
Resources.  Proc. ACL. 
Z. Chen and H. Ji. 2009. Language Specific Issue and 
Feature Exploration in Chinese Event Extraction. 
Proc. HLT-NAACL.  
F. Huang, S. Vogel and A. Waibel. 2004. Improving 
Named Entity Translation Combining Phonetic and 
Semantic Similarities. Proc. HLT/NAACL. 
H. Ji, R. Grishman, D. Freitag, M. Blume, J. Wang, S. 
Khadivi, R. Zens and H. Ney. 2009. Name Transla-
tion for Distillation. Global Automatic Language 
Exploitation. 
H. Ji R. Grishman. 2008. Refining Event Extraction 
Through Cross-document Inference. Proc. ACL. 
H. Ji, R. Grishman and W. Wang. 2008. Phonetic 
Name Matching for Cross-lingual Spoken Sentence 
Retrieval. Proc. IEEE-ACL SLT. 
B. P. Kelley, R. Sharan, R. M. Karp, T. Sittler, D.E. 
Root, B. R. Stockwell and  T. Ideker. 2003. Con-
served pathways within bacteria and yeast as re-
vealed by global protein network alignment. The 
National Academy of Sciences of the United States 
of America. 
A. Klementiev and D. Roth. 2006. Named Entity 
Transliteration and Discovery from Multilingual 
Comparable Corpora. Proc. HLT-NAACL.  
R. Sproat, T. Tao and C. Zhai. 2006. Named Entity 
Transliteration with Comparable Corpora. Proc. 
ACL. 
37
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 54?57,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Graph-based Event Coreference Resolution 
Zheng Chen
The Graduate Center
The City University of New York
zchen1@gc.cuny.edu
Heng Ji
Queens College and The Graduate Center
The City University of New York
hengji@cs.qc.cuny.edu
Abstract
In this paper, we address the problem of event 
coreference resolution as specified in the Au-
tomatic Content Extraction (ACE1
1 Introduction
) program.
In contrast to entity coreference resolution, 
event coreference resolution has not received 
great attention from researchers. In this paper, 
we first demonstrate the diverse scenarios of 
event coreference by an example. We then 
model event coreference resolution as a spec-
tral graph clustering problem and evaluate the
clustering algorithm on ground truth event 
mentions using ECM F-Measure. We obtain 
the ECM-F scores of 0.8363 and 0.8312 re-
spectively by using  two methods for compu-
ting coreference matrices.
Typically, an ACE Event Detection and Recog-
nition (VDR) system consists of two steps: first, 
it detects all mentions of events with certain spe-
cified types occurring in the raw text (event men-
tion detection) and second, it unifies the event 
mentions into equivalence classes so that all the 
mentions in a given class refer to an event (event 
coreference resolution). ACEdefines the follow-
ing terminologies related with VDR:
z Event: a specific occurrence involving partic-
ipants. An ACE event has six attributes (type, 
subtype, modality, polarity, genericity and 
tense), zero or more event arguments, and a 
cluster of event mentions.
z Event trigger: the word that most clearly ex-
presses an event?s occurrence.
z Event argument:  an entity, or a temporal ex-
pression or a value that has a certain role (e.g., 
Time-Within, Place) in an event. 
z Event mention: a sentence (or a text span
extent) that mentions an event, including a 
distinguished trigger and involving arguments.
1 http://www.nist.gov/speech/tests/ace/
In contrast to entity coreference, the scenarios 
in event coreference are more complicated, 
mainly because entity coreference is word (or 
phrase)-level coreference whereas event corefe-
rence is sentence-level coreference and therefore
the coreferring event mentions may have more 
flexible linguistic structures than entity mentions.
We provide an example to demonstrate this di-
versity. 
EM1An {explosion} in a cafe at one of the capital's 
busiest intersections killed one woman and injured 
another Tuesday
EM2Police were investigating the cause of the {ex-
plosion} in 
, police said.
the restroom of the multistory Crocodile 
Cafe in the commercial district of Kizilay dur-
ing the morning rush hour. EM3The {blast} shattered 
walls and windows in the building
EM4Ankara police chief Ercument Yilmaz vi-
sited 
.
the site of the morning blast but refused to say 
if a bomb
EM5The {explosion} comes a month after EM6
had caused the {explosion}.
a bomb
{exploded} at a McDonald's restaurant in Istanbul
EM7
,
causing damage but no injuries.
Radical leftist, Kurdish and Islamic groups are 
active in the country and have carried out {bomb-
ings} in the past.
Table 1. Source text and event mentions
Table 1 shows the source text of a news story. 
As an example, we only tag the event mentions 
which have the event type and subtype of (Con-
flict:Attack). In each event mention, the trigger is 
surrounded by curly brackets, and arguments are 
underlined. 
Table 2 shows the tabular representation of 
those event mentions.
Table 3 shows that the five event mentions in 
event EV1 corefer with each other. We summar-
ize EV1 as follows: a bomb (E4-1) exploded in 
the restroom (E2-1) of a caf? (E1-1 or E1-2) dur-
ing Tuesday morning?s rush hour (combination 
of T1-1, T2-1 and T3-1). EV2 is a different at-
tack event because the target (E6-1) in EV2 dif-
fers from the one (E1-3) in EV1. EV3 tells that 
the bombing attacks have occurred generically 
54
(thus the event attribute ?genericity? is ?General? 
whereas it is ?Specific? in EV1 and EV2).
EM1 Trigger: explosion
Arguments (ID: ROLE):
(E1-1: Place) a cafe at one of the capital's 
busiest intersections
(T1-1: Time-Within) Tuesday
EM2 Trigger: explosion
Arguments:
(E2-1: Place) the restroom of the multistory 
Crocodile Cafe
(E3-1: Place) the commercial district of 
Kizilay
(T2-1: Time-Within) the morning rush hour
EM3 Trigger: blast
Arguments: 
(E1-2: Place) the building
EM4 Trigger: explosion
Arguments: 
(E4-1: Instrument) a bomb
(E1-3: Target) the site of the morning blast
(T3-1: Time-Within) morning
EM5 Trigger: explosion
Arguments: None
EM6 Trigger: exploded
Arguments: 
(E5-1: Instrument) a bomb
(E6-1: Target) a McDonald's restaurant
(E7-1: Place) Istanbul
EM7 Trigger: bombings
(E8-1: Attacker) Radical leftist, Kurdish 
and Islamic groups
(E9-1: Place) the country
(T4-1: Time-Within) the past
Table 2. Tabular representation of event mentions
Event Included event mentions 
EV1 {EM1,EM2,EM3,EM4,EM5}
EV2 {EM6}
EV3 {EM7}
Table 3. Event coreference results
2 Event Coreference Resolution as 
Spectral Graph Clustering
We view the event coreference space as an undi-
rected weighted graph in which the nodes 
represent all the event mentions in a document 
and the edge weights indicate the coreference 
confidence between two event mentions. In real 
implementation,  we initially construct different 
graphs for separate event types 2
2 We view the 33 ACE event subtypes as event types
, such that, in 
each graph, all the event mentions have the same 
event type. Similar to (Nicolae and Nicolae, 
2006), we formally define a framework for event 
coreference resolution.
Let ?? = {??? : 1 ? ? ? ?} be ? event men-
tions in the document and ?? = {??? : 1 ? ? ? ?}
be ? events. Let ?:?? ? ?? be the function 
mapping from an event mention ??? ? ?? to an 
event ??? ? ?? . Let ???:???? ? ?? ? [0,1] be 
the function that computes the coreference confi-
dence between two event mentions ??? , ? ?? ?
?? . Let ? = {?? : 1 ? ? ? ?} be ? event types. 
Thus for each event type ? , we have a graph 
??(?? ,??) , where ?? = {??? |?(??? ). ???? = ?? , ??? ?
??} and ?? = ?(??? , ??? , ?????(??? , ??? ))???? , ??? ? ???.
We then model event coreference resolution as
a spectral graph clustering problem that optimiz-
es the normalized-cut criterion (Shi and Malik, 
2000). Such optimization can be achieved by 
computing the second generalized eigenvector, 
thus the name ?spectral?. In this paper, we do not 
try to propose a new spectral clustering algo-
rithm  or improve the existing algorithm. Instead, 
we focus on how to compute the coreference ma-
trix (equivalently, the affinity matrix in Shi and 
Malik?s algorithm) because a better estimation of 
coreference matrix can reduce the burden on 
clustering algorithm.
3 Coreference Matrix ?
3.1 Method 1: Computing a Coreference Formula
Obviously, the trigger pair and the argument sets 
owned by two event mentions carry much infor-
mation about whether one event mention corefers 
with the other. Based on a corpus, we compute 
the statistics about event mention pairs (with the 
same event type)  listed in Table 4.
Let ??? . ??????? be the trigger in ??? ,
????(??? . ???????) be the stem of the trigger in 
??? , ???????(??? . ???????, ? ?? . ???????) be the 
semantic similarity between the two triggers in 
???and ? ?? as computed in (Seco et al, 2004),  
??? . ??? be the argument (ID and ROLE) set in 
??? . Let 1? be the conjunction operator on ar-
gument pairs whose ID 3
??? = ?
?  1???? +???? where
and ROLE match, 2?
be the conjunction operator on argument pairs 
whose ID matches but ROLE does not match, 3?
be the conjunction operator on argument pairs 
whose ROLE matches but ID does not match, 4?
be the conjunction operator on argument pairs 
whose ID and ROLE do not match. We then pro-
pose the following formula to measure the core-
ference value between ??? and ? ?? .
3 We view two argument IDs ?E1-1? and ?E1-2? as a match 
if they mention the same entity which is ?E1?
55
11
11 12
21
  ( ) ( )
21 22
31
  ( , ) 0
31 32
41
41 42
. . 
. . 
. . 
i j
i j
T
ij
i j
T
if em trigger em trigger
T T
T
elseif stem em trigger stem em trigger
T T
w
T
elseif wordnet em trigger em trigger
T T
T
otherwise
T T
??? ?? ???? ?? ???? ?? ?????? ??
and 
1 2
3 4
1
min{ .arg , .arg }
11 21
[ .arg .arg .arg .arg
11 12 21 22
31 41
.arg .arg .arg .arg ]
31 32 41 42
A
ij
i j
i j i j
i j i j
w
em em
A A
em em em em
A A A A
A A
em em em em
A A A A
 q
?  ?  
?  ? 
The strength of this formula is that it allows to 
give credit to different cases of trigger matching 
and argument pair matching between two event 
mentions.
T11 in those coreferring event mention pairs, how 
many pairs use exactly the same triggers
T12 in those non-coreferring event mention pairs, how 
many pairs use exactly the same triggers
T21 in those coreferring event mention pairs, how 
many pairs do not have the same triggers, but 
have the same stems of triggers
T22 non-coreferring version of T21
T31 in those coreferring event mention pairs, how 
many pairs do not have the same triggers nor the 
same stems, but the semantic similarity between 
two triggers is higher than 0 in WordNet.
T32 non-coreferring version of T31
T41 in those non-coreferring event mention pairs, how 
many pairs are not in T11 or T21 or T31
T42 non- coreferring version that is not T12 or T22 or 
T32
A11 in those coreferring event mention pairs, how 
many argument pairs whose ID and ROLE match
A12 non-coreferring version of A11
A21 in those coreferring event mention pairs, how 
many argument pairs whose ID matches but 
ROLE does not match
A22 non-coreferring version of A21
A31 in those coreferring event mention pairs, how 
many argument pairs whose ROLE matches but 
ID does not match
A32 non-coreferring version of A31
A41 in those non-coreferring event mention pairs, how 
many argument pairs whose ID and ROLE do not 
match
A42 non-coreferring version of A41
Table 4. Statistics of event mention pairs
3.2 Method 2: Applying a Maximum En-
tropy Model
We train a maximum entropy model to produce 
the confidence values for ? . Each confidence 
value tells the probability that there exists corefe-
rence ? between event mention ??? and ? ?? .
??????? , ? ?? ? =
?(? ??? ??(??? , ??? ,?))
?(??? , ? ?? )
where ??(???,  ???,?) is a feature and ?? is its 
weight; ????? , ? ?? ? is the normalizing factor.
The feature sets applied in the model are listed 
in Table 5 by categories.
4 Experiments and Results
4.1 Data and Evaluation Metrics
We developed and tested the spectral clustering
algorithm for event coreference resolution using 
the ACE 2005 English corpus which contains 
560 documents. We used the ground truth event 
mentions and evaluated our algorithm based on 
ECM F-Measure (Luo, 2005). We reserved 60 
documents for testing purpose and used the left 
500 documents for training/developing purpose 
and for computing the statistics discussed above.
We applied 10-fold cross-validation in the expe-
riment of comparing two methods for computing 
coreference matrix.
4.2 Statistics of Event Mention Pairs
The results of the statistics discussed in Section 
3.1 are presented in Table 6.
T11=1042,T12=1297, T21=240,T22=840,
T31=257, T32=2637, T41=784,T42=5628
A11=888, A12= 1485, A21=31, A22=146,
A31=542, A32=6849, A41=323, A42=3000
Table 6. Results of statistics in 500 documents
From Table 6, we observe that if two event 
mentions use the same trigger or if they have 
arguments whose ID and ROLE match, it is more 
probable for them to corefer with each other than 
other cases.
4.3 Comparison of the Two Methods for 
Computing Coreference Matrix
Figure 1. ECM-F scores for both methods
56
Category Features Remarks (EM1: the first event mention, EM2: the second event 
mention)
Lexicon type_subtype pair of event type and subtype in EM1
trigger_pair trigger pair of EM1 and EM2
pos_pair part-of-speech pair of triggers of EM1 and EM2
nominal 1 if the trigger of EM2 is nominal
exact_match 1 if the spellings of triggers in EM1 and EM2 exactly match
stem_match 1 if the stems of triggers in EM1 and EM2 match
trigger_sim quantized semantic similarity score (0-5) using WordNet resource 
Distance token_dist how many tokens between triggers of EM1 and EM2 (quantized)
sentence_dist how many sentences EM1 and EM2 are apart (quantized)
event_dist how many event mentions in between EM1 and EM2 (quantized)
Arguments overlap_num,overlap_roles overlap number of arguments and their roles (role and id exactly 
match) between EM1 and EM2
prior_num, prior_roles the number and the roles of arguments that only appear in EM1
act_num, act_roles the number and the roles of arguments that only appear in EM2
coref_num the number of arguments that corefer each other but have different 
roles between EM1 and EM2
Table 5. EM(Event Mention)-pair features for the maximum entropy model
Figure 1 shows the ECM-F scores for both me-
thods by varying the cut threshold in the cluster-
ing algorithm. Both methods obtain the highest 
ECM-F score at threshold 0.85 and method 1
performs slightly better than method 2 (0.8449 vs. 
0.8418, significant at 85% confidence level,
p<=0.1447). We obtained the ECM-F scores of  
0.8363 and 0.8312 on the test set for method 1
and method 2 respectively. We also obtained
two baseline ECM-F scores, one is 0.535 if we 
consider all the event mentions with the same 
event type as a cluster, the other is 0.7635 if we 
consider each event mention as a cluster.
5 Related Work 
Earlier work on event coreference (e.g. Humph-
reys et al, 1997; Bagga and Baldwin, 1999) in 
MUC was limited to several scenarios, e.g., ter-
rorist attacks, management succession, resigna-
tion. The ACE program takes a further step to-
wards processing more fine-grained events. To 
the best of our knowledge, this paper is the first 
effort to apply graph-based algorithm to the 
problem of event coreference resolution.
Nicolae and Nicolae (2006) proposed a similar 
graph-based framework for entity coreference 
resolution. However, in our task, the event men-
tion has much richer structure than the entity 
mention, thus, it is possible for us to harness the  
useful information from both the triggers and the 
attached arguments in the event mentions.
6 Conclusions and Future Work
In this paper, we addressed the problem of event 
coreference resolution in a graph-based frame-
work, and presented two methods for computing 
the coreference matrix. A practical event corefe-
rence resolver also depends on high-performance 
event extractor. We will further study the impact 
of system generated event mentions on the per-
formance of our coreference resolver. 
Acknowledgments
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023 via 27-
001022, and the CUNY Research Enhancement 
Program and GRTI Program.
References 
A. Bagga and B. Baldwin. 1999. Cross-document 
event coreference: Annotations, experiments, and 
observations. In Proc. ACL-99 Workshop on Core-
ference and Its Applications.
C. Nicolae and G. Nicolae. 2006. Bestcut: A graph
algorithm for coreference resolution. In EMNLP,
pages 275?283, Sydney, Australia, July.
J. Shi and J. Malik.1997. Normalized Cuts and Image 
Segmentation. In Proc. of IEEE Conf. on Comp. 
Vision and Pattern Recognition, Puerto Rico
K. Humphreys, R. Gaizauskas, S. Azzam. 1997. 
Event coreference for information extraction. In 
Proceedings of the ACL Workshop on Operational 
Factors in Practical Robust Anaphora Resolution 
for Unrestricted Texts.
N. Seco, T. Veale, J. Hayes. 2004. An intrinsic infor-
mation content metric for semantic similarity in 
WordNet. In Proc. of ECAI-04, pp. 1089?1090.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP.
57
Coling 2010: Poster Volume, pages 507?515,
Beijing, August 2010
Challenges from Information Extraction to Information Fusion 
Heng Ji 
Computer Science Department 
Queens College and Graduate Center 
City University of New York 
hengji@cs.qc.cuny.edu
Abstract
Information Extraction (IE) technology is fac-
ing new challenges of dealing with large-scale 
heterogeneous data sources from different 
documents, languages and modalities. Infor-
mation fusion, a new emerging area derived 
from IE, aims to address these challenges. We 
specify the requirements and possible solu-
tions to perform information fusion. The is-
sues include redundancy removal, contradic-
tion resolution and uncertainty reduction. We 
believe this is a critical step to advance IE to a 
higher level of performance and portability.  
1 Introduction 
Latest development of Information Extraction 
(IE) techniques has made it possible to extract 
?facts? (entities, relations and events) from un-
structured documents, and converting them into 
structured representations (e.g. databases). Once 
the collection grows beyond a certain size, an 
issue of critical importance is how a user can 
monitor a compact knowledge base or identify 
the interesting portions without having to (re) 
read large amounts of facts. In this situation us-
ers are often more concerned with the speed in 
which they obtain results, rather than obtaining 
the exact answers to their queries (Jagadish et 
al., 1999). The facts extracted from heterogene-
ous data sources (e.g. text, images, speech and 
videos) must then be integrated in a knowledge 
base, so that it can be queried in a uniform way. 
This provides unparalleled challenges and op-
portunities for improved decision making.  
Data can be noisy, incorrect, or misleading. 
Unstructured data, mostly text, is difficult to in-
terpret. In practice it is often the case that there 
are multiple sources which need to be extracted 
and compressed. In a large, diverse, and inter-
connected system, it is difficult to assure accu-
racy or even coherence among the data sources. 
In this environment, traditional IE would be of 
little value. Most current IE systems focus on 
processing a single document and language, and 
are customized for a single data modality. In ad-
dition, automatic IE systems are far from perfect 
and tend to produce errors.  
Achieving really advances in IE requires that 
we take a broader view, one that looks outside a 
single source. We feel the time is now ripe to 
incorporate some information integration tech-
niques in the database community (e.g. Seligman 
et al, 2010) to extend the IE paradigm to real-
time information fusion and raise IE to a higher 
level of performance and portability. This re-
quires us to work on a more challenging problem 
of information fusion - to remove redundancy, 
resolve contradictions and uncertainties by mul-
tiple information providers and design a general 
framework for the veracity analysis problem. 
The goal of this paper is to lay out the current 
status and potential challenges of information 
fusion, and suggest the following possible re-
search avenues. 
? Cross-document: We will discuss how to 
effectively aggregate facts across documents 
via entity and event coreference resolution.
? Cross-lingual: A shrinking fraction of the 
world?s Web pages are written in English, 
and so the ability to access pages across a 
range of languages is becoming increasingly 
important for many applications. This need 
can be addressed in part by cross-lingual in-
formation fusion. We will discuss the chal-
507
lenges of extraction and translation respec-
tively. 
? Cross-media: Advances in speech and im-
age processing make the application of IE 
possible on other data modalities, beyond 
traditional textual documents.  
2 Cross-Document Information Fusion 
Most current IE systems focus on processing one 
document at a time, and except for coreference 
resolution, operate one sentence at a time. The 
systems make only limited use of ?facts? already 
extracted in the current document. The output 
contains rich structures about entities, relations 
and events involving such entities. However, due 
to noise, uncertainty, volatility and unavailability 
of IE components, the collected facts may be 
incomplete, noisy and erroneous. Several recent 
studies have stressed the benefits of using infor-
mation fusion across documents. These methods 
investigate quite different angles while follow a 
common research theme, namely to exploit 
global background knowledge. 
2.1 Information Inference 
Achieving really high performance (especially, 
recall) of IE requires deep semantic knowledge 
and large costly hand-labeled data. Many sys-
tems also exploited lexical gazetteers. However, 
such knowledge is relatively static (it is not up-
dated during the extraction process), expensive 
to construct, and doesn?t include any probabilis-
tic information. Error analysis on relation extrac-
tion shows that a majority (about 78%) of errors 
occur on nominal mentions, and more than 90% 
missing errors occur due to the lack of enough 
patterns to capture the context between two en-
tity mentions. For instance, to describe the ?lo-
cated? relation between a bomber and a bus, 
there are more than 50 different intervening 
strings (e.g. ?killed many people on a?, ??s at-
tack on a?, ?blew apart a?, ?blew himself up on 
a?, ?drove his explosives-laden car into a?, 
?had rigged the?, ?set off a bomb on a?, etc.), 
but the ACE1 training corpora only cover about 
1/3 of these expressions. 
Several recent studies have stressed the bene-
fits of using information redundancy on estimat-
ing the correctness of the IE output (Downey et 
1 http://www.itl.nist.gov/iad/mig/tests/ace/
al., 2005), improving disease event extraction 
(Yangarber, 2006), Message Understanding 
Conference event extraction (Mann, 2007; Pat-
wardhan and Riloff, 2009) and ACE event ex-
traction (Ji and Grishman, 2008). This approach 
is based on the premise that many facts will be 
reported multiple times from different sources in 
different forms. This may occur both within the 
same document and within a cluster of topically 
related and successive documents. Therefore, by 
aggregating similar facts across documents and 
conducting statistical global inference by favor-
ing interpretation consistency, enhanced extrac-
tion performance can be achieved with heteroge-
neous data than uniform data.  
The underlying hypothesis of cross-document 
inference is that the salience of a fact should be 
calculated by taking into consideration both its 
confidence and the confidence of other facts 
connected to it, which is inspired by PageRank 
(Page et al, 1998) and LexRank (Erkan and 
Radev, 2004). For example, a vote by linked en-
tities which are highly voted on by other entities 
is more valuable than a vote from unlinked enti-
ties. There are two major heuristics: (1) an as-
sertion that several information providers agree 
on is usually more trustable than that only one 
provider suggests; and (2) an information pro-
vider is trustworthy if it provides many pieces of 
true information, and a piece of information is 
likely to be true if it is provided by many trust-
worthy providers. (Yin et al, 2008) used the 
above heuristics in a progressive, iterative en-
hancement process for information fusion.  
The results from the previous work are prom-
ising, but the heuristic inferences are highly de-
pendent on the order of applying rules, and the 
performance may have been limited by the 
thresholds which may overfit a small develop-
ment corpus. One promising method might be 
using Markov Logic Networks (Richardson and 
Domingos, 2006), a statistical relational learning 
language, to model these global inference rules 
more declaratively. Markov Logic will make it 
possible to compactly specify probability distri-
butions over the complex relational inferences. It 
can capture non-deterministic (soft) rules that 
tend to hold among facts but do not have to. Ex-
ploiting this approach will also provide greater 
flexibility to incorporate additional linguistic and 
world knowledge into inference. 
508
The information fused across documents can 
be represented as an information network (Ji, 
2009) in which entities can be viewed as vertices 
on the graph and they can be connected by some 
type of static relationship (e.g. those attributes 
defined in NIST TAC-KBP task (McNamee and 
Dang, 2009)), or as a temporal chain linking dy-
namic events (e.g. Bethard and Martin, 2008; 
Chambers and Jurafsky, 2009; Ji et al, 2009a). 
The latter representation is more attractive be-
cause business or international affairs analysts 
often review many news reports to track people, 
companies, and government activities and 
trends. The query logs from the commercial 
search engines show that there is a fair number 
of news related queries (Mishne & de Rijke, 
2006), suggesting that blog search users have an 
interest in the blogosphere response to news sto-
ries as they develop. For example, (Ji et al, 
2009a) extracted centroid entities and then 
linked events centered around the same centroid 
entities on a time line.  
Temporal ordering is a challenging task in 
particular because about half of the event men-
tions don?t include explicit time arguments. The 
text order by itself is a poor predictor of chrono-
logical order (only 3% temporal correlation with 
the true order). Single-document IE technique 
can identify and normalize event time arguments 
from the texts, which results in a much better 
correlation score of 44% (Ji et al, 2009a). But 
this is still far from the ideal performance for 
real applications. In order to alleviate this bottle-
neck, a possible solution is to exploit global 
knowledge from the related documents and 
Wikipedia, and related events to recover and 
predict some implicit time arguments (Filatova 
and Hovy, 2001; Mani et al, 2003; Mann, 2007; 
Eidelman, 2008; Gupta and Ji, 2009).   
2.2 Coreference Resolution 
One of the key challenges for information fusion 
is cross-document entity coreference ? precise 
clustering of mentions into correct entities. 
There are two principal challenges: the same 
entity can be referred to by more than one name 
string and the same name string can refer to 
more than one entity. The recent research has 
been mainly promoted in the web people search 
task (Artiles et al, 2007) such as (Balog et al, 
2008), ACE2008 such as (Baron and Freedman, 
2008) and NIST TAC KBP (McNamee and 
Dang, 2009) evaluations. Interestingly, the qual-
ity of information can often be improved by the 
fused fact network itself, which can be called as 
self-boosting of information fusion. For exam-
ple, if two GPE entities are involved in a ?con-
flict-attack? event, then they are unlikely to be 
connected by a ?part-whole? relation; ?Mah-
moud Abbas? and ?Abu Mazen? are likely to be 
coreferential if they get involved in the same 
?life-born? event. Some prior work (Ji et al, 
2005; Jing et al, 2007) demonstrated the effec-
tiveness of using semantic relations to improve 
entity coreference resolution; while (Downey et 
al., 2005; Sutton and McCallum, 2004; Finkel et 
al., 2005; Mann, 2007) experimented with in-
formation fusion of relations across multiple 
documents. The TextRunner system (Banko et 
al., 2007)  can collapse and compress redundant 
facts extracted from multiple documents based 
on coreference resolution (Yates and Etzioni, 
2009), semantic similarity computation and nor-
malization.
Two relations are central for event fusion: 
contradiction ? part of one event mention con-
tradicts part of another, and redundancy ? part of 
one event mention conveys the same content as 
(or is entailed by) part of another. Once these 
central relations are identified they will provide 
a basis for identifying more complex relations 
such as elaboration, presupposition or conse-
quence. It is important to note that redundancy 
and contradiction among event mentions are 
logical relations that are not captured by tradi-
tional topic-based techniques for similarity de-
tection (e.g. Brants and Stolle, 2002). Contradic-
tions also arise from complex differences in the 
structure of assertions, discrepancies based on 
world-knowledge, and lexical contrasts. Ritter et 
al. (2009) described a contradiction detection 
method based on functional relations and pointed 
out that many contradictory fact pairs from the 
Web appear consistent, and that requires back-
ground knowledge to predict. 
Assessing event coreference is essential: for 
texts to contradict, they must refer to the same 
event. Event coreference resolution is more chal-
lenging than entity coreference because each 
linking decision needs to be made based upon 
the overall similarity of the event trigger and 
multiple arguments. Hasler and Orasan (2009) 
509
further found that in many cases even coreferen-
tial even arguments are not good indicators for 
event coreference. 
Earlier work on event coreference resolution 
(e.g. Bagga and Baldwin, 1999) was limited to 
several MUC scenarios. Recent work (Chen et 
al., 2009) focus on much wider coverage of 
event types defined in ACE. The methods from 
the knowledge fusion community (e.g. Appriou 
et al, 2001; Gregoire, 2006) mostly focus on 
resolving conflicts rather than identifying them 
(i.e. inconsistency problem rather than ambigu-
ity). These approaches allow the conflicts to be 
resolved in a straightforward way but they rely 
on the availability of meta-data (e.g., distribution 
of weights between attributes, probability as-
signment etc.). However, it is not always clear 
where to get this meta-data. 
The event attributes such as Modality, Polarity, 
Genericity and Tense (Sauri et al, 2006) will 
play an important role in event coreference reso-
lution because two event mentions cannot be 
coreferential if any of the attributes conflict with 
each other. Such attempts have been largely ne-
glected in the prior research due to the low 
weights of attribute labeling in the ACE scoring 
metric. (Chen et al, 2009) demonstrated that 
simple automatic event attribute labeling can 
significantly improve event coreference resolu-
tion. In addition, some very recent work includ-
ing (Nicolae and Nicolae, 2006; Ng, 2009; Chen 
et al, 2009) found that graph-cut based cluster-
ing can improve coreference resolution. The 
challenge lies in computing the affinity matrix. 
3 Cross-Lingual Information Fusion  
Cross-lingual comparable corpora are also 
prevalent now because almost all the influential 
events can be reported in multi-languages at the 
first time, but probably in different aspects. 
Therefore, linked fact networks can be con-
structed and lots of research tasks can benefit 
from such structures. Since the two networks are 
similar in structure but not homogeneous, we can 
do alignment and translation which may advance 
information fusion. Cross-lingual information 
fusion is concerned with technologies that fuse 
the information available in various languages 
and present the fused information in the user-
preferred language. The following fundamental 
cross-lingual IE pipelines can be employed: (1) 
Translate source language texts into target lan-
guage, and then run target language IE on the 
translated texts. (2) Run source language IE on 
the source language texts, and then use machine 
translation (MT) word alignments to translate 
(project) extracted information into target lan-
guages. Regardless of the different architectures, 
both pipelines are facing the following chal-
lenges from extraction and translation. 
3.1 Extraction Challenges 
Some recent fusion work focus on cross-lingual 
interaction and inference to improve both sides 
synchronously, beyond the parallel comparisons 
of cross-lingual IE pipelines in (e.g. Riloff et al, 
2002). One of such examples is on cross-lingual 
co-training (e.g. Cao et al, 2003; Chen and Ji, 
2009). In co-training (Blum and Mitchell, 1998), 
the uncertainty of a classifier is defined as the 
portion of instances on which it cannot make 
classification decisions. Exchanging tagged data 
in bootstrapping can help reduce the uncertain-
ties of classifiers. The cross-lingual fusion proc-
ess satisfies the co-training algorithm?s assump-
tions about two views (in this case, two lan-
guages): (1) the two views are individually suffi-
cient for classification (IE systems in both lan-
guages were learned from annotated corpora 
which are enough for reasonable extraction per-
formance); (2) the two views are conditionally 
independent given the class (IE systems in dif-
ferent languages may use different features and 
resources).
(Cao et al, 2003) indicated that uncertainty 
reduction is an important factor for enhancing 
the performance of co-training. It?s important to 
design new uncertainty measures for represent-
ing the degree of uncertainty correlation of the 
two classifiers in co-training. (Chen and Ji, 2009) 
proposed a new co-training framework using 
cross-lingual information projection. They dem-
onstrated that this framework is particularly ef-
fective for a challenging IE task which is situ-
ated at the end of a pipeline and thus suffers 
from the errors propagated from upstream proc-
essing and has low-performance baseline.  
3.2 Translation Challenges 
Because the facts are aggregated from multiple 
languages, the translation errors will bring us 
great challenges. However, in order to extend 
510
cross-lingual information fusion techniques to 
more language pairs, we can start from the much 
more scalable task of ?information? translation 
(Etzioni et al, 2007). The additional processing 
may take the form of machine translation (MT) 
of extracted facts such as names and events. IE 
tasks performed notably worse on machine trans-
lated texts than on texts originally written in 
English, and error analysis indicated that a major 
cause was the low quality of name translation (Ji 
et al, 2009b). Traditional MT systems focus on 
the overall fluency and accuracy of the transla-
tion but fall short in their ability to translate cer-
tain informationally critical words. In particular, 
it appears that better entity name translation can 
substantially improve cross-lingual information 
fusion.
Some recent work (e.g. Klementiev and Roth, 
2006; Ji, 2009) has exploited comparable cor-
pora to enhance information translation. There 
are no document-level or sentence-level align-
ments across languages, but important facts such 
as names, relations and events in one language in 
such corpora tend to co-occur with their coun-
terparts in the other. (Ji, 2009) used a bootstrap-
ping approach to align the information networks 
from bilingual comparable corpora, and discover 
name translations and extract relations links si-
multaneously. The general idea is to start from a 
small seed set of common name pairs, and then 
rely on the link attributes to align their related 
names. Then the new name translations are 
added to the seed set for the next iteration. This 
bootstrapping procedure is repeated until no new 
translations are produced. This approach is based 
on graph traverses and doesn?t need a name 
transliteration module to serve as baseline, or 
compute document-wise temporal distributions.   
The novelty of using comparable corpora lies 
in constructing and mining multi-lingual infor-
mation fusion framework which is capable of 
self-boosting. First, this approach can generate 
information translation pairs with high accuracy 
by using a small seed set. Second, the shortcom-
ings of traditional approaches are due to their 
limited use of IE techniques, and this approach 
can effectively integrate extraction and transla-
tion based on reliable confidence estimation. 
Third, compared to bitexts this approach can 
take advantage of much less expensive compara-
ble corpora. This approach can be extended to 
foster the research in other aspects for informa-
tion fusion. For example, the aligned sub-graphs 
with names, relations and events can be used to 
reduce information redundancy; the outlier (mis-
aligned) sub-graphs can be used to detect the 
novel or local information described in one lan-
guage but not in the other after the fusion proc-
ess. It does happen that the two persons have 
been explicitly reported as Father and Son rela-
tionship in one language, but in the other lan-
guage, they are just reported as two common 
persons.
4 Cross-Media Information Fusion
The research challenges discussed so far con-
cerned with textual data. Besides written texts, 
ever-increasing human generated data is avail-
able as speech recordings, microblogs, images 
and videos. We now discuss how to develop 
techniques for fusing a variety of media sources. 
State-of-the-art IE techniques have been devel-
oped primarily on newspaper articles and a few 
web texts, and it is not clear how systems would 
perform on other sources and how to integrate all 
available information. 
4.1 Coreference Resolution 
The main challenge is on designing a coherent 
information fusion framework that is able to ex-
ploit information across different parts of multi-
media documents and link them via cross-media 
coreference resolution. The framework will han-
dle multimedia information by considering not 
only the document?s text and images data but 
also the layout structure which determines how a 
given text block is related to a particular image 
or video. For example, a Web news page about 
?Health Care Reform in America? is composed 
by text describing some event (e.g., Final Senate 
vote for the reform plans, Obama signs the re-
form agreement), images (e.g., images about 
various government involvements over decades) 
and videos (e.g. Obama?s speech video about the 
decisions) containing additional information re-
garding the real extent of the event or providing 
evidence corroborating the text part.  
Current state-of-the-art information fusion ap-
proaches can be divided into two groups: formal 
?top-down? methods from the generic knowl-
edge fusion community and quantitative ?bot-
tom-up? techniques from the applied Semantic 
511
Web community (Appriou et al, 2001; Gregoire, 
2006). Both approaches have their limitations. It 
will be beneficial to combine both types of ap-
proaches so that the fusion decision can be made 
depending on the type of problem and the 
amount of domain information it possesses. Sag-
gion et al (2004) described a multimedia extrac-
tion approach to create composite index from 
multiple and multi-lingual sources.  Magalhaes 
et al (2008) described a semantic similarity met-
ric based on key word vectors for multi-media 
fusion. Iria and Magalhaes (2009) exploited in-
formation across different parts of a multimedia 
document to improve document classification. It 
is important to go beyond key words and attempt 
representing the documents by the semantic facts 
identified by IE. 
One possible solution is to exploit the linkage 
information. Specifically, coreference resolution 
methods should be applied to four types of cross-
media data: (1) between the captions of images 
and context texts; (2) detecting HTML cross-
media associations and quantifying the level of 
image and text block correlation (3) between the 
texts embedded in images and context texts; (4) 
between the transcribed texts from the speech in 
video clips (via automatic speech recognition) 
and context texts. We can apply a similarity 
graph to incorporate virtual linkages. For exam-
ple, when we see images of two web documents 
containing the same object, we can raise our 
confidence that such documents are semanti-
cally correlated even if the two web documents 
are from different sources. 
4.2 Uncertainty Reduction 
When we combine information from images and 
their associated texts (e.g. meta-data, captions, 
surrounding text, transcription), one of the chal-
lenges lies in the uncertainty of text representa-
tion.  Therefore it is important to study both how 
to learn good models from different sources with 
different kinds of associated uncertainty, and 
how to make use of these, along with their level 
of uncertainty in supporting coherent decisions, 
taking into account characteristics of the data as 
well as of its source.   
The descriptions are usually generated by hu-
mans and thus are prone to error or subjectiv-
ity.  The images, especially the web images, are 
typically labeled by different users in different 
languages and cultural backgrounds.  It is unreal-
istic to expect descriptions to be consistent. In 
speech conversations, many facts are often em-
bedded in questions such as ?It's OK to put De-
mocratic career politicians at the Pentagon and 
the Justice Department if they're Democrats but 
not if they're Republicans, is that right?? This 
challenge can be generally addressed by 
strengthening semantic attribute classification 
methods for Modality, Polarity and Genericity. 
And if the data sources are comparable, a more 
direct method of committee-based voting can 
also be exploited. 
However, the fusion process may itself cause 
data uncertainties. We can follow the co-training 
framework as described in section 3.1 to reduce 
uncertainty in fusion. To handle the missing la-
bels, a promising approach is to use graph-based 
label propagation (Deshpande et al, 2009), 
which can capture complex uncertainties and 
correlations in the data in a uniform manner. It?s 
also worth importing the multi-dimensional un-
certainty analysis framework described in data 
mining community (Aggarwal, 2010). The 
multi-dimensional uncertainty analysis method 
exactly suits the multi-media fusion needs: it 
allows us to combine first-order logic with prob-
abilities, modeling inferential uncertainty about 
multiple aspects - both the context of facts and 
intended meanings.  
4.3 Joint Modeling 
IE is generally applied on top of machine gener-
ated transcription and automatic structuring that 
suffer from errors compared to the true content 
of relations and events. In the context of infor-
mation fusion we can divide the problem of ad-
aptation into two types: (1) radical adaptation 
such as from newswire to biomedical articles; 
(2) modest adaptation such as from newswire to 
wikipedia or automatic speech recognition 
(ASR) output. (1) requires a great deal of new 
development such as ontology definition and 
data annotation; while (2) can be partially ad-
dressed during the information fusion process.  
For example, while dealing with speech input, 
IE systems need to be robust to the noise intro-
duced by earlier speech processing tasks such as 
ASR, sentence segmentation, salience detection 
and and speaker identification. Some earlier 
work (Makhoul et al, 2005; Favre et al, 2008) 
512
showed that using an IE system trained from 
newswire, the performance degrades notably 
when the system is tested on automatic speech 
recognition output. But no general solutions 
have been proposed to address the genre-specific 
challenges for speech data.  
More specifically, pronoun resolution is one 
of the major challenges (Jing et al, 2007). For 
example, in wikipedia a lot of pronouns may 
refer to the entry entity; while in speech conver-
sation we will need to resolve first and second 
person pronouns based on automatic speaker role 
identification; and improve cross-sentence third 
pronoun resolution by exploiting gender and 
animacy knowledge discovery methods. 
The processing methods of text and other me-
dia are typically organized as a pipeline architec-
ture of processing stages (e.g. from pattern rec-
ognition, to information fusion, and to summari-
zation). Each of these stages has been studied 
separately and quite intensively over the past 
decade. It?s critical to move away from ap-
proaches that make chains of independent local 
decisions, and instead toward methods that make 
multiple decisions jointly using global informa-
tion. Joint inference techniques (Roth and Yih, 
2004; Ji et al, 2005; McCallum, 2006) can trans-
form the integration of multi-media into a bene-
fit by reducing the errors in individual stages. In 
doing so, we can take advantage (among other 
properties) of the coherence of a discourse: that a 
correct analysis of a text discourse reveals a 
large number of connections from the image in-
formation in its context, and so (in general) a 
more tightly connected analysis is more likely to 
be correct. For example, prior work has demon-
strated the benefit of jointly modeling name tag-
ging and n-best hypotheses, ASR lattices or 
word confusion networks (Hakkani-T?r et al, 
2006).
5 Conclusion
In the current information explosion era, IE 
technology is facing new challenges of dealing 
with heterogeneous data sources from different 
documents, languages and media which may 
contain a multiplicity of aspects on particular 
entities, relations and events. This new phenom-
ena requires IE to perform both traditional lower 
level processing as well as information fusion of 
factual data based on implicit inferences. This 
paper investigated the issues of information fu-
sion on a massive scale and the challenges have 
not been discussed in previous work. We speci-
fied the requirements and possible solutions for 
various dimensions to perform information fu-
sion. We also overviewed some recent work to 
demonstrate how these goals can be achieved.  
The field of information fusion is relatively 
new; and the nature of different data sources 
provides new ideas and challenges which are not 
present in other research. While much research 
has been performed in the area of data fusion, 
the context of automatic extraction provides a 
different perspective in which the fusion is per-
formed in the context of a lot of uncertainty and 
noise. This new task will provide connections 
between NLP and other areas such as data min-
ing and knowledge discovery. The progress on 
this task would save, anybody concerned with 
staying informed, an enormous amount of time. 
These are certainly ambitious goals and require 
long-term development of fusion and adaptation 
methods. But we hope that this outline of the 
research challenges will bring us closer to the 
goal.
Acknowledgement 
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement 
Number W911NF-09-2-0053, the U.S. NSF 
CAREER Award under Grant IIS-0953149, 
Google, Inc., DARPA GALE Program, CUNY 
Research Enhancement Program, PSC-CUNY 
Research Program, Faculty Publication Program 
and GRTI Program. The views and conclusions 
contained in this document are those of the au-
thors and should not be interpreted as represent-
ing the official policies, either expressed or im-
plied, of the Army Research Laboratory or the 
U.S. Government. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Government purposes notwithstanding any copy-
right notation here on. 
References  
Charu Aggarwal. 2010. On Multi-dimensional Sharp-
ening of Uncertain Data. SIAM: SIAM Conference 
on Data Mining (SDM10).
A. Appriou-, A. Ayoun, et al 2001. Fusion: General 
concepts and characteristics. International Journal 
of Intelligent Systems 16(10). 
513
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 
2007. The SemEval-2007 WePS Evaluation: Es-
tablishing a benchmark for the Web People Search 
Task. Proc. Semeval-2007.
Amit Bagga and Breck Baldwin. 1999. Cross-
document Event Coreference: Annotations, Ex-
periments, and Observations. Proc. ACL1999 
Workshop on Coreference and Its Applications.
K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal 
Name Resolution of Web People Search. Proc. 
WWW2008 Workshop: NLP Challenges in the In-
formation Explosion Era (NLPIX 2008). 
Michele Banko, Michael J Cafarella, Stephen 
Soderland and Oren Etzioni. 2007. Open Informa-
tion Extraction from the Web. Proc. IJCAI 2007.
Alex Baron and Marjorie Freedman. 2008. Who is 
Who and What is What: Experiments in Cross-
Document Co-Reference. Proc. EMNLP 2008.
Steven Bethard and James H. Martin. 2008. Learning 
semantic links from a corpus of parallel temporal 
and causal relations. Proc. ACL-HLT 2008.
Avrim Blum and Tom Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training. 
Proc. of the Workshop on Computational Learning 
Theory. Morgan Kaufmann Publishers. 
T. Brants and R. Stolle. 2002. Finding Similar Docu-
ments in Document Collections. Proc. LREC 
Workshop on Using Semantics for Information 
Retrieval and Filtering. 
Yunbo Cao, Hang Li and Li Lian. 2003. Uncertainty 
Reduction in Collaborative Bootstrapping: 
Measure and Algorithm. Proc. ACL 2003.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and 
their Participants. Proc. ACL 09.
Zheng Chen and Heng Ji. 2009. Can One Language 
Bootstrap the Other: A Case Study on Event Ex-
traction. Proc. HLT-NAACL Workshop on Semi-
supervised Learning for Natural Language Proc-
essing. Boulder, Co. 
Zheng Chen, Heng Ji and Robert Harallick. 2009. A 
Pairwise Coreference Model, Feature Impact and 
Evaluation for Event Coreference Resolution. 
Proc. RANLP 2009 workshop on Events in Emerg-
ing Text Types. 
Amol Deshpande, Lise Getoor and Prithviraj Sen. 
2009. Graphical Models for Uncertain Data. 
Managing and Mining Uncertain Data (Edited 
by Charu Aggarwal). Springer. 
Doug Downey, Oren Etzioni, and Stephen Soderland. 
2005. A Probabilistic Model of Redundancy in In-
formation Extraction. Proc. IJCAI 2005.
Vladimir Eidelman. 2008. Inferring Activity Time in 
News through Event Modeling. Proc. ACL-HLT 
2008.
Gunes Erkan and Dragomir R. Radev. 2004. 
LexPageRank: Prestige in multi-document text 
summarization. Proc. EMNLP 2004.
Oren Etzioni, Kobi Reiter, Stephen Soderland and 
Marcus Sammer. 2007. Lexical Translation with 
Application to Image Search on the Web. Proc. 
Machine Translation Summit XI.
Benoit Favre, Ralph Grishman, Dustin Hillard, Heng 
Ji, Dilek Hakkani-Tur and Mari Ostendorf. 2008. 
Punctuating Speech for Information Extraction. 
Proc. ICASSP 2008.
Elena Filatova and Eduard Hovy. 2001. Assigning 
Time-Stamps to Event-Clauses. Proc. ACL 2001 
Workshop on Temporal and Spatial Information 
Processing. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs 
Sampling. Proc. ACL 2005.
E. Gregoire. 2006. An unbiased approach to iterated 
fusion by weakening. Information Fusion. 7(1). 
Prashant Gupta and Heng Ji. 2009. Predicting Un-
known Time Arguments based on Cross-event 
propagation. Proc. ACL-IJCNLP 2009. 
Dilek Hakkani-T?r, Fr?d?ric B?chet, Giuseppe Ric-
cardi, Gokhan Tur. 2006. Beyond ASR 1-Best: Us-
ing Word Confusion Networks in Spoken Lan-
guage Understanding. Journal of Computer Speech 
and Language, Vol. 20, No. 4, pp. 495-514. 
Laura Hasler and Constantin Orasan. 2009. Do 
coreferential arguments make event mentions 
coreferential? Proc. the 7th Discourse Anaphora 
and Anaphor Resolution Colloquium (DAARC 
2009).
Jose Iria and Joao Magalhaes. 2009. Exploiting 
Cross-Media Correlations in the Categorization 
of Multimedia Web Documents. Proc. CIAM 
2009.
H. V. Jagadish, Jason Madar, and Raymond Ng. 1999. 
Semantic compression and pattern extraction 
with fascicles. VLDB, pages 186?197. 
Heng Ji, David Westbrook and Ralph Grishman. 
2005. Using Semantic Relations to Refine Corefer-
ence Decisions. Proc. HLT/EMNLP 05.
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction Through Cross-document Inference. 
Proc. ACL 2008.
Heng Ji. 2009. Mining Name Translations from 
Comparable Corpora by Creating Bilingual In-
formation Networks. Proc. ACL-IJCNLP 2009 
workshop on Building and Using Comparable 
Corpora (BUCC 2009): from parallel to non-
parallel corpora. 
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias 
Blume, John Wang, Shahram Khadivi, Richard 
Zens and Hermann Ney. 2009a. Name Transla-
514
tion for Distillation. Book chapter for Global 
Automatic Language Exploitation.
Heng Ji, Ralph Grishman, Zheng Chen and Prashant 
Gupta. 2009b. Cross-document Event Extraction, 
Ranking and Tracking. Proc. RANLP 2009.
Hongyan Jing, Nanda Kambhatla and Salim Roukos. 
2007. Extracting Social Networks and Bio-
graphical Facts From Conversational Speech 
Transcripts. Proc. ACL 2007.
A. Klementiev and D. Roth. 2006. Named Entity 
Transliteration and Discovery from Multilingual 
Comparable Corpora. Proc. HLT-NAACL 2006.
Joao Magalhaes, Fabio Ciravegna and Stefan Ruger. 
2008. Exploring Multimedia in a Keyword Space. 
Proc. ACM Multimedia.
Inderjeet Mani, Barry Schiffman and Jianping Zhang. 
2003. Inferring Temporal Ordering of Events in 
News. Proc. HLT-NAACL 2003. 
John Makhoul, Alex Baron, Ivan Bulyko, Long 
Nguyen, Lance Ramshaw, David Stallard, Richard 
Schwartz and Bing Xiang. 2005. The Effects of 
Speech Recognition and Punctuation on Informa-
tion Extraction Performance. Proc. Interspeech.
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Data-
bases. Proc. HLT/NAACL 2007.
Andrew McCallum. 2006. Information Extraction, 
Data Mining and Joint Inference. Proc. SIGKDD.
Paul McNamee and Hoa Dang. 2009. Overview of 
the TAC 2009 Knowledge Base Population 
Track. Proc. TAC 2009 Workshop.
Gilad Mishne and Maarten de Rijke. 2006. Capturing 
Global Mood Levels using Blog Posts. Proc. AAAI 
2006 Spring Symposium on Computational Ap-
proaches to Analysing Weblogs. 
Vincent Ng. 2009. Graph-Cut-Based Anaphoricity 
Determination for Coreference Resolution. Proc.
HLT-NAACL 2009.
Lawrence Page, Sergey Brin, Rajeev Motwani and 
Terry Winograd. 1998. The PageRank Citation 
Ranking: Bringing Order to the Web. Proc. WWW. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Uni-
fied Model of Phrasal and Sentential Evidence 
for Information Extraction. 2009. Proc. EMNLP. 
Matt Richardson and Pedro Domingos. 2006. Markov 
Logic Networks. Machine Learning. 62:107-136. 
Ellen Riloff, Charles Schafer, and David Yarowsky. 
2002. Inducing Information Extraction Systems 
for New Languages via Cross-Language Projec-
tion. Proc. COLING 2002.
Alan Ritter; Stephen Soderland; Doug Downey; Oren 
Etzioni. 2009. It?s a Contradiction ? no, it?s not: 
A Case Study using Functional Relations. Proc. 
EMNLP 2009. 
Dan Roth and Wen-tau Yih. 2004. A Linear Pro-
gramming Formulation for Global Inference in 
Natural Language Tasks. Proc. CONLL2004.
Saggion, H., Cunningham, H., Bontcheva, K., May-
nard, D., Hamza, O., and Wilks, Y. 2004. Multi-
media indexing through multi-source and multi-
language information extraction: the MUMIS pro-
ject. Data Knowlege Engineering, 48, 2, pp. 247-
264. 
Roser Saur? and Marc Verhagen and James Puste-
jovsky. 2006. Annotating and Recognizing Event 
Modality in Text. Proc. FLAIRS 2006.
Len Seligman, Peter Mork, Alon Halevy, Ken Smith, 
Michael J. Carey, Kuang Chen, Chris Wolf, 
Jayant Madhavan and Akshay Kannan. 2010. 
OpenII: An Open Source Information Integration 
Toolkit. Proc. the 2010 international conference 
on Management of data.
Charles Sutton and Andrew McCallum. 2004. 
Collective Segmentation and Labeling of Distant 
Entities in Information Extraction.  Proc. ICML 
Workshop on Statistical Relational Learning and 
Its Connections to Other Fields. 
Roman Yangarber. 2006. Verification of Facts across 
Document Boundaries. Proc. International 
Workshop on Intelligent Information Access.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised Methods for Determining Object and Rela-
tion Synonyms on the Web. Journal of Artificial 
Intelligence. Res. (JAIR) 34: 255-296. 
Xiaoxin Yin, Jiawei Han and Philip S. Yu. 2008. 
Truth Discovery with multiple conflicting infor-
mation providers on the web. IEEE Trans. 
Knowledge and Data Eng., 20:796-808. 
515
Coling 2010: Poster Volume, pages 630?638,
Beijing, August 2010
Enhancing Multi-lingual Information Extraction
via Cross-Media Inference and Fusion 
Adam Lee, Marissa Passantino, Heng Ji 
Computer Science Department 
Queens College and Graduate Center 
City University of New York 
hengji@cs.qc.cuny.edu
Guojun Qi, Thomas Huang 
Department of Electrical and Computer  
Engineering & Beckman Institute 
University of Illinois at Urbana-Champaign 
huang@ifp.uiuc.edu
Abstract
We describe a new information fusion 
approach to integrate facts extracted 
from cross-media objects (videos and 
texts) into a coherent common represen-
tation including multi-level knowledge 
(concepts, relations and events). Beyond 
standard information fusion, we ex-
ploited video extraction results and sig-
nificantly improved text Information Ex-
traction. We further extended our meth-
ods to multi-lingual environment (Eng-
lish, Arabic and Chinese) by presenting 
a case study on cross-lingual comparable 
corpora acquisition based on video com-
parison.
1 Introduction 
An enormous amount of information is widely 
available in various data modalities (e.g. speech, 
text, image and video). For example, a Web 
news page about ?Health Care Reform in 
America? is composed with texts describing 
some events (e.g., Final Senate vote for the 
reform plans, Obama signs the reform 
agreement), images (e.g., images about various 
government involvements over decades) and 
videos/speech (e.g. Obama?s speech video about 
the decisions) containing additional information 
regarding the real extent of the events or 
providing evidence corroborating the text part. 
These cross-media objects exist in redundant 
and complementary structures, and therefore it 
is beneficial to fuse information from various 
data modalities. The goal of our paper is to 
investigate this task from both mono-lingual and 
cross-lingual perspectives. 
  The processing methods of texts and 
images/videos are typically organized into two 
separate pipelines. Each pipeline has been 
studied separately and quite intensively over the 
past decade. It is critical to move away from 
single media processing, and instead toward 
methods that make multiple decisions jointly 
using cross-media inference. For example, video 
analysis allows us to find both entities and 
events in videos, but it?s very challenging to 
specify some fine-grained semantic types such 
as proper names (e.g. ?Obama Barack?) and 
relations among concepts; while the speech 
embedded and the texts surrounding these 
videos can significantly enrich such analysis. On 
the other hand, image/video features can 
enhance text extraction.  For example, entity 
gender detection from speech recognition output 
is challenging because of entity mention 
recognition errors. However, gender detection 
from corresponding images and videos can 
achieve above 90% accuracy (Baluja and Rowley, 
2006). In this paper, we present a case study on 
gender detection to demonstrate how text and 
video extractions can boost each other. 
  We can further extend the benefit of cross-
media inference to cross-lingual information 
extraction (CLIE). Hakkani-Tur et al (2007) 
found that CLIE performed notably worse than 
monolingual IE, and indicated that a major 
cause was the low quality of machine translation 
(MT). Current statistical MT methods require 
large and manually aligned parallel corpora as 
input for each language pair of interest. Some 
recent work (e.g. Munteanu and Marcu, 2005; Ji, 
2009) found that MT can benefit from multi-
lingual comparable corpora (Cheung and Fung, 
2004), but it is time-consuming to identify pairs 
of comparable texts; especially when there is 
630
lack of parallel information such as news release 
dates and topics. However, the images/videos 
embedded in the same documents can provide 
additional clues for similarity computation 
because they are ?language-independent?. We 
will show how a video-based comparison 
approach can reliably build large comparable 
text corpora for three languages: English, 
Chinese and Arabic. 
2 Baseline Systems 
We apply the following state-of-the-art text and 
video information extraction systems as our 
baselines. Each system can produce reliable 
confidence values based on statistical models. 
2.1 Video Concept Extraction 
The video concept extraction system was 
developed by IBM for the TREC Video 
Retrieval Evaluation (TRECVID-2005) 
(Naphade et al, 2005). This system can extract 
2617 concepts defined by TRECVID, such as 
"Hospital", "Airplane" and "Female-Person". It 
uses support vector machines to learn the 
mapping between low level features extracted 
from visual modality as well as from transcripts 
and production related meta-features. It also 
exploits a Correlative Multi-label Learner (Qi et 
al., 2007), a Multi-Layer Multi-Instance Kernel 
(Gu et al, 2007) and Label Propagation through 
Linear Neighborhoods (Wang et al, 2006) to 
extract all other high-level features. For each 
classifier, different models are trained on a set 
of different modalities (e.g., the color moments, 
wavelet textures, and edge histograms), and the 
predictions made by these classifiers are 
combined together with a hierarchical linearly-
weighted fusion strategy across different 
modalities and classifiers. 
2.2 Text Information Extraction 
We use a state-of-the-art IE system (Ji and 
Grishman, 2008) developed for the Automatic 
Content Extraction (ACE) program1 to process 
texts and automatic speech recognition output. 
The pipeline includes name tagging, nominal 
mention tagging, coreference resolution, time 
expression extraction and normalization, rela-
tion extraction and event extraction. Entities 
1 http://www.nist.gov/speech/tests/ace/ 
include coreferred persons, geo-political entities 
(GPE), locations, organizations, facilities, vehi-
cles and weapons; relations include 18 types 
(e.g. ?a town some 50 miles south of Salzburg?
indicates a located relation.); events include the 
33 distinct event types defined in ACE 2005 
(e.g. ?Barry Diller on Wednesday quit as chief 
of Vivendi Universal Entertainment.? indicates a 
?personnel-start? event). Names are identified 
and classified using an HMM-based name tag-
ger. Nominals are identified using a maximum 
entropy-based chunker and then semantically 
classified using statistics from ACE training 
corpora. Relation extraction and event extraction 
are also based on maximum entropy models, 
incorporating diverse lexical, syntactic, semantic 
and ontological knowledge. 
3 Mono-lingual Information Fusion 
and Inference 
3.1 Mono-lingual System Overview 
                                                 
                        
                         
Figure 1. Mono-lingual Cross-Media  
Information Fusion and Inference Pipeline 
Figure 1 depicts the general procedure of our 
mono-lingual information fusion and inference 
Multi-media  
Document 
Enhanced  
Concepts/Entities 
Relations/Events
Texts
Text Information
Extraction
Video Concept 
Extraction 
Entities/ 
Relations/Events
Concepts 
Partitioning 
Multi-level Concept Fusion 
ASR
Global Inference 
Speech Videos/Images 
631
approach. After we apply two baseline systems 
to the multi-media documents, we use a novel 
multi-level concept fusion approach to extract a 
common knowledge representation across texts 
and videos (section 3.2), and then apply a global 
inference approach to enhance fusion results 
(section 3.3). 
3.2 Cross-media Information Fusion 
? Concept Mapping 
For each input video, we apply automatic speech 
recognition to obtain background texts. Then we 
use the baseline IE systems described in section 
2 to extract concepts from texts and videos. We 
construct mappings on the overlapped facts 
across TRECVID and ACE. For example, 
?LOC.Water-Body? in ACE is mapped to 
?Beach, Lakes, Oceans, River, River_Bank? in 
TRECVID.
  Due to different characteristics of video clips 
and texts, these two tasks have quite different 
granularities and focus. For example, 
?PER.Individual? in ACE is an open set includ-
ing arbitrary names, while TRECVID only cov-
ers some famous proper names such as 
?Hu_Jintao? and ?John_Edwards?. Geopolitical 
entities appear very rarely in TRECVID because 
they are more explicitly presented in back-
ground texts. On the other hand, TRECVID de-
fined much more fine-grained nominals than 
ACE, for example, ?FAC.Building-Grounds? in 
ACE can be divided into 52 possible concept
types such as ?Conference_Buildings? and 
?Golf_Course? because they can be more easily 
detected based on video features. We also notice 
that TRECVID concepts can include multiple 
levels of ACE facts, for example 
?WEA_Shooting? concept can be separated into 
?weapon? entities and ?attack? events in ACE. 
These different definitions bring challenges to 
cross-media fusion but also opportunities to ex-
ploit complementary facts to refine both pipe-
lines. We manually resolved these issues and 
obtained 20 fused concept sets. 
? Time-stamp based Multi-level Projection 
After extracting facts from videos and texts, we 
conduct information fusion at all possible levels: 
name, nominal, coreference link, relation or 
event mention. We rely on the timestamp infor-
mation associated with video keyframes or shots 
(sequential keyframes) and background speech 
to align concepts. During this fusion process, we 
compare the normalized confidence values pro-
duced from two pipelines to resolve the follow-
ing three types of cases:
? Contradiction ? A video fact contradicts a 
text fact; we only keep the fact with higher 
confidence.
? Redundancy ? A video fact conveys the 
same content as (or entails, or is entailed by) 
a text fact; we only keep the unique parts of 
the facts. 
? Complementary ? A video fact and a text 
fact are complementary; we merge these 
two to form more complete fact sets. 
? A Common Representation 
In order to effectively extract compact informa-
tion from large amounts of heterogeneous data, 
we design an integrated XML format to repre-
sent the facts extracted from the above multi-
level fusion. We can view this representation as 
a set of directed ?information graphs? G={Gi
(Vi, Ei)}, where Vi is the collection of concepts 
from both texts and videos, and Ei is the collec-
tion of edges linking one concept to the other, 
labeled by relation or event attributes. An exam-
ple is presented in Figure 2. This common rep-
resentation is applied in both mono-lingual and 
multi-lingual information fusion tasks described 
in next sections. 
Figure 2. An example for cross-media common 
fact representation  
3.3 Cross-media Information Inference 
? Uncertainty Problem in Cross-Media Fu-
sion
However, such a simple merging approach usu-
ally leads to unsatisfying results due to uncer-
tainty. Uncertainty in multimedia is induced 
from noise in the data acquisition procedure 
Child 
PLO
British 
Mandate of
Palestine 
Safed
Mahmoud 
Abbas
Leader
Birth-Place
Located
Amina Abbas 
Spouse
Yasser
 Abbas
PLO
Elected/
2008-11-23 
632
(e.g., noise in automatic speech recognition re-
sults and low-quality camera surveillance vid-
eos) as well as human errors and subjectivity. 
Unstructured texts, especially those translated 
from foreign languages, are difficult to interpret. 
In addition, automatic IE systems for both vid-
eos and texts tend to produce errors.  
? Case Study on Mention Gender Detection 
We employ cross-media inference methods to 
reduce uncertainty. We will demonstrate this 
approach on a case study of gender detection for 
persons. Automatic gender detection is crucial 
to many natural language processing tasks such 
as pronoun reference resolution (Bergsma, 
2005). Gender detection for last names has 
proved challenging; Gender for nominals can be 
highly ambiguous in various contexts. Unfortu-
nately most state-of-the-art approaches discover 
gender information without considering specific 
contexts in the document. The results were 
stored either as a knowledge base with prob-
abilities (e.g. Ji and Lin, 2009) or as a static 
gazetteer (e.g. census data). Furthermore, speech 
recognition normally performs poorly on names, 
which brings more challenges to gender detec-
tion for mis-spelled names.   
  We consider two approaches as our baselines. 
The first baseline is to discover gender knowl-
edge from Google N-grams using specific lexi-
cal patterns (e.g. ?[mention] and 
his/her/its/their?) (Ji and Lin, 2009). The other 
baseline is a gazetteer matching approach based 
on census data including person names and gen-
der information, as used in typical text IE sys-
tems.  
  We introduce the third method based on 
male/female concept extraction from associated 
background videos. These concepts are detected 
from context-dependent features (e.g. face rec-
ognition). If there are multiple persons in one 
snippet associated with one shot, we propagate 
gender information to all instances.
  We then linearly combine these three methods 
based on confidence values. For example, the 
confidence of predicting a name mention n as a 
male (M) can be computed by combining prob-
abilities P(n, M, method):
confidence(n,male)=?1*P(n,M,ngram)+
?2*P(n,M,census) +?3*P(n,M,video)
  In this paper we used?1=0.1, ?2=0.1 
and?3=0.8 which are optimized from a devel-
opment set. 
4 Cross-lingual Comparable Corpora 
Acquisition
In this section we extend the information fusion 
approach to a task of discovering comparable 
corpora.
4.1 Comparable Documents  
Figure 3 presents an example of cross-lingual 
comparable documents. They are both about the 
rescue activities for the Haiti earthquake. 
Figure 3. An example for cross-lingual  
multi-media comparable documents 
Figure 4. Cross-lingual Comparable Text  
Corpora Acquisition based on
Video Similarity Computation 
Multi-media  
Document in 
Language j
Multi-media 
Document in
Language i
Facts-Vi
Similarity Computation 
Text T i Video V i
Concept 
Extraction 
Facts-Vj
Video V i 
Concept 
Extraction 
Text T j
Similarity>??
Comparable Docu-
ments <Ti, Tj>
633
  Traditional text translation based methods tend 
to miss such pairs due to poor translation quality 
of informative words (Ji et al, 2009). However, 
the background videos and images are language-
independent and thus can be exploited to iden-
tify such comparable documents. This provides 
a cross-media approach to break language bar-
rier.
4.2 Cross-lingual System Overview 
Figure 4 presents the general pipeline of discov-
ering cross-lingual comparable documents based 
on background video comparison. The detailed 
video similarity computation method is pre-
sented in next section. 
4.3 Video Concept Similarity Computation 
Most document clustering systems use represen-
tations built out of the lexical and syntactic at-
tributes. These attributes may involve string 
matching, agreement, syntactic distance, and 
document release dates. Although gains have 
been made with such methods, there are clearly 
cases where shallow information will not be suf-
ficient to resolve clustering correctly. Therefore, 
we should therefore expect a successful docu-
ment comparison approach to exploit world 
knowledge, inference, and other forms of se-
mantic information in order to resolve hard 
cases. For example, if two documents include 
concepts referring to male-people, earthquake 
event, rescue activities, and facility-grounds 
with similar frequency information, we can de-
termine they are likely to be comparable. In this 
paper we represent each video as a vector of 
semantic concepts extracted from videos and 
then use standard vector space model to com-
pute similarity.  
  Let A=(a1, ?a|?|) and B=(b1, ?b|?|) be such 
vectors for a pair of videos, then we use cosine 
similarity to compute similarity: 
| |
1
| | | |2 2
1 1
cos( , ) i ii
i ii i
a b
A B
a b
=
= =
?
=
? ?
?
? ?
,
where |? | contains all possible concepts. We 
use traditional TF-IDF (Term Frequency-Inverse 
Document Frequency) weights for the vector 
elements ai and bi. Let C be a unique concept, V
is a video consisting of a series of k shots V = 
{S1, ?, Sk}, then: 
1
( , ) ( , )
k
ii
tf C V tf C S k
=
=?
Let p(C, Si) denote the probability that C is ex-
tracted from Si, we define two different ways to 
compute term frequency tf (C, Si):
(1) ( , ) ( , )i itf C S confidence C S=
and
(2) ( , )( , ) iconfidence C Sitf C S ?=
Where Confidence (C, Si) denotes the probabil-
ity of detecting a concept C in a shot Si:
( , ) ( , )i iconfidence C S p C S= if ( , )ip C S ?> ,
                                       otherwise 0. 
Let: ( , ) 1idf C S = if ( , )ip C S ?> , otherwise 0, 
assuming there are j shots in the entire corpus, 
we calculate idf as follows: 
1
( , ) log / ( , )
j
i
i
idf C V j df C S
=
? ?
= ? ?
? ?
?
5 Experimental Results 
This section presents experimental results of all 
the three tasks described above. 
5.1 Data
We used 244 videos from TRECVID 2005 data 
set as our test set. This data set includes 133,918 
keyframes, with corresponding automatic 
speech recognition and translation results (for 
foreign languages) provided by LDC.   
5.2 Information Fusion Results 
Table 1 shows information fusion results for 
English, Arabic and Chinese on multiple levels. 
It indicates that video and text extraction pipe-
lines are complementary ? almost all of the 
video concepts are about nominals and events; 
while text extraction output contains a large 
amount of names and relations. Therefore the 
results after information fusion produced much 
richer knowledge. 
634
Annotation Lev-
els
English Chinese Arabic 
# of videos 104 84 56 
Video Concept 250880 221898 197233 
Name 17350 22154 20057 
Nominal 31528 21852 16253 
Relation 9645 20880 16584 
Text
Event 31132 10348 7148 
Table 1. Information Fusion Results  
  It?s also worth noting that the number of con-
cepts extracted from videos is similar across 
languages, while much fewer events are ex-
tracted from Chinese or Arabic because of 
speech recognition and machine translation er-
rors. We took out 1% of the results to measure 
accuracy against ground-truth in TRECVID and 
ACE training data respectively; the mean aver-
age precision for video concept extraction is 
about 33.6%. On English ASR output the text-
IE system achieved about 82.7% F-measure on 
labeling names, 80.5% F-measure on nominals 
(regardless of ASR errors), 66% on relations 
and 64% on events. 
5.3 Information Inference Results 
From the test set, we chose 650 persons (492 
males and 158 females) to evaluate gender dis-
covery. For baselines, we used Google n-gram 
(n=5) corpus Version II including 1.2 billion 5-
grams extracted from about 9.7 billion sentences 
(Lin et al, 2010) and census data including  
5,014 person names with gender information. 
  Since we only have gold-standard gender in-
formation on shot-level (corresponding to a 
snippet in ASR output), we asked a human an-
notator to associate ground-truth with individual 
persons. Table 2 presents overall precision (P), 
recall (R) and F-measure (F).  
Methods P R F 
Google N-gram 89.1% 70.2% 78.5%
Census 96.2% 19.4% 32.4%
Video Extraction 88.9% 73.8% 80.6%
Combined 89.3% 80.4% 84.6%
Table 2.  Gender Discovery Performance 
  Table 2 shows that video extraction based ap-
proach can achieve the highest recall among all 
three methods. The combined approach 
achieved statistically significant improvement 
on recall. 
  Table 3 presents some examples (?F? for fe-
male and ?M? for male). We found that most 
speech name recognition errors are propagated 
to gender detection in the baseline methods, for 
example, ?Sala Zhang? is mis-spelled in speech 
recognition output (the correct spelling should 
be ?Sarah Chang?) and thus Google N-gram 
approach mistakenly predicted it as a male. 
Many rare names such as ?Wu Ficzek?, 
?Karami? cannot be predicted by the baselines,  
  Error analysis on video extraction based ap-
proach showed that most errors occur on those 
shots including multiple people (males and fe-
males). In addition, since the data set is from 
news domain, there were many shots including 
reporters and target persons at the same time. 
For example, ?Jiang Zemin? was mistakenly 
associated with a ?female? gender because the 
reporter is a female in that corresponding shot. 
5.4 Comparable Corpora Acquisition Re-
sults
For comparable corpora acquisition, we meas-
ured accuracy for the top 50 document pairs. 
Due to lack of answer-keys, we asked a bi-
lingual human annotator to judge results manu-
ally. The evaluation guideline generally fol-
lowed the definitions in (Cheung and Fung, 
2004). A pair of documents is judged as compa-
rable if they share a certain amount of informa-
tion (e.g. entities, events and topics). 
Without using IDF, for different parameter ?
and ? in the similarity metrics, the results are 
summarized in Figure 5. For comparison we 
present the results for mono-lingual and cross- 
lingual separately. Figure 5 indicates that as the 
threshold and normalization values increase, the 
accuracy generally improves. It?s not surprising 
that mono-lingual results are better than cross-
lingual results, because generally more videos 
with comparable topics are in the same language.   
635
Mention
Google
N-gram 
Census
Video
Extraction
Correct
Answer
Context Sentence 
Zhang
Sala
M: 1 
F: 0 
-
F: 0.699 
M: 0.301 
F
World famous meaning violin soloist 
Zhang Sala recently again to Toronto sym-
phony orchestra... 
Peter
M: .979 
F: 0.021 
M: 1 
M: 0.699 
F: 0.301 
M
Iraq, there are in Lebanon Paris pass Peter
after 10 five Dar exile without peace... 
Wu
Ficzek
-
M: 0.699 
F: 0.301 
M
If you want to do a good job indeed Wu
Ficzek
President
M: .953 
F: 0.047 
-
M: 0.704 
F: 0.296 
M
Labor union of Arab heritage publishers 
president to call for the opening of the 
Arab Book Exhibition. 
Jiang
Zemin 
M: 1 
F: 0 
-
F: 0.787 
M: 0.213 
M
It has never stopped the including the for-
mer CPC General Secretary Jiang Zemin? 
Karami 
M: 1 
F: 0 
-
M: 0.694 
F: 0.306 
M
all the Gamal Ismail introduced the needs 
of the Akkar region, referring to the desire 
on the issue of the President Karami to 
give priority disadvantaged areas 
Table 3. Examples for Mention Gender Detection 
Figure 5. Comparable Corpora Acquisition  
without IDF 
  We then added IDF to the optimized threshold 
and obtained results in Figure 6. The accuracy 
for both languages was further enhanced. We can 
see that under any conditions our approach can 
discover comparable documents reliably. In or-
der to measure the impact of concept extraction 
errors, we also evaluated the results for using 
ground-truth concepts as shown in Figure 6. Sur-
prisingly it didn?t provide much higher accuracy 
than automatic concept extraction, mainly be-
cause the similarity can be captured by some 
dominant video concepts. 
Figure 6. Comparable Corpora Acquisition with 
IDF (?=0.6)
6 Related Work 
A large body of prior work has focused on multi-
media information retrieval and document classi-
fication (e.g. Iria and Magalhaes, 2009).  State-
of-the-art information fusion approaches can be 
divided into two groups: formal ?top-down? 
methods from the generic knowledge fusion 
community and quantitative ?bottom-up? tech-
niques from the Semantic Web community (Ap-
priou et al, 2001; Gregoire, 2006). However, 
very limited research methods have been ex-
636
plored to fuse automatically extracted facts from 
texts and videos/images. Our idea of conducting 
information fusion on multiple semantic levels is 
similar to the kernel method described in (Gu et 
al., 2007). 
  Most previous work on cross-media information 
extraction focused on one single domain (e.g. e-
Government (Amato et al, 2010); soccer game 
(Pazouki and Rahmati, 2009)) and struc-
tured/semi-structured texts (e.g. product cata-
logues (Labsky et al, 2005)). Saggion et al 
(2004) described a multimedia extraction ap-
proach to create composite index from multiple 
and multi-lingual sources. We expand the task to 
the more general news domain including unstruc-
tured texts and use cross-media inference to en-
hance extraction performance. 
  Some recent work has exploited analysis of as-
sociated texts to improve image annotation (e.g. 
Deschacht and Moens, 2007; Feng and Lapata, 
2008). Some recent research demonstrated cross-
modal integration can provide significant gains 
in improving the richness of information. For 
example, Oviatt et al (1997) showed that speech 
and pen-based gestures can provide complemen-
tary capabilities because basic subject, verb, and 
object constituents almost always are spoken, 
whereas those describing locative information 
invariably are written or gestured. However, not 
much work demonstrated an effective method of 
using video/image annotation to improve text 
extraction. Our experiments provide some case 
studies in this new direction. Our work can also 
be considered as an extension of global back-
ground inference (e.g. Ji and Grishman, 2008) to 
cross-media paradigm. 
  Extensive research has been done on video clus-
tering. For example, Cheung and Zakhor (2000)
used meta-data extracted from textual and hyper-
link information to detect similar videos on the 
web; Magalhaes et al (2008) described a seman-
tic similarity metric based on key word vectors 
for multi-media fusion. We extend such video 
similarity computing approaches to a multi-
lingual environment. 
7 Conclusion and Future Work 
Traditional Information Extraction (IE) ap-
proaches focused on single media (e.g. texts), 
with very limited use of knowledge from other 
data modalities in the background. In this paper 
we propose a new approach to integrate informa-
tion extracted from videos and texts into a coher-
ent common representation including multi-level 
knowledge (concepts, relations and events). Be-
yond standard information fusion, we attempted 
global inference methods to incorporate video 
extraction and significantly enhanced the per-
formance of text extraction. Finally, we extend 
our methods to multi-lingual environment (Eng-
lish, Arabic and Chinese) by presenting a case 
study on cross-lingual comparable corpora acqui-
sition.
  We used a dataset which includes videos and 
associated speech recognition output (texts), but 
our approach is applicable to any cases in which 
texts and videos appear together (from associated 
texts, captions etc.). The proposed common rep-
resentation will provide a framework for many 
byproducts. For example, the monolingual fused 
information graphs can be used to generate ab-
stractive summaries. Given the fused information 
we can also visualize the facts from background 
texts effectively. We are also interested in using 
video information to discover novel relations and 
events which are missed in the text IE task. 
Acknowledgement 
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement 
Number W911NF-09-2-0053, the U.S. NSF 
CAREER Award under Grant IIS-0953149, 
Google, Inc., DARPA GALE Program, CUNY 
Research Enhancement Program, PSC-CUNY 
Research Program, Faculty Publication Program 
and GRTI Program. The views and conclusions 
contained in this document are those of the au-
thors and should not be interpreted as represent-
ing the official policies, either expressed or im-
plied, of the Army Research Laboratory or the 
U.S. Government. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Government purposes notwithstanding any copy-
right notation here on. 
References 
Amato, F., Mazzeo, A.,  Moscato, V. and Picariello, 
A.  2010. Information Extraction from Multimedia 
Documents for e-Government Applications. 
Information Systems: People, Organizations, Insti-
tutions, and Technologies. pp. 101-108. 
637
Appriou A., A. Ayoun, Benferhat, S., Besnard, P., 
Cholvy, L., Cooke, R., Cuppens, F., Dubois, D., 
Fargier, H., Grabisch, M., Kruse, R., Lang, J. 
Moral, S., Prade, H., Saffiotti, A., Smets, P., Sos-
sai, C. 2001. Fusion: General concepts and charac-
teristics. International Journal of Intelligent Sys-
tems 16(10).
Baluja, S. and Rowley, H. 2006. Boosting Sex Identi-
fication Performance. International Journal of 
Computer Vision. 
Bergsma, S. 2005. Automatic Acquisition of Gender 
Information for Anaphora Resolution. Proc. Cana-
dian AI 2005. 
Cheung, P. and Fung P. 2004. Sentence Alignment in 
Parallel, Comparable, and Quasi-comparable Cor-
pora. Proc.  LREC 2004. 
Cheung, S.-C.  and Zakhor, A. 2000. Efficient video 
similarity measurement and search. Proc. IEEE In-
ternational Conference on Image Processing. 
Deschacht K. and Moens M. 2007. Text Analysis for 
Automatic Image Annotation. Proc. ACL 2007. 
Feng, Y. and Lapata, M. 2008. Automatic Image An-
notation Using Auxiliary Text Information. Proc. 
ACL 2008. 
Gregoire, E. 2006. An unbiased approach to iterated 
fusion by weakening. Information Fusion. 7(1). 
Gu, Z., Mei, T., Hua, X., Tang, J., Wu, X. 2007. 
Multi-Layer Multi-Instance Kernel for Video Con-
cept Detection. Proc. ACM Multimedia 2007. 
Hakkani-Tur, D., Ji, H. and Grishman, R. 2007. Using 
Information Extraction to Improve Cross-lingual 
Document Retrieval. Proc. RANLP 2007 Workshop 
on Multi-Source Multi-lingual Information Extrac-
tion and Summarization.
Iria, J. and Magalhaes, J. 2009. Exploiting Cross-
Media Correlations in the Categorization of Mul-
timedia Web Documents. Proc. CIAM 2009. 
Ji, H. and Grishman, R. 2008. Refining Event Extrac-
tion Through Cross-document Inference. Proc. 
ACL 2008. 
Ji, H. 2009. Mining Name Translations from Compa-
rable Corpora by Creating Bilingual Information 
Networks. Proc. ACL-IJCNLP 2009 workshop on 
Building and Using Comparable Corpora (BUCC 
2009): from parallel to non-parallel corpora.
Ji, H., Grishman, R., Freitag, D., Blume, M., Wang, 
J., Khadivi, S., Zens, R., and Ney, H. 2009. Name 
Translation for Distillation. Handbook of Natural 
Language Processing and Machine Translation: 
DARPA Global Autonomous Language Exploita-
tion. Springer. 
Ji, H. and Lin, D. 2009. Gender and Animacy Knowl-
edge Discovery from Web-Scale N-Grams for Un-
supervised Person Mention Detection. Proc. PA-
CLIC 2009. 
Oviatt, S. L., DeAngeli, A., & Kuhn, K. 1997. Inte-
gration and synchronization of input modes during 
multimodal human-computer interaction. Proceed-
ings of Conference on Human Factors in Comput-
ing Systems (CHI?97), 415-422. New York: ACM 
Press.
Labsky, M., Praks, P., Sv?atek1, V., and Svab, O. 
2005. Multimedia Information Extraction from 
HTML Product Catalogues. Proc. 2005 
IEEE/WIC/ACM International Conference on Web 
Intelligence. pp. 401 ? 404.   
Lin, D., Church, K.,  Ji, H., Sekine, S., Yarowsky, D.,  
Bergsma, S., Patil, K., Pitler, E., Lathbury, R., Rao, 
V., Dalwani, K. and Narsale, S. 2010. New Data, 
Tags and Tools for Web-Scale N-grams. Proc. 
LREC 2010. 
Magalhaes, J., Ciravegna, F. and Ruger, S. 2008. Ex-
ploring Multimedia in a Keyword Space. Proc. 
ACM Multimedia 2008. 
Munteanu, D. S. and Marcu D. 2005. Improving Ma-
chine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics. Vol-
ume 31, Issue 4. pp. 477-504. 
Naphade, M. R., Kennedy, L., Kender, J. R., Chang, 
S.-F., Smith, J. R., Over, P., and Hauptmann, A. A 
light scale concept ontology for multimedia under-
standing for TRECVID 2005. Technical report, 
IBM, 2005. 
Pazouki, E. and Rahmati, M. 2009. A novel multime-
dia data mining framework for information extrac-
tion of a soccer video stream. Intelligent Data 
Analysis. pp. 833-857. 
Qi,G.-J., Hua,X.-S., Rui, Y., Tang, J., Mei, T., and 
Zhang,H.-J. 2007. Correlative Multi-label Video 
Annotation. Proc. ACM Multimedia 2007.
Saggion, H., Cunningham, H., Bontcheva, K., 
Maynard, D., Hamza, O., and Wilks, Y. 2004. Mul-
timedia indexing through multi-source and multi-
language information extraction: the MUMIS pro-
ject. Data Knowlege Engineering, 48, 2, pp. 247-
264. 
Wang, F. and Zhang, C. 2006. Label propagation 
through linear neighborhoods. Proc. ICML 2006.
638
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1556?1566, Dublin, Ireland, August 23-29 2014.
Analysis and Refinement of Temporal Relation Aggregation
Taylor Cassidy
IBM Research
Army Research Laboratory
Adelphi, MD 20783, USA
taylor.cassidy.ctr@mail.mil
Heng Ji
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
jih@rpi.edu
Abstract
To obtain a complete temporal picture of a relation it is necessary to aggregate fragments of tem-
poral information across relation instances in text. This process is non-trivial even for humans
because temporal information can be imprecise and inconsistent, and systems face the additional
challenge that each of their classifications is potentially false. Even a small amount of incorrect
proposed temporal information about a relation can severely affect the resulting aggregate tempo-
ral knowledge. We motivate and evaluate three methods to modify temporal relation information
prior to aggregation to address this challenge.
1 Introduction
Temporal information about relations is conveyed in text at varying levels of completeness and speci-
ficity. A sentence may indicate that a relation starts, ends, or that it is ongoing at a particular time.
Furthermore, a time expression may be expressed at a variety of granularity levels (e.g., hour, day, or
year). For instance, ?Collins, ..., is a 61-year-old veteran who went 444-434 in six seasons as a man-
ager, 1994-1996 with Houston? provides bounds on both the start and end date of the a relation but
at a coarse granularity. Conversely, ?Ivory Coast President Laurent Gbagbo on state television Friday
dissolved parliament? conveys temporal information about an arbitrary part of Gbagbo?s presidency at a
finer granularity: the relation simply holds true at the document creation time (DCT). Single instances in
which a relation of interest is related to a time expression often fail to convey complete, fine-grained tem-
poral information. Thus, it is necessary to aggregate information from multiple relation-time temporal
relationship mentions to gain a complete temporal picture of a relation.
We focus on the aggregation of temporal information about relations within the context of the Tem-
poral Slot-Filling (TSF) Task (Ji et al., 2011; Surdeanu, 2013). TSF focusses on a class of relations
called fluents (Russell and Norvig, 2010), which are properties of named entities whose values may
vary over time. Systems must succinctly describe all temporal information about each query relationR ?
e.g., title(Gbagbo, President) ? available in a source document collection by assigning it a single, final
temporal four-tuple (Amigo et al., 2011). Given a relation mention r ofR and a time expression ?, a four-
tuple T
r
?
=
?
t
(1)
, t
(2)
, t
(3)
, t
(4)
?
characterizes their temporal relationship; namely, t
(1)
and t
(2)
represent
the earliest and latest possible start date forR, while t
(3)
and t
(4)
represent the earliest and latest possible
end dates, as inferred from the relation mention?s context (sec. 3). For instance, a sentence indicating
that Gbagbo was President on 2010-02-12 yields
?
??, 2010-02-12, 2010-02-12,+?
?
, while the sen-
tence ?Gbagbo has been in power since 2000? yields
?
2000-01-01, 2000-12-31, 2000-01-31,+?
?
. The
intuitively best aggregation of these four-tuples expresses what we learn from both texts, that the relation
started in 2000 and remained ongoing at 2010-02-12, i.e.
?
2000-01-01, 2010-02-12, 2010-02-12,+?
?
,
with no clear indication as to its end. Straightforward cases like these were used to justify the simple
aggregation methods used by all TSF systems to date (Surdeanu, 2013; Ji et al., 2011). However, in real-
ity even humans often must deal with vague and/or conflicting temporal information across documents,
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1556
and systems must furthermore deal with the fact that each of their temporal relationship classifications is
potentially false.
To address the various properties of text and temporal representation that influence aggregation and
affect final four-tuple quality, we first improve an existing gold standard dataset (sec. 4.1). We then
describe two key factors affecting systems? aggregation performance: (1) erroneous classifications at-
tributed high confidence by systems, and (2) a lack of relation-bounding classifications (sec. 4.2). We
propose three methods to better prepare a relation?s multiple mention context derived four-tuples for ag-
gregation into a final four-tuple. The first applies simple rules to predicative nominal titles with explicit
time information (e.g., ?former President?), the second filters and re-labels four-tuples based on entity
lifespan (sec. 5.3), and the third adds four-tuples based on mentions of relations other than, but tempo-
rally linked to, the query relation (sec. 5.4). We then discuss results and identify remaining challenges
for aggregating temporal information across relation mentions (sec. 6 and 7). A Glossary of selected
terms can be found in the appendix.
2 Related Work
The most similar work on temporal relation information aggregation are Wang et al. (2012), who use an
Integer Linear Programming framework to enforce the validity of induced temporal relation information
as well as enforce inter-relation constraints, and Dylla et al. (2013), who collect temporal information
about relations, mostly about start and end times, using a temporal probabilistic data base framework
to aggregate and enforce constraints based on relation argument existence. All TSF systems we are
aware of have used either max-constrain or Validity-Ensured Incremental max-constrain aggregation
algorithms (Surdeanu, 2013; Ji et al., 2011), which we cover in section 4. None we are aware of have
applied background knowledge to constrain intermediate four-tuples (sec. 3) before or after aggregation.
In this work we modified our previous work CUNYTSF (Artiles et al., 2011), which is the only publicly
available TSF system we are aware of. CUNYTSF employs two supervised models, one based on a string
kernel defined in terms of dependency paths between named entities involved in a relation and context
time expressions, and the other based on bags-of-words derived from small windows surrounding these
tokens and shallow dependency relations. CUNYTSF achieved the highest and second-highest scores of
five systems in two TSF shared tasks (Surdeanu, 2013; Ji et al., 2011).
3 Temporal Slot Filling (TSF)
The 2013 Temporal Slot-Filling (TSF) (Surdeanu, 2013) task was part of the Knowledge Base Popula-
tion (KBP) track of the Text Analysis Conference (TAC). Systems were given a list of 273 fluent relation
instances as queries, each with a supporting document. Query relations were evenly distributed across re-
lation types, which consisted of people?s titles, marriages, employments or memberships, and residences
(city, state, and country), and companies? top members or employees. The task was to obtain a final four-
tuple T
R
for each query relationR =
?
q, s
?
using the source corpus for provenance. For each element in
T
R
a system must provide a document in which R is entailed, and offsets for the relation arguments (the
query-entity q and slot-filler s) and the normalized time expression from which the four-tuple element is
derived.
The KBP source collection consists of about 1 million newswire, 1 million web text, and 100,000
discussion forum documents. Gold standard annotation was obtained by annotators who, using a tool,
searched the source corpus for documents that provide temporal information about each query relation.
Given a mention r of R in a document d for which temporal information about R could be inferred,
annotators assigned an intermediate temporal relationship label (Table 1) (Ji et al., 2011) to
?
r, ?
?
, where
? is viewed as an interval of dates [?
s
, ?
e
] derived either based on (1) a normalized time expression in
d, or (2) the document creation time of d. We denote the temporal extension of R at the day granularity
R
ex
= [R
s
, R
e
], where R
s
and R
e
are the start and end dates of R. The intermediate label l mediates the
relationship between ? and R
ex
, characterizing a possible relationship between R and ?.
1
After systems
submitted results for the shared task, any corresponding document not included in the original annotation
1
We add AFTER END* and BEFORE START* but omit motivation due to space constraints.
1557
that were determined to express R was exhaustively annotated for temporal information about R. A gold
standard final four-tupleG
R
is obtained for eachR by applying an aggregation procedure (sec. 4.1) to the
intermediate temporal relationship labels assigned to mention-time classification instances (Surdeanu,
2013).
In this work we adopt the evaluation metric used for the TSF shared task (Ji et al., 2011; Surdeanu,
2013).
Intermediate Relation four-tuple
BEGINNING
?
?
s
, ?
e
, ?
s
,?
?
ENDING
?
??, ?
e
, ?
s
, ?
e
?
BEG AND END
?
?
s
, ?
e
, ?
s
, ?
e
?
WITHIN
?
??, ?
e
, ?
s
,?
?
THROUGHOUT
?
??, ?
s
, ?
e
,?
?
BEFORE START
?
?
e
,?, ?
e
,?
?
AFTER END
?
??, ?
s
,??, ?
s
?
BEFORE START*
?
?
s
,?, ?
s
,?
?
AFTER END*
?
??, ?
e
,??, ?
e
?
NONE
?
??,?,??,?
?
Table 1: Intermediate temporal relationship func-
tion for
?
r, ?
?
Invalidity Source Frequency
Conflicting Information 13
Multiple Instances 7
Wrong Intermediate Label 20
Vague Time Normalization 8
Other 8
Table 2: Reasons for Invalidity in Gold Standard
Final Four-Tuples
4 Aggregating Intermediate Relations
Temporal information about instances of R must be aggregated to yield a complete temporal picture of
the relation with respect to the background corpus. We denote with I(R) the set of intermediate four-
tuples associated with R. The purpose of the four-tuple representation is to be as accurate as possible
in representing the extent to which a given corpus provides information about the start and end time of
R, R
s
and R
e
, while preserving the vagueness inherent in the text. Each four-tuple element of I(R)
represents temporal information about R
s
and/or R
e
, most often with respect to the context associated
with a particular mention r of R. Temporal information at a corpus level is derived via a process of
aggregation over the elements of I(R). In this section we describe how both human annotators and
systems have approached aggregation.
4.1 Aggregating Manually Annotated Intermediate Relations
Gold standard four-tuples were obtained by applying the Max-Constrain (MC) algorithm (Equation 1) to
each I(R) obtained via manual annotation using the labels in Table 1 (Surdeanu, 2013; Ji et al., 2011).
2
T
R
=
?
max(t
(1)
),min(t
(2)
),max(t
(3)
),min(t
(4)
)
?
(1)
Here, max(t
(k)
) is the greatest t
(k)
from any intermediate four-tuple T
r
? I(R), while min(t
(k)
) is the
least.
Let a four-tuple T be valid iff. t
(1)
? t
(2)
? t
(3)
? t
(4)
? t
(1)
? t
(4)
, and correct if t
(1)
? R
s
?
t
(2)
? t
(3)
? R
e
? t
(4)
. If R has only one start and one end date, and R
s
? R
e
, and each intermediate
four-tuple T
?
r
? I(R) is valid and correct, then the final four-tuple obtained via MC is guaranteed to be
valid and correct. Fifty-six gold standard final four-tuples were invalid and therefore discarded prior to
evaluation (Surdeanu, 2013). We analyzed them by hand to determine the source of their invalidity (see
Table 2).
3
We corrected instances until IMC (Algorithm 1) yielded a valid four-tuple.
2
See http://surdeanu.info/kbp2013 for more details.
3
Note that there may be more instances of each type described in table 2
4
Here, max(t
(i)
? x
(i)
) := max(
{
t
(i)
? t
(i)
?
?
?t
(i)
? x
(i)
}
), where t
(i)
:=
{
t
(i)
? T
?
?
?T ? I(R)
}
1558
Algorithm 1 Inclusive Max-Constrain (IMC)
4
Require: I(R) = {T
0
, T
1
, . . . , T
N?1
}
Ensure: T
R
X ? max-constrain(I(R)) =
?
x
(1)
, x
(2)
, x
(3)
, x
(4)
?
Y ?
?
max(t
(1)
? x
(2)
),min(t
(2)
? x
(1)
),max(t
(3)
? x
(4)
),min(t
(4)
? x
(3)
)
?
T
R
?
?
max(t
(1)
? y
(2)
),min(t
(2)
? y
(1)
),max(t
(3)
? y
(4)
)
?
,min(t
(4)
? y
(3)
)
return T
R
4.2 System Derived Intermediate Relations
As suggested in section 4.1, MC is sensitive to inconsistent four-tuples. In response to this all prior
work that has not used MC to combine system-produced I(R) has used an algorithm similar to Validity-
Ensured Incremental (VEI) Max-Constrain (Algorithm 2) (Artiles et al., 2011). Here, I(R) is ordered
by classifier confidence and T
R
is initialized as the trivial four-tuple and updated incrementally. Starting
with the highest-confidence four-tuple T
R,0
? I(R), MC is applied to {T
R
, T
R,i
} to yield T
?
. In a given
iteration, T
?
is only accepted as the updated T
R
if it is valid. Intuitively, higher confidence intermediate
four-tuples are more likely to be correct, thus the incremental algorithm tries to ensure that erroneous
low-confidence four-tuples are less likely to be aggregated. In practice, however, a single high-confidence
incorrect label can derail the entire process (sec. 5).
Algorithm 2 Validity-Ensured Incremental (VEI) Max-Constrain Aggregation to yield final four-tuple
Require: I(R) = {T
0
, T
1
, . . . , T
N?1
}
Ensure: T
R
=
?
t
(1)
, t
(2)
, t
(3)
, t
(4)
?
T
R
?
?
??,?,??,?
?
i? 0
while i < N do
T
?
?
?
max(t
(1)
, t
(1)
i
),min(t
(2)
, t
(2)
i
),max(t
(3)
, t
(3)
i
),max(t
(4)
, t
(4)
i
)
?
{Pairwise MC}
if t
?(1)
? t
?(2)
? t
?(3)
? t
?(4)
? t
?(1)
? t
?(4)
then
T
R
? T
?
{Validity Check}
end if
end while
return T
R
5 Challenges and Solutions
This section outlines our modifications to CUNYTSF, inspired by a preliminary error analysis. We
implement three methods geared toward better preparing I(R) for aggregation into a final four-tuple..
5.1 Preliminary Error Analysis
We ran the publicly available system CUNYTSF described in (Artiles et al., 2011) on the queries used
in TSF2013, using the KBP2013 source collection, and evaluated against the corrected gold standard
described in section 4.1.
5
Error analysis revealed the main source of errors to be WITHIN labels with high confidence. To be
exact, the final four-tuple for 116 queries (of 271) was influenced by a WITHIN label that yielded a t
(3)
later than the g
(4)
date, while 20 were influenced by WITHIN dates that were too early. Under VEI,
once a labeled instance
?
r, ?,WITHIN
?
is aggregated into T
R
, if ? > R
e
then any correctly labeled
instance
?
r, ?, ENDING
?
will yield an invalid four-tuple and thus be rejected. (Similarly, correct BEGIN-
NING labels will be blocked by incorrect WITHIN labels that are too early). Even correct WITHIN labels
cannot set the corrupted aggregation back on track, since pairwise MC will always take the later t
(3)
5
System downloaded from http://nlp.cs.rpi.edu/software.html
1559
(algorithm 2). That said, WITHIN labels are often required to retrieve a complete temporal picture of
a relation conveyed in a corpus. WITHIN is the most common intermediate label in the source collec-
tion, constituting 44% of correct labels, and furthermore, over half of the query relations require at least
one WITHIN label to achieve the gold standard final four-tuple, with 10% relying solely on instances
labeled WITHIN. To make matters worse, almost all TSF systems to date (except Garrido et al. (2013))
use neither the BEFORE START* nor AFTER END* labels in their intermediate temporal relationships
classification models, even though high-confidence instances with those labels could prevent the sort of
erroneous WITHIN labels alluded to above.
This analysis motivated three methods to curtail the extent to which aggregation-derailing four-tuples
were included in I(R) described in sections 5.2, 5.3, and 5.4. We favor VEI over IMC for system-derived
I(R) because IMC strongly relies on the assumption that there is a high probability of correctness for
each intermediate relationship annotation.
5.2 Title Time of Predication
Nominal predicates are commonly used in English to refer to fluents. For example, attribution of a title
to a person can be performed using a transitive verb or copula as in ?Serra was elected Governor?,
or ?Serra is the Governor?, or as a Noun Phrase (NP) within a clause, as in ?Governor Jose Serra?
or ?Jose Serra, Governor, ...? (among other ways). We refer to cases in which the subject and object
of the relation are contained within a phrase headed by a Noun as Relational NP?s (RNP).
6
For RNP
that are mentions of fluent relations, there is a time of predication (TOP), i.e. a time at which the
relation conveyed is asserted to hold, though this time is not overtly marked by tense or aspect (in
English) as in the case of VP?s. Tonhauser (2002)?s analysis assumes that the verbal time of predication
(VTOP) is the ?most salient? time in an utterance, thus relational NP?s take their containing clause?s
verbal time of predication by default though contextual justification may override this tendency. We
propose that in news the DCT is just as salient a time since the focus is centered on current affairs, an
important entities are often ?already introduced? into the discourse by virtue of being public figures. Ad-
hoc analysis of the instances considered by CUNYTSF indicate that a compelling reason is required to
override RNP?s from taking both DCT and VTOP. For instance, in, ?O?Donnell ... suggested Wednesday
that the Obama administration - particularly Vice President Joe Biden, who represented Delaware in
the Senate for decades - was behind them?, ?Vice President? holds true at DCT, and rejects the VTOP
of ?represented?, presumably only based on logical inference: no person is both Vice President and
represents (a state) in the Senate at the same time. Similarly, we know that the DCT (2010-08-04) is
an invalid TOP in ?In November 2000, Chinese President Jiang Zemin paid a state visit to Laos, the
first visit to Laos by a Chinese president?, only because of world knowledge, or, ?The following is a
chronology of major events in China- Laotian relations since 1990:?, earlier in the document.
Though NP?s lack tense and aspect, overt temporal modifiers such as former, then-, and ex- make
explicit a post-relational state directly following an RNP?s relation (Tonhauser, 2002).
7
The tendency
for RNP?s to take both the verbal predication time as well as the DCT extends to post-relational states.
There are many examples in the corpus similar to the following: ?Former US President Bill Clinton and
US journalists Euna Lee and Laura Ling returned Wednesday from North Korea, one day after North
Korea?s leader Kim Jong-Il pardoned the two women?. Each RNP holds at the DCT, and ?Wednesday?,
as well as the day before that (the VTOP of ?pardoned?). However, as for VTOP?s further into the
past, whether the post-relational state holds is less clear. For example, in, ?Secretary of State Hillary
Rodham Clinton says former Philippines President Corazon Aquino ?helped bring democracy back? to
her country after years of authoritarian rule?, we cannot rule out the possibility that Aquino helped
bring democracy back as President; whether she did so as former President is left open, to be resolved
by historical knowledge. This is likely because, unless the relation is of the ?Grover Cleveland? type,
once the relation becomes a ?former? relation it will remain so thereafter.
6
We adopt a Noun Phrase rather than a Determiner Phrase framework for simplicity.
7
In this work we omit similar constructions that indicate a pre-relational state at the time of verbal predication, such as
?future-?, ?soon-to-be?, and ?-elect?. These words to not occur often in our data. That said, the extent to which their meanings
are analogous to the overt temporal modifiers that introduce post-relational states is not clear, and requires further investigation.
1560
The nature of the contexts that override default TOP for RNP?s is complicated, and not well under-
stood. In addition, determining VTOP automatically remains a difficult problem in and of itself (Uz-
Zaman et al., 2012). We have shown that newswire data contains relational NP?s whose default times
of predication - both DCT and verbal - are overridden by context. In addition, even post-relation states
of modified RNP?s may reject VTOP?s. Post-relational states introduced by RNP?s modified with ?for-
mer?, ?then?, and ?ex-?, however, do appear to unambiguously take the DCT as a time of predication.
Furthermore, we observe that CUNYTSF often incorrectly classifies modified RNP?s introducing a post-
relational state as expressing
?
r,DCT,WITHIN
?
. To correct these errors we apply hand-written Title
Time of Predication Fix rules to change the label for all such classification instances to AFTER END*
when the associated time expression is (or is closely related to) the DCT, and attribute 100% confidence
to this new label. This correction both removes erroneous WITHIN labels and introduces labeled instances
that bound query relations.
5.3 Entity Existence
VEI suffers when confidence values are inaccurate. For the relation spouse(Marylin Monroe, Arthur
Miller), given the sentence, ?Editor Courtney Hodell said the book would include poems , photographs ,
reflections on third husband Arthur Miller and other men in Monroe ?s life ?, a system is likely to mislabel
?
r, ?
?
as WITHIN, where ? is the document creation time 2010-04-27. The pattern ?husband s? is a
strong indicator of the WITHIN relationship for the spouse relation, so confidence for the resulting four-
tuple
?
??, 2010-04-27, 2010-04-27,?
?
is likely to be high. Once aggregated, it would be impossible
to later aggregate
?
??, 1961-12-31, 1961-01-01, 1961-12-31
?
upon learning of the couple?s divorce in
1961, since the proposed T
?
=
?
??, 1961-12-31, 2010-04-27, 1961-12-31
?
is invalid. A basic clue that
a WITHIN label should be changed to AFTER END* is that q or s no longer exists (either the person has
died or the business has dissolved).
To address this challenge we propose Existence-based Correction and Filtering. For each relation
R we obtain the existence four-tuple E
R
, by applying MC aggregation to the set of birth and death
times in a knowledge base (KB) for the query-entity and slot-filler.
8
The KB is obtained via the Free-
base API and scraping Wikipedia Infoboxes. We use a four-tuple instead of an interval of dates be-
cause birth and/or death information may not be available at the date granularity. Given the relation
spouse(Jennifer Jones, Norton Simon) and the KB excerpt in Table 3, we obtain an existence con-
straint four-tuple
?
1919-03-02, 1919-03-02, 1993-06-02, 1993-06-02
?
.
Entity Birth Death
Jennifer Jones 1919-03-02 2009-12-17
Norton Simon 1907-02-05 1993-06-02
Table 3: Existence Information
We apply algorithm 3, where C contains classifier confidence for each labeled instance in I(R).
Above, I(R) was introduced as a list of intermediate four-tuples for a relation R. In our approach,
each of these four-tuples is derived deterministically (see Table 1). From here on (as in Algorithm 3) we
allow a slightly abuse of notation in which I(R) is viewed as a set of labeled classification instances,
each of which yields a four-tuple for R. We omit pseudo-code to handle the analogous cases where
instances are re-labeled BEFORE START* based on the relative position of ? and 
1
.
5.4 Relation Precedence
The context of a relation mention often contains temporal information not explicitly tied to a time ex-
pression. For example, in, ?Myasnikovich will replace Sergei Sidorsky, who was prime minister since
2003?, there is no date explicitly tied to the transition of power. Many titles are held by one person after
another, in succession, without overlap. Intuitively, if we know the order in which several individuals
held the same title then temporal information about one such relation can be used to constrain the other.
8
For organization query-entities their foundation and defunct dates are considered their ?birth? and ?death? dates.
1561
Algorithm 3 Existence Based Correction & Filtering Algorithm
Require: I(R) =
{?
?
0
, l
0
?
, . . . ,
?
?
k
, l
k
?}
; C = {c
0
, . . . , c
k
}; E
R
=
?

(1)
, 
(2)
, 
(3)
, 
(4)
?
while i < N do
if ?
i
.s ? 
4
? ?(l
i
= NONE) then
if l
i
= ENDING ? ?.s? 
4
? 31 then
c
i
? 1.0 {Most likely R holds at the time of death}
else
l
i
? AFTER END*; c
i
? 1.0
end if
else if ?
i
.s ? 
4
? ?
i
.e ? ?(l
i
= NONE) then
if l
i
= ENDING then
c
i
? 1.0 {Most likely R holds at the time of death}
else
l
i
? AFTER END*; c
i
? 1.0
end if
end if
end while
return I(R)
To address this challenge we propose Precedence-based Query Expansion and Re-labeling. The
title relation is well-represented in Wikipedia, and the infobox for many political title holders contains
fields for preceded by and succeeded by, which specify the person that held the same title before and after
the title holder in question. Given a title query R, we extracted the person who preceded and succeeded
the query entity from the query entity?s infobox (when available). Additional title relation supporter
queries ? R
pre
and R
suc
, respectively ? were generated using these names, and the same title name as in
the official query.
9
After all classification instances are labeled and existence based correction is applied, we transform
all labeled instances for supporter queries into labeled instances for official queries. Given a labeled
instance
?
r
x
, ?, l
?
, where x = pre or suc, we apply the mapping in Table 4 to yield the transformed
labeled classification instance
?
r, ?, l
?
?
. Labeled supporter instances transformed into labeled official
query instances are added to I(R), the set of labeled instances for R. The set I(R) is then passed to
Aggregation (see Algorithm 2).
Supporter Label l Official label l
?
(x = pre) Official label l
?
(when x = suc)
NONE NONE NONE
BEFORE START* BEFORE START* NONE
AFTER END* NONE AFTER END*
All Others BEFORE START* AFTER END*
Table 4: Mapping to convert
?
r
x
, ?, l
?
to
?
r, ?, l
?
?
, where x indicates whether the supporter query pre-
cedes or succeeds the official query
Just about any instance
?
r
pre
, ?, l
?
yields
?
r, ?, BEFORE START*
?
because R
pre
is known to both start
and end before R starts. (And conversely
?
r
suc
, ?
?
tends to yield AFTER END* for
?
r, ?
?
.) This is
because the last (first) day of R
pre
and all days before (after) it are guaranteed to be before (after) the
start (end) of R. However, note that a AFTER END* label for
?
r
pre
, ?
?
yields NONE for R since dates
after the end of R
pre
may be before, during, or after R. For example, the headline, ?Former President
9
In general, knowing that two relations stand in a particular interval relation to one another allows us to posit constraints
on one relation upon discovering temporal information about the other. We apply this intuition to the title relation in this work
since the information is readily available in a structured form (i.e., the preceded by and succeeded by fields in Wikipedia info
boxes).
1562
Lee Teng-hui on visit in Japan Tokyo?, while clearly indicating AFTER END* for R
pre
tells us very little
about the relationship between the document creation time and R.
6 Results and Analysis
We scored the output for five conditions using the modified gold standard (section 4.1). TF means that
title time of predication fix was applied (section 5.2), EC means existence corrections were applied, and
Pr means that precedence-based query expansion was applied (section 5.4).
System P R F
CUNYTSF .337 .294 .314
CUNYTSF + TF .341 .298 .318
CUNYTSF + EC .349 .305 .326
CUNYTSF + TF + EC .353 .309 .329
CUNYTSF + TF + EC + Pr .360 .315 .336
Table 5: Results calculated using official TSF2013 scorer against corrected gold standard (sec. 4.1), with
anydoc and ignore-offsets parameters set to true, augmented to calculate recall and precision
6.1 Title Time of Predication Fix
The gold standard for title had 142 non-infinity tuple element outputs of the form
?
R, i, t
(i)
?
. The
baseline output had 80 values while baseline + TF had 91. Applying TF, 10 baseline outputs were
replaced while 11 were added. In most cases erroneous WITHIN labels are corrected by inserting high-
confidence AFTER END* into I(R). In some cases this allows a correct t
(3)
to replace a later, incorrect
t
(3)
that came from an erroneous WITHIN label. It is important to note that while some changes barely af-
fect F-measure, they are important because they allow for correct information that would have otherwise
been blocked to be aggregated. For example, a bad baseline WITHIN for ?General Prosecutor ?s Office
of Kyrgyzstan on Tuesday charged the country?s former Prime Minister Igor Chudinov with abuse of
power? had blocked a correct WITHIN for ?Kyrgyz Prime Minister Igor Chudinov left Beijing Thursday
evening? - removing this block allowed t
(3)
to change from 2010-05-04 to 2009-10-14, which is the gold
standard value.
6.2 Existence-based Correction and Filtering
Most changes made from existence constraints are beneficial both in terms of an increase in F-measure
and in blocking the aggregation of incorrect information. For instance, it is difficult to prevent labeling
the following sentence with WITHIN for DCT: ?The London home of composer George Frideric Handel
is holding an exhibition about its other famous resident ? Jimi Hendrix?, but the document context per-
mits AFTER END*, given ?Hendrix died in London on Sept. 18 , 1970?. Given the existence constraint
we label the instance AFTER END*.
On the other hand, in some cases we erroneously change WITHIN to BEFORE START* using existence
constraints, but this type of change does little damage. For example, the fact that CNN was founded on
1980-06-01 changes the label on 1980 from WITHIN to BEFORE START* for EMPLOYEE(Novak, CNN),
given ?Novak , editor of the Evans-Novak Political Report , is perhaps best known as a co-host of several
of CNN ?s political talk shows , where he often jousted with liberal guests from 1980 to 2005?. We set
t
(1)
= 1980-01-01 which does not block later inclusion of a correct
?
R, 1980, BEGINNING
?
, which
would set t
(1)
= 1980-01-01 if it were not already set, and does set t
(2)
= 1980-12-31. Changing this
relation?s label from WITHIN to START is not a catastrophic error because it allows for a finer grained,
correct start date to be aggregated using VEI (see Algorithm 2) to yield a superior final four-tuple (though
CUNYTSF finds no suitable candidates to facilitate this).
1563
6.3 Precedence-based Query Expansion & Re-labeling
Output for affected official queries were improved simply because supporter queries were accurately
labeled. For example, ?Kim Choongsoo, Korea?s Central Bank Governor, said here on Thursday his na-
tion?s economic situation was getting better? provides a t
(4)
value for title(Lee Seong-tae, Governor)
due given the successor relation.
Some gains from label transformation are only possible given the title time of predication fix. For ex-
ample, multiple instances of ?former president Chen Shui-bian? and ?Former President Lee Teng-hui?
were converted from WITHIN to AFTER END* for their respective relations. Because Chen succeeded
Lee, the latter instances were transformed to NONE instances for title(Chen, President) using Ta-
ble 4.
10
Changing these labels to NONE made room for a valid t
(3)
= 2000-01-01 based converting the
WITHIN for title(Lee, President) to BEFORE START* for title(Chen, President) given, ?... since
former President Lee Teng-hui promulgated it 19 years ago, Wang said, and the [DPP] did not try to
make any changes to the framework during its eight-year rule between 2000 and 2008 either?.
Label transformation is robust to misclassification. For example, any of BEFORE START*, BEGIN-
NING, WITHIN, or ENDING for a predecessor relation R
pre
will map to before start* for R. But other
types of errors propagate and can lead to disastrous results. For example, due to a normalization quirk
?Utatu President George Strauss? is recognized as ?Johannes Rau?, thus the relation title(Rau, Pres-
ident) was assigned WITHIN at DCT, which is converted to a BEFORE START* for Horst Kohler, Rau?s
successor.
A deeper problem that can lead to error propagation is that fact one person can have the same title in
different contexts. When a title is attributed to a person there is often a geo-political or organization en-
tity involved. Mentions that fail to include this third entity are ambiguous; often, this information needs
to be inferred from other context sentences. Such errors may be propagated from supporter to official
queries. For example, ?Francophonie president Abdou Diouf of Senegal ... ? appears to support the
title(Abdou Diouf, President). Diouf preceded Abdoulaye Wade as President of Senegal, but the con-
text in question (inaccurately) refers to Diouf?s leadership position of Secretary-General (not President)
of Organisation internationale de la Francophonie, thus an erroneous BEFORE START* is aggregated,
blocking a correctly labeled (less confident)
?
r, 2000, START
?
.
7 Conclusion
We have analyzed within the particular context of TSF the process of aggregating partially-specified
temporal information about relations across documents. Our analysis and and results indicate that text
mentions of relations often ground only a portion of the referent relation in time and that correct in-
terpretation relies on background knowledge about relation participants. In future work we plan a more
rigorous data-driven study of nominal time of predication and to attack more ambiguous context-sensitive
cases. In addition we aim to induce relation order from text automatically to multiple relation types as
well as events.
Acknowledgments
This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement No.
W911NF-09-2-0053 (NS-CTA), and in addition the U.K. Ministry of Defense under Agreement No.
W911NF-06-3-0001 (ITA), U.S. NSF CAREER Award under Grant IIS-0953149, U.S. DARPA Award
No. FA8750-13-2-0041 in the Deep Exploration and Filtering of Text (DEFT) Program, IBM Faculty
Award, Google Research Award and RPI faculty start-up grant. The views and conclusions contained in
this document are those of the authors and should not be interpreted as representing the official policies,
either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes notwithstanding any copyright notation here on.
10
Had the title fix not been applied these WITHIN labels would have been converted to BEFORE START*.
1564
References
Enrique Amigo, Artiles Javier, Qi Li, and Heng Ji. 2011. An evaluation framework for aggregated temporal
information extraction. In Pric SIGIR2011 Workshop on Entity-Oriented Search.
Javier Artiles, Qi Li, Taylor Cassidy, and Heng Ji. 2011. Temporal slot filling system description. In Proc. Text
Analytics Conference (TAC2011).
Maximilian Dylla, Iris Miliaraki, and Martin Theobald. 2013. A temporal-probabilistic database model for infor-
mation extraction. Proceedings of the VLDB Endowment, 6(14):1810?1821.
Guillermo Garrido, Anselmo Penas, and Bernardo Cabaleiro. 2013. Uned slot filling and temporal slot filling
systems at tac kbp 2013. system description. In Proc. Text Analytics Conference (TAC2013).
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011. An overview of the tac2011 knowledge base population
track. In Proc. Text Analytics Conference (TAC2011).
Stuart J. Russell and Peter Norvig. 2010. Artificial Intelligence - A Modern Approach (3. internat. ed.). Pearson
Education.
Mihai Surdeanu. 2013. An overview of the tac2013 knowledge base population track. In Proc. Text Analytics
Conference (TAC2013).
Judith Tonhauser. 2002. A dynamic semantic account of the temporal interpretation of noun phrases. In Proceed-
ings of SALT, volume 12, pages 286?305.
Naushad UzZaman, Hector Llorens, James F. Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky.
2012. Tempeval-3: Evaluating events, time expressions, and temporal relations. CoRR, abs/1206.5333.
Yafang Wang, Maximilian Dylla, Marc Spaniol, and Gerhard Weikum. 2012. Coupling label propagation and
constraints for temporal fact extraction. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers-Volume 2, pages 233?237. Association for Computational Linguistics.
Appendix A. Glossary of Selected Terms
Fluent Relation: A property of a person or organization whose value may change over time. For
example, a person?s employer.
Temporal Extension: For a relation R, the temporal extension is the interval [R
s
, R
e
], which represents
the period of time between and including the start date R
s
and end date R
e
of the relation.
Relation Mention: An excerpt of text that expresses a relation.
Time Expression: An excerpt of text that refers to a portion of time, such as ?Tuesday? or ?next year?.
Normalized Time Expression: The portion of time indicated by a time expression expressed in a
standard form.
Granularity: The level at which a portion of time is expressed, in terms of calendar and clock units.
For example, years are of a coarser granularity than days.
Temporal Four-tuple: For a relation R, a temporal four-tuple T
R
=
?
t
(1)
, t
(2)
, t
(3)
, t
(4)
?
represents an
assertion that, based on some evidence, the start date for R is between t
(1)
and t
(2)
, and its end date is
between t
(3)
and t
(4)
.
Final Temporal Four-tuple: The four-tuple assigned to R (by an annotator or system) after aggregating
all temporal information about R.
Valid Temporal Four-tuple: A four-tuple T =
?
t
(1)
, t
(2)
, t
(3)
, t
(4)
?
is valid if and only if iff.
t
(1)
? t
(2)
? t
(3)
? t
(4)
? t
(1)
? t
(4)
.
Correct Temporal Four-tuple: A temporal four-tuple T
R
=
?
t
(1)
, t
(2)
, t
(3)
, t
(4)
?
if and only if
t
(1)
? R
s
? t
(2)
? t
(3)
? R
e
? t
(4)
Intermediate Temporal Relationship: Given a relation mention r of relation R and a normalized time
expression ? (viewed as a temporal interval), the intermediate temporal relationship between the two
characterizes the relationships between the end points of ? and the endpoints of the temporal extension
of R, namely ?
s
, ?
e
, R
s
, and R
e
. In this work, each intermediate temporal relationship used serves as
a mapping from temporal interval to four-tuple (see Table 1 for the relationships used in this work and
their mappings).
1565
Intermediate Temporal Four-tuple Set: For a relation R, a system or annotator may derive an
intermediate temporal four-tuple for each relation mention r and a corresponding time expression ?
by based on an intermediate temporal relationship expressed between the two. The elements of each
intermediate four-tuple are derived using the mapping in Table 1. We denote the set of intermediate
temporal four-tuples for R as I(R).
Query Relation: A relation that serves as input to a TSF system tasked with returning a final temporal
four-tuple for that relation.
Relational Noun Phrase: A noun phrase that expresses a relation. For example, ?President Obama?
expresses a relation that ?Obama??s title is ?President?.
Time of Predication: For a given predicate, the time of predication is a time interval for which the
predicate is asserted to apply to a specified set of arguments.
Post-relational State: A state immediately following the end of a relation characterized by the relation
now longer holding. For example, prepending a title with ?former?, as in ?former President X?,
introduces a state characterized by X no longer holding the title President.
Temporally Linked Relations: Two relations are temporally linked if their temporal extensions are not
independent. For example, if it is known that one?s end precedes the other?s start.
Provenance: The relevant text that supports the output.
1566
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1567?1578, Dublin, Ireland, August 23-29 2014.
The Wisdom of Minority: Unsupervised Slot Filling Validation based on
Multi-dimensional Truth-Finding
Dian Yu
1
, Hongzhao Huang
1
, Taylor Cassidy
2,3
, Heng Ji
1
Chi Wang
4
, Shi Zhi
4
, Jiawei Han
4
, Clare Voss
2
, Malik Magdon-Ismail
1
1
Computer Science Department, Rensselaer Polytechnic Institute
2
U.S. Army Research Lab
3
IBM T. J. Watson Research Center
4
Computer Science Department, Univerisity of Illinois at Urbana-Champaign
1
{yud2,huangh9,jih,magdon}@rpi.edu,
2,3
{taylor.cassidy.ctr,clare.r.voss.civ}@mail.mil
4
{chiwang1,shizhi2,hanj}@illinois.edu
Abstract
Information Extraction using multiple information sources and systems is beneficial due to multi-
source/system consolidation and challenging due to the resulting inconsistency and redundancy.
We integrate IE and truth-finding research and present a novel unsupervised multi-dimensional
truth finding framework which incorporates signals from multiple sources, multiple systems and
multiple pieces of evidence by knowledge graph construction through multi-layer deep linguistic
analysis. Experiments on the case study of Slot Filling Validation demonstrate that our approach
can find truths accurately (9.4% higher F-score than supervised methods) and efficiently (finding
90% truths with only one half the cost of a baseline without credibility estimation).
1 Introduction
Traditional Information Extraction (IE) techniques assess the ability to extract information from
individual documents in isolation. However, similar, complementary or conflicting information may
exist in multiple heterogeneous sources. We take the Slot Filling Validation (SFV) task of the NIST Text
Analysis Conference Knowledge Base Population (TAC-KBP) track (Ji et al., 2011) as a case study. The
Slot Filling (SF) task aims at collecting from a large-scale multi-source corpus the values (?slot fillers?)
for certain attributes (?slot types?) of a query entity, which is a person or some type of organization. KBP
2013 has defined 25 slot types for persons (per) (e.g., age, spouse, employing organization) and 16 slot
types for organizations (org) (e.g., founder, headquarters-location, and subsidiaries). Some slot types
take only a single slot filler (e.g., per:birthplace), whereas others take multiple slot fillers (e.g., org:top
employees).
We call a combination of query entity, slot type, and slot filler a claim. Along with each claim, each
system must provide the ID of a source document and one or more detailed context sentences as evidence
which supports the claim. A response (i.e., a claim, evidence pair) is correct if and only if the claim is
true and the evidence supports it.
Given the responses produced by multiple systems from multiple sources in the SF task, the goal of
the SFV task is to determine whether each response is true or false. Though it?s a promising line of
research, it raises two complications: (1) different information sources may generate claims that vary
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1567
in trustability; and (2) a large-scale number of SF systems using different resources and algorithms may
generate erroneous, conflicting, redundant, complementary, ambiguously worded, or inter-dependent
claims from the same set of documents. Table 1 presents responses from four SF systems for the query
entity Ronnie James Dio and the slot type per:city of death. Systems A, B and D return Los Angeles
with different pieces of evidence
1
extracted from different information sources, though the evidence of
System D does not decisively support the claim. System C returns Atlantic City, which is neither true
nor supported by the corresponding evidence.
Such complications call for ?truth finding?: determining the veracity of multiple conflicting claims
from various sources and systems. We propose a novel unsupervised multi-dimensional truth finding
framework to study credibility perceptions in rich and wide contexts. It incorporates signals from
multiple sources and systems, using linguistic indicators derived from knowledge graphs constructed
from multiple evidences using multi-layer deep linguistic analysis. Experiments demonstrate that our
approach can find truths accurately (9.4% higher F-score than supervised methods) and efficiently (find
90% truths with only one half cost of a baseline without credibility estimation).
System Source Slot Filler Evidence
A Agence France-
Presse, News
Los Angeles The statement was confirmed by publicist Maureen O?Connor, who said Dio
died in Los Angeles.
B New York
Times, News
Los Angeles Ronnie James Dio, a singer with the heavy-metal bands Rainbow, Black
Sabbath and Dio, whose semioperatic vocal style and attachment to demonic
imagery made him a mainstay of the genre, died on Sunday in Los Angeles.
C Discussion Fo-
rum
Atlantic City Dio revealed last summer that he was suffering from stomach cancer shortly
after wrapping up a tour in Atlantic City.
D Associated
Press
Worldstream,
News
Los Angeles LOS ANGELES 2010-05-16 20:31:18 UTC Ronnie James Dio, the metal god
who replaced Ozzy Osbourne in Black Sabbath and later piloted the bands
Heaven, Hell and Dio, has died, according to his wife and manager.
Table 1: Conflicting responses across different SF systems and different sources (query entity = Ronnie
James Dio, slot type = per:city of death).
2 Related Work & Our Novel Contributions
Most previous SFV work (e.g., (Tamang and Ji, 2011; Li and Grishman, 2013)) focused on filtering
incorrect claims from multiple systems by simple heuristic rules, weighted voting, or costly supervised
learning to rank algorithms. We are the first to introduce the truth finding concept to this task.
The ?truth finding? problem has been studied in the data mining and database communities (e.g., (Yin
et al., 2008; Dong et al., 2009a; Dong et al., 2009b; Galland et al., 2010; Blanco et al., 2010; Pasternack
and Roth, 2010; Yin and Tan, 2011; Pasternack and Roth, 2011; Vydiswaran et al., 2011; Ge et al.,
2012; Zhao et al., 2012; Wang et al., 2012; Pasternack and Roth, 2013)). Compared with the previous
work, our truth finding problem is defined under a unique setting: each response consists of a claim and
supporting evidence, automatically generated from unstructured natural language texts by a SF system.
The judgement of a response concerns both the truth of the claim and whether the evidence supports
the claim. This has never been modeled before. We mine and exploit rich linguistic knowledge from
multiple lexical, syntactic and semantic levels from evidence sentences for truth finding. In addition,
previous truth finding work assumed most claims are likely to be true. However, most SF systems have
hit a performance ceiling of 35% F-measure, and false responses constitute the majority class (72.02%)
due to the imperfect algorithms as well as the inconsistencies of information sources. Furthermore,
certain truths might only be discovered by a minority of good systems or from a few good sources. For
example, 62% of the true responses are produced only by 1 or 2 of the 18 SF systems.
1
Hereafter, we refer to ?pieces of evidence? with the shorthand ?evidences?. Note that SF systems may include multiple
sentences as ?evidence? within their responses.
1568
r1 
      Response 
<Claim, Evidence> 
t1 
t2 
System 
s1 
r2 
r3 
s2 
Source 
t3 
r4 t4 
s3 
r5 
Figure 1: Heterogeneous networks for MTM.
3 MTM: A Multi-dimensional Truth-Finding Model
MTM Construction
A response is trustworthy if its claim is true and its evidence supports the claim. A trusted
source always supports true claims by giving convincing evidence, and a good system tends to extract
trustworthy responses from trusted sources. We propose a multi-dimensional truth-finding model (MTM)
to incorporate and compute multi-dimensional credibility scores.
Consider a set of responses R = {r
1
, . . . , r
m
} extracted from a set of sources S = {s
1
, . . . , s
n
} and
provided by a set of systems T = {t
1
, . . . , t
l
}. A heterogeneous network is constructed as shown in
Fig. 1. Let weight matrices be W
rs
m?n
= {w
rs
ij
} and W
rt
m?l
= {w
rt
ik
}. A link w
rs
ij
= 1 is generated
between r
i
and s
j
when response r
i
is extracted from source s
j
, and a link w
rt
ik
= 1 is generated between
r
i
and t
k
when response r
i
is provided by system t
k
.
Credibility Initialization
Each source is represented as a combination of publication venue and genre. The credibility scores
of sources S are initialized uniformly as
1
n
, where n is the number of sources. Given the set of systems
T = {t
1
, . . . , t
l
}, we initialize their credibility scores c
0
(t) based on their interactions on the predicted
responses. Suppose each system t
i
generates a set of responses R
t
i
. The similarity between two systems
t
i
and t
j
is defined as similarity(t
i
, t
j
) =
|R
t
i
?R
t
j
|
log (|R
t
i
|)+log (|R
t
j
|)
(Mihalcea, 2004). Then we construct a
weighted undirected graph G = ?T,E?, where T (G) = {t
1
, . . . , t
l
} and E(G) = {?t
i
, t
j
?}, ?t
i
, t
j
? =
similarity(t
i
, t
j
), and apply the TextRank algorithm (Mihalcea, 2004) on G to obtain c
0
(t).
We got negative results by initializing system credibility scores uniformly. We also got negative results
by initializing system credibility scores using system metadata, such as the algorithms and resources the
system used at each step, its previous performance in benchmark tests, and the confidence values it
produced for its responses. We found the quality of an SF system depends on many different resources
instead of any dominant one. For example, an SF system using a better dependency parser does not
necessarily produce more truths. In addition, many systems are actively being improved, rendering
previous benchmark results unreliable. Furthermore, most SF systems still lack reliable confidence
estimation.
The initialization of the credibility scores for responses relies on deep linguistic analysis of the
evidence sentences and the exploitation of semantic clues, which will be described in Section 4.
Credibility Propagation
1569
We explore the following heuristics in MTM.
HEURISTIC 1: A response is more likely to be true if derived from many trustworthy sources. A source
is more likely to be trustworthy if many responses derived from it are true.
HEURISTIC 2: A response is more likely to be true if it is extracted by many trustworthy systems. A
system is more likely to be trustworthy if many responses generated by it are true.
Based on these two heuristics we design the following credibility propagation approach to mutually
reinforce the trustworthiness of linked objects in MTM.
By extension of Co-HITS (Deng et al., 2009), designed for bipartite graphs, we develop a propagation
method to handle heterogeneous networks with three types of objects: source, response and system. Let
the weight matrices beW
rs
(between responses and sources) andW
rt
(between responses and systems),
and their transposes beW
sr
andW
tr
. We can obtain the transition probability that vertex s
i
in S reaches
vertex r
j
in R at the next iteration, which can be formally defined as a normalized weight p
sr
ij
=
w
sr
ij?
k
w
sr
ik
such that
?
r
j
?R
p
sr
ij
= 1. We compute the transition probabilities p
rs
ji
, p
rt
jk
and p
tr
kj
in an analogous
fashion.
Given the initial credibility scores c
0
(r), c
0
(s) and c
0
(t), we aim to obtain the refined credibility scores
c(r), c(s) and c(t) for responses, sources, and systems, respectively. Starting with sources, the update
process considers both the initial score c
0
(s) and the propagation from connected responses, which we
formulated as:
c(s
i
) = (1? ?
rs
)c
0
(s
i
) + ?
rs
?
r
j
?R
p
rs
ji
c(r
j
) (1)
Similarly, the propagation from responses to systems is formulated as:
c(t
k
) = (1? ?
rt
)c
0
(t
k
) + ?
rt
?
r
j
?R
p
rt
jk
c(r
j
) (2)
Each response?s score c(r
j
) is influenced by both linked sources and systems:
c(r
j
) = (1? ?
sr
? ?
tr
)c
0
(r
j
) + ?
sr
?
s
i
?S
p
sr
ij
c(s
i
) + ?
tr
?
t
k
?T
p
tr
kj
c(t
k
) (3)
where ?
rs
, ?
rt
, ?
sr
and ?
tr
? [0, 1]. These parameters control the preference for the propagated over
initial score for every type of random walk link. The larger they are, the more we rely on link structure
2
.
The propagation algorithm converges (10 iterations in our experimental settings) and a similar theoretical
proof to HITS (Peserico and Pretto, 2009) can be constructed. Algorithm 1 summarizes MTM.
4 Response Credibility Initialization
Each evidence along with a claim is expressed as a few natural language sentences that include the query
entity and the slot filler, along with semantic content to support the claim. We analyze the evidence of
each response in order to initialize that response?s credibility score. This is done using heuristic rules
defined in terms of the binary outputs of various linguistic indicators (Section 4.1).
4.1 Linguistic Indicators
We encode linguistic indicators based on deep linguistic knowledge acquisition and use them to
determine whether responses provide supporting clues or carry negative indications (Section 4.3). These
indicators make use of linguistic features on varying levels - surface form, sentential syntax, semantics,
and pragmatics - and are defined in terms of knowledge graphs (Section 4.2). We define a heuristic rule
for each slot type in terms of the binary-valued linguistic indicator outputs to yield a single binary value
(1 or 0) for each response. If a response?s linguistic indicator value is 1, the credibility score of a response
is initialized at 1.0, and 0.5 otherwise.
2
We set ?
rs
= 0.9, ?
sr
= 0.1, ?
rt
= 0.3 and ?
tr
= 0.2, optimized from a development set. See Section 5.1.
1570
Input: A set of responses (R), sources (S) and systems (T ).
Output: Credibility scores (c(r)) for R.
1: Initialize the credibility scores c
0
(s) for S as c
0
(s
i
) =
1
|S|
;
2: Use TextRank to compute initial credibility scores c
0
(t) for T ;
3: Initialize the credibility scores c
0
(r) using linguistic indicators (Section 4);
4: Construct heterogeneous networks across R, S and T ;
5: k ? 0, diff? 10e6;
6: while k < MaxIteration and diff > MinThreshold do
7: Use Eq. (1) to compute c
k+1
(s);
8: Use Eq. (2) to compute c
k+1
(t);
9: Use Eq. (3) to compute c
k+1
(r);
10: Normalize c
k+1
(s), c
k+1
(t), and c
k+1
(r);
11: diff?
?
(|c
k+1
(r)? c
k
(r)|);
12: k ? k + 1
13: end while
Algorithm 1:Multi-dimensional Truth-Finding.
4.2 Knowledge Graph Construction
A semantically rich knowledge graph is constructed that links a query entity, all of its relevant slot
filler nodes, and nodes for other intermediate elements excerpted from evidence sentences. There is one
knowledge graph per sentence.
Fig. 2 shows a subregion of the knowledge graph built from the sentence: ?Mays, 50, died in his sleep
at his Tampa home the morning of June 28.?. It supports 3 claims: [Mays, per: city of death, Tampa],
[Mays, per: date of death, 06/28/2009] and [Mays, per: age, 50].
Formally, a knowledge graph is an annotated graph of entity mentions, phrases and their links. It must
contain one query entity node and one or more slot filler nodes. The annotation of a node includes its
entity type, subtype, mention type, referent entities, and semantic category (though not every node has
each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation
between the two linked nodes.
The knowledge graph is constructed using the following procedure. First, we annotate the evidence
text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and
event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the
annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is
labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads.
Thus, we extend the boundaries of entity, time, and value mentions (e.g., people?s titles) to include an
entire phrase where possible. We then enrich each node with annotation for entity type, subtype and
mention type. Entity type and subtype refer to the role played by the entity in the world, the latter being
more fine-grained, whereas mention type is syntactic in nature (it may be pronoun, nominal, or proper
name). For example, ?Tampa? in Fig. 2 is annotated as a Geopolitical (entity type) Population-Center
(subtype) Name (mention type) mention. Every time expression node is annotated with its normalized
reference date (e.g., ?June, 28? in Fig. 2 is normalized as ?06/28/2009?).
Second, we perform co-reference resolution, which introduces implicit links between nodes that refer
to the same entity. Thus, an entity mention that is a nominal or pronoun will often be co-referentially
linked to a mention of a proper name. This is important because many queries and slot fillers are
expressed only as nominal mentions or pronouns in evidence sentences, their canonical form appearing
elsewhere in the document.
Finally, we address the fact that a given relation type may be expressed in a variety of ways. For
example, ?the face of ? indicates the membership relation in the following sentence: ?Jennifer Dunn was
the face of the Washington state Republican Party for more than two decades.? We mined a large
1571
Mays 
had 
died 
sleep 
his 
home 
Tampa 
50 
June,28 
amod 
nsubj 
aux 
prep_in 
poss 
prep_at 
prep_of 
nn 
poss 
  located_in 
{PER.Individual, NAM, Billy Mays} 
?Query? 
{NUM } 
?Per:age? 
{Death-Trigger} 
{PER.Individual.PRO, Mays} 
{GPE.Population-Center.NAM, FL-USA} 
? Per:place_of_death? 
{FAC.Building-Grounds.NOM} 
{06/28/2009, TIME-WITHIN}  
? per:date_of_death? 
Figure 2: Knowledge Graph Example.
number of trigger phrases for each slot type by mapping various knowledge bases, including Wikipedia
Infoboxes, Freebase (Bollacker et al., 2008), DBPedia (Auer et al., 2007) and YAGO (Suchanek et
al., 2007), into the Gigaword corpus
3
and Wikipedia articles via distant supervision (Mintz et al.,
2009)
4
. Each intermediate node in the knowledge graph that matches a trigger phrase is then assigned a
corresponding semantic category. For example, ?died? in Fig. 2 is labeled a Death-Trigger.
4.3 Knowledge Graph-Based Verification
We design linguistic indicators in terms of the properties of nodes and paths that are likely to be bear on
the response?s veracity. Formally, a path consists of the list of nodes and links that must be traversed
along a route from a query node to a slot filler node.
Node indicators contribute information about a query entity or slot filler node in isolation, that
may bear on the trustworthiness of the containing evidence sentence. For instance, a slot filler for the
per:date of birth slot type must be a time expression.
Node Indicators
1. Surface: Whether the slot filler includes stop words; whether it is lower cased but appears in news.
These serve as negative indicators.
2. Entity type, subtype and mention type: For example, the slot fillers for ?org:top employees? must be
person names; and fillers for ?org:website? must match the url format. Besides the entity extraction
system, we also exploited the entity attributes mined by the NELL system (Carlson et al., 2010)
from the KBP source corpus.
Each path contains syntactic and/or semantic relational information that may shed light on the manner
in which the query entity and slot filler are related, based on dependency parser output, IE output,
and trigger phrase labeling. Path indicators are used to define properties of the context in which
which query-entity and slot-filler are related in an evidence sentence. For example, whether the path
3
http://catalog.ldc.upenn.edu/LDC2011T07
4
Under the distant supervision assumption, sentences that appear to mention both entities in a binary relation contained in
the knowledge base were assumed to express that relation.
1572
associated with a claim about an organization?s top employee includes a title commonly associated with
decision-making power can be roughly represented using the trigger phrases indicator.
Path Indicators
1. Trigger phrases: Whether the path includes any trigger phrases as described in Section 4.2.
2. Relations and events: Whether the path includes semantic relations or events indicative of the slot
type. For example, a ?Start-Position? event indicates a person becomes a ?member? or ?employee?
of an organization.
3. Path length: Usually the length of the dependency path connecting a query node and a slot filler
node is within a certain range for a given slot type. For example, the path for ?per:title? is usually
no longer than 1. A long dependency path between the query entity and slot filler indicates a lack
of a relationship. In the following evidence sentence, which does not entail the ?per:religion?
relation between ?His? and the religion ?Muslim?, there is a long path (?his-poss-moment-nsubj-
came-advcl-seized-militant-acmod-Muslim?): ?His most noticeable moment in the public eye came
in 1979, when Muslim militants in Iran seized the U.S. Embassy and took the Americans stationed
there hostage.?.
Detecting and making use of interdependencies among various claims is another unique challenge in
SFV. After initial response credibility scores are calculated by combining linguistic indicator values, we
identify responses that have potentially conflicting or potentially supporting slot-filler candidates. For
such responses, their credibility scores are changed in accordance with the binary values returned by the
following indicators.
Interdependent Claims Indicators
1. Conflicting slot fillers: When fillers for two claims with the same query entity and slot type appear
in the same evidence sentence, we apply an additional heuristic rule designed for the slot type in
question. For example, the following evidence sentence indicates that compared to ?Cathleen P.
Black?, ?Susan K. Reed? is more likely to be in a ?org:top employees/members? relation with ?The
Oprah Magazine? due to the latter pair?s shorter dependency path: ?Hearst Magazine?s President
Cathleen P. Black has appointed Susan K. Reed as editor-in-chief of the U.S. edition of The
Oprah Magazine.?. The credibility scores are accordingly changed (or kept at) 0.5 for responses
associated with the former claim, and 1.0 for those associated with the latter.
2. Inter-dependent slot types: Many slot types are inter-dependent, such as ?per:title? and
?per:employee of ?, and various family slots. After determining initial credibility scores for each
response, we check whether evidence exists for any implied claims. For example, given initial
credibility scores of 1.0 for two responses supporting the claims that (1)?David? is ?per:children?
of ?Carolyn Goodman? and (2)?Andrew? is ?per:sibling? of ?David?, we check for any responses
supporting the claim that (3)?Andrew? is ?per:children? of ?Carolyn Goodman?, and set their
credibility scores to 1.0. For example, a response supporting this claim included the evidence
sentence, ?Dr. Carolyn Goodman, her husband, Robert, and their son, David, said goodbye to
David?s brother, Andrew.?.
5 Experimental Results
This section presents the experiment results and analysis of our approach.
5.1 Data
The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the
merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot
1573
Methods Precision Recall F-measure Accuracy Mean Average Precision
1.Random 28.64% 50.48% 36.54% 50.54% 34%
2.Voting 42.16% 70.18% 52.68% 62.54% 62%
3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60%
4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56%
5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70%
Table 2: Overall Performance Comparison.
Filling (SF) task. The source collection has 1,000,257 newswire documents, 999,999 web documents
and 99,063 discussion forum posts, which results in 10 different sources (combinations of publication
venues and genres) in our experiment. There are 100 queries: 50 person and 50 organization entities.
After removing redundant responses within each single system run, we use 45,950 unique responses as
the input to truth-finding. Linguistic Data Consortium (LDC) human annotators manually assessed all
of these responses and produced 12,844 unique responses as ground truth. In order to compare with
state-of-the-art supervised learning methods for SFV (Tamang and Ji, 2011; Li and Grishman, 2013), we
trained a SVMs classifier
5
as a counterpart, incorporating the same set of linguistic indicators, sources
and systems as features. We picked 10% (every 10th line) to compose the development set for MTM and
the training set for the SVMs. The rest is used for blind test.
5.2 Overall Performance
Table 2 shows the overall performance of various truth finding methods on judging each response as true
or false. MTM achieves promising results and even outperforms supervised learning approach. Table 3
presents some examples ranked at the top and the bottom based on the credibility scores produced by
MTM.
We can see that majority voting across systems performs much better than random assessment, but its
accuracy is still low. For example, the true claim T5 was extracted by only one system because most
systems mistakenly identified ?Briton Stuart Rose? as a person name. In comparison, MTM obtained
much better accuracy by also incorporating multiple dimensions of source and evidence information.
Method 3 using linguistic indicators alone, already achieved promising results. For example, many
claims are judged as truths through trigger phrases (T1 and T5), event extraction (T2), coreference (T4),
and node type indicators (T3). On the other hand, many claims are correctly judged as false because
their evidence sentences did not include the slot filler (F1, F4, F5) or valid knowledge paths to connect
the query entity and the slot filler (F2, F3). The performance gain (2.99% F-score) from Method 3 to
Method 5 shows the need for incorporating system and source dimensions. For example, most truths are
from news while many false claims are from newsgroups and discussion forum posts (F1, F2, F5).
The SVMs model got very low recall because of the following two reasons: (1) It ignored the inter-
dependency between multiple dimensions; (2) the negative instances are dominant in the training data,
so the model is biased towards labeling responses as false.
5.3 Truth Finding Efficiency
Table 3 shows that some truths (T1) are produced from low-ranked systems whereas some false responses
from high-ranked systems (F1, F2). Note that systems are ranked by their performance in KBP SF task.
In order to find all the truths, human assessors need to go through all the responses returned by multiple
systems. This process was proven very tedious and costly (Ji et al., 2010; Tamang and Ji, 2011).
Our MTM approach can expedite this process by ranking responses based on their credibility scores
and asking human to assess the responses with high credibility first. Traditionally, when human assess
responses, they follow an alphabetical order or system IDs in a ?passive learning? style. This is set as
our baseline. For comparison, we also present the results using only linguistic indicators, using voting
in which the responses which get more votes across systems are assessed first, and the oracle method
assessing all correct responses first. Table 2 shows our model can successfully rank trustworthy responses
at high positions compared with other approaches.
5
We used the LIBSVM toolkit (Chang and Lin, 2011) with Gaussian radial basis function kernel.
1574
Response Ranked by MTM
Source
System
Rank
Claim
Evidence
Query Entity Slot Type Slot Filler
Top
Truths
T1 China
Banking
Regulatory
Commission
org:top
member-
s/employees
Liu
Mingkang
Liu Mingkang, the chairman of
the China Banking Regulatory
Commission
Central
News
Agency
of Taiwan
News
News 15
T2 Galleon
Group
org:founded
by
Raj
Rajaratnam
Galleon Group, founded by bil-
lionaire Raj Rajaratnam
New York
Times
News 9
T3 Mike Penner per:age 52 L.A. Times Sportswriter Mike
Penner, 52, Dies
New York
Times
News 1
T4 China
Banking
Regulatory
Commission
org:alternate
names
CBRC ...China Banking Regulatory Com-
mission said in the notice. The five
banks ... according to CBRC.
Xinhua,
News
News 5
T5 Stuart Rose per:origin Briton Bolland, 50, will replace Briton
Stuart Rose at the start of 2010.
Agence
France-
Presse
News 3
Bottom
False
Claims
F1 American
Association
for the Ad-
vancement of
Science
org:top
members
employees
Freedman erica.html &gt; American Library
Association, President: Maurice
Freedman &lt; http://www.aft.org
&gt; American Federation of
Teachers ...
Google Newsgroup4
F2 Jade Goody per:origin Britain because Jade Goody?s the only
person to ever I love Britain
Discussion Forum 3
F3 Don Hewitt per:spouse Swap ...whether ?Wife Swap? on ABC
or ?Jon &amp; Kate? on TLC
New York
Times
News 7
F4 Council of
Mortgage
Lenders
org:website www.cml.org.ukme purchases in the U.K. jumped
by 16 percent in April, suggesting
the property market slump may
have bottomed out
Associated
Press
World-
stream
News 18
F5 Don Hewitt per:alternate
names
Hewitt M-
chen
US DoMIna THOMPson LACtaTe
haVeD [3866 words]
Google Newsgroup13
Table 3: Top and Bottom Response Examples Ranked by MTM.
Fig. 3 summarizes the results from the above 6 approaches. The common end point of all curves
represents the cost and benefit of assessing all system responses. We can see that the baseline is very
inefficient at finding the truths. If we employ linguistic indicators, the process can be dramatically
expedited. MTM provides further significant gains, with performance close to the Oracle. With only half
the cost of the baseline, MTM can already find 90% truths.
5.4 Enhance Individual SF Systems
Finally, as a by-product, our MTM approach can also be exploited to validate the responses from each
individual SF system based on their credibility scores. For fair comparison with the official KBP
evaluation, we use the same ground-truth in KBP2013 and standard precision, recall and F-measure
metrics as defined in (Ji et al., 2011). To increase the chance of including truths which may be particularly
difficult for a system to find, LDC prepared a manual key which was assessed and included in the final
ground truth. According to the SF evaluation setting, F-measure is computed based on the number of
unique true claims. After removing redundancy across multiple systems, there are 1,468 unique true
claims. The cutoff criteria for determining whether a response is true or not was optimized from the
development set.
Fig. 4 presents the F-measure scores of the best run from each individual SF system. We can see that
our MTM approach consistently improves the performance of almost all SF systems, in an absolute gain
range of [-1.22%, 5.70%]. It promotes state-of-the-art SF performance from 33.51% to 35.70%. Our
MTM approach provides more gains to SF systems which mainly rely on lexical or syntactic patterns
than other systems using distant supervision or logic rules.
1575
1 
0 10000 20000 30000 40000
0
2000
4000
6000
8000
10000
12000
14000
13 2
4
5
#tr
uth
s
 6 Oracle 
 5 MTM
 4 SVM
 3 Linguistic Indicator
 2 Voting
 1 Baseline
#total responses
6
Figure 3: Truth Finding Efficiency.
0 2 4 6 8 10 12 14 16 18 20
0
5
10
15
20
25
30
35
F-m
es
au
re 
(%
)
System
 Before
 After
Figure 4: Impact on Individual SF Systems.
1576
6 Conclusions and Future Work
Truth finding has received attention from both Natural Language Processing (NLP) and Data Mining
communities. NLP work has mostly explored linguistic analysis of the content, while Data Mining
work proposed advanced models in resolving conflict information from multiple sources. They have
relative strengths and weaknesses. In this paper we leverage the strengths of these two distinct,
but complementary research paradigms and propose a novel unsupervised multi-dimensional truth-
finding framework incorporating signals both from multiple sources, multiple systems and multiple
evidences based on knowledge graph construction with multi-layer linguistic analysis. Experiments on
a challenging SFV task demonstrated that this framework can find high-quality truths efficiently. In the
future we will focus on exploring more inter-dependencies among responses such as temporal and causal
relations.
Acknowledgments
This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), the U.S. Army Research Office under Cooperative Agreement
No. W911NF-13-1-0193, U.S. National Science Foundation grants IIS-0953149, CNS-0931975,
IIS-1017362, IIS-1320617, IIS-1354329, U.S. DARPA Award No. FA8750-13-2-0041 in the Deep
Exploration and Filtering of Text (DEFT) Program, IBM Faculty Award, Google Research Award,
DTRA, DHS and RPI faculty start-up grant. The views and conclusions contained in this document are
those of the authors and should not be interpreted as representing the official policies, either expressed
or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding any copyright notation here on.
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, and Z. Ives. 2007. Dbpedia: A nucleus for a web of open data. In
Proc. the 6th International Semantic Web Conference.
L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. 2010. Probabilistic models to reconcile complex data
from inaccurate data sources. In Proc. Int. Conf. on Advanced Information Systems Engineering (CAiSE?10),
Hammamet, Tunisia, June.
K. Bollacker, R. Cook, and P. Tufts. 2008. Freebase: A shared database of structured general human knowledge.
In Proc. National Conference on Artificial Intelligence.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an
architecture for never-ending language learning. In AAAI.
C. Chang and C. Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent
Systems and Technology (TIST), 2(3):27.
H. Deng, M. R. Lyu, and I. King. 2009. A generalized co-hits algorithm and its application to bipartite graphs. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ?09, pages 239?248, New York, NY, USA. ACM.
X. L. Dong, L. Berti-Equille, and D. Srivastavas. 2009a. Integrating conflicting data: The role of source
dependence. In Proc. 2009 Int. Conf. Very Large Data Bases (VLDB?09), Lyon, France, Aug.
X. L. Dong, L. Berti-Equille, and D. Srivastavas. 2009b. Truth discovery and copying detection in a dynamic
world. In Proc. 2009 Int. Conf. Very Large Data Bases (VLDB?09), Lyon, France, Aug.
A. Galland, S. Abiteboul, A. Marian, and P. Senellart. 2010. Corroborating information from disagreeing views.
In Proc. ACM Int. Conf. on Web Search and Data Mining (WSDM?10), New York, NY, Feb.
L. Ge, J. Gao, X. Yu, W. Fan, and A. Zhang. 2012. Estimating local information trustworthiness via multi-
source joint matrix factorization. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages
876?881. IEEE.
1577
H. Ji, R. Grishman, H. T. Dang, K. Griffitt, and J. Ellis. 2010. An overview of the tac2010 knowledge base
population track. In Proc. Text Analytics Conf. (TAC?10), Gaithersburg, Maryland, Nov.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview of the tac 2011 knowledge base population track. In Text
Analysis Conf. (TAC) 2011.
X. Li and R. Grishman. 2013. Confidence estimation for knowledge base population. In Proc. Recent Advances
in Natural Language Processing (RANLP).
Q. Li and H. Ji. 2014. Incremental joint extraction of entity mentions and relations.
Q. Li, H. Ji, and L. Huang. 2013. Joint event extraction via structured prediction with global features.
M. D. Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase
structure parses. In LREC, pages 449,454.
R. Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In
Proc. ACL2004.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled
data. In Proc. ACL2009.
J. Pasternack and D. Roth. 2010. Knowing what to believe (when you already know something). In
Proceedings of the 23rd International Conference on Computational Linguistics, pages 877?885. Association
for Computational Linguistics.
J. Pasternack and D. Roth. 2011. Making better informed trust decisions with generalized fact-finding. In Proc.
2011 Int. Joint Conf. on Artificial Intelligence (IJCAI?11), Barcelona, Spain, July.
J. Pasternack and D. Roth. 2013. Latent credibility analysis. In Proc. WWW 2013.
E. Peserico and L. Pretto. 2009. Score and rank convergence of hits. In Proceedings of the 32nd international
ACM SIGIR conference on Research and development in information retrieval, pages 770?771. ACM.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In
16th international World Wide Web conference (WWW 2007), New York, NY, USA. ACM Press.
S. Tamang and H. Ji. 2011. Adding smarter systems instead of human annotators: Re-ranking for slot filling
system combination. In Proc. CIKM2011 Workshop on Search & Mining Entity-Relationship data, Glasgow,
Scotland, UK, Oct.
VG Vydiswaran, C.X. Zhai, and D. Roth. 2011. Content-driven trust propagation framework. In Proceedings
of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 974?982.
ACM.
D. Wang, L. Kaplan, H. Le, and T. Abdelzaher. 2012. On truth discovery in social sensing: A maximum likelihood
estimation approach. In Proc. ACM/IEEE Int. Conf. on Information Processing in Sensor Networks (IPSN?12),
pages 233?244, Beijing, China, April.
X. Yin and W. Tan. 2011. Semi-supervised truth discovery. In Proc. 2011 Int. World Wide Web Conf. (WWW?11),
Hyderabad, India, March.
X. Yin, J. Han, and P. S. Yu. 2008. Truth discovery with multiple conflicting information providers on the Web.
IEEE Trans. Knowledge and Data Engineering, 20:796?808.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, and J. Han. 2012. A Bayesian approach to discovering truth from
conflicting sources for data integration. In Proc. 2012 Int. Conf. Very Large Data Bases (VLDB?12), Istanbul,
Turkey, Aug.
1578
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 771?781,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Collaborative Ranking: A Case Study on Entity Linking
Zheng Chen
Computer Science Department
Graduate Center
City University of New York
zchen1@gc.cuny.edu
Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
hengji@cs.qc.cuny.edu
Abstract
In this paper, we present a new ranking
scheme, collaborative ranking (CR). In con-
trast to traditional non-collaborative ranking
scheme which solely relies on the strengths
of isolated queries and one stand-alone rank-
ing algorithm, the new scheme integrates the
strengths from multiple collaborators of a
query and the strengths from multiple ranking
algorithms. We elaborate three specific forms
of collaborative ranking, namely, micro col-
laborative ranking (MiCR), macro collabora-
tive ranking (MaCR) and micro-macro collab-
orative ranking (MiMaCR). Experiments on
entity linking task show that our proposed
scheme is indeed effective and promising.
1 Introduction
Many natural language processing tasks can be for-
malized as a ranking problem, namely to rank a
collection of candidate ?objects? with respect to a
?query?. For example, intensive studies were de-
voted to parsing in which multiple possible pars-
ing trees or forests are ranked with respect to a sen-
tence (Collins, 2000; Charniak and Johnson, 2005;
Huang, 2008), machine translation in which multi-
ple translation hypotheses are ranked with respect to
a source sentence (Och, 2002; Shen et al, 2005),
anaphora resolution in which multiple antecedents
are ranked with respect to an anaphora (Yang et
al., 2008), and question answering in which mul-
tiple possible answers are ranked with respect to a
question (Ravichandran et al, 2003). Previous stud-
ies mainly focused on improving the ranking perfor-
mance using one stand-alone learning algorithm on
isolated queries.
Although a wide range of learning algorithms (un-
supervised, supervised or semi-supervised) is avail-
able, each with its strengths and weaknesses, there
is not a learning algorithm that can work best on
all types of data. In such a situation, it would
be desirable to build a ?collaborative? model by
integrating multiple models. Such an idea forms
the basis of ensemble methodology and it is well-
known that ensemble methods (e.g., bagging, boost-
ing) can improve the performance of many prob-
lems, in which classification is the most intensively
studied (Rokach, 2009). The other situation is re-
lated with isolated queries handled by learning al-
gorithms. The single query may not be formulated
with the best terms or the query itself may not con-
tain comprehensive information required for a high-
performance ranking algorithm. Therefore, tech-
niques of query expansion or query reformulation
can be introduced and previous research has shown
the effectiveness of those techniques in such applica-
tions as information retrieval and question answer-
ing (Manning et al, 2008; Riezler et al, 2007).
Nevertheless, previous research normally considers
query reformulation as a new query for the ranking
system, it would be more desirable to form a larger-
scale ?collaborative? group for the query and make
a unified decision based on the group.
Inspired from human collaborative learning in
which two or more people form a group and ac-
complish work together, we propose a new ranking
scheme, collaborative ranking, which aims to imi-
tate human collaborative learning and enhance sys-
tem ranking performance. The main idea is to seek
collaborations for each query from two levels:
(1) query-level: search a group of query collabo-
rators, and make the joint decision from the group
together with the query using a stand-alone ranking
algorithm.
(2) ranker-level: design a group of multiple
rankers, and make the joint decision from the entire
group on a single query.
771
query query collaborator candidate object query collaboration group ranker ranking output
(a) non-collaborative ranking (b) micro collaborative ranking (c) macro collaborative ranking (d) micro-macro collaborative ranking
Figure 1: Non-collaborative ranking and three collaborative ranking approaches.
Figure 1 presents an intuitive illustration of four
ranking approaches, including the traditional non-
collaborative ranking and three collaborative rank-
ing forms: micro collaborative ranking (MiCR),
macro collaborative ranking (MaCR), and micro-
macro collaborative ranking (MiMaCR).
Compared with the traditional non-collaborative
ranking that only leverages the information con-
tained in a single query and only applies one ranking
function (Figure 1 (a)), the three collaborative rank-
ing approaches have the following advantages:
(1)MiCR (corresponding to query-level collabo-
ration1) leverages the information contained in the
collaborators of a query. Figure 1 (b) demonstrates
that 6 query collaborators together with the query
form a query collaboration group.
(2)MaCR (corresponding to ranker-level collabo-
ration2) integrates the strengths from two or more
rankers. Figure 1 (c) demonstrates an example of 3
rankers.
(3)MiMaCR combines the advantages from
MiCR and MaCR as shown in Figure 1 (d).
In this paper, we will show the efficacy of collab-
orative ranking on the entity linking task defined in
the Knowledge Base Population (KBP) track (Ji et
al., 2010) at Text Analysis Conference (TAC). Each
query in the task is associated with a name string and
its context document. Traditional approaches for en-
tity linking only made use of the lexical or docu-
ment level information contained in the query, how-
ever, it may not be sufficient for the task. The intu-
ition why query-level collaboration may work is that
it leverages more comprehensive information about
the entity mention frommultiple ?collaborators? (re-
1Query is normally expressed by small-scale data structure,
so called micro.
2Ranker is normally implemented by large-scale algorithm,
so called macro.
lated documents containing the name string). Fur-
thermore, previous work on this task mainly focused
on comparing one ranking algorithm with the oth-
ers, however, each ranking algorithm has its own
strengths, and therefore, ranker-level collaboration
can potentially improve the performance. Last, the
combination of query-level and ranker-level collab-
oration can lead to further performance gains.
2 Non-collaborative Ranking
Let q denote a query. Let o(q) =
{
o(q)1 , . . . , o
(q)
n(q)
}
denote the object set associated with q, where n(q)
denotes the size of the o(q). The goal of non-
collaborative ranking is to seek a ranking function
f such that it computes ranking scores for the can-
didates in the object set, i.e., y(q) = f(o(q)) ={
y(q)1 , . . . , y
(q)
n(q)
}
.
Earlier studies on non-collaborative ranking
mainly explored unsupervised approaches, e.g., vec-
tor space model, link based algorithm such as
PageRank (Page et al, 1998). Unsupervised ap-
proaches are based on well-established statistical
and probability theory, nevertheless, they suffer
from some drawbacks, for example, it is hard to
tune parameters. Recently, supervised approaches
(named ?learning to rank?) that automatically learn
ranking functions from training data become the fo-
cus of ranking research. In the literature, super-
vised approaches are categorized into three classes,
namely, pointwise, pairwise, and listwise. We sum-
marize a comparison of the three approaches in Ta-
ble 1. We use the following notations in the table.
LetQ = {q1, . . . , qN} denote the set ofN queries
in the training data, each query qi is associated
with a set of objects o(qi) =
{
o(qi)1 , . . . , o
(qi)
n(qi)
}
and a set of ground-truth ranking scores y(qi) =
772
 pointwise pairwise listwise 
approach 
overview 
common: 1) use training samples; 2) learn the best ranking function by minimizing a given loss function; 
3) apply the ranking function at ranking step 
transform ranking to regression or 
classification on single objects 
transform ranking to classification on 
object pairs 
ranking by learning from 
lists of objects 
training set 
{ !"(#$), %"(#$)&}"'*,?,+-.$/;  !"#,?,$ 
 
 
{%&'()*), &-()*), ./}'"#,?,012*3;-"#,?,012*3;-4'; !"#,?,$  
. = 5+1  !" #$(%&) > #'(%&)
?1 !" #$
(%&) ? #'
(%&)
  
{(#(%&), .(%&))}/02,?,3 
 
loss function 
 
pointwise loss, e.g., square 
loss(Chen et al, 2009) 
pairwise loss, e.g., hinge loss(Zhang, 
2004), exponential loss(Bartlett et al, 
2003), logistic loss(Lin, 2002) 
listwise loss, e.g., cross 
entropy loss(Cao et al, 
2007),cosine loss(Qin et al, 
2007) 
 pros and 
cons 
pros: classification is well studied 
cons:1) only consider one object 
at a time ignoring relationship 
among objects  
pros: classification is well studied 
cons:1) only consider pairwise 
orders; 2) biased towards lists with 
more objects 
pros: fully consider 
relationship among objects 
cons: 1) less well studied in 
theory 
selected 
algorithms 
Discriminative model for IR 
(Nallapati, 2004); 
McRank (Li et al, 2007) 
SVM Ranking(Joachims, 2002); 
RankBoost(Freund et al, 2003); 
RankNet(Burges et al, 2005) 
ListNet (Cao et al, 2007); 
RankCosine(Qin et al, 2007); 
ListMLE (Xia et al, 2008) 
 Table 1: Comparison of pointwise, pairwise and listwise ranking approaches.
{
y(qi)1 , . . . , y
(qi)
n(qi)
}
. Let x(qi)j = ?(qi, o(qi)j ) denote
a feature vector associated with each query-object
pair (qi, o(qi)j ).
3 Collaborative Ranking
3.1 Micro Collaborative Ranking(MiCR)
Micro collaborative ranking is characterized by inte-
grating joint strengths from multiple query collabo-
rators and the query itself. It is based on the follow-
ing assumptions:
? Expandability: Query is expandable, that is, it
is able to find potential collaborators.
? Redundancy: Collaborators and query may
share redundant information.
? Diversity: Collaborators exhibit multifaceted
information that may complement the information
contained in the query.
? Robustness: Noisy collaborators are allowable,
and they could be put under control.
Let cq(q) = {cq1, . . . , cqk} be the k collabo-
rators of a query q. For each object o(q)j associ-
ated with q, we form k + 1 feature vectors x(q)j =
?(q, o(q)j ), x
(cq1)
j = ?(cq1, o
(cq1)
j ), . . . , x
(cqk)
j =
?(cqk, o(cqk)j ) . Let f be a ranking function which
is obtained by either an unsupervised or supervised
approach. There are two important steps that dis-
tinguish MiCR from traditional non-collaborative
ranking approaches:
? Step (1): searching the best k collaborators of q.
? Step (2): simulating the interaction of k collab-
orators at the ranking step.
Solutions for step (1) can vary from case to case.
In our case study presented later, we transform
the collaborator searching problem into a clustering
problem. Collaborators of a query are then formed
by members (excluding the query) in a cluster which
contains the query and k is the size of the cluster mi-
nus one.
We transform the problem of step (2) into solv-
ing a function g1 such that a ranking score y(q)j can
be computed for each object o(q)j . One approach
to computing g1 is to firstly compute the ranking
scores of collaborators and query using the ranking
function f and then combine those ranking scores
in some way (Formula 1). The other approach is to
learn a supervised ranking function f ? which takes
collaborators and query as input (Formula 2).
y(q)j = g1(f
(
x(q)j
)
, f
(
x(cq1)j
)
, . . . , f
(
x(cqk)j
)
) (1)
773
y(q)j = g1(?) = f
? (x(q)j , x
(cq1)
j , . . . , x
(cqk)
j
)
(2)
We present three specific forms of g1 in Formula
1, namely, max, min, and weighted. We can also
define a special case of weighted, called ?average?
in which w0 = w1 . . . = wk = 1/(k + 1).
? max: y(q)j = max(f
(
x(q)j
)
, . . . , f
(
x(cqk)j
)
)
? min: y(q)j = min(f
(
x(q)j
)
, . . . , f
(
x(cqk)j
)
)
? weighted: y(q)j = w0f
(
x(q)j
)
+
k?
i=1
wif
(
x(cqi)j
)
We will discuss three supervised versions of g1
(Formula 2) in section 4.4. A general algorithm for
MiCR is presented in Algorithm 1.
Algorithm 1 MiCR Algorithm.
Input:
a query q; a set of objects o(q); a function g1
Output:
a set of ranking scores y(q)
1: Search k collaborators of q:
cq(q) = {cq1, . . . , cqk}.
2: for j = 1; j <= n(q); j + + do
3: Form k + 1 feature vectors: x(q)j , x(cq1)j , . . . , x(cqk)j .
4: Compute function y(q)j = g1(?).
5: end for
6: return y(q)
3.2 Macro Collaborative Ranking(MaCR)
Macro collaborative ranking is characterized by in-
tegrating joint strengths from multiple rankers. It is
based on the following assumptions:
? Independence: Each ranker can make its own
ranking decisions.
? Diversity: Each ranker has its own strengths in
making ranking decisions.
? Collaboration: Rankers in the group could col-
laborate to make a consensus decision under some
mechanism.
Let x(q)j = ?(q, o(q)j ) be the feature vector formed
from the pair consisting of query q and an associated
object o(q)j . Let F? = {f1, . . . , fm} be m existing
ranking functions. We transform the computation of
collaboration among rankers into solving the follow-
ing composite function g2:
y(q)j = g2(f1
(
x(q)j
)
, . . . , fm
(
x(q)j
)
) (3)
Similar with MiCR, g2 can be expressed by max,
min, weighted (average) respectively:
? max: y(q)j = max{fi
(
x(q)j
)
}mi=1
? min: y(q)j = min{fi
(
x(q)j
)
}mi=1
? weighted: y(q)j =
m?
i=1
wifi
(
x(q)j
)
It is worth noting that max and min can be use-
ful only if the ranking scores produced by various
rankers can be compared to each other directly, how-
ever, in practice, this can hardly be true.
A special form of ranking problem is that only
the best object is required as output. In this case, we
have another version of g2 which is called voting:
? voting: y(q)j =
m?
i=1
sign(fi
(
x(q)j
)
)
in which sign(?) is an indicator function
sign(?) =
{
1 if fi outputs o(q)j as the best object
0 otherwise
A general algorithm for MaCR is presented in Al-
gorithm 2.
Algorithm 2 MaCR Algorithm.
Input:
a query q; a set of objects o(q); a set of m rank-
ing functions F?; a composite function g2
Output:
a set of ranking scores y(q)
1: for j = 1; j <= n(q); j + + do
2: Form a feature vector x(q)j .
3: Compute ranking scores:f1(x(q)j ), . . . , fm(x(q)j ).
4: Compute composite function: y(q)j = g2(?).
5: end for
6: return y(q)
3.3 Micro-Macro Collaborative Ranking
(MiMaCR)
The above two ranking approaches can be further
integrated into a joint model which is named Micro-
Macro Collaborative Ranking (MiMaCR). In order
to compute query-level and ranker-level collabora-
tion jointly, we solve the following complex com-
posite function g3:
y(q)j = g2(g1(?)) (4)
in which, for each object o(q)j , firstly we compute m
micro-ranking scores using m ranking functions on
query-level collaborators:
774
m?
????
????
g1(f1
(
x(q)j
)
, f1
(
x(cq1)j
)
, . . . , f1
(
x(cqk)j
)
)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
g1(fm
(
x(q)j
)
, fm
(
x(cq1)j
)
, . . . , fm
(
x(cqk)j
)
)
and secondly, we compute a macro-ranking score
using g2.
We can similarly define g1 and g2 as those in
MiCR andMaCR. A general algorithm for MiMaCR
is presented in Algorithm 3.
Algorithm 3 MiMaCR Algorithm.
Input:
a query q; a set of objects o(q); a set of ranking
functions F?; functions g1, g2
Output:
a set of ranking scores y(q)
1: Search k collaborators of q:
cq(q) = {cq1, . . . , cqk}.
2: for j = 1; j <= n(q); j + + do
3: Form k + 1 feature vectors: x(q)j , x(cq1)j , . . . , x(cqk)j .
4: Compute m micro-ranking scores using F? and g1.
5: Compute the macro-ranking score using g2.
6: end for
7: return y(q)
4 A Case Study on Entity Linking
To demonstrate the efficacy of our collaborative
ranking scheme, we apply it to the entity linking
task defined in the TAC-KBP2010 program (Ji et
al., 2010) because there is a large amount of train-
ing and evaluation data available and various non-
collaborative ranking approaches have been pro-
posed, as summarized in (McNamee and Dang,
2009; Ji et al, 2010).
4.1 Task Definition
The entity linking task aims to align a textual men-
tion of a named entity (person,organization or geo-
political) to an appropriate entry in a knowledge
base (KB), which may or may not contain the en-
tity. More formally, given a large corpus C, let q =
(q.id, q.string, q.text) denote a query in the task
which is a triple consisting of query id (q.id), name
string (q.string) and context document (q.text ?
C). Let o(q) =
{
o(q)1 , . . . , o
(q)
n(q)
}
denote the candi-
date KB entries associated with the query. Each KB
entry is a tuple consisting of KB id, KB title, KB in-
fobox (a set of attribute-value pairs that summarize
or highlight the key features of the concept or sub-
ject of this entry) and KB text. The goal is to rank
the KB entries and determine whether the top en-
try id should be considered as the answer, otherwise
NIL should be returned.
A specific example of the task is as follows,
given a name string ?Michael Jordan? and its con-
text document ?...England Youth International goal-
keeper Michael Jordan...?. From the name string,
we retrieve a set of candidate KB entries includ-
ing ?Michael Jordan (mycologist)?, ?Michael Jor-
dan (footballer)?, etc. The entity linking system
should return the id of ?Michael Jordan (footballer)?
as the answer, rather than the id of ?Michael Jordan?
who is most well known as a basketball player.
4.2 General Framework
A general framework of entity linking consists of
two crucial components, one for candidate gener-
ation, the other for candidate ranking, as shown
in Figure 2. In this paper, we developed the first
component by following the procedures described
in (Chen et al, 2010) which extensively leveraged
resources mined from Wikipedia. The performance
of the first component is 96.8% measured by recall
(the percentage of queries in which the candidates
cover the true answer). We then focus on the second
component.
 
Knowledge 
Base 
Query Expansion &
Candidate Generation
Candidate Ranking
Answer
Query
Figure 2: A general framework of entity linking sys-
tem.
4.3 Baseline Rankers
We developed 8 baseline rankers, including 4 un-
supervised rankers (f1, f2, f3, f4) and 4 supervised
rankers(f5, f6, f7, f8).
775
?Naive (f1): since the answer for each query can
either be a KB id or NIL, the naive ranker simply
outputs NIL for all queries.
?Entity (f2): f2 is defined as weighted combina-
tion of entity similarities in three types (person, or-
ganization and geo-political). Name entities are ex-
tracted from q.text and KB text respectively using
Stanford NER toolkit3. The formulas to compute en-
tity similarities are defined in (Yoshida et al, 2010).
?Tfidf (f3): f3 is defined as cosine similarity be-
tween q.text and KB text using tfidf weights.
?Profile (f4): f4 is defined as profile similarity
between q.text and KB text (Chen et al, 2010).
We used a slot filling toolkit (Chen et al, 2011) to
generate the profile (attribute-value pairs) for each
query.
?Maxent (f5): a pointwise ranker implemented
using OpenNLP Maxent toolkit4 which is based on
maximum entropy model.
?SVM (f6): a pointwise ranker implemented us-
ing SV M light (Joachims, 1999).
?SVM ranking (f7): a pairwise ranker imple-
mented using SV M rank (Joachims, 2006).
?ListNet (f8): a listwise ranker presented in (Cao
et al, 2007).
The four supervised rankers apply exactly the
same set of features except that SVM ranking (f7)
needs to double expand the feature vector. The fea-
tures are categorized into three levels, surface fea-
tures (Dredze et al, 2010; Zheng et al, 2010), doc-
ument features (Dredze et al, 2010; Zheng et al,
2010), and profiling features (entity slots that are ex-
tracted by the slot filling toolkit (Chen et al, 2011)).
4.4 MiCR for Entity Linking
We convert the collaborator searching problem into
a clustering problem, i.e., for a given query q in the
task, we retrieve at most K = 300 documents from
the large corpus C, each of which contains q.string;
we then apply a clustering algorithm to generate
clusters over the documents, and form query collab-
orators (excluding q.text) from the cluster that con-
tains q.text.
We experimented the following two clustering ap-
proaches:
3http://nlp.stanford.edu/software/CRF-NER.shtml
4http://maxent.sourceforge.net/about.html
(1)agglomerative clustering: it iteratively merges
clusters from singleton documents until a stop
threshold is reached. Document similarity is de-
fined as cosine similarity using tfidf weights. We ap-
plied group-average linking strategy to merge clus-
ters (Manning et al, 2008).
(2)graph-based clustering: it iteratively partitions
clusters from one single cluster until a stop threshold
is reached. Document similarity is similarly defined
as agglomerative clustering. We selected normalized
spectral clustering as our clustering algorithm (Shi
and Malik, 2000).
We first selected f3 as our basic ranking func-
tion, and investigated whether the ranker can ben-
efit from query collaborators formed by either ag-
glomerative clustering or graph clustering. We im-
plemented three versions of composite function g1
(max, min and average), and experimented their per-
formance using three unsupervised rankers f2, f3, f4
respectively.
Last, we implemented three supervised versions
of g1 (Maxent, SVM and ListNet respectively) by
adding cluster-level features and retraining the mod-
els in three supervised rankers f5, f6, f8 respec-
tively. Cluster-level features include maximum,
minimum, average tfidf/entity similarities between
the candidate and the query collaboration group.
4.5 MaCR for Entity Linking
We implemented two versions of composite func-
tion g2, average and voting. Furthermore, we in-
vestigated how the performance can be affected by
incrementally adding more rankers into the ranker
set F?. To do so, we first sorted the 8 rankers ac-
cording to their performance on the development set
from the highest to the lowest, and starting with the
highest performance ranker, we added one ranker at
a time, until we have all the 8 rankers. It is worth
noting that, when there are even number of rankers
in the set F?, ?ties? could take place using voting
function. In order to break the ties, we rank the
candidate higher if it is output as the answer from
a higher performance ranker.
4.6 MiMaCR for Entity Linking
We investigated how the final performance can be
boosted by jointly computing micro-ranking scores
and macro-ranking score.
776
5 Experiments
5.1 Data and Evaluation Metric
We used TAC-KBP2009 evaluation data as our train-
ing (75%) and development set (25%), and used
TAC-KBP2010 evaluation data as our blind testing
set (shown in Table 2).
Corpus Queries 
PER ORG GPE Total 
Training&Dev 627 2710 567 3904 
Testing 750 750 750 2250 
 Table 2: Training, development and testing corpus.
The reference KB consists of 818,741 entries
which are extracted from an October 2008 dump of
English Wikipedia. The source text corpus (denoted
as C in section 4.1) consists of 1,777,888 documents
in 5 genres (mostly Newswire and Web Text).
We used the official evaluation metric for TAC-
KBP2010 entity linking task, that is, micro-averaged
accuracy. It is computed by
micro-averaged accuracy = #correct answers#queries
An answer is considered as correct if the system
output (either a KB entry id or NIL) exactly matches
the key.
5.2 Performance of 8 Baseline Rankers
Table 3 shows the performance of the 8 baseline
rankers in 4 columns: Overall for all queries, PER
for person queries, ORG for organization queries,
and GPE for geo-political queries. Each column
is further split into All, KB (for Non-NIL queries)
and NIL (for NIL queries). It shows that all the four
supervised rankers perform better than the four un-
supervised rankers. Naive ranker obtains the low-
est overall micro-average accuracy (54.5%) but the
highest NIL accuracy (100%). Among the four un-
supervised rankers, profile ranker performs the best,
which clearly shows that the extracted attributes of
entities are effective for disambiguating confusable
names. For example, our data analysis shows that
the attribute value of ?per:alternative-name? from
the context document is particularly useful if a per-
son query is only mentioned by its last name. The
attribute ?per:title? is another important indicator to
discriminate one person from the other. For geo-
political queries, if the query is a city name, at-
tribute ?gpe:state? is useful to distinguish cities with
the same name but in different states or provinces.
Among the four supervised rankers, ListNet outper-
forms SVM ranking and then SVM ranking outper-
forms the two pointwise rankers. It may confirm
previous research findings that listwise ranking is
superior to pairwise ranking and pairwise ranking
is superior to pointwise ranking (Cao et al, 2007;
Zheng et al, 2010). The best baseline ranker (List-
Net) obtains an absolute overall accuracy gain of
26.6% over the naive ranker.
5.3 Impact of MiCR
To study the impact of MiCR, we first select f3
(tfidf ranker) as our ranking function. Figure 3
shows the performance of applying different query
collaborator searching strategies (graph or agglom-
erative clustering) and different versions of g1 (av-
erage, max and min respectively). We intention-
ally adjust the meaning of threshold (x-axis) for both
graph clustering and agglomerative clustering, such
that at threshold 0, both clustering algorithms gen-
erate the largest number of clusters (i.e., each doc-
ument is a cluster), and at threshold 1, they gen-
erate only one cluster. We now take the average
function (Figure 3 (a)) into considerations, as graph
clustering algorithm gradually partitions from one
cluster (corresponding to threshold 1) to more clus-
ters, the number of query collaborators gradually re-
duces, meanwhile, the accuracy gradually increases
and reaches the highest (73.6%) at threshold of 0.45,
which clearly shows that removing noisy collabora-
tors in the query collaboration group can improve
the performance. As the threshold continues drop-
ping below 0.45, the number of query collaborators
reduces and the performance significantly drops un-
til it reaches the baseline performance of tfidf ranker
(68.3%). It clearly shows that maintaining a control-
lable number of query collaborators can improve the
performance. For the agglomerative clustering, it is
the other story. As it continues merging from sin-
gleton clusters (corresponding to threshold 0) to one
single cluster, the performance continues increasing
until in the end it reaches the highest accuracy of
72.6%. However, unlike graph clustering, a peak
never appears in the middle which implies that ag-
glomerative clustering is inferior to graph clustering.
777
 Overall (%) PER (%) ORG (%) GPE (%) 
All KB NIL All KB NIL All KB NIL All KB NIL 
Naive 54.5 0.0 100 70.8 0.0 100 59.7 0 100 33.0 0 100 
Entity 65.6 48.6 79.7 82.1 52.1 94.5 68.4 46.2 83.3 46.1 48.5 41.3 
Tfidf 68.3 45.0 87.7 83.6 54.3 95.7 66.2 45.9 80.0 54.9 40.3 84.6 
Profile 75.0 58.7 88.6 90.8 82.2 94.4 73.3 62.7 80.4 61.0 46.1 91.1 
Maxent 77.4 72.3 81.6 86.5 82.6 94.4 73.3 62.7 80.4 61.0 71.5 72.1 
SVM 78.1 73.0 82.3 91.1 81.7 94.9 78.7 70.0 84.6 64.4 71.1 51.0 
SVM Rank 80.3 66.7 91.7 91.3 76.3 97.6 77.3 59.7 89.1 72.3 66.7 83.8 
ListNet 81.1 69.7 90.6 90.8 77.6 96.2 79.0 64.0 89.1 73.5 69.7 81.4 
 Table 3: Comparison of 8 baseline rankers.
0.0 0.2 0.4 0.6 0.8 1.0
68
69
70
71
72
73
74
0.0 0.2 0.4 0.6 0.8 1.0
67.0
67.5
68.0
68.5
69.0
69.5
70.0
70.5
0.0 0.2 0.4 0.6 0.8 1.0
56
58
60
62
64
66
68
70
(c) Min Function(b) Max Function(a) Average Function
Mic
ro-
Av
era
ge 
Ac
cur
acy
 (%
)
Threshold
 Graph-Ave
 Aggr-Ave
73.6
72.6
68.3
67.4
70.2
68.3
Threshold
Mic
ro-
Av
era
ge 
Ac
cur
acy
 (%
)
 Graph-Max
 Aggr-Max
69.0
56.7
68.3
Threshold
Mic
ro-
Av
era
ge 
Ac
cur
acy
 (%
)
 Graph-Min
 Aggr-Min
Figure 3: MiCR: comparison of average, max, and min functions combined with Graph and Agglomerative
(Aggr)-based query collaborator searching strategies (tfidf ranker).
The max function (Figure 3 (b)) leverages the
strengths from the strongest collaborator in the
group, which can potentially improve KB accuracy,
but meanwhile hurt NIL accuracy. As shown in the
figure, as more collaborators join in the group, the
performance increases first for both graph and ag-
glomerative clustering, however, it starts to deterio-
rate when arriving at a threshold, and in the end, the
performance drops even lower than the baseline of
tfidf ranker.
The min function (Figure 3 (c)) leverages the
strengths from the weakest collaborator in the group,
which can potentially improve NIL accuracy, but
meanwhile hurt KB accuracy. Our data analysis
shows that the gain in NIL accuracy can not afford
the larger loss in non-NIL accuracy, therefore, the
performance continues dropping as the threshold in-
creases. Min function is a counter example showing
that searching query collaborators can not always
lead to benefits.
To summarize so far, the best strategy for tfidf
ranker in MiCR approach is graph-ave (applying
graph clustering and using average function) which
obtains overall accuracy gain of 5.3% over the base-
line (68.3%). We further validate the performance
of graph-ave using f2, f4 ranking functions, for en-
tity ranker, we obtain accuracy gain of 6.3%, and for
profile ranker, we obtain accuracy gain of 3.0%.
We then experiment the three supervised g1 func-
tions (ListNet, Maxent, and SVM respectively)
using graph clustering as the query collaborator
searching strategy. Figure 6 shows that ListNet,
Maxent, SVM rankers obtain accuracy gain of 1.4%,
4.6%, 4.2% respectively over the baselines (corre-
sponding to those points at threshold 0).
5.4 Impact of MaCR
Figure 4 shows that the MaCR approach obtains
absolute accuracy gain of 1.3% (voting function)
and 0.5% (average function) over the best baseline
ranker (81.1%) when we add the 7th ranker (entity
ranker). The improvement of voting function is sta-
tistically significant at a 99.6% confidence level by
conducting Wilcoxon Matched-Pairs Signed-Ranks
Test on the 10 folds of the testing set. However, the
improvement of average function is not significant at
the 0.05 level which implies that average is inferior
to voting. We observe that the performance drops
778
when there are even number of rankers in the ranker
set using voting function, which implies that our tie
breaking strategy is not very effective.
We also experimented the voting function on the
top 10 KBP2009 entity linking systems (each sys-
tem performance is shown in the table embedded in
Figure 5, and experiment is similarly done as de-
scribed in section 4.5). Figure 5 shows that it can
obtain absolute accuracy gain of 4.7% over the top
entity linking system (82.2%). The reasons why we
achieve relative smaller gains using our own ranker
set are as follows: (1) we use the same candidate
object set for all rankers, while different KBP2009
systems may use their own set of objects. (2) our
top 4 supervised rankers apply almost the same set
of features, while different KBP2009 systems may
apply more diversified features. Therefore, diversity
is a highly important factor that makes MaCR ap-
proach effective.
0 1 2 3 4 5 6 7 8 9
80.8
81.0
81.2
81.4
81.6
81.8
82.0
82.2
82.4
82.6
81.1
81.681.681.481.281.080.8
81.7
82.4
81.8
82.2
81.1 81.3
81.7
Micro-
Avera
ge Ac
curacy
(%)
#rankers
 voting average
81.2
Figure 4: MaCR: comparison of voting and average.
0 1 2 3 4 5 6 7 8 9 10 11
82
83
84
85
86
87
83.784.6
8585.9
86.485.8
85.1
82.2Micro
-Avera
ge Acc
uracy 
(%)
#systems (rankers)
System ID Performance1 82.22 80.33 79.84 78.85 76.76 73.57 71.18 68.29 65.910 59.682.2
86.9
Figure 5: MaCR: applying voting function to the top
10 KBP2009 entity linking systems.
5.5 Impact of MiMaCR
We applied the following settings in our Mi-
MaCR approach: selecting graph clustering as the
query collaborator searching strategy, including five
rankers (tfidf, entity, Maxent, SVM and ListNet) in
the ranker set, using average function to compute
micro-ranking scores for the tfidf and entity ranker,
using the three corresponding supervised versions
of g1 to compute micro-ranking scores for Maxent,
SVM and ListNet respectively, and finally apply-
ing voting function to compute the macro-ranking
score. In Figure 6, the curve of ?MiMaCR? shows
how the performance of MiMaCR is affected by
the threshold in graph clustering. We obtain the
best micro-average accuracy of 83.7% at threshold
0.3, which is 2.6 % higher than the best baseline
ranker (81.1%). The improvement is statistically
significant at a 98.6% confidence level by conduct-
ing Wilcoxon Matched-Pairs Signed-Ranks Test on
the 10 folds of the testing set. The score reported
here is on par with the second best in the KBP2010
evaluation.
0.0 0.2 0.4 0.6 0.8 1.077.0
77.578.0
78.579.0
79.580.0
80.581.0
81.582.0
82.583.0
83.584.0
82.082.3 82.5
77.4
81.1
78.1Micro-A
verage
 Accur
acy (%
)
Threshold
 MiMaCR ListNet Maxent SVM
83.7
82.3
Figure 6: MiMaCR: Comparison of MiMaCR and
three supervised versions of g1 (ListNet, Maxent,
and SVM respectively).
6 Related Work
In the literature of information retrieval, query ex-
pansion is a useful technique that involves the pro-
cess of reformulating a query, and as a consequence,
is capable to extend the ability of a query and im-
prove the retrieval performance. Various approaches
for query expansion have been proposed, as summa-
rized in (Manning et al, 2008). TheMiCR presented
in this paper is superior to query expansion in two
aspects, firstly, we leverage more information con-
tained in multiple query collaborators; secondly, we
place great emphasis on interactions among mem-
bers in the query collaboration group.
In the literature of machine learning, there
has been a considerable amount of research on
ensemble-based classification, which is to build a
predictive classification model by integrating multi-
ple classifiers. A comprehensive survey is presented
in (Rokach, 2009). In contrast, ensemble-based
ranking has only recently attracted research interests
(Hoi and Jin, 2008; Wei et al, 2010). Although the
MaCR presented here is in essence ensemble-based
779
ranking, we extend it to MiMaCR which integrates
the strengths from both MiCR and MaCR.
It is worth noting that ?collaborative ranking? pre-
sented here should be distinguished from ?collabo-
rative filtering? in that ?collaborative filtering? uses
the known preferences of a group of users to gen-
erate personalized recommendations while ?collab-
orative ranking? leverages query collaborators and
ranker collaborators to enhance the overall ranking
performance.
There has been an increasing amount of research
on entity linking, especially through KBP2009 and
KBP2010. Various unsupervised or supervised ap-
proaches have been proposed, as summarized in
(McNamee and Dang, 2009; Ji et al, 2010). How-
ever, most of the previous research mainly fo-
cused on one or two ranking algorithms on isolated
queries. In this paper, we have extended the work
by systematically studying the possibility of perfor-
mance enhancement through query-level collabora-
tion and ranker-level collaboration.
7 Conclusions
We presented a new ranking scheme called collab-
orative ranking with three specific forms, MiCR,
MaCR and MiMaCR and demonstrated its effective-
ness on entity linking task. However, our scheme is
not restricted to this specific task and it is generally
applicable to many other other applications such as
question answering. In MiCR, effective searching
of query collaborators and active interplay among
members in the query collaboration group are two
key factors that make MiCR successful. In MaCR,
diversity is a highly important factor to make it suc-
cessful. Overall, MiMaCR can bootstrap the per-
formance to its maximum if integrating MiCR and
MaCR properly. However, the better performance is
at the expense of much more computations.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
Number W911NF-09-2-0053, the U.S. NSF CA-
REER Award under Grant IIS-0953149 and PSC-
CUNY Research Program. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as repre-
senting the official policies, either expressed or im-
plied, of the Army Research Laboratory or the U.S.
Government. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
ment purposes notwithstanding any copyright nota-
tion hereon.
References
P. L. Bartlett, M. I. Jordan and J. D. McAuliffe. 2003.
Convexity, classification, and risk bounds. Technical
Report 638, Statistics Department, University of Cali-
fornia, Berkeley.
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
Rank Using Gradient Descent. In Proceedings of the
22th International Conference on Machine Learning
(ICML 2005).
Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai and H. Li. 2007.
Learning to rank: from pairwise approach to listwise
approach In Proceedings of the 24th International
Conference on Machine Learning (ICML 2007), pages
129-136.
E. Charniak and M. Johnson. 2005. Coarseto-fine-
grained n-best parsing and discriminative reranking.
In ACL-05, pages 173-180.
W. Chen, T.-Y. Liu, Y. Lan, Z. Ma, and H. Li. 2009.
Ranking measures and loss functions in learning to
rank. In Advances in Neural Information Processing
Systems 22 (NIPS 2009), pages 315-323.
Z. Chen, S. Tamang, A. Lee, X. Li, W.-P. Lin, M. Snover,
J. Artiles, M. Passantino and H. Ji. 2010. CUN-
YBLENDER TAC-KBP2010 Entity Linking and Slot
Filling SystemDescription. In Proceedings of Text An-
alytics Conference (TAC2010).
Z. Chen, S. Tamang, A. Lee and H. Ji. 2011. A Toolkit
for Knowledge Base Population. In SIGIR.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of the 17th Interna-
tional Conference on Machine Learning (ICML 2000),
pages 175-182.
M. Dredze, P. McNamee, D. Rao, A. Gerber and T. Finin.
2010. Entity Disambiguation for Knowledge Base
Population. In Proc. COLING 2010.
Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2003.
An efficient boosting algorithm for combining pref-
erences. In Journal of Machine Learning Research,
4:933-969.
S. Hoi and R. Jin. 2008. Semi-supervised ensemble
ranking. In Proc. of the 23rd AAAI Conf. on Artificial
Intelligence.
L. Huang. 2008. Forest Reranking: Discriminative Pars-
ing with Non-Local Features. In ACL-HLT-08, pages
586-594.
780
H. Ji, R. Grishman, H. T. Dang and K. Griffit. 2010. An
Overview of the TAC2010 Knowledge Base Popula-
tion Track. In Proceedings of Text Analytics Confer-
ence (TAC2010).
T. Joachims. 1999. Making large-Scale SVM Learn-
ing Practical. Advances in Kernel Methods - Support
Vector Learning, B. Scho?lkopf and C. Burges and A.
Smola (ed.), MIT-Press, 1999.
T. Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining(KDD 2002).
T. Joachims. 2006. Training Linear SVMs in Linear
Time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Y. Lan, T.-Y. Liu, T. Qin, Z. Ma, and H. Li. 2008. Query-
level stability and generalization in learning to rank.
In Proceedings of the 25th International Conference
on Machine Learning (ICML 2008), pages 512-519.
P. Li, C. Burges, and Q. Wu. 2007. Mcrank: Learn-
ing to rank using multiple classification and gradient
boosting In Advances in Neural Information Process-
ing Systems 20 (NIPS2007).
Y. Lin. 2002. Support vector machines and the bayes
rule in classification. In Data Mining and Knowledge
Discovery, pages 259-275.
C. D. Manning, P. Raghavan and H. Schu?tze. 2008 . In-
troduction to Information Retrieval. Cambridge Uni-
versity Press.
P. McNamee and H. Dang. 2009. Overview of the TAC
2009 Knowledge Base Population Track. In Proceed-
ings of TAC.
R. Nallapati. 2004. Discriminative models for informa-
tion retrieval. In SIGIR.
F. J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, Computer Science Department, RWTH
Aachen, Germany, October.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The PageRank Citation Ranking: Bringing Order to
the Web. Technical report, Stanford Digital Library
Technologies Project.
T. Qin, X.-D. Zhang, M.-F. Tsai, D.-S. Wang, T.-Y. Liu
and H. Li. 2007. Query-level loss functions for infor-
mation retrieval. In Information Processing and Man-
agement.
T. Qin, T.-Y. Liu, X.-D. Zhang,D.-S. Wang, and H. Li.
2008. Global Ranking Using Continuous Conditional
Random Fields. In Advances in Neural Information
Processing Systems 21 (NIPS 2008).
D. Ravichandran, E. Hovy and F. J. Och. 2003. Statis-
tical QA - Classifier vs. Re-ranker: What?s the differ-
ence? In Proceedings of the ACL Workshop on Multi-
lingual Summarization and Question Answering.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal
and Y. Liu. 2007. Statistical Machine Translation for
Query Expansion in Answer Retrieval. In Proceedings
of ACL.
L. Rokach. 2009. Ensemble-based classifiers. Artif In-
tell Rev DOI 10.1007/s10462-009-9124-7.
L. Shen, A. Sarkar, and F. J. Och. 2005. Discriminative
reranking for machine translation. In Proceedings of
HLT-NAACL.
J. Shi and J. Malik. 2000. Normalized Cuts and Image
Segmentation. In Machine Intelligence, vol. 22, no. 8,
pages 888-905.
F. Wei, W. Li and S. Liu. 2010. iRANK: A rank-learn-
combine framework for unsupervised ensemble rank-
ing. In Journal of the American Society for Infor-
mation Science and Technology,61: 1232C1243. doi:
10.1002/asi.21296.
X. Yang and J. Su and C.L. Tan 2008. A Twin-Candidate
Model for Learning-based Anaphora Resolution. In
Computational Linguistics, vol. 34, no. 3, pages 327-
356.
M. Yoshida, M. Ikeda, S. Ono, I. Sato, and H. Nakagawa.
2010. Person name disambiguation by boostrapping.
In SIGIR.
T. Zhang. 2004. Statistical analysis of some multicate-
gory large margin classification methods. In Journal
of Machine Learning Research, 5, 1225-1251.
W. Zhang, J. Su, C. L. Tan and W.T. Wang. 2010. Entity
Linking Leveraging Automatically Generated Annota-
tion. In Proc. COLING 2010.
Z. Zheng, F. Li, M. Huang, X. Zhu. 2010. Learning to
Link Entities with Knowledge Base. In Proc. HLT-
NAACL2010.
781
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
An Iterative Link-based Method for Parallel Web Page Mining 
Le Liu1, Yu Hong1, Jun Lu2, Jun Lang2, Heng Ji3, Jianmin Yao1 
1School of Computer Science & Technology, Soochow University, Suzhou, 215006, China 
2Institute for Infocomm Research, Singapore, 138632 
3Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA 
giden@sina.cn,{tianxianer,lujun59,billlangjun}@gmail.com 
jih@rpi.edu,jyao@suda.edu.cn 
 
Abstracts 
Identifying parallel web pages from bi-
lingual web sites is a crucial step of bi-
lingual resource construction for cross-
lingual information processing. In this 
paper, we propose a link-based approach 
to distinguish parallel web pages from bi-
lingual web sites. Compared with the ex-
isting methods, which only employ the 
internal translation similarity (such as 
content-based similarity and page struc-
tural similarity), we hypothesize that the 
external translation similarity is an effec-
tive feature to identify parallel web pages. 
Within a bilingual web site, web pages 
are interconnected by hyperlinks. The 
basic idea of our method is that the trans-
lation similarity of two pages can be in-
ferred from their neighbor pages, which 
can be adopted as an important source of 
external similarity. Thus, the translation 
similarity of page pairs will influence 
each other. An iterative algorithm is de-
veloped to estimate the external transla-
tion similarity and the final translation 
similarity. Both internal and external 
similarity measures are combined in the 
iterative algorithm. Experiments on six 
bilingual websites demonstrate that our 
method is effective and obtains signifi-
cant improvement (6.2% F-Score) over 
the baseline which only utilizes internal 
translation similarity. 
1 Introduction 
Parallel corpora have played an important role in 
multilingual Natural Language Processing, espe-
cially in Machine Translation (MT) and Cross-
lingual Information Retrieval(CLIR). However, 
it?s time-consuming to build parallel corpora 
manually. Some existing parallel corpora are 
subject to subscription or license fee and thus not 
freely available, while others are domain-specific. 
Therefore, a lot of previous research has focused 
on automatically mining parallel corpora from 
the web. 
In the past decade, there have been extensive 
studies on parallel resource extraction from the 
web (e.g., Chen and Nie, 2000; Resnik 2003; 
Jiang et al., 2009) and many effective Web min-
ing systems have been developed such as 
STRAND, PTMiner, BITS and WPDE. For most 
of these mining systems, there is a typical paral-
lel resource mining strategy which involves three 
steps: (1) locate the bilingual websites (2) identi-
fy  parallel web pages from these bilingual web-
sites and (3) extract bilingual resources from the 
parallel web pages.  
In this paper, we focus on the step (2) which is 
regarded as the core of the mining system 
(Chunyu, 2007). Estimating the translation simi-
larity of two pages is the most basic and key 
problem in this step. Previous approaches have 
tried to tackle this problem by using the infor-
mation within the pages. For example, in the 
STRAND and PTMiner system, a structural fil-
tering process that relies on the analysis of the 
underlying HTML structure of pages is used to 
determine a set of pair-specific structural values, 
and then the values are used to decide whether 
the pages are translations of one another. The 
BITS system filters out bad pairs by using a large 
bilingual dictionary to compute a content-based 
similarity score and comparing the score with a 
threshold. The WPDE system combines URL 
similarity, structure similarity with content-based 
similarity to discover and verify candidate paral-
lel page pairs. Some other features or rules such 
as page size ratio, predefined hypertexts which 
link to different language versions of a web page 
are also used in most of these systems. Here, all 
of the mining systems are simply using the in-
formation within the page in the process of find-
1216
ing parallel web pages. In this paper, we attempt 
to explore other information to identify parallel 
web pages. 
On the Internet, most web pages are linked by 
hyperlinks. We argue that the translation similar-
ity of two pages depends on not only their inter-
nal information but also their neighbors. The 
neighbors of a web page are a set of pages, 
which link to the page. We find that the similari-
ty of neighbors can provide more reliable evi-
dence in estimating the translation similarity of 
two pages.  
The main issues are discussed in this paper as 
follows:  
? Can the neighbors of candidate page pairs 
really contribute to estimating the translation 
similarity?  
? How to estimate the translation similarity of 
candidate page pairs by using their neighbors? 
Our method has the following advantages: 
High performance 
The external and internal information is com-
bined to verify parallel page pairs in our method, 
while in previous mining systems, only internal 
information was used. Experimental results show 
that compared with existing parallel page pair 
identification technologies, our method obtains 
both higher precision and recall (6.2% and 6.3% 
improvement than the baseline, respectively). In 
addition, the external information used in our 
method is a more effective feature than internal 
features alone such as structural similarity and 
content-based similarity. 
Language independent 
In principle, our method is language inde-
pendent and can be easily ported to new lan-
guage pairs, except for the language-specific bi-
lingual lexicons. Our method takes full ad-
vantage of the link information that is language-
independent. For the bilingual lexicons in our 
experiments, compared to previous methods, our 
method does not need a big bilingual lexicon, 
which is good news to less-resource language 
pairs. 
Unsupervised and fewer parameters 
In previous work, some parameters need to be 
optimized. Due to the diversity of web page 
styles, it is not trivial to obtain the best parame-
ters. Some previous researches(Resnik, 2003; 
Zhang et al., 2006) attempt to optimize parame-
ters by employing machine learning method. In 
contrast, in our method, only two parameters 
need to be estimated. One parameter remains 
stable for different style websites. Another pa-
rameter can be easily adjusted to achieve the best 
performance. Therefore, our method can be used 
in other websites with different styles, without 
much effort to optimize these parameters.  
2 Related Work 
A large amount of literature has been published 
on parallel resource mining from the web. Ac-
cording to the existing form of the parallel re-
source on the Internet, related work can be cate-
gorized as follows: 
Mining from bilingual websites 
Most existing web mining systems aimed at 
mining bilingual resource from the bilingual 
websites, such as PTMiner (Nie et al., 1999), 
STRAND (Resnik and Smith, 2003), BITS (Ma 
and Liberman, 1999), PTI (Chen et al., 2004). 
PTMiner uses search engines to pinpoint the 
candidate sites that are likely to contain parallel 
pages, and then uses the collected URLs as seeds 
to further crawl each web site for more URLs. 
Web page pairs are extracted based on manually 
defined URL pattern matching, and further fil-
tered according to several criteria. STRAND us-
es a search engine to search for multilingual 
websites and generated candidate page pairs 
based on manually created substitution rules. 
Then, it filters some candidate pairs by analyzing 
the HTML pages. PTI crawls the web to fetch 
(potentially parallel) candidate multilingual web 
documents by using a web spider. To determine 
the parallelism between potential document pairs, 
a filename comparison module is used to check 
filename resemblance, and a content analysis 
module is used to measure the semantic similari-
ty. BITS was the first to obtain bilingual web-
sites by employing a language identification 
module, and then for each bilingual website, it 
extracts parallel pages based on their content.  
Mining from bilingual web pages 
Parallel/bilingual resources may exist not only 
in two parallel monolingual web pages, but also 
in single bilingual web pages. Jiang et al. (2009) 
used an adaptive pattern-based method to mine 
interesting bilingual data based on the observa-
tion that bilingual data usually appears collec-
tively following similar patterns. They found that 
bilingual web pages are a promising source of 
up-to-date bilingual terms/sentences which cover 
many domains and application scenarios. In ad-
dition, Feng et al. (2010) proposed a new method 
1217
to automatically acquire bilingual web pages 
from the result pages of a search engine.  
Mining from comparable corpus 
Several attempts have been made to extract 
parallel resources from comparable corpora. 
Zhao et al. (2002) proposed a robust, adaptive 
approach for mining parallel sentences from a 
bilingual comparable news collection. In their 
method, sentence length models and lexicon-
based models were combined under a maximum 
likelihood criterion. Smith et al. (2010) found 
that Wikipedia contains a lot of comparable doc-
uments, and adopted a ranking model to select 
parallel sentence pairs from comparable docu-
ments. Bharadwaj et al. (2011) used a SVM clas-
sifier with some new features to identify parallel 
sentences from Wikipedia.  
3 Iterative Link-based Parallel Web 
Pages Mining 
As mentioned, the basic idea of our method is 
that the similarity of two pages can be inferred 
from their neighbors. This idea is illustrated in 
Figure 1.  
A D
E
C
B
A?
D?
E?
C?
B?
?
 
Figure 1 Illustration of the link-based method 
In Figure 1, A, B, C, D and E are some pages 
in the same language; while A?, B?, C?, D? and E? 
are some pages in another language. The solid 
black arrows indicate the links between these 
pages. For example, page A points to C, page B? 
points to C? and so on. Then the page set {A, B, 
D, E} is called the neighbors of page C. Similar-
ly, the page set {A?, B?, D?, E?} contains the 
neighbors of page C?. If the page pairs : <A, A?>, 
<B, B?>, <D, D?> and <E, E?> have high transla-
tion similarities, then it can be inferred that page 
C and C? have a high probability to be a pair of 
parallel pages. Every page has its own neighbors. 
For each web page, our method views link-in and 
link-out hyperlinks as the same. Thus, the linked 
pages will influence each other in estimating the 
translation similarity. For example, the similari-
ties of two pairs <A, A?> and <C, C?> will influ-
ence each other. It is an iterative process. We 
will elaborate the process in the following sec-
tions.  
Since our goal is to find parallel pages in a 
specific website, the key task is to evaluate the 
translation similarity of two pages (which are in 
different languages) as accurately as possible. 
The final similarity of two pages should depend 
both on their internal similarity and external sim-
ilarity. The internal similarity means the similari-
ty estimated by using the information in the page 
itself, such as the structure similarity and the 
content-based similarity of the two pages. On the 
other hand, the external similarity of two pages is 
the similarity depending on their neighbors. The 
final translation similarity is called the En-
hanced Translation Similarity (ETS). The ETS 
of two pages can be calculated as follows:  
   (   )        (   )  (   )  
                                      (   )   [   ]              (1) 
Where,    (   ) is the internal translation simi-
larity of two pages: e and c;     (   ) represents 
the external translation similarity of pages e and 
c.    (   ) indicates the final similarity of two 
pages, which combines the internal with external 
translation similarity. 
In this paper, we conduct the experiments on 
English-Chinese parallel page pair mining. How-
ever, our method is language-independent. Thus, 
it can be applied to other language pairs by only 
replacing a bilingual lexicon. The symbol e and c 
always indicate an English page and a Chinese 
page respectively in this paper. In the following 
sections, we will describe how to calculate the 
   (   ) and     (   ) step by step. 
3.1 Preprocessing 
The input of our method is a bilingual website. 
This paper aims to find English/Chinese parallel 
pages. So a 3-gram language model is used to 
identify (or classify) the language of a certain 
document. The performance of the language 
identification module achieves 99.5% accuracy 
through in-house testing. As a result, a set of 
English pages and a set of Chinese pages are ob-
tained. In order to get the neighbors of a page, 
for each bilingual website, two networks are con-
structed based on the hyperlinks, one for English 
pages and another for Chinese pages. 
3.2 The Internal Translation Similarity 
Following Resnik and Smith (2003), three fea-
tures are used to evaluate the internal translation 
similarity of two pages: 
1218
The size ratio of two pages 
The length ratio of two documents is the sim-
plest criterion for determining whether two doc-
uments are parallel or not. Parallel documents 
tend to be similar in length. And it is reasonable 
to assume that for text E in one language and text 
F in another language, length(E) ? C?length(F), 
where C is a constant that depends on the lan-
guage pair. Here, the content length of a web 
page is regarded as its length. 
The structure similarity of two pages 
The HTML tags describe and control a web 
page?s structure. Therefore, the structure similar-
ity of two pages can be calculated by their 
HTML tags. Here, the HTML tags of each page 
are extracted (except the visual tags such as ?B?, 
?FONT?.) as a linear sequence. Then the struc-
ture similarity of two pages is computed by com-
paring their linearized sequences. In this paper, 
the LCS algorithm (Dan, 1997) is adopted to find 
the longest common sequences of the two HTML 
tag sequences. The ratio of LCS length and the 
average length of two HTML tag sequences are 
used as the structure similarity of the two pages.  
The content-based translation similarity of 
two pages 
The basic idea is that if two documents are 
parallel, they will contain word pairs that are mu-
tual translations (Ma, 1999). So the percentage of 
translation word pairs in the two pages can be 
considered as the content-based similarity. The 
translation words of two documents can be ex-
tracted by using a bilingual lexicon. Here, for 
each word in English document, we will try to 
find a corresponding word in Chinese document.  
Finally, the internal translation similarity of 
two pages is calculated as follows: 
   (   )       (   )  (   )  
                                          (   )   [   ]        (2) 
Where,     (   )  and        (   )  are the con-
tent-based and structural similarity of page   and 
  respectively. In addition, the size ratio of two 
pages is used to filter invalid page pairs.  
3.3 The External and Enhanced Transla-
tion Similarity 
As described above, the external translation 
similarity of two pages depends on their neigh-
bors:  
    (   )     (  ( )   ( )) (3) 
Where, PG(x), a set of pages, is the neighbors of 
page x. Obviously, the similarity of two sets re-
lies on the similarity of the elements in the two 
sets. Here, the elements are namely web pages. 
So,     (   ) equals to    (  ( )   ( )), and 
   (  ( )   ( ))  depends on    (     ) 
(       belongs to    ( )   ( ) , respectively) 
and    (   ) . According to Equation (1), 
   (   )  depends on    (   )  and     (   ) . 
Therefore, it is a process of iteration.    (   ) 
will converge after a certain number of iterations. 
Thus,     (   )  is defined as the enhanced 
similarity of page   and   after the i-th iteration, 
and the same is for     
 (   ) and     (  ( ) 
  ( )) .     (  ( )   ( ))  is computed by 
the following algorithm: 
Algorithm 1: Estimating the external transla-
tion similarity 
Input:      ( )   ( ) 
Output:     
 (   ) 
Procedure:  
   ? 0 
     ?   ( ) 
      ?   ( ) 
While        and       are both not empty: 
             
?                          (   
   (   ))  
    ?     +        (   ) 
Remove   from        
Remove   from       
    
 (   )       ( ( )  ( )) 
                             (   ( )     ( ) ) 
Algorithm 2 Estimating the enhanced transla-
tion similarity 
Input:      , (the English and Chinese page set) 
Output:    (   )           
Initialization: Set ETS(e, c) random value or 
small value 
Procedure:  
LOOP: 
For each   in    : 
For each   in   : 
     (   )         
 (   ) 
                                          (   )     (   ) 
Parameters normalization 
UNTIL    (   ) is stable  
Algorithm 1 tries to find the real parallel pairs 
from   ( ) and   ( ). The similarity of   ( ) 
and   ( ) is calculated based on the similarity 
1219
values of these pairs. Finally,    (   ) is calcu-
lated by the following algorithm 2. 
In Algorithm 2, the input    and    are English 
and Chinese page sets in a certain bilingual web-
site. We use algorithm 2 to estimate the en-
hanced translation similarity. 
3.4 Find the Parallel Page Pairs 
At last, the enhanced translation similarity of 
every pair is obtained, and the parallel page pairs 
can be extracted in terms of these similarities: 
Algorithm 3 Finding parallel page pairs 
Input:       
    (   )              
       (or       ) 
Output:  Parallel Page Pairs List :     
Procedure:  
LOOP: 
                       (   (   )) 
Add       to     
Remove   from     
Remove   from     
UNTIL size of     >       (or    (   )  < 
       ) 
This algorithm is similar to Algorithm 1 in 
each bilingual website. The input      is an 
integer threshold which means that only top 
      page pairs will be extracted in a certain 
website. It needs to be noted that      is al-
ways less than      and     . While the input 
        is another kind of threshold that is 
used for extracting page pairs with high transla-
tion similarity.  
4 Experiments and Analysis 
4.1 Experimental setup 
Our experiments focus on six bilingual websites. 
Most of them are selected from HK government 
websites. All the web pages were retrieved by 
using a web site download tool: HTTrack1. We 
notice that a small amount of pages doesn?t al-
ways contain valuable contents. So, we put a 
threshold (100 bytes in our experiment) on the 
web pages' content to filter meaningless pages. In 
order to evaluate our method, the bilingual page 
pairs of each website are annotated by a human 
annotator. Finally, we got 23109 pages and 
11684 bilingual page pairs in total for testing. 
                                                 
1 http://www.httrack.com/ 
The basic information of these websites is listed 
in Table 1. 
It?s time-consuming to annotate whether two 
pages is parallel or not. Note that if a website 
contains N English pages and M Chinese pages, 
an annotator has to label N*M page pairs. To the 
best of our knowledge, there is no large scale and 
public parallel page pair dataset with human an-
notation. So we try to build a reliable and large-
scale dataset. 
In our experiments, URL similarity is used to 
reduce the workload for annotation. For a certain 
website, firstly, we obtain its URL pattern be-
tween English and Chinese pages manually. For 
example, in the website ?www.gov.hk?, the URL 
pairs like: 
http://www.gov.hk/en/about/govdirectory/   (English) 
http://www.gov.hk/sc/about/govdirectory/   (Chinese) 
The URL pairs always point to a pair of paral-
lel pages. So <?/en/?,?/sc/?> is considered as a 
URL pattern that was used to find parallel pages. 
For the other URLs that can?t match the pattern, 
we have to label them by hand. The column ?No 
pattern pairs? in Table 1 shows that the number 
of parallel page pairs which mismatch any pat-
terns. 
Table 1 Number of pages and bilingual page pairs of 
each websites 
Site ID En/Ch pages 
Total 
pairs 
No pat-
tern pairs 
URL 
S1 1101/1098 1092 20 www.gov.hk 
S2 501/497 487 7 www.customs.gov.hk 
S3 995/775 768 12 www.sbc.edu.sg 
S4 4085/3838 3648 4 www.swd.gov.hk 
S5 660/637 637 0 www.landsd.gov.hk 
S6 4733/4626 4615 8 www.td.gov.hk 
 total 12075/11471 11684 51  
Each website listed in Table 1 has a URL pat-
tern for most parallel web pages. Some previous 
researches used the URL similarity or patterns to 
find parallel page pairs. However, due to the di-
versity of web page styles and website mainte-
nance mechanisms, bilingual websites adopt var-
ied naming schemes for parallel documents (Shi, 
et al, 2006). The effect of URL pattern-based 
mining always depends on the style of website. 
In order to build a large dataset, the URL pattern 
is not used in our method. Our method is able to 
handle bilingual websites without URL pattern 
rules. 
In addition, an English-Chinese dictionary 
with 64K words pairs is used in our experiments. 
Algorithm 3 needs a threshold       or 
1220
       . It is very hard to tune the        
because it varies a lot in different websites and 
language pairs. However, Table 1 shows that the 
number of parallel pages is smaller than that of 
English and Chinese pages. Here, for each web-
site, the      is set to the number of Chinese 
pages (which is always smaller than that of Eng-
lish pages). In this way, the precision will never 
reach 100%, but it is more practical in a real ap-
plication. As a result, in some experiments, we 
only report the F-score, and the precision and 
recall can be calculated as follows:  
          
       (            )
      
                 (4) 
       
       (            )
        
                      (5) 
Where,        for each website is listed in the 
?Total  pairs? column of Table 1. 
4.2 Results and Analysis 
Performance of the Baseline 
Let?s start by presenting the performance of a 
baseline method as follows. The baseline only 
employs the internal translation similarity for 
parallel web pages mining. Algorithm 3 is also 
used to get the page pairs in baseline system. 
Here, the input    (   )  is replaced by 
   (   ) . The parameter   in Equation 2 is a 
discount factor. For different   values, the per-
formance of baseline system on six websites is 
shown in Figure 2. In the Figure 2, it shows that 
when   is set to 0.6, the baseline system achieves 
the best performance. The precision, recall and 
F-score are 85.84%, 87.55% and 86.69% respec-
tively. So in the following experiments, we al-
ways set ? to 0.6.  
 
Figure 2 Performances of baseline system with differ-
ent   value 
Performance of Our Method 
As described in Section 3, our method com-
bines the internal with external translation simi-
larity in estimating the final translation similarity 
(i.e., ETS) of two pages. So, the discount factor 
  in Equation (1) is important in our method. 
Besides, as shown in Algorithm 2, the iterative 
algorithm is used to calculate the similarity. Then, 
one question is that how many iterations are re-
quired in our algorithm. Figure 3 shows the per-
formance of our method on each website. Its hor-
izontal axis represents the number of iterations 
and the vertical axis represents the F-score. And 
for each website, the F-scores with different   
(range from 0.2 to 0.8) are also reported in this 
figure. From Figure 3, it is very easy to find that 
the best iteration number is 3. For almost all the 
websites, the performance of our method 
achieves the maximal values and converges after 
the third iteration. In addition, Figure 3 also indi-
cates that our method is robust for different web-
sites. In the following experiments, the iteration 
number is set to 3. 
Next, let?s turn to the discount factor  . Figure 
4 reports the experimental results on the whole 
dataset. Here, the horizontal axis represents the 
discount factor   and the vertical axis represents 
the F-score.     means that only the internal 
similarity is used in the algorithm, so the F-score 
equals to that in Figure 2 when      . On the 
contrary,     means that only the external 
similarity is used in the method, and the F-score 
is 80.20%. The performance is lower than the 
baseline system when only the external link in-
formation is used, but it is much better than the 
performance of the content-based method and 
structure-based method whose F-scores are 64.82% 
and 64.0% respectively. Besides, it is shown 
from Figure 4, the performance is improved sig-
nificantly when the internal and external similari-
ty measures are combined together. Furthermore, 
it is somewhat surprising that the discount factor 
  is not important as we previously expected. In 
fact, if we discard the cases that   equals to 0 or 
1, the difference between the maximum and min-
imum F-score will be 0.76% which is very small. 
This finding indicates that the internal and exter-
nal similarity can easily be combined and we 
don?t need to make many efforts to tune this pa-
rameter when our method is applied to other 
websites. The reason of this phenomenon is that, 
no matter how much weight (i.e., 1-  ) was as-
signed  to the internal similarity, the internal sim-
ilarity always provides a relatively good initial 
60
65
70
75
80
85
90
95
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
P
er
fo
rm
a
n
ce
(%
) 
? 
F-score Precision Recall
1221
 
Figure 3 Experiment results of our method on each website
iterative direction. In the following experiments, 
the parameter ? is set to 0.6. 
 
Figure 4 The F-scores of our method with different 
the value of ? 
The weight of pages 
The weight of the neighbor pages should also 
be considered. For example, in the most websites, 
it is very common that most of the web pages 
contain a hyperlink which points to the homep-
age of the website. While in most of the Eng-
lish/Chinese websites, almost every English page 
will link to the English homepage and each Chi-
nese page will point to Chinese homepage. The 
English and Chinese homepages are probably 
parallel, but they will be helpless to find parallel 
web pages, because they are neighbors of almost 
every page in the site. On the contrary, some-
times the parallel homepages have negative ef-
fects on finding parallel pages They will increase 
the translation similarity of two pages which are 
not indeed mutual translations. So it is necessary 
to amend the Algorithm 1.  
The weight of each page is calculated accord-
ing to its popularity: 
 ( )     
    
    ( )   
  (6) 
where ( ) indicates the weight of page  ,   is 
the number of all pages,     ( ) is the number 
of pages pointing to page   and   is a constant 
for smoothing.  
In this paper, the weights of pages are used in 
two ways: 
Weight 1: The 9th line of Algorithm 1 is 
amended by the page weight as follows: 
     ?           (   )  ( ( )   ( ))    
Weight 2: The pages with low weight are re-
moved from the input of Algorithm 1. 
The experiment results are shown in Table 2.  
Table 2 The effect of page weight 
Type No Weight Weight 1 Weight 2 
F-score (%) 92.91 92.78 92.75 
Surprisingly, no big differences are found after 
the introduction of the page weight. The side ef-
fect of popular pages is not so large in our meth-
od. In the neighbor pages of a certain page, the 
popular pages are the minority. Besides, the iter-
ative process makes our method more stable and 
robust. 
The impact of the size of bilingual lexicon 
The baseline system mainly combines the con-
tent-based similarity with structure similarity. 
86.69  
92.15  
92.42  
92.67  
92.78  
92.83  
92.91  
92.83  
92.61  
92.40  
80.20  
79
81
83
85
87
89
91
93
95
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F
-s
co
re
(%
) 
? 
1222
And two kinds of similarity measures are also 
used in our method. As Ma and Liberman (1999) 
pointed out, not all translators create translated 
pages that look like the original page which 
means that the structure similarity does not al-
ways work well. Compared to the structure simi-
larity, the content-based is more reliable and has 
wider applicability. Furthermore, the bilingual 
lexicon is the only information that relates to the 
language pairs, and other features (such as struc-
ture and link information) are all language inde-
pendent. So, it?s important to investigate the ef-
fect of lexicon size in our method. We test the 
performance of our method with different size of 
the bilingual dictionary. The experiment results 
are shown in Figure 5. In this figure, the horizon-
tal axis represents the bilingual lexicon size and 
the vertical axis represents the F-score. With the 
decline of the lexicon size, the performances of 
both the baseline method and our method are 
decreased. However, we can find that the descent 
rate of our method is smaller than that of the 
baseline. It indicates that our method does not 
need a big bilingual lexicon which is good news 
for the low-resource language pairs. 
 
Figure 5 The impact of the size of bilingual lexicon 
Error analysis  
Errors occur when the two pages are similar in 
terms of structure, content and their neighbors. 
For example, Figure 6 illustrates a typical web 
page structure. There are 5 parts in the web page: 
 ,  ,  ,   and  . Part   always contains the 
main content of this page. While part  ,  ,   and 
  always contain some hyperlinks such as ?home? 
in part   and ?About us? in part  . Links in   
and   sometimes relate to the content of the page. 
For such a kind of non-parallel page pairs, let?s 
assume that the two pages have the same struc-
ture (as shown in Figure 6). In addition, their 
content part   is very short and contains the 
same or related topics. As a result, the links in 
other 4 parts are likely to be similar. In this case, 
our method is likely to regard the two pages as 
parallel.  
M
U
B
L R
 
Figure 6 A typical web page structure 
There are about 920 errors when our system 
obtains its best performance. By carefully inves-
tigating the error page pairs, we find that more 
than 90% errors fall into the category discussed 
above. The websites used in our experiments 
mainly come from Hong Kong government web-
sites. Some government departments regularly 
publish quarterly or monthly work reports on one 
issue through their websites. These reports look 
very similar except the publish date and some 
data in them. The other 10% errors happen be-
cause of the particularity of the web pages, e.g. 
very short pages, broken pages and so on. 
5 Conclusions and Future Work 
Parallel corpora are valuable resources for a lot 
of NLP research problems and applications, such 
as MT and CLIR. This paper introduces an effi-
cient and effective solution to bilingual language 
processing. We first explore how to extract paral-
lel page pairs in bilingual websites with link in-
formation between web pages. Firstly, we hy-
pothesize that the translation similarity of pages 
should be based on both internal and external 
translation similarity. Secondly, a novel iterative 
method is proposed to verify parallel page pairs. 
Experimental results show that our method is 
much more effective than the baseline system 
with 6.2% improvement on F-Score. Further-
more, our method has some significant contribu-
tions. For example, compared to previous work, 
our method does not depend on bilingual lexi-
cons, and the parameters in our method have lit-
tle effect on the final performance. These fea-
tures improve the applicability of our method. 
In the future work, we will study some method 
on extracting parallel resource from existing par-
allel page pairs, which are challenging tasks due 
to the diversity of page structures and styles. Be-
sides, we will evaluate the effectiveness of our 
mined data on MT or other applications. 
78
80
82
84
86
88
90
92
94
64K 32K 16K 8K 4K 2K 1K
F
-s
co
re
 (
%
) 
Lexicon Size 
Baseline Our Method
1223
Acknowledgments 
This research work has been sponsored by Na-
tional Natural Science Foundation of China 
(Grants No.61373097 and No.61272259), one 
National Natural Science Foundation of Jiangsu 
Province (Grants No.BK2011282), one Major 
Project of College Natural Science Foundation of 
Jiangsu Province (Grants No.11KJA520003) and 
one National Science Foundation of Suzhou City 
(Grants No.SH201212).  
The corresponding author of this paper, ac-
cording to the meaning given to this role by 
School of computer science and technology at 
Soochow University, is Yu Hong 
Reference 
Chen, Jiang and Jianyun Nie. 2000. Automatic con-
struction of parallel English-Chinese corpus for 
cross-language information retrieval. Proceedings 
of the sixth conference on Applied Natural Lan-
guage Processing, 21?28. 
Resnik, Philip and Noah A. Smith. 2003. The Web as 
a Parallel Corpus. Meeting of the Association for 
Computational Linguistics 29(3). 349?380. 
Kit, Chunyu and Jessica Yee Ha Ng. 2007. An Intelli-
gent Web Agent to Mine Bilingual Parallel Pages 
via Automatic Discovery of URL Pairing Patterns. 
Web Intelligence and Intelligent Agent Technology 
Workshops, 526?529. 
Zhang, Ying, Ke Wu, Jianfeng Gao and Phil Vines. 
2006. Automatic Acquisition of Chinese-English 
Parallel Corpus from the Web. Joint Proceedings of 
the Association for Computational Linguistics and 
the International Conference on Computational 
Linguistics, 420?431. 
Nie, Jianyun, Michel Simard, Pierre Isabelle and 
Richard Durand. 1999. Cross-language information 
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. Proceedings of 
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information 
retrieval, 74?81. 
Ma, Xiaoyi and Mark Y. Liberman. 1999. BITS: A 
Method for Bilingual Text Search over the Web. 
Machine Translation Summit VII. 
Chen, Jisong, Rowena Chau and Chung-Hsing Yeh. 
2004. Discovering Parallel Text from the World 
Wide Web. The Australasian Workshop on Data 
Mining and Web Intelligence, vol. 32, 157?161. 
Dunedin, New Zealand. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu 
and Qingsheng Zhu. 2009. Mining Bilingual Data 
from the Web with Adaptively Learnt Patterns. 
Proceedings of the Joint Conference of the 47th 
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, vol. 2, 870?878. 
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin  
Yao and Qiaoming Zhu. 2010. A novel method for 
bilingual web page acquisition from search engine 
web records. Proceedings of the 23rd International 
Conference on Computational Linguistics: Posters, 
294?302.  
Zhao, Bing and Stephan Vogel. 2002. Adaptive Paral-
lel Sentences Mining from Web Bilingual News 
Collection. IEEE International Conference on Data 
Mining, 745?748. 
Smith, Jason R., Chris Quirk and Kristina Toutanova. 
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. Hu-
man Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 403?
411. 
Bharadwaj, Rohit G. and Vasudeva Varma. 2011. 
Language independent identification of parallel 
sentences using wikipedia. Proceedings of the 20th 
International Conference Companion on World 
Wide Web, 11?12. Hyderabad, India. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences: Computerss Science and Computa-
tional Biology. Cambridge University Press  
Shi, Lei, Cheng Niu, Ming Zhou and Jianfeng Gao. 
2006. A DOM Tree Alignment Model for Mining 
Parallel Data from the Web. Proceedings of the 
21st International Conference on Computational 
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, 489?496. 
 
 
1224
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774?1778,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Joint Learning of Chinese Words, Terms and Keywords
Ziqiang Cao
1
Sujian Li
1
Heng Ji
2
1
Key Laboratory of Computational Linguistics, Peking University, MOE, China
2
Computer Science Department, Rensselaer Polytechnic Institute, USA
{ziqiangyeah, lisujian}@pku.edu.cn jih@rpi.edu
Abstract
Previous work often used a pipelined
framework where Chinese word segmen-
tation is followed by term extraction and
keyword extraction. Such framework suf-
fers from error propagation and is un-
able to leverage information in later mod-
ules for prior components. In this paper,
we propose a four-level Dirichlet Process
based model (DP-4) to jointly learn the
word distributions from the corpus, do-
main and document levels simultaneously.
Based on the DP-4 model, a sentence-wise
Gibbs sampler is adopted to obtain proper
segmentation results. Meanwhile, terms
and keywords are acquired in the sampling
process. Experimental results have shown
the effectiveness of our method.
1 Introduction
For Chinese language which does not contain ex-
plicitly marked word boundaries, word segmenta-
tion (WS) is usually the first important step for
many Natural Language Processing (NLP) tasks
including term extraction (TE) and keyword ex-
traction (KE). Generally, Chinese terms and key-
words can be regarded as words which are repre-
sentative of one domain or one document respec-
tively. Previous work of TE and KE normally used
the pipelined approaches which first conducted
WS and then extracted important word sequences
as terms or keywords.
It is obvious that the pipelined approaches are
prone to suffer from error propagation and fail to
leverage information for word segmentation from
later stages. Here, we provide one example in the
disease domain, to demonstrate the common prob-
lems in current pipelined approaches and propose
the basic idea of our joint learning of words, terms
and keywords.
Example: @??(thrombocytopenia) (with)
{? (heparinoid) 	(have) s?(relation).
This is a correctly segmented Chinese sen-
tence. The document containing the example sen-
tence mainly talks about the property of ?{?
 (heparinoid)? which can be regarded as one key-
word of the document. At the same time, the
word@??(thrombocytopenia) appears fre-
quently in the disease domain and can be treated
as a domain-specific term.
However, for such a simple sentence, current
segmentation tools perform poorly. The segmen-
tation result with the state-of-the-art Conditional
Random Fields (CRFs) approach (Zhao et al.,
2006) is as follows:
@(blood platelet) ?(reduction) ?(symptom)
{(of same kind) ?(liver) 	(always)s?(relation)
where @?? is segmented into three com-
mon Chinese words and {? is mixed with its
neighbors.
In a text processing pipeline of WS, TE and
KE, it is obvious that imprecise WS results will
make the overall system performance unsatisfy-
ing. At the same time, we can hardly make use of
domain-level and document-level information col-
lected in TE and KE to promote the performance
of WS. Thus, one question comes to our minds:
can words, terms and keywords be jointly learned
with consideration of all the information from the
corpus, domain, and document levels?
Recently, the hierarchical Dirichlet process
(HDP) model has been used as a smoothed bigram
model to conduct word segmentation (Goldwater
et al., 2006; Goldwater et al., 2009). Meanwhile,
one strong point of the HDP based models is that
they can model the diversity and commonality in
multiple correlated corpora (Ren et al., 2008; Xu
et al., 2008; Zhang et al., 2010; Li et al., 2012;
Chang et al., 2014). Inspired by such existing
work, we propose a four-level DP based model,
1774
0G1GwH mwH imjw imwH1mwH NmmwH3? ?????jmN2?1?0? M| |V
Figure 1: DP-4 Model
named DP-4, to adapt to three levels: corpus, do-
main and document. In our model, various DPs
are designed to reflect the smoothed word distri-
butions in the whole corpus, different domains and
different documents. Same as the DP based seg-
mentation models, our model can be easily used
as a semi-supervised framework, through exerting
on the corpus level the word distributions learned
from the available segmentation results. Refer-
ring to the work of Mochihashi et al. (2009), we
conduct word segmentation using a sentence-wise
Gibbs sampler, which combines the Gibbs sam-
pling techniques with the dynamic programming
strategy. During the sampling process, the impor-
tance values of segmented words are measured in
domains and documents respectively, and words,
terms and keywords are jointly learned.
2 DP-4 Model
Goldwater et al. (2006) applied the HDP model on
the word segmentation task. In essence, Goldwa-
ter?s model can be viewed as a bigram language
model with a unigram back-off. With the lan-
guage model, word segmentation is implemented
by a character-based Gibbs sampler which repeat-
edly samples the possible word boundary posi-
tions between two neighboring words, conditioned
on the current values of all other words. How-
ever, Goldwater?s model can be deemed as mod-
eling the whole corpus only, and does not distin-
guish between domains and documents. To jointly
learn the word information from the corpus, do-
main and document levels, we extend Goldwater?s
model by adding two levels (domain level and doc-
ument level) of DPs, as illustrated in Figure 1.
2.1 Model Description
M DPs (H
m
w
;1 ? m ? M ) are designed specif-
ically to word w to model the bigram distribu-
tions in each domain and these DPs share an
overall base measure H
w
, which is drawn from
DP (?
0
, G
1
) and gives the bigram distribution for
the whole corpus. Assuming the m
th
domain in-
cludes N
m
documents, we use H
m
j
w
(1 ? j ?
N
m
) to model the bigram distribution of the i
th
document in the domain. Usually, given a do-
main, the bigram distributions of different docu-
ments are not conditionally independent and simi-
lar documents exhibit similar bigram distributions.
Thus, the bigram distribution of one document is
generated according to both the bigram distribu-
tion of the domain and the bigram distributions
of other documents in the same domain. That is,
H
m
j
w
? g(?
3
, H
m
w
, H
m
?j
w
) where H
m
?j
w
repre-
sents the bigram distributions of the documents in
the m
th
domain except the j
th
document. Assum-
ing the j
th
document in the m
th
domain contains
N
j
m
words, each word is drawn according toH
m
j
w
.
That is, w
m
j
i
? H
m
j
w
(1 ? i ? N
j
m
). Thus, our
four-level DP model can be summarized formally
as follows:
G
1
? DP (?
0
, G
0
) ;H
w
? DP (?
1
, G
1
)
H
m
w
? DP (?
2
, H
w
) ;H
m
j
w
? g
(
?
3
, H
m
w
, H
m
?j
w
)
w
m
j
i
|w
i?1
= w ? H
d
w
Here, we provide for our model the Chinese
Restaurant Process (CRP) metaphor, which can
create a partition of items into groups. In our
model, the word type of the previous word w
i?1
corresponds to a restaurant and the current word
w
i
corresponds to a customer. Each domain is
analogous to a floor in a restaurant and a room de-
notes a document. Now, we can see that there are
|V | restaurants and each restaurant consists of M
floors. Them
th
floor containsN
m
rooms and each
room has an infinite number of tables with infinite
seating capacity. Customers enter a specific room
on a specific floor of one restaurant and seat them-
selves at a table with the label of a word type. Dif-
ferent from the standard HDP, each customer sits
at an occupied table with probability proportional
to both the numbers of customers already seated
there and the numbers of customers with the same
word type seated in the neighboring rooms, and at
an unoccupied table with probability proportional
to both the constant ?
3
and the probability that the
1775
customers with the same word type are seated on
the same floor.
2.2 Model Inference
It is important to build an accurate G
0
which de-
termines the prior word distribution p
0
(w). Sim-
ilar to the work of Mochihashi et al. (2009), we
consider the dependence between characters and
calculate the prior distribution of a word w
i
using
the string frequency statistics (Krug, 1998):
p
0
(w
i
) =
n
s
(w
i
)
?
n
s
(.)
(1)
where n
s
(w
i
) counts the character string com-
posed of w
i
and the symbol ?.? represents any
word in the vocabulary V .
Then, with the CRP metaphor, we can obtain the
expected word unigram and bigram distributions
on the corpus level according to G
1
and H
w
:
p
1
(w
i
) =
n (w
i
) + ?
0
p
0
(w
i
)
?
n (.) + ?
0
(2)
p
2
(w
i
|w
i?1
= w) =
n
w
(w
i
) + ?
1
p
1
(w
i
)
?
n
w
(.) + ?
1
(3)
where the subscript numbers indicate the corre-
sponding DP levels. n(w
i
) denotes the number of
w
i
and n
w
(w
i
) denotes the number of the bigram
< w,w
i
> occurring in the corpus. Next, we can
easily get the bigram distribution on the domain
level by extending to the third DP.
p
m
3
(w
i
|w
i?1
= w) =
n
m
w
(w
i
) + ?
2
p
2
(w
i
|w
i?1
)
?
n
m
w
(.) + ?
2
(4)
where n
m
w
(w
i
) is the number of the bigram <
w,w
i
> occurring in the m
th
domain.
To model the bigram distributions on the docu-
ment level, it is beneficial to consider the influence
of related documents in the same domain (Wan
and Xiao, 2008). Here, we only consider the in-
fluence from theK most similar documents with a
simple similarity metric s(d
1
, d
2
) which calculates
the Chinese character overlap ratio of two docu-
ments d
1
and d
2
. Let d
j
m
denote the j
th
document
in the m
th
domain and d
j
m
[k](1 ? k ? K) the K
most similar documents. d
j
m
can be deemed to be
?lengthened? by d
j
m
[k](1 ? k ? K). Therefore,
we estimate the count of w
i
in d
j
m
as:
t
d
j
m
w
(w
i
) = n
d
j
m
w
(w
i
)+
?
k
s(d
j
m
[k], d
j
m
)n
d
j
m
[k]
w
(w
i
)
(5)
where n
d
j
m
[k]
w
(w
i
) denotes the count of the bigram
< w,w
i
> occurring in d
j
m
[k]. Next, we model
the bigram distribution in d
j
m
as a DP with the base
measure H
m
w
:
p
d
j
m
4
(w
i
|w
i?1
= w) =
t
d
j
m
w
(w
i
) + ?
3
p
m
3
(w
i
|w
i?1
)
?
t
d
j
m
w
(.) + ?
3
(6)
With CRP, we can also easily estimate the un-
igram probabilities p
m
3
(w
i
) and p
d
j
m
4
(w
i
) respec-
tively on the domain and document levels, through
combining all the restaurants.
To measure whether a word is eligible to be a
term, the score function TH
m
(?) is defined as:
TH
m
(w
i
) =
p
m
3
(w
i
)
p
1
(w
i
)
(7)
This equation is inspired by the work of Nazar
(2011), which extracts terms with consideration of
both the frequency in the domain corpus and the
frequency in the general reference corpus. Similar
to Eq. 7, we define the functionKH
d
j
m
(?) to judge
whether w
i
is an appropriate keyword.
KH
d
j
m
(w
i
) =
p
d
j
m
4
(w
i
)
p
1
(w
i
)
(8)
During each sampling, we make use of Eqs. (7)
and (8) to identify the most possible terms and
keywords. Once a word is identified as a term
or keyword, it will drop out of the sampling pro-
cess in the following iterations. Its CRP explana-
tion is that some customers (terms and keywords)
find their proper tables and keep sitting there after-
wards.
2.3 Sentence-wise Gibbs Sampler
The character-based Gibbs sampler for word seg-
mentation (Goldwater et al., 2006) is extremely
slow to converge, since there exists high correla-
tion between neighboring words. Here, we intro-
duce the sentence-wise Gibbs sampling technique
as well as efficient dynamic programming strat-
egy proposed by Mochihashi et al. (2009). The
basic idea is that we randomly select a sentence
in each sampling process and use the Viterbi al-
gorithm (Viterbi, 1967) to find the optimal seg-
mentation results according to the word distribu-
tions derived from other sentences. Different from
Mochihashi?s work, once terms or keywords are
1776
identified, we do not consider them in the segmen-
tation process. Due to space limitation, the algo-
rithm is not detailed here and can be referred in
(Mochihashi et al., 2009).
3 Experiment
3.1 Data and Setting
It is indeed difficult to find a standard evaluation
corpus for our joint tasks, especially in different
domains. As a result, we spent a lot of time to col-
lect and annotate a new corpus
1
composed of ten
domains (including Physics, Computer, Agricul-
ture, Sports, Disease, Environment, History, Art,
Politics and Economy) and each domain is com-
posed of 200 documents. On average each doc-
ument consists of about 4800 Chinese characters.
For these 2000 documents, three annotators have
manually checked the segmented words, terms and
keywords as the gold standard results for evalu-
ation. As we know, there exists a large amount
of manually-checked segmented text for the gen-
eral domain, which can be used as the training data
for further segmentation. As with other nonpara-
metric Bayesian models (Goldwater et al., 2006;
Mochihashi et al., 2009), our DP-4 model can be
easily amenable to semi-supervised learning by
imposing the word distributions of the segmented
text on the corpus level. The news texts pro-
vided by Peking University (named PKU corpus)
2
is used as the training data. This corpus contains
about 1,870,000 Chinese characters and has been
manually segmented into words.
In our experiments, the concentration coeffi-
cient (?
0
) is finally set to 20 and the other three
(?
1?3
) are set to 15. The parameter K which con-
trols the number of similar documents is set to 3.
3.2 Performance Evaluation
The following baselines are implemented for com-
parison of segmentation results: (1) Forward max-
imum matching (FMM) algorithm with a vocab-
ulary compiled from the PKU corpus; (2) Re-
verse maximum matching (RMM) algorithm with
the compiled vocabulary; (3) Conditional Random
Fields (CRFs)
3
based supervised algorithm trained
from the PKU corpus; (4) HDP based semi-
supervised algorithm (Goldwater et al., 2006) us-
1
Nine domains are from http://www.datatang.
com/data/44139 and we add an extra Disease domain.
2
http://icl.pku.edu.cn
3
We adopt CRF++(http://crfpp.googlecode.
com/svn/trunk/doc/index.html)
ing the PKU corpus. The strength of Mochi-
hashi et al. (2009)?s NPYLM based segmentation
model is its speed due to the sentence-wise sam-
pling technique, and its performance is similar to
Goldwater et al. (2006)?s model. Thus, we do not
consider the NPYLM based model for compari-
son here. Then, the segmentation results of FMM,
RMM, CRF, and HDP methods are used respec-
tively for further extracting terms and keywords.
We use the mutual information to identify the can-
didate terms or keywords composed of more than
two segmented words. As for DP-4, this recogni-
tion process has been done implicitly during sam-
pling. To measure the candidate terms or key-
words, we refer to the metric in Nazar (2011) to
calculate their importance in some specific domain
or document.
The metrics of F
1
and the out-of-vocabulary
Recall (OOV-R) are used to evaluate the segmenta-
tion results, referring to the gold standard results.
The second and third columns of Table 1 show the
F
1
and OOV-R scores averaged on the 10 domains
for all the compared methods. Our method sig-
nificantly outperforms FMM, RMM and HDP ac-
cording to t-test (p-value ? 0.05). From the seg-
mentation results, we can see that the FMM and
RMM methods are highly dependent on the com-
piled vocabulary and their identified OOV words
are mainly the ones composed of a single Chinese
character. The HDP method is heavily influenced
by the segmented text, but it also exhibits the abil-
ity of learning new words. Our method only shows
a slight advantage over the CRF approach. We
check our segmentation results and find that the
performance of the DP-4 model is depressed by
the identified terms and keywords which may be
composed of more than two words in the gold
standard results, because the DP-4 model always
treats the term or keyword as a single word. For
example, in the gold standard, ??W?((Lingnan
Culture)? is segmented into two words ??W? and
???, ?pn??(data interface)? is segmented
into ?pn? and ???? and so on. In fact, our seg-
mentation results correctly treat ??W?? and ?p
n??? as words.
To evaluate the TE and KE performance, the top
50 (TE-50) and 100 (TE-100) accuracy are mea-
sured for the identified terms of one domain, while
the top 5 (KE-5) and 10 (KE-10) accuracy for the
keywords in one document, are shown in the right
four columns of Table 1. We can see that DP-
1777
4 performs significantly better than all the other
methods in TE and KE results.
As for the ten domains, we find our approach
behaves much better than the other approaches on
the following three domains: Disease, Physics and
Computer. It is because the language of these
three domains is much different from that of the
general domain (PKU corpus), while the rest do-
mains are more similar to the general domain.
Method F1 OOV-R TE-50 TE-100 KE-5 KE-10
FMM 0.796 0.136 0.420 0.360 0.476 0.413
RMM 0.794 0.136 0.424 0.352 0.478 0.414
HDP 0.808 0.356 0.672 0.592 0.552 0.506
CRF 0.817 0.330 0.624 0.560 0.543 0.511
DP-4 0.821 0.374 0.704 0.640 0.571 0.545
Table 1: Comparison of WS, TE and KE Perfor-
mance (averaged on the 10 domains).
4 Conclusion
This paper proposes a four-level DP based model
to construct the word distributions from the cor-
pus, domain and document levels simultaneously,
through which Chinese words, terms and key-
words can be learned jointly and effectively. In
the future, we plan to explore how to combine
more features such as part-of-speech tags into our
model.
Acknowledgments
We thank the three anonymous reviewers for
their helpful comments. This work was par-
tially supported by National High Technology Re-
search and Development Program of China (No.
2012AA011101), National Key Basic Research
Program of China (No. 2014CB340504), Na-
tional Natural Science Foundation of China (No.
61273278), and National Key Technology R&D
Program (No: 2011BAH10B04-03). The contact
author of this paper, according to the meaning
given to this role by Peking University, is Sujian
Li.
References
Baobao Chang, Wenzhe Pei, and Miaohong Chen.
2014. Inducing word sense with automatically
learned hidden concepts. In Proceedings of COL-
ING 2014, pages 355?364, Dublin, Ireland, August.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 673?
680.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Manfred Krug. 1998. String frequency: A cognitive
motivating factor in coalescence, language process-
ing, and linguistic change. Journal of English Lin-
guistics, 26(4):286?320.
Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao
Chang. 2012. Update summarization using a multi-
level hierarchical dirichlet process model. In Pro-
ceedings of Coling 2012, pages 1603?1618, Mum-
bai, India.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100?108.
Rogelio Nazar. 2011. A statistical approach to term
extraction. IJES, International Journal of English
Studies, 11(2):159?182.
Lu Ren, David B. Dunson, and Lawrence Carin. 2008.
The dynamic hierarchical dirichlet process. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 824?831.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, pages 260?269.
Xiaojun Wan and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In AAAI, volume 8, pages 855?860.
Tianbing Xu, Zhongfei Zhang, Philip S. Yu, and
Bo Long. 2008. Dirichlet process based evolution-
ary clustering. In ICDM?08, pages 648?657.
Jianwen Zhang, Yangqiu Song, Changshui Zhang, and
Shixia Liu. 2010. Evolutionary hierarchical dirich-
let processes for multiple correlated time-varying
corpora. In Proceedings of the 16th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 1079?1088, New York, NY,
USA.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, volume 1082117.
1778
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846?1851,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Constructing Information Networks Using One Single Model
Qi Li
?
Heng Ji
?
Yu Hong
??
Sujian Li
?
?
Computer Science Department, Rensselaer Polytechnic Institute, USA
?
School of Computer Science and Technology, Soochow University, China
?
Key Laboratory of Computational Linguistics, Peking University, MOE, China
?
{liq7,hongy2,jih}@rpi.edu,
?
lisujian@pku.edu.cn
Abstract
In this paper, we propose a new frame-
work that unifies the output of three infor-
mation extraction (IE) tasks - entity men-
tions, relations and events as an informa-
tion network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other. For example, many rela-
tions can now be considered as the re-
sultant states of events. Our approach
achieves substantial improvements over
traditional pipelined approaches, and sig-
nificantly advances state-of-the-art end-to-
end event argument extraction.
1 Introduction
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely inter-
dependent: entity mentions are core components
of relations and events, and the extraction of rela-
tions and events can help to accurately recognize
entity mentions. In addition, the theory of eventu-
alities (D?olling, 2011) suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to ex-
tract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks (e.g., (Roth and Yih, 2007;
Riedel and McCallum, 2011; Yang and Cardie,
2013; Riedel et al., 2009; Singh et al., 2013; Li et
al., 2013; Li and Ji, 2014)). For example, Roth and
Yih (2007) conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments (Li et al.,
2013), and entity mentions and relations (Li and
Ji, 2014). However, a single model that can ex-
tract all of them has never been studied so far.
Asif Mohammed Hanif detonated explosives in Tel Aviv
AttackPerson Weapon Geopolitical Entity
Place
InstrumentAttacker
Agent-Artifact
Physical
x1 x2 x3 x4 x5 x6 x7 x8x:
y:
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Ar-
rows represent information arcs.
For the first time, we uniformly represent the IE
output from each sentence as an information net-
work, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to con-
struct the information networks (Collins, 2002; Li
et al., 2013; Li and Ji, 2014). In addition to the per-
ceptron update, we also apply k-best MIRA (Mc-
Donald et al., 2005), which refines the perceptron
update in three aspects: it is flexible in using var-
ious loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in (Li et
al., 2013; Li and Ji, 2014), we are able to cap-
ture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention ?Asif Mohammed Hanif ? is an Attacker
of the Attack event triggered by ?detonated?, and
the Weapon mention ?explosives? is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them. Similarly we can infer
the Physical relation between ?Asif Mohammed
Hanif ? and ?Tel Aviv?.
However, in practice many useful interactions
are missing during testing because of the data spar-
1846
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE?05
1
training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet (Baker and
Sato, 2003) to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the de-
tailed implementation of our new framework.
2 Approach
We uniformly represent the IE output from each
sentence as an information network y = (V,E).
Each node v
i
? V is represented as a triple
?u
i
, v
i
, t
i
? of start index u
i
, end index v
i
, and node
type t
i
. A node can be an entity mention or an
event trigger. A particular type of node is ? (nei-
ther entity mention nor event trigger), whose max-
imal length is always 1. Similarly, each infor-
mation arc e
j
? E is represented as ?u
j
, v
j
, r
j
?,
where u
j
and v
j
are the end offsets of the nodes,
and r
j
is the arc type. For instance, in Fig-
ure 1, the event trigger ?detonated? is represented
as ?4, 4, Attack?, the entity mention ?Asif Mo-
hammed Hanif ? is represented as ?1, 3, Person?,
and their argument arc is ?4, 3, Attacker?. Our
goal is to extract the whole information network y
for a given sentence x.
2.1 Decoding Algorithm
Our joint decoding algorithm is based on ex-
tending the segment-based algorithm described in
our previous work (Li and Ji, 2014). Let x =
(x
1
, ..., x
m
) be the input sentence. The decoder
performs two types of actions at each token x
i
from left to right:
? NODEACTION(i, j): appends a new node
?j, i, t? ending at the i-th token, where i? d
t
<
j ? i, and d
t
is the maximal length of type-t
nodes in training data.
? ARCACTION(i, j): for each j < i, incremen-
tally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: ?i, j, r?.
After each action, the top-k hypotheses are se-
lected according to their features f(x, y
?
) and
1
http://www.itl.nist.gov/iad/mig//tests/ace
weights w:
best
k
y
?
?buffer
f(x, y
?
) ?w
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type con-
straints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the final
prediction. The upper-bound time complexity of
the decoding algorithm is O(d ? b ? m
2
), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
2.2 Parameter Estimation
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
z = argmax
y
?
?Y(x)
f(x, y
?
) ?w
and then updates the feature weights by using:
w
new
= w + f(x, y)? f(x, z)
We relax the exact inference problem by the afore-
mentioned beam-search procedure. The stan-
dard perceptron will cause invalid updates be-
cause of inexact search. Therefore we apply early-
update (Collins and Roark, 2004), an instance of
violation-fixing methods (Huang et al., 2012). In
the rest of this paper, we override y and z to denote
prefixes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA (McDonald et al., 2005),
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formu-
lated as a quadratic programming problem:
min ?w
new
?w?
s.t. w
new
f(x, y)?w
new
f(x, z) ? L(y, z)
?z ? best
k
(x,w)
We employ the following three loss functions
for comparison:
1847
Freq. Relation Type Event Type Arg-1 Arg-2 Example
159 Physical Transport Artifact Destination He
(arg-1)
was escorted
(trigger)
into Iraq
(arg-2)
.
46 Physical Attack Target Place Many people
(arg-1)
were in the cafe
(arg-2)
during the blast
(trigger)
.
42 Agent-Artifact Attack Attacker Instrument Terrorists
(arg-1)
might use
(trigger)
the devices
(arg-2)
as weapons.
41 Physical Transport Artifact Origin The truck
(arg-1)
was carrying
(trigger)
Syrians fleeing the war in Iraq
(arg-2)
.
33 Physical Meet Entity Place They
(arg-1)
have reunited
(trigger)
with their friends in Norfolk
(arg-2)
.
32 Physical Die Victim Place Two Marines
(arg-1)
were killed
(trigger)
in the fighting in Kut
(arg-2)
.
28 Physical Attack Attacker Place Protesters
(arg-1)
have been clashing
(trigger)
with police in Tehran
(arg-2)
.
26 ORG-Affiliation End-Position Person Entity NBC
(arg-2)
is terminating
(trigger)
freelance reporter Peter Arnett
(arg-1)
.
Table 1: Frequent overlapping relation and event types in the training set.
? The first one is F
1
loss:
L
1
(y, z) = 1?
2 ? |y ? z|
|y|+ |z|
When counting the numbers, we treat each node
and arc as a single unit. For example, in Fig-
ure 1, |y| = 6.
? The second one is 0-1 loss:
L
2
(y, z) =
{
1 y 6= z
0 y = z
It does not discriminate the extent to which z
deviates from y.
? The third loss function counts the difference be-
tween y and z:
L
3
(y, z) = |y|+ |z| ? 2 ? |y ? z|
Similar to F
1
loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
2.3 Joint Relation-Event Features
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized fea-
tures in pipelined approaches. In addition to the
features as described in (Li et al., 2013; Li and
Ji, 2014), we can make use of joint features be-
tween relations and events, given the fact that
relations are often ending or starting states of
events (D?olling, 2011). Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y
?
dur-
ing the search, if both arguments of a relation par-
ticipate in an event, we compose the correspond-
ing argument roles and relation type as a joint fea-
ture for y
?
. For example, for the structure in Fig-
ure 1, we obtain the following joint relation-event
features:
Attacker Instrument
Agent-Artifact
Attacker Place
Physical
Split Sentences Mentions Relations Triggers Arguments
Train 7.2k 25.7k 4.8k 2.8k 4.5k
Dev 1.7k 6.3k 1.2k 0.7k 1.1k
Test 1.5k 5.3k 1.1k 0.6k 1.0k
Table 2: Data set
0 20 40 60 80 100Number of instances0
2
4
6
8
10
12
14
Freq
uenc
y
Trigger WordsFrame IDs
Figure 2: Distribution of triggers and their frames.
2.4 Semantic Frame Features
One major challenge of constructing information
networks is the data sparsity problem in extract-
ing event triggers. For instance, in the sen-
tence: ?Others were mutilated beyond recogni-
tion.? The Injure trigger ?mutilated? does not oc-
cur in our training data. But there are some sim-
ilar words such as ?stab? and ?smash?. We uti-
lize FrameNet (Baker and Sato, 2003) to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a ba-
sic type of semantic concept, and contains a num-
ber of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame ?Cause harm? is closely
related with Injure event and contains 68 lexical
units such as ?stab?, ?smash? and ?mutilate?.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
1848
Methods
Entity Mention (%)
Relation (%)
Event Trigger (%)
Event Argument (%)
P R F
1
P R F
1
P R F
1
P R F
1
Pipelined Baseline
83.6 75.7 79.5
68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7
Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1
Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A
Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6
Joint w/ MIRA w/ F
1
Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4
Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9
Joint w/ MIRA w/ L
3
Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8
Table 3: Overall performance on test set.
once in the training data from 100 to 60 and al-
leviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, ?Cause harm? will be a feature for ?mu-
tilated?. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
3 Experiments
3.1 Data and Evaluation
We use ACE?05 corpus to evaluate our method
with the same data split as in (Li and Ji, 2014). Ta-
ble 2 summarizes the statistics of the data set. We
report the performance of extracting entity men-
tions, relations, event triggers and arguments sep-
arately using the standard F
1
measures as defined
in (Ji and Grishman, 2008; Chan and Roth, 2011):
? An entity mention is correct if its entity type (7
in total) and head offsets are correct.
? A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
? An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
? An argument link is correct if its event subtype,
offsets and role match those of any of the refer-
ence argument mentions.
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by hand-
crafted patterns (Chang and Manning, 2012).
3.2 Results
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall perfor-
mance of our approaches and baseline methods.
Our joint model with perceptron update out-
performs the state-of-the-art pipelined approach
in (Li et al., 2013; Li and Ji, 2014), and further
improves the joint event extraction system in (Li
et al., 2013) (p < 0.05 for entity mention extrac-
tion, and p < 0.01 for other subtasks, accord-
ing to Wilcoxon Signed RankTest). For the k-
best MIRA update, the L
3
loss function achieved
better performance than F
1
loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also significantly outperforms perceptron update
on relation extraction and event argument extrac-
tion (p < 0.01). It is particularly encouraging to
see the end output of an IE system (event argu-
ments) has made significant progress (12.2% ab-
solute gain over traditional pipelined approach).
3.3 Discussions
3.3.1 Feature Study
Rank Feature Weight
1 Frame=Killing Die 0.80
2 Frame=Travel Transport 0.61
3 Physical(Artifact, Destination) 0.60
4 w
1
=?home? Transport 0.59
5 Frame=Arriving Transport 0.54
6 ORG-AFF(Person, Entity) 0.48
7 Lemma=charge Charge-Indict 0.45
8 Lemma=birth Be-Born 0.44
9 Physical(Artifact,Origin) 0.44
10 Frame=Cause harm Injure 0.43
Table 4: Top Features about Event Triggers.
Table 4 lists the weights of the most significant
features about event triggers. The 3
rd
, 6
th
, and
9
th
rows are joint relation-event features. For in-
stance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
1849
and FrameNet based features are of vital impor-
tance to event trigger labeling. We tested the im-
pact of each type of features by excluding them in
the experiments of ?MIRA w/ L
3
loss?. We found
that FrameNet based features provided 0.8% and
2.2% F
1
gains for event trigger and argument la-
beling respectively. Joint relation-event features
also provided 0.6% F
1
gain for relation extraction.
3.3.2 Remaining Challenges
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remain-
ing errors suggest to incorporate external world
knowledge. For example, some words act as trig-
gers for some certain types of events only when
they appear together with some particular argu-
ments:
? ?Williams picked up the child again and this
time, threw
Attack
her out the window.?
The word ?threw? is used as an Attack event
trigger because the Victim argument is a ?child?.
? ?Ellison to spend $10.3 billion to get
Merge Org
his company.? The common word ?get? is
tagged as a trigger of Merge Org, because its
object is ?company?.
? ?We believe that the likelihood of them
using
Attack
those weapons goes up.?
The word ?using? is used as an Attack event
trigger because the Instrument argument is
?weapons?.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
? ?we are paying great attention to their ability to
defend
Attack
on the ground.?,
our system fails to extract ?defend? as an Attack
trigger. In the training data, ?defend? appears mul-
tiple times, but none of them is tagged as Attack.
For instance, in the sentence:
? ?North Korea could do everything to defend it-
self. ?
?defend? is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to ac-
tual events. Event coreference is necessary to rec-
ognize them correctly. For example, in the follow-
ing two sentences from the same document:
? ?It?s important that people all over the world
know that we don?t believe in the war
Attack
.?,
? ?Nobody questions whether this
Attack
is right
or not.?
?this? refers to ?war? in its preceding contexts.
Without event coreference resolution, it is difficult
to tag it as an Attack event trigger.
4 Conclusions
We presented the first joint model that effectively
extracts entity mentions, relations and events
based on a unified representation: information
networks. Experiment results on ACE?05 cor-
pus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance significantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger la-
beling.
Although our primary focus in this paper is in-
formation extraction in the ACE paradigm, we be-
lieve that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.
References
Collin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proc. ACL, pages 161?164.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551?560.
1850
Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proc. LREC, pages 3735?3740.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
Johannes D?olling. 2011. Aspectual coercion and even-
tuality structure. pages 189?226.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73?82.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. ACL, pages 91?98.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proc. EMNLP.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic ap-
proach to bio-molecular event extraction. In Proc.
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing: Shared Task.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640?1649.
1851
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285?288,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Utility Evaluation of Cross-document Information Extraction 

Heng Ji
a
, Zheng Chen
a
, Jonathan Feldman
a
, Antonio Gonzalez
a
, Ralph Grishman
b
, Vivek Upadhyay
a 
a
Computer Science Department, Queens College and the Graduate Center, City University of New York 
New York, NY 11367, USA 
b
Computer Science Department, New York University, New York, NY 10003, USA 
hengji@cs.qc.cuny.edu, zchen1@gc.cuny.edu, agonzalez117@qc.cuny.edu, grishman@cs.nyu.edu, 
vivekqc@gmail.com 
 
  
 
Abstract 
We describe a utility evaluation to determine 
whether cross-document information extrac-
tion (IE) techniques measurably improve user 
performance in news summary writing. Two 
groups of subjects were asked to perform the 
same time-restricted summary writing tasks, 
reading news under different conditions: with 
no IE results at all, with traditional single-
document IE results, and with cross-document 
IE results. Our results show that, in compari-
son to using source documents only, the qual-
ity of summary reports assembled using IE 
results, especially from cross-document IE, 
was significantly better and user satisfaction 
was higher. We also compare the impact of 
different user groups on the results.  
1 Introduction 
Information Extraction (IE) is a task of identifying 
??facts?? (entities, relations and events) within un-
structured documents, and converting them into 
structured representations (e.g., databases). IE 
techniques have been effectively applied to differ-
ent domains (e.g. daily news, Wikipedia, biomedi-
cal reports, financial analysis and legal 
documentations) and different languages. Recently 
we described a new cross-document IE task (Ji et 
al., 2009) to extract events across-documents and 
track them on a time line.  Compared to traditional 
single-document IE, this new task can extract more 
salient, accurate and concise event information. 
However, a significant question remains: will 
the events extracted by IE, especially this new 
cross-document IE task, actually help end-users to 
make better use of the large volumes of news? In 
order to investigate whether we have reached this 
goal, we performed an extrinsic utility (i.e., use-
fulness) and usability evaluation on IE results. 
Two groups of subjects were asked to perform the 
same time-restricted summary writing tasks, read-
ing news under different conditions: with no IE 
results at all, with traditional single-document IE 
results, and with cross-document IE results. Our 
results show that, in comparison to using source 
documents only, the quality of summary reports 
assembled using IE techniques, especially from 
cross-document IE, was significantly better. Also, 
as extraction quality increases from no IE at all to 
single-document IE and then to cross-document IE, 
user satisfaction increases. We also compare the 
impact of different user groups on the results. To 
the best of our knowledge, this is the first system-
atic evaluation of cross-document IE from a us-
ability perspective.  
2 Overview of IE Systems 
We applied the English single-document IE system 
(Ji and Grishman, 2008) and cross-document IE 
system presented in (Ji et al, 2009). Both systems 
were developed for the ACE program1.  
The single-document IE system can extract 
events from individual documents. The core stages 
include entity extraction, time expression extrac-
tion and normalization, relation extraction and 
event extraction. Events include the 33 distinct 
types defined in ACE05. The extraction results are 
presented in tabular form.  
The cross-document IE system can identify im-
portant person entities which are frequently in-
                                                          
1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 
285
volved in events as ??centroid entities??; and then for 
each centroid entity, link and order the events cen-
tered around it on a time line and associate them to 
a geographical map. The event chains are pre-
sented in a user-friendly graphical interface (Ji and 
Chen, 2009). Both systems link the events back to 
their context documents.  
3 Evaluation Methods 
3.1 Study Execution 
Our measurement challenge is to assess how IE 
techniques affect users?? abilities to perform real-
world tasks. We followed the summary writing 
task described in the Integrated Feasibility Ex-
periment of the DARPA TIDES program (Colbath 
and Kubala, 2003) and the daily task conducted by 
intelligence analysts (Bodnar, 2003). Each task in 
our evaluation is based on writing a summary of 
ACE-type events involving a specific centroid en-
tity, using one of three levels of support: 
? Level (I): Read the news articles, with assistance 
of keyword based sentence search; 
? Level (II): (I) + with assistance from single-
document IE results; 
? Level (III): (I) + with assistance from cross-
document IE results. 
The summary writing task for each entity using 
any level should be finished in 10 minutes. The 
users can choose to trust the IE results to create 
new sentences or select relevant sentences from 
the source documents. The IE systems were ap-
plied to a corpus of 106 articles from ACE 2005 
training data. 
3.2 Summary Scoring 
We measure user responses in three aspects:  
? Observer-based Quantity -- How many sen-
tences are extracted in each summary? How 
many of them are uniquely correct? 
? Observer-based Quality-- How fluent and coher-
ent are the sentences in each summary?  
? User-based Usability -- How does the user feel 
about the system?  
3.3 User Group Selection 
We selected user groups based on the principles 
that we should run as many tests as we can afford 
(Nielsen, 1994), and at least 5 to insure that we 
detect any major usability problems (Faulkner, 
2003). Two different groups of users were asked to 
conduct the evaluation: 
(1) Hallway Evaluation 
We chose the first group of users with a ??Hallway 
Testing?? user-study method described in (Nielsen, 
1994). We randomly asked 11 PhD students in the 
field of natural language processing to conduct the 
evaluation. In order to evaluate these three levels 
independently, each student was asked to write at 
most one summary, using one of the three levels, 
for any single centroid entity. To avoid the impact 
of diverse text comprehension abilities, each stu-
dent was involved in all of these three levels for 
different centroid entities. 
(2) Remote Evaluation 
An effective utility evaluation will require users 
with a diversity of prior knowledge and computer 
experience. Therefore we asked the second group 
of 11 users in a remote usability testing mode 
(Hammontree et al, 1994). We sent out the request 
to university-wide undergraduate student mailing 
lists and found 11 users to work on the evaluation. 
The evaluation procedure follows the Hallway 
Testing method, except that the tests are carried 
out in the user??s own environment (rather than labs) 
helping further simulate real-life scenario testing. 
Also the users didn??t meet with the observers and 
thus they were not aware of any expectations for 
results. 
4 Evaluation Results 
In this section we will focus on reporting the re-
sults from Hallway Evaluation, while providing 
comparisons with Remote Evaluation. 
4.1 Observer-based Quantity 
The summaries were judged by two annotators and 
the judgements reconciled. A summary sentence is 
judged as uniquely correct if it: (1) includes rele-
vant events involving the centroid entity; and (2) 
the same information was not included in previous 
sentences in the current summary. This metric can 
be considered as an approximate com bination of 
the ??content responsiveness??, ??non-
redundancy??and ??focus?? criteria in the NIST TAC 
summarization track2.  Table 1 presents the  
                                                          
2http://www.nist.gov/tac/2009/Summarization/update.su
mm.09.guidelines.html 
286
Cen-
troid 
(I) (II) (III) Cen-
troid 
(I) (II) (III) Cen-
troid 
(I) (II) (III) 
Bush 3/1/0 5/1/2 6/0/0 Al-douri 4/3/3 4/2/0 6/0/1 Ba??asyir 3/1/0 3/0/0 5/0/0
Ibrahim 4/0/1 5/0/0 8/0/0 Giuliani 2/0/0 3/2/0 5/0/0 Erdogan 1/0/1 4/0/0 4/0/0
Toefting 0/0/0 7/1/0 4/0/0 Blair 2/0/1 3/0/0 5/0/0 Diller 3/0/0 4/1/0 3/0/0
Putin 2/1/0 4/3/2 7/1/1 Pasko 3/0/0 3/0/0 2/0/0 Overall 27/6/6 45/10/5 55/1/2
 
Table 1. # (uniquely correct sentences)/ #(redundant correct sentences)/ 
#(spurious sentences) in a summary in Hallway Evaluation 
 
quantified Hallway Testing results for each cen-
troid separately and the overall score. It shows that 
overall Level (II) contained 18 more correct sen-
tences than the baseline (I), while (III) achieved 11 
further correct sentences. (I) obtained significantly 
fewer sentences without assistance from IE tools. 
We conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on a query entity basis for ac-
curacy - number of (uniquely correct sen-
tences)/number of (total extracted sentences in a 
summary). The results show that (III) is signifi-
cantly better than (I) at a 99.2% confidence level, 
and better than (II) at a 96.9% confidence level. (II) 
is not significantly better than (I). 
We can also see that for some centroid entities 
such as ??Putin??, ??Al-douri?? and ??Giuliani??, (II) 
generated more sentences but also introduced more 
redundant information. The user feedback has in-
dicated that they did not have enough time to re-
move redundancy. In contrast, (III) yielded much 
less redundant information. In fact, the average 
time the users spent using (III) was only about 7.2 
minutes. Therefore we can conclude that cross-
document IE can produce more informative sum-
maries in a more efficient way. 
Error analysis showed that the major error types 
propagated from IE to summaries are as follows. 
1. Event time errors. For example, the summary 
sentence ??Toefting was convicted in September 
2001 of assaulting a pair of restaurant workers in 
the capital?? was judged as incorrect because the 
time argument should be ??October 2002??. 
2. Pronoun resolution errors. When a pronoun is 
mistakenly linked to an entity, incorrect event ar-
guments will be included in the summaries.  
3. Event type errors. When an event is mis-
classified, the users tend to use incorrect templates 
and thus generate wrong summaries. 
4. Negative events. Sometimes the event attrib-
ute classifier makes mistakes and the users include 
negative events in the summaries. 
4.2 Impact of User Groups 
In the Remote Testing, the accuracy results from 
the three levels are as follows: 21/37, 28/37 and 
31/36. Thus both user groups benefited from using 
IE techniques, but the enhancements vary a lot. In 
the Hallway Testing, the users were better trained 
and more familiar with IE tools (including the 
graphical interface of cross-document IE); and thus 
they can benefit more from the IE techniques. In 
contrast, in the Remote Evaluation, the users had 
quite diverse knowledge backgrounds. For exam-
ple, one remote user was only able to find 1-2 sen-
tences using any of the three levels; while another, 
more skilled remote user found more than 5 sen-
tences with any level. However the Remote 
Evaluation is important to gather the feedback of 
the more subjective usability evaluation in section 
4.4. Because the users in Hallway Testing may be 
aware of the observations that the observer is hop-
ing to achieve, they may provide potentially biased 
feedback. 
4.3 Observer-based Quality 
The evaluation also showed that (III) produced 
summaries with better quality. We asked the ob-
servers to give a score between [1, 10] to each 
summary according to the following TAC summa-
rization quality criteria: Readability/Fluency, Ref-
erential Clarity and Structure/Coherence. Table 2 
shows the evaluation results for the three different 
methods. 
 
Criteria (I) (II) (III) 
Readability/Fluency 9.4 8.5 8.2 
Referential Clarity 6.1 8.3 8.7 
Structure/Coherence 7.1 7.6 8.5 
 
Table 2. Observer-based Average Quality 
 
In their detailed feedback, the users indicated 
that (III) has the following advantages: (1) Better 
287
Cen-nt-r eoindt (n-Ir )Bur sneor hn3Cdo or /-1r /hht0
e/ or  o3Cne/dr ne1oer 5oh/tior )222ur6/-r eohnAoer t-0
l-n4-r  (3or /eat3o- ir ti(-ar henii01nht3o- r
(-??oeo-hoyr)bur6/-rao-oe/ or/5i e/h (Aorit33/e(oiyr
mner  8or 5(nae/C8(h/dr oAo- ir )oyayr o3CdnG3o- uEr
in3or tioeir 4oeor /5dor  nr tior iCoh(??(hr  o3Cd/ oir
ith8r/irgTf7r4/ir8(eo1r5GrD7Pr/ rk2sf.r nr4e( or
it33/e(oiyr mner o#/3CdoEr /r io- o-hor gqti8r /-1r
qd/(er3o r/ r6/3Crc/A(1r/-1r 8orpHr 8eoor (3oir(-r
s/eh8r Bwwb.r 4/ir 1oe(Ao1r ??en3r  8eoor 1(????oeo- r
g6n- /h 0soo (-a.r oAo- ir (-r  8oroAo- r h8/(-iyr )vur
6/-r hn--oh r eod/ o1r oAo- ir (- nr 3neor hn-h(ior
it33/e(oiyrmnero#/3CdoErioAoe/droAo- ir4oeorhn-0
-oh o1r nrao-oe/ or  8or??nddn4(-ario- o-hoirgT/ilnr
4/ir /CCo/do1r??ner  eo/in-rhe(3orn-rLCe(dr ,WErBwwbr
and thenreodo/io1rn-rxt-or,MErBwwb.yrk8oreo/1/5(d0
( Gr ihneoir (-rk/5dorBr /dinr (-1(h/ or  8/ r /r3neoro??0
??oh (Aor  o3Cd/ or ao-oe/ (n-r 3o 8n1r i8ntd1r 5or
1oAodnCo1r nrCen1thor3neor??dto- rit33/e(oir5/io1r
n-r2freoitd iyr
4.4 User-based Usability 
k8or tioer ??oo15/hlr ??en3r 5n 8r oA/dt/ (n-ir /dinr
i8n4o1r  8/ r )22ur /-1r )222ur eoitd ir 4oeor  eti o1r /d0
3ni roSt/ddGEr/-1r)222ur4/irhd/(3o1r nrCenA(1or 8or
3ni r tio??tdr ??t-h (n-iyr k8or Cni( (Aor hn33o- ir
/5nt r)222ur(-hdt1orgko3Cne/drR(-l(-ar/ddn4irdna(0
h/dreo/in-(-ar/-1rao-oe/d(9/ (n-.Erg6o- en(1rio/eh8r
8odCir  nr ??nhtir (33o1(/ odG.Er g%C/ (/drR(-l(-ar /d0
dn4ir  nr 5en4ior /ddr  8orCd/hoir 48(h8r /rCoein-r 8/ir
A(i( o1.Er g??/3or 1(i/35(at/ (n-r 8odCir  nr ??(d oer (e0
eodoA/- r (-??ne3/ (n-.Er g6/-r ??(-1r loGr (-??ne3/ (n-r
??en3r oAo- r h8/(-i.Er gk(3od(-or 8odCir hneeod/ or
oAo- i.Ir /-1r  8or -oa/ (Aor hn33o- ir (-hdt1or
g%n3o (3oir 2fr oeeneir 3(ido/1r dnh/ (-ar  8or io-0
 o-hoi.Erg??nritCCne rn??r-/3orC/(erio/eh8r??ner3oo 0
(-ar oAo- i.Erg??nrhndnero3C8/i(ir n??r oAo- ir n-r  8or
ne(a(-/dr 1nht3o- i.r /-1r g??nr itaaoi (n-ir n??r  o30
Cd/ oir nrhn3Cniorit33/eGrio- o-hoi.yrr
5 Conclusion and Future Work 
k8enta8r /r t (d( Gr oA/dt/ (n-r n-r it33/eGr 4e( (-ar
4or 8/Aor CenAo1r  8/ r 2fr  oh8-(StoiEr oiCoh(/ddGr
henii01nht3o- r2fErh/-r/(1r-o4ir5en4i(-aErio/eh8r
/-1r/-/dGi(iyr2-rC/e (htd/eEr o3Cne/droAo- r e/hl(-ar
/heniir 1nht3o- ir 8odCir tioeir Coe??ne3r 5o  oer / r
??/h 0a/ 8oe(-ar  8/-r  8oGr1nr4( 8nt r2fyrpioeir /dinr
Cen1tho1r3neor(-??ne3/ (Aorit33/e(oir4( 8rhenii0
1nht3o- r2fr 8/-r4( 8r e/1( (n-/dri(-ado01nht3o- r
2fyr??or/dinrhn3C/eo1r/-1r/-/dG9o1r 8or1(????oeo-hoir
5o 4oo-r  4nr tioer aentCiyr %th8r 3o/iteoir n??r  8or
5o-o??( ir  nr  8or oAo- t/dr o-1r tioeir /dinr CenA(1o1r
??oo15/hlrn-r48/ r4nelir4oddr /-1r (1o- (??(o1r /11(0
 (n-/dr eoio/eh8r Cen5do3iEr ith8r /ir  nr o#C/-1r  8or
ho- en(1r  nr /rC/(ern??ro- ( (oir /-1r  nrCenA(1orhn-??(0
1o-hor3o e(hir(-r 8or(- oe??/hoyr2-r 8or??t teor4or/(3r
 nrio rtCr/-rn-d(-or-o4ir/e (hdor/-/dGi(iriGi o3r/-1r
Coe??ne3rd/eaoer/-1reoatd/ert (d( GroA/dt/ (n-iyrr
Acknowledgement 
k8(ir 4nelr 4/ir itCCne o1r 5Gr  8or py%yr ??%mr
6L7ff7r L4/e1r t-1oer Pe/- r 22%0wjMb,vjEr  8or
py%yr Le3Gr 7oio/eh8r R/5ne/ neGr t-1oer 6nnCoe/0
 (Aor Laeoo3o- r ??t35oer ??j,,??m0wj0B0wwMbEr
PnnadoEr 2-hyEr 6p??Fr 7oio/eh8r f-8/-ho3o- r Ten0
ae/3Erm/htd GrTt5d(h/ (n-rTenae/3r/-1rP7k2rTen0
ae/3yrk8orA(o4ir/-1rhn-hdti(n-irhn- /(-o1r(-r 8(ir
1nht3o- r /eor  8niorn??r  8or /t 8neir /-1r i8ntd1r -n r
5or(- oeCeo o1r/ireoCeoio- (-ar  8orn????(h(/drCnd(h(oiEr
o( 8oero#Ceoiio1rner(3Cd(o1Ern??r 8orLe3Gr7oio/eh8r
R/5ne/ neGr ner  8or py%yr PnAoe-3o- yr k8or py%yr
PnAoe-3o- r (ir /t 8ne(9o1r  nr eoCen1thor /-1r 1(i0
 e(5t oreoCe(- ir ??nerPnAoe-3o- rCteCnioir -n 4( 80
i /-1(-ar/-GrhnCGe(a8 r-n / (n-r8oeorn-yr
References  
xn8-r ??yr qn1-/eyr Bwwbyr ??/e-(-ar L-/dGi(ir ??ner  8or 2-0
??ne3/ (n-rLaoOr7o 8(-l(-ar  8or 2- odd(ao-horTenhoiiyr
Center for Strategic Intelligence Research, Joint 
Military Intelligence CollegeEr??/i8(-a n-Ercy6yr
%o/-r 6nd5/ 8r /-1rme/-h(ir Ht5/d/yrBwwbyr kLT0NROrL-r
Lt n3/ o1r L-/dGi :ir Lii(i /- yr Proc. HLT-NAACL 
2003 (demonstrations)yr
R/te/rm/tdl-oeyrBwwbyrqoGn-1r 8or??(Ao0tioer/iit3C (n-Or
qo-o??( irn??r(-heo/io1ri/3Cdori(9oir(-rti/5(d( Gr oi (-ayr
Behavior Research Methods Instruments and Com-
putersrbM)buErb;j0b[byr
sn- Gr ]/33n- eooEr T/tdr ??o(doer /-1r ??/-1(-(r ??/G/lyr
,jjvyr 7o3n or pi/5(d( Gr koi (-ayr Interactionsyr znd0
t3or,Er2iitorbyrT/aoiOrB,0BMyr
]o-arx(r /-1rQ8o-ar68o-yrBwwjyr6enii01nht3o- rko30
Cne/dr /-1r %C/ (/dr Toein-r ke/hl(-ar %Gi o3r co3n-0
i e/ (n-yrProc. HLT-NAACL 2009yr
]o-ar x(Er 7/dC8r Pe(i83/-Er Q8o-ar 68o-r /-1r Te/i8/- r
PtC /yr Bwwjyr 6enii01nht3o- r fAo- r f# e/h (n-Er
7/-l(-ar /-1r ke/hl(-ayr Proc. Recent Advances in 
Natural Language Processing 2009.r
x/ln5r ??(odio-yr ,jjvyr pi/5(d( Gr f-a(-ooe(-ayr snea/-r
H/t??3/--rTt5d(i8oeiyr
288
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1148?1158,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Knowledge Base Population:  Successful Approaches and Challenges 
 
Heng Ji Ralph Grishman 
Computer Science Department Computer Science Department 
Queens College and Graduate Center  
City University of New York 
New York University 
New York, NY 11367, USA New York, NY 10003, USA 
hengji@cs.qc.cuny.edu grishman@cs.nyu.edu 
 
 
 
  
 
Abstract 
In this paper we give an overview of the 
Knowledge Base Population (KBP) track at 
the 2010 Text Analysis Conference. The main 
goal of KBP is to promote research in discov-
ering facts about entities and augmenting a 
knowledge base (KB) with these facts. This is 
done through two tasks, Entity Linking ? link-
ing names in context to entities in the KB ? 
and Slot Filling ? adding information about an 
entity to the KB.  A large source collection of 
newswire and web documents is provided 
from which systems are to discover informa-
tion. Attributes (?slots?) derived from 
Wikipedia infoboxes are used to create the 
reference KB. In this paper we provide an 
overview of the techniques which can serve as 
a basis for a good KBP system, lay out the 
remaining challenges by comparison with tra-
ditional Information Extraction (IE) and Ques-
tion Answering (QA) tasks, and provide some 
suggestions to address these challenges. 
1 Introduction 
Traditional information extraction (IE) evaluations, 
such as the Message Understanding Conferences 
(MUC) and Automatic Content Extraction (ACE), 
assess the ability to extract information from indi-
vidual documents in isolation. In practice, how-
ever, we may need to gather information about a 
person or organization that is scattered among the 
documents of a large collection.  This requires the 
ability to identify the relevant documents and to 
integrate facts, possibly redundant, possibly com-
plementary, possibly in conflict, coming from 
these documents. Furthermore, we may want to use 
the extracted information to augment an existing 
data base.  This requires the ability to link indi-
viduals mentioned in a document, and information 
about these individuals, to entries in the data base. 
On the other hand, traditional Question Answering 
(QA) evaluations made limited efforts at disam-
biguating entities in queries (e.g. Pizzato et al, 
2006), and limited use of relation/event extraction 
in answer search (e.g. McNamee et al, 2008). 
  The Knowledge Base Population (KBP) shared 
task, conducted as part of the NIST Text Analysis 
Conference, aims to address and evaluate these 
capabilities, and bridge the IE and QA communi-
ties to promote research in discovering facts about 
entities and expanding a knowledge base with 
these facts. KBP is done through two separate sub-
tasks, Entity Linking and Slot Filling; in 2010, 23 
teams submitted results for one or both sub-tasks. 
A variety of approaches have been proposed to 
address both tasks with considerable success; nev-
ertheless, there are many aspects of the task that 
remain unclear. What are the fundamental tech-
niques used to achieve reasonable performance? 
What is the impact of each novel method? What 
types of problems are represented in the current 
KBP paradigm compared to traditional IE and QA? 
In which way have the current testbeds and evalua-
tion methodology affected our perception of the 
task difficulty? Have we reached a performance 
ceiling with current state of the art techniques?  
What are the remaining challenges and what are 
the possible ways to address these challenges? In 
this paper we aim to answer some of these ques-
tions based on our detailed analysis of evaluation 
results. 
1148
2 Task Definition and Evaluation Metrics 
This section will summarize the tasks conducted at 
KBP 2010. The overall goal of KBP is to auto-
matically identify salient and novel entities, link 
them to corresponding Knowledge Base (KB) en-
tries (if the linkage exists), then discover attributes 
about the entities, and finally expand the KB with 
any new attributes.  
  In the Entity Linking task, given a person (PER), 
organization (ORG) or geo-political entity (GPE, a 
location with a government) query that consists of 
a name string and a background document contain-
ing that name string, the system is required to pro-
vide the ID of the KB entry to which the name 
refers; or NIL if there is no such KB entry. The 
background document, drawn from the KBP cor-
pus, serves to disambiguate ambiguous name 
strings. 
In selecting among the KB entries, a system 
could make use of the Wikipedia text associated 
with each entry as well as the structured fields of 
each entry.  In addition, there was an optional task 
where the system could only make use of the struc-
tured fields; this was intended to be representative 
of applications where no backing text was avail-
able.  Each site could submit up to three runs with 
different parameters. 
  The goal of Slot Filling is to collect from the cor-
pus information regarding certain attributes of an 
entity, which may be a person or some type of or-
ganization. Each query in the Slot Filling task con-
sists of the name of the entity, its type (person or 
organization), a background document containing 
the name (again, to disambiguate the query in case 
there are multiple entities with the same name), its 
node ID (if the entity appears in the knowledge 
base), and the attributes which need not be filled.  
Attributes are excluded if they are already filled in 
the reference data base and can only take on a sin-
gle value. Along with each slot fill, the system 
must provide the ID of a document which supports 
the correctness of this fill.  If the corpus does not 
provide any information for a given attribute, the 
system should generate a NIL response (and no 
document ID). KBP2010 defined 26 types of at-
tributes for persons (such as the age, birthplace, 
spouse, children, job title, and employing organiza-
tion) and 16 types of attributes for organizations 
(such as the top employees, the founder, the year 
founded, the headquarters location, and subsidiar-
ies).  Some of these attributes are specified as only 
taking a single value (e.g., birthplace), while some 
can take multiple values (e.g., top employees). 
  The reference KB includes hundreds of thousands 
of entities based on articles from an October 2008 
dump of English Wikipedia which includes 
818,741 nodes. The source collection includes 
1,286,609 newswire documents, 490,596 web 
documents and hundreds of transcribed spoken 
documents. 
  To score Entity Linking, we take each query and 
check whether the KB node ID (or NIL) returned 
by a system is correct or not.  Then we compute 
the Micro-averaged Accuracy, computed across all 
queries. 
  To score Slot Filling, we first pool all the system 
responses (as is done for information retrieval 
evaluations) together with a set of manually-
prepared slot fills.  These responses are then as-
sessed by hand.  Equivalent answers (such as ?Bill 
Clinton? and ?William Jefferson Clinton?) are 
grouped into equivalence classes.  Each system 
response is rated as correct, wrong, or redundant (a 
response which is equivalent to another response 
for the same slot or an entry already in the knowl-
edge base). Given these judgments, we count 
Correct = total number of non-NIL system output 
slots judged correct 
System = total number of non-NIL system output 
slots 
Reference = number of single-valued slots with a 
correct non-NIL response + 
 number of equivalence classes for all list-
valued slots 
Recall = Correct / Reference 
Precision = Correct / System 
F-Measure = (2 ? Recall ? Precision) / (Recall + 
Precision) 
3 Entity Linking: What Works 
In Entity Linking, we saw a general improvement 
in performance over last year?s results ? the top 
system achieved 85.78% micro-averaged accuracy. 
When measured against a benchmark based on in-
ter-annotator agreement, two systems? perform-
ance approached and one system exceeded the 
benchmark on person entities.  
3.1 A General Architecture 
A typical entity linking system architecture is de-
picted in Figure 1. 
 
1149
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. General Entity Linking  
System Architecture 
It includes three steps: (1) query expansion ? ex-
pand the query into a richer set of forms using 
Wikipedia structure mining or coreference resolu-
tion in the background document. (2) candidate 
generation ? finding all possible KB entries that a 
query might link to; (3) candidate ranking ? rank 
the probabilities of all candidates and NIL answer.  
Table 1 summarizes the systems which ex-
ploited different approaches at each step. In the 
following subsections we will highlight the new 
and effective techniques used in entity linking. 
3.2 Wikipedia Structure Mining 
Wikipedia articles are peppered with structured 
information and hyperlinks to other (on average 
25) articles (Medelyan et al, 2009). Such informa-
tion provides additional sources for entity linking: 
(1). Query Expansion: For example, WebTLab 
(Fernandez et al, 2010) used Wikipedia link struc-
ture (source, anchors, redirects and disambigua-
tion) to extend the KB and compute entity co-
occurrence estimates. Many other teams including 
CUNY and Siel used redirect pages and disam-
biguation pages for query expansion. The Siel team 
also exploited bold texts from first paragraphs be-
cause they often contain nicknames, alias names 
and full names.  
 
Methods  System Examples System 
Ranking 
Range 
Wikipedia Hyperlink Mining  CUNY (Chen et al, 2010), NUSchime (Zhang et al, 
2010), Siel (Bysani et al, 2010), SMU-SIS (Gottipati et 
al., 2010), USFD (Yu et al, 2010), WebTLab team (Fer-
nandez et al, 2010) 
[2, 15]  
Query 
Expansion 
 
 Source document coreference 
resolution 
CUNY (Chen et al, 2010) 9 
 
Document semantic analysis 
and context modeling 
ARPANI (Thomas et al, 2010), CUNY (Chen et al, 
2010), LCC (Lehmann et al, 2010) 
[1,14] Candidate 
Generation 
 IR CUNY (Chen et al, 2010), Budapestacad (Nemeskey et 
al., 2010), USFD (Yu et al, 2010) 
[9, 16] 
Unsupervised Similarity 
Computation (e.g. VSM) 
CUNY (Chen et al, 2010), SMU-SIS (Gottipati et al, 
2010), USFD (Yu et al, 2010) 
[9, 14] 
Supervised  
Classification 
LCC (Lehmann et al, 2010), NUSchime (Zhang et al, 
2010), Stanford-UBC (Chang et al, 2010),  HLTCOE 
(McNamee, 2010), UC3M (Pablo-Sanchez et al, 2010) 
[1, 10] 
Rule-based LCC (Lehmann et al, 2010), BuptPris (Gao et al, 2010) [1, 8] 
Global Graph-based Ranking CMCRC (Radford et al, 2010) 3 
Candidate 
Ranking 
IR Budapestacad (Nemeskey et al, 2010) 16 
 
Table 1. Entity Linking Method Comparison
Query 
Query Expansion 
Wiki 
hyperlink 
mining
Source doc 
Coreference 
Resolution
KB Node Candidate Generation 
KB Node Candidate Ranking 
Wiki KB 
+Texts 
unsupervised 
similarity  
computation 
supervised 
classifica-
tion 
IR 
Answer 
IR
Document semantic analysis
Graph
-based 
1150
(2). Candidate Ranking: Stanford-UBC used 
Wikipedia hyperlinks (clarification, disambigua-
tion, title) for query re-mapping, and encoded lexi-
cal and part-of-speech features  from Wikipedia 
articles containing hyperlinks to the queries to train 
a supervised classifier; they reported a significant 
improvement on micro-averaged accuracy, from 
74.85% to 82.15%.  In fact, when the mined attrib-
utes become rich enough, they can be used as an 
expanded query and sent into an information re-
trieval engine in order to obtain the relevant source 
documents. Budapestacad team (Nemeskey et al, 
2010) adopted this strategy. 
3.3 Ranking Approach Comparison 
The ranking approaches exploited in the KBP2010 
entity linking systems can be generally categorized 
into four types:  
(1). Unsupervised or weakly-supervised learning, 
in which annotated data is minimally used to tune 
thresholds and parameters. The similarity measure 
is largely based on the unlabeled contexts. 
(2). Supervised learning, in which a pair of entity 
and KB node is modeled as an instance for classi-
fication. Such a classifier can be learned from the 
annotated training data based on many different 
features. 
(3). Graph-based ranking, in which context entities 
are taken into account in order to reach a global 
optimized solution together with the query entity. 
(4). IR (Information Retrieval) approach, in which 
the entire background source document is consid-
ered as a single query to retrieve the most relevant 
Wikipedia article. 
The first question we will investigate is how 
much higher performance can be achieved by us-
ing supervised learning? Among the 16 entity link-
ing systems which participated in the regular 
evaluation, LCC (Lehmann et al, 2010), HLTCOE 
(McNamee, 2010), Stanford-UBC (Chang et al, 
2010), NUSchime (Zhang et al, 2010) and UC3M 
(Pablo-Sanchez et al, 2010) have explicitly used 
supervised classification based on many lexical 
and name tagging features, and most of them are 
ranked in top 6 in the evaluation. Therefore we can 
conclude that supervised learning normally leads to 
a reasonably good performance. However, a high-
performing entity linking system can also be im-
plemented in an unsupervised fashion by exploit-
ing effective characteristics and algorithms, as we 
will discuss in the next sections. 
3.4 Semantic Relation Features 
Almost all entity linking systems have used seman-
tic relations as features (e.g. BuptPris (Gao et al, 
2010), CUNY (Chen et al, 2010) and HLTCOE).  
The semantic features used in the BuptPris system 
include name tagging, infoboxes, synonyms, vari-
ants and abbreviations. In the CUNY system, the 
semantic features are automatically extracted from 
their slot filling system. The results are summa-
rized in Table 2, showing the gains over a baseline 
system (using only Wikipedia title features in the 
case of BuptPris, using tf-idf weighted word fea-
tures for CUNY). As we can see, except for person 
entities in the BuptPris system, all types of entities 
have obtained significant improvement by using 
semantic features in entity linking. 
 
System Using Se-
mantic 
Features 
PER ORG GPE Overall 
No 83.89 59.47 33.38 58.93 BuptPris 
 Yes 79.09 74.13 66.62 73.29 
No 84.55 63.07 57.54 59.91 CUNY 
 
 Yes 92.81 65.73 84.10 69.29 
 
Table 2. Impact of Semantic Features on Entity 
Linking (Micro-Averaged Accuracy %)  
3.5 Context Inference 
In the current setting of KBP, a set of target enti-
ties is provided to each system in order to simplify 
the task and its evaluation, because it?s not feasible 
to require a system to generate answers for all pos-
sible entities in the entire source collection. How-
ever, ideally a fully-automatic KBP system should 
be able to automatically discover novel entities 
(?queries?) which have no KB entry or few slot 
fills in the KB, extract their attributes, and conduct 
global reasoning over these attributes in order to 
generate the final output. At the very least, due to 
the semantic coherence principle (McNamara, 
2001), the information of an entity depends on the 
information of other entities. For example, the 
WebTLab team and the CMCRC team extracted all 
entities in the context of a given query, and disam-
biguated all entities at the same time using a Pag-
eRank-like algorithm (Page et al, 1998) or a 
Graph-based Re-ranking algorithm. The SMU-SIS 
team (Gottipati and Jiang, 2010) re-formulated 
queries using contexts. The LCC team modeled 
1151
contexts using Wikipedia page concepts, and com-
puted linkability scores iteratively. Consistent im-
provements were reported by the WebTLab system 
(from 63.64% to 66.58%). 
4 Entity Linking: Remaining Challenges 
4.1 Comparison with Traditional Cross-
document Coreference Resolution 
Part of the entity linking task can be modeled as a 
cross-document entity resolution problem which 
includes two principal challenges: the same entity 
can be referred to by more than one name string 
and the same name string can refer to more than 
one entity. The research on cross-document entity 
coreference resolution can be traced back to the 
Web People Search task (Artiles et al, 2007) and 
ACE2008 (e.g. Baron and Freedman, 2008).   
Compared to WePS and ACE, KBP requires link-
ing an entity mention in a source document to a 
knowledge base with or without Wikipedia arti-
cles. Therefore sometimes the linking decisions 
heavily rely on entity profile comparison with 
Wikipedia infoboxes. In addition, KBP introduced 
GPE entity disambiguation. In source documents, 
especially in web data, usually few explicit attrib-
utes about GPE entities are provided, so an entity 
linking system also needs to conduct external 
knowledge discovery from background related 
documents or hyperlink mining. 
4.2 Analysis of Difficult Queries 
There are 2250 queries in the Entity Linking 
evaluation; for 58 of them at most 5 (out of the 46) 
system runs produced correct answers. Most of 
these queries have corresponding KB entries. For 
19 queries all 46 systems produced different results 
from the answer key. Interestingly, the systems 
which perform well on the difficult queries are not 
necessarily those achieved top overall performance 
? they were ranked 13rd, 6th, 5th, 12nd, 10th, and 16th 
respectively for overall queries. 11 queries are 
highly ambiguous city names which can exist in 
many states or countries (e.g. ?Chester?), or refer 
to person or organization entities. From these most 
difficult queries we observed the following chal-
lenges and possible solutions. 
 
 
 
? Require deep understanding of context enti-
ties for GPE queries 
 
In a document where the query entity is not a cen-
tral topic, the author often assumes that the readers 
have enough background knowledge (?anchor? lo-
cation from the news release information, world 
knowledge or related documents) about these enti-
ties.  For 6 queries, a system would need to inter-
pret or extract attributes for their context entities. 
For example, in the following passage: 
 
?There are also photos of Jake on IHJ in 
Brentwood, still looking somber? 
 
in order to identify that the query ?Brentwood? is 
located in California, a system will need to under-
stand that ?IHJ? is ?I heart Jake community? and 
that the ?Jake? referred to lives in Los Angeles, of 
which Brentwood is a part. 
In the following example, a system is required to 
capture the knowledge that ?Chinese Christian 
man? normally appears in ?China? or there is a 
?Mission School? in ?Canton, China? in order to 
link the query ?Canton? to the correct KB entry. 
This is a very difficult query also because the more 
common way of spelling ?Canton? in China is 
?Guangdong?. 
 
?and was from a Mission School in Canton, ? 
but for the energetic efforts of this Chinese Chris-
tian man and the Refuge Matron? 
 
? Require external hyperlink analysis 
 
Some queries require a system to conduct detailed 
analysis on the hyperlinks in the source document 
or the Wikipedia document. For example, in the 
source document ??Filed under: Falcons 
<http://sports.aol.com/fanhouse/category/atlanta-
falcons/>?, a system will need to analyze the 
document which this hyperlink refers to. Such 
cases might require new query reformulation and 
cross-document aggregation techniques, which are 
both beyond traditional entity disambiguation 
paradigms. 
 
1152
? Require Entity Salience Ranking 
 
Some of these queries represent salient entities and 
so using web popularity rank (e.g. ranking/hit 
counts of Wikipedia pages from search engine) can 
yield correct answers in most cases (Bysani et al, 
2010; Dredze et al, 2010). In fact we found that a 
na?ve candidate ranking approach based on web 
popularity alone can achieve 71% micro-averaged 
accuracy, which is better than 24 system runs in 
KBP2010.   
Since the web information is used as a black box 
(including query expansion and query log analysis) 
which changes over time, it?s more difficult to du-
plicate research results. However, gazetteers with 
entities ranked by salience or major entities 
marked are worth encoding as additional features.   
For example, in the following passages: 
 
... Tritschler brothers competed in gymnastics at the 
1904 Games in St Louis 104 years ago? and ?A char-
tered airliner carrying Democratic White House hope-
ful Barack Obama was forced to make an unscheduled 
landing on Monday in St. Louis after its flight crew 
detected mechanical problems? 
 
although there is little background information to 
decide where the query ?St Louis? is located, a sys-
tem can rely on such a major city list to generate 
the correct linking. Similarly, if a system knows 
that ?Georgia Institute of Technology? has higher 
salience than ?Georgian Technical University?, it 
can correctly link a query ?Georgia Tech? in most 
cases. 
5 Slot Filling: What Works 
5.1 A General Architecture 
The slot-filling task is a hybrid of traditional IE (a 
fixed set of relations) and QA (responding to a 
query, generating a unified response from a large 
collection).  Most participants met this challenge 
through a hybrid system which combined aspects 
of QA (passage retrieval) and IE (answer extrac-
tion).  A few used off-the-shelf QA, either bypass-
ing question analysis or (if QA was used as a 
?black box?) creating a set of questions corre-
sponding to each slot. 
The basic system structure (Figure 2) involved 
three phases:  document/passage retrieval (retriev-
ing passages involving the queried entity), answer 
extraction (getting specific answers from the re-
trieved passages), and answer combination (merg-
ing and selecting among the answers extracted). 
   The solutions adopted for answer extraction re-
flected the range of current IE methods as well as 
QA answer extraction techniques (see Table 3). 
Most systems used one main pipeline, while 
CUNY and BuptPris adopted a hybrid approach of 
combining multiple approaches. 
One particular challenge for KBP, in compari-
son with earlier IE tasks, was the paucity of train-
ing data. The official training data, linked to 
specific text from specific documents, consisted of 
responses to 100 queries; the participants jointly 
prepared responses to another 50.  So traditional 
supervised learning, based directly on the training 
data, would provide limited coverage.  Coverage 
could be improved by using the training data as 
seeds for a bootstrapping procedure.     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. General Slot Filling System Architecture 
 
IE  
(Distant Learning/ 
Bootstrapping) 
Query 
Source  
Collection 
IR Document  Level 
IR, QA Sentence/Passage Level 
Pattern
Answer  
Level Classifier
QA 
Training 
Data/ 
External 
KB
Rules 
Answers 
Query  
Expansion
Knowledge 
Base
Redundancy 
Removal 
1153
Methods System Examples 
Distant Learning (large 
seed, one iteration) 
CUNY (Chen et al, 2010)  
Pattern 
Learning Bootstrapping (small 
seed, multiple iterations) 
NYU (Grishman and Min, 2010) 
Distant Supervision Budapestacad (Nemeskey et al, 2010), lsv (Chrupala et al, 
2010), Stanford (Surdeanu et al, 2010), UBC (Intxaurrondo 
et al, 2010) 
 
 
Trained 
IE 
 
Supervised  
Classifier 
Trained from KBP train-
ing data and other re-
lated tasks 
BuptPris (Gao et al, 2010), CUNY (Chen et al, 2010), IBM 
(Castelli et al, 2010), ICL (Song et al, 2010),  LCC 
(Lehmann et al, 2010), lsv (Chrupala et al, 2010), Siel 
(Bysani et al, 2010) 
QA CUNY (Chen et al, 2010), iirg (Byrne and Dunnion, 2010) 
Hand-coded Heuristic Rules BuptPris (Gao et al, 2010), USFD (Yu et al, 2010) 
 
  Table 3. Slot Filling Answer Extraction Method Comparison 
 
  On the other hand, there were a lot of 'facts' avail-
able ? pairs of entities bearing a relationship corre-
sponding closely to the KBP relations ? in the form 
of filled Wikipedia infoboxes.  These could be 
used for various forms of indirect or distant learn-
ing, where instances in a large corpus of such pairs 
are taken as (positive) training instances.  How-
ever, such instances are noisy ? if a pair of entities 
participates in more than one relation, the found 
instance may not be an example of the intended 
relation ? and so some filtering of the instances or 
resulting patterns may be needed.  Several sites 
used such distant supervision to acquire patterns or 
train classifiers, in some cases combined with di-
rect supervision using the training data (Chrupala 
et al, 2010). 
  Several groups used and extended existing rela-
tion extraction systems, and then mapped the re-
sults into KBP slots.  Mapping the ACE relations 
and events by themselves provided limited cover-
age  (34% of slot fills in the training data), but was 
helpful when combined with other sources (e.g. 
CUNY).  Groups with more extensive existing ex-
traction systems could primarily build on these 
(e.g. LCC, IBM). 
   For example, IBM (Castelli et al, 2010) ex-
tended their mention detection component to cover 
36 entity types which include many non-ACE 
types; and added new relation types between enti-
ties and event anchors. LCC and CUNY applied 
active learning techniques to cover non-ACE types 
of entities, such as ?origin?, ?religion?, ?title?, 
?charge?, ?web-site? and ?cause-of-death?, and 
effectively develop lexicons to filter spurious an-
swers.  
  Top systems also benefited from customizing and 
tightly integrating their recently enhanced extrac-
tion techniques into KBP.  For example, IBM, 
NYU (Grishman and Min, 2010) and CUNY ex-
ploited entity coreference in pattern learning and 
reasoning. It is also notable that traditional extrac-
tion components trained from newswire data suffer 
from noise in web data. In order to address this 
problem, IBM applied their new robust mention 
detection techniques for noisy inputs (Florian et al, 
2010); CUNY developed a component to recover 
structured forms such as tables in web data auto-
matically and filter spurious answers. 
5.2 Use of External Knowledge Base 
Many instance-centered knowledge bases that have 
harvested Wikipedia are proliferating on the se-
mantic web.  The most well known are probably 
the Wikipedia derived resources, including DBpe-
dia (Auer 2007), Freebase (Bollacker 2008) and 
YAGO (Suchanek et al, 2007) and Linked Open 
Data (http://data.nytimes.com/). The main motiva-
tion of the KBP program is to automatically distill 
information from news and web unstructured data 
instead of manually constructed knowledge bases, 
but these existing knowledge bases can provide a 
large number of seed tuples to bootstrap slot filling 
or guide distant learning.  
Such resources can also be used in a more direct 
way. For example, CUNY exploited Freebase and 
LCC exploited DBpedia as fact validation in slot 
filling. However, most of these resources are 
manually created from single data modalities and 
only cover well-known entities. For example, 
while Freebase contains 116 million instances of 
1154
7,300 relations for 9 million entities, it only covers 
48% of the slot types and 5% of the slot answers in 
KBP2010 evaluation data. Therefore, both CUNY 
and LCC observed limited gains from the answer 
validation approach from Freebase. Both systems 
gained about 1% improvement in recall with a 
slight loss in precision. 
5.3 Cross-Slot and Cross-Query Reasoning 
Slot Filling can also benefit from extracting re-
vertible queries from the context of any target 
query, and conducting global ranking or reasoning 
to refine the results. CUNY and IBM developed 
recursive reasoning components to refine extrac-
tion results. For a given query, if there are no other 
related answer candidates available, they built "re-
vertible? queries in the contexts, similar to (Prager 
et al, 2006), to enrich the inference process itera-
tively. For example, if a is extracted as the answer 
for org:subsidiaries of the query q,  we can con-
sider a as a new revertible query and verify that a 
org:parents answer of a is q. Both systems signifi-
cantly benefited from recursive reasoning (CUNY 
F-measure on training data was enhanced from 
33.57% to 35.29% and IBM F-measure was en-
hanced from 26% to 34.83%). 
6 Slot Filling: Remaining Challenges 
Slot filling remains a very challenging task; only 
one system exceeded 30% F-measure on the 2010 
evaluation.  During the 2010 evaluation data anno-
tation/adjudication process, an initial answer key 
annotation was created by a manual search of the 
corpus (resulting in 797 instances), and then an 
independent adjudication pass was applied to as-
sess these annotations together with pooled system 
responses. The Precision, Recall and F-measure for 
the initial human annotation are only about 70%, 
54% and 61% respectively. While we believe the 
annotation consistency can be improved, in part by 
refinement of the annotation guidelines, this does 
place a limit on system performance. 
   Most of the shortfall in system performance re-
flects inadequacies in the answer extraction stage, 
reflecting limitations in the current state-of-the-art 
in information extraction.  An analysis of the 2010 
training data shows that cross-sentence coreference 
and some types of inference are critical to slot fill-
ing.  In only 60.4% of the cases do the entity name 
and slot fill appear together in the same sentence, 
so a system which processes sentences in isolation 
is severely limited in its performance.  22.8% of 
the cases require cross-sentence (identity) corefer-
ence; 15% require some cross-sentence inference 
and 1.8% require cross-slot inference. The infer-
ences include: 
 
? Non-identity coreference: in the following pas-
sage: ?Lahoud is married to an Armenian and the 
couple have three children. Eldest son Emile Emile 
Lahoud was a member of parliament between 2000 
and 2005.? the semantic relation between ?chil-
dren? and ?son? needs to be exploited in order 
to generate ?Emile Emile Lahoud? as the 
per:children of the query entity ?Lahoud?; 
 
? Cross-slot inference based on revertible que-
ries, propagation links or even world knowl-
edge to capture some of the most challenging 
cases. In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the ?coherence? 
of the story (i.e. consistency among all gener-
ated answers (query profiles)). In the following 
example: 
?People Magazine has confirmed that actress Julia 
Roberts has given birth to her third child a boy 
named Henry Daniel Moder. Henry was born 
Monday in Los Angeles and weighed 8? lbs. Rob-
erts, 39, and husband Danny Moder, 38, are al-
ready parents to twins Hazel and Phinnaeus who 
were born in November 2006.?  
 
the following reasoning rules are needed to 
generate the answer ?Henry Daniel Moder? as 
per:children of ?Danny Moder?: 
 ChildOf (?Henry Daniel Moder?, ?Julia Roberts?) 
    ?  Coreferential (?Julia Roberts?, ?Roberts?)  
   ?  SpouseOf (?Roberts?, ?Danny Moder?) ?  
ChildOf (?Henry Daniel Moder?, ?Danny Moder?) 
 
    KBP Slot Filling is similar to ACE Relation Ex-
traction, which has been extensively studied for the 
past 7 years. However, the amount of training data 
is much smaller, forcing sites to adjust their train-
ing strategies. Also, some of the constraints of 
ACE relation mention extraction ? notably, that 
both arguments are present in the same sentence ? 
are not present, making the role of coreference and 
cross-sentence inference more critical. 
The role of coreference and inference as limiting 
factors, while generally recognized, is emphasized 
1155
by examining the 163 slot values that the human 
annotators filled but that none of the systems were 
able to get correct.  Many of these difficult cases 
involve a combination of problems, but we esti-
mate that at least 25% of the examples involve 
coreference which is beyond current system capa-
bilities, such as nominal anaphors: 
?Alexandra Burke is out with the video for her second 
single ? taken from the British artist?s debut album? 
?a woman charged with running a prostitution ring ? 
her business, Pamela Martin and Associates? 
  (underlined phrases are coreferential).    
While the types of inferences which may be re-
quired is open-ended, certain types come up re-
peatedly, reflecting the types of slots to be filled:  
systems would benefit from specialists which are 
able to reason about times, locations, family rela-
tionships, and employment relationships. 
7 Toward System Combination 
The increasing number of diverse approaches 
based on different resources provide new opportu-
nities for both entity linking and slot filling tasks to 
benefit from system combination.  
  The NUSchime entity linking system trained a 
SVM based re-scoring model to combine two indi-
vidual pipelines. Only one feature based on confi-
dence values from the pipelines was used for re-
scoring. The micro-averaged accuracy was en-
hanced from 79.29%/79.07% to 79.38% after 
combination. We also applied a voting approach on 
the top 9 entity linking systems and found that all 
combination orders achieved significant gains, 
with the highest absolute improvement of 4.7% in 
micro-averaged accuracy over the top entity link-
ing system. 
The CUNY slot filling system trained a maxi-
mum-entropy-based re-ranking model to combine 
three individual pipelines, based on various global 
features including voting and dependency rela-
tions. Significant gain in F-measure was achieved:  
from 17.9%, 27.7% and 21.0% (on training data) to 
34.3% after combination. When we applied the 
same re-ranking approach to the slot filling sys-
tems which were ranked from the 2nd to 14th, we 
achieved 4.3% higher F-score than the best of 
these systems. 
8 Conclusion 
Compared to traditional IE and QA tasks, KBP has 
raised some interesting and important research is-
sues: It places more emphasis on cross-document 
entity resolution which received limited effort in 
ACE; it forces systems to deal with redundant and 
conflicting answers across large corpora; it links 
the facts in text to a knowledge base so that NLP 
and data mining/database communities have a bet-
ter chance to collaborate; it provides opportunities 
to develop novel training methods such as distant 
(and noisy) supervision through Infoboxes (Sur-
deanu et al, 2010; Chen et al, 2010).  
  In this paper, we provided detailed analysis of the 
reasons which have made KBP a more challenging 
task, shared our observations and lessons learned 
from the evaluation, and suggested some possible 
research directions to address these challenges 
which may be helpful for current and new partici-
pants, or IE and QA researchers in general. 
Acknowledgements 
The first author was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement Num-
ber W911NF-09-2-0053, the U.S. NSF CAREER 
Award under Grant IIS-0953149 and PSC-CUNY Re-
search Program. The views and conclusions contained 
in this document are those of the authors and should not 
be interpreted as representing the official policies, either 
expressed or implied, of the Army Research Laboratory 
or the U.S. Government. The U.S. Government is au-
thorized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright 
notation hereon. 
References  
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2007. 
The SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. Proc. 
the 4th International Workshop on Semantic Evalua-
tions (Semeval-2007). 
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann and Z. Ives. 
2007. DBpedia: A nucleus for a web of open data. 
Proc. 6th International Semantic Web Conference. 
K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal 
Name Resolution of Web People Search. Proc. 
WWW2008 Workshop: NLP Challenges in the Infor-
mation Explosion Era (NLPIX 2008).  
 
1156
Alex Baron and Marjorie Freedman. 2008. Who is Who 
and What is What: Experiments in Cross-Document 
Co-Reference. Proc. EMNLP 2008. 
K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A 
Shared Database of Structured General Human 
Knowledge. Proc. National Conference on Artificial 
Intelligence (Volume 2). 
Lorna Byrne and John Dunnion. 2010. UCD IIRG at 
TAC 2010. Proc. TAC 2010 Workshop. 
Praveen Bysani, Kranthi Reddy, Vijay Bharath Reddy, 
Sudheer Kovelamudi, Prasad Pingali and Vasudeva 
Varma. 2010. IIIT Hyderabad in Guided Summariza-
tion and Knowledge Base Population. Proc. TAC 
2010 Workshop. 
Vittorio Castelli, Radu Florian and Ding-jung Han. 
2010. Slot Filling through Statistical Processing and 
Inference Rules. Proc. TAC 2010 Workshop. 
Angel X. Chang, Valentin I. Spitkovsky, Eric Yeh, 
Eneko Agirre and Christopher D. Manning. 2010. 
Stanford-UBC Entity Linking at TAC-KBP. Proc. 
TAC 2010 Workshop. 
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li, 
Wen-Pin Lin, Matthew Snover, Javier Artiles, 
Marissa Passantino and Heng Ji. 2010. CUNY-
BLENDER TAC-KBP2010 Entity Linking and Slot 
Filling System Description. Proc. TAC 2010 Work-
shop. 
Grzegorz Chrupala, Saeedeh Momtazi, Michael Wie-
gand, Stefan Kazalski, Fang Xu, Benjamin Roth, Al-
exandra Balahur, Dietrick Klakow. Saarland 
University Spoken Language Systems at the Slot Fill-
ing Task of TAC KBP 2010. Proc. TAC 2010 Work-
shop. 
Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber 
and Tim Finin. 2010. Entity Disambiguation for 
Knowledge Base Population. Proc. COLING 2010. 
Norberto Fernandez, Jesus A. Fisteus, Luis Sanchez and 
Eduardo Martin. 2010. WebTLab: A Cooccurence-
based Approach to KBP 2010 Entity-Linking Task. 
Proc. TAC 2010 Workshop. 
Radu Florian, John F. Pitrelli, Salim Roukos and Imed 
Zitouni. 2010. Improving Mention Detection Robust-
ness to Noisy Input. Proc. EMNLP2010. 
Sanyuan Gao, Yichao Cai, Si Li, Zongyu Zhang, Jingyi 
Guan, Yan Li, Hao Zhang, Weiran Xu and Jun Guo. 
2010. PRIS at TAC2010 KBP Track. Proc. TAC 
2010 Workshop. 
Swapna Gottipati and Jing Jiang. 2010. SMU-SIS at 
TAC 2010 ? KBP Track Entity Linking. Proc. TAC 
2010 Workshop. 
Ralph Grishman and Bonan Min. 2010. New York Uni-
versity KBP 2010 Slot-Filling System. Proc. TAC 
2010 Workshop. 
Ander Intxaurrondo, Oier Lopez de Lacalle and Eneko 
Agirre. 2010. UBC at Slot Filling TAC-KBP2010. 
Proc. TAC 2010 Workshop. 
John Lehmann, Sean Monahan, Luke Nezda, Arnold 
Jung and Ying Shi. 2010. LCC Approaches to 
Knowledge Base Population at TAC 2010. Proc. 
TAC 2010 Workshop. 
Paul McNamee and Hoa Dang. 2009. Overview of the 
TAC 2009 Knowledge Base Population Track. Proc. 
TAC 2009 Workshop. 
Paul McNamee, Hoa Trang Dang, Heather Simpson, 
Patrick Schone and Stephanie M. Strassel. 2010. An 
Evaluation of Technologies for Knowledge Base 
Population. Proc. LREC2010. 
Paul McNamee, Rion Snow, Patrick Schone and James 
Mayfield. 2008. Learning Named Entity Hyponyms 
for Question Answering. Proc. IJCNLP2008. 
Paul McNamee. 2010. HLTCOE Efforts in Entity Link-
ing at TAC KBP 2010. Proc. TAC 2010 Workshop. 
Danielle S McNamara. 2001. Reading both High-
coherence and Low-coherence Texts: Effects of Text 
Sequence and Prior Knowledge. Canadian Journal of 
Experimental Psychology. 
Olena Medelyan, Catherine Legg, David Milne and Ian 
H. Witten. 2009. Mining Meaning from Wikipedia. 
International Journal of Human-Computer Studies 
archive. Volume 67 , Issue 9. 
David Nemeskey, Gabor Recski, Attila Zseder and An-
dras Kornai. 2010. BUDAPESTACAD at TAC 2010. 
Proc. TAC 2010 Workshop. 
Cesar de Pablo-Sanchez, Juan Perea and Paloma Marti-
nez. 2010. Combining Similarities with Regression 
based Classifiers for Entity Linking at TAC 2010. 
Proc. TAC 2010 Workshop. 
Lawrence Page, Sergey Brin, Rajeev Motwani and 
Terry Winograd. 1998. The PageRank Citation Rank-
ing: Bringing Order to the Web. Proc. the 7th Interna-
tional World Wide Web Conference. 
Luiz Augusto Pizzato, Diego Molla and Cecile Paris. 
2006. Pseudo Relevance Feedback Using Named En-
tities for Question Answering. Proc. the Australasian 
Language Technology Workshop 2006. 
J. Prager, P. Duboue, and J. Chu-Carroll. 2006. Improv-
ing QA Accuracy by Question Inversion. Proc. ACL-
COLING 2006. 
1157
Will Radford, Ben Hachey, Joel Nothman, Matthew 
Honnibal and James R. Curran. 2010. CMCRC at 
TAC10: Document-level Entity Linking with Graph-
based Re-ranking. Proc. TAC 2010 Workshop. 
Yang Song, Zhengyan He and Houfeng Wang. 2010. 
ICL_KBP Approaches to Knowledge Base Popula-
tion at TAC2010. Proc. TAC 2010 Workshop. 
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007. 
Yago: A Core of Semantic Knowledge. Proc. 16th 
International World Wide Web Conference. 
Mihai Surdeanu, David McClosky, Julie Tibshirani, 
John Bauer, Angel X. Chang, Valentin I. Spitkovsky, 
Christopher D. Manning. 2010. A Simple Distant 
Supervision Approach for the TAC-KBP Slot Filling 
Task. Proc. TAC 2010 Workshop. 
Ani Thomas, Arpana Rawai, M K Kowar, Sanjay 
Sharma, Sarang Pitale and Neeraj Kharya. 2010. 
Bhilai Institute of Technology Durg at TAC 2010: 
Knowledge Base Population Task Challenge. Proc. 
TAC 2010 Workshop. 
Jingtao Yu, Omkar Mujgond and Rob Gaizauskas. 
2010. The University of Sheffield System at TAC 
KBP 2010. Proc. TAC 2010 Workshop. 
Wei Zhang, Yan Chuan Sim, Jian Su and Chew Lim 
Tan. 2010. NUS-I2R: Learning a Combined System 
for Entity Linking. Proc. TAC 2010 Workshop. 
 
1158
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73?82,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Event Extraction via Structured Prediction with Global Features
Qi Li Heng Ji Liang Huang
Departments of Computer Science and Linguistics
The Graduate Center and Queens College
City University of New York
New York, NY 10016, USA
{liqiearth, hengjicuny, liang.huang.sh}@gmail.com
Abstract
Traditional approaches to the task of ACE
event extraction usually rely on sequential
pipelines with multiple stages, which suf-
fer from error propagation since event trig-
gers and arguments are predicted in isola-
tion by independent local classifiers. By
contrast, we propose a joint framework
based on structured prediction which ex-
tracts triggers and arguments together so
that the local predictions can be mutu-
ally improved. In addition, we propose
to incorporate global features which ex-
plicitly capture the dependencies of multi-
ple triggers and arguments. Experimental
results show that our joint approach with
local features outperforms the pipelined
baseline, and adding global features fur-
ther improves the performance signifi-
cantly. Our approach advances state-of-
the-art sentence-level event extraction, and
even outperforms previous argument la-
beling methods which use external knowl-
edge from other sentences and documents.
1 Introduction
Event extraction is an important and challeng-
ing task in Information Extraction (IE), which
aims to discover event triggers with specific types
and their arguments. Most state-of-the-art ap-
proaches (Ji and Grishman, 2008; Liao and Gr-
ishman, 2010; Hong et al, 2011) use sequential
pipelines as building blocks, which break down
the whole task into separate subtasks, such as
trigger identification/classification and argument
identification/classification. As a common draw-
back of the staged architecture, errors in upstream
component are often compounded and propagated
to the downstream classifiers. The downstream
components, however, cannot impact earlier deci-
sions. For example, consider the following sen-
tences with an ambiguous word ?fired?:
(1) In Baghdad, a cameraman died when an
American tank fired on the Palestine Hotel.
(2) He has fired his air defense chief .
In sentence (1), ?fired? is a trigger of type Attack.
Because of the ambiguity, a local classifier may
miss it or mislabel it as a trigger of End-Position.
However, knowing that ?tank? is very likely to be
an Instrument argument of Attack events, the cor-
rect event subtype assignment of ?fired? is obvi-
ously Attack. Likewise, in sentence (2), ?air de-
fense chief? is a job title, hence the argument clas-
sifier is likely to label it as an Entity argument for
End-Position trigger.
In addition, the local classifiers are incapable
of capturing inter-dependencies among multiple
event triggers and arguments. Consider sentence
(1) again. Figure 1 depicts the corresponding
event triggers and arguments. The dependency be-
tween ?fired? and ?died? cannot be captured by the
local classifiers, which may fail to attach ?camera-
man? to ?fired? as a Target argument. By using
global features, we can propagate the Victim ar-
gument of the Die event to the Target argument
of the Attack event. As another example, know-
ing that an Attack event usually only has one At-
tacker argument, we could penalize assignments
in which one trigger has more than one Attacker.
Such global features cannot be easily exploited by
a local classifier.
Therefore, we take a fresh look at this prob-
lem and formulate it, for the first time, as a struc-
tured learning problem. We propose a novel joint
event extraction algorithm to predict the triggers
and arguments simultaneously, and use the struc-
tured perceptron (Collins, 2002) to train the joint
model. This way we can capture the dependencies
between triggers and argument as well as explore
73
In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.
AttackDie
Instrument
Place
Victim
Target
Instrument
Target
Place
Figure 1: Event mentions of example (1). There are two event mentions that share three arguments,
namely the Die event mention triggered by ?died?, and the Attack event mention triggered by ?fired?.
arbitrary global features over multiple local pre-
dictions. However, different from easier tasks such
as part-of-speech tagging or noun phrase chunking
where efficient dynamic programming decoding is
feasible, here exact joint inference is intractable.
Therefore we employ beam search in decoding,
and train the model using the early-update percep-
tron variant tailored for beam search (Collins and
Roark, 2004; Huang et al, 2012).
We make the following contributions:
1. Different from traditional pipeline approach,
we present a novel framework for sentence-
level event extraction, which predicts triggers
and their arguments jointly (Section 3).
2. We develop a rich set of features for event
extraction which yield promising perfor-
mance even with the traditional pipeline
(Section 3.4.1). In this paper we refer to them
as local features.
3. We introduce various global features to ex-
ploit dependencies among multiple triggers
and arguments (Section 3.4.2). Experi-
ments show that our approach outperforms
the pipelined approach with the same set of
local features, and significantly advances the
state-of-the-art with the addition of global
features which brings a notable further im-
provement (Section 4).
2 Event Extraction Task
In this paper we focus on the event extraction task
defined in Automatic Content Extraction (ACE)
evaluation.1 The task defines 8 event types and
33 subtypes such as Attack, End-Position etc. We
introduce the terminology of the ACE event ex-
traction that we used in this paper:
1http://projects.ldc.upenn.edu/ace/
? Event mention: an occurrence of an event
with a particular type and subtype.
? Event trigger: the word most clearly ex-
presses the event mention.
? Event argument: an entity mention, tempo-
ral expression or value (e.g. Job-Title) that
serves as a participant or attribute with a spe-
cific role in an event mention.
? Event mention: an instance that includes one
event trigger and some arguments that appear
within the same sentence.
Given an English text document, an event ex-
traction system should predict event triggers with
specific subtypes and their arguments from each
sentence. Figure 1 depicts the event triggers and
their arguments of sentence (1) in Section 1. The
outcome of the entire sentence can be considered a
graph in which each argument role is represented
as a typed edge from a trigger to its argument.
In this work, we assume that argument candi-
dates such as entities are part of the input to the
event extraction, and can be from either gold stan-
dard or IE system output.
3 Joint Framework for Event Extraction
Based on the hypothesis that facts are inter-
dependent, we propose to use structured percep-
tron with inexact search to jointly extract triggers
and arguments that co-occur in the same sentence.
In this section, we will describe the training and
decoding algorithms for this model.
3.1 Structured perceptron with beam search
Structured perceptron is an extension to the stan-
dard linear perceptron for structured prediction,
which was proposed in (Collins, 2002). Given a
sentence instance x ? X , which in our case is a
sentence with argument candidates, the structured
perceptron involves the following decoding prob-
74
lem which finds the best configuration z ? Y ac-
cording to the current model w:
z = argmax
y??Y(x)
w ? f(x, y?) (1)
where f(x, y?) represents the feature vector for in-
stance x along with configuration y?.
The perceptron learns the model w in an on-
line fashion. Let D = {(x(j), y(j))}nj=1 be the set
of training instances (with j indexing the current
training instance). In each iteration, the algorithm
finds the best configuration z for x under the cur-
rent model (Eq. 1). If z is incorrect, the weights
are updated as follows:
w = w + f(x, y)? f(x, z) (2)
The key step of the training and test is the de-
coding procedure, which aims to search for the
best configuration under the current parameters. In
simpler tasks such as part-of-speech tagging and
noun phrase chunking, efficient dynamic program-
ming algorithms can be employed to perform ex-
act inference. Unfortunately, it is intractable to
perform the exact search in our framework be-
cause: (1) by jointly modeling the trigger labeling
and argument labeling, the search space becomes
much more complex. (2) we propose to make use
of arbitrary global features, which makes it infea-
sible to perform exact inference efficiently.
To address this problem, we apply beam-search
along with early-update strategy to perform inex-
act decoding. Collins and Roark (2004) proposed
the early-update idea, and Huang et al (2012) later
proved its convergence and formalized a general
framework which includes it as a special case. Fig-
ure 2 describes the skeleton of perceptron train-
ing algorithm with beam search. In each step of
the beam search, if the prefix of oracle assign-
ment y falls out from the beam, then the top re-
sult in the beam is returned for early update. One
could also use the standard-update for inference,
however, with highly inexact search the standard-
update generally does not work very well because
of ?invalid updates?, i.e., updates that do not fix a
violation (Huang et al, 2012). In Section 4.5 we
will show that the standard perceptron introduces
many invalid updates especially with smaller beam
sizes, also observed by Huang et al (2012).
To reduce overfitting, we used averaged param-
eters after training to decode test instances in our
experiments. The resulting model is called aver-
aged perceptron (Collins, 2002).
Input: Training set D = {(x(j), y(j))}ni=1,
maximum iteration number T
Output: Model parameters w
1 Initialization: Set w = 0;
2 for t? 1...T do
3 foreach (x, y) ? D do
4 z ? beamSearch (x, y,w)
5 if z 6= y then
6 w? w + f(x, y[1:|z|])? f(x, z)
Figure 2: Perceptron training with beam-
search (Huang et al, 2012). Here y[1:i] de-
notes the prefix of y that has length i, e.g.,
y[1:3] = (y1, y2, y3).
3.2 Label sets
Here we introduce the label sets for trigger and ar-
gument in the model. We use L ? {?} to denote
the trigger label alphabet, where L represents the
33 event subtypes, and ? indicates that the token
is not a trigger. Similarly, R ? {?} denotes the
argument label sets, whereR is the set of possible
argument roles, and ? means that the argument
candidate is not an argument for the current trig-
ger. It is worth to note that the set R of each par-
ticular event subtype is subject to the entity type
constraints defined in the official ACE annotation
guideline2. For example, the Attacker argument
for an Attack event can only be one of PER, ORG
and GPE (Geo-political Entity).
3.3 Decoding
Let x = ?(x1, x2, ..., xs), E? denote the sentence
instance, where xi represents the i-th token in the
sentence and E = {ek}mk=1 is the set of argument
candidates. We use
y = (t1, a1,1, . . . , a1,m, . . . , ts, as,1, . . . , as,m)
to denote the corresponding gold standard struc-
ture, where ti represents the trigger assignment for
the token xi, and ai,k represents the argument role
label for the edge between xi and argument candi-
date ek.
2http://projects.ldc.upenn.edu/ace/docs/English-Events-
Guidelines v5.4.3.pdf
75
y = (t1, a1,1, a1,2, t2, a2,1, a2,2,| {z }
arguments for x2
t3, a3,1, a3,2)
g(1) g(2) h(2, 1) h(3, 2)
Figure 3: Example notation with s = 3,m = 2.
For simplicity, throughout this paper we use
yg(i) and yh(i,k) to represent ti and ai,k, respec-
tively. Figure 3 demonstrates the notation with
s = 3 and m = 2. The variables for the toy sen-
tence ?Jobs founded Apple? are as follows:
x = ?(Jobs,
x2? ?? ?
founded, Apple),
E? ?? ?
{JobsPER,AppleORG}?
y = (?,?,?, Start Org? ?? ?
t2
, Agent, Org? ?? ?
args for founded
,?,?,?)
Figure 4 describes the beam-search procedure
with early-update for event extraction. During
each step with token i, there are two sub-steps:
? Trigger labeling We enumerate all possible
trigger labels for the current token. The linear
model defined in Eq. (1) is used to score each
partial configuration. Then the K-best par-
tial configurations are selected to the beam,
assuming the beam size is K.
? Argument labeling After the trigger label-
ing step, we traverse all configurations in the
beam. Once a trigger label for xi is found in
the beam, the decoder searches through the
argument candidates E to label the edges be-
tween each argument candidate and the trig-
ger. After labeling each argument candidate,
we again score each partial assignment and
select the K-best results to the beam.
After the second step, the rank of different trigger
assignments can be changed because of the argu-
ment edges. Likewise, the decision on later argu-
ment candidates may be affected by earlier argu-
ment assignments.
The overall time complexity for decoding is
O(K ? s ?m).
3.4 Features
In this framework, we define two types of fea-
tures, namely local features and global features.
We first introduce the definition of local and global
features in this paper, and then describe the im-
plementation details later. Recall that in the lin-
ear model defined in Eq. (1), f(x, y) denotes the
features extracted from the input instance x along
Input: Instance x = ?(x1, x2, ..., xs), E? and
the oracle output y if for training.
K: Beam size.
L ? {?}: trigger label alphabet.
R? {?}: argument label alphabet.
Output: 1-best prediction z for x
1 Set beam B ? [] /*empty configuration*/
2 for i? 1...s do
3 buf ? {z? ? l | z? ? B, l ? L ? {?}}
B ?K-best(buf )
4 if y[1:g(i)] 6? B then
5 return B[0] /*for early-update*/
6 for ek ? E do /*search for arguments*/
7 buf ? ?
8 for z? ? B do
9 buf ? buf ? {z? ? ?}
10 if z?g(i) 6= ? then /*xi is a trigger*/
11 buf ? buf ? {z? ? r | r ? R}
12 B ?K-best(buf )
13 if y[1:h(i,k)] 6? B then
14 return B[0] /*for early-update*/
15 return B[0]
Figure 4: Decoding algorithm for event extrac-
tion. z?l means appending label l to the end of
z. During test, lines 4-5 & 13-14 are omitted.
with configuration y. In general, each feature in-
stance f in f is a function f : X ? Y ? R, which
maps x and y to a feature value. Local features are
only related to predictions on individual trigger or
argument. In the case of unigram tagging for trig-
ger labeling, each local feature takes the form of
f(x, i, yg(i)), where i denotes the index of the cur-
rent token, and yg(i) is its trigger label. In practice,
it is convenient to define the local feature function
as an indicator function, for example:
f1(x, i, yg(i)) =
{
1 if yg(i) = Attack and xi = ?fire?
0 otherwise
The global features, by contrast, involve longer
range of the output structure. Formally,
each global feature function takes the form of
f(x, i, k, y), where i and k denote the indices
of the current token and argument candidate in
decoding, respectively. The following indicator
function is a simple example of global features:
f101(x, i, k, y) =
?
??
??
1 if yg(i) = Attack and
y has only one ?Attacker?
0 otherwise
76
Category Type Feature Description
Trigger
Lexical
1. unigrams/bigrams of the current and context words within the window of size 2
2. unigrams/bigrams of part-of-speech tags of the current and context words within the
window of size 2
3. lemma and synonyms of the current token
4. base form of the current token extracted from Nomlex (Macleod et al, 1998)
5. Brown clusters that are learned from ACE English corpus (Brown et al, 1992; Miller et
al., 2004; Sun et al, 2011). We used the clusters with prefixes of length 13, 16 and 20 for
each token.
Syntactic
6. dependent and governor words of the current token
7. dependency types associated the current token
8. whether the current token is a modifier of job title
9. whether the current token is a non-referential pronoun
Entity
Information
10. unigrams/bigrams normalized by entity types
11. dependency features normalized by entity types
12. nearest entity type and string in the sentence/clause
Argument
Basic
1. context words of the entity mention
2. trigger word and subtype
3. entity type, subtype and entity role if it is a geo-political entity mention
4. entity mention head, and head of any other name mention from co-reference chain
5. lexical distance between the argument candidate and the trigger
6. the relative position between the argument candidate and the trigger: {before, after,
overlap, or separated by punctuation}
7. whether it is the nearest argument candidate with the same type
8. whether it is the only mention of the same entity type in the sentence
Syntactic
9. dependency path between the argument candidate and the trigger
10. path from the argument candidate and the trigger in constituent parse tree
11. length of the path between the argument candidate and the trigger in dependency graph
12. common root node and its depth of the argument candidate and parse tree
13. whether the argument candidate and the trigger appear in the same clause
Table 1: Local features.
3.4.1 Local features
In general there are two kinds of local features:
Trigger features The local feature func-
tion for trigger labeling can be factorized as
f(x, i, yg(i)) = p(x, i) ? q(yg(i)), where p(x, i) is
a predicate about the input, which we call text fea-
ture, and q(yg(i)) is a predicate on the trigger label.
In practice, we define two versions of q(yg(i)):
q0(yg(i)) = yg(i) (event subtype)
q1(yg(i)) = event type of yg(i)
q1(yg(i)) is a backoff version of the standard un-
igram feature. Some text features for the same
event type may share a certain distributional sim-
ilarity regardless of the subtypes. For example,
if the nearest entity mention is ?Company?, the
current token is likely to be Personnel no matter
whether it is End-Postion or Start-Position.
Argument features Similarly, the local fea-
ture function for argument labeling can be rep-
resented as f(x, i, k, yg(i), yh(i,k)) = p(x, i, k) ?
q(yg(i), yh(i,k)), where yh(i,k) denotes the argu-
ment assignment for the edge between trigger
word i and argument candidate ek. We define two
versions of q(yg(i), yh(i,k)):
q0(yg(i), yh(i,k)) =
?
??
??
yh(i,k) if yh(i,k) is Place,
Time or None
yg(i) ? yh(i,k) otherwise
q1(yg(i), yh(i,k)) =
{
1 if yh(i,k) 6=None
0 otherwise
It is notable that Place and Time arguments are
applicable and behave similarly to all event sub-
types. Therefore features for these arguments are
not conjuncted with trigger labels. q1(yh(i,k)) can
be considered as a backoff version of q0(yh(i,k)),
which does not discriminate different argument
roles but only focuses on argument identification.
Table 1 summarizes the text features about the in-
put for trigger and argument labeling. In our ex-
periments, we used the Stanford parser (De Marn-
effe et al, 2006) to create dependency parses.
3.4.2 Global features
Table 2 summarizes the 8 types of global features
we developed in this work. They can be roughly
divided into the following two categories:
77
Category Feature Description
Trigger
1. bigram of trigger types occur in the same sentence or the same clause
2. binary feature indicating whether synonyms in the same sentence have the same trigger label
3. context and dependency paths between two triggers conjuncted with their types
Argument
4. context and dependency features about two argument candidates which share the same role within the
same event mention
5. features about one argument candidate which plays as arguments in two event mentions in the same
sentence
6. features about two arguments of an event mention which are overlapping
7. the number of arguments with each role type of an event mention conjuncted with the event subtype
8. the pairs of time arguments within an event mention conjuncted with the event subtype
Table 2: Global features.
Transport
(transport)
Entity
(women)
Entity
(children)
Art
ifac
t Artifact
conj and
(a)
Entity
(cameramen)
Die
(died)
Attack
(fired)
Vic
tim
Target
advcl
(b)
End-Position
(resigned)
Entity Entity
[co-chief executive of [Vivendi Universal Entertainment]]
Pos
itio
n Entity
Overlapping
(c)
Figure 5: Illustration of global features (4-6) in Table 2.
Event Probability
Attack 0.34
Die 0.14
Transport 0.08
Injure 0.04
Meet 0.02
Table 3: Top 5 event subtypes that co-occur with
Attack event in the same sentence.
Trigger global feature This type of feature
captures the dependencies between two triggers
within the same sentence. For instance: feature (1)
captures the co-occurrence of trigger types. This
kind of feature is motivated by the fact that two
event mentions in the same sentence tend to be se-
mantically coherent. As an example, from Table 3
we can see that Attack event often co-occur with
Die event in the same sentence, but rarely co-occur
with Start-Position event. Feature (2) encourages
synonyms or identical tokens to have the same la-
bel. Feature (3) exploits the lexical and syntactic
relation between two triggers. A simple example
is whether an Attack trigger and a Die trigger are
linked by the dependency relation conj and.
Argument global feature This type of feature
is defined over multiple arguments for the same
or different triggers. Consider the following sen-
tence:
(3) Trains running to southern Sudan were used
to transport abducted women and children.
The Transport event mention ?transport? has
two Artifact arguments, ?women? and ?chil-
dren?. The dependency edge conj and be-
tween ?women? and ?children? indicates that
they should play the same role in the event men-
tion. The triangle structure in Figure 5(a) is an ex-
ample of feature (4) for the above example. This
feature encourages entities that are linked by de-
pendency relation conj and to play the same role
Artifact in any Transport event.
Similarly, Figure 5(b) depicts an example of
feature (5) for sentence (1) in Section 1. In this ex-
ample, an entity mention is Victim argument to Die
event and Target argument to Attack event, and the
two event triggers are connected by the typed de-
pendency advcl. Here advcl means that the word
?fired? is an adverbial clause modier of ?died?.
Figure 5(c) shows an example of feature (6) for
the following sentence:
(4) Barry Diller resigned as co-chief executive of
Vivendi Universal Entertainment.
The job title ?co-chief executive of Vivendi Uni-
versal Entertainment? overlaps with the Orga-
nization mention ?Vivendi Universal Entertain-
ment?. The feature in the triangle shape can be
considered as a soft constraint such that if a Job-
Title mention is a Position argument to an End-
Position trigger, then the Organization mention
78
which appears at the end of it should be labeled
as Entity argument for the same trigger.
Feature (7-8) are based on the statistics about
different arguments for the same trigger. For in-
stance, in many cases, a trigger can only have one
Place argument. If a partial configuration mis-
takenly classifies more than one entity mention as
Place arguments for the same trigger, then it will
be penalized.
4 Experiments
4.1 Data set and evaluation metric
We utilized the ACE 2005 corpus as our testbed.
For comparison, we used the same test set with 40
newswire articles (672 sentences) as in (Ji and Gr-
ishman, 2008; Liao and Grishman, 2010) for the
experiments, and randomly selected 30 other doc-
uments (863 sentences) from different genres as
the development set. The rest 529 documents (14,
840 sentences) are used for training.
Following previous work (Ji and Grishman,
2008; Liao and Grishman, 2010; Hong et al,
2011), we use the following criteria to determine
the correctness of an predicted event mention:
? A trigger is correct if its event subtype and
offsets match those of a reference trigger.
? An argument is correctly identified if its event
subtype and offsets match those of any of the
reference argument mentions.
? An argument is correctly identified and clas-
sified if its event subtype, offsets and argu-
ment role match those of any of the reference
argument mentions.
Finally we use Precision (P), Recall (R) and F-
measure (F1) to evaluate the overall performance.
4.2 Baseline system
Chen and Ng (2012) have proven that perform-
ing identification and classification in one step is
better than two steps. To compare our proposed
method with the previous pipelined approaches,
we implemented two Maximum Entropy (Max-
Ent) classifiers for trigger labeling and argument
labeling respectively. To make a fair comparison,
the feature sets in the baseline are identical to the
local text features we developed in our framework
(see Figure 1).
4.3 Training curves
We use the harmonic mean of the trigger?s F1
measure and argument?s F1 measure to measure
the performance on the development set.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21# of training iteration0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
0.60
Harm
onic
 me
an
local+globallocal
Figure 6: Training curves on dev set.
Figure 6 shows the training curves of the aver-
aged perceptron with respect to the performance
on the development set when the beam size is 4.
As we can see both curves converge around itera-
tion 20 and the global features improve the over-
all performance, compared to its counterpart with
only local features. Therefore we set the number
of iterations as 20 in the remaining experiments.
4.4 Impact of beam size
The beam size is an important hyper parameter in
both training and test. Larger beam size will in-
crease the computational cost while smaller beam
size may reduce the performance. Table 4 shows
the performance on the development set with sev-
eral different beam sizes. When beam size = 4, the
algorithm achieved the highest performance on the
development set with trigger F1 = 67.9, argument
F1 = 51.5, and harmonic mean = 58.6. When
the size is increased to 32, the accuracy was not
improved. Based on this observation, we chose
beam size as 4 for the remaining experiments.
4.5 Early-update vs. standard-update
Huang et al (2012) define ?invalid update? to be
an update that does not fix a violation (and instead
reinforces the error), and show that it strongly
(anti-)correlates with search quality and learning
quality. Figure 7 depicts the percentage of in-
valid updates in standard-update with and with-
out global features, respectively. With global fea-
tures, there are numerous invalid updates when the
79
Beam size 1 2 4 8 16 32
Training time (sec) 993 2,034 3,982 8,036 15,878 33,026
Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8
Table 4: Comparison of training time and accuracy on the dev set.
1 2 4 8 16 32beam size0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
% of
 inva
lid u
pdat
es
local+globallocal
Figure 7: Percentage of the so-called ?invalid up-
dates? (Huang et al, 2012) in standard perceptron.
Strategy F1 on Dev F1 on TestTrigger Arg Trigger Arg
Standard (b = 1) 68.3 47.4 64.4 49.8
Early (b = 1) 68.9 49.5 65.2 52.1
Standard (b = 4) 68.4 50.5 67.1 51.4
Early (b = 4) 67.9 51.5 67.5 52.7
Table 5: Comparison between the performance
(%) of standard-update and early-update with
global features. Here b stands for beam size.
beam size is small. The ratio decreases mono-
tonically as beam size increases. The model with
only local features made much smaller numbers
of invalid updates, which suggests that the use of
global features makes the search problem much
harder. This observation justify the application of
early-update in this work. To further investigate
the difference between early-update and standard-
update, we tested the performance of both strate-
gies, which is summarized in Table 5. As we can
see the performance of standard-update is gener-
ally worse than early-update. When the beam size
is increased (b = 4), the gap becomes smaller as
the ratio of invalid updates is reduced.
4.6 Overall performance
Table 6 shows the overall performance on the blind
test set. In addition to our baseline, we compare
against the sentence-level system reported in Hong
et al (2011), which, to the best of our knowledge,
is the best-reported system in the literature based
on gold standard argument candidates. The pro-
posed joint framework with local features achieves
comparable performance for triggers and outper-
forms the staged baseline especially on arguments.
By adding global features, the overall performance
is further improved significantly. Compared to
the staged baseline, it gains 1.6% improvement
on trigger?s F-measure and 8.8% improvement on
argument?s F-measure. Remarkably, compared to
the cross-entity approach reported in (Hong et al,
2011), which attained 68.3% F1 for triggers and
48.3% for arguments, our approach with global
features achieves even better performance on ar-
gument labeling although we only used sentence-
level information.
We also tested the performance with argument
candidates automatically extracted by a high-
performing name tagger (Li et al, 2012b) and an
IE system (Grishman et al, 2005). The results
are summarized in Table 7. The joint approach
with global features significantly outperforms the
baseline and the model with only local features.
We also show that it outperforms the sentence-
level baseline reported in (Ji and Grishman, 2008;
Liao and Grishman, 2010), both of which at-
tained 59.7% F1 for triggers and 36.6% for argu-
ments. Our approach aims to tackle the problem of
sentence-level event extraction, thereby only used
intra-sentential evidence. Nevertheless, the perfor-
mance of our approach is still comparable with the
best-reported methods based on cross-document
and cross-event inference (Ji and Grishman, 2008;
Liao and Grishman, 2010).
5 Related Work
Most recent studies about ACE event extraction
rely on staged pipeline which consists of separate
local classifiers for trigger labeling and argument
labeling (Grishman et al, 2005; Ahn, 2006; Ji and
Grishman, 2008; Chen and Ji, 2009; Liao and Gr-
ishman, 2010; Hong et al, 2011; Li et al, 2012a;
Chen and Ng, 2012). To the best of our knowl-
edge, our work is the first attempt to jointly model
these two ACE event subtasks.
80
Methods
Trigger
Identification (%)
Trigger Identification
+ classification (%)
Argument
Identification (%) Argument Role (%)P R F1 P R F1 P R F1 P R F1
Sentence-level in Hong et al (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5
Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9
Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5
Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
Cross-entity in Hong et al (2011)? N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
Table 6: Overall performance with gold-standard entities, timex, and values. ?beyond sentence level.
Methods Trigger F1 Arg F1
Ji and Grishman (2008)
cross-doc Inference
67.3 42.6
Ji and Grishman (2008)
sentence-level
59.7 36.6
MaxEnt classifiers 64.7 (?1.2) 33.7 (?10.2)
Joint w/ local 63.7 (?2.0) 35.8 (?10.7)
Joint w/ local + global 65.6 (?1.9) 41.8 (?10.9)
Table 7: Overall performance (%) with predicted
entities, timex, and values. ? indicates the perfor-
mance drop from experiments with gold-standard
argument candidates (see Table 6).
For the Message Understanding Conference
(MUC) and FAS Program for Monitoring Emerg-
ing Diseases (ProMED) event extraction tasks,
Patwardhan and Riloff (2009) proposed a proba-
bilistic framework to extract event role fillers con-
ditioned on the sentential event occurrence. Be-
sides having different task definitions, the key
difference from our approach is that their role
filler recognizer and sentential event recognizer
are trained independently but combined in the test
stage. Our experiments, however, have demon-
strated that it is more advantageous to do both
training and testing with joint inference.
There has been some previous work on joint
modeling for biomedical events (Riedel and Mc-
Callum, 2011a; Riedel et al, 2009; McClosky et
al., 2011; Riedel and McCallum, 2011b). (Mc-
Closky et al, 2011) is most closely related to our
approach. They casted the problem of biomedi-
cal event extraction as a dependency parsing prob-
lem. The key assumption that event structure can
be considered as trees is incompatible with ACE
event extraction. In addition, they used a separate
classifier to predict the event triggers before ap-
plying the parser, while we extract the triggers and
argument jointly. Finally, the features in the parser
are edge-factorized. To exploit global features,
they applied a MaxEnt-based global re-ranker. In
comparison, our approach is a unified framework
based on beam search, which allows us to exploit
arbitrary global features efficiently.
6 Conclusions and Future Work
We presented a joint framework for ACE event ex-
traction based on structured perceptron with inex-
act search. As opposed to traditional pipelined
approaches, we re-defined the task as a struc-
tured prediction problem. The experiments proved
that the perceptron with local features outperforms
the staged baseline and the global features further
improve the performance significantly, surpassing
the current state-of-the-art by a large margin.
As shown in Table 7, the overall performance
drops substantially when using predicted argu-
ment candidates. To improve the accuracy of end-
to-end IE system, we plan to develop a complete
joint framework to recognize entities together with
event mentions for future work. Also we are inter-
ested in applying this framework to other IE tasks
such as relation extraction.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149,
U.S. NSF EAGER Award under Grant No. IIS-
1144111, U.S. DARPA Award No. FA8750-13-2-
0041 in the ?Deep Exploration and Filtering of
Text? (DEFT) Program, a CUNY Junior Faculty
Award, and Queens College equipment funds. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
81
References
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1?8.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Zheng Chen and Heng Ji. 2009. Language specific
issue and feature exploration in chinese event ex-
traction. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 209?212.
Chen Chen and Vincent Ng. 2012. Joint modeling for
chinese event extraction with rich linguistic features.
In COLING, pages 529?544.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description.
In Proceedings of ACE 2005 Evaluation Workshop.
Washington.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of ACL, pages 1127?1136.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254?262.
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012a. Employing compositional seman-
tics and discourse consistency in chinese event ex-
traction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1006?1016.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012b. Joint bilingual name tagging for
parallel corpora. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 1727?1731.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of ACL, pages 789?797.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX, volume 98, pages 187?193.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL, pages 1626?1635.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL,
volume 4, pages 337?342.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 151?
160.
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1?
12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 46?50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings
of the Workshop on Current Trends in Biomedical
Natural Language Processing: Shared Task, pages
41?49.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521?529.
82
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 239?249,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linking Tweets to News: A Framework to Enrich Short Text Data in
Social Media
Weiwei Guo? and Hao Li? and Heng Ji? and Mona Diab?
?Department of Computer Science, Columbia University
?Computer Science Department and Linguistic Department,
Queens College and Graduate Center, City University of New York
?Department of Computer Science, George Washington University
weiwei@cs.columbia.edu, {haoli.qc,hengjicuny}@gmail.com, mtdiab@gwu.edu
Abstract
Many current Natural Language Process-
ing [NLP] techniques work well assum-
ing a large context of text as input data.
However they become ineffective when
applied to short texts such as Twitter feeds.
To overcome the issue, we want to find
a related newswire document to a given
tweet to provide contextual support for
NLP tasks. This requires robust model-
ing and understanding of the semantics of
short texts.
The contribution of the paper is two-fold:
1. we introduce the Linking-Tweets-to-
News task as well as a dataset of linked
tweet-news pairs, which can benefit many
NLP applications; 2. in contrast to previ-
ous research which focuses on lexical fea-
tures within the short texts (text-to-word
information), we propose a graph based
latent variable model that models the in-
ter short text correlations (text-to-text in-
formation). This is motivated by the ob-
servation that a tweet usually only cov-
ers one aspect of an event. We show that
using tweet specific feature (hashtag) and
news specific feature (named entities) as
well as temporal constraints, we are able to
extract text-to-text correlations, and thus
completes the semantic picture of a short
text. Our experiments show significant im-
provement of our new model over base-
lines with three evaluation metrics in the
new task.
1 Introduction
Recently there has been an increasing interest in
language understanding of Twitter messages. Re-
searchers (Speriosui et al, 2011; Brody and Di-
akopoulos, 2011; Jiang et al, 2011) were in-
terested in sentiment analysis on Twitter feeds,
and opinion mining towards political issues or
politicians (Tumasjan et al, 2010; Conover et al,
2011). Others (Ramage et al, 2010; Jin et al,
2011) summarized tweets using topic models. Al-
though these NLP techniques are mature, their
performance on tweets inevitably degrades, due to
the inherent sparsity in short texts. In the case
of sentiment analysis, while people are able to
achieve 87.5% accuracy (Maas et al, 2011) on a
movie review dataset (Pang and Lee, 2004), the
performance drops to 75% (Li et al, 2012) on
a sentence level movie review dataset (Pang and
Lee, 2005). The problem worsens when some
existing NLP systems cannot produce any results
given the short texts. Considering the following
tweet:
Pray for Mali...
A typical event extraction/discovery system (Ji
and Grishman, 2008) fails to discover the war
event due to the lack of context information (Ben-
son et al, 2011), and thus fails to shed light on the
users focus/interests.
To enable the NLP tools to better understand
Twitter feeds, we propose the task of linking a
tweet to a news article that is relevant to the tweet,
thereby augmenting the context of the tweet. For
example, we want to supplement the implicit con-
text of the above tweet with a news article such as
the following entitled:
State of emergency declared in Mali
where abundant evidence can be fed into an off-
the-shelf event extraction/discovery system. To
create a gold standard dataset, we download tweets
spanning over 18 days, each with a url linking to a
news article of CNN or NYTIMES, as well as all
the news of CNN and NYTIMES published during
the period. The goal is to predict the url referred
news article based on the text in each tweet.1 We
1The data and code is publicly available at www.cs.
239
believe many NLP tasks will benefit from this task.
In fact, in the topic modeling research, previous
work (Jin et al, 2011) already showed that by in-
corporating webpages whose urls are contained
in tweets, the tweet clustering purity score was
boosted from 0.280 to 0.392.
Given the few number of words in a tweet (14
words on average in our dataset), the traditional
high dimensional surface word matching is lossy
and fails to pinpoint the news article. This con-
stitutes a classic short text semantics impediment
(Agirre et al, 2012). Latent variable models are
powerful by going beyond the surface word level
and mapping short texts into a low dimensional
dense vector (Socher et al, 2011; Guo and Diab,
2012b). Accordingly, we apply a latent variable
model, namely, the Weighted Textual Matrix Fac-
torization [WTMF] (Guo and Diab, 2012b; Guo
and Diab, 2012c) to both the tweets and the news
articles. WTMF is a state-of-the-art unsupervised
model that was tested on two short text similar-
ity datasets: (Li et al, 2006) and (Agirre et al,
2012), which outperforms Latent Semantic Anal-
ysis [LSA] (Landauer et al, 1998) and Latent
Dirichelet Allocation [LDA] (Blei et al, 2003) by
a large margin. We employ it as a strong baseline
in this task as it exploits and effectively models the
missing words in a tweet, in practice adding thou-
sands of more features for the tweet, by contrast
LDA, for example, only leverages observed words
(14 features) to infer the latent vector for a tweet.
Apart from the data sparseness, our dataset pro-
poses another challenge: a tweet usually covers
only one aspect of an event. In our previous ex-
ample, the tweet only contains the location Mali
while the event is about French army participated
in Mali war. In this scenario, we would like to find
the missing elements of the tweet such as French,
war from other short texts, to complete the seman-
tic picture of Pray in Mali tweet. One drawback
of WTMF for our purposes is that it simply mod-
els the text-to-word information without leverag-
ing the correlation between short texts. While
this is acceptable on standard short text similarity
datasets (data points are independently generated),
it ignores some valuable information characteristi-
cally present in our dataset: (1) The tweet specific
features such as hashtags. Hashtags prove to be
a direct indication of the semantics of tweets (Ra-
mage et al, 2010); (2) The news specific features
columbia.edu/?weiwei
such as named entities in a document. Named en-
tities acquired from a news document, typically
with high accuracy using Named Entity Recog-
nition [NER] tools, may be particularly informa-
tive. If two texts mention the same entities then
they might describe the same event; (3) The tem-
poral information in both genres (tweets and news
articles). We note that there is a higher chance
of event description overlap between two texts if
their time of publication is similar.
In this paper, we study the problem of min-
ing and exploiting correlations between texts us-
ing these features. Two texts may be considered
related or complementary if they share a hash-
tag/NE or satisfies the temporal constraints. Our
proposed latent variable model not only models
text-to-word information, but also is aware of the
text-to-text information (illustrated in Figure 1):
two linked texts should have similar latent vec-
tors, accordingly the semantic picture of a tweet is
completed by receiving semantics from its related
tweets. We incorporate this additional information
in the WTMF model. We also show the differ-
ent impact of the text-to-text relations in the tweet
genre and news genre. We are able to achieve sig-
nificantly better results than with a text-to-words
WTMF model. This work can be regarded as a
short text modeling approach that extends previ-
ous work however with a focus on combining the
mining of information within short texts coupled
with utilizing extra shared information across the
short texts.
2 Task and Data
The task is given the text in a tweet, a system aims
to find the most relevant news article. For gold
standard data, we harvest all the tweets that have a
single url link to a CNN or NYTIMES news arti-
cle, dated from the 11th of Jan to the 27th of Jan,
2013. In evaluation, we consider this url-referred
news article as the gold standard ? the most rele-
vant document for the tweet, and remove the url
from the text of the tweet. We also collect all the
news articles from both CNN and NYTIMES from
RSS feeds during the same timeframe. Each tweet
entry has the published time, author, text; each
news entry contains published time, title, news
summary, url. The tweet/news pairs are extracted
by matching urls. We manually filtered ?trivial?
tweets where the tweet content is simply the news
title or news summary. The final dataset results in
240
(a)	 ?
t1	 ? t2	 ?
n2	 ?n1	 ?
t3	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ? w6	 ? w7	 ? w8	 ?
t1	 ? t2	 ?
n2	 ?n1	 ?
t3	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ? w6	 ? w7	 ? w8	 ?
#healthcare	 ? Obama	 ?
temporal	 ? (b)	 ?
Figure 1: (a) WTMF. (b) WTMF-G: the tweet nodes t and news nodes n are connected by hashtags, named entities or
temporal edges (for simplicity, the missing tokens are not shown in the figure)
34,888 tweets and 12,704 news articles.
It is worth noting that the news corpus is not
restricted to current events. It covers various gen-
res and topics, such as travel guides. e.g. World?s
most beautiful lakes, and health issues, e.g. The
importance of a ?stop day?, etc.
2.1 Evaluation metric
For our task evaluation, ideally, we would like
the system to be able to identify the news arti-
cle specifically referred to by the url within each
tweet in the gold standard. However, this is very
difficult given the large number of potential can-
didates, especially those with slight variations.
Therefore, following the Concept Definition Re-
trieval task in (Guo and Diab, 2012b) and (Steck,
2010) we use a metric for evaluating the ranking
of the correct news article to evaluate the systems,
namely, ATOPt, area under the TOPKt(k) recall
curve for a tweet t. Basically, it is the normal-
ized ranking ? [0, 1] of the correct news article
among all candidate news articles: ATOPt = 1
means the url-referred news article has the highest
similarity value with the tweet (a correct NARU);
ATOPt = 0.95 means the similarity value with
correct news article is larger than 95% of the can-
didates, i.e. within the top 5% of the candidates.
ATOPt is calculated as follows:
ATOPt =
? 1
0
TOPKt(k)dk (1)
where TOPKt(k) = 1 if the url referred news arti-
cle is in the ?top k? list, otherwise TOPKt(k) = 0.
Here k ? [0, 1] is the relative position (when
k = 1, it means all the candidates).
We also include other metrics to examine if the
system is able to rank the url referred news arti-
cle in the first few returned results: TOP10 recall
hit rate to evaluate whether the correct news is in
the top 10 results, and RR, Reciprocal Rank= 1/r
(i.e., RR= 1/3 when the correct news article is
ranked at the 3rd highest place).
3 Weighted Textual Matrix Factorization
The WTMF model (Guo and Diab, 2012a) has
been successfully applied to the short text simi-
larity task, achieving state-of-the-art unsupervised
performance. This can be attributed to the fact that
it models the missing tokens as features, thereby
adding many more features for a short text. The
missing words of a sentence are defined as all the
vocabulary of the training corpus minus the ob-
served words in a sentence. Missing words serve
as negative examples for the semantics of a short
text: the short text should not be related to the
missing words.
As per (Guo and Diab, 2012b), the corpus is
represented in a matrix X , where each cell stores
the TF-IDF values of words. The rows of X are
words and columns are short texts. As in Figure
2, matrix X is approximated by the product of a
K?M matrix P and a K?N matrix Q. Accord-
ingly, each sentence sj is represented by a K di-
mensional latent vector Q?,j . Similarly a word wi
is generalized by P?,i. Therefore, the inner product
of a word vector P?,i and a short text vector Q?,j is
to approximate the cell Xij (shaded part in Figure
2). In this way, the missing words are modeled by
requiring the inner product of a word vector and
short text vector to be close to 0 (the word and the
short text should be irrelevant).
Since 99% cells in X are missing tokens (0
value), the impact of observed words is signifi-
cantly diminished. Therefore a small weight wm
is assigned for each 0 cell (missing tokens) in the
matrix X in order to preserve the influence of ob-
served words. P andQ are optimized by minimize
the objective function:
241
Figure 2: Weighted Textual Matrix Factorization
?
i
?
j
Wij (P?,i ?Q?,j ?Xij)2 + ?||P ||22 + ?||Q||22
Wi,j =
{
1, if Xij 6= 0
wm, if Xij = 0
(2)
where ? is a regularization term.
4 Creating Text-to-text Relations via
Twitter/News Features
WTMF exploits the text-to-word information in a
very nuanced way, while the dependency between
texts is ignored. In this Section, we introduce how
to create text-to-text relations.
4.1 Hashtags and Named Entities
Hashtags highlight the topics in tweets, e.g., The
#flu season has started. We believe two tweets
sharing the same hashtag should be related, hence
we place a link between them to explicitly inform
the model that these two tweets should be similar.
We find only 8,701 tweets out of 34,888 include
hashtags. In fact, we observe many hashtag words
are mentioned in tweets without explicitly being
tagged with #. To overcome the hashtag sparse-
ness issue, one can resort to keywords recommen-
dation algorithms to mine hashtags for the tweets
(Yang et al, 2012). In this paper, we adopt a sim-
ple but effective approach: we collect all the hash-
tags in the dataset, and automatically hashtag any
word in a tweet if that word appears hashtagged in
any other tweets. This process resulted in 33,242
tweets automatically labeled with hashtags. For
each tweet, and for each hashtag it contains, we
extract k tweets that contain this hashtag, assum-
ing they are complementary to the target tweet,
and link the k tweets to the target tweet. If there
are more than k tweets found, we choose the top
k ones that are most chronologically close to the
target tweet. The statistics of links can be found in
table 2.
Named entities are some of the most salient fea-
tures in a news article. Directly applying Named
Entity Recognition (NER) tools on news titles or
tweets results in many errors (Liu et al, 2011) due
to the noise in the data, such as slang and capital-
ization. Accordingly, we first apply the NER tool
on news summaries, then label named entities in
the tweets in the same way as labeling the hash-
tags: if there is a string in the tweet that matches
a named entity from the summaries, then it is la-
beled as a named entity in the tweet. 25,132 tweets
are assigned at least one named entity.2 To create
the similar tweet set, we find k tweets that also
contain the named entity.
4.2 Temporal Relations
Tweets published in the same time interval have
a larger chance of being similar than those are
not chronologically close (Wang and McCallum,
2006). However, we cannot simply assume any
two tweets are similar only based on the times-
tamp. Therefore, for a tweet we link it to the
k most similar tweets whose published time is
within 24 hours of the target tweet?s timestamp.
We use the similarity score returned by WTMF
model to measure the similarity of two tweets.
We experimented with other features such as au-
thorship. We note that it was not a helpful feature.
While authorship information helps in the task of
news/tweets recommendation for a user (Corso et
al., 2005; Yan et al, 2012), the authorship infor-
mation is too general for this task where we target
on ?recommending? a news article for a tweet.
4.3 Creating Relations on News
We extract the 3 subgraphs (based on hash-
tags, named entities and temporal) on news ar-
ticles. However, automatically tagging hashtags
or named entities leads to much worse perfor-
mance (around 93% ATOP values, a 3% decrease
from baseline WTMF). There are several reasons
for this: 1. When a hashtag-matched word ap-
pears in a tweet, it is often related to the central
meaning of the tweet, however news articles are
generally much longer than tweets, resulting in
many more hashtags/named entities matches even
though these named entities may not be closely re-
lated. 2. The noise introduced during automatic
NER accumulates much faster given the large
number of named entities in news data. There-
fore we only extract temporal relations for news
articles.
2Note that there are some false positive named entities
detected such as apple. We plan to address removing noisy
named entities and hashtags in future work
242
5 WTMF on Graphs
We propose a novel model to incorporate the links
generated as described in the previous section.
If two texts are connected by a link, it means
they should be semantically similar. In the WTMF
model, we would like the latent vectors of two
text nodes Q?,j1 , Q?,j2 to be as similar as possible,
namely that their cosine similarity to be close to 1.
To implement this, we add a regularization term in
the objective function of WTMF (equation 2) for
each linked pairs Q?,j1 , Q?,j2 :
? ? ( Q?,j1 ?Q?,j2|Q?,j1 ||Q?,j2 |
? 1)2 (3)
where |Q?,j | denotes the length of vectorQ?,j . The
coefficient ? denotes the importance of the text-to-
text links. A larger ? means we put more weights
on the text-to-text links and less on the text-to-
word links. We refer to this model as WTMF-G
(WTMF on graphs).
5.1 Inference
Alternating Least Square [ALS] is used for in-
ference in weighted matrix factorization (Srebro
and Jaakkola, 2003). However, ALS is no longer
applicable here with the new regularization term
(equation 3) involving the length of text vectors
|Q?,j |, which is not in quadratic form. Therefore
we approximate the objective function by treating
the vector length |Q?,j | as fixed values during the
ALS iterations:
P?,i =
(
QW? (i)Q> + ?I
)?1
QW? (i)X?,i
Q?,j =
(
PW? (j)P> + ?I + ?L2(j)Q?,s(j)diag(L2(s(j)))Q>?,s(j)
)?1
(
PW? (j)X>j,? + ?L(j)Q?,s(j)Ln(j)
)
(4)
We define n(j) as the linked neighbors of short
text j, and Q?,n(j) as the set of latent vectors of
j?s neighbors. The reciprocal of length of these
vectors in the current iteration are stored in Ls(j).
Similarly, the reciprocal of the length of the short
text vector Q?,j is Lj . W? (i) = diag(W?,i) is an
M ?M diagonal matrix containing the ith row of
weight matrixW . Due to limited space, the details
of the optimization are not shown in this paper;
they can be found in (Steck, 2010).
6 Experiments
6.1 Experiment Setting
Corpora: We use the same corpora as in (Guo
and Diab, 2012b): Brown corpus (each sentence is
treated as a document), sense definitions of Wik-
tionary and Wordnet (Fellbaum, 1998). The tweets
and news articles are also included in the cor-
pus, generating 441,258 short texts and 5,149,122
words. The data is tokenized, POS-tagged by
Stanford POS tagger (Toutanova et al, 2003),
and lemmatized by WordNet::QueryData.pm. The
value of each word in matrixX is its TF-IDF value
in the short text.
Baselines: We present 4 baselines: 1. Informa-
tion Retrieval model [IR], which simply treats a
tweet as a document, and performs traditional sur-
face word matching. 2. LDA-? with Gibbs Sam-
pling as inference method. We use the inferred
topic distribution ? as a latent vector to represent
the tweet/news. 3. LDA-wvec. The problem with
LDA-? is the inferred topic distribution latent vec-
tor is very sparse with only a few non-zero val-
ues, resulting in many tweet/news pairs receiving
a high similarity value as long as they are in the
same topic domain. Hence following (Guo and
Diab, 2012b), we first compute the latent vector
of a word by P (z|w) (topic distribution per word),
then average the word latent vectors weighted by
TF-IDF values to represent the short text, which
yields much better results. 4. WTMF. In these
baselines, hashtags and named entities are simply
treated as words.
To curtail variation in results due to random-
ness, each reported number is the average of 10
runs. For WTMF and WTMF-G, we assign the
same initial random values and run 20 iterations.
In both systems we fix the missing words weight
as wm = 0.01 and regularization coefficient at
? = 20, which is the best condition of WTMF
found in (Guo and Diab, 2012b; Guo and Diab,
2012c). For LDA-? and LDA-wvec, we run Gibbs
Sampling based LDA for 2000 iterations and aver-
age the model over the last 10 iterations.
Evaluation: The similarity between a tweet and
a news article is measured by cosine similarity. A
news article is represented as the concatenation of
its title and its summary, which yields better per-
formance.3
As in (Guo and Diab, 2012b), for each tweet,
we collect the 1,000 news articles published prior
to the tweet whose dates of publication are clos-
est to that of the tweet. 4 The cosine similarity
3While these are separated, WTMF receive ATOP
95.558% for representing news article as titles and 94.385%
for representing news article as summaries
4Ideally we want to include all the news articles published
243
Models Parameters ATOP TOP10 RRdev test dev test dev test
IR - 90.795% 90.743% 73.478% 74.103% 46.024% 46.281%
LDA-? ? = 0.05, ? = 0.05 81.368% 81.251% 32.328% 31.207% 13.134% 12.469%
LDA-wvec ? = 0.05, ? = 0.05 94.148% 94.196% 53.500% 53.952% 28.743% 27.904%
WTMF - 95.964% 96.092% 75.327% 76.411% 45.310% 46.270%
WTMF-G k = 3, ? = 3 96.450% 96.543% 76.485% 77.479% 47.516% 48.665%
WTMF-G k = 5, ? = 3 96.613% 96.701% 76.029% 77.176% 47.197% 48.189%
WTMF-G k = 4, ? = 3 96.510% 96.610% 77.782% 77.782% 47.917% 48.997%
Table 1: ATOP Performance (latent dimension D = 100 for LDA/WTMF/WTMF-G)
0 1 2 3 4
96
96.2
96.4
96.6
96.8
ATO
P
?
 
 
dev
test
(a) ATOP
0 1 2 3 475
75.5
76
76.5
77
77.5
78
TOP
10
?
 
 
dev
test
(b) TOP10
0 1 2 3 445
46
47
48
49
RR
?
 
 
dev
test
(c) RR
Figure 3: Impact of ? (D = 100, k = 4)
score between the url referred news article and the
tweet is compared against the scores of these 1,000
news articles to calculate the metric scores. 1/10 of
the tweet/news pairs are used as development set,
based on which all the parameters are tuned. The
metrics ATOP, TOP10 and RR are used to evaluate
the performance of systems.
6.2 Results
Table 1 summarizes the performance of the base-
lines and WTMF-G at latent dimension D = 100.
All the parameters are chosen based on the de-
velopment set. For WTMF-G, we try different
values of k (the number of neighbors linked to a
tweet/news for a hashtag/NE/time constraint) and
? (the weight of link information). We choose to
model the links in four subgraphs: (a) hashtags
in tweet; (b) named entities in tweet; (c) time in
tweet; (d) time in news article. For LDA we tune
the hyperparameter ? (Dirichlet prior for topic dis-
tribution of a document) and ? (Dirichlet prior for
word distribution given a topic). It is worth noting
that ATOP measures the overall ranking in 1000
samples while TOP10/RR focus on whether the
aligned news article is in the first few returned re-
sults.
Same as reported in (Guo and Diab, 2012b),
LDA-? has the worst results due to directly using
prior to the tweet, however, that will give a bias to some
tweets, since the latter tweets have a larger candidate set than
the earlier ones
the inferred topic distribution of a text ?. The in-
ferred topic vector has only a few non-zero values,
hence a lot of information is missing. LDA-wvec
preserves more information by creating a dense la-
tent vector from the topic distribution of a word
P (z|w), and thus does much better in ATOP.
It is interesting to see that IR model has a
very low ATOP (90.795%) and an acceptable RR
(46.281%) score, in contrast to LDA-wvec with
a high ATOP (94.148%) and a low RR(27.904%)
score. This is caused by the nature of the two mod-
els. LDA-wvec is able to identify global coarse
grained topic information (such as politics vs. eco-
nomics), hence receiving a high ATOP by exclud-
ing the most irrelevant news articles, however it
does not distinguish fine grained difference such
as Hillary vs. Obama. IR model exerts the oppo-
site influence via word matching. It ranks a cor-
rect news article very high if overlapping words
exist (leading to a high RR), or the news article is
ranked very low if no overlapping words (hence a
low ATOP).
We can conclude WTMF is a very strong base-
line given that it achieves high scores with three
metrics. As a latent variable model, it is able to
capture global topics (+1.89% ATOP over LDA-
wvec); moreover, by explicitly modeling missing
words, the existence of a word is also encoded in
the latent vector (+2.31% TOP10 and ?0.011%
RR over IR model).
244
50 75 100 125 15095
95.5
96
96.5
97
ATO
P
D
 
 
WTMFWTMF?G
(a) ATOP
50 75 100 125 15070
72
74
76
78
80
TOP
10
D
 
 
WTMFWTMF?G
(b) TOP10
50 75 100 125 15040
42
44
46
48
50
RR
D
 
 
WTMFWTMF?G
(c) RR
Figure 4: Impact of latent dimension D (k = 4)
Conditions Links ATOP TOP10 RRdev test dev test dev test
hashtags tweets 375,371 +0.397% +0.379% +1.015% +1.021% +0.504% +0.641%
NE tweets 164,412 +0.141% +0.130% +0.598% +0.479% +0.278% +0.294%
time tweet 139,488 +0.126% +0.136% +0.512% +0.503% +0.241% +0.327%
time news 50,008 +0.036% +0.026% +0.156% +0.256% +1.890% +1.924%
full model (all 4 subgraphs) 573,999 +0.546% +0.518% +1.556% +1.371% +2.607% +2.727%
full model minus hashtags tweets 336,963 +0.288% +0.276% +1.129% +1.037% +2.488% +2.541%
full model minus NE tweets 536,333 +0.528% +0.503% +1.518% +1.393% +2.580% +2.680%
full model minus time tweet 466,207 +0.457% +0.426% +1.281% +1.145% +2.449% +2.554%
full model minus time news 523,991 +0.508% +0.490% +1.300% +1.190% +0.632% +0.785%
author tweet 21,318 +0.043% +0.042% +0.028% +0.057% ?0.003% ?0.017%
full model plus author tweet 593,483 +0.575% +0.545% +1.465% +1.336% +2.415% +2.547%
Table 2: Contribution of subgraphs when D = 100, k = 4, ? = 3 (gain over baseline WTMF)
With WTMF being a very challenging baseline,
WTMF-G can still significantly improve all 3 met-
rics. In the case k = 4, ? = 3 compared to WTMF,
WTMF-G receives +1.371% TOP10, +2.727%
RR, and +0.518% ATOP value (this is a signifi-
cant improvement of ATOP value considering that
it is averaged on 30,000 data points, at an already
high level of 96% reducing error rate by 13%). All
the improvement of WTMF-G over WTMF is sta-
tistically signicant at the 99% condence level with
a two-tailed paired t-test.
We also present results using different number
of links k in WTMF-G in table 1. We experi-
ment with k = {3, 4, 5}. k = 4 is found to
be the optimal value (although k = 5 has a bet-
ter ATOP). Figure 3 demonstrates the impact of
? = {0, 1, 2, 3, 4} on each metric when k = 4.
Note when ? = 0 no link is used, which is the
baseline WTMF. We can see using links is always
helpful. When ? = 4, we receive a higher ATOP
value but lower TOP10 and RR.
Figure 4 illustrates the impact of dimension
D = {50, 75, 100, 125, 150} on WTMF and
WTMF-G (k = 4) on the test set. The trends
hold in different D values with a consistent im-
provement. Generally a larger D leads to a better
performance. In all conditions WTMF-G outper-
forms WTMF.
6.3 Contribution of Subgraphs
We are interested in the contribution of each fea-
ture subgraph. Therefore we list the impact of
individual components in table 2. The impact of
each subgraph is evaluated in two conditions: (a)
the subgraph-only; (b) the full-model-minus the
subgraph. The full model is the combination of the
4 subgraphs (which is also the best model k = 4
in table 1). In the last two rows of table 2 we also
present the results of using authorship only and the
full model plus authorship. The 2nd column lists
the number of links in the subgraph. To highlight
the difference, we report the gain of each model
over the baseline model WTMF.
We have several interesting observations from
table 2. It is clear that the hashtag sub-
graph on tweets is the most useful subgraph:
with hashtag tweet it has the best ATOP and
TOP10 values among subgraph-only condition
(ATOP: +0.379% vs. 2nd best +0.136%, TOP10:
+1.021% vs. 2nd best +0.503%), while in the
full-model-minus condition, minus hashtag has
the lowest ATOP and TOP10. Observing that it
also contains the most links, we believe the cover-
age is another important reason for the great per-
formance.
It seems the named entity subgraph helps the
least. Looking into the extracted named entities
and hashtags, we find many popular named enti-
245
ties are covered by hashtags. That said, adding
named entity subgraph into final model has a pos-
itive contribution.
It is worth noting that the time news subgraph
has the most positive influence on RR. This is be-
cause temporal information is very salient in news
domain: usually there are several reports to de-
scribe an event within a short period, therefore the
news latent vector is strengthened by receiving se-
mantics from its neighbors.
At last, we analyze the influence of author-
ship of tweets. Adding authorship into the full
model greatly hurts the scores of TOP10 and RR,
whereas it is helpful to ATOP. This is understand-
able since by introducing author links between
tweets, to some degree we are averaging the la-
tent vectors of tweets written by the same per-
son. Therefore, for a tweet whose topic is vague
and hard to detect, it will get some prior knowl-
edge of topics through the author links (hence in-
crease ATOP), whereas this prior knowledge be-
comes noise for the tweets that are already handled
very well by the model (hence decrease TOP10
and RR).
6.4 Error Analysis
We look closely into ATOP results to obtain an in-
tuitive feel for what is captured and what is not.
For example, the ATOP score of WTMF for the
tweet-news pair below is 89.9%:
Tweet: ...stoked growing speculation that Pak-
istan?s powerful military was quietly supporting
moves... @declanwalsh
News: Pakistan Supreme Court Orders Arrest of
Prime Minister
By identifying ?Pakistan? and ?Supreme Court?
as hashtags/named entity, WTMF-G is able to
propagate the semantics from the following two
informative tweets to the original tweet, hence
achieving a higher ATOP score of 91.9%.
#Pakistan Supreme Court orders the arrest of the
PM on corruption charges.
A discouraging sign from a tumultuous political
system: Pakistan?s Supreme Court ordered the ar-
rest of PM Ashraf today.
Below is an example that shows the deficiency of
both WTMF and WTMF-G:
Tweet: Another reason to contemplate moving: an
early death
News: America flunks its health exam
In this case WTMF and WTMF-G achieve a
low ATOP of 69.8% and 75.1%, respectively. The
only evidence the latent variable models rely on
is lexical items (WTMF-G extract additional text-
to-text correlation by word matching). To pin-
point the url referred news articles, other advanced
NLP features should be exploited. In this case, we
believe sentiment information could be helpful ?
both tweet and the news article contain a negative
polarity.
7 Related Work
Short Text Semantics: The field of short text se-
mantics has progressed immensely in recent years.
Early work focus on word pair similarity in the
high dimensional space. The word pair similarity
is either knowledge based (Mihalcea et al, 2006;
Tsatsaronis et al, 2010) or corpus based (Li et
al., 2006; Islam and Inkpen, 2008), where co-
occurrence information cannot be efficiently ex-
ploited. Guo and Diab (2012b; 2012a; 2013) show
the superiority of the latent space approach in the
WTMF model achieving state-of-the-art perfor-
mance on two datasets. However, all of them only
reply on text-to-word information. In this paper,
we focus on modeling inter-text relations induced
by Twitter/news features. We extend the WTMF
model and adapt it into tweets modeling, achiev-
ing significantly better results.
Modeling Tweets in a Latent Space: Ramage
et al (2010) also use hashtags to improve the la-
tent representation of tweets in a LDA framework,
Labeled-LDA (Ramage et al, 2009), treating each
hashtag as a label. Similar to the experiments pre-
sented in this paper, the result of using Labeled-
LDA alone is worse than the IR model, due to the
sparseness in the induced LDA latent vector. Jin et
al. (2011) apply an LDA based model on cluster-
ing by incorporating url referred documents. The
semantics of long documents are transferred to the
topic distribution of tweets.
News recommendation: A news recommen-
dation system aims to recommend news articles
to a user based on the features (e.g., key words,
tags, category) in the documents that the user likes
(hence these documents form a training set) (Clay-
pool et al, 1999; Corso et al, 2005; Lee and Park,
2007). Our paper resembles it in searching for a
related news article. However, we target on rec-
ommending news article only based on a tweet,
which is a much smaller context than the set of
favorite documents chosen by a user .
246
Research on Tweets: In (Duan et al, 2010), url
availability is an important feature for tweets rank-
ing. However, the number of tweets with an ex-
plicit url is very limited. Huang et al (2012) pro-
pose a graph-based framework to propagate tweet
ranking scores, where relevant web documents is
found to be helpful to discover informative tweets.
Both work can take advantage of our work to ei-
ther extract potential url features or retrieve topi-
cally similar web documents.
(Sankaranarayanan et al, 2009) aims at captur-
ing tweets that correspond to late breaking news.
However, they cluster tweets and simply choose
a url referred news in those tweets as the related
news for the whole cluster (the urls are visible
to the systems). (Abel et al, 2011) is most re-
lated work to our paper, however their focus is the
user profiling task, therefore they do not provide
a paired tweet/news data set and have to conduct
manual evaluation.
8 Conclusion
We propose a Linking-Tweets-to-News task,
which potentially benefits many NLP applications
where off-the-shelf NLP tools can be applied to
the most relevant news. We also collect a gold
standard dataset by crawling tweets each with a url
referring to a news article. We formalize the link-
ing task as a short text modeling problem, and ex-
tract Twitter/news specific features to extract text-
to-text relations, which are incorporated into a la-
tent variable model. We achieve significant im-
provement over baselines.
Acknowledgements
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF- 09-2-0053 (NS-CTA), the U.S.
NSF CAREER Award under Grant IIS-0953149,
the U.S. NSF EAGER Award under Grant No. IIS-
1144111, the U.S. DARPA FA8750-13-2-0041 -
Deep Exploration and Filtering of Text (DEFT)
Program and CUNY Junior Faculty Award. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011. Semantic enrichment of twitter posts for user
profile construction on the social web. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using
word lengthening to detect sentiment in microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing.
Mark Claypool, Anuja Gokhale, Tim Miranda, Pavel
Murnikov, Dmitry Netes, and Matthew Sartin. 1999.
Combining content-based and collaborative filters in
an online newspaper. In In Proceedings of the ACM
SIGIR Workshop on Recommender Systems.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Gianna M. Del Corso, Antonio Gulli, and Francesco
Romani. 2005. Ranking a stream of news. In
WWW, pages 97?106.
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In COLING.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Weiwei Guo and Mona Diab. 2012a. Learning the la-
tent semantics of a concept by its definition. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics.
Weiwei Guo and Mona Diab. 2012c. Weiwei: A sim-
ple unsupervised latent semantics based approach
for sentence similarity. In First Joint Conference on
Lexical and Computational Semantics (*SEM).
247
Weiwei Guo and Mona Diab. 2013. Improving lexical
semantics for sentential semantics: Modeling selec-
tional preference and similar words in a latent vari-
able model. In The 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo
Deng, Dong Wang, Hieu Le, Tarek Abdelzather, Ji-
awei Han, Alice Leung, John Hancock, and Clare
Voss. 2012. Tweet ranking based on heterogeneous
networks. In Proceedings of the 24th International
Conference on Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL-08: HLT.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of Association for Computational Lin-
guistics.
Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Pro-
ceedings of the 20th ACM international conference
on Information and knowledge management.
Thomas K Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic anal-
ysis. Discourse Processes, 25.
H. J. Lee and Sung Joo Park. 2007. Moners: A
news recommender for the mobile web. Expert Syst.
Appl., 32(1):143?150.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence simi-
larity based on semantic nets and corpus statistics.
IEEE Transaction on Knowledge and Data Engi-
neering, 18.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and
Dequan Zheng. 2012. Combining social cognitive
theories with linguistic features for multi-genre sen-
timent analysis. In In Proceedings of the 26th Pa-
cific Asia Conference on Language, Information and
Computation.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In The Semanic Web: Research and Applications.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Articial In-
telligence.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Daniel Ramage, Susan Dumais, and Dan Liebling.
2010. Characterizing microblogs with topic mod-
els. In Proceedings of the Fourth International AAAI
Conference on Weblogs and Social Media.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings
of the 17th ACM SIGSPATIAL International Confer-
ence on Advances in Geographic Information Sys-
tems.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings
of Advances in Neural Information Processing Sys-
tems.
Michael Speriosui, Nikita Sudan, Sid Upadhyay, and
Jason Baldridge. 2011. Twitter polarity classifica-
tion with label propagation over lexical links and the
follower graph. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the
Twentieth International Conference on Machine
Learning.
Harald Steck. 2010. Training and testing of rec-
ommender systems on data missing not at random.
In Proceedings of the 16th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In HLT-NAACL.
248
George Tsatsaronis, Iraklis Varlamis, and Michalis
Vazirgiannis. 2010. Text relatedness based on a
word thesaurus. Journal of Articial Intelligence Re-
search, 37.
Andranik Tumasjan, Timm Oliver Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
elections with twitter: What 140 characters reveal
about political sentiment. In ICWSM.
Xuerui Wang and Andrew McCallum. 2006. Top-
ics over time: a non-markov continuous-time model
of topical trends. In In Proceedings of the 12th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012.
Tweet recommendation with graph co-ranking. In
Proceedings of the 24th International Conference on
Computational Linguistics.
Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei.
2012. We know what @you #tag: does the dual role
affect hashtag adoption? In Proceedings of the 21st
international conference on World Wide Web.
249
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?614,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Name-aware Machine Translation
Haibo Li? Jing Zheng? Heng Ji? Qi Li? Wen Wang?
? Computer Science Department and Linguistics Department
Queens College and Graduate Center, City University of New York
New York, NY, USA 10016
{lihaibo.c, hengjicuny, liqiearth}@gmail.com
? Speech Technology & Research Laboratory
SRI International
Menlo Park, CA, USA 94025
{zj, wwang}@speech.sri.com
Abstract
We propose a Name-aware Machine
Translation (MT) approach which can
tightly integrate name processing into MT
model, by jointly annotating parallel cor-
pora, extracting name-aware translation
grammar and rules, adding name phrase
table and name translation driven decod-
ing. Additionally, we also propose a new
MT metric to appropriately evaluate the
translation quality of informative words,
by assigning different weights to differ-
ent words according to their importance
values in a document. Experiments on
Chinese-English translation demonstrated
the effectiveness of our approach on en-
hancing the quality of overall translation,
name translation and word alignment over
a high-quality MT baseline1.
1 Introduction
A shrinking fraction of the world?s Web pages are
written in English, therefore the ability to access
pages across a range of languages is becoming in-
creasingly important. This need can be addressed
in part by cross-lingual information access tasks
such as entity linking (McNamee et al, 2011; Cas-
sidy et al, 2012), event extraction (Hakkani-Tur
et al, 2007), slot filling (Snover et al, 2011) and
question answering (Parton et al, 2009; Parton
and McKeown, 2010). A key bottleneck of high-
quality cross-lingual information access lies in the
performance of Machine Translation (MT). Tradi-
tional MT approaches focus on the fluency and
accuracy of the overall translation but fall short
in their ability to translate certain content word-
s including critical information, especially names.
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/NAMT.tgz
A typical statistical MT system can only trans-
late 60% person names correctly (Ji et al, 2009).
Incorrect segmentation and translation of names
which often carry central meanings of a sentence
can also yield incorrect translation of long con-
texts. Names have been largely neglected in the
prior MT research due to the following reasons:
? The current dominant automatic MT scoring
metrics (such as Bilingual Evaluation Under-
study (BLEU) (Papineni et al, 2002)) treat
all words equally, but names have relative low
frequency in text (about 6% in newswire and
only 3% in web documents) and thus are vast-
ly outnumbered by function words and com-
mon nouns, etc..
? Name translations pose a greater complexity
because the set of names is open and highly
dynamic. It is also important to acknowledge
that there are many fundamental differences
between the translation of names and other
tokens, depending on whether a name is ren-
dered phonetically, semantically, or a mixture
of both (Ji et al, 2009).
? The artificial settings of assigning low
weights to information translation (compared
to overall word translation) in some large-
scale government evaluations have discour-
aged MT developers to spend time and ex-
plore resources to tackle this problem.
We propose a novel Name-aware MT (NAMT)
approach which can tightly integrate name pro-
cessing into the training and decoding processes of
an end-to-end MT pipeline, and a new name-aware
metric to evaluate MT which can assign different
weights to different tokens according to their im-
portance values in a document. Compared to pre-
vious methods, the novel contributions of our ap-
proach are:
1. Tightly integrate joint bilingual name tag-
ging into MT training by coordinating tagged
604
names in parallel corpora, updating word seg-
mentation, word alignment and grammar ex-
traction (Section 3.1).
2. Tightly integrate name tagging and transla-
tion into MT decoding via name-aware gram-
mar (Section 3.2).
3. Optimize name translation and context trans-
lation simultaneously and conduct name
translation driven decoding with language
model (LM) based selection (Section 3.2).
4. Propose a new MT evaluation metric which
can discriminate names and non-informative
words (Section 4).
2 Baseline MT
As our baseline, we apply a high-performing
Chinese-English MT system (Zheng, 2008; Zheng
et al, 2009) based on hierarchical phrase-based
translation framework (Chiang, 2005). It is based
on a weighted synchronous context-free grammar
(SCFG). All SCFG rules are associated with a set
of features that are used to compute derivation
probabilities. The features include:
? Relative frequency in two directions P (?|?)
andP (?|?), estimating the likelihoods of one
side of the rule r: X ?< ?, ? > translating
into the other side, where ? and ? are strings
of terminals and non-terminals in the source
side and target side. Non-terminals in ? and
? are in one-to-one correspondence.
? Lexical weights in two directions: Pw(?|?)
andPw(?|?), estimating likelihoods of word-
s in one side of the rule r: X ?< ?, ? >
translating into the other side (Koehn et al,
2003).
? Phrase penalty: a penalty exp(1) for a rule
with no non-terminal being used in deriva-
tion.
? Rule penalty: a penalty exp(1) for a rule
with at least one non-terminal being used in
derivation.
? Glue rule penalty: a penalty exp(1) if a glue
rule used in derivation.
? Translation length: number of words in trans-
lation output.
Our previous work showed that combining mul-
tiple LMs trained from different sources can lead
to significant improvement. The LM used for de-
coding is a log-linear combination of four word
n-gram LMs which are built on different English
corpora (details described in section 5.1), with
the LM weights optimized on a development set
and determined by minimum error rate training
(MERT), to estimate the probability of a word giv-
en the preceding words. All four LMs were trained
using modified Kneser-Ney smoothing algorithm
(Chen and Goodman, 1996) and converted into
Bloom filter LMs (Talbot and Brants, 2008) sup-
porting memory map.
The scaling factors for all features are optimized
by minimum error rate training algorithm to max-
imize BLEU score (Och, 2003). Given an input
sentence in the source language, translation into
the target language is cast as a search problem,
where the goal is to find the highest-probability
derivation that generates the source-side sentence,
using the rules in our SCFG. The source-side
derivation corresponds to a synchronous target-
side derivation and the terminal yield of this target-
side derivation is the output of the system. We em-
ploy our CKY-style chart decoder, named SRInter-
p, to solve the search problem.
3 Name-aware MT
We tightly integrate name processing into the
above baseline to construct a NAMT model. Fig-
ure 1 depicts the general procedure.
3.1 Training
This basic training process of NAMT requires us
to apply a bilingual name tagger to annotate par-
allel training corpora. Traditional name tagging
approaches for single languages cannot address
this requirement because they were all built on da-
ta and resources which are specific to each lan-
guage without using any cross-lingual features.
In addition, due to separate decoding processes
the results on parallel data may not be consistent
across languages. We developed a bilingual joint
name tagger (Li et al, 2012) based on condition-
al random fields that incorporates both monolin-
gual and cross-lingual features and conducts join-
t inference, so that name tagging from two lan-
guages can mutually enhance each other and there-
fore inconsistent results can be corrected simulta-
neously. This joint name tagger achieved 86.3%
bilingual pair F-measure with manual alignment
and 84.4% bilingual pair F-measure with automat-
ic alignment as reported in (Li et al, 2012). Given
a parallel sentence pair we first apply Giza++ (Och
and Ney, 2003) to align words, and apply this join-
605
Dec
odin
g
Hier
arch
ical 
Phra
sed-
base
d M
T
Tran
slate
d Te
xt
Tran
slate
Bi-te
xt 
Data Sou
rce Text
Join
t
Nam
e Ta
gger
Sou
rce L
angu
age 
Nam
e Ta
gger
Nam
e Tr
ansl
ator
Trai
ning
Nam
e Pa
ir M
iner
Extr
act s
ourc
e lan
guag
e na
mes
 
and
 add
 the
m to
 dict
iona
ries 
for 
sour
ce la
ngu
age 
nam
e ta
gger
 E
xtra
ct n
ame
 pair
s an
d 
add
 the
m to
tran
slati
on 
dict
iona
ry
Extr
act a
nd a
dd 
nam
e pa
irs t
o
phra
se ta
ble
GIZA
++
Rule
 Extr
acto
r
Extr
act S
CFG
 rule
s wi
th 
com
bina
tion
 of n
ame
-rep
lace
d 
data
 and
 orig
inal 
bi-te
xt d
ata
Rep
lace
 nam
es w
ith 
non
-term
inals
 and
 
com
bine
 with
 the
 
orig
inal 
para
llel d
ata
Figure 1: Architecture of Name-aware Machine Translation System.
t bilingual name tagger to extract three types of
names: (Person (PER), Organization (ORG) and
Geo-political entities (GPE)) from both the source
side and the target side. We pair two entities from
two languages, if they have the same entity type
and are mapped together by word alignment. We
ignore two kinds of names: multi-word names
with conflicting boundaries in two languages and
names only identified in one side of a parallel sen-
tence.
We built a NAMT system from such name-
tagged parallel corpora. First, we replace tagged
name pairs with their entity types, and then
use Giza++ and symmetrization heuristics to re-
generate word alignment. Since the name tags ap-
pear very frequently, the existence of such tags
yields improvement in word alignment quality.
The re-aligned parallel corpora are used to train
our NAMT system based on SCFG. Since the joint
name tagger ensures that each tagged source name
has a corresponding translation on the target side
(and vice versa), we can extract SCFG rules by
treating the tagged names as non-terminals.
However, the original parallel corpora contain
many high-frequency names, which can already be
handled well by the baseline MT. Some of these
names carry special meanings that may influence
translations of the neighboring words, and thus re-
placing them with non-terminals can lead to infor-
mation loss and weaken the translation model. To
address this issue, we merged the name-replaced
parallel data with the original parallel data and ex-
tract grammars from the combined corpus. For ex-
ample, given the following sentence pair:
? -???e???e????? .
? China appeals to world for non involvement
in Angola conflict .
after name tagging it becomes
? GPE??e???e GPE?? .
? GPE appeals to world for non involvement in
GPE conflict .
Both sentence pairs are kept in the combined data
to build the translation model.
3.2 Decoding
During decoding phase, we extract names with
the baseline monolingual name tagger described
in (Li et al, 2012) from a source document. It-
s performance is comparable to the best report-
ed results on Chinese name tagging on Automat-
ic Content Extraction (ACE) data (Ji and Grish-
man, 2006; Florian et al, 2006; Zitouni and Flo-
rian, 2008; Nguyen et al, 2010). Then we ap-
ply a state-of-the-art name translation system (Ji
et al, 2009) to translate names into the target lan-
guage. The name translation system is composed
of the following steps: (1) Dictionary matching
based on 150,041 name translation pairs; (2) Sta-
tistical name transliteration based on a structured
perceptron model and a character based MT mod-
el (Dayne and Shahram, 2007); (3) Context infor-
mation extraction based re-ranking.
In our NAMT framework, we add the following
extensions to name translation.
We developed a name origin classifier based on
Chinese last name list (446 name characters) and
name structure parsing features to distinguish Chi-
nese person names and foreign person names (Ji,
2009), so that pinyin conversion is applied for Chi-
nese names while name transliteration is applied
only for foreign names. This classifier works rea-
sonably well in most cases (about 92% classifica-
tion accuracy), except when a common Chinese
last name appears as the first character of a foreign
606
name, such as ?1?? which can be translated ei-
ther as ?Jolie? or ?Zhu Li?.
For those names with fewer than five instances
in the training data, we use the name translation
system to provide translations; for the rest of the
names, we leave them to the baseline MT mod-
el to handle. The joint bilingual name tagger was
also exploited to mine bilingual name translation
pairs from parallel training corpora. The mapping
score between a Chinese name and an English
name was computed by the number of aligned to-
kens. A name pair is extracted if the mapping
score is the highest among all combinations and
the name types on both sides are identical. It is
necessary to incorporate word alignment as addi-
tional constraints because the order of names is of-
ten changed after translation. Finally, the extract-
ed 9,963 unique name translation pairs were also
used to create an additional name phrase table for
NAMT. Manual evaluation on 2,000 name pairs
showed the accuracy is 86%.
The non-terminals in SCFG rules are rewritten
to the extracted names during decoding, therefore
allow unseen names in the test data to be trans-
lated. Finally, based on LMs, our decoder ex-
ploits the dynamically created phrase table from
name translation, competing with originally ex-
tracted rules, to find the best translation for the
input sentence.
4 Name-aware MT Evaluation
Traditional MT evaluation metrics such as
BLEU (Papineni et al, 2002) and Translation Ed-
it Rate (TER) (Snover et al, 2006) assign the
same weights to all tokens equally. For exam-
ple, incorrect translations of ?the? and ?Bush? will
receive the same penalty. However, for cross-
lingual information processing applications, we
should acknowledge that certain informationally
critical words are more important than other com-
mon words. In order to properly evaluate the trans-
lation quality of NAMT methods, we propose to
modify the BLEU metric so that they can dynam-
ically assign more weights to names during evalu-
ation.
BLEU considers the correspondence between a
system translation and a human translation:
BLEU = BP ? exp
( N?
n=1
wn log pn
)
(1)
where BP is brevity penalty defined as follows:
BP =
{
1 if c > r,
e(1?r/c) if c ? r. (2)
where wn is a set of positive weights summing to
one and usually uniformly set as wn = 1/N , c is
the length of the system translation and r is the
length of reference translation, and pn is modified
n-gram precision defined as:
pn =
?
C?Candidates
?
n-gram?C
Countclip(n-gram)
?
C??Candidates
?
n-gram??C?
Countclip(n-gram?)
(3)
where C and C ? are translation candidates in the
candidate sentence set, if a source sentence is
translated to many candidate sentences.
As in BLEU metric, we first count the maxi-
mum number of times an n-gram occurs in any s-
ingle reference translation. The total count of each
candidate n-gram is clipped at sentence level by it-
s maximum reference count. Then we add up the
weights of clipped n-grams and divide them by the
total weight of all n-grams.
Based on BLEU score, we design a name-aware
BLEU metric as follows. Depending on whether a
token t is contained in a name in reference trans-
lation, we assign a weight weightt to t as follows:
weightt ={
1? e?tf(t,d)?idf(t,D), if t never appears in names
1 + PEZ , if t occurs in name(s)
(4)
where PE is the sum of penalties of non-name
tokens and Z is the number of tokens within all
names:
PE =
?
t never appears in names
e?tf(t,d)?idf(t,D) (5)
In this paper, the tf ? idf score is computed at sen-
tence level, therefore, D is the sentence set and
each d ? D is a sentence.
The weight of an n-gram in reference translation
is the sum of weights of all tokens it contains.
weightngram =
?
t?ngram
weightt (6)
Next, we compute the weighted modified n-
gram precision Countweight?clip(n-gram) as fol-
lows:
Countweight?clip(n-gram) =?
if the ngrami is correctly translated
weightngrami (7)
607
The Countclip(n-gram) in the equation 3 is
substituted with aboveCountweight?clip(n-gram).
When we sum up the total weight of all n-grams of
a candidate translation, some n-grams may contain
tokens which do not exist in reference translation.
We assign the lowest weight of tokens in reference
translation to these rare tokens.
We also add an item, name penalty NP , to
penalize the output sentences which contain too
many or too few names:
NP = e?(uv?1)2/2? (8)
where u is the number of name tokens in system
translation and v is the number of name tokens in
reference translation.
Finally the name-aware BLEU score is defined
as:
BLEUNA = BP ?NP ? exp
( N?
n=1
wn logwpn
)
(9)
This new metric can also be applied to evalu-
ate MT approaches which emphasize other types
of facts such as events, by simply replacing name
tokens by other fact tokens.
5 Experiments
In this section we present the experimental results
of NAMT compared to the baseline MT.
5.1 Data Set
We used a large Chinese-English MT training cor-
pus from various sources and genres (including
newswire, web text, broadcast news and broadcast
conversations) for our experiments. We also used
some translation lexicon data and Wikipedia trans-
lations. The majority of the data sets were col-
lected or made available by LDC for U.S. DARPA
Translingual Information Detection, Extraction
and Summarization (TIDES) program, Global Au-
tonomous Language Exploitation (GALE) pro-
gram, Broad Operational Language Translation
(BOLT) program and National Institute of Stan-
dards and Technology (NIST) MT evaluations.
The training corpus includes 1,686,458 sentence
pairs. The joint name tagger extracted 1,890,335
name pairs (295,087 Persons, 1,269,056 Geo-
political entities and 326,192 Organizations).
Four LMs, denoted LM1, LM2, LM3, and
LM4, were trained from different English cor-
pora. LM1 is a 7-gram LM trained on the tar-
get side of Chinese-English and Egyptian Arabic-
English parallel text, English monolingual discus-
sion forums data R1-R4 released in BOLT Phase
1 (LDC2012E04, LDC2012E16, LDC2012E21,
LDC2012E54), and English Gigaword Fifth Edi-
tion (LDC2011T07). LM2 is a 7-gram LM trained
only on the English monolingual discussion fo-
rums data listed above. LM3 is a 4-gram LM
trained on the web genre among the target side
of all parallel text (i.e., web text from pre-BOLT
parallel text and BOLT released discussion fo-
rum parallel text). LM4 is a 4-gram LM trained
on the English broadcast news and conversation
transcripts released under the DARPA GALE pro-
gram. Note that for LM4 training data, some tran-
scripts were quick transcripts and quick rich tran-
scripts released by LDC, and some were generated
by running flexible alignment of closed captions or
speech recognition output from LDC on the audio
data (Venkataraman et al, 2004).
In order to demonstrate the effectiveness and
generality of our approach, we evaluated our ap-
proach on seven test sets from multiple genres and
domains. We asked four annotators to annotate
names in four reference translations of each sen-
tence and an expert annotator to adjudicate result-
s. The detailed statistics and name distribution of
each test data set is shown in Table 1. The per-
centage of names occurred fewer than 5 times in
training data are listed in the brackets in the last
column of the table.
5.2 Overall Performance
Besides the new name-aware MT metric, we also
adopt two traditional metrics, TER to evaluate the
overall translation performance and Named Entity
Weak Accuracy (NEWA) (Hermjakob et al, 2008)
to evaluate the name translation performance.
TER measures the amount of edits required to
change a system output into one of the reference
translations. Specifically:
TER = # of editsaverage # of reference words (10)
Possible edits include insertion, substitution dele-
tion and shifts of words.
The NEWA metric is defined as follows. Us-
ing a manually assembled name variant table, we
also support the matching of name variants (e.g.,
?World Health Organization? and ?WHO?).
NEWA = Count # of correctly translated namesCount # of names in references (11)
608
Corpus Genre Sentence # Word # Token # GPE(%) PER(%) ORG(%) All namesin source in reference (% occurred < 5)
BOLT 1 forum 1,200 20,968 24,193 875(82.9) 90(8.5) 91(8.6) 1,056 (51.4)
BOLT 2 forum 1,283 23,707 25,759 815(73.7) 141(12.8) 149(13.5) 1,105 (65.9)
BOLT 3 forum 2,000 38,595 42,519 1,664(80.4) 204(9.8) 204(9.8) 2,072 (47.4)
BOLT 4 forum 1,918 41,759 47,755 1,852(80.0) 348(25.0) 113(5.0) 2,313 (53.3)
BOLT 5 blog 950 23,930 26,875 352(42.5) 235(28.3) 242(29.2) 829 (55.3)
NIST2006 news&blog 1,664 38,442 45,914 1,660(58.2) 568(19.9) 625(21.9) 2,853 (73.1)
NIST2008 news&blog 1,357 32,646 37,315 700(47.9) 367(25.1) 395(27.0) 1,462 (72.0)
Table 1: Statistics and Name Distribution of Test Data Sets.
Metric System BOLT 1 BOLT 2 BOLT 3 BOLT 4 BOLT 5 NIST2006 NIST2008
BLEU
Baseline 14.2 14.0 17.3 15.6 15.3 35.5 29.3
NPhrase 14.1 14.4 17.1 15.4 15.3 35.4 29.3
NAMT 14.2 14.6 16.9 15.7 15.5 36.3 30.0
Name-aware BLEU
Baseline 18.2 17.9 18.6 17.6 18.3 36.1 31.7
NPhrase 18.1 18.8 18.5 18.1 18.0 35.8 31.8
NAMT 18.4 19.5 19.7 18.2 18.9 39.4 33.1
TER
Baseline 70.6 71.0 69.4 70.3 67.1 58.7 61.0
NPhrase 70.6 70.4 69.4 70.4 67.1 58.7 60.9
NAMT 70.3 70.2 69.2 70.1 66.6 57.7 60.5
NEWA
All
Baseline 69.7 70.1 73.9 72.3 60.6 66.5 60.4
NPhrase 69.8 71.1 73.8 72.5 60.6 68.3 61.9
NAMT 71.4 72.0 77.7 75.1 62.7 72.9 63.2
GPE
Baseline 72.8 78.4 80.0 78.7 81.3 79.2 76.0
NPhrase 73.6 79.3 79.2 78.9 82.3 82.6 79.5
NAMT 74.2 80.2 82.8 80.4 79.3 85.5 79.3
PER
Baseline 53.3 44.7 45.1 49.4 48.9 54.2 51.2
NPhrase 52.2 45.4 48.9 48.5 47.6 55.1 50.9
NAMT 55.6 45.4 58.8 55.2 56.2 60.0 52.3
ORG
Baseline 56.0 49.0 52.9 38.1 41.7 44.0 41.3
NPhrase 50.5 50.3 54.4 40.7 41.3 42.2 40.7
NAMT 60.4 52.3 55.4 41.6 45.0 51.0 44.8
Table 2: Translation Performance (%).
For better comparison with NAMT, besides the
original baseline, we develop the other baseline
system by adding name translation table into the
phrase table (NPhrase).
Table 2 presents the performance of overal-
l translation and name translation. We can see
that except for the BOLT3 data set with BLEU
metric, our NAMT approach consistently outper-
formed the baseline system for all data sets with
all metrics, and provided up to 23.6% relative er-
ror reduction on name translation. According to
Wilcoxon Matched-Pairs Signed-Ranks Test, the
improvement is not significant with BLEU metric,
but is significant at 98% confidence level with all
of the other metrics. The gains are more signifi-
cant for formal genres than informal genres main-
ly because most of the training data for name tag-
ging and name translation were from newswire.
Furthermore, using external name translation table
only did not improve translation quality in most
test sets except for BOLT2. Therefore, it is im-
portant to use name-replaced corpora for rule ex-
traction to fully take advantage of improved word
alignment.
Many errors from the baseline MT approach oc-
curred because some parts of out-of-vocabulary
names were mistakenly segmented into common
words. For example, the baseline MT system mis-
takenly translated a person name ?Y?? (Sun
Honglei)? into ?Sun red thunder?. In informal
genres such as discussion forums and web blogs,
even common names often appear in rare form-
s due to misspelling or morphing. For example,
?e8l (Obama)? was mistakenly translated into
?Ma Olympic?. Such errors can be compounded
when word re-ordering was applied. For example,
the following sentence: ????????/:
'J/iy (Guo Meimei?s strength real-
ly is formidable, I really admire her)? was mis-
takenly translated into ?Guo the strength of the
America and the America also really strong , ah
, really admire her? by the baseline MT system
because the person name ???? (Guomeimei)?
was mistakenly segmented into three words ??
(Guo)?, ?? (the America)? and ?? (the Ameri-
ca)?. But our NAMT approach successfully iden-
tified and translated this name and also generated
better overall translation: ?Guo Meimei ?s power
is also really strong , ah , really admire her?.
609
B L E U N a m e - a w a r eB L E U024681 01 2
1 41 61 82 0Score A u t o m a t i c  M e t r i c s H u m .  1  H u m .  2 H u m .  30 . 00 . 51 . 01 . 52 . 02 . 5
3 . 03 . 54 . 0 b a s e l i n e N A M T Score H u m a n  E v a l u a t i o n
Figure 2: Scores based on Automatic Metrics and Human
Evaluation.
5.3 Name-aware BLEU vs The Human
Evaluation
In order to investigate the correlation between
name-aware BLEU scores and human judgment
results, we asked three bi-lingual speakers to judge
our translation output from the baseline system
and the NAMT system, on a Chinese subset of 250
sentences (each sentence has two corresponding
translations from baseline and NAMT) extracted
randomly from 7 test corpora. The annotators rat-
ed each translation from 1 (very bad) to 5 (very
good) and made their judgments based on whether
the translation is understandable and conveys the
same meaning.
We computed the name-aware BLEU scores on
the subset and also the aggregated average scores
from human judgments. Figure 2 shows that
NAMT consistently achieved higher scores with
both name-aware BLEU metric and human judge-
ment. Furthermore, we calculated three Pearson
product-moment correlation coefficients between
human judgment scores and name-aware BLEU s-
cores of these two MT systems. Give the sample
size and the correlation coefficient value, the high
significance value of 0.99 indicates that name-
aware BLEU tracks human judgment well.
5.4 Word Alignment
It is also important to investigate the impact of our
NAMT approach on improving word alignmen-
t. We conducted the experiment on the Chinese-
English Parallel Treebank (Li et al, 2010) with
ground-truth word alignment. The detailed pro-
cedure following NAMT framework is as follows:
(1) Ran the joint bilingual name tagger; (2) Re-
placed each name string with its name type (PER,
ORG or GPE), and ran Giza++ on the replaced
sentences; (3) Ran Giza++ on the words within
Words Method P R F 
Baseline Giza++ 69.8 47.8 56.7 
Joint Name 
Tagging 
70.4 48.1 57.1 
 
Overall 
Words 
Ground-truth 
Name Tagging 
(Upper-bound) 
71.3 48.9 58.0 
Baseline Giza++ 86.0 31.4 46.0 Words 
Within 
Names 
Joint Name 
Tagging 
77.6 37.2 50.3 
 
 
 
 
 
 
 
 
 
 
Table 3: Impact of Joint Bilingual Name Tagging on Word
Alignment (%).
each name pair. (4) Merged (2) and (3) to pro-
duce the final word alignment results. In order to
compare with the upper-bound gains, we also mea-
sured the performance of applying ground-truth
name tagging with the above procedures.
The experiment results are shown in Table 3.
For the words within names, our approach provid-
ed significant gains by enhancing F-measure from
46.0% to 50.3%. Only 10.6% words are within
names, therefore the upper-bound gains on over-
all word alignment is only 1.3%. Our joint name
tagging approach achieved 0.4% (statistically sig-
nificant) improvement over the baseline. In Fig-
ure 3 we categorized the sentences according to
the percentage of name words in each sentence and
measured the improvement for each category. We
can clearly see that as the sentences include more
names, the gains achieved by our approach tend to
be greater.
5.5 Remaining Error Analysis
Although the proposed model has significantly en-
hanced translation quality, some challenges re-
main. We analyze some major sources of the re-
maining errors as follows.
1. Name Structure Parsing.
We found that the gains of our NAMT approach
were mainly achieved for names with one or two
components. When the name structure becomes
too complicated to parse, name tagging and name
translation are likely to produce errors, especially
for long nested organizations. For example, ??0
???b?@? (Anti-malfeasance Bureau of
Gutian County Procuratorate) consists of a nested
organization name with a GPE as modifier: ??
0???b? (Gutian County Procuratorate) and
an ORG name: ??@? (Anti-malfeasance Bu-
reau).
2. Name abbreviation tagging and translation.
Some organization abbreviations are also dif-
ficult to extract because our name taggers have
610
0~10 10~20 20~30 30~40 >40
-0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
F-Measure Gains in Overall Word Alignment (%)
#name tokens/#all tokens(%)
 Baseline Giza++
 Joint Name Tagging
 Ground-truth Name Tagging (Upper-bound)
Figure 3: Word alignment gains according to the percentage
of name words in each sentence.
not incorporated any coreference resolution tech-
niques. For example, without knowing that ?FAW?
refers to ?First Automotive Works? in ?FAW has
also utilized the capital market to directly fi-
nance, and now owns three domestic listed compa-
nies?, our system mistakenly labeled it as a GPE.
The same challenge exists in name alignment and
translation (for example, ? i (Min Ge)? refer-
s to ? -??Zi}?XProceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1083?1093,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Resolving Entity Morphs in Censored Data
Hongzhao Huang1, Zhen Wen2, Dian Yu1, Heng Ji1,
Yizhou Sun3, Jiawei Han4, He Li5
1Computer Science Department and Linguistics Department,
Queens College and Graduate Center, City University of New York, New York, NY, USA
2IBM T. J. Watson Research Center, Hawthorne, NY, USA
3College of Computer and Information Science, Northeastern University, Boston, MA, USA
4Computer Science Department, Univerisity of Illinois at Urbana-Champaign, Urbana, IL, USA
5Admaster Inc., China
{hongzhaohuang1,yudiandoris1,hengjicuny1, liheact5}@gmail.com,
zhenwen@us.ibm.com2, yzsun@ccs.neu.edu3, hanj@illinois.edu4
Abstract
In some societies, internet users have to
create information morphs (e.g. ?Peace
West King? to refer to ?Bo Xilai?) to avoid
active censorship or achieve other com-
munication goals. In this paper we aim
to solve a new problem of resolving en-
tity morphs to their real targets. We ex-
ploit temporal constraints to collect cross-
source comparable corpora relevant to any
given morph query and identify target can-
didates. Then we propose various novel
similarity measurements including surface
features, meta-path based semantic fea-
tures and social correlation features and
combine them in a learning-to-rank frame-
work. Experimental results on Chinese
Sina Weibo data demonstrate that our ap-
proach is promising and significantly out-
performs baseline methods1.
1 Introduction
Language constantly evolves to maximize com-
municative success and expressive power in daily
social interactions. The proliferation of online so-
cial media significantly expedites this evolution,
as new phrases triggered by social events may be
disseminated rapidly in social media. To automati-
cally analyze such fast evolving language in social
media, new computational models are demanded.
In this paper, we focus on one particular lan-
guage evolution that creates new ways to commu-
nicate sensitive subjects because of the existence
of internet information censorship. We call this
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/Morphing.tar.gz
phenomenon information morph. For example,
when Chinese online users talk about the former
politician ?Bo Xilai?, they use a morph ?Peace
West King? instead, a historical figure four hun-
dreds years ago who governed the same region
as Bo. Morph can be considered as a special
case of alias used for hiding true entities in ma-
licious environment (Hsiung et al, 2005; Pantel,
2006). However, social network plays an impor-
tant role in generating morphs. Usually morphs
are generated by harvesting the collective wisdom
of the crowd to achieve certain communication
goals. Aside from the purpose of avoiding cen-
sorship, other motivations for using morph include
expressing sarcasm/irony, positive/negative senti-
ment or making descriptions more vivid toward
some entities or events. Table 1 presents the wide
range of cases that are used to create the morphs.
We can see that a morph can be either a regular
term with new meaning or a newly created term.
Morph Target Motivation
Peace West King Bo Xilai Sensitive
Blind Man Chen Guangcheng Sensitive
Miracle Brother Wang Yongping Irony
Kim Fat Kim Joing-il Negative
Kimchi Country South Korea Vivid
Table 1: Morph Examples and Motivations.
We believe that successful resolution of morphs
is a crucial step for automated understanding of
the fast evolving social media language, which
is important for social media marketing (Bar-
wise and Meehan, 2010). Another application
is to help common users without enough back-
ground/cultural knowledge to understand internet
language for their daily use. Furthermore, our ap-
proaches can also be applied for satire or other im-
plicit meaning recognition, as well as information
extraction (Bollegala et al, 2011).
1083
However, morph resolution in social media is
challenging due to the following reasons. First,
the sensitive real targets that exist in the same
data source under active censorship are often au-
tomatically filtered. Table 2 presents the distribu-
tions of some examples of morphs and their tar-
gets in English Twitter and Chinese Sina Weibo.
For example, the target ?Chen Guangcheng? only
appears once in Weibo. Thus, the co-occurrence
of a morph and its target is quite low in the vast
amount of information in social media. Second,
most morphs were not created based on pronunci-
ations, spellings or other encryptions of their origi-
nal targets. Instead, they were created according to
semantically related entities in historical and cul-
tural narratives (e.g. ?Peace West King? as morph
of ?Bo Xilai?) and thus very difficult to capture
based on typical lexical features. Third, tweets
from Twitter/Chinese Weibo are short (only up to
140 characters) and noisy, resulting in difficult ex-
traction of rich and accurate evidences due to the
lack of enough contexts.
 Frequency in  Twitter Frequency in  Weibo Morph Target Morph Target Morph Target Hu Ji Hu Jintao 1 3,864 2,611 71 Blind  Man Chen  Guangcheng 18 2,743 20,941 1 Baby Wen Jiabao 2238 2021 26,279 8  
Table 2: Distributions of Morph Examples
To the best of our knowledge, this is the first
work to use NLP and social network analysis tech-
niques to automatically resolve morphed informa-
tion. To address the above challenges, our paper
offers the following novel contributions.
? We detect target candidates by exploiting the
dynamics of the social media to extract tem-
poral distribution of entities, based on the as-
sumption that the popularity of an individ-
ual is correlated between censored and un-
censored text within a certain time window.
? Our approach builds and analyzes heteroge-
neous information networks from multiple
sources, such as Twitter, Sina Weibo and web
documents in formal genre (e.g. news) be-
cause a morph and its target tend to appear in
similar contexts.
? We propose two new similarity measures, as
well as integrating temporal information into
the similarity measures to generate global se-
mantic features.
? We model social user behaviors and use so-
cial correlation to assist in measuring seman-
tic similarities because the users who posted
a morph and its corresponding target tend to
share similar interests and opinions.
Our experiments demonstrate that the pro-
posed approach significantly outperforms tradi-
tional alias detection methods (Hsiung et al,
2005).
2 Approach Overview   Morph Query   Comparable Data Acquisition   
Target Candidate Ranking 
Target
Learning to Rank 
Semantic Features
Semantic Annotation and  Target Candidate Identification
Surface Features Social Features
Censored Data Uncensored Data
Figure 1: Overview of Morph Resolution
Given a morph query m, the goal of morph res-
olution is to find its real target. Figure 1 depicts
the general procedure of our approach. It consists
of two main sub-tasks:
? Target Candidate Identification: For each
m, discover a list of target candidates E =
{e1, e2, ..., eN}. First, relevant comparable
data sets that include m are retrieved. In
this paper we collect comparable censored
data from Weibo and uncensored data from
Twitter and Web documents such as news ar-
ticles. We then apply various annotations
such as word segmentation, part-of-speech
tagging, noun phrase chunking, name tagging
and event extraction to these data sets.
? Target Candidate Ranking: Rank the target
candidates in E. We explore various features
including surface, semantic and social fea-
tures, and incorporate them into a learning to
1084
rank framework. Finally, the top ranked can-
didate is produced as the resolved target.
3 Target Candidate Identification
The general goal of the first step is to identify a list
of target candidates for each morph query from the
comparable corpora including Sina Weibo, Chi-
nese News websites and English Twitter. How-
ever, obviously we cannot consider all of the
named entities in these sources as target candi-
dates due to the sheer volume of information. In
addition, morphs are not limited to named entity
forms. In order to narrow down the scope of tar-
get candidates, we propose a Temporal Distribu-
tion Assumption as follows. The intuition is that
a morph m and its real target e should have sim-
ilar temporal distributions in terms of their occur-
rences. Suppose the data sets are separated into Z
temporal slots (e.g. by day), the assumption can
be stated as:
Let Tm = {tm1, tm2, ..., tmZm} be the set of
temporal slots each morph m occurs, and Te =
{te1, te2, ..., teZe} be the set of slots a target can-
didate e occurs. Then e is considered as a target
candidate of m if and only if, for each tmi ? Tm
(i = 1, 2, ..., Zm), there exist a j ? {1, 2, ..., Ze}
such that tmi ? tej ? ?, where ? is a threshold
value (in this paper we set the threshold to 7 days,
which is optimized from a development set). For
comparison we also attempted topic modeling ap-
proach to detect target candidates, as shown in sec-
tion 5.3.
4 Target Candidate Ranking
Next, we propose a learning-to-rank framework to
rank target candidates based on various levels of
novel features based on surface, semantic and so-
cial analysis.
4.1 Surface Features
We first extract surface features between the
morph and the candidate based on measuring or-
thographic similarity measures which were com-
monly used in entity coreference resolution (e.g.
(Ng, 2010; Hsiung et al, 2005)). The measures
we use include ?string edit distance?, ?normalized
string edit distance? (Wagner and Fischer, 1974)
and ?longest common subsequence? (Hirschberg,
1977).
4.2 Semantic Features
4.2.1 Motivations
Fortunately, although a morph and its target may
have very different orthographic forms, they tend
to be embedded in similar semantic contexts
which involve similar topics and events. Figure 2
presents some example messages under censor-
ship (Weibo) and not under censorship (Twitter
and Chinese Daily). We can see that they include
similar topics, events (e.g., ?fell from power?,
?gang crackdown?, ?sing red songs?), and se-
mantic relations (e.g., family relations with ?Bo
Guagua?). Therefore if we can automatically ex-
tract and exploit these indicative semantic con-
texts, we can narrow down the real targets effec-
tively.
?Pe
ace W
est K
ingfr
om C
hong
qing
fell fr
om p
ower
, still
 nee
d to 
sing 
red s
ongs
?
?T
here
 is no
 diffe
renc
e be
twee
n tha
t 
guy?s
 plag
iarism
 and
 Buh
ou?s
gang
 
crack
down
.
?R
eme
mbe
r tha
t Buh
ousa
id tha
t his 
famil
y wa
s not
 rich 
at th
e pre
ss 
confe
renc
e a f
ew d
ays b
efore
 he 
fell fr
om p
ower
. His 
sonB
o 
Guag
uais 
supp
orted
 by h
is 
scho
larsh
ip.
?B
o Xila
i: ten
 thou
sand
 lette
rs of
 
accu
satio
n ha
ve be
en re
ceive
d du
ring 
Chon
gqing
gang
 crac
kdow
n.
?T
he w
ebpa
ge o
f ?Tia
nzeE
cono
mic 
Stud
y Ins
titute
?own
ed b
y the
 liber
al 
party
 has 
been
 clos
ed. T
his is
 the 
first 
affec
ted w
ebsit
e of 
the li
bera
l par
ty 
after
 Bo X
ilaife
ll from
 pow
er.
?B
o Xila
igave
 an e
xplan
ation
 abo
ut th
e 
sour
ce of
 hiss
on, B
o Gu
agua
?s 
tuitio
n.
?B
o Xila
iled C
hong
qing 
city l
eade
rs 
and 
40 d
istric
t and
 coun
ty pa
rty a
nd 
gove
rnme
nt lea
ders
 to si
ng re
d son
gs.
Weib
o(ce
nsore
d)
Twitt
er an
d Ch
inese
 New
s (un
cens
ored)
Figure 2: Cross-source Comparable Data Example
(each morph and target pair is shown in the same
color)
4.2.2 Information Network Construction
We define an information network as a directed
graph G = (V, E) with an object type mapping
function ? : V ? A and a link type mapping func-
tion ? : E ? R, where each object v ? V belongs
to one particular object type ?(v) ? A, each link
e ? E belongs to a particular relation ?(e) ? R.
If two links belong to the same relation type, then
they share the same starting object type as well as
the same ending object type. An information net-
work is homogeneous if and only if there is only
one type for both objects and links, and an infor-
mation network is heterogeneous when the objects
are from multiple distinct types or there exist more
than one type of links.
In order to construct the information networks
for morphs, we apply the Standford Chinese word
1085
segmenter with Chinese Penn Treebank segmen-
tation standard (Chang et al, 2008) and Stan-
ford part-of-speech tagger (Toutanova et al, 2003)
to process each sentence in the comparable data
sets. Then we apply a hierarchical Hidden Markov
Model (HMM) based Chinese lexical analyzer
ICTCLAS (Zhang et al, 2003) to extract named
entities, noun phrases and events.
We have also attempted using the results from
Dependency Parsing, Relation Extraction and
Event Extraction tools (Ji and Grishman, 2008)
to enrich the link types. Unfortunately the state-
of-the-art techniques for these tasks still perform
poorly on social media in terms of both accuracy
and coverage of important information, these so-
phisticated semantic links all produced negative
impact on the target ranking performance. There-
fore we limited the types of vertices into: Morph
(M), Entity(E), which includes target candidates,
Event (EV), and Non-Entity Noun Phrases (NP);
and used co-occurrence as the edge type. We ex-
tract entities, events, and non-entity noun phrases
that occur in more than one tweet as neighbors.
And for two vertices xi and xj , the weight wij
of their edge is the frequency they co-occur to-
gether within the tweets. A network schema of
such networks is shown in Figure 3. Figure 4
M
E NP
EV
Figure 3: Network Schema of Morph-Related Het-
erogeneous Information Network
presents an example of a heterogeneous informa-
tion network from the motivation examples fol-
lowing the above network schema, which connects
the morphs ?Peace West King?, ?Buhou? and their
corresponding target ?Bo Xilai?.
4.2.3 Meta-Path-Based Semantic Similarity
Measurements
Given the constructed network, a straightforward
solution for finding the target for a morph is to use
link-based similarity search. However, now ob-
jects are linked to different types of neighbors, if
all neighbors are treated as the same, it may cause
information loss problems. For example, the en-
tity ??? (Chongqing)? is a very important aspect
characterizing the politician ????(Bo Xilai)?
Ga
ng 
Cra
ckd
ow
n 
Fel
l Fr
om
 
Pow
er
Ch
ong
qin
g
Sin
g R
ed 
Son
gs
Bu
hou
Pea
ce 
We
st 
Kin
g
Bo
 Xil
ai
Bo
 Gu
agu
a
Ent
ity
Ent
ity
Ent
ity
Eve
nt
Eve
nt
Eve
nt
Mo
rph
Mo
rph
Figure 4: Example of Morph-Related Heteroge-
neous Information Network
since he governed it, and if a morph m which is
also highly correlated with ??? (Chongqing)?, it
is very likely that ?Bo Xilai? is the real target ofm.
Therefore, the semantic features generated from
neighbors such as the entity ??? (Chongqing)?
should be treated differently from other types of
neighbors such as ???(talented people)? .
In this work, we propose to measure the simi-
larity of two nodes over heterogeneous networks
as shown in Figure 3, by distinguishing neighbors
into three types according to the network schema
(i.e. entities, events, non-entity noun phrases). We
then adopt meta-path-based similarity measures
(Sun et al, 2011a; Sun et al, 2011b), which are
defined over heterogeneous networks to extract se-
mantic features. A meta-path is a path defined over
a network, and composed of a sequence of rela-
tions between different object types. For example,
as shown in Figure 3, a morph and its target can-
didate can be connected by three meta-paths, in-
cluding ?M - E - E?, ?M - EV - E?, and ?M - NP
- E?. Intuitively, each meta-path provides a unique
angle to measure how similar two objects are.
For the determined meta-paths, we extract se-
mantic features using the similarity measures pro-
posed in (Sun et al, 2011a; Hsiung et al, 2005).
We denote the neighbor sets of certain type for a
morph m and a target candidate e as ?(m) and
?(e), and a meta-path as P . We now list several
meta-path-based similarity measures below.
Common neighbors (CN). It measures the num-
ber of common neighbors that m and e share as
|?(m) ? ?(e)|.
Path count (PC). It measures the number of path
instances betweenm and e following meta-pathP .
Pairwise random walk (PRW). For a meta-
path P that can be decomposed into two shorter
1086
meta-paths with the same length P = (P1P2),
pairwise random walk measures the probabil-
ity of the pairwise random walk starting from
both m and e and reaching the same mid-
dle object. More formally, it is computed as?
(p1p2)?(P1P2) prob(p1)prob(p?12 ), where p?12 isthe inverse of p2.
Kullback-Leibler distance (KLD). For m and
e, the pairwise random walk probability of their
neighbors can be represented as two probability
vectors, then Kullback-Leibler distance (Hsiung
et al, 2005) can be used to compute sim(m, e).
Beyond the above similarity measures, we also
propose to use cosine-similarity-style normaliza-
tion method to modify common neighbor and pair-
wise random walk measures so that we can ensure
the morph node and the target candidate node are
strongly connected and also have similar popular-
ity. The modified algorithms penalize features in-
volved with the highly popular objects, since they
are more likely to have accidental interactions with
each other.
Normalized common neighbors (NCN). Nor-
malized common neighbors can be measured as
sim(m, e) = |?(m)??(e)|?
|?(m)|
?
|?(e)|
. It refines the simple
counting of common neighbors by avoiding bias
to highly visible or concentrated objects.
Pairwise random walk/cosine (PRW/cosine).
Pairwise random walk measures linkage weights
disproportionately with their visibility to their
neighbors, which may be too strong. Instead, we
propose to use a tamer normalization method as?
(p1p2)?(P1P2) f(p1)f(p?12 ), where.
f(p1) = count(m,x)??
x?? count(m,x)
,
f(p2) = count(e, x)??
x?? count(e, x)
,
and ? is the set of middle objects connecting the
decomposed meta-paths p1 and p?12 , count(y, x)
is the total number of paths between y and the mid-
dle object x, y could be m or e.
The above similarity measures can also be ap-
plied to homogeneous networks that do not differ-
entiate the neighbor types.
4.2.4 Global Semantic Feafure Generation
A morph tends to have higher temporal correlation
with its real target, and share more similar topics
compared to other irrelevant targets. Therefore,
we propose to incorporate temporal information
into similarity measures to generate global seman-
tic features.
Let T = t1 ? t2 ? ... ? tN be a set of temporal
slots (i.e. by day),E be the set of target candidates
for each morphm. Then for each ti ? T , and each
e ? E, the local semantic features simti(m, e)
is extracted based only on the information posted
within ti using one of the similarity measures in-
troduced in Section 4.2.3. Then we propose two
approaches to generate global semantic features.
The first approach is adding the similarity score
between m and e in each temporal slot to attain
the first set of global features:
simglobal sum(m, e) =
?
ti?T
simti(m, e).
The second method first normalizes the similarity
score in each temporal slot ti, them sum the nor-
malized scores to generate the second set of global
features, which can be calculated as
simglobal norm(m, e) =
?
ti?T
normti(m, e).
where normti(m, e) = simti (m,e)?e?E simti (m,e) .
4.2.5 Integrate Cross Source/Cross Genre
Information
Due to internet information censorship or surveil-
lance, users may need to use morphs to post sensi-
tive information. For example, the Chinese Weibo
message ?????,??????? (Already
put in prison, still need to serve Buhou?? include
a morph ?? (Buhou). In contrast, users are less
restricted in some other uncensored social media
such as Twitter. For example, the tweet from Twit-
ter ?...?????????????????...
(...call Bo Xilai?peace west king? or ?buhou?...)?
contains both the morph and the real target ??
? (Bo Xilai). Therefore, we propose to integrate
information from another source (e.g. Twitter) to
help resolution of sensitive morphs in Weibo.
Another difficulty from morph resolution in
micro-blogging is that tweets are only allowed to
contain maximum 140 characters with a lot of
noise and diverse topics. The shortness and di-
versity of tweets may limit the power of content
analysis for semantic feature extraction. However,
formal genres such as web documents are cleaner
and contain richer contexts, thus can provide more
topically related information. In this work, we also
exploit the background web documents from the
1087
embedded URLs in tweets to enrich information
network construction. After applying the same an-
notation techniques as tweets for uncensored data
sets, sentence-level co-occurrence relations are ex-
tracted and integrated into the network as shown in
Figure 3.
4.3 Social Features
It has been shown that there exist correlation be-
tween neighbors in social networks (Anagnos-
topoulos et al, 2008; Wen and Lin, 2010). Be-
cause of such social correlation, close social
neighbors in social media such as Twitter and
Weibo may post similar information, or share sim-
ilar opinion. Therefore, we can utilize social cor-
relation to assist in resolving morphs.
As social correlation can be defined as a func-
tion of social distance between a pair of users, we
use social distance as a proxy to social correla-
tion in our approach. The social distance between
user i and j is defined by considering the degree
of separation in their interaction (e.g. retweet-
ing and mentioning) and the amount of the in-
teraction. Similar definition has been shown ef-
fective in characterizing social distance in social
networks extracted from communication data (Lin
et al, 2012; Wen and Lin, 2010). Specifically,
it is dist(i, j) = ?K?1k=1 1strength(vk,vk+1) , where
v1, ..., vk are the nodes on the shortest path from
user i to user j, and strength(vk, vk+1) measures
the strength of interactions between vk and vk+1
as: strength(i, j) = log(Xij)maxj log(Xij) , where Xij isthe total interactions between user i and j, includ-
ing both retweeting and mentioning (If Xij < 10,
we set strength(i, j) = 0).
We integrate social correlation and temporal in-
formation to define our social features. The in-
tuition is that when a morph is used by an user,
the real target may also in the posts by the user or
his/her close friends within a certain time period.
Let T be the set of temporal slots a morph m oc-
curs, Ut be the set of users whose posts include m
in slot t where t ? T , and Uc be the set of close
friends (i.e., social distance < 0.5) for Ut. The
social features are defined as
s(m, e) =
?
t?T f(e, t, Ut, Uc)
|T | .
where f(e, t, Ut, Uc) is a indicator function which
return 1 if one of the users in Ut or Uc posts tweets
include the target candidate e within 7 days before
t.
4.4 Learning-to-Rank
Similar to (Hsiung et al, 2005; Sun et al, 2011a),
we then model the probability of linkage predic-
tion between a morph m and its target candidate
e as a function incorporating the surface, semantic
and social features. Given a training pair ?m, e?,
we choose the standard logistic regression model
to learn weights for the features defined above.
The learnt model is used to predict the probabil-
ity of linking an unseen morph and its target can-
didate. Based on the descending ranking order of
the probability, we select top k candidates as the
final answers based on the answer size k.
5 Experiments
Next, we present the experiment under various set-
tings shown in Table 3, and the impacts of cross
source and cross genre information.
5.1 Data and Evaluation Metric
We collected 1, 553, 347 tweets from Chinese Sina
Weibo from May 1 to June 30 to construct the
censored data set, and retrieved 66, 559 web doc-
uments from the embedded URLs in tweets as the
initial uncensored data set. Retweets and redun-
dant web documents are filtered to ensure more
reliable frequency counting of co-occurrence rela-
tions. We asked two native Chinese annotators to
analyze the data, and construct a test set consisted
of 107 morph entities (81 persons and 26 loca-
tions) and their real targets as our references. We
verified the references by Web resources includ-
ing the summary of popular morphs in Wikipedia
2. In addition, we used 23 sensitive morphs and
the entities that appear in the tweets as queries and
retrieved 25, 128 Chinese tweets from 10% Twit-
ter feeds within the same time period, as well as
7, 473 web documents from the embedded URLs
and added them into the uncensored data set.
To evaluate the system performance, we use
leave-one-out cross validation by computing ac-
curacy as Acc@k = CkQ , where Ck is the to-tal number of correctly resolved morphs at top
k ranked answers, and Q is the total number of
morph queries. We consider a morph as correctly
resolved at the top k answers if the top k answer
set contains the real target of the morph.
2http://zh.wikipedia.org/wiki/??????????
1088
Feature sets Descriptions
Surf Surface features
HomB Semantic features extracted from homogeneous CN, PC, PRW, and KLD
HomE HomB + semantic features extracted from homogeneous NCN and PRW/cosine
HetB Semantic features extracted from heterogeneous CN, PC, PRW and KLD
HetE HetB + Semantic features extracted from heterogeneous NCN and PRW/cosine
Glob? Global semantic features
Social Social network features
Table 3: Description of feature sets. ? Glob only uses the same set of similarity measures when combined
with other semantic features.
5.2 Resolution Performance
5.2.1 Single Genre Information
We first study the contributions of each set of sur-
face and semantic features, as shown in the first
five rows in Table 4. The poor performance based
on surface features shows that morph resolution
task is very challenging since 70% of morphs are
not orthographically similar to their real targets.
Thus, capturing a morph?s semantic meaning is
crucial. Overall, the results demonstrate the ef-
fectiveness of our proposed methods. Specifi-
cally, comparing ?HomB? and ?HetB?, ?HomE?
and ?HetE?, we can see that the semantic fea-
tures based on heterogeneous networks have ad-
vantages over those based on homogeneous net-
works. This corroborates that different neighbor
sets contribute differently, and such discrepancies
should be captured. And comparisions of ?HomB?
and ?HomE?, ?HetB? and ?HetE?demonstrate the
effectiveness of our two new proposed measures.
To evaluate the importance of each similarity mea-
sures, we delete the semantic features obtained
from each measure in ?HetE? and re-evaluate the
system. We find that NCN is the most effective
measure, while KLD is the least important one.
Further adding the global semantic features signif-
icantly improves the performance. This indicates
that capturing both temporal correlations and se-
mantics of morphing simultaneously are important
for morph resolution.
Table 5 shows that combination of surface and
semantic features further improves the perfor-
mance, showing that they are complementary. For
example, using only surface features, the real tar-
get ?????Steve Jobs?? of the morph ???
? (Qiao Boss)? is not top ranked since some other
candidates such as ??? (George)? are more or-
thographically similar. However, ?Steve Jobs? is
ranked top when combined with semantic features.
Features Surf HomB HomE HetB HetE
Acc@1 0.028 0.201 0.192 0.224 0.252
Acc@5 0.159 0.313 0.369 0.393 0.421
Acc@10 0.243 0.346 0.407 0.439 0.467
Acc@20 0.313 0.411 0.467 0.50 0.523
Features + Glob + Glob + Glob + Glob
Acc@1 0.230 0.285 0.257 0.285
Acc@5 0.402 0.407 0.449 0.458
Acc@10 0.435 0.458 0.50 0.495
Acc@20 0.486 0.523 0.565 0.542
Table 4: The System Performance Based on Each
Single Feature Set.
Features Surf +
HomB
Surf +
HomE
Surf +
HetB
Surf +
HetE
Acc@1 0.234 0.238 0.262 0.276
Acc@5 0.416 0.444 0.481 0.519
Acc@10 0.477 0.505 0.533 0.570
Acc@20 0.519 0.561 0.565 0.598
Features + Glob + Glob + Glob + Glob
Acc@1 0.290 0.341 0.322 0.346
Acc@5 0.505 0.495 0.528 0.533
Acc@10 0.551 0.551 0.579 0.584
Acc@20 0.594 0.603 0.636 0.631
Table 5: The System Performance Based on Com-
binations of Surface and Semantic Features.
5.2.2 Cross Source and Cross Genre
Information
We integrate the cross source information from
Twitter, and the cross genre information from web
documents into Weibo tweets for information net-
work construction, and extract a new set of se-
mantic features. Table 6 shows that further gains
can be achieved. Notice that integrating tweets
from Twitter mainly improves the ranking for top
k where k > 1. This is because Weibo dominates
our dataset, and in Weibo many of these sensi-
tive morphs are mostly used with their traditional
meanings instead of the morph senses. Further
performance improvement is achieved by integrat-
ing information from background formal web doc-
uments which can provide richer context and rela-
tions.
1089
Features Surf +
HomB +
Glob
Surf +
HomE +
Glob
Surf +
HetB +
Glob
Surf +
HetE +
Glob
Acc@1 0.290 0.341 0.322 0.346
Acc@5 0.505 0.495 0.528 0.533
Acc@10 0.551 0.551 0.579 0.584
Acc@20 0.594 0.603 0.636 0.631
Features + Twit-
ter
+ Twit-
ter
+ Twit-
ter
+ Twit-
ter
Acc@1 0.308 0.336 0.336 0.346
Acc@5 0.514 0.519 0.547 0.565
Acc@10 0.579 0.594 0.594 0.636
Acc@20 0.631 0.640 0.668 0.668
Features + Web + Web + Web + Web
Acc@1 0.327 0.360 0.341 0.379
Acc@5 0.528 0.519 0.565 0.575
Acc@10 0.594 0.589 0.622 0.645
Acc@20 0.631 0.650 0.678 0.678
Table 6: The System Performance of Integrating
Cross Source and Cross Genre Information.
5.2.3 Effects of Social Features
Table 7 shows that adding social features can im-
prove the best performance achieved so far. This is
because a group of people with close relationships
may share similar opinion. As an example, two
tweets ?...of course the reputation of Buhou is a
little too high! //@User1: //@User2: Chongqing
event tells us...)? and ?...do not follow Bo Xi-
lai...@User1...) are from two users in the same
social group.One includes a morph ?Buhou? and
the other includes its target ?Bo Xilai?.
Features Surf +
HomB +
Glob +
Twitter
+ Web
Surf +
HomE +
Glob +
Twitter
+ Web
Surf +
HetB +
Glob +
Twitter
+ Web
Surf +
HetE +
Glob +
Twitter
+ Web
Acc@1 0.327 0.360 0.341 0.379
Acc@5 0.528 0.519 0.565 0.575
Acc@10 0.594 0.589 0.622 0.645
Acc@20 0.631 0.650 0.678 0.678
Features + Social + Social + Social + Social
Acc@1 0.336 0.369 0.365 0.379
Acc@5 0.537 0.547 0.589 0.594
Acc@10 0.594 0.601 0.645 0.659
Acc@20 0.645 0.664 0.701 0.701
Table 7: The Effects of Social Features.
5.3 Effects of Candidate Detection
The performance with and without candidate de-
tection step (using all features) is shown in Ta-
ble 8. The gain is small since the combination
of all features in the learning to rank framework
can already well capture the relationship between
a morph and a target candidate. Nevertheless, the
temporal distribution assumption is effective. It
helps to filter out 80% of unrelated targets and
speed up the system 5 times, while retain 98.5%
of the morph candidates that can be detected.
System Acc@1 Acc@5 Acc@10 Acc@20
Without 0.365 0.579 0.645 0.696
With 0.379 0.594 0.659 0.701
Table 8: The Effects of Temporal Constraint
We also attempted using topic modeling ap-
proach to detect target candidates. Due to the large
amount of data, we first split the data set on a daily
basis, then applied Probabilistic Latent Semantic
Analysis (PLSA) (Hofmann, 1999). Named enti-
ties which co-occur at least ? times with a morph
query in the same topic are selected as its target
candidates. As shown in Table9 (K is the num-
ber of predefined topics), PLSA is not quite effec-
tive mainly because traditional topic modeling ap-
proaches do not perform well on short texts from
social media. Therefore, in this paper we choose
a simple method based on temporal distribution to
detect target candidates.
Method All Temporal PLSA( PLSA(
K = 5 K = 5
? = 1) ? = 2)
Acc 0.935 0.921 0.935 0.925
No. 8, 111 1, 964 6, 380 4, 776
Method PLSA( PLSA( PLSA( PLSA(
K = 10 K = 10 K = 20 K = 20
? = 1) ? = 2) ? = 1) ? = 2)
Acc 0.935 0.907 0.888 0.757
No. 5, 117 3, 138 3, 702 1, 664
Table 9: Accuracy of Target Candidate Detection
5.4 Discussions
Compared with the standard alias detection
(?Surf+HomB?) approach (Hsiung et al, 2005),
our proposed approach achieves significantly bet-
ter performance (99.9% confidence level by the
Wilcoxon Matched-Pairs Signed-Ranks Test for
Acc@1). We further explore two types of factors
which may affect the system performance as fol-
lows.
One important aspect affecting the resolution
performance is the morph & non-morph ambigu-
ity. We categorize a morph query as ?Unique? if
the string is mainly used as a morph when it oc-
curs, such as ??? (Bodu)? which is used to re-
fer to ?Bo Xilai?; otherwise as ?Common? (e.g.
??? (Baby)? ,??? (President)? ). Table 10
presents the separate scores for these two cate-
gories. We can see that the morphs in ?Unique?
1090
category have much better resolution performance
than those in ?Common? category.
Category Number Acc@1 Acc@5 Acc@10 Acc@20
Unique 72 0.479 0.715 0.771 0.819
Common 35 0.171 0.343 0.40 0.429
Table 10: Performance of Two Categories
We also investigate the effects of popularity of
morphs on the resolution performance. We split
the queries into 5 bins with equal size based on the
non-descending frequency, and evaluate Acc@1
separately. As shown in Table11, we can see that
the popularity is not highly correlated with the per-
formance.
Rank 0 ?
20%
20% ?
40%
40% ?
60%
60% ?
80%
80% ?
100%
All 0.333 0.476 0.341 0.429 0.318
Unique 0.321 0.679 0.379 0.571 0.483
Common 0.214 0.214 0.071 0.071 0.286
Table 11: Effects of Popularity of Morphs
6 Related Work
To analyze social media behavior under active
censorship, (Bamman et al, 2012) automatically
discovered politically sensitive terms from Chi-
nese tweets based on message deletion analysis.
In contrast, our work goes beyond target idendi-
fication by resolving implicit morphs to their real
targets.
Our work is closely related to alias detec-
tion (Hsiung et al, 2005; Pantel, 2006; Bollegala
et al, 2011; Holzer et al, 2005). We demon-
strated that state-of-the-art alias detection meth-
ods did not perform well on morph resolution. In
this paper we exploit cross-genre information and
social correlation to measure semantic similarity.
(Yang et al, 2011; Huang et al, 2012) also showed
the effectiveness of exploiting information from
formal web documents to enhance tweet summa-
rization and tweet ranking.
Other similar research lines are the TAC-KBP
Entity Linking (EL) (Ji et al, 2010; Ji et al, 2011),
which links a named entity in news and web docu-
ments to an appropriate knowledge base (KB) en-
try, the task of mining name translation pairs from
comparable corpora (Udupa et al, 2009; Ji, 2009;
Fung and Yee, 1998; Rapp, 1999; Shao and Ng,
2004; Hassan et al, 2007) and the link predic-
tion problem (Adamic and Adar, 2001; Liben-
Nowell and Kleinberg, 2003; Sun et al, 2011b;
Hasan et al, 2006; Wang et al, 2007; Sun et al,
2011a). Most of the work focused on unstruc-
tured or structured data with clean and rich re-
lations (e.g. DBLP). In contrast, our work con-
structs heterogeneous information networks from
unstructured, noisy multi-genre text without ex-
plicit entity attributes.
7 Conclusion and Future Work
To the best of our knowledge, this is the first work
of resolving implicit information morphs from the
data under active censorship. Our promising re-
sults can well serve as a benchmark for this new
problem. Both of the Meta-path based and so-
cial correlation based semantic similarity mea-
surements are proven powerful and complemen-
tary.
In this paper we have focused on entity morphs.
In the future we will extend our method to dis-
cover other types of information morphs, such as
events and nominal mentions. In addition, auto-
matic identification of candidate morphs is another
challenging task, especially when the mentions are
ambiguous and can also refer to other real enti-
ties. Our ongoing work includes identifying can-
didate morphs from scratch, as well as discovering
morphs for a given target based on anomaly anal-
ysis and textual coherence modeling.
Acknowledgments
Thanks to the three anonymous reviewers for their
insightful comments. This work was supported
by the U.S. Army Research Laboratory under Co-
operative Agreement No. W911NF- 09-2-0053
(NS-CTA), the U.S. NSF CAREER Award under
Grant IIS-0953149, the U.S. NSF EAGER Award
under Grant No. IIS-1144111, the U.S. DARPA
FA8750-13-2-0041 - Deep Exploration and Filter-
ing of Text (DEFT) Program, the U.S. DARPA un-
der Agreement No. W911NF-12-C-0028, CUNY
Junior Faculty Award, NSF IIS-0905215, CNS-
0931975, CCF-0905014, and MIAS, a DHS-IDS
Center for Multimodal Information Access and
Synthesis at UIUC. The views and conclusions
contained in this document are those of the au-
thors and should not be interpreted as representing
the official policies, either expressed or implied,
of the U.S. Government. The U.S. Government is
authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copy-
right notation here on.
1091
References
Lada A. Adamic and Eytan Adar. 2001. Friends
and neighbors on the web. SOCIAL NETWORKS,
25:211?230.
Aris Anagnostopoulos, Ravi Kumar, and Mohammad
Mahdian. 2008. Influence and correlation in social
networks. In KDD, pages 7?15.
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2012. Censorship and deletion practices in
chinese social media. First Monday, 17(3).
Patrick Barwise and Sea?n Meehan. 2010. The one
thing you must get right when building a brand.
Harvard Business Review, 88(12):80?84.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2011. Au-
tomatic discovery of personal name aliases from
the web. Knowledge and Data Engineering, IEEE
Transactions on, 23(6):831?844.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, StatMT ?08, pages 224?232.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 1, ACL ?98, pages
414?420.
Mohammad Al Hasan, Vineet Chaoji, Saeed Salem,
and Mohammed Zaki. 2006. Link prediction using
supervised learning. In In Proc. of SDM 06 work-
shop on Link Analysis, Counterterrorism and Secu-
rity.
Ahmed Hassan, Haytham Fahmy, and Hany Has-
san. 2007. Improving named entity translation
by exploiting comparable and parallel corpora. In
RANLP.
Daniel S. Hirschberg. 1977. Algorithms for the
longest common subsequence problem. J. ACM,
24(4):664?675.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?99,
pages 50?57.
Ralf Holzer, Bradley Malin, and Latanya Sweeney.
2005. Email alias detection using social network
analysis. In Conference on Knowledge Discovery
in Data: Proceedings of the 3 rd international work-
shop on Link discovery, volume 21, pages 52?57.
Paul Hsiung, Andrew Moore, Daniel Neill, and Jeff
Schneider. 2005. Alias detection in link data sets.
In Proceedings of the International Conference on
Intelligence Analysis, May.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo
Deng, Dong Wang, Hieu Khac Le, Tarek F. Ab-
delzaher, Jiawei Han, Alice Leung, John Hancock,
and Clare R. Voss. 2012. Tweet ranking based on
heterogeneous networks. In COLING, pages 1239?
1256.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254?262.
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Text Analysis Conference (TAC)
2010.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Text Analysis Conference (TAC) 2011.
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information
networks. In Proceedings of the 2nd Workshop
on Building and Using Comparable Corpora: from
Parallel to Non-parallel Corpora, BUCC ?09, pages
34?37.
David Liben-Nowell and Jon Kleinberg. 2003. The
link prediction problem for social networks. In
Proceedings of the twelfth international conference
on Information and knowledge management, CIKM
?03, pages 556?559.
Ching-Yung Lin, Lynn Wu, Zhen Wen, Hanghang
Tong, Vicky Griffiths-Fisher, Lei Shi, and David
Lubensky. 2012. Social network analysis in enter-
prise. Proceedings of the IEEE, 100(9):2759?2776.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: the first fifteen years. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 1396?
1411.
Patrick Pantel. 2006. Alias detection in malicious en-
vironments. In AAAI Fall Symposium on Capturing
and Using Patterns for Evidence Detection, pages
14?20.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526.
Li Shao and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04.
Yizhou Sun, Rick Barber, Manish Gupta, Charu C. Ag-
garwal, and Han Jiawei. 2011a. Co-author relation-
ship prediction in heterogeneous bibliographic net-
works. In Proceedings of the 2011 International
Conference on Advances in Social Networks Anal-
ysis and Mining, ASONAM ?11, pages 121?128.
1092
Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and
Tianyi Wu. 2011b. Pathsim: Meta path-based top-k
similarity search in heterogeneous information net-
works. PVLDB, 4(11):992?1003.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and
Jagadeesh Jagarlamudi. 2009. Mint: a method
for effective and scalable mining of named entity
transliterations from large comparable corpora. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL ?09, pages 799?807.
Robert A. Wagner and Michael J. Fischer. 1974.
The string-to-string correction problem. J. ACM,
21(1):168?173.
Chao Wang, Venu Satuluri, and Srinivasan
Parthasarathy. 2007. Local probabilistic mod-
els for link prediction. In Proceedings of the 2007
Seventh IEEE International Conference on Data
Mining, ICDM ?07, pages 322?331.
Zhen Wen and Ching-Yung Lin. 2010. On the qual-
ity of inferring interests from social neighbors. In
KDD, pages 373?382.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and
Juanzi Li. 2011. Social context summarization. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, SIGIR ?11, pages 255?264.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the second SIGHAN work-
shop on Chinese language processing - Volume 17,
SIGHAN ?03, pages 184?187.
1093
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 380?390,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Collective Tweet Wikification based on Semi-supervised Graph
Regularization
Hongzhao Huang
1
, Yunbo Cao
2
, Xiaojiang Huang
2
, Heng Ji
1
, Chin-Yew Lin
2
1
Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
2
Microsoft Research Asia, Beijing 100080, P.R.China
{huangh9,jih}@rpi.edu
1
,
{yunbo.cao,xiaojih,cyl}@microsoft.com
2
Abstract
Wikification for tweets aims to automat-
ically identify each concept mention in a
tweet and link it to a concept referent in
a knowledge base (e.g., Wikipedia). Due
to the shortness of a tweet, a collective
inference model incorporating global ev-
idence from multiple mentions and con-
cepts is more appropriate than a non-
collecitve approach which links each men-
tion at a time. In addition, it is chal-
lenging to generate sufficient high quality
labeled data for supervised models with
low cost. To tackle these challenges, we
propose a novel semi-supervised graph
regularization model to incorporate both
local and global evidence from multi-
ple tweets through three fine-grained re-
lations. In order to identify semantically-
related mentions for collective inference,
we detect meta path-based semantic rela-
tions through social networks. Compared
to the state-of-the-art supervised model
trained from 100% labeled data, our pro-
posed approach achieves comparable per-
formance with 31% labeled data and ob-
tains 5% absolute F1 gain with 50% la-
beled data.
1 Introduction
With millions of tweets posted daily, Twitter en-
ables both individuals and organizations to dis-
seminate information, from current affairs to
breaking news in a timely fashion. In this
work, we study the wikification (Disambiguation
to Wikipedia) task (Mihalcea and Csomai, 2007)
for tweets, which aims to automatically identify
each concept mention in a tweet, and link it to a
concept referent in a knowledge base (KB) (e.g.,
Wikipedia). For example, as shown in Figure 1,
Hawks is an identified mention, and its correct ref-
erent concept in Wikipedia is Atlanta Hawks. An
end-to-end wikification system needs to solve two
sub-problems: (i) concept mention detection, (ii)
concept mention disambiguation.
Wikification is a particularly useful task for
short messages such as tweets because it allows
a reader to easily grasp the related topics and en-
riched information from the KB. From a system-
to-system perspective, wikification has demon-
strated its usefulness in a variety of applica-
tions, including coreference resolution (Ratinov
and Roth, 2012) and classification (Vitale et al,
2012).
Sufficient labeled data is crucial for supervised
models. However, manual wikification annota-
tion for short documents is challenging and time-
consuming (Cassidy et al, 2012). The challenges
are: (i) unlinkability, a valid concept may not ex-
ist in the KB. (ii) ambiguity, it is impossible to
determine the correct concept due to the dearth
of information within a single tweet or multiple
correct answer. For instance, it would be diffi-
cult to determine the correct referent concept for
?Gators? in t
1
in Figure 1. Linking ?UCONN?
in t
3
to University of Connecticut may also be ac-
ceptable since Connecticut Huskies is the athletic
team of the university. (iii) prominence, it is chal-
lenging to select a set of linkable mentions that
are important and relevant. It is not tricky to select
?Fans?, ?slump?, and ?Hawks? as linkable men-
tions, but other mentions such as ?stay up? and
?stay positive? are not prominent. Therefore, it
is challenging to create sufficient high quality la-
beled tweets for supervised models and worth con-
sidering semi-supervised learning with the explo-
ration of unlabeled data.
380
    
Stay up Hawk Fans. We are going 
through a slump now, but we have to 
stay positive. Go Hawks!
Congrats to UCONN and Kemba Walker. 
5 wins in 5 days, very impressive...
Just getting to the Arena, we play the 
Bucks tonight. Let's get it!
Fan (person); Mechanical fan
Slump (geology);  Slump (sports)
Atlanta Hawks;  Hawks (film)
University of Connecticut; Connecticut Huskies
Kemba Walker
Arena; Arena (magazine); Arena (TV series)
Bucks County, Pennsylvania; Milwaukee Bucks
Tweets Concept Candidates
Go Gators!!! Florida Gators football; Florida Gators men's basketballt1
t2
t3
t4
Figure 1: An illustration of Wikification Task for Tweets. Concept mentions detected in tweets are
marked as bold, and correctly linked concepts are underlined. The concept candidates are ranked by
their prior popularity which will be explained in section 4.1, and only top 2 ranked concepts are listed.
However, when selecting semi-supervised
learning frameworks, we noticed another unique
challenge that tweets pose to wikification due
to their informal writing style, shortness and
noisiness. The context of a single tweet usually
cannot provide enough information for prominent
mention detection and similarity computing for
disambiguation. Therefore, a collective inference
model over multiple tweets in the semi-supervised
setting is desirable. For instance, the four tweets
in Figure 1 are posted by the same author within
a short time period. If we perform collective
inference over them we can reliably link am-
biguous mentions such as ?Gators?, ?Hawks?,
and ?Bucks? to basketball teams instead of other
concepts such as the county Bucks County.
In order to address these unique challenges
for wikification for the short tweets, we employ
graph-based semi-supervised learning algorithms
(Zhu et al, 2003; Smola and Kondor, 2003; Blum
et al, 2004; Zhou et al, 2004; Talukdar and
Crammer, 2009) for collective inference by ex-
ploiting the manifold (cluster) structure in both
unlabeled and labeled data. These approaches
normally assume label smoothness over a defined
graph, where the nodes represent a set of labeled
and unlabeled instances, and the weighted edges
reflect the closeness of each pair of instances. In
order to construct a semantic-rich graph capturing
the similarity between mentions and concepts for
the model, we introduce three novel fine-grained
relations based on a set of local features, social
networks and meta paths.
The main contributions of this paper are sum-
marized as follows:
? To the best of our knowledge, this is the first
effort to explore graph-based semi-supervised
learning algorithms for the wikification task.
? We propose a novel semi-supervised graph reg-
ularization model performing collective infer-
ence for joint mention detection and disam-
biguation. Our approach takes advantage of
three proposed principles to incorporate both lo-
cal and global evidence from multiple tweets.
? We propose a meta path-based unified frame-
work to detect both explicitly and implicitly rel-
evant mentions.
2 Preliminaries
Concept and Concept Mention We define a con-
cept c as a Wikipedia article (e.g., Atlanta Hawks),
and a concept mentionm as an n-gram from a spe-
cific tweet. Each concept has a set of textual repre-
sentation fields (Meij et al, 2012), including title
(the title of the article), sentence (the first sentence
of the article), paragraph (the first paragraph of
the article), content (the entire content of the arti-
cle), and anchor (the set of all anchor texts with
incoming links to the article).
Wikipedia Lexicon Construction We first
construct an offline lexicon with each entry as
?m, {c
1
, ..., c
k
}?, where {c
1
, ..., c
k
} is the set of
possible referent concepts for the mention m.
Following the previous work (Bunescu, 2006;
Cucerzan, 2007; Hachey et al, 2013), we extract
the possible mentions for a given concept c using
the following resources: the title of c; the aliases
appearing in the introduction and infoboxes of c
(e.g., The Evergreen State is an alias of Wash-
ington state); the titles of pages redirecting to c
(e.g., State of Washington is a redirecting page of
Washington (state)); the titles of the disambigua-
381
tion pages containing c; and all the anchor texts
appearing in at least 5 pages with hyperlinks to c
(e.g., WA is a mention for the concept Washing-
ton (state) in the text ?401 5th Ave N [[Seattle]],
[[Washington (state)?WA]] 98109 USA?. We also
propose three heuristic rules to extract mentions
(i.e., different combinations of the family name
and given name for a person, the headquarters of
an organization, and the city name for a sports
team).
Concept Mention Extraction Based on the
constructed lexicon, we then consider all n-grams
of size? n (n=7 in this paper) as concept mention
candidates if their entries in the lexicon are not
empty. We first segment @usernames and #hash-
tags into regular tokens (e.g., @amandapalmer is
segmented as amanda palmer and #WorldWater-
Day is split as World Water Day) using the ap-
proach proposed by (Wang et al, 2011). Segmen-
tation assists finding concept candidates for these
non-regular mentions.
3 Principles and Approach Overview
    
R elational Graph Construction
Knowledge Base 
(Wikipedia)
Labeled and 
Unlabeled Tweets
Wikipedia Lex icon Construction
Concept Mention and 
Concept Candidate E x traction
Local Compatibility
(local features, 
cosine similarity)
Coreference
(meta path,
mention 
similarity)
Semantic R elatedness
(meta path, concept 
semantic relatedness)
Semi- Supervised Graph R egularization
<Mention, Concept>
Pairs
Figure 2: Approach Overview.
3.1 Principles
A single tweet may not provide enough evidence
to identify prominent mentions and infer their cor-
rect referent concepts due to the lack of contextual
information. To tackle this problem, we propose to
incorporate global evidence from multiple tweets
and performing collective inference for both men-
tion identification and disambiguation. We first in-
troduce the following three principles that our ap-
proach relies on.
Principle 1 (Local compatibility): Two pairs
of ?m, c? with strong local compatibility tend to
have similar labels. Mentions and their correct
referent concepts usually tend to share a set of
characteristics such as string similarity betweenm
and c (e.g., ?Chicago, Chicago? and ?Facebook,
Facebook?). We define the local compatibility to
model such set of characteristics.
Principle 2 (Coreference): Two coreferential
mentions should be linked to the same concept.
For example, if we know ?nc? and ?North Car-
olina? are coreferential, then they should both be
linked to North Carolina.
Principle 3 (Semantic Relatedness): Two
highly semantically-related mentions are more
likely to be linked to two highly semantically-
related concepts. For instance, when ?Sweet 16?
and ?Hawks? often appear together within rel-
evant contexts, they can be reliably linked to
two baseketball-related concepts NCAA Men?s Di-
vision I Basketball Championship and Atlanta
Hawks, respectively.
3.2 Approach Overview
Given a set of tweets ?t
1
, ..., t
|T |
?, our system first
generates a set of candidate concept mentions, and
then extracts a set of candidate concept referents
for each mention based on the Wikipedia lexicon.
Given a pair of mention and its candidate referent
concept ?m, c?, the remaining task of wikification
is to assign either a positive label if m should be
selected as a prominently linkable mention and c
is its correct referent concept, or otherwise a neg-
ative label. The label assignment is obtained by
our semi-supervised graph regularization frame-
work based on a relational graph, which is con-
structed from local compatibility, coreference, and
semantic relatedness relations. The overview of
our approach is as illustrated in Figure 2.
4 Relational Graph Construction
We first construct the relational graphG = ?V,E?,
where V = {v
1
, ..., v
n
} is a set of nodes and E =
{e
1
, ..., e
m
} is a set of edges. Each v
i
= ?m
i
, c
i
?
represents a tuple of mention m
i
and its referent
concept candidate c
i
. An edge is added between
two nodes v
i
and v
j
if there is a proposed rela-
tion based on the three principles described in sec-
tion 3.1.
4.1 Local Compatibility
We first compute local compatibility (Principle 1)
by considering a set of novel local features to cap-
382
ture the importance and relevance of a mention m
to a tweet t, as well as the correctness of its link-
age to a concept c. We have designed a number
of features which are similar to those commonly
used in wikification and entity linking work (Meij
et al, 2012; Guo et al, 2013; Mihalcea and Cso-
mai, 2007).
Mention Features We define the following fea-
tures based on information from mentions.
? IDF
f
(m) = log(
|C|
df(m)
), where |C| is the total
number of concepts in Wikipedia and df(m) is
the total number of concepts in whichm occurs,
and f indicates the field property, including ti-
tle, content, and anchor.
? Keyphraseness(m) =
|C
a
(m)|
df(m)
to measure
how likely m is used as an anchor in Wikipedia,
where C
a
(m) is the set of concepts where m
appears as an anchor.
? LinkProb(m) =
?
c?C
a
(m)
count(m,c)
?
c?C
count(m,c)
, where
count(m, c) indicates the number of occurrence
of m in c.
? SNIL(m) and SNCL(m) to count the number
of concepts that are equal to or contain a sub-n-
gram of m, respectively (Meij et al, 2012).
Concept Features The concept features are
solely based on Wikipedia, including the number
of incoming and outgoing links for c, and the num-
ber of words and characters in c.
Mention + Concept Features This set of fea-
tures considers information from both mentions
and concepts:
? prior popularity prior(m, c) =
count(m,c)?
c
?
count(m,c
?
)
, where count(m, c) mea-
sures the frequency of the anchor links from m
to c in Wikipedia.
? TF
f
(m, c) =
count
f
(m,c)
|f |
to measure the rela-
tive frequency of m in each field representation
f of c, normalized by the length of f . The fields
include title, sentence, paragraph, content and
anchor.
? NCT (m, c), TCN(m, c), and TEN(m, c) to
measure whether m contains the title of c,
whether the title of c contains m, and whether
m equals to the title of c, respectively.
Context Features This set of features include
(i) Context Capitalization features, which indicate
whether the current mention, the token before, and
the token after are capitalized. (ii) tf-idf based fea-
tures, which include the dot product of two word
vectors v
c
and v
t
, and the average tf-idf value of
common items in v
c
and v
t
, where v
c
and v
t
are
the top 100 tf-idf word vectors in c and t.
Local Compatibility Computation For each
node v
i
= ?m
i
, c
i
?, we collect its local features
as a feature vector F
i
= ?f
1
, f
2
, ..., f
d
?. To avoid
features with large numerical values that domi-
nate other features, the value of each feature is
re-scaled using feature standardization approach.
The cosine similarity is then adopted to compute
the local compatibility of two nodes and construct
a k nearest neighbor (kNN) graph, where each
node is connected to its k nearest neighboring
nodes. We compute the weight matrix that rep-
resents the local compatibility relation as:
W
loc
ij
=
{
cosine(F
i
, F
j
) j ? kNN(i)
0 Otherwise
4.2 Meta Path
    
Mention
Hashtag
Tweet User
post - 1
post
contain - 1
contain
contain - 1 contain
Figure 3: Schema of the Twitter network.
In this subsection, we introduce the concept
meta path which will be used to detect corefer-
ence (section 4.3) and semantic relatedness rela-
tions (section 4.4).
A meta-path is a path defined over a network
and composed of a sequence of relations between
different object types (Sun et al, 2011b). In our
experimental setting, we can construct a natu-
ral Twitter network summarized by the network
schema in Figure 3. The network contains four
types of objects: Mention (M), Tweet (T), User
(U), and Hashtag (H). Tweets and mentions are
connected by links ?contain? and ?contained by?
(denoted as ?contain
?1
?); and other linked rela-
tionships can be described similarly.
We then define the following five types of meta
paths to connect two mentions as:
? ?M - T - M?,
? ?M - T - U - T - M?,
? ?M - T - H - T - M?,
? ?M - T - U - T - M - T - H - T - M?,
? ?M - T - H - T - M - T - U - T - M?.
383
Each meta path represents one particular seman-
tic relation. For instance, the first three paths are
basic ones expressing the explicit relations that
two mentions are from the same tweet, posted by
the same user, and share the same #hashtag, re-
spectively. The last two paths are concatenated
ones which are constructed by concatenating the
first three simple paths to express the implicit rela-
tions that two mentions co-occur with a third men-
tion sharing either the same authorship or #hash-
tag. Such complicated paths can be exploited to
detect more semantically-related mentions from
wider contexts. For example, the relational link
between ?narita airport? and ?Japan? would be
missed without using the path ?narita airport - t
1
- u
1
- t
2
- american - t
3
- h
1
- t
4
- Japan? since they
don?t directly share any authorships or #hashtags.
4.3 Coreference
A coreference relation (Principle 2) usually occurs
across multiple tweets due to the highly redundant
information in Twitter. To ensure high precision,
we propose a simple yet effective approach utiliz-
ing the rich social network relations in Twitter.
We consider two mentions m
i
and m
j
corefer-
ential if m
i
and m
j
share the same surface form
or one is an abbreviation of the other, and at least
one meta path exists betweenm
i
andm
j
. Then we
define the weight matrix representing the corefer-
ential relation as:
W
coref
ij
=
?
?
?
1.0 if m
i
and m
j
are coreferential,
and c
i
= c
j
0 Otherwise
4.4 Semantic Relatedness
Ensuring topical coherence (Principle 3) has been
beneficial for wikification on formal texts (e.g.,
News) by linking a set of semantically-related
mentions to a set of semantically-related concepts
simultaneously (Han et al, 2011; Ratinov et al,
2011; Cheng and Roth, 2013). However, the short-
ness of a single tweet means that it may not pro-
vide enough topical clues. Therefore, it is impor-
tant to extend this evidence to capture semantic re-
latedness information from multiple tweets.
We define the semantic relatedness score be-
tween two mentions as SR(m
i
,m
j
) = 1.0 if at
least one meta path exists between m
i
and m
j
,
otherwise SR(m
i
,m
j
) = 0. In order to compute
the semantic relatedness of two concepts c
i
and
c
j
, we adopt the approach proposed by (Milne and
Witten, 2008a):
SR(c
i
, c
j
) = 1?
logmax(|C
i
|, |C
j
|)? log |C
i
? C
j
|
log(|C|)? logmin(|C
i
|, |C
j
|)
,
where |C| is the total number of concepts in
Wikipedia, and C
i
and C
j
are the set of concepts
that have links to c
i
and c
j
, respectively.
Then we compute a weight matrix representing
the semantic relatedness relation as:
W
rel
ij
=
{
SR(N
i
, N
j
) if SR(N
i
, N
j
) ? ?
0 Otherwise
where SR(N
i
, N
j
) = SR(m
i
,m
j
) ? SR(c
i
, c
j
)
and ? = 0.3, which is optimized from a develop-
ment set.
4.5 The Combined Relational Graph
    
hawks, 
 Atlanta Hawks
uconn, 
Connecticut 
Huskies
bucks, 
Milwaukee 
Bucks
kemba walker, 
Kemba Walker
0 .40 4
gators, 
Florida Gators 
men's basketball
now, 
N ow
days, 
D ay
tonight, 
Tonight
0 .9 32
0 .7 6 4
0 .6 6 5
0 .46 7
0 .56 3 0 .538
0 .447
Figure 4: A example of the relational graph con-
structed for the example tweets in Figure 1. Each
node represents a pair of ?m, c?, separated by a
comma. The edge weight is obtained from the lin-
ear combination of the weights of the three pro-
posed relations. Not all mentions are included due
to the space limitations.
Based on the above three weight matricesW
loc
,
W
coref
, and W
rel
, we first obtain their corre-
sponding transition matrices P
loc
, P
coref
, and
P
rel
, respectively. The entry P
ij
of the transition
matrix P for a weight matrix W is computed as
P
ij
=
W
ij?
k
W
ik
such that
?
k
P
ik
= 1. Then we
obtain the combined graph G with weight matrix
W , where W
ij
= ?P
loc
ij
+ ?P
coref
ij
+ ?P
rel
ij
. ?,
?, and ? are three coefficients between 0 and 1
with the constraint that ?+ ? + ? = 1. They con-
trol the contributions of these three relations in our
semi-supervised graph regularization model. We
choose transition matrix to avoid the domination
of one relation over others. An example graph of
G is shown in Figure 4. Compared to the referent
graph which considers each mention or concept
as a node in previous graph-based re-ranking ap-
proaches (Han et al, 2011; Shen et al, 2013), our
384
novel graph representation has two advantages: (i)
It can easily incorporate more features related to
both mentions and concepts. (ii) It is more appro-
priate for our graph-based semi-supervised model
since it is difficult to assign labels to a pair of men-
tion and concept in the referent graph.
5 Semi-supervised Graph Regularization
Given the constructed relational graph with the
weighted matrix W and the label vector Y of all
nodes, we assume the first l nodes are labeled as
Y
l
and the remaining u nodes (u = n? l) are ini-
tialized with labels Y
0
u
. Then our goal is to refine
Y
0
u
and obtain the final label vector Y
u
.
Intuitively, if two nodes are strongly connected,
they tend to hold the same label. We propose a
novel semi-supervised graph regularization frame-
work based on the graph-based semi-supervised
learning algorithm (Zhu et al, 2003):
Q(Y) = ?
n
?
i=l+1
(y
i
?y
0
i
)
2
+
1
2
?
i,j
W
ij
(y
i
?y
j
)
2
.
The first term is a loss function that incorporates
the initial labels of unlabeled examples into the
model. In our method, we adopt prior popular-
ity (section 4.1) to initialize the labels of the un-
labeled examples. The second term is a regular-
izer that smoothes the refined labels over the con-
structed graph. ? is a regularization parameter that
controls the trade-off between initial labels and the
consistency of labels on the graph. The goal of the
proposed framework is to ensure that the refined
labels of unlabeled nodes are consistent with their
strongly connected nodes, as well as not too far
away from their initial labels.
The above optimization problem can be solved
directly since Q(Y) is convex (Zhu et al, 2003;
Zhou et al, 2004). Let I be an identity matrix
and D
W
be a diagonal matrix with entries D
ii
=
?
j
W
ij
. We can split the weighted matrix W into
four blocks as W =
[
W
ll
W
lu
W
ul
W
uu
]
, where W
mn
is
anm?nmatrix. D
w
is split similarly. We assume
that the vector of the labeled examples Y
l
is fixed,
so we only need to infer the refined label vector of
the unlabeled examples Y
u
. In order to minimize
Q(Y), we need to find Y
?
u
such that
?Q
?Y
u
?
?
?
?
Y
u
=Y
?
u
= (D
uu
+ ?I
uu
)Y
u
?W
uu
Y
u
?
W
ul
Y
l
? ?Y
0
u
= 0.
Therefore, a closed form solution can be derived
as Y
?
u
= (D
uu
+ ?I
uu
?W
uu
)
?1
(W
ul
Y
l
+ ?Y
0
u
).
However, for practical application to a large-
scale data set, an iterative solution would be more
efficient to solve the optimization problem. Let
Y
t
u
be the refined labels after the t
th
iteration, the
iterative solution can be derived as:
Y
t+1
u
= (D
uu
+?I
uu
)
?1
(W
uu
Y
t
u
+W
ul
Y
l
+?Y
0
u
).
The iterative solution is more efficient since
(D
uu
+ ?I
uu
) is a diagonal matrix and its inverse
is very easy to compute.
6 Experiments
In this section we compare our approach with
state-of-the-art methods as shown in Table 1.
6.1 Data and Scoring Metric
For our experiments we use a public data set (Meij
et al, 2012) including 502 tweets posted by 28
verified users. The data set was annotated by two
annotators. We randomly sample 102 tweets for
development and the remaining for evaluation. We
use a Wikipedia dump on May 3, 2013 as our
knowledge base, which includes 30 million pages.
For computational efficiency, we also filter some
mention candidates by applying the preprocess-
ing approach proposed in (Ferragina and Scaiella,
2010), and remove all the concepts with prior pop-
ularity less than 2% from an mention?s concept set
for each mention, similar to (Guo et al, 2013).
A mention and concept pair ?m, c? is judged as
correct if and only if m is linkable and c is the
correct referent concept for m. To evaluate the
performance of a wikification system, we use the
standard precision, recall and F1 measures.
6.2 Experimental Results
The overall performance of various approaches
is shown in Table 2. The results of the super-
vised method proposed by (Meij et al, 2012) are
obtained from 5-fold cross validation. For our
semi-supervised setting, we experimentally sam-
ple 200 tweets for training and use the remain-
ing set as unlabeled and testing sets. In our semi-
supervised regularization model, the matrix W
loc
is constructed by a kNN graph (k = 20). The reg-
ularization parameter ? is empirically set to 0.1,
and the coefficients ?, ?, and ? are learnt from the
development set by considering all the combina-
385
Methods Descriptions
TagMe The same approach that is described in (Ferragina and Scaiella, 2010), which aims to annotate short
texts based on prior popularity and semantic relatedness of concepts. It is basically an unsupervised
approach, except that it needs a development set to tune the probability threshold for linkable mentions.
Meij A state-of-the-art system described in (Meij et al, 2012), which is a supervised approach based on the
random forest model. It performs mention detection and disambiguation jointly, and it is trained from
400 labeled tweets.
SSRegu
1
Our proposed model based on Principle 1, using 200 labeled tweets.
SSRegu
12
Our proposed model based on Principle 1 and 2, using 200 labeled tweets.
SSRegu
13
Our proposed model based on Principle 1 and 3, using 200 labeled tweets.
SSRegu
123
Our proposed full model based on Principle 1, 2 and 3, using 200 labeled tweets.
Table 1: Description of Methods.
Methods Precision Recall F1
TagMe 0.329 0.423 0.370
Meij 0.393 0.598 0.475
SSRegu
1
0.538 0.435 0.481
SSRegu
12
0.638 0.438 0.520
SSRegu
13
0.541 0.457 0.495
SSRegu
123
0.650 0.441 0.525
Table 2: Overall Performance.
tions of values from 0 to 1 at 0.1 intervals
1
. In
order to randomize the experiments and make the
comparison fair, we conduct 20 test runs for each
method and report the average scores across the 20
trials.
The relatively low performance of the baseline
system TagMe demonstrates that only relying on
prior popularity and topical information within a
single tweet is not enough for an end-to-end wik-
ification system for the short tweets. As an exam-
ple, it is difficult to obtain topical clues in order
to link the mention ?Clinton? to Hillary Rodham
Clinton by relying on the single tweet ?wolfblitzer-
cnn: Behind the scenes on Clinton?s Mideast trip
#cnn?. Therefore, the system mistakenly links it
to the most popular concept Bill Clinton.
In comparision with the supervised baseline
proposed by (Meij et al, 2012), our model
SSRegu
1
relying on local compatibility already
achieves comparable performance with 50% of
labeled data. This is because that our model
performs collective inference by making use of
the manifold (cluster) structure of both labeled
and unlabeled data, and that the local compat-
ibility relation is detected with high precision
2
(89.4%). For example, the following three pairs
of mentions and concepts ?pelosi, Nancy Pelosi?,
?obama, Barack Obama?, and ?gaddafi, Muam-
1
These three coefficients are slightly different with differ-
ent training data, a sample of them is: ? = 0.4, ? = 0.5, and
? = 0.1
2
Here we define precision as the percentage of links that
holds the same label.
mar Gaddafi? have strong local compatibility with
each other since they share many similar char-
acteristics captured by the local features such as
string similarity between the mention and the con-
cept. Suppose the first pair is labeled, then its pos-
itive label will be propagated to other unlabeled
nodes through the local compatibility relation, and
correctly predict the labels of other nodes.
Incorporating coreferential or semantic related-
ness relation into SSRegu
1
provides further gains,
demonstrating the effectiveness of these two re-
lations. For instance, ?wh? is correctly linked to
White House by incorporating evidence from its
coreferential mention ?white house?. The corefer-
ential relation (Principle 2) is demonstrated to be
more beneficial than the semantic relatedness re-
lation (Principle 3) because the former is detected
with much higher precision (99.7%) than the latter
(65.4%).
Our full model SSRegu
123
achieves significant
improvement over the supervised baseline (5% ab-
solute F1 gain with 95.0% confidence level by
the Wilcoxon Matched-Pairs Signed-Ranks Test),
showing that incorporating global evidence from
multiple tweets with fine-grained relations is ben-
eficial. For instance, the supervised baseline fails
to link ?UCONN? and ?Bucks? in our examples
to Connecticut Huskies and Milwaukee Bucks, re-
spectively. Our full model corrects these two
wrong links by propagating evidence through the
semantic links as shown in Figure 4 to obtain mu-
tual ranking improvement. The best performance
of our full model also illustrates that the three re-
lations complement each other.
We also study the disambiguation performance
for the annotated mentions, as shown in Table 3.
We can easily see that our proposed approach
using 50% labeled data achieves similar perfor-
mance with the state-of-the-art supervised model
with 100% labeled data. When the mentions are
given, the unpervised approach TagMe has already
386
Methods TagMe Meij SSRegu
123
Accuracy 0.710 0.779 0.772
Table 3: Disambiguation Performance.
Methods Precision Recall F1
SSRegu
12
0.644 0.423 0.510
SSRegu
13
0.543 0.441 0.486
SSRegu
123
0.657 0.419 0.512
Table 4: The Performance of Systems Without Us-
ing Concatenated Meta Paths.
achieved reasonable performance. In fact, mention
detection actually is the performance bottleneck of
a tweet wikification system (Guo et al, 2013). Our
system performs better in identifying the promi-
nent mention.
6.3 Effect of Concatenated Meta Paths
In this work, we propose a unified framework uti-
lizing meta path-based semantic relations to ex-
plore richer relevant context. Beyond the basic
meta paths, we introduce concatenated ones by
concatenating the basic ones. The performance of
the system without using the concatenated meta
paths is shown in Table 4. In comparison with
the system based on all defined meta paths, we
can clearly see that the systems using concate-
nated ones outperform those relying on the sim-
ple ones. This is because the concatenated meta
paths can incorporate more relevant information
with implicit relations into the models by increas-
ing 1.6% coreference links and 9.3% semantic re-
latedness links. For example, the mention ?narita
airport? is correctly disambiguated to the concept
?Narita International Airport? with higher confi-
dence since its semantic relatedness relation with
?Japan? is detected with the concatenated meta
path as described in section 4.2.
6.4 Effect of Labeled Data Size
5 0 1 0 0 1 5 0 2 0 0 2 5 0 3 0 0 3 5 0 4 0 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 L a b e l e d  T w e e t  S i z e S S R e g u 1 2 3 M e i j
Figure 5: The effect of Labeled Tweet Size.
In previous experiments, we experimentally set
the number of labeled tweets to be 200 for over-
all performance comparision with the baselines.
In this subsection, we study the effect of labeled
data size on our full model. We randomly sam-
ple 100 tweets as testing data, and randomly se-
lect 50, 100, 150, 200, 250, and 300 tweets as
labeled data. 20 test runs are conducted and the
average results are reported across the 20 trials,
as shown in Figure 5. We find that as the size
of the labeled data increases, our proposed model
achieves better performance. It is encouraging to
see that our approach, with only 31.3% labeled
tweets (125 out of 400), already achieves a perfor-
mance that is comparable to the state-of-the-art su-
pervised model trained from 100% labeled tweets.
6.5 Parameter Analysis
0 . 1 0 . 5 1 2 5 1 0 2 0 3 0 4 0 5 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 R e g u l a r i z a t i o n  P a r a m e t e r  ? S S R e g u 1 2 3
Figure 6: The effect of parameter ?.
In previous experiments, we empirically set the
parameter ? = 0.1. ? is the regularization pa-
rameter that controls the trade-off between initial
labels and the consistency of labels on the graph.
When ? increases, the model tends to trust more in
the initial labels. Figure 6 shows the performance
of our models by varying ? from 0.02 to 50. We
can easily see that the system performce is stable
when ? < 0.4. However, when ? ? 0.4, the sys-
tem performance dramatically decreases, showing
that prior popularity is not enough for an end-to-
end wikification system.
7 Related Work
The task of linking concept mentions to a knowl-
edge base has received increased attentions over
the past several years, from the linking of concept
mentions in a single text (Mihalcea and Csomai,
2007; Milne and Witten, 2008b; Milne and Witten,
2008a; Kulkarni et al, 2009; He et al, 2011; Rati-
nov et al, 2011; Cassidy et al, 2012; Cheng and
Roth, 2013), to the linking of a cluster of corefer-
387
ent named entity mentions spread throughout dif-
ferent documents (Entity Linking) (McNamee and
Dang, 2009; Ji et al, 2010; Zhang et al, 2010; Ji et
al., 2011; Zhang et al, 2011; Han and Sun, 2011;
Han et al, 2011; Gottipati and Jiang, 2011; He et
al., 2013; Li et al, 2013; Guo et al, 2013; Shen et
al., 2013; Liu et al, 2013).
A significant portion of recent work considers
the two sub-problems mention detection and men-
tion disambiguation separately and focus on the
latter by first defining candidate concepts for a
deemed mention based on anchor links. Men-
tion disambiguation is then formulated as a rank-
ing problem, either by resolving one mention at
each time (non-collective approaches), or by dis-
ambiguating a set of relevant mentions simulta-
neously (collective approaches). Non-collective
methods usually rely on prior popularity and con-
text similarity with supervised models (Mihalcea
and Csomai, 2007; Milne and Witten, 2008b; Han
and Sun, 2011), while collective approaches fur-
ther leverage the global coherence between con-
cepts normally through supervised or graph-based
re-ranking models (Cucerzan, 2007; Milne and
Witten, 2008b; Han and Zhao, 2009; Kulkarni et
al., 2009; Pennacchiotti and Pantel, 2009; Ferrag-
ina and Scaiella, 2010; Fernandez et al, 2010;
Radford et al, 2010; Cucerzan, 2011; Guo et al,
2011; Han and Sun, 2011; Han et al, 2011; Rati-
nov et al, 2011; Chen and Ji, 2011; Kozareva et
al., 2011; Cassidy et al, 2012; Shen et al, 2013;
Liu et al, 2013). Especially note that when apply-
ing the collective methods to short messages from
social media, evidence from other messages usu-
ally needs to be considered (Cassidy et al, 2012;
Shen et al, 2013; Liu et al, 2013). Our method
is a collective approach with the following novel
advancements: (i) A novel graph representation
with fine-grained relations, (ii) A unified frame-
work based on meta paths to explore richer rele-
vant context, (iii) Joint identification and linking
of mentions under semi-supervised setting.
Two most similar methods to ours were pro-
posed by (Meij et al, 2012; Guo et al, 2013)
by performing joint detection and disambiguation
of mentions. (Meij et al, 2012) studied several
supervised machine learning models, but without
considering any global evidence either from a sin-
gle tweet or other relevant tweets. (Guo et al,
2013) explored second order entity-to-entity rela-
tions but did not incorporate evidence from multi-
ple tweets.
This work is also related to graph-based semi-
supervised learning (Zhu et al, 2003; Smola
and Kondor, 2003; Zhou et al, 2004; Talukdar
and Crammer, 2009), which has been success-
fully applied in many Natural Language Process-
ing tasks (Niu et al, 2005; Chen et al, 2006).
We introduce a novel graph that incorporates three
fine-grained relations. Our work is further re-
lated to meta path-based heterogeneous informa-
tion network analysis (Sun et al, 2011b; Sun et
al., 2011a; Kong et al, 2012; Huang et al, 2013),
which has demonstrated advantages over homoge-
neous information network analysis without dif-
ferentiating object types and relational links.
8 Conclusions
We have introduced a novel semi-supervised graph
regularization framework for wikification to si-
multaneously tackle the unique challenges of an-
notation and information shortage in short tweets.
To the best of our knowledge, this is the first work
to explore the semi-supervised collective inference
model to jointly perform mention detection and
disambiguation. By studying three novel fine-
grained relations, detecting semantically-related
information with semantic meta paths, and ex-
ploiting the data manifolds in both unlabeled and
labeled data for collective inference, our work can
dramatically save annotation cost and achieve bet-
ter performance, thus shed light on the challenging
wikification task for tweets.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149, U.S.
DARPA Award No. FA8750-13-2-0041 in the
Deep Exploration and Filtering of Text (DEFT)
Program, IBM Faculty Award, Google Research
Award and RPI faculty start-up grant. The views
and conclusions contained in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
388
References
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2004. Semi-supervised learning using randomized
mincuts. In Proceedings of the Twenty-first Interna-
tional Conference on Machine Learning, ICML ?04.
Razvan Bunescu. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In EACL,
pages 9?16.
T. Cassidy, H. Ji, L. Ratinov, A. Zubiaga, and
H. Huang. 2012. Analysis and enhancement of wik-
ification for microblogs with context expansion. In
Proceedings of COLING 2012.
Z. Chen and H. Ji. 2011. Collaborative ranking: A
case study on entity linking. In Proc. EMNLP2011.
J. Chen, D. Ji, C Tan, and Z. Niu. 2006. Rela-
tion extraction using label propagation based semi-
supervised learning. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
X. Cheng and D. Roth. 2013. Relational inference
for wikification. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007.
S. Cucerzan. 2011. Tac entity linking by performing
full-document entity extraction and disambiguation.
In Proc. TAC 2011 Workshop.
N. Fernandez, J. A. Fisteus, L. Sanchez, and E. Mar-
tin. 2010. Webtlab: A cooccurence-based approach
to kbp 2010 entity-linking task. In Proc. TAC 2010
Workshop.
P. Ferragina and U. Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ?10.
S. Gottipati and J. Jiang. 2011. Linking entities to a
knowledge base with query expansion. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing.
Y. Guo, W. Che, T. Liu, and S. Li. 2011. A graph-
based method for entity linking. In Proc. IJC-
NLP2011.
S. Guo, M. Chang, and E. Kiciman. 2013. To link
or not to link? a study on end-to-end tweet entity
linking. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
B. Hachey, W. Radford, J. Nothman, M. Honnibal, and
J. Curran. 2013. Evaluating entity linking with
wikipedia. Artif. Intell.
X. Han and L. Sun. 2011. A generative entity-mention
model for linking entities with knowledge base. In
Proc. ACL2011.
X. Han and J. Zhao. 2009. Named entity disam-
biguation by leveraging wikipedia semantic knowl-
edge. In Proceedings of the 18th ACM conference
on Information and knowledge management, CIKM
2009.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: A graph-based method. In Proc.
SIGIR2011.
J. He, M. de Rijke, M. Sevenster, R. van Ommering,
and Y. Qian. 2011. Generating links to background
knowledge: A case study using narrative radiology
reports. In Proceedings of the 20th ACM inter-
national conference on Information and knowledge
management. ACM.
Z. He, S. Liu, Y. Song, M. Li, M. Zhou, and H. Wang.
2013. Efficient collective entity linking with stack-
ing. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing.
H. Huang, Z. Wen, D. Yu, H. Ji, Y. Sun, J. Han, and
H. Li. 2013. Resolving entity morphs in censored
data. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Text Analysis Conference (TAC)
2010.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Text Analysis Conference (TAC) 2011.
X. Kong, P. Yu, Y. Ding, and J. Wild. 2012. Meta
path-based collective classification in heterogeneous
information networks. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, CIKM ?12.
Z. Kozareva, K. Voevodski, and S. Teng. 2011. Class
label enhancement via related instances. In Proc.
EMNLP2011.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In KDD.
Y. Li, C. Wang, F. Han, J. Han, D. Roth, and X. Yan.
2013. Mining evidences for named entity dis-
ambiguation. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?13.
389
X. Liu, Y. Li, H. Wu, M. Zhou, F. Wei, and Y. Lu.
2013. Entity linking for tweets. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
P. McNamee and H.T. Dang. 2009. Overview of the
tac 2009 knowledge base population track. In Text
Analysis Conference (TAC) 2009.
E. Meij, W. Weerkamp, and M. de Rijke. 2012.
Adding semantics to microblog posts. In Proceed-
ings of the fifth ACM international conference on
Web search and data mining, WSDM ?12.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking
documents to encyclopedic knowledge. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07.
D. Milne and I.H. Witten. 2008a. Learning to link
with wikipedia. In An effective, low-cost measure of
semantic relatedness obtained from wikipedia links.
the Wikipedia and AI Workshop of AAAI.
D. Milne and I.H. Witten. 2008b. Learning to link
with wikipedia. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
Z. Niu, D. Ji, and C. Tan. 2005. Word sense dis-
ambiguation using label propagation based semi-
supervised learning. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
M. Pennacchiotti and P. Pantel. 2009. Entity extraction
via ensemble semantics. In Proc. EMNLP2009.
W. Radford, B. Hachey, J. Nothman, M. Honnibal, and
J. R. Curran. 2010. Cmcrc at tac10: Document-
level entity linking with graph-based re-ranking. In
Proc. TAC 2010 Workshop.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proc. of the Annual Meeting of
the Association of Computational Linguistics (ACL).
W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Link-
ing named entities in tweets with knowledge base
via user interest modeling. In Proceedings of the
19th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?13.
A. Smola and R. Kondor. 2003. Kernels and regular-
ization on graphs. COLT.
Y. Sun, R. Barber, M. Gupta, C. Aggarwal, and J. Han.
2011a. Co-author relationship prediction in hetero-
geneous bibliographic networks. In Proceedings of
the 2011 International Conference on Advances in
Social Networks Analysis and Mining, ASONAM
?11.
Y. Sun, J. Han, X. Yan, P. Yu, and T. Wu. 2011b. Path-
sim: Meta path-based top-k similarity search in het-
erogeneous information networks. PVLDB, 4(11).
P. Talukdar and K. Crammer. 2009. New regularized
algorithms for transductive learning. In Proceed-
ings of the European Conference on Machine Learn-
ing and Knowledge Discovery in Databases: Part II,
ECML PKDD ?09.
D. Vitale, P. Ferragina, and U. Scaiella. 2012. Clas-
sification of short texts by deploying topical annota-
tions. In ECIR, pages 376?387.
K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale
nlp: A case study on url word breaking. In Proceed-
ings of the 20th International Conference on World
Wide Web, WWW ?11.
W. Zhang, J. Su, C. Tan, and W. Wang. 2010. En-
tity linking leveraging automatically generated an-
notation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010).
W. Zhang, J. Su, and C. L. Tan. 2011. A wikipedia-lda
model for entity linking with batch size changing. In
Proc. IJCNLP2011.
D. Zhou, O. Bousquet, T. Lal, J. Weston, and
B. Sch?olkopf. 2004. Learning with local and global
consistency. In Advances in Neural Information
Processing Systems 16.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In ICML.
390
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 402?412,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Incremental Joint Extraction of Entity Mentions and Relations
Qi Li Heng Ji
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
{liq7,jih}@rpi.edu
Abstract
We present an incremental joint frame-
work to simultaneously extract entity men-
tions and relations using structured per-
ceptron with efficient beam-search. A
segment-based decoder based on the idea
of semi-Markov chain is adopted to the
new framework as opposed to traditional
token-based tagging. In addition, by virtue
of the inexact search, we developed a num-
ber of new and effective global features
as soft constraints to capture the inter-
dependency among entity mentions and
relations. Experiments on Automatic Con-
tent Extraction (ACE)
1
corpora demon-
strate that our joint model significantly
outperforms a strong pipelined baseline,
which attains better performance than the
best-reported end-to-end system.
1 Introduction
The goal of end-to-end entity mention and re-
lation extraction is to discover relational struc-
tures of entity mentions from unstructured texts.
This problem has been artificially broken down
into several components such as entity mention
boundary identification, entity type classification
and relation extraction. Although adopting such
a pipelined approach would make a system com-
paratively easy to assemble, it has some limita-
tions: First, it prohibits the interactions between
components. Errors in the upstream components
are propagated to the downstream components
without any feedback. Second, it over-simplifies
the problem as multiple local classification steps
without modeling long-distance and cross-task de-
pendencies. By contrast, we re-formulate this
task as a structured prediction problem to reveal
the linguistic and logical properties of the hidden
1
http://www.itl.nist.gov/iad/mig//tests/ace
structures. For example, in Figure 1, the output
structure of each sentence can be interpreted as a
graph in which entity mentions are nodes and re-
lations are directed arcs with relation types. By
jointly predicting the structures, we aim to address
the aforementioned limitations by capturing: (i)
The interactions between two tasks. For exam-
ple, in Figure 1a, although it may be difficult for
a mention extractor to predict ?1,400? as a Per-
son (PER) mention, the context word ?employs?
between ?tire maker? and ?1,400? strongly in-
dicates an Employment-Organization (EMP-ORG)
relation which must involve a PER mention. (ii)
The global features of the hidden structure. Var-
ious entity mentions and relations share linguis-
tic and logical constraints. For example, we
can use the triangle feature in Figure 1b to en-
sure that the relations between ?forces?, and each
of the entity mentions ?Somalia
/GPE
?, ?Haiti
/GPE
?
and ?Kosovo
/GPE
?, are of the same type (Physical
(PHYS), in this case).
Following the above intuitions, we introduce
a joint framework based on structured percep-
tron (Collins, 2002; Collins and Roark, 2004) with
beam-search to extract entity mentions and rela-
tions simultaneously. With the benefit of inexact
search, we are also able to use arbitrary global
features with low cost. The underlying learning
algorithm has been successfully applied to some
other Natural Language Processing (NLP) tasks.
Our task differs from dependency parsing (such as
(Huang and Sagae, 2010)) in that relation struc-
tures are more flexible, where each node can have
arbitrary relation arcs. Our previous work (Li et
al., 2013) used perceptron model with token-based
tagging to jointly extract event triggers and argu-
ments. By contrast, we aim to address a more chal-
lenging task: identifying mention boundaries and
types together with relations, which raises the is-
sue that assignments for the same sentence with
different mention boundaries are difficult to syn-
402
The tire maker| {z }
ORG
still employs 1,400| {z }
PER
.
EMP-ORG
(a) Interactions between Two Tasks
... US|{z}
GPE
forces| {z }
PER
in Somalia| {z }
GPE
, Haiti|{z}
GPE
and Kosovo| {z }
GPE
.
EMP-ORG
PHYS
conj and
GPE
PER
GPE
PHYSPHY
S
conj and
(b) Example of Global Feature
Figure 1: End-to-End Entity Mention and Relation Extraction.
chronize during search. To tackle this problem,
we adopt a segment-based decoding algorithm de-
rived from (Sarawagi and Cohen, 2004; Zhang and
Clark, 2008) based on the idea of semi-Markov
chain (a.k.a, multiple-beam search algorithm).
Most previous attempts on joint inference of en-
tity mentions and relations (such as (Roth and Yih,
2004; Roth and Yih, 2007)) assumed that entity
mention boundaries were given, and the classifiers
of mentions and relations are separately learned.
As a key difference, we incrementally extract en-
tity mentions together with relations using a single
model. The main contributions of this paper are as
follows:
1. This is the first work to incrementally predict
entity mentions and relations using a single
joint model (Section 3).
2. Predicting mention boundaries in the joint
framework raises the challenge of synchroniz-
ing different assignments in the same beam. We
solve this problem by detecting entity mentions
on segment-level instead of traditional token-
based approaches (Section 3.1.1).
3. We design a set of novel global features based
on soft constraints over the entire output graph
structure with low cost (Section 4).
Experimental results show that the proposed
framework achieves better performance than
pipelined approaches, and global features provide
further significant gains.
2 Background
2.1 Task Definition
The entity mention extraction and relation
extraction tasks we are addressing are those
of the Automatic Content Extraction (ACE)
program
2
. ACE defined 7 main entity types
including Person (PER), Organization (ORG),
Geographical Entities (GPE), Location (LOC),
2
http://www.nist.gov/speech/tests/ace
Facility (FAC), Weapon (WEA) and Vehicle
(VEH). The goal of relation extraction
3
is to
extract semantic relations of the targeted types
between a pair of entity mentions which ap-
pear in the same sentence. ACE?04 defined 7
main relation types: Physical (PHYS), Person-
Social (PER-SOC), Employment-Organization
(EMP-ORG), Agent-Artifact (ART), PER/ORG
Affiliation (Other-AFF), GPE-Affiliation
(GPE-AFF) and Discourse (DISC). ACE?05 kept
PER-SOC, ART and GPE-AFF, split PHYS into
PHYS and a new relation type Part-Whole,
removed DISC, and merged EMP-ORG and
Other-AFF into EMP-ORG.
Throughout this paper, we use? to denote non-
entity or non-relation classes. We consider rela-
tion asymmetric. The same relation type with op-
posite directions is considered to be two classes,
which we refer to as directed relation types.
Most previous research on relation extraction
assumed that entity mentions were given In this
work we aim to address the problem of end-to-end
entity mention and relation extraction from raw
texts.
2.2 Baseline System
In order to develop a baseline system repre-
senting state-of-the-art pipelined approaches, we
trained a linear-chain Conditional Random Fields
model (Lafferty et al, 2001) for entity mention ex-
traction and a Maximum Entropy model for rela-
tion extraction.
Entity Mention Extraction Model We re-cast
the problem of entity mention extraction as a se-
quential token tagging task as in the state-of-the-
art system (Florian et al, 2006). We applied the
BILOU scheme, where each tag means a token is
the Beginning, Inside, Last, Outside, and Unit of
an entity mention, respectively. Most of our fea-
tures are similar to the work of (Florian et al,
3
Throughout this paper we refer to relation mention as re-
lation since we do not consider relation mention coreference.
403
2004; Florian et al, 2006) except that we do not
have their gazetteers and outputs from other men-
tion detection systems as features. Our additional
features are as follows:
? Governor word of the current token based on de-
pendency parsing (Marneffe et al, 2006).
? Prefix of each word in Brown clusters learned
from TDT5 corpus (Sun et al, 2011).
Relation Extraction Model Given a sentence
with entity mention annotations, the goal of base-
line relation extraction is to classify each mention
pair into one of the pre-defined relation types with
direction or ? (non-relation). Most of our relation
extraction features are based on the previous work
of (Zhou et al, 2005) and (Kambhatla, 2004). We
designed the following additional features:
? The label sequence of phrases covering the two
mentions. For example, for the sentence in Fig-
ure 1a, the sequence is ?NP VP NP?. We also
augment it by head words of each phrase.
? Four syntactico - semantic patterns described in
(Chan and Roth, 2010).
? We replicated each lexical feature by replacing
each word with its Brown cluster.
3 Algorithm
3.1 The Model
Our goal is to predict the hidden structure of
each sentence based on arbitrary features and con-
straints. Let x ? X be an input sentence, y
?
? Y
be a candidate structure, and f(x, y
?
) be the fea-
ture vector that characterizes the entire structure.
We use the following linear model to predict the
most probable structure y? for x:
y? = argmax
y
?
?Y(x)
f(x, y
?
) ?w (1)
where the score of each candidate assignment is
defined as the inner product of the feature vector
f(x, y
?
) and feature weights w.
Since the structures contain both entity men-
tions relations, and we also aim to exploit global
features. There does not exist a polynomial-time
algorithm to find the best structure. In practice
we apply beam-search to expand partial configu-
rations for the input sentence incrementally to find
the structure with the highest score.
3.1.1 Joint Decoding Algorithm
One main challenge to search for entity mentions
and relations incrementally is the alignment of dif-
ferent assignments. Assignments for the same sen-
tence can have different numbers of entity men-
tions and relation arcs. The entity mention ex-
traction task is often re-cast as a token-level se-
quential labeling problem with BIO or BILOU
scheme (Ratinov and Roth, 2009; Florian et al,
2006). A naive solution to our task is to adopt this
strategy by treating each token as a state. How-
ever, different assignments for the same sentence
can have various mention boundaries. It is un-
fair to compare the model scores of a partial men-
tion and a complete mention. It is also difficult to
synchronize the search process of relations. For
example, consider the two hypotheses ending at
?York? for the same sentence:
AllanU-PER from? NewB-ORG YorkI-ORG Stock Exchange
AllanU-PER from? NewB-GPE YorkL-GPE Stock Exchange
PHYS
PHYS
The model would bias towards the incorrect as-
signment ?New
/B-GPE
York
/L-GPE
? since it can
have more informative features as a complete
mention (e.g., a binary feature indicating if the
entire mention appears in a GPE gazetter). Fur-
thermore, the predictions of the two PHYS rela-
tions cannot be synchronized since ?New
/B-FAC
York
/I-FAC
? is not yet a complete mention.
To tackle these problems, we employ the idea of
semi-Markov chain (Sarawagi and Cohen, 2004),
in which each state corresponds to a segment
of the input sequence. They presented a vari-
ant of Viterbi algorithm for exact inference in
semi-Markov chain. We relax the max operation
by beam-search, resulting in a segment-based de-
coder similar to the multiple-beam algorithm in
(Zhang and Clark, 2008). Let
?
d be the upper bound
of entity mention length. The k-best partial assign-
ments ending at the i-th token can be calculated as:
B[i] = k-BEST
y
?
?{y
[1..i]
|y
[1:i?d]
?B[i?d], d=1...
?
d}
f(x, y
?
) ?w
where y
[1:i?d]
stands for a partial configuration
ending at the (i-d)-th token, and y
[i?d+1,i]
corre-
sponds to the structure of a new segment (i.e., sub-
sequence of x) x
[i?d+1,i]
. Our joint decoding algo-
rithm is shown in Figure 2. For each token index
i, it maintains a beam for the partial assignments
whose last segments end at the i-th token. There
are two types of actions during the search:
404
Input: input sentence x = (x
1
, x
2
, ..., x
m
).
k: beam size.
T ? {?}: entity mention type alphabet.
R? {?}: directed relation type alphabet.
4
d
t
: max length of type-t segment, t ? T ? {?}.
Output: best configuration y? for x
1 initialize m empty beams B[1..m]
2 for i? 1...m do
3 for t ? T ? {?} do
4 for d? 1...d
t
, y
?
? B[i? d] do
5 k ? i? d+ 1
6 B[i]? B[i] ? APPEND(y
?
, t, k, i)
7 B[i]? k-BEST(B[i])
8 for j ? (i? 1)...1 do
9 buf? ?
10 for y
?
? B[i] do
11 if HASPAIR(y
?
, i, j) then
12 for r ? R ? {?} do
13 buf? buf ? LINK(y
?
, r, i, j)
14 else
15 buf? buf ? {y
?
}
16 B[i]? k-BEST(buf)
17 return B[m][0]
Figure 2: Joint Decoding for Entity Men-
tions and Relations. HASPAIR(y
?
, i, j) checks
if there are two entity mentions in y
?
that
end at token x
i
and token x
j
, respectively.
APPEND(y
?
, t, k, i) appends y
?
with a type-t
segment spanning from x
k
to x
i
. Similarly
LINK(y
?
, r, i, j) augments y
?
by assigning a di-
rected relation r to the pair of entity mentions
ending at x
i
and x
j
respectively.
1. APPEND (Lines 3-7). First, the algorithm
enumerates all possible segments (i.e., subse-
quences) of x ending at the current token with
various entity types. A special type of seg-
ment is a single token with non-entity label (?).
Each segment is then appended to existing par-
tial assignments in one of the previous beams to
form new assignments. Finally the top k results
are recorded in the current beam.
2. LINK (Lines 8-16). After each step of APPEND,
the algorithm looks backward to link the newly
identified entity mentions and previous ones (if
any) with relation arcs. At the j-th sub-step,
it only considers the previous mention ending
at the j-th previous token. Therefore different
4
The same relation type with opposite directions is con-
sidered to be two classes in R.
configurations are guaranteed to have the same
number of sub-steps. Finally, all assignments
are re-ranked with new relation information.
There are m APPEND actions, each is followed by
at most (i?1) LINK actions (line 8). Therefore the
worst-case time complexity is O(
?
d ?k ?m
2
), where
?
d is the upper bound of segment length.
3.1.2 Example Demonstration
the
tire
maker still
employs
1,400
.
?
PER
ORG
...
x
y EMP-ORG
Figure 3: Example of decoding steps. x-axis
and y-axis represent the input sentence and en-
tity types, respectively. The rectangles denote seg-
ments with entity types, among which the shaded
ones are three competing hypotheses ending at
?1,400?. The solid lines and arrows indicate cor-
rect APPEND and LINK actions respectively, while
the dashed indicate incorrect actions.
Here we demonstrate a simple but concrete ex-
ample by considering again the sentence described
in Figure 1a. Suppose we are at the token ?1,400?.
At this point we can propose multiple entity men-
tions with various lengths. Assuming ?1,400
/PER
?,
?1,400
/?
? and ?(employs 1,400)
/PER
? are possi-
ble assignments, the algorithm appends these new
segments to the partial assignments in the beams
of the tokens ?employs? and ?still?, respectively.
Figure 3 illustrates this process. For simplicity,
only a small part of the search space is presented.
The algorithm then links the newly identified men-
tions to the previous ones in the same configu-
ration. In this example, the only previous men-
tion is ?(tire maker)
/ORG
?. Finally, ?1,400
/PER
? will
be preferred by the model since there are more
indicative context features for EMP-ORG relation
between ?(tire maker)
/PER
? and ?1,400
/PER
?.
405
3.2 Structured-Perceptron Learning
To estimate the feature weights, we use struc-
tured perceptron (Collins, 2002), an extension
of the standard perceptron for structured pre-
diction, as the learning framework. Huang et
al. (2012) proved the convergency of structured
perceptron when inexact search is applied with
violation-fixing update methods such as early-
update (Collins and Roark, 2004). Since we use
beam-search in this work, we apply early-update.
In addition, we use averaged parameters to reduce
overfitting as in (Collins, 2002).
Figure 4 shows the pseudocode for struc-
tured perceptron training with early-update. Here
BEAMSEARCH is identical to the decoding algo-
rithm described in Figure 2 except that if y
?
, the
prefix of the gold standard y, falls out of the beam
after each execution of the k-BEST function (line 7
and 16), then the top assignment z and y
?
are re-
turned for parameter update. It is worth noting that
this can only happen if the gold-standard has a seg-
ment ending at the current token. For instance, in
the example of Figure 1a, B[2] cannot trigger any
early-update since the gold standard does not con-
tain any segment ending at the second token.
Input: training set D = {(x
(j)
, y
(j)
)}
N
i=1
,
maximum iteration number T
Output: model parameters w
1 initialize w? 0
2 for t? 1...T do
3 foreach (x, y) ? D do
4 (x, y
?
, z)? BEAMSEARCH (x, y,w)
5 if z 6= y then
6 w? w + f(x, y
?
)? f(x, z)
7 return w
Figure 4: Perceptron algorithm with beam-
search and early-update. y
?
is the prefix of the
gold-standard and z is the top assignment.
3.3 Entity Type Constraints
Entity type constraints have been shown effective
in predicting relations (Roth and Yih, 2007; Chan
and Roth, 2010). We automatically collect a map-
ping table of permissible entity types for each rela-
tion type from our training data. Instead of apply-
ing the constraints in post-processing inference,
we prune the branches that violate the type con-
straints during search. This type of pruning can
reduce search space as well as make the input for
parameter update less noisy. In our experiments,
only 7 relation mentions (0.5%) in the dev set and
5 relation mentions (0.3%) in the test set violate
the constraints collected from the training data.
4 Features
An advantage of our framework is that we can
easily exploit arbitrary features across the two
tasks. This section describes the local features
(Section 4.1) and global features (Section 4.2) we
developed in this work.
4.1 Local Features
We design segment-based features to directly eval-
uate the properties of an entity mention instead of
the individual tokens it contains. Let y? be a pre-
dicted structure of a sentence x. The entity seg-
ments of y? can be expressed as a list of triples
(e
1
, ..., e
m
), where each segment e
i
= ?u
i
, v
i
, t
i
?
is a triple of start index u
i
, end index v
i
, and entity
type t
i
. The following is an example of segment-
based feature:
f
001
(x, y?, i) =
?
?
?
?
?
1 if x
[y?.u
i
,y?.v
i
]
= tire maker
y?.t
(i?1)
, y?.t
i
= ?,ORG
0 otherwise
This feature is triggered if the labels of the (i?1)-
th and the i-th segments are ??,ORG?, and the text
of the i-th segment is ?tire maker?. Our segment-
based features are described as follows:
Gazetteer features Entity type of each segment
based on matching a number of gazetteers includ-
ing persons, countries, cities and organizations.
Case features Whether a segment?s words are
initial-capitalized, all lower cased, or mixture.
Contextual features Unigrams and bigrams of
the text and part-of-speech tags in a segment?s
contextual window of size 2.
Parsing-based features Features derived from
constituent parsing trees, including (a) the phrase
type of the lowest common ancestor of the tokens
contained in the segment, (b) the depth of the low-
est common ancestor, (c) a binary feature indicat-
ing if the segment is a base phrase or a suffix of a
base phrase, and (d) the head words of the segment
and its neighbor phrases.
In addition, we convert each triple ?u
i
, v
i
, t
i
? to
BILOU tags for the tokens it contains to imple-
ment token-based features. The token-based men-
406
tion features and local relation features are identi-
cal to those of our pipelined system (Section 2.2).
4.2 Global Entity Mention Features
By virtue of the efficient inexact search, we are
able to use arbitrary features from the entire
structure of y? to capture long-distance dependen-
cies. The following features between related entity
mentions are extracted once a new segment is ap-
pended during decoding.
Coreference consistency Coreferential entity
mentions should be assigned the same entity type.
We determine high-recall coreference links be-
tween two segments in the same sentence using
some simple heuristic rules:
? Two segments exactly or partially string match.
? A pronoun (e.g., ?their?,?it?) refers to previous
entity mentions. For example, in ?they have
no insurance on their cars?, ?they? and ?their?
should have the same entity type.
? A relative pronoun (e.g., ?which?,?that?, and
?who?) refers to the noun phrase it modifies in
the parsing tree. For example, in ?the starting
kicker is nikita kargalskiy, who may be 5,000
miles from his hometown?, ?nikita kargalskiy?
and ?who? should both be labeled as persons.
Then we encode a global feature to check
whether two coreferential segments share the same
entity type. This feature is particularly effective
for pronouns because their contexts alone are of-
ten not informative.
Neighbor coherence Neighboring entity men-
tions tend to have coherent entity types. For ex-
ample, in ?Barbara Starr was reporting from the
Pentagon?, ?Barbara Starr? and ?Pentagon? are
connected by a dependency link prep from and
thus they are unlikely to be a pair of PER men-
tions. Two types of neighbor are considered: (i)
the first entity mention before the current segment,
and (ii) the segment which is connected by a sin-
gle word or a dependency link with the current
segment. We take the entity types of the two seg-
ments and the linkage together as a global feature.
For instance, ?PER prep from PER? is a feature
for the above example when ?Barbara Starr? and
?Pentagon? are both labeled as PER mentions.
Part-of-whole consistency If an entity men-
tion is semantically part of another mention (con-
nected by a prep of dependency link), they should
be assigned the same entity type. For example,
in ?some of Iraq?s exiles?, ?some? and ?exiles?
are both PER mentions; in ?one of the town?s two
meat-packing plants?, ?one? and ?plants? are both
FAC mentions; in ?the rest ofAmerica?, ?rest? and
?America? are both GPE mentions.
4.3 Global Relation Features
Relation arcs can also share inter-dependencies or
obey soft constraints. We extract the following
relation-centric global features when a new rela-
tion hypothesis is made during decoding.
Role coherence If an entity mention is involved
in multiple relations with the same type, then its
roles should be coherent. For example, a PER
mention is unlikely to have more than one em-
ployer. However, a GPE mention can be a physical
location for multiple entity mentions. We combine
the relation type and the entity mention?s argument
roles as a global feature, as shown in Figure 5a.
Triangle constraint Multiple entity mentions
are unlikely to be fully connected with the same
relation type. We use a negative feature to penalize
any configuration that contains this type of struc-
ture. An example is shown in Figure 5b.
Inter-dependent compatibility If two entity
mentions are connected by a dependency link, they
tend to have compatible relations with other enti-
ties. For example, in Figure 5c, the conj and de-
pendency link between ?Somalia? and ?Kosovo?
indicates they may share the same relation type
with the third entity mention ?forces?.
Neighbor coherence Similar to the entity men-
tion neighbor coherence feature, we also combine
the types of two neighbor relations in the same
sentence as a bigram feature.
5 Experiments
5.1 Data and Scoring Metric
Most previous work on ACE relation extraction
has reported results on ACE?04 data set. As
we will show later in our experiments, ACE?05
made significant improvement on both relation
type definition and annotation quality. Therefore
we present the overall performance on ACE?05
data. We removed two small subsets in informal
genres - cts and un, and then randomly split the re-
maining 511 documents into 3 parts: 351 for train-
ing, 80 for development, and the rest 80 for blind
test. In order to compare with state-of-the-art we
also performed the same 5-fold cross-validation on
bnews and nwire subsets of ACE?04 corpus as in
previous work. The statistics of these data sets
407
(GPE Somalia)
(PER forces)
(GPE US)
EMP-ORGEMP
-OR
G
?
(a)
(GPE Somalia)
(PER forces)
(GPE Haiti)
PHYSPHY
S
PHYS
?
(b)
(GPE Somalia)
(PER forces)
(GPE Kosovo)
PHYSPHY
S
conj and
(c)
Figure 5: Examples of Global Relation Features.
0 5 10 15 20 25# of training iterations0.70
0.72
0.74
0.76
0.78
0.80
F_1 s
core
mention local+globalmention local
(a) Entity Mention Performance
0 5 10 15 20 25# of training iterations0.30
0.35
0.40
0.45
0.50
0.55
F_1 s
core
relation local+globalrelation local
(b) Relation Performance
Figure 6: Learning Curves on Development Set.
are summarized in Table 1. We ran the Stanford
CoreNLP toolkit
5
to automatically recover the true
cases for lowercased documents.
Data Set # sentences # mentions # relations
ACE?05
Train 7,273 26,470 4,779
Dev 1,765 6,421 1,179
Test 1,535 5,476 1,147
ACE?04 6,789 22,740 4,368
Table 1: Data Sets.
We use the standard F
1
measure to evaluate the
performance of entity mention extraction and re-
lation extraction. An entity mention is considered
correct if its entity type is correct and the offsets
of its mention head are correct. A relation men-
tion is considered correct if its relation type is
correct, and the head offsets of two entity men-
tion arguments are both correct. As in Chan and
5
http://nlp.stanford.edu/software/corenlp.shtml
Roth (2011), we excluded the DISC relation type,
and removed relations in the system output which
are implicitly correct via coreference links for fair
comparison. Furthermore, we combine these two
criteria to evaluate the performance of end-to-end
entity mention and relation extraction.
5.2 Development Results
In general a larger beam size can yield better per-
formance but increase training and decoding time.
As a tradeoff, we set the beam size as 8 through-
out the experiments. Figure 6 shows the learn-
ing curves on the development set, and compares
the performance with and without global features.
From these figures we can clearly see that global
features consistently improve the extraction per-
formance of both tasks. We set the number of
training iterations as 22 based on these curves.
5.3 Overall Performance
Table 2 shows the overall performance of various
methods on the ACE?05 test data. We compare
our proposed method (Joint w/ Global) with the
pipelined system (Pipeline), the joint model with
only local features (Joint w/ Local), and two hu-
man annotators who annotated 73 documents in
ACE?05 corpus.
We can see that our approach significantly out-
performs the pipelined approach for both tasks. As
a real example, for the partial sentence ?a marcher
from Florida? from the test data, the pipelined ap-
proach failed to identify ?marcher? as a PER men-
tion, and thus missed the GEN-AFF relation be-
tween ?marcher? and ?Florida?. Our joint model
correctly identified the entity mentions and their
relation. Figure 7 shows the details when the
joint model is applied to this sentence. At the
token ?marcher?, the top hypothesis in the beam
is ???,???, while the correct one is ranked sec-
ond best. After the decoder processes the token
?Florida?, the correct hypothesis is promoted to
the top in the beam by the Neighbor Coherence
features for PER-GPE pair. Furthermore, after
408
Model
Entity Mention (%)
Relation (%) Entity Mention + Relation (%)
Score P R F
1
P R F
1
P R F
1
Pipeline 83.2 73.6 78.1 67.5 39.4 49.8 65.1 38.1 48.0
Joint w/ Local 84.5 76.0 80.0 68.4 40.1 50.6 65.3 38.3 48.3
Joint w/ Global 85.2 76.9 80.8 68.9 41.9 52.1 65.4 39.8 49.5
Annotator 1 91.8 89.9 90.9 71.9 69.0 70.4 69.5 66.7 68.1
Annotator 2 88.7 88.3 88.5 65.2 63.6 64.4 61.8 60.2 61.0
Inter-Agreement 85.8 87.3 86.5 55.4 54.7 55.0 52.3 51.6 51.9
Table 2: Overall performance on ACE?05 corpus.
steps hypotheses rank
(a)
ha? marcher?i
1
ha? marcherPERi
2
(b)
ha? marcher? from?i
1
ha? marcherPER from?i
4
(c)
ha? marcherPER from? FloridaGPEi
1
ha? marcher? from? FloridaGPEi
2
(d)
ha? marcherPER from? FloridaGPEi
GEN-AFF
1
ha? marcher? from? FloridaGPEi
4
Figure 7: Two competing hypotheses for ?a
marcher from Florida? during decoding.
linking the two mentions by GEN-AFF relation,
the ranking of the incorrect hypothesis ???,???
is dropped to the 4-th place in the beam, resulting
in a large margin from the correct hypothesis.
The human F
1
score on end-to-end relation ex-
traction is only about 70%, which indicates it is a
very challenging task. Furthermore, the F
1
score
of the inter-annotator agreement is 51.9%, which
is only 2.4% above that of our proposed method.
Compared to human annotators, the bottleneck
of automatic approaches is the low recall of rela-
tion extraction. Among the 631 remaining miss-
ing relations, 318 (50.3%) of them were caused
by missing entity mention arguments. A lot of
nominal mention heads rarely appear in the train-
ing data, such as persons (?supremo?, ?shep-
herd?, ?oligarchs?, ?rich?), geo-political entity
mentions (?stateside?), facilities (?roadblocks?,
?cells?), weapons (?sim lant?, ?nukes?) and ve-
hicles (?prams?). In addition, relations are often
implicitly expressed in a variety of forms. Some
examples are as follows:
? ?Rice has been chosen by President Bush to
become the new Secretary of State? indicates
?Rice? has a PER-SOC relation with ?Bush?.
? ?U.S. troops are now knocking on the door of
Baghdad? indicates ?troops? has a PHYS rela-
tion with ?Baghdad?.
? ?Russia and France sent planes to Baghdad? in-
dicates ?Russia? and ?France? are involved in
an ART relation with ?planes? as owners.
In addition to contextual features, deeper se-
mantic knowledge is required to capture such im-
plicit semantic relations.
5.4 Comparison with State-of-the-art
Table 3 compares the performance on ACE?04
corpus. For entity mention extraction, our joint
model achieved 79.7% on 5-fold cross-validation,
which is comparable with the best F
1
score 79.2%
reported by (Florian et al, 2006) on single-
fold. However, Florian et al (2006) used some
gazetteers and the output of other Information Ex-
traction (IE) models as additional features, which
provided significant gains ((Florian et al, 2004)).
Since these gazetteers, additional data sets and ex-
ternal IE models are all not publicly available, it is
not fair to directly compare our joint model with
their results.
For end-to-end entity mention and relation ex-
traction, both the joint approach and the pipelined
baseline outperform the best results reported
by (Chan and Roth, 2011) under the same setting.
6 Related Work
Entity mention extraction (e.g., (Florian et al,
2004; Florian et al, 2006; Florian et al, 2010; Zi-
touni and Florian, 2008; Ohta et al, 2012)) and
relation extraction (e.g., (Reichartz et al, 2009;
Sun et al, 2011; Jiang and Zhai, 2007; Bunescu
and Mooney, 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004; Zhou et al, 2007;
Qian and Zhou, 2010; Qian et al, 2008; Chan
and Roth, 2011; Plank and Moschitti, 2013)) have
drawn much attention in recent years but were
409
Model
Entity Mention (%)
Relation (%) Entity Mention + Relation (%)
Score P R F
1
P R F
1
P R F
1
Chan and Roth (2011) - 42.9 38.9 40.8 -
Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9
Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1
Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3
Table 3: 5-fold cross-validation on ACE?04 corpus. Bolded scores indicate highly statistical significant
improvement as measured by paired t-test (p < 0.01)
usually studied separately. Most relation extrac-
tion work assumed that entity mention boundaries
and/or types were given. Chan and Roth (2011) re-
ported the best results using predicted entity men-
tions.
Some previous work used relations and en-
tity mentions to enhance each other in joint
inference frameworks, including re-ranking (Ji
and Grishman, 2005), Integer Linear Program-
ming (ILP) (Roth and Yih, 2004; Roth and Yih,
2007; Yang and Cardie, 2013), and Card-pyramid
Parsing (Kate and Mooney, 2010). All these
work noted the advantage of exploiting cross-
component interactions and richer knowledge.
However, they relied on models separately learned
for each subtask. As a key difference, our ap-
proach jointly extracts entity mentions and rela-
tions using a single model, in which arbitrary soft
constraints can be easily incorporated. Some other
work applied probabilistic graphical models for
joint extraction (e.g., (Singh et al, 2013; Yu and
Lam, 2010)). By contrast, our work employs an
efficient joint search algorithm without modeling
joint distribution over numerous variables, there-
fore it is more flexible and computationally sim-
pler. In addition, (Singh et al, 2013) used gold-
standard mention boundaries.
Our previous work (Li et al, 2013) used struc-
tured perceptron with token-based decoder to
jointly predict event triggers and arguments based
on the assumption that entity mentions and other
argument candidates are given as part of the in-
put. In this paper, we solve a more challeng-
ing problem: take raw texts as input and identify
the boundaries, types of entity mentions and rela-
tions all together in a single model. Sarawagi and
Cohen (2004) proposed a segment-based CRFs
model for name tagging. Zhang and Clark (2008)
used a segment-based decoder for word segmenta-
tion and pos tagging. We extended the similar idea
to our end-to-end task by incrementally predicting
relations along with entity mention segments.
7 Conclusions and Future Work
In this paper we introduced a new architecture
for more powerful end-to-end entity mention and
relation extraction. For the first time, we ad-
dressed this challenging task by an incremental
beam-search algorithm in conjunction with struc-
tured perceptron. While detecting mention bound-
aries jointly with other components raises the chal-
lenge of synchronizing multiple assignments in
the same beam, a simple yet effective segment-
based decoder is adopted to solve this problem.
More importantly, we exploited a set of global fea-
tures based on linguistic and logical properties of
the two tasks to predict more coherent structures.
Experiments demonstrated our approach signifi-
cantly outperformed pipelined approaches for both
tasks and dramatically advanced state-of-the-art.
In future work, we plan to explore more soft and
hard constraints to reduce search space as well as
improve accuracy. In addition, we aim to incorpo-
rate other IE components such as event extraction
into the joint model.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award and RPI faculty start-up
grant. The views and conclusions contained in
this document are those of the authors and should
not be interpreted as representing the official poli-
cies, either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.
410
References
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. HLT/EMNLP, pages 724?731.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proc.
COLING, pages 152?160.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551?560.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL,
pages 423?429.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proc. HLT-NAACL, pages 1?8.
Radu Florian, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex models:
A case study in mention detection. In Proc. ACL.
Radu Florian, John F. Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection robust-
ness to noisy input. In Proc. EMNLP, pages 335?
345.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
ACL, pages 1077?1086.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142?151.
Heng Ji and Ralph Grishman. 2005. Improving name
tagging by reference resolution and relation detec-
tion. In Proc. ACL, pages 411?418.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extrac-
tion. In Proc. HLT-NAACL.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proc. ACL,
pages 178?181.
Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proc. ACL, pages 203?212.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML, pages 282?289.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73?82.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 449,454.
Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and
Sophia Ananiadou. 2012. Open-domain anatomi-
cal entity mention detection. In Proc. ACL Work-
shop on Detecting Structure in Scholarly Discourse,
pages 27?36.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proc.
ACL, pages 1498?1507.
Longhua Qian and Guodong Zhou. 2010. Clustering-
based stratified seed sampling for semi-supervised
relation classification. In Proc. EMNLP, pages 346?
355.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proc. COLING, pages 697?704.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. CONLL, pages 147?155.
Frank Reichartz, Hannes Korte, and Gerhard Paass.
2009. Composite kernels for relation extraction. In
Proc. ACL-IJCNLP (Short Papers), pages 365?368.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proc. CoNLL.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proc. NIPS.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proc. ACL, pages 521?529.
411
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640?1649.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Proc. COLING
(Posters), pages 1399?1407.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proc. ACL, pages 1147?1157.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL, pages 419?426.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL, pages 427?434.
Guodong Zhou, Min Zhang, Dong-Hong Ji, and
Qiaoming Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proc. EMNLP-CoNLL, pages
728?736.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proc. EMNLP,
pages 600?609.
412
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 278?282,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How to Speak a Language without Knowing It
Xing Shi and Kevin Knight
Information Sciences Institute
Computer Science Department
University of Southern California
{xingshi, knight}@isi.edu
Heng Ji
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
jih@rpi.edu
Abstract
We develop a system that lets people over-
come language barriers by letting them
speak a language they do not know. Our
system accepts text entered by a user,
translates the text, then converts the trans-
lation into a phonetic spelling in the user?s
own orthography. We trained the sys-
tem on phonetic spellings in travel phrase-
books.
1 Introduction
Can people speak a language they don?t know?
Actually, it happens frequently. Travel phrase-
books contain phrases in the speaker?s language
(e.g., ?thank you?) paired with foreign-language
translations (e.g., ? ????????). Since the speaker
may not be able to pronounce the foreign-language
orthography, phrasebooks additionally provide
phonetic spellings that approximate the sounds of
the foreign phrase. These spellings employ the fa-
miliar writing system and sounds of the speaker?s
language. Here is a sample entry from a French
phrasebook for English speakers:
English: Leave me alone.
French: Laissez-moi tranquille.
Franglish: Less-ay mwah trahn-KEEL.
The user ignores the French and goes straight
to the Franglish. If the Franglish is well designed,
an English speaker can pronounce it and be under-
stood by a French listener.
Figure 1 shows a sample entry from another
book?an English phrasebook for Chinese speak-
ers. If a Chinese speaker wants to say ???
????????, she need only read off the
Chinglish ?????????????, which
approximates the sounds of ?Thank you for this
wonderful meal? using Chinese characters.
Phrasebooks permit a form of accurate, per-
sonal, oral communication that speech-to-speech
Figure 1: Snippet from phrasebook
translation devices lack. However, the user is lim-
ited to a small set of fixed phrases. In this paper,
we lift this restriction by designing and evaluating
a software program with the following:
? Input: Text entered by the speaker, in her own
language.
? Output: Phonetic rendering of a foreign-
language translation of that text, which, when
pronounced by the speaker, can be under-
stood by the listener.
The main challenge is that different languages
have different orthographies, different phoneme
inventories, and different phonotactic constraints,
so mismatches are inevitable. Despite this, the
system?s output should be both unambiguously
pronounceable by the speaker and readily under-
stood by the listener.
Our goal is to build an application that covers
many language pairs and directions. The current
paper describes a single system that lets a Chinese
person speak English.
We take a statistical modeling approach to this
problem, as is done in two lines of research that are
most related. The first is machine transliteration
(Knight and Graehl, 1998), in which names and
technical terms are translated across languages
with different sound systems. The other is re-
spelling generation (Hauer and Kondrak, 2013),
where an English speaker is given a phonetic hint
about how to pronounce a rare or foreign word
to another English speaker. By contrast, we aim
278
Chinese ?????
English It?s eight o?clock now
Chinglish ????????? (yi si ai te e ke lao ke nao)
Chinese ??????????
English this shirt is very stylish and not very expensive
Chinglish ????????????????????????
Chinese ??????????15??
English our minimum charge for delivery is fifteen dollars
Chinglish ????????????????????
Table 1: Examples of <Chinese, English, Chinglish> tuples from a phrasebook.
to help people issue full utterances that cross lan-
guage barriers.
2 Evaluation
Our system?s input is Chinese. The output is
a string of Chinese characters that approximate
English sounds, which we call Chinglish. We
build several candidate Chinese-to-Chinglish sys-
tems and evaluate them as follows:
? We compute the normalized edit distance
between the system?s output and a human-
generated Chinglish reference.
? A Chinese speaker pronounces the system?s
output out loud, and an English listener takes
dictation. We measure the normalized edit
distance against an English reference.
? We automate the previous evaluation by re-
place the two humans with: (1) a Chinese
speech synthesizer, and (2) a English speech
recognizer.
3 Data
We seek to imitate phonetic transformations found
in phrasebooks, so phrasebooks themselves are a
good source of training data. We obtained a col-
lection of 1312 <Chinese, English, Chinglish>
phrasebook tuples
1
(see Table 1).
We use 1182 utterances for training, 65 for de-
velopment, and 65 for test. We know of no other
computational work on this type of corpus.
Our Chinglish has interesting gross empirical
properties. First, because Chinglish and Chinese
are written with the same characters, they render
the same inventory of 416 distinct syllables. How-
ever, the distribution of Chinglish syllables differs
1
Dataset can be found at http://www.isi.edu/
natural-language/mt/chinglish-data.txt
a great deal from Chinese (Table 2). Syllables ?si?
and ?te? are very popular, because while conso-
nant clusters like English ?st? are impossible to re-
produce exactly, the particular vowels in ?si? and
?te? are fortunately very weak.
Frequency Rank Chinese Chinglish
1 de si
2 shi te
3 yi de
4 ji yi
5 zhi fu
Table 2: Top 5 frequent syllables in Chinese
(McEnery and Xiao, 2004) and Chinglish
We find that multiple occurrences of an English
word type are generally associated with the same
Chinglish sequence. Also, Chinglish characters do
not generally span multiple English words. It is
reasonable for ?can I? to be rendered as ?kan nai?,
with ?nai? spanning both English words, but this
is rare.
4 Model
We model Chinese-to-Chinglish translation with
a cascade of weighted finite-state transducers
(wFST), shown in Figure 2. We use an online
MT system to convert Chinese to an English word
sequence (Eword), which is then passed through
FST A to generate an English sound sequence
(Epron). FST A is constructed from the CMU Pro-
nouncing Dictionary (Weide, 2007).
Next, wFST B translates English sounds into
Chinese sounds (Pinyin-split). Pinyin is an official
syllable-based romanization of Mandarin Chinese
characters, and Pinyin-split is a standard separa-
tion of Pinyin syllables into initial and final parts.
Our wFST allows one English sound token to map
279
Figure 2: Finite-state cascade for modeling the re-
lation between Chinese and Chinglish.
to one or two Pinyin-split tokens, and it also allows
two English sounds to map to one Pinyin-split to-
ken.
Finally, FST C converts Pinyin-split into Pinyin,
and FST D chooses Chinglish characters. We also
experiment with an additional wFST E that trans-
lates English words directly into Chinglish.
5 Training
FSTs A, C, and D are unweighted, and remain so
throughout this paper.
5.1 Phoneme-based model
We must now estimate the values of FST B pa-
rameters, such as P(si|S). To do this, we first
take our phrasebook triples and construct sample
string pairs <Epron, Pinyin-split> by pronounc-
ing the phrasebook English with FST A, and by
pronouncing the phrasebook Chinglish with FSTs
D and C. Then we run the EM algorithm to learn
FST B parameters (Table 3) and Viterbi align-
ments, such as:
g r ae n d
g e r uan d e
5.2 Phoneme-phrase-based model
Mappings between phonemes are context-
sensitive. For example, when we decode English
?grandmother?, we get:
labeled Epron Pinyin-split P (p|e)
d d 0.46
d e 0.40
d i 0.06
s 0.01
ao r u 0.26
o 0.13
ao 0.06
ou 0.01
Table 3: Learned translation tables for the
phoneme based model
g r ae n d m ah dh er
g e r an d e m u e d e
where as the reference Pinyin-split sequence is:
g e r uan d e m a d e
Here, ?ae n? should be decoded as ?uan? when
preceded by ?r?. Following phrase-based meth-
ods in statistical machine translation (Koehn et
al., 2003) and machine transliteration (Finch and
Sumita, 2008), we model substitution of longer se-
quences. First, we obtain Viterbi alignments using
the phoneme-based model, e.g.:
g r ae n d m ah dh er
g e r uan d e m a d e
Second, we extract phoneme phrase pairs con-
sistent with these alignments. We use no phrase-
size limit, but we do not cross word boundaries.
From the example above, we pull out phrase pairs
like:
g? g e
g r? g e r
...
r? r
r ae n? r uan
...
We add these phrase pairs to FST B, and call
this the phoneme-phrase-based model.
5.3 Word-based model
We now turn to WFST E, which short-cuts di-
rectly from English words to Pinyin. We create
<English, Pinyin> training pairs from our phrase-
book simply by pronouncing the Chinglish with
FST D. We initially allow each English word type
to map to any sequence of Pinyin, up to length 7,
with uniform probability. EM learns values for pa-
rameters like P (nai te|night), plus Viterbi align-
ments such as:
280
Model
Top-1 Overall Top-1 Valid
Coverage
Average Edit Distance Average Edit Distance
Word based 0.664 0.042 29/65
Word-based hybrid training 0.659 0.029 29/65
Phoneme based 0.611 0.583 63/65
Phoneme-phrase based 0.194 0.136 63/65
Hybrid training and decoding 0.175 0.115 63/65
Table 4: English-to-Pinyin decoding accuracy on a test set of 65 utterances. Numbers are average edit
distances between system output and Pinyin references. Valid average edit distance is calculated based
only on valid outputs (e.g. 29 outputs for word based model).
accept tips
a ke sha pu te ti pu si
Notice that this model makes alignment errors
due to sparser data (e.g., the word ?tips? and ?ti pu
si? only appear once each in the training data).
5.4 Hybrid training
To improve the accuracy of word-based EM align-
ment, we use the phoneme based model to de-
code each English word in the training data to
Pinyin. From the 100-best list of decodings, we
collect combinations of start/end Pinyin syllables
for the word. We then modify the initial, uniform
English-to-Pinyin mapping probabilities by giving
higher initial weight to mappings that respect ob-
served start/end pairs. When we run EM, we find
that alignment errors for ?tips? in section 5.3 are
fixed:
accept tips
a ke sha pu te ti pu si
5.5 Hybrid decoding
The word-based model can only decode 29 of the
65 test utterances, because wFST E fails if an ut-
terance contains a new English word type, pre-
viously unseen in training. The phoneme-based
models are more robust, able to decode 63 of the
65 utterances, failing only when some English
word type falls outside the CMU pronouncing dic-
tionary (FST A).
Our final model combines these two, using the
word-based model for known English words, and
the phoneme-based models for unknown English
words.
6 Experiments
Our first evaluation (Table 4) is intrinsic, measur-
ing our Chinglish output against references from
the test portion of our phrasebook, using edit dis-
tance. Here, we start with reference English and
measure the accuracy of Pinyin syllable produc-
tion, since the choice of Chinglish character does
not affect the Chinglish pronunciation. We see that
the Word-based method has very high accuracy,
but low coverage. Our best system uses the Hy-
brid training/decoding method. As Table 6 shows,
the ratio of unseen English word tokens is small,
thus large portion of tokens are transformed us-
ing word-based method. The average edit dis-
tance of phoneme-phrase model and that of hy-
brid training/decoding model are close, indicating
that long phoneme-phrase pairs can emulate word-
pinyin mappings.
Unseen Total Ratio
Word Type 62 249 0.249
Token 62 436 0.142
Table 6: Unseen English word type and tokens in
test data.
Model
Valid Average
Edit Distance
Reference English 0.477
Phoneme based 0.696
Hybrid training and decoding 0.496
Table 7: Chinglish-to-English accuracy in dicta-
tion task.
Our second evaluation is a dictation task. We
speak our Chinglish character sequence output
aloud and ask an English monolingual person to
transcribe it. (Actually, we use a Chinese synthe-
sizer to remove bias.) Then we measure edit dis-
tance between the human transcription and the ref-
erence English from our phrasebook. Results are
shown in Table 7.
281
Chinese ?????????
Reference English what do you have for the Reunion dinner
Reference Chinglish ??????????????
Hybrid training/decoding Chinglish ??????????????
Dictation English what do you have for the reunion dinner
ASR English what do you high for 43 Union Cena
Chinese ???
Reference English wait for me
Reference Chinglish ???? (wei te fo mi)
Hybrid training/decoding Chinglish ???? (wei te fo mi)
Dictation English wait for me
ASR English wait for me
Table 5: Chinglish generated by hybrid training and decoding method and corresponding recognized
English by dictation and automatic synthesis-recognition method.
Model
Valid Average
Edit Distance
Word based 0.925
Word-based hybrid training 0.925
Phoneme based 0.937
Phoneme-phrase based 0.896
Hybrid training and decoding 0.898
Table 8: Chinglish-to-English accuracy in auto-
matic synthesis-recognition (ASR) task. Numbers
are average edit distance between recognized En-
glish and reference English.
Finally, we repeat the last experiment, but re-
moving the human from the loop, using both
automatic Chinese speech synthesis and English
speech recognition. Results are shown in Table 8.
Speech recognition is more fragile than human
transcription, so edit distances are greater. Table 5
shows a few examples of the Chinglish generated
by the hybrid training and decoding method, as
well as the recognized English from the dictation
and ASR tasks.
7 Conclusions
Our work aims to help people speak foreign lan-
guages they don?t know, by providing native pho-
netic spellings that approximate the sounds of for-
eign phrases. We use a cascade of finite-state
transducers to accomplish the task. We improve
the model by adding phrases, word boundary con-
straints, and improved alignment.
In the future, we plan to cover more language
pairs and directions. Each target language raises
interesting new challenges that come from its nat-
ural constraints on allowed phonemes, syllables,
words, and orthography.
References
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proceedings of the
Workshop on Technologies and Corpora for Asia-
Pacific Speech Translation (TCAST), pages 13?18.
Bradley Hauer and Grzegorz Kondrak. 2013. Auto-
matic generation of English respellings. In Proceed-
ings of NAACL-HLT, pages 634?643.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Anthony McEnery and Zhonghua Xiao. 2004. The
lancaster corpus of Mandarin Chinese: A corpus for
monolingual and contrastive language study. Reli-
gion, 17:3?4.
R Weide. 2007. The CMU pronunciation dictionary,
release 0.7a.
282
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495?500,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Two-Stage Hashing for Fast Document Retrieval
Hao Li
?
Wei Liu
?
Heng Ji
?
?
Computer Science Department,
Rensselaer Polytechnic Institute, Troy, NY, USA
{lih13,jih}@rpi.edu
?
IBM T. J. Watson Research Center, Yorktown Heights, NY, USA
weiliu@us.ibm.com
Abstract
This work fulfills sublinear time Near-
est Neighbor Search (NNS) in massive-
scale document collections. The primary
contribution is to propose a two-stage
unsupervised hashing framework which
harmoniously integrates two state-of-the-
art hashing algorithms Locality Sensitive
Hashing (LSH) and Iterative Quantization
(ITQ). LSH accounts for neighbor candi-
date pruning, while ITQ provides an ef-
ficient and effective reranking over the
neighbor pool captured by LSH. Further-
more, the proposed hashing framework
capitalizes on both term and topic similar-
ity among documents, leading to precise
document retrieval. The experimental re-
sults convincingly show that our hashing
based document retrieval approach well
approximates the conventional Informa-
tion Retrieval (IR) method in terms of re-
trieving semantically similar documents,
and meanwhile achieves a speedup of over
one order of magnitude in query time.
1 Introduction
A Nearest Neighbor Search (NNS) task aims at
searching for top K objects (e.g., documents)
which are most similar, based on pre-defined sim-
ilarity metrics, to a given query object in an ex-
isting dataset. NNS is essential in dealing with
many search related tasks, and also fundamen-
tal to a broad range of Natural Language Pro-
cessing (NLP) down-stream problems including
person name spelling correction (Udupa and Ku-
mar, 2010), document translation pair acquisition
(Krstovski and Smith, 2011), large-scale similar
noun list generation (Ravichandran et al, 2005),
lexical variants mining (Gouws et al, 2011), and
large-scale first story detection (Petrovic et al,
2010).
Hashing has recently emerged to be a popular
solution to tackling fast NNS, and been success-
fully applied to a variety of non-NLP problems
such as visual object detection (Dean et al, 2013)
and recognition (Torralba et al, 2008a; Torralba
et al, 2008b), large-scale image retrieval (Kulis
and Grauman, 2012; Liu et al, 2012; Gong et al,
2013), and large-scale machine learning (Weiss et
al., 2008; Liu et al, 2011; Liu, 2012). However,
hashing has received limited attention in the NLP
field to the date. The basic idea of hashing is to
represent each data object as a binary code (each
bit of a code is one digit of ?0? or ?1?). When
applying hashing to handle NLP problems, the ad-
vantages are two-fold: 1) the capability to store
a large quantity of documents in the main mem-
ory. for example, one can store 250 million doc-
uments with 1.9G memory using only 64 bits for
each document while a large news corpus such as
the English Gigaword fifth edition
1
stores 10 mil-
lion documents in a 26G hard drive; 2) the time
efficiency of manipulating binary codes, for ex-
ample, computing the hamming distance between
a pair of binary codes is several orders of magni-
tude faster than computing the real-valued cosine
similarity over a pair of document vectors.
The early explorations of hashing focused on
using random permutations or projections to con-
struct randomized hash functions, e.g., the well-
known Min-wise Hashing (MinHash) (Broder et
al., 1998) and Locality Sensitive Hashing (LSH)
(Andoni and Indyk, 2008). In contrast to such
data-independent hashing schemes, recent re-
search has been geared to studying data-dependent
hashing through learning compact hash codes
from a training dataset. The state-of-the-art unsu-
pervised learning-based hashing methods include
Spectral Hashing (SH) (Weiss et al, 2008), An-
chor Graph Hashing (AGH) (Liu et al, 2011),
and Iterative Quantization (ITQ) (Gong et al,
1
http://catalog.ldc.upenn.edu/LDC2011T07
495
2013), all of which endeavor to make the learned
hash codes preserve or reveal some intrinsic struc-
ture, such as local neighborhood structure, low-
dimensional manifolds, or the closest hypercube,
underlying the training data. Despite achieving
data-dependent hash codes, most of these ?learn-
ing to hash? methods cannot guarantee a high suc-
cess rate of looking a query code up in a hash ta-
ble (referred to as hash table lookup in literature),
which is critical to the high efficacy of exploit-
ing hashing in practical uses. It is worth noting
that we choose to use ITQ in the proposed two-
stage hashing framework for its simplicity and ef-
ficiency. ITQ has been found to work better than
SH by Gong et al (2013), and be more efficient
than AGH in terms of training time by Liu (2012).
To this end, in this paper we propose a novel
two-stage unsupervised hashing framework to si-
multaneously enhance the hash lookup success
rate and increase the search accuracy by integrat-
ing the advantages of both LSH and ITQ. Further-
more, we make the hashing framework applicable
to combine different similarity measures in NNS.
2 Background and Terminology
? Binary Codes: A bit (a single bit is ?0? or
?1?) sequence assigned to represent a data
object. For example, represent a document
as a 8-bit code ?11101010?.
? Hash Table: A linear table in which all bi-
nary codes of a data set are arranged to be
table indexes, and each table bucket contains
the IDs of the data items sharing the same
code.
? Hamming Distance: The number of bit po-
sitions in which bits of the two codes differ.
? Hash Table Lookup: Given a query q with
its binary code h
q
, find the candidate neigh-
bors in a hash table such that the Hamming
distances from their codes to h
q
are no more
than a small distance threshold . In practice
 is usually set to 2 to maintain the efficiency
of table lookups.
? Hash Table Lookup Success Rate: Given a
query q with its binary code h
q
, the probabil-
ity to find at least one neighbor in the table
buckets whose corresponding codes (i.e., in-
dexes) are within a Hamming ball of radius 
centered at h
q
.
? Hamming Ranking: Given a query q with
its binary code h
q
, rank all data items accord-
ing to the Hamming distances between their
codes and h
q
; the smaller the Hamming dis-
tance, the higher the data item is ranked.
3 Document Retrieval with Hashing
In this section, we first provide an overview of ap-
plying hashing techniques to a document retrieval
task, and then introduce two unsupervised hash-
ing algorithms: LSH acts as a neighbor-candidate
filter, while ITQ works towards precise reranking
over the candidate pool returned by LSH.
3.1 Document Retrieval
The most traditional way of retrieving nearest
neighbors for documents is to represent each docu-
ment as a term vector of which each element is the
tf-idf weight of a term. Given a query document
vector q, we use the Cosine similarity measure to
evaluate the similarity between q and a document
x in a dataset:
sim(q,x) =
q
>
x
?q??x?
. (1)
Then the traditional document retrieval method
exhaustively scans all documents in the dataset
and returns the most similar ones. However, such
a brute-force search does not scale to massive
datasets since the search time complexity for each
query is O(n); additionally, the computational
cost spent on Cosine similarity calculation is also
nontrivial.
3.2 Locality Sensitive Hashing
The core idea of LSH is that if two data points are
close, then after a ?projection? operation they will
remain close. In other words, similar data points
are more likely to be mapped into the same bucket
with a high collision probability. In a typical LSH
setting of k bits and L hash tables, a query point
q ? R
d
and a dataset point x ? R
d
collide if and
only if
h
ij
(q) ? h
ij
(x), i ? [1 : L], j ? [1 : k], (2)
where the hash function h
ij
(?) is defined as
h
ij
(x) = sgn
(
w
>
ij
x
)
, (3)
in which w
ij
? R
d
is a random projection di-
rection with components being independently and
identically drawn from a normal distribution, and
the sign function sgn(x) returns 1 if x > 0 and -1
otherwise. Note that we use ?1/-1? bits for deriva-
tions and training, and ?1/0? bits for the hashing
496
implementation including converting data to bi-
nary codes, arranging binary codes into hash ta-
bles, and hash table lookups.
3.3 Iterative Quantization
The central idea of ITQ is to learn the binary codes
achieving the lowest quantization error that en-
coding raw data to binary codes incurs. This is
pursued by seeking a rotation of the zero-centered
projected data. Suppose that a set of n data points
X = {x
i
? R
d
}
n
i=1
are provided. The data matrix
is defined as X = [x
1
,x
2
, ? ? ? ,x
n
]
>
? R
n?d
.
In order to reduce the data dimension from d to
the desired code length c < d, Principal Compo-
nent Analysis (PCA) or Latent Semantic Analy-
sis (LSA) is first applied to X. We thus obtain
the zero-centered projected data matrix as V =
(I ?
1
n
11
>
)XU where U ? R
d?c
is the projec-
tion matrix.
After the projection operation, ITQ minimizes
the quantization error as follows
Q(B,R) = ?B?VR?
2
F
, (4)
where B ? {1,?1}
n?c
is the code matrix each
row of which contains a binary code, R ? R
c?c
is the target orthogonal rotation matrix, and ? ? ?
F
denotes the Frobenius norm. Finding a local min-
imum of the quantization error in Eq. (4) begins
with a random initialization of R, and then em-
ploys a K-means clustering like iterative proce-
dure. In each iteration, each (projected) data point
is assigned to the nearest vertex of the binary hy-
percube, and R always satisfying RR
>
= I is
subsequently updated to minimize the quantiza-
tion loss given the current assignment; the two
steps run alternatingly until a convergence is en-
countered. Concretely, the two updating steps are:
1. Fix R and update B: minimize the follow-
ing quantization loss
Q(B,R) = ?B?
2
F
+ ?VR?
2
F
? 2tr
(
R
>
V
>
B
)
= nc+ ?V?
2
F
? 2tr
(
R
>
V
>
B
)
= constant? 2tr
(
R
>
V
>
B
)
,
(5)
achieving B = sgn(VR);
2. Fix B and update R: perform the SVD of
the matrix V
>
B ? R
c?c
to obtain V
>
B =
S?
?
S
>
, and then set R = S
?
S
>
.
Figure 1: The two-stage hashing framework.
3.4 Two-Stage Hashing
There are three main merits of LSH. (1) It tries to
preserve the Cosine similarity of the original data
with a probabilistic guarantee (Charikar, 2002).
(2) It is training free, and thus very efficient in
hashing massive databases to binary codes. (3) It
has a very high hash table lookup success rate. For
example, in our experiments LSH with more than
one hash table is able to achieve a perfect 100%
hash lookup success rate. Unfortunately, its draw-
back is the low search precision that is observed
even with long hash bits and multiple hash tables.
ITQ tries to minimize the quantization error of
encoding data to binary codes, so its advantage
is the high quality (potentially high precision of
Hamming ranking) of the produced binary codes.
Nevertheless, ITQ frequently suffers from a poor
hash lookup success rate when longer bits (e.g.,
? 48) are used (Liu, 2012). For example, in
our experiments ITQ using 384 bits has a 18.47%
hash lookup success rate within Hamming radius
2. Hence, Hamming ranking (costing O(n) time)
must be invoked for the queries for which ITQ
fails to return any neighbors via hash table lookup,
which makes the searches inefficient especially on
very large datasets.
Taking into account the above advantages and
disadvantages of LSH and ITQ, we propose a two-
stage hashing framework to harmoniously inte-
grate them. Fig. 1 illustrates our two-stage frame-
work with a toy example where identical shapes
denote ground-truth nearest neighbors.
In this framework, LSH accounts for neigh-
bor candidate pruning, while ITQ provides an ef-
ficient and effective reranking over the neighbor
pool captured by LSH. To be specific, the pro-
497
posed framework enjoys two advantages:
1. Provide a simple solution to accomplish both
a high hash lookup success rate and high precision,
which does not require scanning the whole list of
the ITQ binary codes but scanning the short list
returned by LSH hash table lookup. Therefore, a
high hash lookup success rate is attained by the
LSH stage, while maintaining high search preci-
sion due to the ITQ reranking stage.
2. Enable a hybrid hashing scheme combining
two similarity measures. The term similarity is
used during the LSH stage that directly works
on document tf-idf vectors; during the ITQ stage,
the topic similarity is used since ITQ works on
the topic vectors obtained by applying Latent se-
mantic analysis (LSA) (Deerwester et al, 1990)
to those document vectors. LSA (or PCA), the
first step in running ITQ, can be easily acceler-
ated via a simple sub-selective sampling strategy
which has been proven theoretically and empiri-
cally sound by Li et al (2014). As a result, the
nearest neighbors returned by the two-stage hash-
ing framework turns out to be both lexically and
topically similar to the query document. To sum-
marize, the proposed two-stage hashing frame-
work works in an unsupervised manner, achieves a
sublinear search time complexity due to LSH, and
attains high search precision thanks to ITQ. After
hashing all data (documents) to LSH and ITQ bi-
nary codes, we do not need to save the raw data in
memory. Thus, our approach can scale to gigan-
tic datasets with compact storage and fast search
speed.
4 Experiments
Data and Evaluations
For the experiments, we use the English portion
of the standard TDT-5 dataset, which consists of
278, 109 documents from a time spanning April
2003 to September 2003. 126 topics are anno-
tated with an average of 51 documents per topic,
and other unlabeled documents are irrelevant to
them. We select six largest topics for the top-K
NNS evaluation, with each including more than
250 documents. We randomly select 60 docu-
ments from each of the six topics for testing. The
six topics are (1). Bombing in Riyadh, Saudi Ara-
bia (2). Mad cow disease in North America (3).
Casablanca bombs (4). Swedish Foreign Minister
killed (5). Liberian former president arrives in ex-
ile and (6). UN official killed in attack. For each
document, we apply the Stanford Tokenizer
2
for
tokenization; remove stopwords based on the stop
list from InQuery (Callan et al, 1992), and apply
Porter Stemmer (Porter, 1980) for stemming.
If one retrieved document shares the same topic
label with the query document, they are true neigh-
bors. We evaluate the precision of the top-K candi-
date documents returned by each method and cal-
culate the average precision across all queries.
Results
We first evaluate the quality of term vectors and
ITQ binary codes by conducting the whole list
Cosine similarity ranking and hamming distance
ranking, respectively. For each query document,
the top-K candidate documents with highest Co-
sine similarity scores and shortest hamming dis-
tances are returned, then we calculate the average
precision for each K. Fig. 2(a) demonstrates that
ITQ binary codes could preserve document simi-
larities as traditional term vectors. It is interesting
to notice that ITQ binary codes are able to outper-
form traditional term vectors. It is mainly because
some documents are topically related but share
few terms thus their relatedness can be captured by
LSA. Fig. 2(a) also shows that the NNS precision
keep increasing as longer ITQ code length is used
and is converged when ITQ code length equals to
384 bits. Therefore we set ITQ code length as 384
bits in the rest of the experiments.
Fig. 2(b) - Fig. 2(e) show that our two-stage
hashing framework surpasses LSH with a large
margin for both small K (e.g., K ? 10) and
large K (e.g., K ? 100) in top-K NNS. It also
demonstrates that our hashing based document re-
trieval approach with only binary codes from LSH
and ITQ well approximates the conventional IR
method. Another crucial observation is that with
ITQ reranking, a small number of LSH hash ta-
bles is needed in the pruning step. For example,
LSH(40bits) + ITQ(384bits) and LSH(48bits) +
ITQ(384bits) are able to reach convergence with
only four LSH hash tables. In that case, we can
alleviate one main drawback of LSH as it usually
requires a large number of hash tables to maintain
the hashing quality.
Since the LSH pruning time can be ignored,
the search time of the two-stage hashing scheme
equals to the time of hamming distance rerank-
ing in ITQ codes for all candidates produced from
LSH pruning step, e.g., LSH(48bits, 4 tables) +
2
http://nlp.stanford.edu/software/corenlp.shtml
498
(a)
0 50 100 150
0.65
0.7
0.75
0.8
0.85
0.9
0.95
number of top?K returned documents
Precis
ion
 
 Traditional IRITQ(448bits)ITQ(384bits)ITQ(320bits)ITQ(256bits)ITQ(192bits)
(b)
1 2 3 4 5 6 7 8 9 100
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
number of hash tables
Top?1
0 Prec
ision
 
 LSH(64bits)LSH(56bits)LSH(48bits)LSH(40bits)
(c)
1 2 3 4 5 6 7 8 9 100.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
number of hash tables
Top?1
0 Prec
ision
 
 
Traditional IRLSH(40bits)+ITQ(384bits)LSH(48bits)+ITQ(384bits)LSH(56bits)+ITQ(384bits)LSH(64bits)+ITQ(384bits)
(d)
1 2 3 4 5 6 7 8 9 100
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
number of hash tables
Top?1
00 Pre
cision
 
 LSH(64bits)LSH(56bits)LSH(48bits)LSH(40bits)
(e)
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
number of hash tables
Top?1
00 Pre
cision
 
 
Traditional IRLSH(40bits)+ITQ(384bits)LSH(48bits)+ITQ(384bits)LSH(56bits)+ITQ(384bits)LSH(64bits)+ITQ(384bits)
(f)
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
number of hash tables
Perce
ntage
 
 LSH(40bits)LSH(48bits)LSH(56bits)LSH(64bits)
Figure 2: (a) ITQ code quality for different code length, (b) LSH Top-10 Precision, (c) LSH +
ITQ(384bits) Top-10 Precision, (d) LSH Top-100 Precision, (e) LSH + ITQ(384bits) Top-100 Precision,
(f) The percentage of visited data samples by LSH hash lookup.
ITQ(384bits) takes only one thirtieth of the search
time as the traditional IR method. Fig. 2 (f)
shows the ITQ data reranking percentage for dif-
ferent LSH bit lengths and table numbers. As the
LSH bit length increases or the hash table num-
ber decreases, a lower percentage of the candidates
will be selected for reranking, and thus costs less
search time.
The percentage of visited data samples by LSH
hash lookup is a key factor that influence the
NNS precision in the two-stage hashing frame-
work. Generally, higher rerank percentage results
in better top-K NNS Precision. Further more, by
comparing Fig. 2 (c) and (e), it shows that our
framework works better for small K than for large
K. For example, scanning 5.52% of the data is
enough for achieving similar top-10 NNS result
as the traditional IR method while 36.86% of the
data is needed for top-100 NNS. The reason of the
lower performance with large K is that some true
neighbors with the same topic label do not share
high term similarities and may be filtered out in
the LSH step when the rerank percentage is low.
5 Conclusion
In this paper, we proposed a novel two-stage un-
supervised hashing framework for efficient and ef-
fective nearest neighbor search in massive docu-
ment collections. The experimental results have
shown that this framework achieves not only com-
parable search accuracy with the traditional IR
method in retrieving semantically similar docu-
ments, but also an order of magnitude speedup in
search time. Moreover, our approach can com-
bine two similarity measures in a hybrid hashing
scheme, which is beneficial to comprehensively
modeling the document similarity. In our future
work, we plan to design better data representa-
tion which can well fit into the two-stage hash-
ing theme; we also intend to apply the proposed
hashing approach to more informal genres (e.g.,
tweets) and other down-stream NLP applications
(e.g., first story detection).
Acknowledgements
This work was supported by the U.S. ARL
No. W911NF-09-2-0053 (NSCTA), NSF IIS-
0953149, DARPA No. FA8750- 13-2-0041, IBM,
Google and RPI. The views and conclusions con-
tained in this document are those of the authors
and should not be interpreted as representing the
official policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.
499
References
A. Andoni and P. Indyk. 2008. Near-optimal hash-
ing algorithms for approximate nearest neighbor in
high dimensions. Communications of the ACM,
51(1):117?122.
A. Z. Broder, M. Charikar, A. M. Frieze, and
M. Mitzenmacher. 1998. Min-wise independent
permutations. In Proc. STOC.
J. P. Callan, W. B. Croft, and S. M. Harding. 1992. The
inquery retrieval system. In Proc. the Third Interna-
tional Conference on Database and Expert Systems
Applications.
M. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proc. STOC.
T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-
narasimhan, and J. Yagnik. 2013. Fast, accurate
detection of 100,000 object classes on a single ma-
chine. In Proc. CVPR.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. JASIS, 41(6):391?407.
Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.
2013. Iterative quantization: A procrustean ap-
proach to learning binary codes for large-scale im-
age retrieval. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(12):2916?2929.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsuper-
vised mining of lexical variants from noisy text. In
Proc. EMNLP.
K. Krstovski and D. A. Smith. 2011. A minimally su-
pervised approach for detecting and ranking docu-
ment translation pairs. In Proc. the sixth ACL Work-
shop on Statistical Machine Translation.
B. Kulis and K. Grauman. 2012. Kernelized locality-
sensitive hashing. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 34(6):1092?
1104.
Y. Li, C. Chen, W. Liu, and J. Huang. 2014. Sub-
selective quantization for large-scale image search.
In Proc. AAAI Conference on Artificial Intelligence
(AAAI).
W. Liu, J. Wang, S. Kumar, and S.-F. Chang. 2011.
Hashing with graphs. In Proc. ICML.
W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang.
2012. Supervised hashing with kernels. In Proc.
CVPR.
W. Liu. 2012. Large-scale machine learning for classi-
fication and search. In PhD Thesis, Graduate School
of Arts and Sciences, Columbia University.
S. Petrovic, M. Osborne, and V. Lavrenko. 2010.
Streaming first story detection with application to
twitter. In Proc. HLT-NAACL.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005.
Randomized algorithms and nlp: Using locality sen-
sitive hash functions for high speed noun clustering.
In Proc. ACL.
A. Torralba, R. Fergus, and W. T. Freeman. 2008a. 80
million tiny images: A large data set for nonpara-
metric object and scene recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
30(11):1958?1970.
A. Torralba, R. Fergus, and Y. Weiss. 2008b. Small
codes and large image databases for recognition. In
Proc. CVPR.
R. Udupa and S. Kumar. 2010. Hashing-based ap-
proaches to spelling correction of personal names.
In Proc. EMNLP.
Y. Weiss, A. Torralba, and R. Fergus. 2008. Spectral
hashing. In NIPS 21.
500
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706?711,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Be Appropriate and Funny: Automatic Entity Morph Encoding
Boliang Zhang
1
, Hongzhao Huang
1
, Xiaoman Pan
1
, Heng Ji
1
, Kevin Knight
2
Zhen Wen
3
, Yizhou Sun
4
, Jiawei Han
5
, Bulent Yener
1
1
Computer Science Department, Rensselaer Polytechnic Institute
2
Information Sciences Institute, University of Southern California
3
IBM T. J. Watson Research Center
4
College of Computer and Information Science, Northeastern University
5
Computer Science Department, Univerisity of Illinois at Urbana-Champaign
1
{zhangb8,huangh9,panx2,jih,yener}@rpi.edu,
2
knight@isi.edu
3
zhenwen@us.ibm.com,
4
yzsun@ccs.neu.edu,
5
hanj@illinois.edu
Abstract
Internet users are keen on creating differ-
ent kinds of morphs to avoid censorship,
express strong sentiment or humor. For
example, in Chinese social media, users
often use the entity morph ???? (In-
stant Noodles)? to refer to ???? (Zhou
Yongkang)? because it shares one char-
acter ?? (Kang)? with the well-known
brand of instant noodles ???? (Master
Kang)?. We developed a wide variety of
novel approaches to automatically encode
proper and interesting morphs, which can
effectively pass decoding tests
1
.
1 Introduction
One of the most innovative linguistic forms in so-
cial media is Information Morph (Huang et al,
2013). Morph is a special case of alias to hide the
original objects (e.g., sensitive entities and events)
for different purposes, including avoiding censor-
ship (Bamman et al, 2012; Chen et al, 2013),
expressing strong sentiment, emotion or sarcasm,
and making descriptions more vivid. Morphs are
widely used in Chinese social media. Here is an
example morphs: ?????????????
????. (Because of Gua Dad?s issue, Instant
Noodles faces down with Antenna.)?, where
? ??? (Gua Dad)? refers to ???? (Bo Xilai)?
because it shares one character ?? (Gua)? with
???? (Bo Guagua)? who is the son of ???
? (Bo Xilai)?;
? ???? (Instant Noodles)? refers to ????
(Zhou Yongkang)? because it shares one char-
acter ?? (kang)? with the well-known instant
noodles brand ???? (Master Kang)?;
1
The morphing data set is available for research purposes:
http://nlp.cs.rpi.edu/data/morphencoding.tar.gz
? ??? (Antenna)? refers to ???? (Wen Ji-
abao)? because it shares one character ??
(baby)? with the famous children?s television
series ????? (Teletubbies)?;
In contrast with covert or subliminal chan-
nels studied extensively in cryptography and se-
curity, Morphing provides confidentiality against
a weaker adversary which has to make a real time
or near real time decision whether or not to block
a morph within a time interval t. It will take longer
than the duration t for a morph decoder to decide
which encoding method is used and exactly how it
is used; otherwise adversary can create a codebook
and decode the morphs with a simple look up.
We note that there are other distinct characteristics
of morphs that make them different from crypto-
graphic constructs: (1) Morphing can be consid-
ered as a way of using natural language to com-
municate confidential information without encryp-
tion. Most morphs are encoded based on seman-
tic meaning and background knowledge instead
of lexical changes, so they are closer to Jargon.
(2) There can be multiple morphs for an entity.
(3) The Shannon?s Maxim ?the enemy knows the
system? does not always hold. There is no com-
mon code-book or secret key between the sender
and the receiver of a morph. (4) Social networks
play an important role in creating morphs. One
main purpose of encoding morphs is to dissemi-
nate them widely so they can become part of the
new Internet language. Therefore morphs should
be interesting, fun, intuitive and easy to remem-
ber. (5) Morphs rapidly evolve over time, as some
morphs are discovered and blocked by censorship
and newly created morphs emerge.
We propose a brand new and challenging re-
search problem - can we automatically encode
morphs for any given entity to help users commu-
nicate in an appropriate and fun way?
706
2 Approaches
2.1 Motivation from Human Approaches
Let?s start from taking a close look at human?s
intentions and general methods to create morphs
from a social cognitive perspective. In Table 1
and Table 2, we summarize 548 randomly selected
morphs into different categories. In this paper we
automate the first seven human approaches, with-
out investigating the most challenging Method 8,
which requires deep mining of rich background
and tracking all events involving the entities.
2.2 M1: Phonetic Substitution
Given an entity name e, we obtain its pho-
netic transcription pinyin(e). Similarly, for each
unique term t extracted from Tsinghua Weibo
dataset (Zhang et al, 2013) with one billion
tweets from 1.8 million users from 8/28/2012 to
9/29/2012, we obtain pinyin(t). According to the
Chinese phonetic transcription articulation man-
ner
2
, the pairs (b, p), (d, t), (g,k), (z,c), (zh,ch),
( j,q), (sh,r), (x,h), (l,n), (c,ch), (s,sh) and (z,zh)
are mutually transformable.
If a part of pinyin(e) and pinyin(t) are identi-
cal or their initials are transformable, we substi-
tute the part of e with t to form a new morph.
For example, we can substitute the characters of
??? ?? (Bill Gates) [Bi Er Gai Ci]? with
??? (Nose and ear) [Bi Er]? and ??? (Lid)
[Gai Zi]? to form new morph ??? ?? (Nose
and ear Lid) [Bi Er Gai Zi]?. We rank the candi-
dates based on the following two criteria: (1) If the
morph includes more negative words (based on a
gazetteer including 11,729 negative words derived
from HowNet (Dong and Dong, 1999), it?s more
humorous (Valitutti et al, 2013). (2) If the morph
includes rarer terms with low frequency, it is more
interesting (Petrovic and Matthews, 2013).
2.3 M2: Spelling Decomposition
Chinese characters are ideograms, hieroglyphs
and mostly picture-based. It allows us to natu-
rally construct a virtually infinite range of combi-
nations from a finite set of basic units - radicals (Li
and Zhou, 2007). Some of these radicals them-
selves are also characters. For a given entity name
e = c
1
...c
n
, if any character c
k
can be decomposed
into two radicals c
1
k
and c
2
k
which are both char-
acters or can be converted into characters based
on their pictograms (e.g., the radical ??? can be
2
http://en.wikipedia.org/wiki/Pinyin#Initials and finals
converted into??? (grass) ), we create a morph by
replacing c
k
with c
1
k
c
2
k
in e. Here we use a charac-
ter to radical mapping table that includes 191 rad-
icals (59 of them are characters) and 1328 com-
mon characters. For example, we create a morph
???? (Person Dumb Luo)? for ??? (Paul)?
by decomposing ?? (Pau-)? into ?? (Person)?
and ?? (Dull)?. A natural alternative is to com-
posing two chracter radicals in an entity name to
form a morph. However, very few Chinese names
include two characters with single radicals.
2.4 M3: Nickname Generation
We propose a simple method to create morphs by
duplicating the last character of an entity?s first
name. For example, we create a morph ???
(Mimi)? to refer to ??? (Yang Mi)?.
2.5 M4: Translation and Transliteration
Given an entity e, we search its English translation
EN(e) based on 94,015 name translation pairs (Ji
et al, 2009). Then, if any name component in
EN(e) is a common English word, we search for
its Chinese translation based on a 94,966 word
translation pairs (Zens and Ney, 2004), and use the
Chinese translation to replace the corresponding
characters in e. For example, we create a morph
??? ?? (Larry bird)? for ??? ?? (Larry
Bird)? by replacing the last name ??? (Bird)?
with its Chinese translation ??? (bird)?.
2.6 M5: Semantic Interpretation
For each character c
k
in the first name of a given
entity name e, we search its semantic interpreta-
tion sentence from the Xinhua Chinese character
dictionary including 20,894 entries
3
. If a word
in the sentence contains c
k
, we append the word
with the last name of e to form a new morph. Sim-
ilarly to M1, we prefer positive, negative or rare
words. For example, we create a morph ????
(Bo Mess)? for ???? (Bo Xi Lai)? because the
semantic interpretation sentence for ?? (Lai)? in-
cludes a negative word ??? (Mess)?.
2.7 M6: Historical Figure Mapping
We collect a set of 38 famous historical figures
including politicians, emperors, poets, generals,
ministers and scholars from a website. For a given
entity name e, we rank these candidates by ap-
plying the resolution approach as described in our
previous work (Huang et al, 2013) to measure the
similarity between an entity and a historic figure
3
http://xh.5156edu.com/
707
Category
Frequency
Distribution
Examples
Entity Morph Comment
(1) Avoid censorship 6.56% ??? (Bo Xi-
lai)
B?? (B Secre-
tary)
?B? is the first letter of ?Bo? and ?Secretary? is
the entity?s title.
(2) Express strong
sentiment, sarcasm,
emotion
15.77% ??? (Wang
Yongping)
? ? ? (Miracle
Brother)
Sarcasm on the entity?s public speech: ?It?s a mir-
acle that the girl survived (from the 2011 train col-
lision)?.
(3) Be humorous or
make descriptions
more vivid
25.91% ?? (Yang Mi) ???? (Tender
Beef Pentagon)
The entity?s face shape looks like the shape of fa-
mous KFC food ?Tender Beef Pentagon?.
Mixture 25.32% ? ? ?
(Gaddafi)
???? (Crazy
Duck Colonel)
Sarcasm on Colonel Gaddafi?s violence.
Others 23.44% ??? (Chi-
ang Kai-shek)
??? (Peanut) Joseph Stilwell, a US general in China during
World War II, called Chiang Kai-shek ????
(Peanut)? in his diary because of his stubbornness.
Table 1: Morph Examples Categorized based on Human Intentions
No. Category
Frequency
Distribution
Example
Entity Morph Comment
M1 Phonetic Sub-
stitution
12.77% ? ? ?
(Sarkozy)
??? (Silly Po-
lite)
The entity?s phonetic transcript ?Sa Ke Qi? is
similar to the morph?s ?Sha Ke Qi?.
M2 Spelling De-
composition
0.73% ??? (Hu
Jintao)
?? (Old Moon) The entity?s last name is decomposed into the
morph ??? (Old Moon)??
M3 Nickname Gen-
eration
12.41% ??? (Jiang
Zemin)
?? (Old Jiang) The morph is a conventional name for old people
with last name ?Jiang?.
M4 Translation &
Transliteration
3.28% ?? (Bush) ?? (shrub) The morph is the Chinese translation of ?bush?.
M5 Semantic Inter-
pretation
20.26% ??? (Kim
Il Sung)
??? (Kim Sun) The character ??? in the entity name means ??
? (Sun)?.
M6 Historical Fig-
ure Mapping
3.83% ??? (Bo
Xilai)
??? (Conquer
West King)
The entity shares characteristics and political ex-
periences similar to the morph.
M7 Characteristics
Modeling
20.62% ??? (Kim
Il Sung)
??? (Kim Fat) ??? (Fat)? describes ???? (Kim Il
Sung)??s appearance.
M8
Reputation and
public perception
26.09%
? ? ?
(Obama)
?? (Staring at
the sea)
Barack Obama received a calligraphy ????
? (Staring at sea and listening to surf)? as a
present when he visited China.
??? (Ma
Jingtao)
???? (Roar
Bishop)
In the films Ma Jingtao starred, he always used
exaggerated roaring to express various emotions.
??? (Ma
Yingjiu)
??? (Ma Se-
cession)
The morph derives from Ma Yingjiu?s political
position on cross-strait relations.
Table 2: Morph Examples Categorized based on Human Generation Methods
based on their semantic contexts. For example,
this approach generates a morph ??? (the First
Emperor)? for ???? (Mao Zedong)? who is the
first chairman of P. R. China and ??? (the Sec-
ond Emperor )? for ???? (Deng Xiaoping )?
who succeeded Mao.
2.8 M7: Characteristics Modeling
Finally, we propose a novel approach to auto-
matically generate an entity?s characteristics using
Google word2vec model (Mikolov et al, 2013).
To make the vocabulary model as general as pos-
sible, we use all of the following large corpora
that we have access to: Tsinghua Weibo dataset,
Chinese Gigaword fifth edition
4
which includes
10 million news documents, TAC-KBP 2009-2013
Source Corpora (McNamee and Dang, 2009; Ji et
4
http://catalog.ldc.upenn.edu/LDC2011T13
al., 2010; Ji et al, 2011; Ji and Grishman, 2011)
which include 3 million news and web documents,
and DARPA BOLT program?s discussion forum
corpora with 300k threads. Given an entity e, we
compute the semantic relationship between e and
each word from these corpora. We then rank the
words by: (1) cosine similarity, (2) the same cri-
teria as in section 2.6. Finally we append the top
ranking word to the entity?s last name to obtain
a new morph. Using this method, we are able
to generate many vivid morphs such as ?? ??
(Yao Wizard)? for ??? (Yao Ming)?.
3 Experiments
3.1 Data
We collected 1,553,347 tweets from Chinese Sina
Weibo from May 1 to June 30, 2013. We extracted
708
187 human created morphs based on M1-M7 for
55 person entities. Our approach generated 382
new morphs in total.
3.2 Human Evaluation
We randomly asked 9 Chinese native speakers
who regularly access Chinese social media and are
not involved in this work to conduct evaluation in-
dependently. We designed the following three cri-
teria based on Table 1:
? Perceivability: Who does this morph refer to?
(i) Pretty sure, (ii) Not sure, and (iii) No clues.
? Funniness: How interesting is the morph? (i)
Funny, (ii) Somewhat funny, and (iii) Not funny.
? Appropriateness: Does the morph describe the
target entity appropriately? (i) Make sense, (ii)
Make a little sense, and (iii) Make no sense.
The three choices of each criteria account for
100% (i), 50% (ii) and 0% (iii) satisfaction rate,
respectively. If the assessor correctly predicts the
target entity with the Perceivability measure, (s)he
is asked to continue to answer the Funniness and
Appropriateness questions; otherwise the Funni-
ness and Appropriateness scores are 0. The hu-
man evaluation results are shown in Table 4. The
Fleiss?s kappa coefficient among all the human as-
sessors is 0.147 indicating slight agreement.
From Table 4 we can see that overall the sys-
tem achieves 66% of the human performance
with comparable stability as human. In partic-
ular, Method 4 based on translation and translit-
eration generates much more perceivable morphs
than human because the system may search in a
larger vocabulary. Interestingly, similar encour-
aging results - system outperforms human - have
been observed by previous back-transliteration
work (Knight and Graehl, 1998).
It?s also interesting to see that human assessors
can only comprehend 76% of the human generated
morphs because of the following reasons: (1) the
morph is newly generated or it does not describe
the characteristics of the target entity well; and (2)
the target entity itself is not well known to human
assessors who do not keep close track of news top-
ics. In fact only 64 human generated morphs and
72 system generated morphs are perceivable by all
human assessors.
For Method 2, the human created morphs are
assessed as much more and funny than the sys-
tem generated ones because human creators use
this approach only if: (1). the radicals still reflect
the meaning of the character (e.g., ?? (worry)?
is decomposed into two radicals ??? (heart au-
tumn)? instead of three ????? (grain fire heart)
because people tend to feel sad when the leaves
fall in the autumn), (2). the morph reflects some
characteristics of the entity (e.g., ???? (Jiang
Zemin)? has a morph ????? (Water Engi-
neer Zemin)? because he gave many instructions
on water conservancy construction); or (3). The
morph becomes very vivid and funny (e.g., the
morph ?????? (Muji Yue Yue Bird)? for
???? is assessed as very funny because ??
?(Muji)? looks like a Japanese name, ???(Yue
Yue)? can also refer to a famous chubby woman,
and ??? (bird man)? is a bad word referring to
bad people); or (4). The morph expresses strong
sentiment or sarcasm; or (5) The morph is the
name of another entity (e.g., the morph ???(Gu
Yue)? for ????(Hu Jintao)? is also the name
of a famous actor who often acts as Mao Zedong).
The automatic approach didn?t explore these intel-
ligent constraints and thus produced more boring
morph. Moreover, sometimes human creators fur-
ther exploit traditional Chinese characters, gener-
alize or modify the decomposition results.
Table 3 presents some good (with average score
above 80%) and bad (with average score below
20%) examples.
Good Examples
Entity Morph Method
??? (Osama bin
Laden)
??? (The silly turn-
ing off light)
M1
??? (Chiang Kai-
shek)
???? (Grass Gen-
eral Jie Shi)
M2
???? (Bill Gates) ???? (Bill Gates) M4
Bad Examples
Entity Morph Method
?? (Kobe) ?? (Arm) M1
? ? ? ? ?
(Medvedev)
??? (Mei Virtue) M5
??? (Jeremy Lin) ?? (Lao Tze) M6
Table 3: System Generated Morph Examples
To understand whether users would adopt sys-
tem generated morphs for their social media com-
munication, we also ask the assessors to recite
the morphs that they remember after the survey.
Among all the morphs that they remember cor-
rectly, 20.4% are system generated morphs, which
is encouraging.
3.3 Automatic Evaluation
Another important goal of morph encoding is to
avoid censorship and freely communicate about
709
Human System Human System Human System Human System Human System Human System Human System Human System# of morphs 17 124 4 21 10 54 9 28 64 87 9 18 74 50 187 382Perceivability 75 76 95 86 94 81 61 71 87 59 66 5 77 34 76 67Funniness 78 49 92 43 44 41 70 47 70 35 74 28 79 44 76 46Appropriateness 71 51 89 59 81 43 75 49 76 36 78 18 82 38 79 43Average 75 59 92 57 73 55 69 56 78 43 73 17 79 39 77 52Standard Deviation 12.29 21.81 7.32 11.89 13.2 9.2 17.13 20.3 18.83 17.54 10.01 21.23 15.18 15.99 15.99 18.14
h s2568 58984214.3 29691742 45712641 1153922692 26766901.8 811317052 1278447812 1E+05255.7 329.12568 58984 214.3 2969 1742 4571 2641 11539 22692 26766 901.8 8113 17052 12784
M6 M7 OverallM1 M2 M3 M4 M5
Table 4: Human Evaluation Satisfaction Rate (%)
certain entities. To evaluate how well the new
morphs can pass censorship, we simulate the cen-
sorship using an automatic morph decoder con-
sisted of a morph candidate identification system
based on Support Vector Machines incorporating
anomaly analysis and our morph resolution sys-
tem (Huang et al, 2013). We use each system gen-
erated morph to replace its corresponding human-
created morphs in Weibo tweets and obtain a new
?morphed? data set. The morph decoder is then
applied to it. We define discovery rate as the per-
centage of morphs identified by the decoder, and
the ranking accuracy Acc@k to evaluate the reso-
lution performance. We conduct this decoding ex-
periment on 247 system generated and 151 human
generated perceivable morphs with perceivability
scores > 70% from human evaluation.
Figure 1 shows that in general the decoder
achieves lower discovery rate on system gener-
ated morphs than human generated ones, because
the identification component in the decoder was
trained based on human morph related features.
This result is promising because it demonstrates
that the system generated morphs contain new and
unique characteristics which are unknown to the
decoder. In contrast, from Figure 2 we can see
that system generated morphs can be more easily
resolved into the right target entities than human
generated ones which are more implicit.
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
M1	 ? M2	 ? M3	 ? M4	 ? M5	 ? M6	 ? M7	 ? ALL	 ?
Human	 ?created	 ?	 ?morph	 ? System	 ?generated	 ?morph	 ?
Figure 1: Discovery Rate (%)
4 Related Work
Some recent work attempted to map between Chi-
nese formal words and informal words (Xia et al,
2005; Xia and Wong, 2006; Xia et al, 2006; Li
Figure 2: Resolution Acc@K Accuracy (%)
and Yarowsky, 2008; Wang et al, 2013; Wang and
Kan, 2013). We incorporated the pronunciation,
lexical and semantic similarity measurements pro-
posed in these approaches. Some of our basic se-
lection criteria are also similar to the constraints
used in previous work on generating humors (Val-
itutti et al, 2013; Petrovic and Matthews, 2013).
5 Conclusions and Future Work
This paper proposed a new problem of encoding
entity morphs and developed a wide variety of
novel automatic approaches. In the future we will
focus on improving the language-independent ap-
proaches based on historical figure mapping and
culture and reputation modeling. In addition, we
plan to extend our approaches to other types of in-
formation including sensitive events, satires and
metaphors so that we can generate fable stories.
We are also interested in tracking morphs over
time to study the evolution of Internet language.
Acknowledgments
This work was supported by U.S. ARL No.
W911NF-09-2-0053, DARPA No. FA8750-13-
2-0041 and No. W911NF-12-C-0028, ARO
No. W911NF-13-1-0193, NSF IIS-0953149,
CNS-0931975, IIS-1017362, IIS-1320617, IIS-
1354329, IBM, Google, DTRA, DHS and RPI.
The views and conclusions in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
710
References
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2012. Censorship and deletion practices in
Chinese social media. First Monday, 17(3).
Le Chen, Chi Zhang, and Christo Wilson. 2013.
Tweeting under pressure: analyzing trending topics
and evolving word choice on sina weibo. In Pro-
ceedings of the first ACM conference on Online so-
cial networks, pages 89?100.
Zhendong Dong and Qiang Dong. 1999. Hownet. In
http://www.keenage.com.
Hongzhao Huang, Zhen Wen, Dian Yu, Heng Ji,
Yizhou Sun, Jiawei Han, and He Li. 2013. Resolv-
ing entity morphs in censored data. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL2013).
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the Association for Computational
Linguistics (ACL2011).
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name extraction
and translation for distillation. Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Text Analysis
Conference (TAC) 2010.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac 2011 knowledge base popula-
tion track. In Proc. Text Analysis Conference (TAC)
2011.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Zhifei Li and David Yarowsky. 2008. Mining and
modeling relations between formal and informal chi-
nese phrases from web corpora. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP2008), pages 1031?
1040.
Jianyu Li and Jie Zhou. 2007. Chinese character struc-
ture analysis based on complex networks. Phys-
ica A: Statistical Mechanics and its Applications,
380:629?638.
Paul McNamee and Hoa Trang Dang. 2009.
Overview of the tac 2009 knowledge base popula-
tion track. In Proceedings of Text Analysis Confer-
ence (TAC2009).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111?3119.
Sasa Petrovic and David Matthews. 2013. Unsuper-
vised joke generation from big data. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL2013).
Alessandro Valitutti, Hannu Toivonen, Antoine
Doucet, and Jukka M. Toivanen. 2013. ?let every-
thing turn well in your wife?: Generation of adult
humor using lexical constraints. In Proceedings
of the Association for Computational Linguistics
(ACL2013).
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from chinese microtext: Joint word recog-
nition and segmentation. In Proceedings of the As-
sociation for Computational Linguistics (ACL2013).
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of International Joint Conference on Natu-
ral Language Processing (IJCNLP2013).
Yunqing Xia and Kam-Fai Wong. 2006. Anomaly de-
tecting within dynamic chinese chat text. In Proc.
Workshop On New Text Wikis And Blogs And Other
Dynamic Text Sources.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. Nil
is not nothing: Recognition of chinese network in-
formal language expressions. In 4th SIGHAN Work-
shop on Chinese Language Processing at IJCNLP,
volume 5.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006.
A phonetic-based approach to chinese chat text nor-
malization. In Proceedings of COLING-ACL2006,
pages 993?1000.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proceedings of HLT-NAACL2004.
Jing Zhang, Biao Liu, Jie Tang, Ting Chen, and Juanzi
Li. 2013. Social influence locality for modeling
retweeting behaviors. In Proceedings of the 23rd
International Joint Conference on Artificial Intelli-
gence (IJCAI?13), pages 2761?2767.
711
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 7,
Baltimore, Maryland, USA, 22 June 2014. c?2014 Association for Computational Linguistics
Wikification and Beyond:  
The Challenges of Entity and Concept Grounding 
Dan Roth Heng Ji 
University of Illinois at Urbana-Champaign Rensselaer Polytechnic Institute 
danr@illinois.edu jih@rpi.edu 
  
Ming-Wei Chang Taylor Cassidy 
Microsoft Research Army Research Lab & IBM Research 
minchang@microsoft.com taylor.cassidy.ctr@mail.mil 
 
 
 
  
1 Introduction 
Contextual disambiguation and grounding of 
concepts and entities in natural language are es-
sential to progress in many natural language un-
derstanding tasks and fundamental to many ap-
plications. Wikification aims at automatically 
identifying concept mentions in text and linking 
them to referents in a knowledge base (KB) (e.g., 
Wikipedia). Consider the sentence, "The Times 
report on Blumenthal (D) has the potential to 
fundamentally reshape the contest in the Nutmeg 
State.". A Wikifier should identify the key enti-
ties and concepts and map them to an encyclope-
dic resource (e.g., ?D? refers to Democratic Par-
ty, and ?the Nutmeg State? refers to Connecticut.  
   Wikification benefits end-users and Natural 
Language Processing (NLP) systems. Readers 
can better comprehend Wikified documents as 
information about related topics is readily acces-
sible. For systems, a Wikified document eluci-
dates concepts and entities by grounding them in 
an encyclopedic resource or an ontology. Wikifi-
cation output has improved NLP down-stream 
tasks, including coreference resolution, user in-
terest discovery , recommendation and search. 
  This task has received increased attention in 
recent years from the NLP and Data Mining 
communities, partly fostered by the U.S. NIST 
Text Analysis Conference Knowledge Base Pop-
ulation (KBP) track, and several versions of it 
has been studied. These include Wikifying all 
concept mentions in a single text document; 
Wikifying a cluster of co-referential named enti-
ty mentions that appear across documents (Entity 
Linking), and Wikifying a whole document to a 
single concept. Other works relate this task to 
coreference resolution within and across docu-
ments and in the context of multiple text genres. 
2 Content Overview 
This tutorial will motivate Wikification as a 
broad paradigm for cross-source linking for 
knowledge enrichment. We will discuss multiple 
dimensions of the task definition, present the 
building blocks of a state-of-the-art Wikifier, 
share key lessons learned from analysis of re-
sults, and discuss recently proposed ideas for 
advancing work in this area in response to key 
challenges. We will touch on new research areas 
including interactive Wikification, social media, 
and censorship. The tutorial will be useful for all 
those with interests in cross-source information 
extraction and linking, knowledge acquisition, 
and the use of acquired knowledge in NLP. We 
will provide a concise roadmap of recent per-
spectives and results, and point to some of our 
available Wikification resources.  
3 Outline 
? Introduction and Motivation 
? Methodological presentation of a skeletal Wik-
ification system 
o Mention and candidate identification 
o Knowledge representation  
o Local and global context analysis 
o Role of Machine Learning 
? Obstacles & Advanced Methods 
o Joint modeling 
o Collective inference 
o Scarcity of supervision signals 
o Diverse text genres and social media 
? Remaining Challenges and Future Work 
o Rich semantic knowledge acquisition 
o Cross-lingual Wikification 
References 
http://nlp.cs.rpi.edu/kbp/2014/elreading.html 7
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 1?9,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Graph-based Clustering for Computational Linguistics: A Survey 
 
 
Zheng Chen 
The Graduate Center 
The City University of New York 
zchen1@gc.cuny.edu 
Heng Ji 
Queens College and The Graduate Center 
The City University of New York 
hengji@cs.qc.cuny.edu 
 
  
 
Abstract 
 
In this survey we overview graph-based clus-
tering and its applications in computational 
linguistics. We summarize graph-based clus-
tering as a five-part story: hypothesis, model-
ing, measure, algorithm and evaluation. We 
then survey three typical NLP problems in 
which graph-based clustering approaches 
have been successfully applied.  Finally, we 
comment on the strengths and weaknesses of 
graph-based clustering and envision that 
graph-based clustering is a promising solu-
tion for some emerging NLP problems. 
1 Introduction 
In the passing years, there has been a tremend-
ous body of work on graph-based clustering, 
either done by theoreticians or practitioners. 
Theoreticians have been extensively investigat-
ing cluster properties, quality measures and var-
ious clustering algorithms by taking advantage 
of elegant mathematical structures built in graph 
theory. Practitioners have been investigating the 
graph clustering algorithms for specific applica-
tions and claiming their effectiveness by taking 
advantage of the underlying structure or other 
known characteristics of the data. Although 
graph-based clustering has gained increasing 
attentions from Computational Linguistic (CL) 
community (especially through the series of 
TextGraphs workshops), it is studied case by 
case and as far as we know, we have not seen 
much work on comparative study of various 
graph-based clustering algorithms for certain 
NLP problems. The major goal of this survey is 
to ?bridge? the gap between theoretical aspect 
and practical aspect in graph-based clustering, 
especially for computational linguistics. 
From the theoretical aspect, we state that the 
following five-part story describes the general 
methodology of graph-based clustering: 
(1) Hypothesis. The hypothesis is that a graph 
can be partitioned into densely connected sub-
graphs that are sparsely connected to each other. 
(2) Modeling. It deals with the problem of trans-
forming data into a graph or modeling the real 
application as a graph.  
(3) Measure. A quality measure is an objective 
function that rates the quality of a clustering.  
(4) Algorithm. An algorithm is to exactly or 
approximately optimize the quality measure.  
(5) Evaluation. Various metrics can be used to 
evaluate the performance of clustering by com-
paring with a ?ground truth? clustering. 
From the practical aspect, we focus on three 
typical NLP applications, including coreference 
resolution, word clustering and word sense dis-
ambiguation, in which graph-based clustering 
approaches have been successfully applied and 
achieved competitive performance. 
2 Graph-based Clustering Methodology 
We start with the basic clustering problem. Let 
?? = {??1, ? , ????}  be a set of data points, ?? =
??????? ??? ,??=1,?,??  be the similarity matrix in which 
each element indicates the similarity ?????? ? 0 be-
tween two data points ????  and ???? . A nice way to 
represent the data is to construct a graph on 
which each vertex represents a data point and 
the edge weight carries the similarity of two 
vertices. The clustering problem in graph pers-
pective is then formulated as partitioning the 
graph into subgraphs such that the edges in the 
same subgraph have high weights and the edges 
between different subgraphs have low weights. 
In the next section, we define essential graph 
notation to facilitate discussions in the rest of 
this survey. 
1
2.1 Graph Notation 
A graph is a triple G=(V,E,W)  where ?? =
{??1, ? , ????} is a set of vertices, E?V?V is a set 
of edges, and ?? = ??????? ??? ,??=1,?,??  is called adja-
cency matrix in which each element indicates a 
non-negative weight ( ?????? ? 0)  between two 
vertices ????  and ???? .  
In this survey we target at hard clustering 
problem which means we partition vertices of 
the graph into non-overlapping clusters, i.e., let 
?? = (??1, ? ,????) be a partition of ?? such that 
(1) ???? ? ? for ?? ? {1, ? ,??}.  
(2) ???? ? ???? = ? for ??, ?? ? {1, ? ,??} and ?? ? ??  
(3) ??1 ??? ???? = ?? 
2.2 Hypothesis 
The hypothesis behind graph-based clustering 
can be stated in the following ways:  
(1) The graph consists of dense subgraphs such 
that a dense subgraph contains more well-
connected internal edges connecting the 
vertices in the subgraph than cutting edges 
connecting the vertices across subgraphs.  
(2) A random walk that visits a subgraph will 
likely stay in the subgraph until many of its 
vertices have been visited (Dongen, 2000). 
(3) Among all shortest paths between all pairs 
of vertices, links between different dense 
subgraphs are likely to be in many shortest 
paths (Dongen, 2000). 
2.3 Modeling 
Modeling addresses the problem of transform-
ing the problem into graph structure, specifical-
ly, designating the meaning of vertices and 
edges in the graph, computing the edge weights 
for weighted graph, and constructing the graph. 
Luxburg (2006) stated three most common me-
thods to construct a graph: ?? -neighborhood 
graph, ??-nearest neighbor graph, and fully con-
nected graph. Luxburg analyzed different beha-
viors of the three graph construction methods, 
and stated that some graph-cluster algorithms 
(e.g., spectral clustering) can be quite sensitive 
to the choice of graphs and parameters (?? and ??). 
As a general recommendation, Luxburg sug-
gested exploiting ??-nearest neighbor graph as 
the first choice, which is less vulnerable to the 
choices of parameters than other graphs. Unfor-
tunately, theoretical justifications on the choices 
of graphs and parameters do not exist and as a 
result, the problem has been ignored by practi-
tioners.  
2.4 Measure 
A measure is an objective function that rates the 
quality of a clustering, thus called quality meas-
ure. By optimizing the quality measure, we can 
obtain the ?optimal? clustering. 
It is worth noting that quality measure should 
not be confused with vertex similarity measure 
where it is used to compute edge weights. Fur-
thermore, we should distinguish quality meas-
ure from evaluation measure which will be dis-
cussed in section 2.6. The main difference is 
that cluster quality measure directly identifies a 
clustering that fulfills a desirable property while 
evaluation measure rates the quality of a cluster-
ing by comparing with a ground-truth clustering.  
We summarize various quality measures in 
Table 1, from the basic density measures (intra-
cluster and inter-cluster), to cut-based measures 
(ratio cut, ncut, performance, expansion, con-
ductance, bicriteria), then to the latest proposed 
measure modularity. Each of the measures has 
strengths and weaknesses as commented in Ta-
ble 1. Optimizing each of the measures is NP-
hard. As a result, many efficient algorithms, 
which have been claimed to solve the optimal 
problem with polynomial-time complexity, 
yield sub-optimal clustering.  
2.5  Algorithm 
We categorize graph clustering algorithms into 
two major classes: divisive and agglomerative 
(Table 2). In the divisive clustering class, we 
categorize algorithms into several subclasses, 
namely, cut-based, spectral clustering, multile-
vel, random walks, shortest path. Divisive clus-
tering follows top-down style and recursively 
splits a graph into subgraphs. In contrast, ag-
glomerative clustering works bottom-up and 
iteratively merges singleton sets of vertices into 
subgraphs. The divisive and agglomerative al-
gorithms are also called hierarchical since they 
produce multi-level clusterings, i.e., one cluster-
ing follows the other by refining (divisive) or 
coarsening (agglomerative). Most graph cluster-
ing algorithms ever proposed are divisive. We 
list the quality measure and the running com-
plexity for each algorithm in Table 2. 
2
Measures Comments 
intra-cluster density 
inter-cluster density 
? Maximizing intra-cluster density is equivalent to minimizing inter-cluster 
density and vice versa 
? Drawback: both favor cutting small sets of isolated vertices in the graph 
(Shi and Malik, 2000) 
ratio cut (Hagan and Kahng, 
1992) 
ncut (Shi and Malik, 2000) 
? Ratio cut is suitable for unweighted graph, and ncut is a better choice for 
weighted graph 
? Overcome the drawback of intra-cluster density or inter-cluster density 
? Drawback: both favor clusters with equal size  
performance (Dongen, 2000; 
Brandes et al, 2003) 
? Performance takes both intra-cluster density and inter-cluster density  
into considerations simultaneously 
expansion, conductance, 
bicriteria 
(Kannan et al, 2000) 
? Expansion is suitable for unweighted graph, and conductance is a better 
choice for weighted graph 
? Both expansion and conductance impose quality within clusters, but not 
inter-cluster quality; bicriteria takes both into considerations 
modularity (Newman and 
Girvan,2004) 
? Evaluates the quality of clustering with respect to a randomized graph  
? Drawbacks: (1) It requires global knowledge of the graph?s topology, 
i.e., the number of edges. Clauset (2005) proposed an improved measure 
Local Modularity. (2) Resolution limit problem: it fails to identify clus-
ters smaller than a certain scale. Ruan and Zhang (2008) proposed an 
improved measure HQcut. (3) It fails to distinguish good from bad clus-
tering between different graphs with the same modularity value. Chen et 
al. (2009) proposed an improved measure Max-Min Modularity 
Table 1. Summary of Quality Measures 
Category Algorithms optimized 
measure 
running 
complexity 
divisive cut-based Kernighan-Lin algorithm 
(Kernighan and Lin, 1970) 
intercluster ??(|??|3) 
cut-clustering algorithm 
(Flake et al, 2003) 
bicriteria ??(|??|) 
spectral unnormalized spectral clustering 
(Luxburg, 2006) 
ratiocut ??(|??||??|) 
normalized spectral clustering I  
(Luxburg, 2006; Shi and Malik, 2000) 
ncut ??(|??||??|) 
normalized spectral clustering II 
 (Luxburg, 2006; Ng, 2002) 
ncut ??(|??||??|) 
iterative conductance cutting (ICC) 
 (Kannan et al,2000) 
conductance ??(|??||??|) 
geometric MST clustering (GMC) 
(Brandes et al, 2007)  
pluggable(any 
quality measure) 
??(|??||??|) 
modularity oriented 
(White and Smyth,2005) 
modularity ??(|??||??|) 
multilevel multilevel recursive bisection 
(Karypis and Kumar, 1999) 
intercluster ??(|??|????????) 
multilevel ??-way partitioning 
(Karypis and Kumar, 1999) 
intercluster ??(|??|
+ ??????????) 
random Markov Clustering Algorithm (MCL) 
(Dongen, 2000) 
performance ??(??2|??|) 
shortest 
path 
betweenness  
(Girvan and Newman, 2003) 
modularity ??(|??||??|2) 
information centrality 
(Fortunato et al, 2004) 
modularity ??(|??||??|3) 
agglomerative modularity oriented 
(Newman, 2004) 
modularity ??(|??||??|) 
Table 2. Summary of Graph-based Clustering Algorithms (|??|: the number of vertices, |??|: the 
number of edges, ??: the number of clusters, ??: the number of resources allocated for each vertex) 
3
The first set of algorithms (cut-based) is asso-
ciated with max-flow min-cut theorem (Ford and 
Fulkerson, 1956) which states that ?the value of 
the maximum flow is equal to the cost of the 
minimum cut?. One of the earliest algorithm, 
Kernighan-Lin algorithm (Kernighan and Lin, 
1970) splits the graph by performing recursive 
bisection (split into two parts at a time), aiming 
to minimize inter-cluster density (cut size). The 
high complexity of the algorithm ( ??(|??|3) 
makes it less competitive in real applications. 
Flake et al (2003) proposed a cut-clustering al-
gorithm which optimizes the bicriterion measure 
and the complexity is proportional to the number 
of clusters ?? using a heuristic, thus the algorithm 
is competitive in practice. 
The second set of algorithms is based on spec-
tral graph theory with Laplacian matrix as the 
mathematical tool. The connection between clus-
tering and spectrum of Laplacian matrix (??) bas-
ically lies in the following important proposition: 
the multiplicity ?? of the eigenvalue 0 of ?? equals 
to the number of connected components in the 
graph. Luxburg (2006) and Abney (2007) pre-
sented a comprehensive tutorial on spectral clus-
tering. Luxburg (2006) discussed three forms of 
Laplacian matrices (one unnormalized form and 
two normalized forms) and their three corres-
ponding spectral clustering algorithms (unnorma-
lized, normalized I and normalized II). Unnorma-
lized clustering aims to optimize ratiocut meas-
ure while normalized clustering aims to optimize 
ncut measure (Shi and Malik, 2000), thus spec-
tral clustering actually relates with cut-based 
clustering. The success of spectral clustering is 
mainly based on the fact that it does not make 
strong assumptions on the form of the clusters 
and can solve very general problems like intert-
wined spirals which k-means clustering handles 
much worse. Unfortunately, spectral clustering 
could be unstable under different choices of 
graphs and parameters as mentioned in section 
2.3. Luxburg et al (2005) compared unnorma-
lized clustering with normalized version and 
proved that normalized version always converges 
to a sensible limit clustering while for unnorma-
lized case the same only holds under strong addi-
tional assumptions which are not always satisfied. 
The running complexity of spectral clustering 
equals to the complexity of computing the eigen-
vectors of Laplacian matrix which is ??(|??|3) . 
However, when the graph is sparse, the complex-
ity is reduced to ??(|??||??|) by applying efficient 
Lanczos algorithm. 
The third set of algorithms is based on multi-
level graph partitioning paradigm (Karypis and 
Kumar, 1999) which consists of three phases: 
coarsening phase, initial partitioning phase and 
refinement phase. Two approaches have been 
developed in this category, one is multilevel re-
cursive bisection which recursively splits into 
two parts by performing multilevel paradigm 
with complexity of ??(|??|????????) ; the other is 
multilevel ?? -way partitioning which performs 
coarsening and refinement only once and directly 
partitions the graph into ??  clusters with com-
plexity of ??(|??| + ??????????). The latter approach 
is superior to the former one for less running 
complexity and comparable (sometimes better) 
clustering quality.  
The fourth set of algorithms is based on the 
second interpretation of the hypothesis in section 
2.2, i.e., a random walk is likely to visit many 
vertices in a cluster before moving to the other 
cluster. An outstanding approach in this category 
is presented in Dogen (2000), named Markov 
clustering algorithm (MCL). The algorithm itera-
tively applies two operators (expansion and infla-
tion) by matrix computation until convergence. 
Expansion operator simulates spreading of ran-
dom walks and inflation models demotion of in-
ter-cluster walks; the sequence matrix computa-
tion results in eliminating inter-cluster interac-
tions and leaving only intra-cluster components. 
The complexity of MCL is ??(??2|??|) where ?? is 
the number of resources allocated for each vertex. 
A key point of random walk is that it is actually 
linked to spectral clustering (Luxburg, 2006), 
e.g., ncut can be expressed in terms of transition 
probabilities and optimizing ncut can be 
achieved by computing the stationary distribu-
tion of a random walk in the graph. 
The final set of algorithms in divisive category 
is based on the third interpretation of the hypo-
thesis in section 2.2, i.e., the links between clus-
ters are likely to be in the shortest paths. Girvan 
and Newman (2003) proposed the concept of 
edge betweenness which is the number of short-
est paths connecting any pair of vertices that pass 
through the edge. Their algorithm iteratively re-
moves one of the edges with the highest bet-
weenness. The complexity of the algorithm is 
??(|??||??|2). Instead of betweenness, Fortunato et 
al. (2004) used information centrality for each 
edge and stated that it performs better than bet-
weenness but with a higher complexity of 
??(|??||??|3). 
The agglomerative category contains much 
fewer algorithms. Newman (2004) proposed an 
4
algorithm that starts each vertex as singletons, 
and then iteratively merges clusters together in 
pairs, choosing the join that results in the greatest 
increase (or smallest decrease) in modularity 
score. The algorithm converges if there is only 
cluster left in the graph, then from the clustering 
hierarchy, we choose the clustering with maxi-
mum modularity. The complexity of the algo-
rithm is ??(|??||??|). 
The algorithms we surveyed in this section are 
by no means comprehensive as the field is long-
standing and still evolving rapidly. We also refer 
readers to other informative references, e.g., 
Schaeffer (2007), Brandes et al (2007) and 
Newman (2004). 
A natural question arises: ?which algorithm 
should we choose?? A general answer to this 
question is that no algorithm is a panacea. First, 
as we mentioned earlier, a clustering algorithm is 
usually proposed to optimize some quality meas-
ure, therefore, it is not fair to compare an algo-
rithm that favors one measure with the other one 
that favors some other measure. Second, there is 
not a perfect measure that captures the full cha-
racteristics of cluster structures; therefore a per-
fect algorithm does not exist. Third, there is no 
definition for so called ?best clustering?. The 
?best? depends on applications, data characteris-
tics, and granularity. 
2.6 Evaluation 
We discussed various quality measures in section 
2.4, however, a clustering optimizing some 
quality measure does not necessarily translate 
into effectiveness in real applications with re-
spect to the ground truth clustering and thus an 
evaluation measure plays the role of evaluating 
how well the clustering matches the gold stan-
dard. Two questions arise: (1) what constraints 
(properties, criteria) should an ideal evaluation 
measure satisfy? (2) Do the evaluation measures 
ever proposed satisfy the constraints?  
For the first question, there have been several 
attempts on it: Dom (2001) developed a parame-
tric technique for describing the quality of a clus-
tering and proposed five ?desirable properties? 
based on the parameters; Meila (2003) listed 12 
properties associated with the proposed entropy 
measure; Amigo et al (2008) proposed four con-
straints including homogeneity, completeness, 
rag bag, and cluster size vs. quantity. A parallel 
comparison shows that the four constraints pro-
posed by Amigo et al (2008) have advantages 
over the constraints proposed in the other two 
papers, for one reason, the four constraints can 
describe all the important constraints in Dom 
(2001) and Meila (2003), but the reverse does 
not hold; for the other reason, the four con-
straints can be formally verified for each evalua-
tion measure, but it is not true for the constraints 
in Dom (2001). 
Table 3 lists the evaluation measures ever pro-
posed (including those discussed in Amigo et al, 
2008 and some other measures known for corefe-
rence resolution). To answer the second question 
proposed in this section, we conclude the find-
ings in Amigo et al (2008) plus our new findings 
about MUC and CEAF as follows: (1) all the 
measures except B-Cubed fail the rag bag con-
straint and only B-Cubed measure can satisfy all 
the four constraints; (2) two entropy based meas-
ures (VI and V) and MUC only fail the rag bag 
constraint; (3) all the measures in set mapping 
category fail completeness constraint (4) all the 
measures in pair counting category fail cluster 
size vs. quantity constraint; (5) CEAF, unfortu-
nately, fails homogeneity, completeness, rag bag 
constraints. 
Category Evaluation Measures 
set mapping  purity, inverse purity, F-measure 
pair counting rand index, Jaccard Coefficient, 
Folks and Mallows FM 
entropy entropy, mutual information, VI, 
V 
editing  
distance 
editing distance 
coreference 
resolution 
MUC (Vilain et al,1995),  
B-Cubed (Bagga and Baldwin, 
1998), CEAF (Luo, 2005) 
Table 3. Summary of Evaluation Measures 
3 Applying Graph Clustering to NLP 
A variety of structures in NLP can be naturally 
represented as graphs, e.g., co-occurrence graphs, 
coreference graphs, word/sentence/ document 
graphs. In recent years, there have been an in-
creasing amount of interests in applying graph-
based clustering to some NLP problems, e.g., 
document clustering (Zhong and Ghosh, 2004), 
summarization (Zha, 2002), coreference resolu-
tion (Nicolae and Nicolae, 2006), word sense 
disambiguation (Dorow and Widdows, 2003; 
V?ronis, 2004; Agirre et al, 2007), word cluster-
ing (Matsuo et al, 2006; Biemann, 2006). Many 
authors chose one or two their favorite graph 
clustering algorithms and claimed the effective-
ness by comparing with supervised algorithms 
(which need expensive annotations) or other non-
5
graph clustering algorithms. As far as we know, 
there is not much work on the comparative study 
of various graph-based clustering algorithms for 
certain NLP problems. As mentioned at the end 
of section 2.5, there is not a graph clustering al-
gorithm that is effective for all applications. 
However, it is interesting to find out, for a spe-
cific NLP problem, if graph clustering methods 
can be applied, (1) how the parameters in the 
graph model affects the performance? (2) Does 
the NLP problem favor some quality measure 
and some graph clustering algorithm rather than 
the others? Unfortunately, this survey neither 
provides answers for these questions; instead, we 
overview a few NLP case studies in which some 
graph-based clustering methods have been suc-
cessfully applied. 
3.1 Coreference Resolution 
Coreference resolution is typically defined as the 
problem of partitioning a set of mentions into 
entities. An entity is an object or a set of objects 
in the real world such as person, organization, 
facility, while a mention is a textual reference to 
an entity. The approaches to solving coreference 
resolution have shifted from earlier linguistics-
based (rely on domain knowledge and hand-
crafted rules) to machine-learning based ap-
proaches. Elango (2005) and Chen (2010) pre-
sented a comprehensive survey on this topic. One 
of the most prevalent approaches for coreference 
resolution is to follow a two-step procedure: (1) a 
classification step that computes how likely one 
mention corefers with the other and (2) a 
clustering step that groups the mentions into 
clusters such that all mentions in a cluster refer 
to the same entity. In the past years, NLP 
researchers have explored and enriched this 
methodogy from various directions (either in 
classification or clustering step). Unfortunately, 
most of the proposed clustering algorithms, e.g., 
closest-first clustering (Soon et al, 2001), best-
first clustering (Ng and Cardie, 2002), suffer 
from a drawback: an instant decision is made (in 
greedy style) when considering two mentions are 
coreferent or not, therefore, the algorithm makes 
no attempt to search through the space of all 
possible clusterings, which results in a sub-
optimal clustering (Luo et al, 2004). Various 
approaches have been proposed to alleviate this 
problem, of which graph clustering methodology 
is one of the most promising solutions.  
The problem of coreference resolution can be 
modeled as a graph such that the vertex 
represents a mention, and the edge weight carries 
the coreference likelihood between two mentions. 
Nicolae and Nicolae (2006) proposed a new 
quality measure named BESTCUT which is to 
optimize the sum of ?correctly? placed vertices 
in the graph. The BESTCUT algorithm works by 
performing recursive bisection (similar to Ker-
nighan-Lin algorithm) and in each iteration, it 
searches the best cut that leads to partition into 
halves. They compared BESTCUT algorithm 
with (Luo et al, 2004)?s Belltree and (Ng and 
Cardie, 2002)?s Link-Best algorithm and showed 
that using ground-truth entities, BESTCUT out-
performs the other two with statistical signific-
ance (4.8% improvement over Belltree and Link-
Best algorithm in ECM F-measure). Nevertheless, 
we believe that the BESTCUT algorithm is not 
the only choice and the running complexity of 
BESTCUT,??(|??||??| + |??|2??????|??|), is not com-
petitive, thus could be improved by other graph 
clustering algorithms. 
Chen and Ji (2009a) applied normalized spec-
tral algorithm to conduct event coreference reso-
lution: partitioning a set of mentions into events. 
An event is a specific occurrence involving par-
ticipants. An event mention is a textual reference 
to an event which includes a distinguished trig-
ger (the word that most clearly expresses an 
event occurs) and involving arguments (enti-
ties/temporal expressions that play certain roles 
in the event).  A graph is similarly constructed as 
in entity coreference resolution except that it in-
volves quite different feature engineering (most 
features are related with event trigger and argu-
ments). The graph clustering approach yields 
competitive results by comparing with an agglo-
merative clustering algorithm proposed in (Chen 
et al, 2009b), unfortunately, a scientific compar-
ison among the algorithms remains unexplored. 
3.2 Word Clustering 
Word clustering is a problem defined as cluster-
ing a set of words (e.g., nouns, verbs) into groups 
so that similar words are in the same cluster.  
Word clustering is a major technique that can 
benefit many NLP tasks, e.g., thesaurus construc-
tion, text classification, and word sense disam-
biguation. Word clustering can be solved by fol-
lowing a two-step procedure: (1) classification 
step by representing each word as a feature vec-
tor and computing the similarity of two words; (2) 
clustering step which applies some clustering 
algorithm, e.g., single-link clustering, complete-
link clustering, average-link clustering, such that 
similar words are grouped together.  
Matsuo et al (2006) presented a graph cluster-
6
ing algorithm for word clustering based on word 
similarity measures by web counts. A word co-
occurrence graph is constructed in which the ver-
tex represents a word, and the edge weight is 
computed by applying some similarity measure 
(e.g., PMI, ?2) on a co-occurrence matrix, which 
is the result of querying a pair of words to a 
search engine. Then an agglomerative graph 
clustering algorithm (Newman, 2004), which is 
surveyed in section 2.5, is applied. They showed 
that the similarity measure  ?2  performs better 
than PMI, for one reason, PMI performs worse 
when a word group contains rare or frequent 
words, for the other reason, PMI is sensitive to 
web output inconsistency, e.g., the web count of 
??1 is below the web count of ??1????????2 in ex-
treme case. They also showed that their graph 
clustering algorithm outperforms average-link 
agglomerative clustering by almost 32% using ?2 
similarity measure. The concern of their ap-
proach is the running complexity for constructing 
co-occurrence matrix, i.e., for ??  words, ??(??2) 
queries are required which is intractable for a 
large graph.  
Ichioka and Fukumoto (2008) applied similar 
approach as Matsuo et al (2006) for Japanese 
Onomatopoetic word clustering, and showed that 
the approach outperforms ??-means clustering by 
16.2%. 
3.3 Word Sense Disambiguation (WSD) 
Word sense disambiguation is the problem of 
identifying which sense of a word (meaning) is 
conveyed in the context of a sentence, when the 
word is polysemic. In contrast to supervised 
WSD which relies on pre-defined list of senses 
from dictionaries, unsupervised WSD induces 
word senses directly from the corpus. Among 
those unsupervised WSD algorithms, graph-
based clustering algorithms have been found 
competitive with supervised methods, and in 
many cases outperform most vector-based clus-
tering methods. 
Dorow and Widdows (2003) built a co-
occurrence graph in which each node represents 
a noun and two nodes have an edge between 
them if they co-occur more than a given thre-
shold. They then applied Markov Clustering al-
gorithm (MCL) which is surveyed in section 2.5, 
but cleverly circumvent the problem of choosing 
the right parameters. Their algorithm not only 
recognizes senses of polysemic words, but also 
provides high-level readable cluster name for 
each sense. Unfortunately, they neither discussed 
further how to identify the sense of a word in a 
given context, nor compared their algorithm with 
other algorithms by conducting experiments. 
V?ronis (2004) proposed a graph based model 
named HyperLex based on the small-world prop-
erties of co-occurrence graphs. Detecting the dif-
ferent senses (uses) of a word reduces to isolat-
ing the high-density components (hubs) in the 
co-occurrence graph. Those hubs are then used to 
perform WSD. To obtain the hubs, HyperLex 
finds the vertex with highest relative frequency 
in the graph at each iteration and if it meets some 
criteria, it is selected as a hub. Agirre (2007) 
proposed another method based on PageRank for 
finding hubs. HyperLex can detect low-frequency 
senses (as low as 1%) and most importantly, it 
offers an excellent precision (97% compared to 
73% for baseline). Agirre (2007) further con-
ducted extensive experiments by comparing the 
two graph based models (HyperLex and Page-
Rank) with other supervised and non-supervised 
graph methods and concluded that graph based 
methods perform close to supervised systems in 
the lexical sample task and yield the second-best 
WSD systems for the Senseval-3 all-words task. 
4 Conclusions 
In this survey, we organize the sparse related 
literature of graph clustering into a structured 
presentation and summarize the topic as a five 
part story, namely, hypothesis, modeling, meas-
ure, algorithm, and evaluation. The hypothesis 
serves as a basis for the whole graph clustering 
methodology, quality measures and graph clus-
tering algorithms construct the backbone of the 
methodology, modeling acts as the interface be-
tween the real application and the methodology, 
and evaluation deals with utility. We also survey 
several typical NLP problems, in which graph-
based clustering approaches have been success-
fully applied. 
We have the following final comments on the 
strengths and weaknesses of graph clustering 
approaches:  
(1) Graph is an elegant data structure that can 
model many real applications with solid ma-
thematical foundations including spectral 
theory, Markov stochastic process.  
(2) Unlike many other clustering algorithms 
which act greedily towards the final clustering 
and thus may miss the optimal clustering, 
graph clustering transforms the clustering 
problem into optimizing some quality meas-
ure. Unfortunately, those optimization prob-
lems are NP-Hard, thus, all proposed graph 
7
clustering algorithms only approximately 
yield ?optimal? clustering.  
(3) Graph clustering algorithms have been criti-
cized for low speed when working on large 
scale graph (with millions of vertices). This 
may not be true since new graph clustering 
algorithms have been proposed, e.g., the mul-
tilevel graph clustering algorithm (Karypis 
and Kumar, 1999) can partition a graph with 
one million vertices into 256 clusters in a few 
seconds on current generation workstations 
and PCs. Nevertheless, scalability problem of 
graph clustering algorithm still needs to be 
explored which is becoming more important 
in social network study.  
We envision that graph clustering methods can 
lead to promising solutions in the following 
emerging NLP problems:  
(1) Detection of new entity types, relation types 
and event types (IE area). For example, the 
eight event types defined in the ACE 1
(2) Web people search (IR area). The main issue 
in web people search is the ambiguity of the 
person name. Thus by extracting attributes 
(e.g., attended schools, spouse, children, 
friends) from returned web pages, construct-
ing person graphs (involving those attributes) 
and applying graph clustering, we are opti-
mistic to achieve a better person search en-
gine. 
 pro-
gram may not be enough for wider usage and 
more event types can be induced by graph 
clustering on verbs. 
 
 
Acknowledgments 
This work was supported by the U.S. National 
Science Foundation Faculty Early Career Devel-
opment (CAREER) Award under Grant IIS-
0953149, the U.S. Army Research Laboratory 
under Cooperative Agreement Number 
W911NF-09-2-0053, Google, Inc., CUNY Re-
search Enhancement Program, Faculty Publica-
tion Program and GRTI Program. The views and 
conclusions contained in this document are those 
of the authors and should not be interpreted as 
representing the official policies, either ex-
pressed or implied, of the Army Research Labor-
atory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute 
reprints for Government purposes notwithstand-
ing any copyright notation here on. 
                                                 
1 http://www.nist.gov/speech/tests/ace/ 
References 
A. Bagga and B. Baldwin.1998. Algorithms for scor-
ing coreference chains. Proc. The First Interna-
tional Conference on Language Resources and 
Evaluation Workshop on Linguistics Coreference. 
A. Clauset. 2005. Finding local community structure 
in networks. Physical Review E, 72:026132. 
B. Dom. 2001. An information-theoretic external 
cluster-validity measure. IBM Research Report. 
B. Dorow, D. Widdows. 2003. Discovering corpus-
specific word-senses. In Proc. EACL. 
B. W. Kernighan and S. Lin.1970. An efficient heuris-
tic procedur for partitioning graphs. Bell Syst. 
Techn. J.,Vol. 49, No. 2, pp. 291?307. 
C. Biemann. 2006. Chinese Whispers - an Efficient-
Graph Clustering Algorithm and its Application to 
Natural Language Processing Problems. In Proc. of 
the HLT-NAACL-06 Workshop on Textgraphs-06. 
C. Nicolae and G. Nicolae. 2006. Bestcut: A graph 
algorithm for coreference resolution. In EMNLP, 
pages 275?283, Sydney, Australia. 
E. Agirre, D. Martinez, O.L. de Lacalle and A.Soroa. 
2007. Two graph-based algorithms for state-of-the-
art WSD. In Proc. EMNLP. 
E. Amigo,  J. Gonzalo,  J. Artiles  and F. Verdejo. 
2008. A comparison of extrinsic clustering evalua-
tion metrics based on formal constraints. Informa-
tion Retrieval. 
E. Terra and C. L. A. Clarke. Frequency Estimates for 
Statistical Word Similarity Measures. In Proc. 
HLT/NAACL 2003. 
G. Karypis and V. Kumar. 1999. Multilevel algo-
rithms for multiconstraint graph partitioning. in 
Proceedings of the 36th ACM/IEEE conference on 
Design automation conference, (New Orleans, 
Louisiana), pp. 343 ? 348. 
G. W. Flake, R. E. Tarjan and K. Tsioutsiouliklis. 
2003. Graph clustering and minimum cut trees. In-
ternet Mathematics, 1(4):385?408. 
H. Zha.2002. Generic summarization and keyphrase 
extraction using mutual reinforcement principle 
and sentence clustering. In Proc. of SIGIR2002, pp. 
113-120. 
J. Chen, O. R. Za?ane, R. Goebel. 2009. Detecting 
Communities in Social Networks Using Max-Min 
Modularity. SDM 2009: 978-989. 
J. Ruan and W. Zhang.2008. Identifying network 
communities with a high resolution. Physical Re-
view E, 77:016104. 
J. Shi and J. Malik. 2000. Normalized Cuts and Image 
Segmentation. IEEE Trans. Pattern Analysis and 
Machine Intelligence, vol. 22, no. 8, pp. 888-905. 
8
J. V?ronis. 2004. HyperLex: Lexical Cartography for 
Information Retrieval. Computer Speech & Lan-
guage 18(3). 
K. Ichioka  and F. Fukumoto. 2008. Graph-based 
clustering for semantic classification of onomato-
poetic words. In Proc. of the 3rd Textgraphs Work-
shop on Graph-based Algorithms for Natural Lan-
guage Processing. 
L. Hagen and A. B. Kahng. 1992. New spectral me-
thods for ratio cut partitioning and clustering. IEEE 
Transactions Computer-Aided Design, Santa Clara 
CA, 422-427. 
L. R. Ford, D. R. Fulkerson. 1956. Maximal flow 
through a network. Canadian Journal of Mathe-
matics 8: 399?404. 
M. E. J. Newman. 2004. Detecting community struc-
ture in networks. Eur. Phys. J. B, 38, 321?330. 
M. E. J. Newman. 2004. Fast algorithm for detecting 
community structure in networks. Phys Rev E. 69, 
2004.  
M. E. J. Newman and M. Girvan. 2004. Finding and 
evaluating community structure in networks. Phys. 
Rev. E 69,026113. 
M.  Girvan and M. E. J. Newman. 2002. Community 
structure in social and biological networks. Proc. 
Natl. Acad. Sci. USA 99, 7821-7826. 
M. Meila. 2003. Comparing clusterings. In Proceed-
ings of COLT03. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly  and L. 
Hirschman.1995. A model-theoretic coreference 
scoring scheme. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6). 
P. Elango. 2005. Coreference Resolution: A Survey. 
Technical Report, University of Wisconsin Madi-
son. 
R. Kannan, S. Vempala, and A. Vetta. 2000. On clus-
terings:good, bad and spectral. In Proceedings of 
the 41st Annual Symposium on Foundations of 
Computer Science. 
S. Abney. 2007. Semi-supervised Learning for Com-
putational Linguistics, Chapman and Hall. 
S. E. Schaeffer. 2007. Graph clustering. Computer 
Science Review, 1(1):27?64. 
S. Fortunato, V. Latora, and M. Marchiori. 2004. A 
Method to Find Community Structures Based on 
Information Centrality. Phys Rev E.70, 056104. 
S. van Dongen. 2000. Graph Clustering by Flow Si-
mulation. PhD thesis, University of Utrecht. 
S. White and P. Smyth. 2005. A spectral clustering 
approach to finding communities in graphs. In 
SIAM International Conference on Data Mining. 
U. Brandes, M. Gaertler, and D. Wagner. 2003. Expe-
riments on graph clustering algorithms. Proc. 11th 
European Symp. Algorithms, LNCS 2832:568-579. 
U. Brandes, M. Gaertler, and D.Wagner. 2007. Engi-
neering graph clustering: Models and experimental 
evaluation. J. Exp. Algorithmics, 12:1.1. 
U. Luxburg, O. Bousquet, M. Belkin. 2005. Limits of 
spectral clustering. In L. K. Saul, Y. Weiss and L. 
Bottou (Eds.), Advances in neural information 
processing systems 17. Cambridge, MA: MIT Press. 
U. Luxburg.2006. A tutorial on spectral clustering. 
Technical Report 149, Max Plank Institute for Bio-
logical Cybernetics. 
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc. 
Of the ACL, pages 104?111. 
W. M. Soon, H. T. Ng and D. Lim.2001. A machine 
learning approach to coreference resolution of 
noun phrases. Computational Linguistics, 
27(4):521?544. 
X. Luo. 2005. On coreference resolution performance 
metrics. Proc. of HLT-EMNLP. 
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla and S. 
Roukos. 2004. A mention-synchronous coreference 
resolution algorithm based on the Bell Tree. In 
Proc. of ACL-04, pp.136?143. 
Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka. 
2006. Graph-based word clustering using web 
search engine. In Proc. of EMNLP 2006. 
Z. Chen and H. Ji. 2009a. Graph-based Event Corefe-
rence Resolution. In Proc. ACL-IJCNLP 2009 
workshop on TextGraphs-4: Graph-based Methods 
for Natural Language Processing. 
Z. Chen, H. Ji, R. Haralick. 2009b. A Pairwise Core-
ference Model, Feature Impact and Evaluation for 
Event Coreference Resolution. In Proc. RANLP 
2009 workshop on Events in Emerging Text Types. 
Z. Chen. 2010. Graph-based Clustering and its Appli-
cation in Coreference Resolution. Technical Report, 
the Graduate Center, the City University of New 
York. 
 
9
Cross-lingual Slot Filling from Comparable Corpora
Matthew Snover, Xiang Li, Wen-Pin Lin, Zheng Chen, Suzanne Tamang,
Mingmin Ge, Adam Lee, Qi Li, Hao Li, Sam Anzaroot, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
This paper introduces a new task of
crosslingual slot filling which aims to dis-
cover attributes for entity queries from
crosslingual comparable corpora and then
present answers in a desired language. It is
a very challenging task which suffers from
both information extraction and machine
translation errors. In this paper we ana-
lyze the types of errors produced by five
different baseline approaches, and present
a novel supervised rescoring based valida-
tion approach to incorporate global evi-
dence from very large bilingual compara-
ble corpora. Without using any additional
labeled data this new approach obtained
38.5% relative improvement in Precision
and 86.7% relative improvement in Recall
over several state-of-the-art approaches.
The ultimate system outperformed mono-
lingual slot filling pipelines built on much
larger monolingual corpora.
1 Introduction
The slot filling task at NIST TAC Knowledge
Base Population (KBP) track (Ji et al, 2010)
is a relatively new and popular task with the
goal of automatically building profiles of enti-
ties from large amounts of unstructured data,
and using these profiles to populate an existing
knowledge base. These profiles consist of nu-
merous slots such as ?title?, ?parents? for per-
sons and ?top-employees? for organizations. A
variety of approaches have been proposed to ad-
dress both tasks with considerable success; nev-
ertheless, all of the KBP tasks so far have been
limited to monolingual processing. However, as
the shrinking fraction of the world?s Web pages
are written in English, many slot fills can only
be discovered from comparable documents in
foreign languages. By comparable corpora we
mean texts that are about similar topics, but
are not in general translations of each other.
These corpora are naturally available, for ex-
ample, many news agencies release multi-lingual
news articles on the same day. In this paper we
propose a new and more challenging crosslin-
gual slot filling task, to find information for any
English query from crosslingual comparable cor-
pora, and then present its profile in English.
We developed complementary baseline ap-
proaches which combine two difficult problems:
information extraction (IE) and machine trans-
lation (MT). In this paper we conduct detailed
error analysis to understand how we can exploit
comparable corpora to construct more complete
and accurate profiles.
Many correct answers extracted from our
baselines will be reported multiple times in any
external large collection of comparable docu-
ments. We can thus take advantage of such in-
formation redundancy to rescore candidate an-
swers. To choose the best answers we consult
large comparable corpora and corresponding IE
results. We prefer those answers which fre-
quently appear together with the query in cer-
tain IE contexts, including co-occurring names,
coreference links, relations and events. For ex-
ample, we prefer ?South Korea? instead of ?New
York Stock Exchange? as the ?per:employee of ?
answer for ?Roh Moo-hyun? using global ev-
idence from employment relation extraction.
Such global knowledge from comparable corpora
110
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 110?119,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
provides substantial improvement over each in-
dividual baseline system and even state-of-the-
art monolingual slot filling systems. Compared
to previous methods of exploiting comparable
corpora, our approach is novel in multiple as-
pects because it exploits knowledge from: (1)
both local and global statistics; (2) both lan-
guages; and (3) both shallow and deep analysis.
2 Related Work
Sudo et al (2004) found that for a crosslin-
gual single-document IE task, source language
extraction and fact translation performed no-
tably better than machine translation and tar-
get language extraction. We observed the same
results. In addition we also demonstrate that
these two approaches are complementary and
can be used to boost each other?s results in a
statistical rescoring model with global evidence
from large comparable corpora.
Hakkani-Tur et al (2007) described a filtering
mechanism using two crosslingual IE systems
for improving crosslingual document retrieval.
Many previous validation methods for crosslin-
gual QA, such as those organized by Cross Lan-
guage Evaluation Forum (Vallin et al, 2005), fo-
cused on local information which involves only
the query and answer (e.g. (Kwork and Deng,
2006)), keyword translation (e.g. (Mitamura et
al., 2006)) and surface patterns (e.g. (Soubbotin
and Soubbotin, 2001)). Some global valida-
tion approaches considered information redun-
dancy based on shallow statistics including co-
occurrence, density score and mutual informa-
tion (Clarke et al, 2001; Magnini et al, 2001;
Lee et al, 2008), deeper knowledge from depen-
dency parsing (e.g. (Shen et al, 2006)) or logic
reasoning (e.g. (Harabagiu et al, 2005)). How-
ever, all of these approaches made limited efforts
at disambiguating entities in queries and limited
use of fact extraction in answer search and vali-
dation.
Several recent IE studies have stressed the
benefits of using information redundancy on
estimating the correctness of the IE out-
put (Downey et al, 2005; Yangarber, 2006;
Patwardhan and Riloff, 2009; Ji and Grish-
man, 2008). Some recent research used com-
parable corpora to re-score name translitera-
tions (Sproat et al, 2006; Klementiev and Roth,
2006) or mine new word translations (Fung and
Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao
and Zhai, 2005; Hassan et al, 2007; Udupa et
al., 2009; Ji, 2009). To the best of our knowl-
edge, this is the first work on mining facts from
comparable corpora for answer validation in a
new crosslingual entity profiling task.
3 Experimental Setup
3.1 Task Definition
The goal of the KBP slot filling task is to extract
facts from a large source corpus regarding cer-
tain attributes (?slots?) of an entity, which may
be a person or organization, and use these facts
to augment an existing knowledge base (KB).
Along with each slot answer, the system must
provide the ID of a document which supports
the correctness of this answer. KBP 2010 (Ji et
al., 2010) defines 26 types of attributes for per-
sons (such as the age, birthplace, spouse, chil-
dren, job title, and employing organization) and
16 types of attributes for organizations (such
as the top employees, the founder, the year
founded, the headquarters location, and the sub-
sidiaries).
The new problem we define in this paper is an
extension of this task to a crosslingual paradigm.
Given a query in a target language t and a col-
lection of documents in a source language s,
a system must extract slot answers about the
query and present the answers in t. In this pa-
per we examine a specific setting of s=Chinese
and t=English.
To score crosslingual slot filling, we pool all
the system responses and group equivalent an-
swers into equivalence classes. Each system re-
sponse is rated as correct, wrong, inexact or re-
dundant. Given these judgments, we calculate
the precision, recall and F-measure of each sys-
tem, crediting only correct answers.
3.2 Data and Query Selection
We use the comparable corpora of English
TDT5 (278,358 documents) and Chinese TDT5
111
(56,424 documents) as our source collection.
For query selection, we collected all the en-
tities from the entire source collection and
counted their frequencies. We then selected 50
informative entities (25 persons and 25 organiza-
tions) which were located in the middle range of
frequency counts. Among the 25 person queries,
half are Chinese-specific names, and half are
non-Chinese names. The 25 organizations fol-
low a representative distribution according to
the entity subtypes defined in NIST Automatic
Content Extraction (ACE) program1.
3.3 Baseline Pipelines
3.3.1 Overview
We employ the following two types of base-
line crosslingual slot filling pipelines to process
Chinese documents. Figure 1 and Table 1 shows
the five system pipelines we have used to con-
duct our experiments.
Type A Translate Chinese texts into English,
and apply English slot filling systems to the
translations.
Type B Translate English queries into Chinese,
apply Chinese slot filling systems to Chinese
texts, and translate answers back to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Machine 
Translation 
English 
Texts 
Chinese 
Texts 
English Candidate Answers 
English
Query
English Slot Filling 
 Answer 
Translation  Pattern Matching 
 Supervised
Classification
Chinese Slot Filling 
 Supervised 
Classification 
Chinese 
Query 
 Query 
Translation
Figure 1: Overview of Baseline Crosslingual Slot Fill-
ing Pipelines
1http://www.itl.nist.gov/iad/mig/tests/ace/
 
Pipeline Label Components Data 
(1) English Supervised Classification Mono-
lingual (2) English Pattern Matching 
English 
TDT5 
 
(3) 
MT+English 
Supervised 
Classification Type A  
(4) 
MT+English 
Pattern Matching Cross-lingual 
Type 
B 
 
 
(5) 
Query Translation 
+Chinese Supervised 
Classification 
+Answer Translation 
Chinese 
TDT5 
 
 
 
Table 1: Monolingual and Crosslingual Baseline Slot
Filling Pipelines
3.3.2 Monolingual Slot Filling
We applied a state-of-the-art bilingual slot
filling system (Chen et al, 2010) to process
bilingual comparable corpora. This baseline
system includes a supervised ACE IE pipeline
and a bottom-up pattern matching pipeline.
The IE pipeline includes relation extraction and
event extraction based on maximum entropy
models that incorporate diverse lexical, syntac-
tic, semantic and ontological knowledge. The
extracted ACE relations and events are then
mapped to KBP slot fills. In pattern matching,
we extract and rank patterns based on a dis-
tant supervision approach (Mintz et al, 2009)
that uses entity-attribute pairs from Wikipedia
Infoboxes and Freebase (Bollacker et al, 2008).
We set a low threshold to include more answer
candidates, and then a series of filtering steps
to refine and improve the overall pipeline re-
sults. The filtering steps include removing an-
swers which have inappropriate entity types or
have inappropriate dependency paths to the en-
tities.
3.3.3 Document and Name Translation
We use a statistical, phrase-based MT sys-
tem (Zens and Ney, 2004) to translate Chinese
documents into English for Type A Approaches.
The best translation is computed by using a
weighted log-linear combination of various sta-
tistical models: an n-gram language model, a
phrase translation model and a word-based lex-
112
icon model. The latter two models are used in
source-to-target and target-to-source directions.
The model scaling factors are optimized with re-
spect to the BLEU score similar to (Och, 2003).
The training data includes 200 million running
words in each language. The total language
model training data consists of about 600 mil-
lion running words.
We applied various name mining approaches
from comparable corpora and parallel corpora,
as described in (Ji et al, 2009) to extract and
translate names in queries and answers in Type
B approaches. The accuracy of name translation
is about 88%. For those names not covered by
these pairs, we relied on Google Translate 2 to
obtain results.
4 Analysis of Baseline Pipelines
In this section we analyze the coverage (Sec-
tion 4.1) and precision (Section 4.2) results of
the baseline pipelines. We then illustrate the
potential for global validation from comparable
corpora through a series of examples.
4.1 Coverage Analysis: Toward
Information Fusion
Table 2 summarizes the Precision (P), Recall
(R) and F-measure (F) of baseline pipelines and
the union of their individual results.
Table 2: Baseline Pipeline Results 
System P R F 
(1) 0.08 0.54 0.15 
(2) 0.02 0.35 0.03 Mono- 
lingual Union of 
(1)+(2) 
0.03 0.69 0.05 
(3) 0.04 0.04 0.04 
(4) 0.03 0.25 0.05 
Union of 
(3)+(4) 0.03 0.26 0.05 
(5) 0.04 0.46 0.08 
Cross- 
lingual 
Union of 
(3)+(4)+(5) 0.03 0.56 0.05 
Compara
ble 
Corpora 
Union of 
(1)+(2)+(3)+
(4)+(5) 
0.02 1 0.04 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2http://translate.google.com/
Although crosslingual pipelines used a much
smaller corpus than monolingual pipelines, they
extracted comparable number of correct answers
(66 vs. 81) with a slightly better precision.
In fact, the crosslingual pipeline (5) performs
even better than monolingual pipeline (2), es-
pecially on the employment slots. In particu-
lar, 96.35% of the correct answers for Chinese-
specific person queries (e.g. ?Tang Jiaxuan?)
were extracted from Chinese data. Even for
those facts discovered from English data, they
are about quite general slots such as ?title? and
?employee of ?. In contrast, Chinese data covers
more diverse biographical slots such as ?family
members? and ?schools attended?.
Compared to the union of Type A approaches
(pipelines (3)+(4)), Pipeline (5) returned many
more correct answers with higher precision. The
main reason is that Type A approaches suffer
from MT errors. For example, MT mistakenly
translated the query name ?Celine Dion? into
?Clinton? and thus English slot filling compo-
nents failed to identify any answers. One can
hypothesize that slot filling on MT output can
be improved by re-training extraction compo-
nents directly from MT output. However, our
experiments of learning patterns from MT out-
put showed negative impact, mainly because
MT errors were too diverse to generalize. In
other cases even though slot filling produced cor-
rect results, MT still failed to translate the an-
swer names correctly. For example, English slot
filling successfully found a potential answer for
?org:founded by? of the query ?Microsoft? from
the following MT output: ?The third largest of
the Microsoft common founder Alan Doss , aged
50, and net assets of US 22 billion.?; however,
the answer string ?Paul Allen? was mistakenly
translated into ?Alan Doss?. MT is not so cru-
cial for ?per:title? slot because it does not require
translation of contexts.
To summarize, 59% of the missing errors were
due to text, query or answer translation errors
and 20% were due to slot filling errors. Never-
theless, the union of (3)+(4)+(5) still contain
more correct answers. These baseline pipelines
were developed from a diverse set of algorithms,
and typically showed strengths in specific slots.
113
In general we can conclude that monolin-
gual and crosslingual pipelines are complemen-
tary. Combining the responses from all baseline
pipelines, we can get similar number of correct
answers compared to one single human annota-
tor.
4.2 Precision Analysis: Toward Global
Validation
The spurious errors from baseline crosslingual
slot filling pipelines reveal both the shortcom-
ings of the MT system and extraction across
languages. Table 3 shows the distribution of
spurious errors.
Pipeline Spurious Errors Distribution
Content Translation 
+ Extraction 
85% 
Query Translation 13% 
Type A 
Answer Translation 2% 
Word Segmentation 34% 
Relation Extraction 33% 
Coreference 17% 
Semantic Type 13% 
Type B 
Slot Type 3% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3: Distribution of Spurious Errors
Table 3 indicates a majority (85%) of spurious
errors from Type A pipelines were due to ap-
plying monolingual slot filling methods to MT
output which preserves Chinese structure.
As demonstrated in previous work (e.g. (Par-
ton and McKeown, 2010; Ji et al, 2009)),
we also found that many (14.6%) errors were
caused by the low quality of name translation
for queries and answers.
For example, ?????/McGinty? was mis-
takenly translated into the query name ?Kim
Jong-il?, which led to many incorrect answers
such as ?The British Royal joint military re-
search institute? for ?per:employee of ?.
In contrast, the spurious errors from Type B
pipelines were more diverse. Chinese IE com-
ponents severely suffered from word segmen-
tation errors (34%), which were then directly
propagated into Chinese document retrieval and
slot filling. Many segmentation errors occurred
with out-of-vocabulary names, especially per-
son names and nested organization names. For
example, the name ????/Yao Mingbao? was
mistakenly segmented into two words ???/Yao
Ming? and ??/bao?, and thus the document was
mistakenly retrieved for the query ?Yao Ming?.
In many cases (33%) Chinese relation and
event extraction components failed to cap-
ture Chinese-specific structures due to the lim-
ited size of training corpora. For example,
from the context ???????????????
?/Xiao Wan-chang, who were invited to be-
come the economics consultant for Chen Shui-
bian?, Chinese slot filling system mistakenly ex-
tracted ?consultant? as a ?per:title? answer for
the query ?Chen Shui-bian? using a common
pattern ?<query><title>?.
13% of errors were caused due to invalid se-
mantic types for certain slots. For example,
many metaphoric titles such as ?tough guy?
don?t match the definition of ?per:title? in the
annotation guideline ?employment or member-
ship position?.
5 Global Validation
Based on the above motivations we propose to
incorporate global evidence from a very large
collection of comparable documents to refine
local decisions. The central idea is to over-
generate candidate answers from multiple weak
baselines to ensure high upper-bound of recall,
and then conduct effective global validation to
filter spurious errors while keeping good answers
in order to enhance precision.
5.1 Supervised Rescoring
Ideally, we want to choose a validation model
which can pick out important features in a con-
text wider than that used by baseline pipelines.
Merging individual systems to form the union of
answers can be effective, but Table 2 shows that
simple union of all pipelines produced worse F-
measure than the best pipeline.
In this paper we exploit the reranking
paradigm, commonly used in information re-
trieval, to conduct global validation. By model-
ing the empirical distribution of labeled training
data, statistical models are used to identify the
114
strengths and weaknesses (e.g. high and low pre-
cision slots) of individual systems, and rescore
answers accordingly. Specially, we develop a
supervised Maximum Entropy (MaxEnt) based
model to rescore the answers from the pipelines,
selecting only the highest-scoring answers.
The rescorer was trained (using cross-
validation) on varying subsets of the features.
The threshold at which an answer is deemed to
be true is chosen to maximize the F-Measure on
the training set.
5.2 Validation Features
Table 4 describes the validation features used for
rescoring, where q is the query, q? the Chinese
translation of q, t the slot type, a the candidate
answer, a? the Chinese form of a, s the context
sentence and d is the context document support-
ing a.
The feature set benefits from multiple dimen-
sions of crosslingual slot filling. These features
were applied to both languages wherever anno-
tation resources were available.
In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the ?coherence?
of the story (i.e. consistency among all gener-
ated answers - query profiles). We use feature
f2 to check whether the same answer was gen-
erated for conflicting slots, such as per:parents
and per:children.
Compared to traditional QA tasks, slot fill-
ing is a more fine-grained task in which differ-
ent slots are expected to obtain semantically
different answers. Therefore, we explored se-
mantic constraints in both local and global con-
texts. For example, we utilized bilingual name
gazetteers from ACE training corpora, Google
n-grams (Ji and Lin, 2009) and the geonames
website 3 to encode features f6, f8 and f9; The
org:top members/employees slot requires a sys-
tem to distinguish whether a person member/
employee is in the top position, thus we encoded
f10 for this purpose.
The knowledge used in our baseline pipelines
is relatively static ? it is not updated during the
3http://www.geonames.org/statistics/
extraction process. Achieving high performance
for cross-lingual slot filling requires that we take
a broader view, one that looks outside a sin-
gle document or a single language in order to
exploit global knowledge. Fortunately, as more
and more large crosslingual comparable corpora
are available, we can take advantage of informa-
tion redundancy to validate answers. The basic
intuition is that if a candidate answer a is cor-
rect, it should appear together with the query
q repeatedly, in different documents, or even in
certain coreference links, relations and events.
For example, ?David Kelly - scientist?, and
??????/Shintaro Ishihara - ??/governor?
pairs appear frequently in ?title? coreference
links in both English and Chinese corpora;
?Elizabeth II? is very often involved in an ?em-
ployment? relation with ?United Kingdom? in
English corpora. On the other hand, some in-
correct answers with high global statistics can be
filtered out using these constraints. For exam-
ple, although the query ????/Tang Jiaxuan?
appears frequently together with the candidate
per:title answer ???/personnel?, it is linked by
few coreference links; in contrast, it?s coreferen-
tial with the correct title answer ?????/State
Council member? much more frequently.
We processed cross-lingual comparable cor-
pora to extract coreference links, relations and
events among mentions (names, nominals and
time expressions etc.) and stored them in an
external knowledge base. Any pair of <q, a>
is then compared to the entries in this knowl-
edge base. We used 157,708 documents from
Chinese TDT5 and Gigaword to count Chinese
global statistics, and 7,148,446 documents from
DARPA GALE MT training corpora to count
English global statistics, as shown in features
f12 and f13. Fact based global features f14, f15,
f16 and f17, were calculated from 49,359 Chi-
nese and 280,513 English documents (annotated
by the bilingual IE system in Section 3.3.2.
6 Experiments
In this section, we examine the overall perfor-
mance of this method. We then discuss the
usefulness of the individual sets of features. In
115
Characteristics 
Scope Depth Language 
Description 
f1: frequency of <q, a, t> that appears in all baseline outputs Global 
(Cross-
system) 
Shallow 
 English f2: number of conflicting slot types in which answer a appears in all baseline 
outputs 
f3: conjunction of t and whether a is a year answer Shallow English 
f4: conjunction of t and whether a includes numbers or letters 
f5: conjunction of place t and whether a is a country name 
f6: conjunction of per:origin t and whether a is a nationality 
f7: if t=per:title, whether a is an acceptable title 
f8: if t requires a name answer, whether a is a name 
Local 
Deep 
 
English 
 
f9: whether a has appropriate semantic type 
f10: conjunction of org:top_members/employees and whether there is a high-level 
title in s 
Global 
(Within-
Document) 
Deep English 
f11: conjunction of alternative name and whether a is an acronym of q 
Chinese f12: conditional probability of q/q' and a/a' appear in the same document Shallow 
(Statistics) English f13: conditional probability  of q/q' and a/a' appear in the same sentence 
Both f14:  co-occurrence of q/q' and a/a'  appear in coreference links 
English f15: co-occurrence of q/q' and a/a'  appear in relation/event links 
English f16: conditional probability of q/q' and a/a' appear in relation/event links 
Global 
(Cross-
document 
in 
comparable 
corpora) 
Deep 
(Fact-
based) 
English f17: mutual information of q/q' and a/a' appear in relation/event links 
 
Table 4: Validation Features for Crosslingual Slot Filling
the following results, the baseline features are
always used in addition to any other features.
6.1 Overall Performance
Because of the data scarcity, ten-fold cross-
validation, across queries, was used to train
and test the system. Quantitative results after
combining answers from multiple pipelines are
shown in Table 5. We used two basic features,
one is the slot type and the other is the entity
type of the query (i.e. person or organization).
This basic feature set is already successful in im-
proving the precision of the pipelines, although
this results in a number of correct answers be-
ing discarded as well. By adding the additional
validation features described previously, both
the f-score and precision of the models are im-
proved. In the case of the cross-lingual pipelines
(3+4+5) the number of correct answers chosen
is almost doubled while increasing the precision
of the output.
6.2 Impact of Global Validation
A comparison of the benefits of global versus lo-
cal features are shown in Table 6, both of which
dramatically improve scores over the baseline
features. The global features are universally
Pipelines F P R
Basic Features
1+2 0.31 0.31 0.30
3+4+5 0.26 0.39 0.20
1+2+3+4+5 0.27 0.29 0.25
Full Features
1+2 0.37 0.30 0.46
3+4+5 0.36 0.35 0.37
1+2+3+4+5 0.31 0.28 0.35
Table 5: Using Basic Features to Filter Answers
more beneficial than the local features, although
the local features generate results with higher
precision at the expense of the number of correct
answers returned. The global features are espe-
cially useful for pipelines 3+4+5, where the per-
formance using just these features reaches those
of using all other features ? this does not hold
true for the monolingual pipelines however.
6.3 Impact of Fact-driven Deep
Knowledge
The varying benefit of fact-driven cross-
document features and statistical cross-
document features are shown in Table 7.
116
Pipelines F P R
Local Features
1+2 0.34 0.35 0.33
3+4+5 0.29 0.40 0.22
1+2+3+4+5 0.27 0.32 0.24
Global Features
1+2 0.35 0.30 0.42
3+4+5 0.37 0.36 0.38
1+2+3+4+5 0.33 0.29 0.38
Table 6: The Benefit of Global versus Local Features
While both feature sets are beneficial, the
monolingual pipelines (1+2) benefit more
from statistical features while the cross-lingual
pipelines (3+4+7) benefit slightly more from
the fact-based features. Despite this bias, the
overall results when the features are used in
all pipelines are very close with the fact-based
features being slightly more useful overall.
Pipelines F P R
Fact-Based Features
1+2 0.33 0.27 0.42
3+4+5 0.35 0.43 0.29
1+2+3+4+5 0.30 0.27 0.34
Statistical Features
1+2 0.37 0.34 0.40
3+4+5 0.34 0.35 0.33
1+2+3+4+5 0.29 0.25 0.34
Table 7: Fact vs. Statistical Cross-Doc Features
Translation features were only beneficial to
pipelines 3, 4, and 5, and provided a slight in-
crease in precision from 0.39 to 0.42, but pro-
vided no noticeable benefit when used in con-
junction with results from pipelines 1 and 2.
This is because the answers where translation
features would be most useful were already be-
ing selected by pipelines 1 and 2 using the base-
line features.
6.4 Discussion
The use of any re-scoring, even with baseline
features, provides large gains over the union of
the baseline pipelines, removing large number
of incorrect answers. The use of more sophis-
ticated features provided substantial gains over
the baseline features. In particular, global fea-
tures proved very effective. Further feature en-
gineering to address the remaining errors and
the dropped correct answer would likely provide
increasing gains in performance.
In addition, two human annotators, indepen-
dently, conducted the same task on the same
data, with a second pass of adjudication. The F-
scores of inter-annotator agreement were 52.0%
for the first pass and 73.2% for the second pass.
This indicates that slot filling remains a chal-
lenging task for both systems and human anno-
tators?only one monolingual system exceeded
30% F-score in the KBP2010 evaluation.
7 Conclusion and Future Work
Crosslingual slot filling is a challenging task
due to limited performance in two separate ar-
eas: information extraction and machine trans-
lation. Various methods of combining tech-
niques from these two areas provided weak yet
complementary baseline pipelines. We proposed
an effective approach to integrate these base-
lines and enhance their performance using wider
and deeper knowledge from comparable cor-
pora. The final system based on cross-lingual
comparable corpora outperformed monolingual
pipelines on much larger monolingual corpora.
The intuition behind our approach is that
over-generation of candidate answers from weak
baselines provides a potentially strong recall
upper-bound. The remaining enhancement be-
comes simpler: filtering errors. Our experiments
also suggest that our rescoring models tend to
over-fit due to small amount of training data.
Manual annotation and assessment are quite
costly, motivating future work in active learning
and semi-supervised learning methods. In addi-
tion, we plan to apply our results as feedback to
improve MT performance on facts using query
and answer-driven language model adaptation.
We have demonstrated our approach on English-
Chinese pair, but the framework is language-
independent; ultimately we would like to extend
the task to extracting information from more
languages.
117
Acknowledgments
This work was supported by the U.S. NSF CAREER
Award under Grant IIS-0953149 and PSC-CUNY
Research Program. Any opinions, findings, and con-
clusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.
References
K. Bollacker, R. Cook, and P. Tufts. 2008. Free-
base: A shared database of structured general hu-
man knowledge. In Proc. National Conference on
Artificial Intelligence.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Marissa Passantino, and Heng Ji. 2010. Top-
down and bottom-up: A combined approach to
slot filling. Lecture Notes in Computer Science,
6458:300?309, December.
C. L. A. Clarke, G. V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. In Proc. SIGIR2001.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in
Information Extraction. In Proc. IJCAI 2005.
Pascale Fung and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel
and comparable texts. In COLING-ACL.
Dilek Hakkani-Tur, Heng Ji, and Ralph Grishman.
2007. Using information extraction to improve
cross-lingual document retrieval. In Proc. RANLP
workshop on Multi-source, Multilingual Informa-
tion Extraction and Summarization.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing two
question answering systems in trec 2005. In Proc.
TREC2005.
Ahmed Hassan, Haytham Fahmy, and Hany Has-
san. 2007. Improving named entity translation
by exploiting comparable and parallel corpora. In
RANLP.
Heng Ji and Ralph Grishman. 2008. Refining Event
Extraction through Cross-Document Inference. In
Proc. of ACL-08: HLT, pages 254?262.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proc.
PACLIC2009.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name translation
for distillation. Handbook of Natural Language
Processing and Machine Translation: DARPA
Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and
Kira Griffitt. 2010. An overview of the tac2010
knowledge base population track. In Proc.
TAC2010.
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information
networks. In ACL-IJCNLP 2009 workshop on
Building and Using Comparable Corpora (BUCC
2009): from Parallel to Non-parallel Corpora.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL 2006.
K.-L. Kwork and P. P. Deng. 2006. Chinese
question-answering: Comparing monolingual with
english-chinese cross-lingual results. In Asia In-
formation Retrieval Symposium.
Cheng-Wei Lee, Yi-Hsun Lee, and Wen-Lian Hsu.
2008. Exploring shallow answer ranking features
in cross-lingual and monolingual factoid question
answering. Computational Linguistics and Chi-
nese Language Processing, 13:1?26, March.
B. Magnini, M. Negri, R. Prevete, and H. Tanev.
2001. Is it the right answer?: Exploiting web
redundancy for answer validation. In Proc.
ACL2001.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL-IJCNLP
2009.
Teruko Mitamura, Mengqiu Wang, Hideki Shima,
and Frank Lin. 2006. Keyword translation accu-
racy and cross-lingual question answering in chi-
nese and japanese. In EACL 2006 Workshop on
MLQA.
F. J. Och. 2003. Minimum error rate training in
statistical machine translaton. In Proc.ACL2003.
Kristen Parton and Kathleen McKeown. 2010. Mt
error detection for cross-lingual question answer-
ing. Proc. COLING2010.
Siddharth Patwardhan and Ellen Riloff. 2009. A
Unified Model of Phrasal and Sentential Evidence
for Information Extraction. In Proc. EMNLP
2009.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In COL-
ING2004.
D. Shen, G. Saarbruechen, and D. Klakow. 2006.
Exploring correlation of dependency relation
paths for answer extraction. In Proc. ACL2006.
118
M. M. Soubbotin and S. M. Soubbotin. 2001. Pat-
terns of potential answer expressions as clues to
the right answers. In Proc. TREC2001.
Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006. Named entity transliteration with compa-
rable corpora. In ACL 2006.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2004. Cross-lingual information extraction evalu-
ation. In Proc. COLING2004.
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language
information integration. In Proc. KDD2005.
Raghavendra Udupa, K. Saravanan, A. Kumaran,
and Jagadeesh Jagarlamudi. 2009. Mint: A
method for effective and scalable mining of named
entity transliterations from large comparable cor-
pora. In EACL2009.
Alessandro Vallin, Bernardo Magnini, Danilo Gi-
ampiccolo, Lili Aunimo, Christelle Ayache, Petya
Osenova, Anselmo Peas, Maaren de Rijke, Bogdan
Sacaleanu, Diana Santos, and Richard Sutcliffe.
2005. Overview of the clef 2005 multilingual ques-
tion answer track. In Proc. CLEF2005.
Roman Yangarber. 2006. Verification of Facts across
Document Boundaries. In Proc. International
Workshop on Intelligent Information Access.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proc. HLT/NAACL 2004.
119
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 43?52,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Language-Independent Name Translation Mining from
Wikipedia Infoboxes
Wen-Pin Lin, Matthew Snover, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
danniellin@gmail.com, msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
The automatic generation of entity profiles
from unstructured text, such as Knowledge
Base Population, if applied in a multi-lingual
setting, generates the need to align such pro-
files from multiple languages in an unsuper-
vised manner. This paper describes an unsu-
pervised and language-independent approach
to mine name translation pairs from entity pro-
files, using Wikipedia Infoboxes as a stand-in
for high quality entity profile extraction. Pairs
are initially found using expressions that are
written in language-independent forms (such
as dates and numbers), and new translations
are then mined from these pairs. The algo-
rithm then iteratively bootstraps from these
translations to learn more pairs and more
translations. The algorithm maintains a high
precision, over 95%, for the majority of its
iterations, with a slightly lower precision of
85.9% and an f-score of 76%. A side effect
of the name mining algorithm is the unsuper-
vised creation of a translation lexicon between
the two languages, with an accuracy of 64%.
We also duplicate three state-of-the-art name
translation mining methods and use two ex-
isting name translation gazetteers to compare
with our approach. Comparisons show our
approach can effectively augment the results
from each of these alternative methods and re-
sources.
1 Introduction
A shrinking fraction of the world?s web pages are
written in English, while about 3,000 languages are
endangered (Krauss, 2007). Therefore the ability
to access information across a range of languages,
especially low-density languages, is becoming in-
creasingly important for many applications. In this
paper we hypothesize that in order to extend cross-
lingual information access to all the language pairs
on the earth, or at least to some low-density lan-
guages which are lacking fundamental linguistic re-
sources, we can start from the much more scalable
task of ?information? translation, or more specifi-
cally, new name translation.
Wikipedia, as a remarkable and rich online ency-
clopedia with a wealth of general knowledge about
varied concepts, entities, events and facts in the
world, may be utilized to address this need. As
of March 2011 Wikipedia contains pages from 275
languages1, but statistical machine translation (MT)
techniques can only process a small portion of them
(e.g. Google translate can only translate between
59 languages). Wikipedia infoboxes are a highly
structured form of data and are composed of a set
of subject-attribute-value triples that summarize or
highlight the key features of the concept or sub-
ject of each article. A large number of instance-
centered knowledge-bases that have harvested this
structured data are available. The most well-known
are probably DBpedia (Auer et al, 2007), Free-
base (Bollacker et al, 2007) and YAGO (Suchanek
et al, 2007). However, almost all of these ex-
isting knowledge bases contain only one language.
Even for high-density languages, more than 70% of
Wikipedia pages and their infobox entries do not
contain cross-lingual links.
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
43
Recent research into Knowledge Base Population,
the automatic generation of profiles for named enti-
ties from unstructured text has raised the possibility
of automatic infobox generation in many languages.
Cross-lingual links between entities in this setting
would require either expensive multilingual human
annotation or automatic name pairing. We hypoth-
esize that overlaps in information across languages
might allow automatic pairing of profiles, without
any preexisting translational capabilities. Wikipedia
infoboxes provide a proxy for these high quality
cross lingual automatically generated profiles upon
which we can explore this hypothesis.
In this paper we propose a simple and general un-
supervised approach to discover name translations
from knowledge bases in any language pair, using
Wikipedia infoboxes as a case study. Although dif-
ferent languages have different writing systems, a
vast majority of the world?s countries and languages
use similar forms for representing information such
as time/calendar date, number, website URL and
currency (IBM, 2010). In fact most languages com-
monly follow the ISO 8601 standard2 so the formats
of time/date are the same or very similar. Therefore,
we take advantage of this language-independent for-
matting to design a new and simple bootstrapping
based name pair mining approach. We start from
language-independent expressions in any two lan-
guages, and then extract those infobox entries which
share the same slot values. The algorithm itera-
tively mines more name pairs by utilizing these pairs
and comparing other slot values. In this unsuper-
vised manner we don?t need to start from any name
transliteration module or document-wise temporal
distributions as in previous work.
We conduct experiments on English and Chinese
as we have bi-lingual annotators available for eval-
uating results. However, our approach does not re-
quire any language-specific knowledge so it?s gen-
erally applicable to any other language pairs. We
also compare our approach to state-of-the-art name
translation mining approaches.
1.1 Wikipedia Statistics
A standard Wikipedia entry includes a title, a docu-
ment describing the entry, and an ?infobox? which
2http://en.wikipedia.org/wiki/ISO_8601
is a fixed-format table designed to be added to
the top right-hand corner of the article to con-
sistently present a summary of some unifying at-
tributes (or ?slots?) about the entry. For example,
in the Wikipedia entry about the singer ?Beyonce
Knowles?, the infobox includes information about
her birth date, origin, song genres, occupation, etc.
As of November 2010, there were 10,355,225 En-
glish Wikipedia entries, and 772,826 entries. Only
27.2% of English Wikipedia entries have cross-
lingual hyperlinks referring to their corresponding
Chinese entries.
Wikipedia entries are created and updated expo-
nentially (Almeida et al, 2007) because of the in-
creasing number of contributors, many of whom are
not multi-lingual speakers. Therefore it is valuable
to align the cross-lingual entries by effective name
mining.
1.2 Motivating Example
Figure 1: A Motivating Example
Figure 1 depicts a motivating example for our ap-
proach. Based on the assumption that if two per-
son entries had the same birth date and death date,
44
they are likely to be the same person, we can find
the entity pair of (Michael Jackson /???.???).
We can get many name pairs using similar language-
independent clues. Then starting from these name
pairs, we can iteratively get new pairs with a large
portion of overlapped slots. For example, since
??????? and ?The Jackson 5? share many slot
values such as ?member? and ?years active?, they
are likely to be a translation pair. Next we can use
the new pair of (The Jackson 5 / ?????) to
mine more pairs such as ?????? and ?Steeltown
Records.?
2 Data and Pre-Processing
Because not all Wikipedia contributors follow the
standard naming conventions and date/number for-
mats for all languages, infoboxes include some
noisy instances. Fortunately the NIST TAC Knowl-
edge Base Population (KBP) task (Ji et al, 2010) de-
fined mapping tables which can be directly used to
normalize different forms of slot types3. For exam-
ple, we can group ?birthdate?, ?date of birth?, ?date-
birth? and ?born? to ?birth date.? In addition, we also
normalized all date slot values into one standard for-
mat as ?YYYY MM DD.? For example, both ?1461-
8-5? and ?5 August, 1461? are normalized as ?1461
08 05.? Only those Wikipedia entries that have at
least one slot corresponding to the Knowledge Base
Population task are used for name mining. Entries
with multiple infoboxes are also discarded as these
are typically ?List of ? entries and do not corre-
spond to a particular named entity. The number of
entries in the resulting data set are shown in Table 1.
The set of slots were finally augmented to include
the entry?s name as a new slot. The cross-lingual
links between Chinese and English Wikipedia pages
were used as the gold standard that the unsupervised
algorithm attempted to learn.
Language Entries Slot Values E-Z Pairs
English (E) 634,340 2,783,882 11,109Chinese (Z) 21,152 110,466
Table 1: Processed Data Statistics
3It is important to note that the vast majority of Chinese
Wikipedia pages store slot types in English in the underlying
wiki source, removing the problem of aligning slot types be-
tween languages.
3 Unsupervised Name Pair Mining
The name pair mining algorithm takes as input a set
of English infoboxes E and Chinese infoboxes Z.
Each infobox consists of a set of slot-value pairs,
where each slot or value may occur multiple times in
a single infobox. The output of the algorithm is a set
of pairs of English and Chinese infoboxes, match-
ing an infobox in one language to the corresponding
infobox in the other language. There is nothing in-
herently designed in the algorithm for English and
Chinese, and this method could be applied to any
language pair.
Because the algorithm is unsupervised, it begins
with no initial pairs, nor is there any initial trans-
lation lexicon between the two languages. As the
new pairs are learned, both the entries titles and the
values of their infoboxes are used to generate new
translations which can be used to learn more cross-
lingual name pairs.
3.1 Search Algorithm
The name pair mining algorithm considers all pairs
of English and Chinese infoboxes4, assigns a score,
described in Section 3.2, to each pair and then greed-
ily selects the highest scoring pairs, with the follow-
ing constraints:
1. Each infobox can only be paired to a single in-
fobox in the other language, with the highest
scoring infobox being selected. While there are
some instances of two entries in one language
for one entity which both have translation links
to the same page in another language, these are
rare occurrences and did not occur for the KBP
mapped data used in these experiments.
2. An pair (e, z) can only be added if the score
for the pair is at least 95%5 percent higher than
the score for the second best pair for both e and
z. This eliminates the problem of ties in the
data, and follows the intuition that if there are
4The algorithm does not need to compare all pairs of in-
foboxes as the vast majority will have a score of 0. Only those
pairs with some equivalent slot-value pairs need to be scored.
The set of non-zero scoring pairs can thus be quickly found by
indexing the slot-value pairs.
5The value of 95% was arbitrarily chosen; variations in this
threshold produce only small changes in performance.
45
multiple pairs with very similar scores it is ben-
eficial to postpone the decision until more evi-
dence becomes available.
To improve the speed of the algorithm, the top 500
scoring pairs, that do not violate these constraints,
are added at each iteration. The translation lexicon
is then updated. The translation lexicon is updated
each iteration from the total set of pairs learned us-
ing the following procedure. For each pair (e, z) in
the learned pairs, new translations are added for each
of the following conditions:
1. A translation of the name of e to the name z is
added.
2. If a slot s in e has one value, ve, and that slot
in z has one value, vz , a translation ve ? vz is
added.
3. If a slot s has multiple values in e and z, but all
but one of these values, for both e and z, have
translations to values in the other entry, then a
translation is learned for the resulting untrans-
lated value.
These new translations are all given equal weight
and are added to the translation lexicon even if the
evidence for this translation occurs in only a sin-
gle name pair6. These translations can be used to
align more name pairs in subsequent iterations by
providing more evidence that a given pair should be
aligned. After a translation is learned, we consider
the English side to be equivalent to the Chinese side
when scoring future infobox pairs.
The algorithm halts when there are no longer any
new name pairs with non-zero score which also sat-
isfy the search constraints described above.
3.2 Scoring Function
A score can be calculated for the pairing of an En-
glish infobox, e and a Chinese infobox, z according
to the following formula:
?
s?slots
{
IZ(s) + IE(s) ?v1, v2 : z.s.v1 ? e.s.v2
0 otherwise
(1)
6Assigning a probability to each translation learned based
upon the number of entries providing evidence for the transla-
tion could be used to further refine the predictions of the model,
but was not explored in this work.
A slot-value pair in Chinese, z.s.v1, is considered
equivalent to a slot-value pair in English, e.s.v2, if
the values are the same (typically only the case with
numerical values) or if there is a known translation
from v1 to v2. These translations are automatically
learned during the name-mining process. Initially
there are no known translations between the two lan-
guages.
The term IL(s) in equation 1 reflects how infor-
mative the slot s is in either English (E) or Chinese
(Z), and is calculated as the number of unique val-
ues for that slot for that language divided by the to-
tal number of slot-value pairs for that language, as
shown in equation 2.
IL(slot s) =
|{v|i ? L ? ?i.s.v}|
|{i.s.v|i ? L}|
(2)
If a slot s contains unique values such that a slot
and value pair is never repeated then IL(s) is 1.0
and indicates that the slot distinguishes entities very
well. Slots such as ?date of birth? are less infor-
mative since many individuals share the same birth-
date, and slots such as ?origin? are the least informa-
tive since so many people are from the same coun-
tries. A sampling of the IL(s) scores is shown in
Table 2. The slots ?origin? and ?religion? are the two
lowest scoring slots in both languages, while ?in-
fobox name? (the name of wikipedia page in ques-
tion), ?website?, ?founded? are the highest scoring
slot types.
Slot IZ IE
origin 0.21 0.03
religion 0.24 0.08
parents 0.57 0.60
date of birth 0.84 0.33
spouse 0.97 0.86
founded by 0.97 0.94
website 0.99 0.96
infobox name 1.00 1.00
Table 2: Sample I(s) Values
4 Evaluation
In this section we present the evaluation results of
our approach.
46
4.1 Evaluation Method
Human evaluation of mined name pairs can be dif-
ficult as a human assessor may frequently need to
consult the infoboxes of the entries along with con-
textual documents to determine if a Chinese entry
and an English entry correspond to the same en-
tity. This is especially true when the translations are
based on meanings instead of pronunciations. An al-
ternative way of mining name pairs from Wikipedia
is to extract titles from a Chinese Wikipedia page
and its corresponding linked English page if the link
exists (Ji et al, 2009). This method results in a
very high precision but can miss pairs if no such
link between the pages exists. We utilized these
cross-lingual page links as an answer key and then
only performed manual evaluation, using a bilingual
speaker, on those pairs generated by our algorithm
that were not in the answer key.
4.2 Results
Figure 2 shows the precision, recall and f-score of
the algorithm as it learns more pairs. The final
output of the mining learned 8799 name pairs, of
which 7562 were correct according to the cross-
lingual Wikipedia links. This results in a precision
of 85.94%, a recall of 68.07% and a F1 score of
75.9%. The precision remains above 95% for the
first 7,000 name pairs learned. If highly precise an-
swers are desired, at the expense of recall, the algo-
rithm could be halted earlier. The translation lexicon
contained 18,941 entries, not including translations
learned from the entry names themselves.
Assessment Number
Link Missing From Wikipedia 35 2.8%
Same Name, Different Entity 17 1.4%
Partially Correct 98 7.9%
Incorrect 1,087 87.9%
Table 3: Human Assessment of Errors
Because the answer key for name mining is au-
tomatically extracted from the cross-lingual links
in Wikipedia, it is possible that correct name pairs
could be missing from the answer key if no cross-
lingual link exists. To examine if any such pairs
were learned, a manual assessment of the name pairs
that were not in the answer key was performed, as
shown in Table 4.2. This assessment was performed
by bilingual speakers with an inter-annotator agree-
ment rate of 93.75%.
The vast majority, 87.9%, of the presumably er-
roneous name pairs assessed that were missing from
the answer-key were actually incorrect pairs. How-
ever, 35, or 2.8%, of the name pairs were actually
correct with their corresponding Wikipedia pages
lacking cross-lingual links (these corrections are
not reflected in the previous results reported above,
which were based solely on the pairs in the an-
swer key). For a small portion, 1.4%, of the errors,
the name translation is correct but the entries actu-
ally refer to different entities with the same name.
One such example is (Martin Rowlands / ???).
The English entity, ?Martin Rowlands? is an ath-
lete (an English football player), while the Chinese
entity is a former Hong Kong government official,
whose name translates to English as ?Martin Row-
lands?, as revealed on his Wikipedia page. Neither
entity has an entry in the other language. The fi-
nal category are partially correct answers, such as
the pair (Harrow, London / ???), where the En-
glish entry refers to an area within the London Bor-
ough of Harrow, while the Chinese entry refers to
the London Borough of Harrow as a whole. The
English entry ?Harrow, London? does not have a
corresponding entry in Chinese, although there is
an entry in both language for the larger Borough it-
self. All of these cases represent less 15% of the
learned name pairs though as 85.94% of the name
pairs were already determined to be correct based
on cross-lingual Wikipedia links.
Judgement Percent
Correct 64.4%
Partial 18.4%
Incorrect 15.1%
Not Translations 2.1%
Table 4: Slot Value Translation Assessment from Ran-
dom Sample of 1000
The name mining algorithm bootstraps many
name pairs by using possible translations between
the slot values in previously learned pairs. The fi-
nal translation lexicon learned had 18,941 entries.
A random sample of 1,000 entries from the trans-
47
Figure 2: Performance of Unsupervised Name Mining
lation lexicon was assessed by a human annotator,
and judged as correct, partial, incorrect or not trans-
lations, as shown in Table 4.2. Partial translations
were usually cases where a city was written with
its country name in language and as just the city
name in the other languages, such as ?Taipei Taiwan
Republic of China? and ????? (Taipei). Cases
are marked as ?not translations? if both sides are in
the same language, typically English, such as ?Eric
Heiden? in English being considered a translation of
?Eric Arthur Heiden? from a Chinese entry (not in
Chinese characters though). This normally occurs if
the Chinese page contained English words that were
not translated or transliterated.
An example7 of the name mining is shown in Fig-
ure 3, where the correct name pair for (George W.
Bush / ????????) is learned in iteration i,
is mined for additional translations and then pro-
vides evidence in iteration i+1 for the correct name
pair (Laura Bush / ?????????). When
learning the name pair for ?George W. Bush?, ev-
idence is first found from the slots marked as equiv-
alent (approx). Translations for ?Harvard Busi-
ness School? and ? Republican Party? were learned
in previous iterations from other name pairs and
now provide evidence, along with the identical val-
ues in the ?date of birth? slot for the pair (George
W. Bush / ????????). After learning this
7Many slot value pairs that were not relevant for the calcu-
lation are not shown to save space. Otherwise, this example is
as learned in the unsupervised name mining.
pair, new translations are extracted from the pair
for ?George W. Bush?, ?George Walker Bush?,
?President of the United States?, ?Laura Bush?,
and ?Yale University?. The translations for ?Laura
Bush? and ?George W. Bush? provide crucial in-
formation in the next iteration that the pair (Laura
Bush / ?????????) is correct. From this,
more translations are learned, although not all of
these translations are fully correct, such as ?Author
Teacher Librarian First Lady? which is now pos-
tulated to be a translation of ????? (Librar-
ian), which is only partially true, as the other pro-
fessions are not represented in the translation. While
such translations may not be fully correct, they still
could prove useful for learning future name pairs (al-
though this is unlikely in this case since there are
very few entries with ?first lady? as part of their ti-
tle.
5 Discussion
Besides retaining high accuracy, the final list of
name pairs revealed several advantages of our ap-
proach.
Most previous name translation methods are lim-
ited to names which are phonetically transliterated
(e.g. translate Chinese name ???? (You shen
ke)? to ?Yushchenko? in English). But many other
types of names such as organizations are often ren-
dered semantically, for example, the Chinese name
????? (jie fang zhi hu)? is translated into ?Lib-
eration Tiger? in English. Some other names in-
48
Iteration i
George W. Bush ???????? (George Walker Bush)
alt names George Walker Bush alt names ?????? (George Bush)
title President of the United States title ???? (President of the
USA)
date of birth 1946-7-6 ? date of birth 1946-7-6
member of Republican Party ? member of ??? (Republican Party)
spouse Laura Bush spouse ????????? (Laura
Welch Bush)
schools attended Yale University schools attended ???? (Yale University)
schools attended Harvard Business School ? schools attended ????? (Harvard Business
School)
Iteration i + 1
Laura Bush ????????? (Laura Welch Bush)
alt names Laura Bush ? alt names ????????? (Laura
Welch Bush)
alt names ????????? (Laura
Lane Welch)
date of birth 1946-11-4 ? date of birth 1946-11-4
place of birth Midland Texas place of birth ???????? (Texas
Midland)
title Author Teacher Librarian First
Lady
title ????? (Librarian)
title First Lady of the United States ? title ??????(First Lady of
USA)
spouse George W. Bush ? spouse ???????? (George
Walker Bush)
Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses
volve both semantic and phonetic translations, or
none of them. Our approach is able to discover all
these different types, regardless of their translation
sources. For example, our approach successfully
mined a pair (Tarrytown / ???) where ?Tarry-
town? is translated into ????? neither by its pro-
nunciation ?bai you cun? nor its meaning ?tar vil-
lage.?
Name abbreviations are very challenging to trans-
late because they need expansions based on con-
texts. However our approach mined many abbrevia-
tions using slot value comparison. For example, the
pair of (Yctc /????) was successfully mined al-
though its English full name ?Yeh-Chiang Technol-
ogy Corp.? did not appear in the infoboxes.
Huang (2005) also pointed out that name transla-
tion benefited from origin-specific features. In con-
trast, our approach is able to discover name pairs
from any origins. For example, we discovered the
person name pair (Seishi Yokomizo / ????) in
which ?Seishi Yokomizo? was transliterated based
on Japanese pronunciation.
Furthermore, many name translations are context
dependent. For example, a person name in Chinese
?????????? could be translated into ?Yasser
Arafat? (PLO Chairman) or ?Yasir Arafat? (Crick-
eter) based on different contexts. Our method can
naturally disambiguate such entities based on slot
comparison at the same time as translation mining.
More importantly, our final list includes a large
portion of uncommon names, which can be valu-
able to address the out-of-vocabulary problem in
both MT and cross-lingual information processing.
Especially we found many of them are not in the
name pairs mined from the cross-lingual Wikipedia
title links, such as (Axis Communications / ???),
(Rowan Atkinson / ??????), (ELSA Technol-
ogy /?????) and (Nelson Ikon Wu /???).
49
6 Comparison with Previous Methods and
Resources
There have been some previous methods focusing on
mining name translations using weakly-supervised
learning. In addition there are some existing name
translation gazetteers which were manually con-
structed. We duplicated a variety of alternative
state-of-the-art name translation mining methods
and mined some corresponding name pair sets for
comparison. In fact we were able to implement the
techniques in previous approaches but could not du-
plicate the same number of results because we could
not access the same data sets. Therefore the main
purpose of this experiment is not to claim our ap-
proach outperforms these existing methods, rather
to investigate whether we can mine any new infor-
mation on top of these methods from reasonable
amounts of data.
1. Name Pair Mining from Bitexts
Within each sentence pair in a parallel cor-
pus, we ran an HMM based bilingual name
tagger (references omitted for anonymous re-
view). If the types of the name tags on both
sides are identical, we extract the name pairs
from this sentence. Then at the corpus-wide
level, we count the frequency for each name
pair, and only keep the name pairs that are fre-
quent enough. The corpora used for this ap-
proach were all DARPA GALE MT training
corpora.
2. Comparable Corpora
We implemented an information extraction
driven approach as described in Ji (2009) to
extract name pairs from comparable corpora.
This approach is based on extracting infor-
mation graphs from each language and align
names by a graph traverse algorithm. The cor-
pora used for this approach were 2000 English
documents and 2000 Chinese documents from
the Gigaword corpora.
3. Using patterns for Web mining
We constructed heuristic patterns such as par-
enthetical structure ?Chinese name (English
name)? (Lin et al, 2008) to extract name pairs
from web data with mixed Chinese and En-
glish. We used about 1,000 web pages for this
experiment.
4. Bilingual Gazetteer
We exploited an LDC bilingual name dictio-
nary (LDC2005T34) and a Japanese-English
person name dictionary including 20126
Japanese names written in Chinese charac-
ters (Kurohashi et al, 1994).
5. ACE2007 Entity Translation Training Data
We also used ACE 2007 entity translation train-
ing corpus which includes 119 Chinese-English
document pairs.
Table 5 shows the number of correct and unique
pairs mined pairs from each of the above ap-
proaches, as well as how these name mining meth-
ods can be augmented using the infobox name min-
ing described in this paper. The names mined from
our approach greatly extend the total number of cor-
rect translations with only a small number of con-
flicting name translations.
7 Related Work
Most of the previous name translation work com-
bined supervised transliteration approaches with
Language Model based re-scoring (Al-Onaizan and
Knight, 2002; Huang et al, 2004; Huang, 2005).
Our goal of addressing name translation for a large
number of languages is similar to the panlingual lex-
ical translation project (Etzioni et al, 2007). Some
recent research used comparable corpora to re-score
name transliterations (Sproat et al, 2006; Klemen-
tiev and Roth, 2006) or mine new word transla-
tions (Udupa et al, 2009; Ji, 2009; Fung and Yee,
1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al,
2007). However, most of these approaches needed
large amount of seeds and suffered from informa-
tion extraction errors, and thus relied on phonetic
similarity or document similarity to re-score candi-
date name translation pairs.
Some recent cross-lingual information access
work explored attribute mining from Wikipedia
pages. For example, Bouma et al (2009) aligned at-
tributes in Wikipedia infoboxes based on cross-page
links. Navigli and Ponzetto (2010) built a multi-
lingual semantic network by integrating the cross-
lingual Wikipedia page links and WordNet. Ji et
50
# Name Infobox Mining
Method Pairs # New # Conflicting
Automatic
(1) Bitexts 2,451 8,673 78
(2) Comparable Corpora 288 8,780 13
(3) Patterns for Web Mining 194 8799 0
Manual (4) Bilingual Gazetteer 59,886 8,689 74(5) ACE2007 Training Data 1,541 8,718 52
Table 5: Name Pairs Mined Using Previous Methods
al. (2009) described various approaches to auto-
matically mine name translation pairs from aligned
phrases (e.g. cross-lingual Wikipedia title links)
or aligned sentences (bi-texts). G et al (2009)
mined candidate words from Wikipedia and vali-
dated translations based on parallecl corpora. Some
other work mined name translations from mono-
lingual documents that include foreign language
texts. For example, Lin et al (2008) described a
parenthesis translation mining method; You et al
(2010) applied graph alignment algorithm to ob-
tain name translation pairs based on co-occurrence
statistics. This kind of data does not commonly exist
for low-density languages. Sorg and Cimiano (2008)
discovered cross-lingual links between English and
German using supervised classification based on
support vector machines. Adar et al (2009) aligned
cross-lingual infoboxes using a boolean classifier
based on self-supervised training with various lin-
guistic features. In contrast, our approach described
in this paper is entirely based on unsupervised learn-
ing without using any linguistic features. de Melo
and Weikum (2010) described an approach to detect
imprecise or wrong cross-lingual Wikipedia links
based on graph repair operations. Our algorithm can
help recover those missing cross-lingual links.
8 Conclusion and Future Work
In this paper we described a simple, cheap and ef-
fective self-boosting approach to mine name trans-
lation pairs from Wikipedia infoboxes. This method
is implemented in a completely unsupervised fash-
ion, without using any manually created seed set,
training data, transliteration or pre-knowledge about
the language pair. The underlying motivation is
that some certain expressions, such as numbers and
dates, are written in language-independent forms
among a large majority of languages. Therefore our
approach can be applied to any language pairs in-
cluding low-density languages as long as they share
a small set of such expressions. Experiments on
English-Chinese pair showed that this approach is
able to mine thousands of name pairs with more
than 85% accuracy. In addition the resulting name
pairs can be used to significantly augment the results
from existing approaches. The mined name pairs are
made publicly available.
In the future we will apply our method to mine
other entity types from more language pairs. We
will also extend our name discovery method to all
infobox pairs, not just those that can be mapped
into KBP-like slots. As a bi-product, our method
can be used for automatic cross-lingual Wikipedia
page linking, as well as unsupervised translation lex-
icon extraction, although this might require confi-
dence estimates on the translations learned. Once
our approach is applied to a panlingual setting (most
languages on the Wikipedia), we can also utilize
the voting results across multiple languages to au-
tomatically validate information or correct poten-
tial errors in Wikipedia infoboxes. Finally, as au-
tomatic name profile generation systems are gener-
ated cross-lingually, our method could be attempted
to automatic cross-lingual mappings between enti-
ties.
Acknowledgement
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
Number W911NF-09-2-0053, the U.S. NSF CA-
REER Award under Grant IIS-0953149 and PSC-
CUNY Research Program. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as repre-
51
senting the official policies, either expressed or im-
plied, of the Army Research Laboratory or the U.S.
Government. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
ment purposes notwithstanding any copyright nota-
tion hereon.
References
Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009.
Information arbitrage across multi-lingual wikipedia.
In Second ACM International Conference on Web
Search and Data Mining (WSDM?09), Barcelona,
Spain, February 2009, February.
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In ACL 2002.
Rodrigo B. Almeida, BarzanMosafari, and Junghoo Cho.
2007. On the evolution of wikipedia. In Int. Conf. on
Weblogs and Social Media.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. In The 6th
International Semantic Web Conference.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general hu-
man knowledge. In The National Conference on Arti-
ficial Intelligence (Volume 2).
Gosse Bouma, Sergio Duarte, and Zahurul Islam. 2009.
Cross-lingual alignment and complettion of wikipedia
templates. In The Third International Workshop on
Cross Lingual Information Access: Addressing the In-
formation Need of Multilingual Societies.
Gerard de Melo and Gerhard Weikum. 2010. Untangling
the cross-lingual link structure of wikipedia. In 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, Sweden.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach for
translating new words from nonparallel and compara-
ble texts. In COLING-ACL.
Rohit Bharadwaj G, Niket Tandon, and Vasudeva Varma.
2009. An iterative approach to extract dictionaries
from wikipedia for under-resourced languages. In
Proc. ICON2010, February.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by exploit-
ing comparable and parallel corpora. In RANLP.
Fei Huang, Stephan Vogel, and Alex Waibel. 2004. Im-
proving named entity translation combining phonetic
and semantic similarities. In HLT/NAACL2004.
Fei Huang. 2005. Cluster-specific name transliteration.
In HLT-EMNLP 2005.
IBM. 2010. Ibm globalization library.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard Zens,
and Hermann Ney. 2009. Name translation for distil-
lation. Handbook of Natural Language Processing and
Machine Translation: DARPA Global Autonomous
Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and Kira
Griffitt. 2010. An overview of the tac2010 knowledge
base population track. In Text Analytics Conference
(TAC2010).
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information net-
works. In ACL-IJCNLP 2009 workshop on Building
and Using Comparable Corpora (BUCC 2009): from
Parallel to Non-parallel Corpora.
Michael E. Krauss. 2007. Keynote-mass Language Ex-
tinction and Documentation: The Race Over Time. The
Vanishing Languages of the Pacific Rim. Oxford Uni-
versity Press.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of japanese
morphological analyzer juman. In The International
Workshop on Sharable Natural Language Resources
and pp.22-28.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining parenthetical translations
from the web by word alignment. In ACL2008.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), Uppsala,
Sweden.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new
word translations from comparable corpora. In COL-
ING2004.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of wikipedia - a
classification-based approach. In AAAI 2008 Work-
shop on Wikipedia and Artifical Intelligence, June.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowledge.
In The 16th International World Wide Web conference.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL2009.
Gae-won You, Seung won Hwang, Young-In Song, Long
Jiang, and Zaiqing Nie. 2010. Mining name transla-
tions from entity graph mapping. In EMNLP2010.
52
Proceedings of the 25th International Conference on Computational Linguistics, pages 74?81,
Dublin, Ireland, August 23-29 2014.
Cross-media Cross-genre Information Ranking Multi-media Information
Networks
Tongtao Zhang
Rensselaer Polytechnic Institute
zhangt13@rpi.edu
Haibo Li
Nuance
lihaibo.c@gmail.com
Hongzhao Huang
R.P.I.
huangh9@rpi.edu
Heng Ji
R.P.I.
jih@rpi.edu
Min-Hsuan Tsai
mtsai2@illinois.edu
Shen-Fu Tsai
University of Illinois at Urbana-Champaign
stsai8@illinois.edu
Thomas Huang
huang@ifp.uiuc.edu
Abstract
Current web technology has brought us a scenario that information about a certain topic is widely dis-
persed in data from different domains and data modalities, such as texts and images from news and social
media. Automatic extraction of the most informative and important multimedia summary (e.g. a ranked
list of inter-connected texts and images) from massive amounts of cross-media and cross-genre data can
significantly save users? time and effort that is consumed in browsing. In this paper, we propose a novel
method to address this new task based on automatically constructed Multi-media Information Networks
(MiNets) by incorporating cross-genre knowledge and inferring implicit similarity across texts and im-
ages. The facts from MiNets are exploited in a novel random walk-based algorithm to iteratively propagate
ranking scores across multiple data modalities. Experimental results demonstrated the effectiveness of our
MiNets-based approach and the power of cross-media cross-genre inference.
1 Introduction
Recent development on web technology ? especially on fast connection and large-scale storage systems ? has
enabled social and news media to fulfill their jobs more efficiently in time and depth. However, such development
also raises some problems such as overwhelming social media information and distracting news media contents.
In emergent scenarios such as facing an incoming disaster (e.g., Hurricane Irene in 2011 or Sandy in 2012), tweets
and news are often repeatedly spread and forwarded in certain circles and contents are often overlapped by each
other. However, browsing these messages and pages is almost unpleasant and inefficient. Therefore, an automatic
summarization on piles of tweets and news is always necessary and welcomed, among which ranking is the most
intuitive way to inform the users about the most informative content.
A passive solution is prompting the users to add more key words when typing the search query as most search
engines do. However, without prior knowledge or due to the word limit, it is never trivial for the users to establish
a satisfied ranking list for topics which attract more public attention. Recent changes on some Google Search have
integrated image search and adopted some heterogenous content analysis, nevertheless, the connection between
image and the keywords are still arbitrarily determined by the users, thus it is still far from optimal.
Active solutions which attempt to summarize information only focused on single data modalities. For example,
Zanzotto et al. (2011) provided a comprehensive comparison about summarization methods for tweets. Zhao et al.
(2011) developed a context-sensitive topical PageRank (Brin and Page, 1998) method to extract topical key phrase
from Twitter as a way to summarize twitter content. As a new prospective, Feng and Lapata (2010) used LDA to
annotate images, but this does not firmly integrate the information across different data types. Huang et al. (2012)
presented a tweet ranking approach but only focused on single data modality (i.e., text).
Other conventional solutions towards analyzing the relationship or links between the instances have long been
proposed and applied, such as PageRank (Brin and Page, 1998)and VisualRank (Jing and Baluja, 2008). The
former is excessively used in heterogeneous networks (i.e., webpages and resources) but they are mainly based on
linkage itself. VisualRank, which is based on PageRank, is a content-based linkage method but is confined with
homogeneous networks.
Above all, our goal is to integrate cross-media inference and create the linkage among the information extracted
from those heterogenous data. Our novel Multi-media Information Networks (MiNets) representation initializes
our idea about a basic ontology of the ranking system.
The main contribution of this work is to fill in the domain gaps across different network genres and bridge them
in a principled method. In this work, we manage to discover the hidden links or structures between the heteroge-
neous networks in different genres. We combine joint inference to resolve information conflicts across multi-genre
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
74
networks. We can also effectively measure, share and transfer complementary information and knowledge across
multi-genre networks using structured correspondence.
The work is presented in sections as follows. We firstly introduce an overview of our system in Section 2.
Detailed approaches in information extraction and constructing meta-information network are then followed in
Section 3. Measurement across the multimedia information are proposed in Section 4 and 5. In Section 6 we
demonstrate the results and performance gain.
2 Approach Overview
Within the context of an event where users generate a vast amount of multi-media messages in forms of tweets and
images, we aim to provide a ranked subset of the most informative ones. Given a set of tweets T = {t
1
, ..., t
n
},
and a set of images P = {p
1
, ..., p
m
} as input, our approach provides ordered lists of the most informative tweets
or images (a.k.a objects) so that the informativeness of an object in position i is higher than or equal to that of an
object in position i + 1. We consider the degree of informativeness of a certain object as the extent to which it
provides valuable information to people who are involved in or tracking the event in question.
During emergent events, there are tight correlations between social media and web documents. Important infor-
mation shared in social media tends to be posted in web documents. Therefore we also integrate information in a
formal genre such as web documents to enhance the ranking quality of tweets and images. It consists of two main
sub-tasks:
? Multimedia Information Network (MiNet) Construction:
Construct MiNet from cross-media and cross-genre information (i.e. tweets, images, sentences of web doc-
uments). Given a set of tweets and images on a specific topic as input, the formal genre web documents and
images from the embedded URLs in those tweets are retrieved. Afterwards, a set of sentences and images are
extracted from the web documents. Then we exploit advanced text Information Extraction and image Concept
Extraction techniques to extract meta-information and construct the meta-information network. Together with
three sets of heterogeneous input data, MiNet is constructed.
? MiNet-Based Information Ranking: Rank the tweets and images. By extending and adapting Tri-
HITS (Huang et al., 2012), we propose EN-Tri-HITs, an random walk-based propagation algorithm which
iteratively propagate ranking scores for sentences, tweets, and images across MiNet to refine the tweet and
image rankings.
3 Meta-information Network
When integrating information from different data modalities, meta-information network plays a pivotal role for
representing interesting concepts and relations between them. We automatically construct the initial informa-
tion networks using our state-of-the-art information extraction and image concept extraction techniques. A meta-
information network is a heterogeneous network including a set of ?information graphs? which is formally defined
as: G = {G
i
: G
i
= (V
i
, E
i
)}, where V
i
is the collection of concept nodes, and E
i
is the collection of edges
linking one concept to the other. An example is depicted in Figure 1. The meta-information network contains
human knowledge pertaining to the target domain that could improve the performance of text process and image
analysis. In this paper, we first construct meta-information networks separately from texts and images, and then
fuse and enrich them through effective cross-media linking methods.
3.1 Information Extraction from Texts
Extracting salient types of facts for a meta-information network is challenging. In this paper we tackle this problem
from two angles to balance the trade-off between quality and granularity/annotation cost. On one hand, to reveal
deep semantics in meta-information network, we focus on achieving high-quality extraction for pre-defined fine-
grained types such as those in NIST Automatic Content Extraction (ACE)
1
. For example, a ?Person/Individual?
node may include attributes such as ?Birth-Place?, and a ?Organization/Employee? node may include attributes
such as ?City-of-Headquarter?. These two nodes may be connected via a ?Employment/End-Position? link.
We apply an Information Extraction (IE) system (Li et al., 2013) to extract entities, relations and events defined
in ACE2005. There are 7 types of entities, 18 types of relations and 33 types of events. This system is based
on a joint framework using structured perceptron with efficient beam-search and incorporating diverse lexical,
syntactic, semantic and ontological features. We convert the IE output into the graph structured representation of
meta-information network by mapping each entity as a node, and link entity nodes by semantic relations or events
they are involved. For example, the relations between entities are naturally mapped to links in the meta-information
network, such as the ?employment? relation between ?Bill Read? and ?Hurricane Center?. In addition, if an event
1
http://www.itl.nist.gov/iad/894.01/tests/ace/
75
flood
Multimedia information Networks
Meta-information Networks
ORG
flooding from Irene killed one person Puerto RicoBill Read Hurricane Center said
Employment
Speaker
Agent Subject
Clause Subject
PlaceAgent
Victim
storm
Wikipedia
Verifiedby wiki Verifiedby wiki Verifiedby wiki
Verified Entity Concepts
Tweets Sentences of web documents Images
Contents Structured information
PER GPEEvent TriggerPredicate Predicate Noun phrase Noun phrase
Example:SRL
IE concept
type
Conceptfrom image Concept from image
Figure 1: An example of meta-information network. Sentence: ?Bill Read, Hurricane Center director, said that
flooding from Irene killed at least one person in Puerto Rico?
argument is an entity, we also add an ?Event Argument? link between the event trigger and the entity, such as the
link between ?Irene? and ?killed?.
On the other hand, in order to enrich the meta-information network, we extract more coarse-grained salient
fact types based on Semantic Role Labeling (SRL) (Pradhan et al., 2008). For example, given the sentence ?In
North Carolina, 10 counties are being evacuated.?, the ?evacuation? event is not included in ACE. However, the
SRL system can successfully detect the predicate (?evacuated?) and its semantic roles (?10 counties? and ?North
Carolina?). These argument heads and predicates are added into the meta-information network as vertices, and
edges are added between each predicate-argument pairs.
We merge entity mentions across tweets and web documents based on a cross-document entity clustering system
described in (Chen and Ji, 2011). Moreover, for the same type of nodes from the SRL system, we also merge them
by string matching across documents.
3.2 Concept Extraction from Images
We also developed a concept modeling approach by extending the similar framework in previous work (Tsai et al.,
2012), Probabilistic Logical Tree (PLT), to extract semantic concepts from images. PLT integrates the logical and
statistical inferences in a unifying framework where the existing primitive concepts are connected into a potentially
unlimited vocabulary of high-level concepts by basic logical operations. In contrast, most existing image concept
extraction algorithms either only learn a flat correlative concept structure, or a simple hierarchical structure without
logical connections.
With an efficient statistical learning algorithm, the complex concepts in upper level of PLT are modeled upon
some logically connected primitive concepts. This statistical learning approach is very flexible, where each concept
in PLT can be modeled from distinctive feature spaces with the most suitable feature descriptors (e.g., visual
features such as color and shape for scenery concepts).
For our case study on ?Hurricane Irene? scenario, we apply this algorithm to extract the hierarchical concept
trees with roots ?flood? or ?storm? from all the images in web documents whose URLs are contained in tweets.
The main problem is the classifications of the concepts such that it may be properly be placed onto an ontology.
In order to enrich the hierarchy, we seek to classify these linkages through the use of the semi-structured and
structured data that exists on Wikipedia. We use pattern matching to extract is-a relations from the first paragraphs
of Wikipedia articles. For example, starting from our initial concept ?Hurricane Irene?, we can find its is-a relation
with ?Tropical Cyclone?, and then climb up one more level to ?Storm? where we can further mine lower concepts
such as ?Tornado? and ?Snow Storm?.
76
4 Multi-media Information Networks
A Multimedia Information Network (MINet) is a structured collection made up of a set of multimedia documents
(e.g., texts and images) and links between these documents. Each link corresponds to a specific relationship
between nodes, such as hyperlinks between web documents or similarity links between tweets. In this paper, we
construct our MINet based on two forms of contents from different domains: tweets, web documents (plain texts)
and images.
4.1 Within-media Linking
4.1.1 Text-Text Similarity
Taking web document for example, we construct the meta-information network G = {G
i
: G
i
= (V
i
, E
i
)} for all
web documents D, in which each web document d
i
? D corresponds to G
i
. Given the meta-information network
G, we compute the weight of each vertex v
j
? V
i
as weight
v
j
=
nf(v
j
,d)
AV E(D)
,
where nf(v
j
, d) is the mention number of node v
j
appearing in a document d and AV E(D) is the average
number of mentions in a document d, which is defined as AV E(D) =
?
d?D
concept mentions in d
|D|
.
Similarly, we define the weight of each link e
k
? E
i
as weight
e
k
=
nf(e
k
,d)
AV E(D)
, where nf(e
k
, d) is the mention
number of the node e
k
in a document d and AV E(D) is the average number of mentions in a document d, which
is defined as AV E(D) =
?
d?D
relation mentions in d
|D|
.
If two edges share the same type and link nodes corresponding to the same tokens, we consider them as two
mentions involved in a relation. Based on the weight of each concept mention and relation mention, we count their
frequencies and transform them into vectors. Finally, we compute cosine similarity between every two vectors.
4.1.2 Image-Image Similarity
We extract Histogram of Oriented Gradients (HOG) features (Dalal and Triggs, 2005) from patches in images and
apply Hierarchical Gaussianization (Zhou et al., 2009) to those HOG feature vectors. We learn a Gaussian mixture
model (GMM) to obtain the statistics of the patches of an image by adapting the distribution of the extracted HOG
features from these image patches and each image is represented by a super-vector. Based on the obtained image
representation, the image-image similarity is simply a cosine similarity between two HG super-vectors.
4.2 Cross-media Linking
In order to obtain cross-media similarity, we propose a method based on transfer learning technique (Qi et al.,
2012). Given a set of m points [p
1
, p
2
, . . . , p
m
] in the source (image) domain P , a set of n points [t
1
, t
2
, . . . , t
n
]
in the target (text) domain T , and a set of N corresponding pairs C = {(p
a
i
, t
b
i
}
N
i=1
in these two domains, we aim
to find a cross-media similarity function:
G(p, t) = `((Up)
T
(V t)) = `(p
T
St)), (1)
where U and V are the linear embedding of P and T , respectively. S = U
T
V is the cross-domain similarity
matrix and `(?) =
1
1+e
??
is the logistic sigmoid function.
The key to S in Equation 1 is to solve the optimization problem blow:
min
S
?
L
s
(S) + ?
?
L
d
(S) + ?
?
?(S), (2)
where
?
L
s
(S) =
?
(x,y)?C
log(1 + exp(?p
T
St)), and
?
?(S) = ?S?
?
is the nuclear norm that is the surrogate of
the matrix rank. Also, we have
?
L
d
(S) =
1
2
?
K
P
(p, p
?
)d
T
(p, p
?
) +
1
2
?
K
T
(t, t
?
)d
P
(t, t
?
),
where K(?, ?) is the similarity matrix among the points in a single domain and d(?, ?) defines the distance between
two points due to the transfer.
Taking one step further, we have
?
L
d
(S) = tr(L
T
Q
T
(S)
T
K
P
Q
P
(S)) + tr(L
X
Q
P
(S)
T
K
T
Q
T
(S)),
where L
P
and L
T
are the Laplacian matrices for K
P
and K
T
, respectively.
To solve the optimization problem (2) with nuclear norm regularization we follow the proximal gradient method
(Toh and Yun, 2010) with the following gradients:
5
?
L
s
(S) = P (J
C
?H)P
T
,5
?
L
d
(S) = P ((K
P
Q
P
L
T
+ L
P
Q
T
K
T
) ?H)P
T
(3)
77
JC
is an m ? n matrix with its (i, j)-th entry 1 if (p
i
, t
j
) ? C, otherwise 0. H is also an m ? n matrix whose
(i, j)-th entry where H
ij
= `
?
(p
T
i
St
j
).
Hence we have
5
?
L(S) = P (G ?H)T
T
, (4)
where G = J
C
+ ?K
P
Q
P
L
T
+ ?L
P
Q
T
K
T
. With the gradient in (4), one can solve the problem (2) using the
proximal gradient method.
5 MiNet-Based Information Ranking: EN-Tri-HITs
5.1 Initializing Ranking Scores
1 Input: A set of tweets (T ), and images (P ) and web documents (W ) on a given topic.
2 Output: Ranking scores (S
t
) for T and (S
p
) for P .
1: Use TextRank to compute initial ranking scores S
0
p
for P ,
S
0
t
for T and S
0
w
for W ;
2: Construct multimedia information networks across P , T
and W ;
3: k ? 0, diff ? 10e6;
4: while k < MaxIteration and diff > MinThreshold do
5: Use Eq. (5) (6) and (7) to compute S
k+1
p
, S
k+1
t
and S
k+1
w
;
6: Normalize S
k+1
p
, S
k+1
t
and S
k+1
w
;
7: diff ? max(
?
(|S
k+1
t
? S
k
t
|),
?
(|S
k+1
p
? S
k
p
|));
8: k ? k + 1
9: end while
Algorithm 1: EN-Tri-HITS: Random walk on multimedia information networks
Graph-based ranking algorithms have been widely used to analyze relations between vertices in graphs. In
this paper, we adapted PageRank (Brin and Page, 1998; Mihalcea and Tarau, 2004; Jing and Baluja, 2008) to
compute initial ranking scores in tweet-only and image-only networks where edges between tweets or images are
determined by their cosine similarity.
The ranking score is computed as follows:
S(V
i
) = (1? d) + d ?
?
V
j
?In(V
i
)
w
ji
?
V
k
?Out(V
j
)
w
jk
S(V
j
),
where V
i
is a vertex with S(V
i
) as its ranking score; In(V
i
) and Out(V
i
) are the incoming edge set and outgoing
edge set of V
i
, respectively; w
ij
is the weight for the edge between two vertices V
i
and V
j
. An edge links two
vertices that represent text units when their cosine similarity of shared content exceeds or equals to a predefined
threshold ?
t
.
5.2 Random Walk on Multimedia Information Networks
We introduce a novel algorithm to incorporate both initial ranking scores and global evidence from multimedia
information networks. It propagates ranking scores across MiNets iteratively. Our algorithm is a natural extension
of Tri-HITS (Huang et al., 2012) based on the mutual reinforcement to boost linked objects.
By extending Tri-HITS, we develop enhanced Tri-HITS (EN-Tri-HITs) to handle multimedia information net-
works with three types of objects: Tweets (T ), sentences of web documents (W ) and images (P ). EN-Tri-HITs
is able to handle more complicated network structure with more links. Given the similarity matrices M
tw
(be-
tween tweets and sentences of web documents), M
wp
(between sentences of web documents and images) andM
tp
(between tweets and images), and initial ranking scores of S
0
(p), S
0
(t) and S
0
(w), we aim to refine the initial
ranking scores and obtain the final ranking scores S(w), S(t) and S(p). Starting from images S(p), the update
process considers both the initial score S
0
(p) and the propagation from connected tweets S(t) and web documents
S(w), which can be expressed as:
?
S
w
(p
j
) =
?
i?W
m
wp
ij
S(w
i
),
?
S
t
(p
j
) =
?
k?T
m
tp
kj
S(t
k
),
S(p
j
) = (1? ?
wp
? ?
tp
)S
0
(p
j
) + ?
wp
?
S
w
(p
j
)
?
j
?
S
w
(p
j
)
+ ?
tp
?
S
t
(p
j
)
?
j
?
S
t
(p
j
)
, (5)
78
Set ID Tweets
Web Doc
Images
(Sentences)
1 1171 41(1272) 183
2 1116 47(1634) 265
3 1184 69(1639) 346
All 3471 157(4545) 794
Table 1: Data Statistics: Numbers of each item in
the dataset.
word word word word
+IE +SRL +IE+SRL
I+W 0.545 0.539 0.521 0.583
I+T 0.422 0.436 0.407 0.489
I+W+T 0.526 0.513 0.492 0.541
Table 2: NDCG@5 of Images. The image ranking
baseline performance is 0.421. I stands for Image;
W Web Documents; T Tweets
where ?
wp
, ?
tp
? [0, 1] (?
wp
+ ?
tp
? 1) are the parameters to balance between initial and propagated ranking
scores. Similar to Tri-HITS, EN-Tri-HITS normalizes the propagated ranking scores
?
S
w
(p
i
) and
?
S
t
(p
i
).
Similarly, we define the propagations from images and web documents to tweets as follows:
?
S
p
(t
k
) =
?
i?P
m
pt
ik
S(p
i
),
?
S
w
(t
k
) =
?
j?W
m
wt
jk
S(w
j
),
S(t
k
) = (1? ?
wt
? ?
pt
)S
0
(t
k
) + ?
wt
?
S
p
(t
k
)
?
k
?
S
p
(t
k
)
+ ?
pt
?
S
w
(t
k
)
?
k
?
S
w
(t
k
)
, (6)
where M
pt
is the transpose of M
tp
, ?
pt
and ?
wt
are parameters to balance between initial and propagated ranking
scores.
Each sentence of web documents S(w
j
) may be influenced by the propagation from both tweets and images:
?
S
t
(w
i
) =
?
k?T
m
tw
ki
S(t
k
),
?
S
p
(w
i
) =
?
i?P
m
pw
ji
S(p
j
),
S(w
i
) = (1? ?
tw
? ?
pw
)S
0
(w
i
) + ?
tw
?
S
t
(w
i
)
?
i
?
S
t
(w
i
)
+ ?
pw
?
S
p
(w
i
)
?
i
?
S
p
(w
i
)
, (7)
where M
pw
is the transpose of M
wp
, ?
tw
and ?
pw
are parameters to balance between initial and propagated
ranking scores.
Algorithm 1 summarizes En-Tri-HITS.
6 Experiments
6.1 Data and Scoring Metric
Currently there are no information ranking related benchmark data sets publicly available, therefore we build our
own data set and network ontology.
We crawled 3471 tweets during a three-hour period and extracted key phrases from these tweets, then we use
the key phrases as image search queries. The image search queries are submitted to Bing Image Search API and
we take the top 10 images for each query. We extract a 512-d GIST feature from each image for meta information
training. For image similarity metrics, we resize images to a maximum of 240 ? 240 and segmented into patches
with three different sizes (16, 25 and 31) by a 6-pixel step size. A 128-d Histogram of Oriented Gradients (HOG)
feature is extracted from each patch and followed by a PCA dimension reduction to 80-d. The size of dimension
of the final feature vector for each image is 42,496.
We create the ground truth based on human assessment of informativeness on a 5-star likert scale, with grade 5
as the most informative and 1 as the least informative. Table 1 presents an overview on our data sets. We conduct
3-fold cross-validation for our experiments.
To evaluate tweet ranking, we use nDCG as our evaluation metric (J?arvelin and Kek?al?ainen, 2002), which
considers both the informativeness and the position of a tweet:
nDCG(?, k) =
1
|?|
|?|
?
i=1
DCG
ik
IDCG
ik
, DCG
ik
=
k
?
j=1
2
rel
ij
? 1
log(1 + j)
,
where ? is the set of documents in the test set, with each document corresponding to an hour of tweets in our case,
rel
ij
is the human-annotated label for the tweet j in the document i, and IDCG
ik
is the DCG score of the ideal
ranking. The average nDCG score for the top k tweets is: Avg@k =
?
k
i=1
nDCG(?, i)/k. To favor diversity of
top ranked tweets, redundant tweets are penalized to lower down the final score.
79
6.2 Impact of Cross-media Inference
Table 2 and Figure 2 present the image ranking results. The results indicate that methods integrating heterogeneous
networks outperform the baseline of image ranking (0.421). When web documents are aligned with images (row
1), the ranking quality improves significantly, proving that web documents can help detect informative images by
adding support from text media of formal genre. However, the text media of informal genre, such as tweets, almost
cannot help improve the ranking performance.
1 2 3 4 5 6 7 8 9 1 00 . 20 . 30 . 40 . 50 . 60 . 7
0 . 80 . 91 . 0NDCG@N N
 I m a g e + T w e e t + W e b  D o c + W e b  D o c & T w e e t
Figure 2: NDCG@n score of Images with Various n
word word word word
+IE +SRL +IE+SRL
T 0.675 0.691 0.697 0.700
T+W 0.766 0.771 0.757 0.809
T+I 0.675 0.691 0.667 0.700
T+W+I 0.722 0.771 0.757 0.809
Table 3: NDCG@5 of Tweets
6.3 Impact of Cross-genre Inference
Methods that integrate heterogeneous networks after filtering, outperform the baseline TextRank, as shown in
Table 3. When tweets are aligned with web documents, the ranking quality improves significantly, proving that
web documents can help infer informative tweets by adding support from a formal genre. The fact that tweets with
low initial ranking scores are aligned with web documents helps promote their ranking positions. For example,
the ranking of the tweet ?Hurricane Irene: City by City Forecasts http://t.co/x1t122A? is improved compared to
TextRank, benefitting from the fact that 10 retrieved web documents are about this topic.
6.4 Remaining Error Analysis
Enhanced Tri-HITS shows encouraging improvements in ranking quality with respect to a state-of-the-art model
such as TextRank. However, there are still some issues to be addressed for further improvements.
(i) Long tweets preferred. We tracked tweets containing the keywords ?Hurricane? and ?Irene?. Using such a
query might also return tweets that are not related to the event being followed. This may occur either because
the terms are ambiguous, or because of spam being injected into trending conversations to make it visible. For
example, the tweet ?Hurricane Kitty: http://t.co/cdIexE3? is an advertisement, which is not topically related to
Irene.
(ii) Deep semantic analysis of the content, especially for images. We rely on distinct terms to refer to the
same concept. More extensive semantic analyses of text can help identify those terms, possibly enhancing the
propagation process. For example, we can explore existing text dictionaries such as WordNet (Miller, 1995) to
mine synonym/hypernym/hyponym relations, and Brown clusters (Brown et al., 1992) to mine other types of
relations in order to enrich the concepts extracted from images.
7 Conclusion and Future Work
In this paper, we propose a comprehensive information ranking approach which facilitates measurement on cross-
media/cross-genre informativeness based on a novel multi-media information network representation MiNet. We
establish links via information extraction method from text and images and verification with Wikipedia. In ad-
dition, we propose similarity measurement on intra-media and cross-media using transfer learning techniques.
We also introduce a novel En-Tri-Hits algorithm to evaluate the ranking scores across MiNet. Experiments have
demonstrated that our cross-media/cross-genre ranking method is able to significantly boost the performance of
multi-media tweet ranking. In the future, we aim to focus on enhancing the quality of concept extraction by
exploiting cross-media inference that goes beyond simple fusion.
Acknowledgement
This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-
09-2-0053 (NS-CTA), U.S. NSF CAREER Award under Grant IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the ?Deep Exploration and Filtering of Text? (DEFT) Program, IBM Faculty award and RPI faculty
start-up grant. The views and conclusions contained in this document are those of the authors and should not
be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The
80
U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any
copyright notation here on.
References
Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer
Networks, 30(1-7):107?117.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467?479.
Zheng Chen and Heng Ji. 2011. Collaborative ranking: A case study on entity linking. In Proc. EMNLP2011.
Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In In CVPR, pages
886?893.
Yansong Feng and Mirella Lapata. 2010. Topic models for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 831?839, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo Deng, Dong Wang, Hieu Le, Tarek Abdelzaher, Jiawei Han,
Alice Leung, John Hancock, and Clare Voss. 2012. Tweet ranking based on heterogeneous networks. In Proc.
COLING 2012, pages 1239?1256, Mumbai, India. The COLING 2012 Organizing Committee.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf.
Syst., 20(4):422?446, October.
Yushi Jing and Shumeet Baluja. 2008. Visualrank: Applying pagerank to large-scale image search. IEEE Trans.
Pattern Anal. Mach. Intell., 30(11):1877?1890.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In
Proc. ACL2013, pages 73?82.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of EMNLP, volume 4.
Barcelona: ACL.
George A. Miller. 1995. Wordnet: A lexical database for english. COMMUNICATIONS OF THE ACM, 38:39?41.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. In Computa-
tional Linguistics Special Issue on Semantic Role Labeling, volume 34, pages 289?310.
Guo-Jun Qi, Charu C. Aggarwal, and Thomas S. Huang. 2012. Transfer learning of distance metrics by cross-
domain metric sampling across heterogeneous spaces. In SDM, pages 528?539.
Kim-Chuan Toh and Sangwoon Yun. 2010. An accelerated proximal gradient algorithm for nuclear norm regular-
ized linear least squares problems. Pacific Journal of Optimization.
Shen-Fu Tsai, Henry Hao Tang, Feng Tang, and Thomas S. Huang. 2012. Ontological inference framework
with joint ontology construction and learning for image understanding. In IEEE International Conference on
Multimedia and Expo (ICME) 2012.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in
twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11,
pages 659?669, Stroudsburg, PA, USA. Association for Computational Linguistics.
Wayne X. Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee P. Lim, and Xiaoming Li. 2011.
Topical keyphrase extraction from Twitter. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies - Volume 1, HLT ?11, pages 379?388, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Xi Zhou, Na Cui, Zhen Li, Feng Liang, and Thomas S. Huang. 2009. Hierarchical gaussianization for image
classification. In ICCV, pages 1971?1977.
81
