Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 1?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discourse Topics and Metaphors
Beata Beigman Klebanov
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Daniel Diermeier
Northwestern University
d-diermeier@northwestern.edu
Abstract
Using metaphor-annotated material that is
sufficiently representative of the topical
composition of a similar-length document in
a large background corpus, we show that
words expressing a discourse-wide topic of
discussion are less likely to be metaphorical
than other words in a document. Our
results suggest that to harvest metaphors more
effectively, one is advised to consider words
that do not represent a discourse topic.
Traditionally, metaphor detectors use the
observation that a metaphorically used item creates
a local incongruity because there is a violation
of a selectional restriction, such as providing a
non-vehicle object to the verb derail in Protesters
derailed the conference. Current state of art
in metaphor detection therefore tends to be
?localistic? ? the distributional profile of the target
word in its immediate grammatical or collocational
context in a background corpus or a database
like WordNet is used to determine metaphoricity
(Mason, 2004; Krishnakumaran and Zhu, 2007;
Birke and Sarkar, 2006; Gedigian et al, 2006; Fass,
1991).
However, some theories of metaphor postulate
certain features of metaphors that connect it to the
surrounding text beyond the small grammatical or
proximal locality. For example, for Kittay (1987)
metaphor is a discourse phenomenon; although
the minimal metaphoric unit is a clause, often
much larger chunks of text constitute a metaphor.
Consider, for example, the TRAIN metaphor in the
following excerpt from a Sunday Times article on
20 September 1992:
Thatcher warned EC leaders to stop their
endless round of summits and take notice
of their own people. ?There is a fear that
the European train will thunder forward,
laden with its customary cargo of gravy,
towards a destination neither wished for
nor understood by electorates. But the
train can be stopped,? she said.
In the example above, the quotation is not in itself
a metaphor, as there is no indication that something
other than the actual train is being discussed (and
so no local incongruities exist). Only when situated
in the context prepared by the first sentence (and
indeed the rest of the article), the train imagery
becomes a metaphor.
According to Kittay, a metaphor occurs when
a semantic field is used to discuss a different
content domain. The theory therefore predicts that a
metaphorically used semantic domain would be off-
topic in the given document.
Although a single document can have singular,
idiosyncratic topics, it is likelier to discuss a mix of
topics that are typical of the discourse of which it is
part. We therefore derive the following hypothesis:
Words in a given document that represent a common
topic of discussion in a corpus of relevant documents
would be predominantly non-metaphorical. That is,
a smaller share of metaphorically used words in a
document would fall in such topical words than the
share of topical words in the document.
We test this hypothesis in the current article.
1
Using a large background corpus, we estimate
the topical composition of the target documents
(section 1) that were annotated for metaphors
(section 2). We then report the results of the
experiment (section 3) that strongly support the
hypothesis, and discuss the findings (section 4). The
concluding section provides a summary and outlines
the significance of the results for the practice of
metaphor detection.
1 Topic identification
1.1 EUI corpus
Our aimwas to create a large corpus of British media
discourse regarding the emerging European Union
institutions, with both Euro-phile and Euro-sceptic
camps represented. Our corpus consists of 12,814
articles drawn from three British newspapers: The
Guardian (34%), The Times (38%), and The
Independent (28%), dating from 1990 to 2000.
We used LexisNexis Academic1 to search for the
Subject index term European Union Institutions
(henceforth, EUI).2 After results are retrieved, we
further narrow them down to only documents on the
subject European Union Institutions in the detailed
subject index of the retrieved results.3,4
1.2 Identification of discourse topics
We converted all 12,858 documents5 (henceforth,
EUI+M corpus) into plain text format and removed
1http://academic.lexisnexis.com/online-services/academic-
features.aspx
2In LexisNexis subject index hierarchy: Government
and Public Administration/International Organization and
Bodies/International Governmental Organizations/European
Union Institutions.
3In the initial search, an article that scores 72% on the
subject would be retrieved, but it would not be classified as
being on this subject, and so would not be included in the final
dataset. Articles in the final dataset tend to score about 90% on
the subject, according to LexisNexis index.
4There is a gap in LexisNexis? index coverage of The
Times during 1996-7 and of The Independent during 2000. To
avoid under-representation of the newspaper and of the relevant
years in the sample, we added articles returned for the search
SECTION(Home news) AND (European Union OR Brussels)
on The Times 01/1996 through 04/1998, and SECTION(News
AND NOT Foreign) AND (European Union OR Brussels) on
The Independent throughout 2000.
512,814 EUI corpus plus 44 documents annotated for
metaphors, to be described in section 2.
words from a list of 153 common function words.
We then constructed an indexing vocabulary V that
included all and only words that (a) contained only
letters; and (b) appeared at least 6 times in the
collection. All documents were indexed using this
21,046 word vocabulary. We will designate all the
indexed words in document i as Di.
To identify the main discourse topics in
the EUI+M corpus, we submitted the indexed
documents to an unsupervised clustering method
Latent Dirichlet Allocation (Blei et al, 2003)
(henceforth, LDA).6 The designation of the clusters
as topics is supported by findings reported in Blei
et al (2003) that the clusters contain information
relevant for topic discrimination. Additionally,
Chanen and Patrick (2007) show that LDA achieves
significant correlations with humans on a topic
characterization task, where humans produced not
just a topic classification but also identified phrases
they believed were indicative of each class.
Using the default settings of LDA
implementation,7 we analyzed the corpus into
100 topics. Table 1 exemplifies some of the
emergent topics.
1.3 Topical words in a text
LDA is a generative model of text. According to its
outlook, every text is about a small (typically 5-7)
number of topics, and each indexed word in the text
belongs to one of these topics. However, in many
cases, the relationship between the word and the
topic is quite tentative, as the word is not particularly
likely given the topic. We therefore use parameter k
to control topic assignments ? we only take LDA?s
assignment of word to topic if the word is in the
top k most likely words for that topic. For k=25,
about 15% of in-vocabulary words in a document
are assigned to a topic; for k=400, about half the
in-vocabulary words are assigned to some topic. We
designate byTki all indexed words in document i that
are assigned to some topic for the given value of k.
The ratio |Tki ||Di| describes the proportion of discoursetopical words in the indexed words for the given
document.
6No stemming was performed.
7downloaded from http://www.cs.princeton.edu/?blei/lda-c/
2
Table 1: Examples of topics identified by LDA in the
EUI+M corpus. All words are taken from top 25 most
likely words given the topic. We boldface one word per
cluster, that could provide, in our view, an appropriate
label for the cluster.
foreign nato military war russian defence soviet
piece un kosovo sanctions bosnia moscow
rail tunnel transport train pounds channel eurostar
ferry trains passengers services paris eurotunnel
countries europe enlargement new membership
members eastern conference reform voting summit
commission foreign join poland negotiations
parliament mep party socialist strasbourg christian
vote leader labour conservative right political green
democrat elections epp
television commission satellite tv broadcasting
tickets film broadcasters bbc programmes media
industry channel public directive
court article justice member directive treaty
question provisions case law regulation judgment
interpretation rules order proceedings
social workers employment working hours
jobs week employers legislation unions
employees chapter rights health minimum
bank central euro monetary rates currency
interest bundesbank markets economic exchange
finance inflation dollar german
players football clubs uefa league fifa game cup
fishing fish fishermen fisheries quota vessels
boats waters sea fleet
racism racist ethnic xenophobia black minorities
jury discrimination white relations
drugs patent research human companies genetic
scientists health medical biotechnology disease
children parents punishment school rights family
childcare corporal education law father mother
controls immigration border asylum checks
passport police citizens crime europol
energy nuclear emissions oil electricity gas
environment carbon tax pollution fuel global cut
commission fraud commissioners brussels report
allegations officials inquiry meps corruption
mismanagement staff santer
2 Metaphor annotation
Ideally, we should have sampled a small sub-corpus
from the EUI corpus for metaphor annotation;
however, the choice of the data for annotation
predated the construction of the EUI corpus.
Our interest being in the way metaphors used
in public discourse help shape attitudes towards
a complex, ongoing and fateful political reality,
we came across Musolff?s (2000) work on the
British discourse on the European integration
process throughout the 1990s. Working in the
corpus linguistics tradition, Musolff (2000) studied
a number of metaphors recurrent in this discourse,
making available a selection of materials he used,
marked with the metaphors.8
One caveat to directly using the database is the
lack of clarity regarding the metaphor annotation
procedure. In particular, the author does not
report how many people participated, or any inter-
annotator agreement figures. We therefore chose
4 out of Musolff?s list of source domains, took
all articles corresponding to them (128 documents),
along with 23 articles from other source domains,
and submitted them to a group of 8 undergraduate
annotators, on top of Musolff?s original markup that
is treated as another annotator.
Annotators received the following instructions,
reflecting our focus on the persuasive use of
metaphor, as part of an argument:
Generally speaking, a metaphor is a
linguistic expression whereby something
is compared to something else that it is
clearly literally not, in order to make a
point. Thus, in Tony Blair?s famous ?I
haven?t got a reverse gear?, Tony Blair
is compared to a car in order to stress
his unwillingness/inability to retract his
statements or actions. We would say in
this case that a metaphor from a VEHICLE
domain is used. In this study we will
consider metaphors from 4 domains.
For the 4 chosen domains we provided the
following descriptions, along with 2 examples for
each:
8available from http://www.dur.ac.uk/andreas.musolff/Arcindex.htm
3
AUTHORITY Metaphors that have to do with
discipline and authority, like school, religion,
royalty, asylum, prison, etc.
LOVE Metaphors from love/romance and family.
BUILD Metaphors that have to do with building
(the process) and houses and other buildings or
constructions, their parts and uses.
VEHICLE Metaphors that have to do with land-
borne vehicles, their parts, operation and
maintenance.
People were instructed to mark every paragraph
where a metaphor from a given domain occurs. They
were also asked to provide a comment that briefly
summarizes the ground for their decision, saying
what is being compared to what.9
Table 2 shows the inter-annotator agreement
figures.
Table 2: Inter-annotator agreement, measured on 2364
paragraphs (151 documents).11
Source Domain of Metaphor ?
LOVE 0.66
VEHICLE 0.66
AUTHORITY 0.39
BUILD 0.43
LOVE and VEHICLE are close to acceptable
reliability, with the other two types scoring low.
In order to understand the nature of disagreements,
we submitted the annotated materials plus some
random annotations to 7 out of the original 8 people
for validation, 4-8 weeks after they completed
the annotations, asking them to accept or reject
9In the topics vs metaphors experiment, we test the
hypothesis on words rather than paragraphs. For metaphors
from a pre-specified domain, such as VEHICLE or LOVE, it
was usually clear which words in the paragraph belong to the
domain and are used metaphorically. People?s comments often
explicitly used words from the paragraph, or made it otherwise
clear through their description. For OpenMeta phase (please see
below), where people were asked to mark metaphors from any
source domain, they were also asked to single out the words in
the paragraph that witness the metaphor, and these are the words
used in the current experiment.
11These are results for binary classification for each metaphor
type rather than a multiclass classification, since some articles
have more than one type and some have none.
metaphor markups. We found that metaphors
initially marked by at least 4 people (out of 9) were
accepted as valid by people who did not initially
mark them in 91% of the cases, on average across
the metaphor types. These are thus uncontroversial
cases, with the missing annotations likely due to
attention slips rather than to genuine differences of
opinion. Metaphors initially marked by 1-3 people
were more controversial, with the average validation
rate of 41% (Beigman Klebanov et al, 2008).
Evidently, some of the metaphors are clearer-
cut than others, yet even the more difficult cases
got non-negligible support at validation time from
people who did not initially mark them. We
therefore decided to regard the whole of the
annotated data as valid for the purpose of the current
research. Our focus is on finding metaphors (recall),
and less on making sure all candidate metaphors are
acceptable to all annotators; it suffices to know that
even the minority opinion often finds support.
In the second stage of the research, we expanded
the repertoire of the metaphor types to include
additional source domains, mainly from Musolff?s
list. The dataset has so far been subjected to
non-expert annotations by a group of the total of
15 undergraduate students. Metaphors from the
source domains of VEHICLE, LOVE, BUILDING,
AUTHORITY, WAR, SHOW, SCHOOL, RELIGION,
MEDICINE were annotated by different subsets of
the students.
The outcome of the second stage of the project is
not sufficient for addressing the issue of discourse
topics vs metaphors, however, as there are instances
of metaphors in the text that do not fall into any
of the source domains singled out by Musolff as
recurrent ones in the discourse under consideration.
We are now at an early stage of the third phrase
we call OpenMeta, where annotators are asked to
mark all metaphors they can detect, not confining
themselves to a given list of source domains.
Only annotators who participated in the previous,
type-constrained, version of the task participate in
OpenMeta project. So far, we have 44 documents
annotated by 3 people for open-domain metaphors.
This subset features as full a coverage of all
metaphors used in the documents as we were able
to obtain so far, and it is going to serve as test data
for the topics vs metaphors hypothesis.
4
Our test set is thus biased towards recurrent
metaphorical domains (those named by Musolff),
and towards metaphors that are relatively salient
to a naive reader, from recurrent or other source
domains. Metaphors marked in the test data are
those afforded a high degree of rhetorical presence
in the discourse ? either quantitatively, because
they are repeated and elaborated, or qualitatively,
because they are striking enough to arrest the
naive reader?s attention. According to the Presence
Theory in rhetoric (Perelman and Olbrechts-Tyteca,
1969; Gross and Dearin, 2003; Atkinson et al,
2008), elements afforded high presence are key to
the rhetorical design of the argument. These are
not so much metaphors we live by without even
noticing, such as those often studied in Conceptual
Metaphor literature, like VALUE AS SIZE or TIME
AS SPACE; these are metaphors that are clearly a
matter of the author?s conscious choice, closest in
the current theorizing to Steen?s (2008) notion of
deliberate metaphors.
2.1 Pseudo sampling
The annotated data is not really a sample of the
corpus. In fact, it is not known to us exactly how the
documents were chosen; although all 44 metaphor
annotated documents are from the newspapers and
dates participating in the EUI corpus, only 20% are
actually in the EUI corpus. How can we establish
that there is a fit between the EUI collection and
the annotated texts? We check how well discourse
topics cover the documents, in the corpus and in
the annotated material. Specifically, for a fixed
k, is there a difference in the |Tki ||Di| for annotateddocuments as opposed to the corpus at large? Using
a random sample of 50 documents from EUI corpus,
a 2-tailed t-test yielded p < 0.05, for all k, the
trend being towards a better coverage of the EUI
documents than of the metaphor annotated ones.
We hypothesized that this was due to the large
discrepancy in the lengths of the texts: An average
text in the EUI sample is 432 words long, whereas
the metaphor annotated texts are 775 words long on
average, with the shortest having 343 words. Shorter
texts tend to be less elaborate and more ?to the
point?, with a higher percentage of topical words.
To neutralize the effect of length on topical
coverage, we chose from the EUI sample only
documents that were at least 343 words long,
resulting in 31 documents. Comparing those to the
44 metaphor annotated documents, we found p >
0.37 for every k, i.e. the annotated documents are
indistinguishable in topical coverage from similar-
length documents in the EUI corpus.
3 Experiment
3.1 Summary of notation
V All and only non-stop words containing only
letters that appeared in at least 6 documents in
the collection.
Di All words in document i that are in V.
Tki All words in document i that are in V and are
in the top k words for some topic active in
document i according to LDA output.
Mi All words in document i that are in V and are
marked as metaphors in this document.
3.2 Hypothesis
We hypothesize that words in a given document
that are high-ranking representatives of a common
topic of discussion in a relevant corpus are less
likely to be metaphorical than other words in the
document. That is, such words would contain a
smaller proportion of metaphors than their share in
text. Using the definitions above: For an average
document i and any k, |Tki ||Di| >
|Mi?Tki |
|Mi| .
3.3 Results
As we hypothesized, metaphors are under-
represented in topically used words. Thus, for
k=25, about 15% of the indexed words in the
document are deemed topical, containing about
3% of the metaphorically used indexed words
in that document. For k=400, about 53% of the
indexed words are topical, capturing only 22% of
the metaphors.
4 Discussion
4.1 Metaphors from salient domains
A number of domains singled out by Musolff (2000)
as being recurrent metaphors in the corpus, such
5
0.000.100.20
0.300.400.50
0.60
25 50 100 150 200 250 300 350 400k
Figure 1: As hypothesized, |Tki ||Di| , shown in circles, is
larger than |Mi?Tki ||Mi| , shown in squares, for various k.
as VEHICLE or LOVE, are also things people care
about politically, hence they also correspond to
recurrent topics of discussion (see clusters titled
transport and childcare in table 1). It has been
shown experimentally that the subject?s in-depth
familiarity with the source domain is necessary
for the metaphor to work as intended ? see for
example Gentner and Gentner (1983) work on using
water flow metaphors for electricity. Our results
suggest that participants in political discourse draw
on domains not only familiar in general, but indeed
highly salient in the specific discourse itself.
As a consequence, an extended metaphor from a
discourse-topical domain can be easily mistaken by
the topic detection software for a topical use of the
relevant items. Consider, for example, an extract
from a 19 December 1991 article in Times:
Denis Healey, former Labour Chancellor
of the Exchequer, urged the primeminister
to stop playing Tory party politics with
the negotiations over Europe and drew an
image of Mr Major as a driver. He said:
?I understand that if you are driving a car
and sitting behind you is a lady with a
handbag and a man with fangs, you may
feel it wiser to drive in the slow lane. My
own advice is that he should pull into a
lay-by, turf the others out and then hand
the wheel over to firmer and safer hands.?
LDA considered {drive driving} to belong to
the topic that deals with safety and road accidents,
including in its 200 most likely words {crash
died accidents pedestrians traffic safety cars maps
motorists}, although additional metaphorically used
items from the same semantic domain, such as
lane and wheel, were not among the top 200
representatives of this topic.
It is an intriguing direction for future research
to compare the topical and metaphorical uses of
such domains, in order to determine which aspects
loom large indeed, being both matters of literal
concern and prolific generators of metaphors, and
how these are manipulated for persuasive effects.
The example above suggests that in the British EU-
related discourse in 1990s safety of driving is both
a topic-of-discussion (?Cyclists and pedestrians are
more vulnerable on British roads than anywhere else
in the European Union?, proclaims The Times on 18
February 2000) and a metaphorical axis, stressing
the importance of care and control, the hallmark
of the Euro-sceptic stance towards the European
integration process.
4.2 Topical metaphors
Putting aside topic detector?s mistakes on extended
metaphors from certain domains such as discussed
in the previous section, what do metaphors in the
topical vocabulary look like? The last topic shown
in table 1 has to do with criticism towards EU
bureaucracy, reflecting extensive discussions in the
British media in the late 1990s of alleged corruption
and mismanagement in the European Commission.
Together with the words cited in the table, this topic
lists root as one of its 300 most likely words.
This word shows up as a metaphor in 3 of our test
documents. In two of them it is used precisely in the
context projected by the topic:
In limpid language, whose meaning no
bureaucrat can twist, these four wise
men and one wise woman delivered, to
their great credit, a coruscating indictment
not just of individual commissioners, but
of the entire management and corporate
culture of the European Commission.
They have made an incontestable case, in
Tony Blair?s words, for ?root and branch
reform?.
6
Here, root is used in the root and branch idiom
suggesting a complete change, a reform, which
comes as part of a bundle with severe criticism.
Yet the figurative nature of this expression as a
metaphor from PLANT domain is apparent to naive
readers, making it an instance of imagery routinely
going together with criticism in this corpus. A
related metaphorical sense of root is attested in
similar contexts in the corpus, further explaining its
connection to the topic:
Not unless they insist on credible systems
to hold commissioners and bureaucrats to
account. And not unless they appoint
a new team with a brief not just to
root out malpractices but to shut down
entire programmes, such as tourism and
humanitarian aid, which the Commission
is incompetent to manage and which
should never have been added to its ever-
expanding empire.
A bloodied European Commission looks
likely to cling on to power today after
an eleventh-hour threat to quit by its
President, Jacques Santer, called the bluff
of the European Parliament ... All
week MEPs had been talking up the
?nuclear option? of sacking the full
Commission body over a burgeoning
fraud and nepotism scandal that dates
from 1995 ... Early 1997: Finnish
Commissioner Erkki Liikanen announces
plan to root out nepotism in Commission
and improve financial controls.
In the third document with root metaphor, root
is used in a different environment, and is not
considered topical by LDA:
For at the root of this conflict lies the
German denial that unemployment has
anything to do with cyclical fluctuations
in the economy.
Our quantitative results show that cases such
as root are more an exception than a rule. Yet,
from the perspective of the argumentative use of
metaphors, such cases are instructive of the way
certain metaphors get ?attached? to certain topics of
discussion. In this case, the majority of mentions
of root in this critical context come from Tony
Blair?s expression that was cited and referenced
widely enough to acquire a statistical association
with the discussion of the Commission?s failings
in the corpus. Indeed, the political significance of
Blair?s successful appropriation of the issue was not
lost on the media:
Tony Blair has swiftly positioned himself
as the champion of ?root and branch?
reform. Not to be outdone, William Hague
unveiled a ?10-point plan? for reform
of the Commission, no doubt drawing
on his extensive McKinsey management
expertise.
In future work, we plan to look closely at the
topical metaphors, as they potentially represent
outcomes of leadership battles fought in the media,
and can thus have political consequences.
5 Conclusion
Using metaphor-annotated material that is
sufficiently representative of the topical composition
of a similar-length document in a large background
corpus, we showed that words expressing a
discourse-wide topic of discussion are less likely to
be metaphorical than other words in a document.
This is, to our knowledge, the first quantitative
demonstration of the connection between
metaphoricity of a given word and its role in the
relevant background discourse. It complements the
traditionally ?localistic? outlook on metaphors that
is based on the observation that a metaphorically
used item creates a local incongruity because there
is a violation of a selectional restrictions between
verbs and their arguments (Fass, 1991; Mason,
2004; Gedigian et al, 2006; Birke and Sarkar, 2006)
or in the adjective-noun pairs (Krishnakumaran and
Zhu, 2007). Global discourse-level information
can potentially be used to focus metaphor detectors
operating at the local level on items with higher
metaphoric potential.
Reining and Lo?nneker-Rodman (2007) use
minimal topical information to focus their search
for metaphors. Working with a French-language
7
corpus discussing European politics, Reining and
Lo?nneker-Rodman (2007) proposed harvesting
salient collocates of the lemma Europe, that
represents the main topic of discussion and is
thus hypothesized to be the main target domain
of metaphors in this corpus. Indeed, numerous
instances of metaphors were collected using a
4-word window around the lemma in their corpus.
Our work can be understood as developing a
more nuanced approach to finding the likely target
domains in the corpus ? those words that represent
a topic of discussion rather than the means to
discuss a topic. Thus, it is not just Europe per se
that is the target, but, more specifically, aspects
such as monetary integration, employment, energy,
immigration, transportation, and defense, among
others. Our results suggest that to harvest deliberate
metaphors more effectively, one is advised to
consider words that do not represent a discourse
topic.
References
Nathan Atkinson, David Kaufer, and Suguru Ishizaki.
2008. Presence and Global Presence in Genres of Self-
Presentation: A Framework for Comparative Analysis.
Rhetoric Society Quarterly, 38(3):1?27.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing Disagreements. In COL-
ING 2008 Workshop on Human Judgments in Compu-
tational Linguistics, pages 2?7, Manchester, UK.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of EACL, pages 329?
336.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Resarch, 3:993?1022.
Ari Chanen and Jon Patrick. 2007. Measuring correla-
tion between linguists judgments and Latent Dirichlet
Allocation topics. In Proceedings of the Australasian
Language Technology workshop, pages 13?20, Mel-
bourne, Australia.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Matt Gedigian, John Bryant, Srinivas Narayanan, and
Branimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of NAACL Workshop on Scalable Natural
Language Understanding, pages 41?48.
Deidre Gentner and Donald Gentner. 1983. Flowing wa-
ters or teeming crowds: Mental models of electricity.
In D. Gentner and A. Stevens, editors, Mental models.
Hillsdale, NJ: Lawrence Erlbaum.
Alan Gross and Ray Dearin. 2003. Chaim Perelman.
Albany: SUNY Press.
Eva Feder Kittay. 1987. Metaphor: Its cognitive force
and linguistic structure. Oxford: Calderon Press.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, New York.
Zachary J. Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Chaim Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Wilkin-
son, J. and Weaver, P. (trans). Notre Dame, IN: Uni-
versity of Notre Dame Press.
Astrid Reining and Birte Lo?nneker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings of
the Workshop on Computational Approaches to Figu-
rative Language, pages 5?12, Rochester, New York.
Gerard Steen. 2008. The Paradox of Metaphor: Why
We Need a Three-Dimensional Model of Metaphor.
Metaphor and Symbol, 23(4):213?241.
8
Proceedings of the ACL 2010 Conference Short Papers, pages 253?257,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Vocabulary Choice as an Indicator of Perspective
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
Northwestern University and Washington University in St. Louis
beata,d-diermeier@northwestern.edu, beigman@wustl.edu
Abstract
We establish the following characteris-
tics of the task of perspective classifi-
cation: (a) using term frequencies in a
document does not improve classification
achieved with absence/presence features;
(b) for datasets allowing the relevant com-
parisons, a small number of top features is
found to be as effective as the full feature
set and indispensable for the best achieved
performance, testifying to the existence
of perspective-specific keywords. We re-
late our findings to research on word fre-
quency distributions and to discourse ana-
lytic studies of perspective.
1 Introduction
We address the task of perspective classification.
Apart from the spatial sense not considered here,
perspective can refer to an agent?s role (doctor vs
patient in a dialogue), or understood as ?a par-
ticular way of thinking about something, espe-
cially one that is influenced by one?s beliefs or
experiences,? stressing the manifestation of one?s
broader perspective in some specific issue, or ?the
state of one?s ideas, the facts known to one, etc.,
in having a meaningful interrelationship,? stress-
ing the meaningful connectedness of one?s stances
and pronouncements on possibly different issues.1
Accordingly, one can talk about, say, opinion
on a particular proposed legislation on abortion
within pro-choice or pro-life perspectives; in this
case, perspective essentially boils down to opi-
nion in a particular debate. Holding the issue con-
stant but relaxing the requirement of a debate on a
specific document, we can consider writings from
pro- and con- perspective, in, for example, the
death penalty controversy over a course of a period
of time. Relaxing the issue specificity somewhat,
1Google English Dictionary, Dictionary.com
one can talk about perspectives of people on two
sides of a conflict; this is not opposition or sup-
port for any particular proposal, but ideas about
a highly related cluster of issues, such as Israeli
and Palestinian perspectives on the conflict in all
its manifestations. Zooming out even further, one
can talk about perspectives due to certain life con-
tingencies, such as being born and raised in a par-
ticular culture, region, religion, or political tradi-
tion, such perspectives manifesting themselves in
certain patterns of discourse on a wide variety of
issues, for example, views on political issues in the
Middle East from Arab vs Western observers.
In this article, we consider perspective at all
the four levels of abstraction. We apply the same
types of models to all, in order to discover any
common properties of perspective classification.
We contrast it with text categorization and with
opinion classification by employing models rou-
tinely used for such tasks. Specifically, we con-
sider models that use term frequencies as features
(usually found to be superior for text categoriza-
tion) and models that use term absence/presence
(usually found to be superior for opinion classi-
fication). We motivate our hypothesis that pre-
sence/absence features would be as good as or
better than frequencies, and test it experimentally.
Secondly, we investigate the question of feature
redundancy often observed in text categorization.
2 Vocabulary Selection
A line of inquiry going back at least to Zipf strives
to characterize word frequency distributions in
texts and corpora; see Baayen (2001) for a sur-
vey. One of the findings in this literature is that
a multinomial (called ?urn model? by Baayen)
is not a good model for word frequency distri-
butions. Among the many proposed remedies
(Baayen, 2001; Jansche, 2003; Baroni and Evert,
2007; Bhat and Sproat, 2009), we would like to
draw attention to the following insight articulated
253
most clearly in Jansche (2003). Estimation is im-
proved if texts are construed as being generated by
two processes, one choosing which words would
appear at all in the text, and then, for words that
have been chosen to appear, how many times they
would in fact appear. Jansche (2003) describes a
two-stage generation process: (1) Toss a z-biased
coin; if it comes up heads, generate 0; if it comes
up tails, (2) generate according to F (?), where
F (?) is a negative binomial distribution and z is a
parameter controlling the extent of zero-inflation.
The postulation of two separate processes is
effective for predicting word frequencies, but is
there any meaning to the two processes? The first
process of deciding on the vocabulary, or word
types, for the text ? what is its function? Jansche
(2003) suggests that the zero-inflation component
takes care of the multitude of vocabulary words
that are not ?on topic? for the given text, including
taboo words, technical jargon, proper names. This
implies that words that are chosen to appear are
all ?on topic?. Indeed, text segmentation studies
show that tracing recurrence of words in a text
permits topical segmentation (Hearst, 1997; Hoey,
1991). Yet, if a person compares abortion to infan-
ticide ? are we content with describing this word
as being merely ?on topic,? that is, having a certain
probability of occurrence once the topic of abor-
tion comes up? In fact, it is only likely to occur
if the speaker holds a pro-life perspective, while a
pro-choicer would avoid this term.
We therefore hypothesize that the choice of vo-
cabulary is not only a matter of topic but also
of perspective, while word recurrence has mainly
to do with the topical composition of the text.
Therefore, tracing word frequencies is not going to
be effective for perspective classification beyond
noting the mere presence/absence of words, dif-
ferently from the findings in text categorization,
where frequency-based features usually do better
than boolean features for sufficiently large voca-
bulary sizes (McCallum and Nigam, 1998).
3 Data
Partial Birth Abortion (PBA) debates: We use
transcripts of the debates on Partial Birth Abor-
tion Ban Act on the floors of the US House and
Senate in 104-108 Congresses (1995-2003). Simi-
lar legislation was proposed multiple times, passed
the legislatures, and, after having initially been ve-
toed by President Clinton, was signed into law
by President Bush in 2003. We use data from
278 legislators, with 669 speeches in all. We
take only one speech per speaker per year; since
many serve multiple years, each speaker is repre-
sented with 1 to 5 speeches. We perform 10-fold
cross-validation splitting by speakers, so that all
speeches by the same speaker are assigned to the
same fold and testing is always inter-speaker.
When deriving the label for perspective, it is im-
portant to differentiate between a particular leg-
islation and a pro-choice / pro-life perspective.
A pro-choice person might still support the bill:
?I am pro-choice, but believe late-term abortions
are wrong. Abortion is a very personal decision
and a woman?s right to choose whether to ter-
minate a pregnancy subject to the restrictions of
Roe v. Wade must be protected. In my judgment,
however, the use of this particular procedure can-
not be justified.? (Rep. Shays, R-CT, 2003). To
avoid inconsistency between vote and perspective,
we use data from pro-choice and pro-life non-
governmental organizations, NARAL and NRLC,
that track legislators? votes on abortion-related
bills, showing the percentage of times a legislator
supported the side the organization deems consis-
tent with its perspective. We removed 22 legisla-
tors with a mixed record, that is, those who gave
20-60% support to one of the positions.2
Death Penalty (DP) blogs: We use University
of Maryland Death Penalty Corpus (Greene and
Resnik, 2009) of 1085 texts from a number of pro-
and anti-death penalty websites. We report 4-fold
cross-validation (DP-4) using the folds in Greene
and Resnik (2009), where training and testing data
come from different websites for each of the sides,
as well as 10-fold cross-validation performance on
the entire corpus, irrespective of the site.3
Bitter Lemons (BL): We use the GUEST part
of the BitterLemons corpus (Lin et al, 2006), con-
taining 296 articles published in 2001-2005 on
http://www.bitterlemons.org by more than 200 dif-
ferent Israeli and Palestinian writers on issues re-
lated to the conflict.
Bitter Lemons International (BL-I): We col-
lected 150 documents each by a different per-
2Ratings are from: http://www.OnTheIssues.org/. We fur-
ther excluded data from Rep. James Moran, D-VA, as he
changed his vote over the years. For legislators rated by nei-
ther NRLC nor NARAL, we assumed the vote aligns with the
perspective.
3The 10-fold setting yields almost perfect performance
likely due to site-specific features beyond perspective per se,
hence we do not use this setting in subsequent experiments.
254
son from either Arab or Western perspectives
on Middle Eastern affairs in 2003-2009 from
http://www.bitterlemons-international.org/. The
writers and interviewees on this site are usually
former diplomats or government officials, aca-
demics, journalists, media and political analysts.4
The specific issues cover a broad spectrum, includ-
ing public life, politics, wars and conflicts, educa-
tion, trade relations in and between countries like
Lebanon, Jordan, Iraq, Egypt, Yemen, Morocco,
Saudi Arabia, as well as their relations with the
US and members of the European Union.
3.1 Pre-processing
We are interested in perspective manifestations
using common English vocabulary. To avoid the
possibility that artifacts such as names of senators
or states drive the classification, we use as features
words that contain only lowercase letters, possibly
hyphenated. No stemming is performed, and no
stopwords are excluded.5
Table 1: Summary of corpora
Data #Docs #Features # CV folds
PBA 669 9.8 K 10
BL 296 10 K 10
BL-I 150 9 K 10
DP 1085 25 K 4
4 Models
For generative models, we use two versions
of Naive Bayes models termed multi-variate
Bernoulli (here, NB-BOOL) and multinomial (here,
NB-COUNT), respectively, in McCallum and
Nigam (1998) study of event models for text cate-
gorization. The first records presence/absence of a
word in a text, while the second records the num-
ber of occurrences. McCallum and Nigam (1998)
found NB-COUNT to do better than NB-BOOL for
sufficiently large vocabulary sizes for text catego-
rization by topic. For discriminative models, we
use linear SVM, with presence-absence, norma-
lized frequency, and tfidf feature weighting. Both
types of models are commonly used for text clas-
sification tasks. For example, Lin et al (2006) use
4We excluded Israeli, Turkish, Iranian, Pakistani writers
as not clearly representing either perspective.
5We additionally removed words containing support, op-
pos, sustain, overrid from the PBA data, in order not to in-
flate the performance on perspective classification due to the
explicit reference to the upcoming vote.
NB-COUNT and SVM-NORMF for perspective clas-
sification; Pang et al (2002) consider most and
Yu et al (2008) all of the above for related tasks
of movie review and political party classification.
We use SVMlight (Joachims, 1999) for SVM and
WEKA toolkit (Witten and Frank, 2005; Hall et
al., 2009) for both version of Naive Bayes. Param-
eter optimization for all SVMmodels is performed
using grid search on the training data separately
for each partition into train and test data.6
5 Results
Table 2 summarizes the cross-validation results for
the four datasets discussed above. Notably, the
SVM-BOOL model is either the best or not signif-
icantly different from the best performing model,
although the competitors use more detailed textual
information, namely, the count of each word?s ap-
pearance in the text, either raw (NB-COUNT), nor-
malized (SVM-NORMF), or combined with docu-
ment frequency (SVM-TFIDF).
Table 2: Classification accuracy. Scores sig-
nificantly different from the best performance
(p2t<0.05 on paired t-test) are given an asterisk.
Data NB SVM
BOOL COUNT BOOL NORMF TFIDF
PBA *0.93 0.96 0.96 0.96 0.97
DP-4 0.82 0.82 0.83 0.82 0.727
DP-10 *0.88 *0.93 0.98 *0.97 *0.97
BL 0.89 0.88 0.89 0.86 0.84
BL-I 0.68 0.66 0.73 0.65 0.65
We conclude that there is no evidence for the
relevance of the frequency composition of the
text for perspective classification, for all levels of
venue- and topic-control, from the tightest (PBA
debates) to the loosest (Western vs Arab authors
on Middle Eastern affairs). This result is a clear
indication that perspective classification is quite
different from text categorization by topic, where
count-based features usually perform better than
boolean features. On the other hand, we have not
6Parameter c controlling the trade-off between errors
on training data and margin is optimized for all datasets,
with the grid c = {10?6, 10?5, . . . , 105}. On the DP
data parameter j controlling penalties for misclassification
of positive and negative cases is optimized as well (j =
{10?2, 10?1, . . . , 102}), since datasets are unbalanced (for
example, there is a fold with 27%-73% split).
7Here SVM-TFIDF is doing somewhat better than SVM-
BOOL on one of the folds and much worse on two other folds;
paired t-test with just 4 pairs of observations does not detect
a significant difference.
255
observed that boolean features are reliably better
than count-based features, as reported for the sen-
timent classification task in the movie review do-
main (Pang et al, 2002).
We note the low performance on BL-I, which
could testify to a low degree of lexical consolida-
tion in the Arab vs Western perspectives (more on
this below). It is also possible that the small size of
BL-I leads to overfitting and low accuracies. How-
ever, PBA subset with only 151 items (only 2002
and 2003 speeches) is still 96% classifiable, so size
alone does not explain low BL-I performance.
6 Consolidation of perspective
We explore feature redundancy in perspective
classification.We first investigate retention of only
N best features, then elimination thereof. As a
proxy of feature quality, we use the weight as-
signed to the feature by the SVM-BOOL model
based on the training data. Thus, to get the per-
formance with N best features, we take the N2
highest and lowest weight features, for the posi-
tive and negative classes, respectively, and retrain
SVM-BOOL with these features only.8
Table 3: Consolidation of perspective. Nbest
shows the smallest N and its proportion out of
all features for which the performance of SVM-
BOOL with only the best N features is not sig-
nificantly inferior (p1t>0.1) to that of the full
feature set. No-Nbest shows the largest num-
ber N for which a model without N best fea-
tures is not significantly inferior to the full model.
N={50, 100, 150, . . . , 1000}; for DP and BL-I, ad-
ditionally N={1050, 1100, ..., 1500}; for PBA, ad-
ditionally N={10, 20, 30, 40}.
Data Nbest No-Nbest
N % N %
PBA 250 2.6% 10 <1%
BL 500 4.9% 100 <1%
DP 100 <1% 1250 5.2%
BL-I 200 2.2% 950 11%
We observe that it is generally sufficient to use
a small percentage of the available words to ob-
tain the same classification accuracy as with the
full feature set, even in high-accuracy cases such
as PBA and BL. The effectiveness of a small
subset of features is consistent with the observa-
tion in the discourse analysis studies that rivals
8We experimented with the mutual information based fea-
ture selection as well, with generally worse results.
in long-lasting controversies tend to consolidate
their vocabulary and signal their perspective with
certain stigma words and banner words, that is,
specific keywords used by a discourse commu-
nity to implicate adversaries and to create sym-
pathy with own perspective, respectively (Teubert,
2001). Thus, in abortion debates, using infanti-
cide as a synonym for abortion is a pro-life stigma.
Note that this does not mean the rest of the fea-
tures are not informative for classification, only
that they are redundant with respect to a small per-
centage of top weight features.
When N best features are eliminated, perfor-
mance goes down significantly with even smaller
N for PBA and BL datasets. Thus, top features
are not only effective, they are also crucial for ac-
curate classification, as their discrimination capa-
city is not replicated by any of the other vocabu-
lary words. This finding is consistent with Lin
and Hauptmann (2006) study of perspective vs
topic classification: While topical differences be-
tween two corpora are manifested in difference in
distributions of great many words, they observed
little perspective-based variation in distributions
of most words, apart from certain words that are
preferentially used by adherents of one or the other
perspective on the given topic.
For DP and BL-I datasets, the results seem
to suggest perspectives with more diffused key-
word distribution (No-NBest figures are higher).
We note, however, that feature redundancy exper-
iments are confounded in these cases by either a
low power of the paired t-test with only 4 pairs
(DP) or by a high variance in performance among
the 10 folds (BL-I), both of which lead to nume-
rically large discrepancy in performance that is not
deemed significant, making it easy to ?match? the
full set performance with small-N best features as
well as without large-N best features. Better com-
parisons are needed in order to verify the hypo-
thesis of low consolidation.
In future work, we plan to experiment with ad-
ditional features. For example, Greene and Resnik
(2009) reported higher classification accuracies
for the DP-4 data using syntactic frames in which
a selected group of words appeared, rather than
mere presence/absence of the words. Another di-
rection is exploring words as members of seman-
tic fields ? while word use might be insufficiently
consistent within a perspective, selection of a se-
mantic domain might show better consistency.
256
References
Herald Baayen. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Marco Baroni and Stefan Evert. 2007. Words
and Echoes: Assessing and Mitigating the Non-
Randomness Problem in Word Frequency Distribu-
tion Modeling. In Proceedings of the ACL, pages
904?911, Prague, Czech Republic.
Suma Bhat and Richard Sproat. 2009. Knowing the
Unseen: Estimating Vocabulary Size over Unseen
Samples. In Proceedings of the ACL, pages 109?
117, Suntec, Singapore, August.
Stephan Greene and Philip Resnik. 2009. More
than Words: Syntactic Packaging and Implicit Sen-
timent. In Proceedings of HLT-NAACL, pages 503?
511, Boulder, CO, June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringe, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Michael Hoey. 1991. Patterns of Lexis in Text. Oxford
University Press.
Martin Jansche. 2003. Parametric Models of Linguis-
tic Count Data. In Proceedings of the ACL, pages
288?295, Sapporo, Japan, July.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspec-
tives? A test of different perspectives based on sta-
tistical distribution divergence. In Proceedings of
the ACL, pages 1057?1064, Morristown, NJ, USA.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL, pages
109?116, Morristown, NJ, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proceedings of AAAI-98 Workshop
on Learning for Text Categorization, pages 41?48,
Madison, WI, July.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of
EMNLP, Philadelphia, PA, July.
Wolfgang Teubert. 2001. A Province of a Federal
Superstate, Ruled by an Unelected Bureaucracy ?
Keywords of the Euro-Sceptic Discourse in Britain.
In Andreas Musolff, Colin Good, Petra Points, and
Ruth Wittlinger, editors, Attitudes towards Europe:
Language in the unification process, pages 45?86.
Ashgate Publishing Ltd, Hants, England.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2 edition.
Bei Yu, Stefan Kaufmann, and Daniel Diermeier.
2008. Classifying party affiliation from political
speech. Journal of Information Technology and Pol-
itics, 5(1):33?48.
257
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 2?7
Manchester, August 2008
Analyzing Disagreements
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
Kellogg School of Business
Northwestern University
{beata,e-beigman,d-diermeier}@northwestern.edu
Abstract
We address the problem of distinguishing
between two sources of disagreement in
annotations: genuine subjectivity and slip
of attention. The latter is especially likely
when the classification task has a default
class, as in tasks where annotators need to
find instances of the phenomenon of inter-
est, such as in a metaphor detection task
discussed here. We apply and extend a data
analysis technique proposed by Beigman
Klebanov and Shamir (2006) to first dis-
till reliably deliberate (non-chance) anno-
tations and then to estimate the amount of
attention slips vs genuine disagreement in
the reliably deliberate annotations.
1 Introduction
Classification tasks fall into two broad categories.
Those in the first category proceed by requiring
that every item is explicitly assigned a tag out of
a given set of tags; part-of-speech tagging is an
example (Santorini, 1990).
In the second group of tasks, the annotator is
asked to identify a phenomenon of interest, thus
implicitly classifying items as belonging to the
phenomenon (marked) and not belonging to it (left
unmarked). When the studied phenomenon is ex-
pected to have low incidence, this is a time-saving
strategy, as annotators do not need to bother with
explicitly marking (almost) everything as a non-
phenomenon. A recent example of such a task is
Beigman Klebanov and Shamir (2006), where an-
notators were asked to provide anchors for words
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
deemed anchored in the text (i.e. associatively
connected to a previous item in the text), thus leav-
ing words that did not receive an anchor implic-
itly marked as un-anchored. Psychological exper-
iments where people are asked to respond to the
occurrence of a given phenomenon can also be
viewed as implicit classifications; for example, see
Spiro?s (2007) work on identification of bound-
aries of musical phrases by listeners. The task
of metaphor detection discussed in this paper also
falls under the implicit classification category.
While such a strategy uses annotators? time effi-
ciently, some of the observed disagreements could
be due to an annotator missing an occurrence of
the relevant phenomenon, rather than genuinely
disagreeing on the matter of occurrence.
We show in section 2 that our metaphor
identification task features less-than-perfect inter-
annotator agreement. Section 3 uses Beigman Kle-
banov and Shamir?s (2006) methodology to find
annotations that can be reliably attributed to a de-
liberate decision by at least some of the annotators.
We then discuss the use of validation experiment to
distinguish between slips of attention and genuine
disagreements (sections 4,5).
2 Metaphor Detection Study
For a project studying the use of metaphors in pub-
lic discourse, a dataset of 151 articles from the
British press was subjected to annotation.1 Partic-
ipants were asked to mark paragraphs that contain
occurrences of metaphors from LOVE, VEHICLE,
AUTHORITY and BUILDING domains (hence-
forth, metaphor types).
For example, the following paragraph in 20
September 1992 issue of Sunday Times contains an
1This is part of the data discussed in (Musolff, 2000).
2
extended metaphor from the VEHICLE domain:
Thatcher warned EC leaders to stop their
endless round of summits and take no-
tice of their own people. ?There is a
fear that the European train will thunder
forward, laden with its customary cargo
of gravy, towards a destination nei-
ther wished for nor understood by elec-
torates. But the train can be stopped,?
she said.
The title2 of one of the articles in the 19 Octo-
ber 1999 issue of The Guardian contains a LOVE
metaphor:
Euro-flirting is not only a matter of de-
sire.
The discussion in this paper is based on the out-
put of 9 annotators who performed metaphor iden-
tification (henceforth, production task), and of 7
annotators (out of 9) who took part in the sub-
sequent validation study (henceforth, validation
task). Subjects were not told about validation until
after they finished production on the whole of the
dataset. A time gap of 2 weeks existed between
the end of the production study and the start of
the validation, each of the tasks taking 6 weeks,
in weekly installments of 25 articles each.
For the production task, the annotators were
instructed to mark every paragraph where a
metaphor from the given metaphor type appeared;
the 151-article dataset yields 2364 paragraphs.
This paradigm corresponds to the implicit clas-
sification task discussed earlier, in that only the
positive (metaphor-containing) cases are given an
explicit markup. The incidence of positive cases
is quite low ? VEHICLE, the most ubiquitous
type, featured in 4% of the paragraphs, on average
across annotators.
We note that the appearances of the different
metaphor types are not mutually exclusive, and,
indeed, there is no a-priori reason to suppose
any relationship between them. For example, the
following paragraph from the leading article in
15 November 1995 issue of The Guardian was
marked by some annotators as containing both
LOVE and VEHICLE metaphors:
The first European bank notes - proba-
bly to be called ?euros? - will not be in
2A title is treated as a paragraph in our annotations.
circulation until 2002 judging by yester-
day?s report from the European Mone-
tary Institute. But this doesn?t mean that
monetary union has been delayed be-
yond 1999 because the printing of Euro-
pean bank notes will have been preceded
by a period of three years when na-
tional currencies will have been locked
together in indissoluble monetary matri-
mony [...] Although France looks as if it
might buckle under the strain of meet-
ing the fiscal criteria and in Germany
the SDP is having doubts (though only
about whether the new currency will be
strong enough) the Maastricht train is
still theoretically on the rails. Nobody
has changed the timetable.
We therefore treat the detection of metaphors
from each metaphor type as a separate binary
classification task. Table 1 shows the inter-
annotator agreement for the production task using
the ? statistic (Carletta, 1996; Krippendorff, 1980;
Siegel and Castellan, 1988).
Table 1: Metaphor annotation data (production),
by metaphor type. The third column shows the
percentage of paragraphs (out of 2364) marked as
having a metaphor of the given type, on average
across 9 annotators.
Type ? marked
VEHICLE 0.66 4.0%
LOVE 0.66 2.5%
AUTHORITY 0.39 2.7%
BUILD 0.43 1.7%
Clearly, it is not the case that the whole of the
dataset was reliably annotated, even for the better-
agreed-upon metaphor types like VEHICLE and
LOVE. Hence, additional procedures are needed
to distill reliable annotations. We apply Beigman
Klebanov and Shamir?s (2006) statistical tech-
nique to find a subset of the data that is sufficiently
reliable, and later corroborate the statistical analy-
sis through the validation task.
3 Reliably Deliberate Annotations
In Beigman Klebanov and Shamir (2006), 22
subjects performed the anchoring annotation; the
overall inter-annotator agreement was ?=0.45.
3
Thus, some of the data was clearly unreliable, as
in our metaphor detection task, but the possibility
existed that some other part was in fact annotated
sufficiently reliably.
Beigman Klebanov and Shamir?s (2006) analy-
sis proceeded thus: Suppose each of the 20 anno-
tators3 (i = 1...20) was flipping a coin with the
probability of heads p
i
equal to the proportion of
?anchored? markups in annotator i?s data. What
is the level of agreement for which this scenario is
sufficiently improbable? For their data, the random
anchoring hypothesis could be rejected with 99%
confidence for cases marked by at least 13 people.
Items featuring at least this level of agreement can
be considered, with high probability, as deliber-
ately annotated as ?anchored?, as at least some of
those who marked them were not flipping a coin.
Following the procedure in Beigman Klebanov
and Shamir (2006), we wish to determine a re-
liably deliberate subset of our metaphor annota-
tions. We induce 9 random pseudo-annotators
from the 9 actual ones, each marking paragraphs
at random as containing a metaphor of a given
type or not. Pseudo-annotator i flips a coin
with p(heads) = p
i
, which is the proportion of
metaphor markups by the i?th annotator for the
most common metaphor type (VEHICLE).
Assuming each annotator flips her coin, we cal-
culate the probability of 3 or more coins coming up
heads simultaneously;4 this probability is 0.0045.
Thus, with 99.5% confidence, a metaphor markup
by at least 3 people is not a result of coinflip, at
least for some of the annotators. We note, how-
ever, that 99.5% confidence is insufficient for our
case: It allows for random highly agreed markup
in 0.5% of the instances. Given that only up to
4% of the instances have positive markups, this
would yield a high percentage of random items
in the positive instances. The probability of 4 or
more pseudo-annotators having their coins come
up heads simultaneously is below 0.0003; we con-
sider this sufficient confidence for our case, and
regard metaphor markups produced by at least 4
people as reliably deliberate.
Note that we cannot find a similar threshold for
no-metaphor annotations, as a lack of metaphor
3Two people were excluded as outliers.
4In Beigman Klebanov and Shamir (2006), a normal ap-
proximation is used to handle collective decision making by
20 pseudo-annotators. In the current case, 9 annotators is a
sufficiently small number to allow an exact probability calcu-
lation over the 512 possibilities.
annotation could happen by chance with a high
probability (p = 0.69). In view of the potential use
of the dataset for evaluating metaphor detection
algorithms, a putative metaphor suggested by the
algorithm cannot be rejected based on the lack of
metaphor annotation in the data. A complementary
procedure would be needed, for example, collect-
ing human judgments for the putative metaphors
separately.
4 Attention Slips vs Genuine
Disagreements
Deliberate annotation does not guarantee agree-
ment. It remained the case that some of the reliably
deliberate data in Beigman Klebanov and Shamir
(2006) was actually produced by only some of the
original subjects. Indeed, some of the deliberately
marked metaphors were annotated by only 4 out
of the 9 participants. For cases where the posi-
tive annotations were produced deliberately, what
is the status of negative annotations accorded to
the same items? Were these mere attention slips,
or genuine differences of opinion? Note that this
question cannot be meaningfully posed regarding
the parts of annotations for which the hypothesis
of random positive marking could not be rejected
with sufficiently high probability, since, obviously,
apparent disagreements there could be simply a re-
sult of different coinflip outcomes.
Beigman Klebanov and Shamir (2006) hypoth-
esized that dissenting annotations of the reliable
pairs would be cases of attention slips, rather than
genuine differences of opinion. In other words,
while there was no initial agreement, these items
were potentially agreeable. To test the hypothe-
sis, they devised a validation experiment, where
subjects were presented with all pairs marked by
at least one annotator, plus some random pairs,
and were asked to cross out things they disagree
with. The reasoning was as follows: If attention
slip was the cause for a dissenting negative anno-
tation, when the subject is asked about the relevant
item, i.e. it is explicitly brought to her attention,
she would accept it, whereas if a case is that of
a genuine disagreement, she would reject it. To
control for the possibility that people just accept
everything so that not to be dissonant with others,
some random annotations were also included.
The results reported by Beigman Klebanov and
Shamir (2006) largely bore out the hypothesis.
First, people did not tend to accept everything,
4
as only 15% of judgments of random annota-
tions and only 62% of judgments on all human-
generated annotations were ?accept? judgments.
However, 94% of judgments of the reliable anno-
tations were ?accept? judgments. Hence, the rate
of genuine disagreement on the reliably deliberate
part of Beigman Klebanov and Shamir?s (2006)
data turned out to be quite low.
We are interested in estimating the degree of
genuine disagreements in metaphor production.
Using Beigman Klebanov and Shamir?s method-
ology, we collected all paragraphs marked as con-
taining a metaphor of a given type by at least one
of the 9 annotators, plus added random markups.
This data was submitted to 7 subjects for valida-
tion.
Table 2: Percentage of ?Accept? validations for
random (Rand) and human (Hum) metaphor pro-
duction data, as well as for the partition of the
human data into reliably deliberate (Rel) and unre-
liable (URel) subsets. For each subset, the number
of data instances covered by the subset is shown.
Subscripts indicate metaphor type: (V)EHICLE,
(L)OVE, (A)UTHORITY, (B)UILD. The bottom
line shows the average over metaphor types.
Subset # Acc Subset # Acc
Rand
V
94 5% Hum
V
194 73%
Rand
L
56 6% Hum
L
137 64%
Rand
A
62 12% Hum
A
258 51%
Rand
B
40 1% Hum
B
126 68%
Rand 252 6% Hum 715 62%
Subset # Acc Subset # Acc
URel
V
92 49% Rel
V
102 94%
URel
L
81 43% Rel
L
56 95%
URel
A
218 42% Rel
A
40 96%
URel
B
86 55% Rel
B
40 96%
URel 477 46% Rel 238 95%
Table 2 reports the percentage of ?accept? votes
for random and human metaphor production data,
as well as for reliably deliberate and unreliable
subsets of the human data. As in Beigman Kle-
banov and Shamir?s case, the validation experi-
ment clearly distinguishes between random, hu-
man in general, and reliably deliberate subsets, and
puts the estimated degree of genuine disagreement
in metaphor identification at 5% on average, with
little variation across the metaphor types. That
is, given that, with high probability, at least some
humans deliberately identified a paragraph as con-
taining a metaphor, the chance for its rejection is
about 5%. The rest of observed production dis-
agreements, for the reliably deliberate subset, are
remedied at validation time, thus probably consti-
tuting attention slips during production. The reli-
ably deliberate subset contains 33% (238/715) of
all human-generated data.
5 Separating self and others
One potential confounder in the above analysis
is conflation of self-consistency with affirmation
of someone else?s annotations. It is possible that
many of the validation-time ?accept? votes are
cases of people accepting their own earlier annota-
tion; the proportion of such cases is expected to in-
crease the more people marked the metaphor dur-
ing production. Therefore, to get a more precise
estimate of the degree of genuine disagreement,
we control for self-affirmation, and calculate the
proportion of ?accept? validations in cases where
the person did not mark the metaphor during pro-
duction. Specifically, if X of the 7 people who par-
ticipated in both production and validation marked
the metaphor at production,5 we check the split of
the remaining 7-X votes during validation. Table 3
presents average other-affirmation rates for the re-
liably deliberate and unreliable human produced
data. Note that only 184 out of the 238 deliberately
reliable cases can be used, as the remaining 54 are
cases where all 7 annotators produced the markup,
so there is no disagreement.
Table 3: Percentage of ?Accept? validations for re-
liably deliberate (Rel) and unreliable (URel) sub-
sets of the metaphor production data, given that the
subject himself did NOT produce the metaphor.
Subset # Acc Subset # Acc
URel
V
92 44% Rel
V
78 90%
URel
L
81 39% Rel
L
38 92%
URel
A
218 35% Rel
A
30 91%
URel
B
86 53% Rel
B
38 91%
URel 477 41% Rel 184 91%
5The actual total of the production annotations could be up
to X+2, as there were 2 more annotators in production than
in validation.
5
According to the table, 91% cases of disagree-
ments in the reliably deliberate data are remedied
at validation time. That is, given that, with high
probability, at least some human deliberately iden-
tified a paragraph as containing a metaphor, the
chance for its rejection by a human who initially
apparently disagreed with the annotation is only
about 9%.
Finally, validation data allows an investigation
of the stability of people?s judgments by calculat-
ing self-rejection rates, i.e. estimating the prob-
ability of rejecting during validation an instance
that the same annotator marked as containing a
metaphor during production. Table 4 shows the
results.
Table 4: Percentage of ?Reject? validations for re-
liably deliberate (Rel) and unreliable (URel) sub-
sets of the metaphor production data, given that the
subject himself produced the annotation.
Subset # Rej Subset # Rej
URel
V
72 25% Rel
V
102 4%
URel
L
55 26% Rel
L
56 5%
URel
A
198 22% Rel
A
40 2%
URel
B
60 23% Rel
B
40 2%
URel 3856 23% Rel 238 4%
For the reliably deliberate data, i.e. cases where
at least 4 people produced the markup, the average
self-rejection rate is 4%. This low figure further
supports the designation of the reliably deliberate
subset as such, i.e. containing stable annotations,
as in 96% of the cases a person who produced the
markup is likely to re-affirm it when asked again,
even after a substantial time delay.7
For the ?unreliable? data, i.e. cases where only
one or two people marked the metaphor during
production, the average self-rejection rate is 23%.
Self-rejection means either that the initial positive
markup was a mistake, or that it is difficult for the
annotator to make up his mind about the annota-
tion of the item. In any case, high self-rejection
6Note that only 385 of the 477 items in the unreliable data
could be used for the calculation. The remaining items were
not produced by any of the 7 people who participated in both
production and validation, but only by one or both of the 2
additional production-task annotators.
7The time difference between production and validation
per article ranged between 4 and 8 weeks, due to differences
in the order in which the different subjects were given the
articles.
rate means that the relevant production annotations
cannot be trusted to contain a settled judgment that
could be then agreed or disagreed with by other an-
notators, or indeed replicated by a computational
model.
We consider self-rejected cases potential indica-
tors of a difficulty on the annotator?s part to de-
cide on the correct markup. We plan a more de-
tailed investigation of the materials to see whether
these cases exhibit any interesting common prop-
erties that could help characterize the difficulties in
metaphor identification task.
6 Conclusion
In this article, we showed an application of
Beigman Klebanov and Shamir?s (2006) method-
ology for analyzing annotation data to metaphor
identification annotations. The approach allowed
establishing an agreement threshold beyond which
the annotations are reliably deliberate, in the sense
that, with high probability, at least some of the
annotators who detected a metaphor were not flip-
ping a coin. This threshold is agreement of 4 out
of 9 annotators, for 99.9% reliability.
To investigate the nature of disagreements in the
reliably deliberate subset, we followed Beigman
Klebanov and Shamir (2006) in conducting a val-
idation study, where subjects were asked to ac-
cept or reject markups produced during the ini-
tial annotation study, as well as some random
annotations. Sharpening the methodology some-
what, we showed that in 91% of reliably deliber-
ate cases where an annotator did not produce the
markup himself, he accepted it during validation.
Hence, the bulk of the initial disagreements were
amended during validation, with the residual 9%
being likely locations for genuine difference of
opinion.
Further analysis of validation data revealed that
the reliably deliberate subset features low self-
rejection rates, meaning that people are consis-
tent with their own production. This was not the
case for the subset deemed unreliable during sta-
tistical analysis, where a 23% self-rejection rate
was observed. We hypothesize that some of these
would be hard-to-decide cases with respect to the
metaphor identification task, and hence warrant a
closer look in order to characterize annotator diffi-
culties with the task.
6
7 Acknowledgment
We would like to thank an anonymous reviewer
for a thorough and insightful review that helped as
improve this article significantly.
References
Beigman Klebanov, Beata and Eli Shamir. 2006.Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109?126.
Carletta, Jean. 1996. Assessing agreement on clas-
sification tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Krippendorff, Klaus. 1980. Content Analysis. SagePublications.
Musolff, Andreas. 2000. Mirror images of Europe:
Metaphors in the public debate about Europe in
Britain and Germany. Mu?nchen: Iudicium.
Santorini, Beatrice. 1990. Part-of-speechtagging guidelines for the Penn Tree-
bank project (3rd revision, 2nd printing).ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz.
Siegel, Sidney and John Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill Book Company.
Spiro, Neta. 2007. What contributes to the percep-
tion of musical phrases in Western classical music?Ph.D. thesis, University of Amsterdam, The Nether-
lands.
7
