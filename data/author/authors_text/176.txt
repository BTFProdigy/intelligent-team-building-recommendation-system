Using Conditional Random Fields to Predict Pitch Accents in
Conversational Speech
Michelle L. Gregory
Linguistics Department
University at Buffalo
Buffalo, NY 14260
mgregory@buffalo.edu
Yasemin Altun
Department of Computer Science
Brown University
Providence, RI 02912
altun@cs.brown.edu
Abstract
The detection of prosodic characteristics is an im-
portant aspect of both speech synthesis and speech
recognition. Correct placement of pitch accents aids
in more natural sounding speech, while automatic
detection of accents can contribute to better word-
level recognition and better textual understanding.
In this paper we investigate probabilistic, contex-
tual, and phonological factors that influence pitch
accent placement in natural, conversational speech
in a sequence labeling setting. We introduce Con-
ditional Random Fields (CRFs) to pitch accent pre-
diction task in order to incorporate these factors ef-
ficiently in a sequence model. We demonstrate the
usefulness and the incremental effect of these fac-
tors in a sequence model by performing experiments
on hand labeled data from the Switchboard Corpus.
Our model outperforms the baseline and previous
models of pitch accent prediction on the Switch-
board Corpus.
1 Introduction
The suprasegmental features of speech relay critical
information in conversation. Yet, one of the ma-
jor roadblocks to natural sounding speech synthe-
sis has been the identification and implementation
of prosodic characteristics. The difficulty with this
task lies in the fact that prosodic cues are never ab-
solute; they are relative to individual speakers, gen-
der, dialect, discourse context, local context, phono-
logical environment, and many other factors. This is
especially true of pitch accent, the acoustic cues that
make one word more prominent than others in an
utterance. For example, a word with a fundamen-
tal frequency (f0) of 120 Hz would likely be quite
prominent in a male speaker, but not for a typical fe-
male speaker. Likewise, the accent on the utterance
?Jon?s leaving.? is critical in determining whether
it is the answer to the question ?Who is leaving??
(?JON?s leaving.?) or ?What is Jon doing?? (?Jon?s
LEAVING.?). Accurate pitch accent prediction lies
in the successful combination of as many of the con-
textual variables as possible. Syntactic information
such as part of speech has proven to be a success-
ful predictor of accentuation (Hirschberg, 1993; Pan
and Hirschberg, 2001). In general, function words
are not accented, while content words are. Vari-
ous measures of a word?s informativeness, such as
the information content (IC) of a word (Pan and
McKeown, 1999) and its collocational strength in a
given context (Pan and Hirschberg, 2001) have also
proven to be useful models of pitch accent. How-
ever, in open topic conversational speech, accent is
very unpredictable. Part of speech and the infor-
mativeness of a word do not capture all aspects of
accentuation, as we see in this example taken from
Switchboard, where a function word gets accented
(accented words are in uppercase):
I, I have STRONG OBJECTIONS to THAT.
Accent is also influenced by aspects of rhythm
and timing. The length of words, in both number
of phones and normalized duration, affect its likeli-
hood of being accented. Additionally, whether the
immediately surrounding words bear pitch accent
also affect the likelihood of accentuation. In other
words, a word that might typically be accented may
be unaccented because the surrounding words also
bear pitch accent. Phrase boundaries seem to play
a role in accentuation as well. The first word of in-
tonational phrases (IP) is less likely to be accented
while the last word of an IP tends be accented. In
short, accented words within the same IP are not in-
dependent of each other.
Previous work on pitch accent prediction, how-
ever, neglected the dependency between labels. Dif-
ferent machine learning techniques, such as deci-
sion trees (Hirschberg, 1993), rule induction sys-
tems (Pan and McKeown, 1999), bagging (Sun,
2002), boosting (Sun, 2002) have been used in a
scenario where the accent of each word is pre-
dicted independently. One exception to this line
of research is the use of Hidden Markov Models
(HMM) for pitch accent prediction (Pan and McK-
eown, 1999; Conkie et al, 1999). Pan and McKe-
own (1999) demonstrate the effectiveness of a se-
quence model over a rule induction system, RIP-
PER, that treats each label independently by show-
ing that HMMs outperform RIPPER when the same
variables are used.
Until recently, HMMs were the predominant for-
malism to model label sequences. However, they
have two major shortcomings. They are trained
non-discriminatively using maximum likelihood es-
timation to model the joint probability of the ob-
servation and label sequences. Also, they require
questionable independence assumptions to achieve
efficient inference and learning. Therefore, vari-
ables used in Hidden Markov models of pitch ac-
cent prediction have been very limited, e.g. part of
speech and frequency (Pan and McKeown, 1999).
Discriminative learning methods, such as Maximum
Entropy Markov Models (McCallum et al, 2000),
Projection Based Markov Models (Punyakanok and
Roth, 2000), Conditional Random Fields (Lafferty
et al, 2001), Sequence AdaBoost (Altun et al,
2003a), Sequence Perceptron (Collins, 2002), Hid-
den Markov Support Vector Machines (Altun et
al., 2003b) and Maximum-Margin Markov Net-
works (Taskar et al, 2004), overcome the limita-
tions of HMMs. Among these methods, CRFs is
the most common technique used in NLP and has
been successfully applied to Part-of-Speech Tag-
ging (Lafferty et al, 2001), Named-Entity Recog-
nition (Collins, 2002) and shallow parsing (Sha and
Pereira, 2003; McCallum, 2003).
The goal of this study is to better identify which
words in a string of text will bear pitch accent.
Our contribution is two-fold: employing new pre-
dictors and utilizing a discriminative model. We
combine the advantages of probabilistic, syntactic,
and phonological predictors with the advantages of
modeling pitch accent in a sequence labeling setting
using CRFs (Lafferty et al, 2001).
The rest of the paper is organized as follows: In
Section 2, we introduce CRFs. Then, we describe
our corpus and the variables in Section 3 and Sec-
tion 4. We present the experimental setup and report
results in Section 5. Finally, we discuss our results
(Section 6) and conclude (Section 7).
2 Conditional Random Fields
CRFs can be considered as a generalization of lo-
gistic regression to label sequences. They define
a conditional probability distribution of a label se-
quence y given an observation sequence x. In this
paper, x = (x1, x2, . . . , xn) denotes a sentence of
length n and y = (y1, y2, . . . , yn) denotes the la-
bel sequence corresponding to x. In pitch accent
prediction, xt is a word and yt is a binary label de-
noting whether xt is accented or not.
CRFs specify a linear discriminative function F
parameterized by ? over a feature representation of
the observation and label sequence ?(x,y). The
model is assumed to be stationary, thus the feature
representation can be partitioned with respect to po-
sitions t in the sequence and linearly combined with
respect to the importance of each feature ?k, de-
noted by ?k. Then the discriminative function can
be stated as in Equation 1:
F (x,y; ?) =
?
t
??,?t(x,y)? (1)
Then, the conditional probability is given by
p(y|x; ?) = 1Z(x,?)F (x,y; ?) (2)
where Z(x,?) = ?y? F (x, y?; ?) is a normaliza-
tion constant which is computed by summing over
all possible label sequences y? of the observation se-
quence x.
We extract two types of features from a sequence
pair:
1. Current label and information about the obser-
vation sequence, such as part-of-speech tag of
a word that is within a window centered at the
word currently labeled, e.g. Is the current word
pitch accented and the part-of-speech tag of
the previous word=Noun?
2. Current label and the neighbors of that label,
i.e. features that capture the inter-label depen-
dencies, e.g. Is the current word pitch accented
and the previous word not accented?
Since CRFs condition on the observation se-
quence, they can efficiently employ feature repre-
sentations that incorporate overlapping features, i.e.
multiple interacting features or long-range depen-
dencies of the observations, as opposed to HMMs
which generate observation sequences.
In this paper, we limit ourselves to 1-order
Markov model features to encode inter-label de-
pendencies. The information used to encode the
observation-label dependencies is explained in de-
tail in Section 4.
In CRFs, the objective function is the log-loss of
the model with ? parameters with respect to a train-
ing set D. This function is defined as the negative
sum of the conditional probabilities of each training
label sequence yi, given the observation sequence
xi, where D ? {(xi,yi) : i = 1, . . . ,m}. CRFs are
known to overfit, especially with noisy data if not
regularized. To overcome this problem, we penalize
the objective function by adding a Gaussian prior
(a term proportional to the squared norm ||?||2) as
suggested in (Johnson et al, 1999). Then the loss
function is given as:
L(?;D) = ?
m
?
i
log p(yi|xi; ?) +
1
2c||?||
2
= ?
m
?
i
F (xi,yi; ?) + logZ(xi,?)
+ 12c||?||
2 (3)
where c is a constant.
Lafferty et al (2001), proposed a modification
of improved iterative scaling for parameter estima-
tion in CRFs. However, gradient-based methods
have often found to be more efficient for minimizing
Equation 3 (Minka, 2001; Sha and Pereira, 2003).
In this paper, we use the conjugate gradient descent
method to optimize the above objective function.
The gradients are computed as in Equation 4:
??L =
m
?
i
?
t
Ep[?t(xi,y)] ? ?t(xi,yi)
+ c? (4)
where the expectation is with respect to all possi-
ble label sequences of the observation sequence xi
and can be computed using the forward backward
algorithm.
Given an observation sequence x, the best label
sequence is given by:
y? = arg max
y
F (x,y; ??) (5)
where ?? is the parameter vector that minimizes
L(?;D). The best label sequence can be identified
by performing the Viterbi algorithm.
3 Corpus
The data for this study were taken from the Switch-
board Corpus (Godfrey et al, 1992), which con-
sists of 2430 telephone conversations between adult
speakers (approximately 2.4 million words). Partic-
ipants were both male and female and represented
all major dialects of American English. We used a
portion of this corpus that was phonetically hand-
transcribed (Greenberg et al, 1996) and segmented
into speech boundaries at turn boundaries or pauses
of more than 500 ms on both sides. Fragments con-
tained seven words on average. Additionally, each
word was coded for probabilistic and contextual
information, such as word frequency, conditional
probabilities, the rate of speech, and the canonical
pronunciation (Fosler-Lussier and Morgan, 1999).
The dataset used in all analysis in this study con-
sists of only the first hour of the database, comprised
of 1,824 utterances with 13,190 words. These utter-
ances were hand coded for pitch accent and intona-
tional phrase brakes.
3.1 Pitch Accent Coding
The utterances were hand labeled for accents and
boundaries according to the Tilt Intonational Model
(Taylor, 2000). This model is characterized by a
series of intonational events: accents and bound-
aries. Labelers were instructed to use duration, am-
plitude, pausing information, and changes in f0 to
identify events. In general, labelers followed the ba-
sic conventions of EToBI for coding (Taylor, 2000).
However, the Tilt coding scheme was simplified.
Accents were coded as either major or minor (and
some rare level accents) and breaks were either ris-
ing or falling. Agreement for the Tilt coding was
reported at 86%. The CU coding also used a simpli-
fied EToBI coding scheme, with accent types con-
flated and only major breaks coded. Accent and
break coding pair-wise agreement was between 85-
95% between coders, with a kappa ? of 71%-74%
where ? is the difference between expected agree-
ment and actual agreement.
4 Variables
The label we were predicting was a binary distinc-
tion of accented or not. The variables we used for
prediction fall into three main categories: syntac-
tic, probabilistic variables, which include word fre-
quency and collocation measures, and phonological
variables, which capture aspects of rhythm and tim-
ing that affect accentuation.
4.1 Syntactic variables
The only syntactic category we used was a four-
way classification for hand-generated part of speech
(POS): Function, Noun, Verb, Other, where Other
includes all adjectives and adverbs1 . Table 1 gives
the percentage of accented and unaccented items by
POS.
1We also tested a categorization of 14 distinct part of speech
classes, but the results did not improve, so we only report on the
four-way classification.
Accented Unaccented
Function 21% 79%
Verb 59% 41%
Noun 30% 70%
Other 49% 51%
Table 1: Percentage of accented and unaccented
items by POS.
Variable Definition Example
Unigram log p(wi) and, I
Bigram log p(wi|wi?1) roughing it
Rev Bigram log p(wi|wi+1) rid of
Joint log p(wi?1, wi) and I
Rev Joint log p(wi, wi+1) and I
Table 2: Definition of probabilistic variables.
4.2 Probabilistic variables
Following a line of research that incorporates the
information content of a word as well as collo-
cation measures (Pan and McKeown, 1999; Pan
and Hirschberg, 2001) we have included a number
of probabilistic variables. The probabilistic vari-
ables we used were the unigram frequency, the pre-
dictability of a word given the preceding word (bi-
gram), the predictability of a word given the follow-
ing word (reverse bigram), the joint probability of a
word with the preceding (joint), and the joint prob-
ability of a word with the following word (reverse
joint). Table 2 provides the definition for these,
as well as high probability examples from the cor-
pus (the emphasized word being the current target).
Note all probabilistic variables were in log scale.
The values for these probabilities were obtained
using the entire 2.4 million words of SWBD2. Table
3 presents the Spearman?s rank correlation coeffi-
cient between the probabilistic measures and accent
(Conover, 1980). These values indicate the strong
correlation of accents to the probabilistic variables.
As the probability increases, the chance of an accent
decreases. Note that all values are significant at the
p < .001 level.
We also created a combined part of speech and
unigram frequency variable in order to have a vari-
able that corresponds to the variable used in (Pan
2Our current implementation of CRF only takes categorical
variables, thus for the experiments, all probabilistic variables
were binned into 5 equal categories. We also tried more bins
and produced similar results, so we only report on the 5-binned
categories. We computed correlations between pitch accent and
the original 5 variables as well as the binned variables and they
are very similar.
Variables Spearman?s ?
Unigram -.451
Bigram -.309
Reverse Bigram -.383
Joint -.207
Reverse joint -.265
Table 3: Spearman?s correlation values for the prob-
abilistic measures.
and McKeown, 1999).
4.3 Phonological variables
The last category of predictors, phonological vari-
ables, concern aspects of rhythm and timing of an
utterance. We have two main sources for these vari-
ables: those that can be computed solely from a
string of text (textual), and those that require some
sort of acoustic information (acoustic). Sun (2002)
demonstrated that the number of phones in a syl-
lable, the number of syllables in a word, and the
position of a word in a sentence are useful predic-
tors of which syllables get accented. While Sun was
concerned with predicting accented syllables, some
of the same variables apply to word level targets as
well. For our textual phonological features, we in-
cluded the number of syllables in a word and the
number of phones (both in citation form as well as
transcribed form). Instead of position in a sentence,
we used the position of the word in an utterance
since the fragments do not necessarily correspond
to sentences in the database we used. We also made
use of the utterance length. Below is the list of our
textual features:
? Number of canonical syllables
? Number of canonical phones
? Number of transcribed phones
? The length of the utterance in number of words
? The position of the word in the utterance
The main purpose of this study is to better pre-
dict which words in a string of text receive accent.
So far, all of our predictors are ones easily com-
puted from a string of text. However, we have in-
cluded a few variables that affect the likelihood of
a word being accented that require some acoustic
data. To the best of our knowledge, these features
have not been used in acoustic models of pitch ac-
cent prediction. These features include the duration
of the word, speech rate, and following intonational
phrase boundaries. Given the nature of the SWBD
corpus, there are many disfluencies. Thus, we also
Feature ?2 Sig
canonical syllables 1636 p < .001
canonical phones 2430 p < .001
transcribed phones 2741 p < .001
utt length 80 p < .005
utt position 295 p < .001
duration 3073 p < .001
speech rate 101 p < .001
following pause 27 p < .001
foll filled pause 328 p < .001
foll IP boundary 1047 p < .001
Table 4: Significance of phonological features on
pitch accent prediction.
included following pauses and filled pauses as pre-
dictors. Below is the list of our acoustic features:
? Log of duration in milliseconds normalized
by number of canonical phones binned into 5
equal categories.
? Log Speech Rate; calculated on strings of
speech bounded on either side by pauses of
300 ms or greater and binned into 5 equal cat-
egories.
? Following pause; a binary distinction of
whether a word is followed by a period of si-
lence or not.
? Following filled pause; a binary distinction of
whether a word was followed by a filled pause
(uh, um) or not.
? Following IP boundary
Table 4 indicates that each of these features sig-
nificantly affect the presence of pitch accent. While
certainly all of these variables are not independent
of on another, using CRFs, one can incorporate all
of these variables into the pitch accent prediction
model with the advantage of making use of the de-
pendencies among the labels.
4.4 Surrounding Information
Sun (2002) has shown that the values immediately
preceding and following the target are good predic-
tors for the value of the target. We also experi-
mented with the effects of the surrounding values
by varying the window size of the observation-label
feature extraction described in Section 2. When the
window size is 1, only values of the word that is la-
belled are incorporated in the model. When the win-
dow size is 3, the values of the previous and the fol-
lowing words as well as the current word are incor-
porated in the model. Window size 5 captures the
values of the current word, the two previous words
and the two following words.
5 Experiments and Results
All experiments were run using 10 fold cross-
validation. We used Viterbi decoding to find the
most likely sequence and report the performance in
terms of label accuracy. We ran all experiments with
varying window sizes (w ? {1, 3, 5}). The baseline
which simply assigns the most common label, un-
accented, achieves 60.53 ? 1.50%.
Previous research has demonstrated that part of
speech and frequency, or a combination of these
two, are very reliable predictors of pitch accent.
Thus, to test the worthiness of using a CRF model,
the first experiment we ran was a comparison of an
HMM to a CRF using just the combination of part of
speech and unigram. The HMM score (referred as
HMM:POS, Unigram in Table 5) was 68.62 ? 1.78,
while the CRF model (referred as CRF:POS, Uni-
gram in Table 5) performed significantly better at
72.56 ? 1.86. Note that Pan and McKeown (1999)
reported 74% accuracy with their HMM model.
The difference is due to the different corpora used
in each case. While they also used spontaneous
speech, it was a limited domain in the sense that
it was speech from discharge orders from doctors
at one medical facility. The SWDB corpus is open
domain conversational speech.
In order to capture some aspects of the IC and
collocational strength of a word, in the second ex-
periment we ran part of speech plus all of the prob-
abilistic variables (referred as CRF:POS, Prob in
Table 5). The model accuracy was 73.94%, thus
improved over the model using POS and unigram
values by 1.38%.
In the third experiment we wanted to know if TTS
applications that made use of purely textual input
could be aided by the addition of timing and rhythm
variables that can be gleaned from a text string.
Thus, we included the textual features described in
Section 4.3 in addition to the probabilistic and syn-
tactic features (referred as CRF:POS, Prob, Txt in
Table 5). The accuracy was improved by 1.73%.
For the final experiment, we added the acoustic
variable, resulting in the use of all the variables de-
scribed in Section 4 (referred as CRF:All in Table
5). We get about 0.5% increase in accuracy, 76.1%
with a window of size w = 1.
Using larger windows resulted in minor increases
in the performance of the model, as summarized in
Table 5. Our best accuracy was 76.36% using all
features in a w = 5 window size.
Model:Variables w = 1 w = 3 w = 5
Baseline 60.53
HMM: POS,Unigram 68.62
CRF: POS, Unigram 72.56
CRF: POS, Prob 73.94 74.19 74.51
CRF: POS, Prob, Txt 75.67 75.74 75.89
CRF: All 76.1 76.23 76.36
Table 5: Test accuracy of pitch accent prediction on
SWDB using various variables and window sizes.
6 Discussion
Pitch accent prediction is a difficult task, in that, the
number of different speakers, topics, utterance frag-
ments and disfluent production of the SWBD corpus
only increase this difficulty. The fact that 21% of
the function words are accented indicates that mod-
els of pitch accent that mostly rely on part of speech
and unigram frequency would not fair well with this
corpus. We have presented a model of pitch accent
that captures some of the other factors that influence
accentuation. In addition to adding more probabilis-
tic variables and phonological factors, we have used
a sequence model that captures the interdependence
of accents within a phrase.
Given the distinct natures of corpora used, it is
difficult to compare these results with earlier mod-
els. However, in experiment 1 (HMM: POS, Uni-
gram vs CRF: POS, Unigram) we have shown that
a CRF model achieves a better performance than an
HMM model using the same features. However,
the real strength of CRFs comes from their ability
to incorporate different sources of information effi-
ciently, as is demonstrated in our experiments.
We did not test directly the probabilistic measures
(or collocation measures) that have been used before
for this task, namely information content (IC) (Pan
and McKeown, 1999) and mutual information (Pan
and Hirschberg, 2001). However, the measures we
have used encompass similar information. For ex-
ample, IC is only the additive inverse of our unigram
measure:
IC(w) = ? log p(w) (6)
Rather than using mutual information as a measure
of collocational strength, we used unigram, bigram
and joint probabilities. A model that includes both
joint probability and the unigram probabilities of wi
and wi?1 is comparable to one that includes mutual
information.
Just as the likelihood of a word being accented
is influenced by a following silence or IP bound-
ary, the collocational strength of the target word
with the following word (captured by reverse bi-
gram and reverse joint) is also a factor. With the
use of POS, unigram, and all bigram and joint prob-
abilities, we have shown that (a) CRFs outperform
HMMs, and (b) our probabilistic variables increase
accuracy from a model that include POS + unigram
(73.94% compared to 72.56%).
For tasks in which pitch accent is predicted solely
based on a string of text, without the addition of
acoustic data, we have shown that adding aspects
of rhythm and timing aids in the identification of
accent targets. We used the number of words in
an utterance, where in the utterance a word falls,
how long in both number of syllables and number
of phones all affect accentuation. The addition of
these variables improved the model by nearly 2%.
These results suggest that Accent prediction models
that only make use of textual information could be
improved with the addition of these variables.
While not trying to provide a complete model
of accentuation from acoustic information, in this
study we tested a few acoustic variables that have
not yet been tested. The nature of the SWBD cor-
pus allowed us to investigate the role of disfluencies
and widely variable durations and speech rate on ac-
centuation. Especially speech rate, duration and sur-
rounding silence are good predictors of pitch accent.
The addition of these predictors only slightly im-
proved the model (about .5%). Acoustic features are
very sensitive to individual speakers. In the corpus,
there are many different speakers of varying ages
and dialects. These variables might become more
useful if one controls for individual speaker differ-
ences. To really test the usefulness of these vari-
ables, one would have to combine them with acous-
tic features that have been demonstrated to be good
predictors of pitch accent (Sun, 2002; Conkie et al,
1999; Wightman et al, 2000).
7 Conclusion
We used CRFs with new measures of collocational
strength and new phonological factors that capture
aspects of rhythm and timing to model pitch accent
prediction. CRFs have the theoretical advantage of
incorporating all these factors in a principled and ef-
ficient way. We demonstrated that CRFs outperform
HMMs also experimentally. We also demonstrated
the usefulness of some new probabilistic variables
and phonological variables. Our results mainly have
implications for the textual prediction of accents in
TTS applications, but might also be useful in au-
tomatic speech recognition tasks such as automatic
transcription of multi-speaker meetings. In the near
future we would like to incorporate reliable acoustic
information, controlling for individual speaker dif-
ference and also apply different discriminative se-
quence labeling techniques to pitch accent predic-
tion task.
8 Acknowledgements
This work was partially funded by CAREER award
#IIS 9733067 IGERT. We would also like to thank
Mark Johnson for the idea of this project, Dan Ju-
rafsky, Alan Bell, Cynthia Girand, and Jason Bre-
nier for their helpful comments and help with the
database.
References
Y. Altun, T. Hofmann, and M. Johnson. 2003a.
Discriminative learning for label sequences via
boosting. In Proc. of Advances in Neural Infor-
mation Processing Systems.
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003b.
Hidden markov support vector machines. In
Proc. of 20th International Conference on Ma-
chine Learning.
M. Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In Proc.
of Empirical Methods of Natural Language Pro-
cessing.
A. Conkie, G. Riccardi, and R. Rose. 1999.
Prosody recognition from speech utterances us-
ing acoustic and linguistic based models of
prosodic events. In Proc. of EUROSPEECH?99.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on conversa-
tional pronunci ations. In Speech Communica-
tion.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and develo pment. In Proc. of the Inter-
national Conference on Acoustics, Speech, and
Signal Processing.
S. Greenberg, D. Ellis, and J. Hollenback. 1996. In-
sights into spoken language gleaned from pho-
netic transcripti on of the Switchboard corpus.
In Proc. of International Conference on Spoken
Language Processsing.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cial Intelligence, 63(1-2):305?340.
M. Johnson, S. Geman, S. Canon, Z. Chi, and
S. Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proc. of ACL?99
Association for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proc. of 18th International Conference on Ma-
chine Learning.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proc.
of 17th International Conference on Machine
Learning.
A. McCallum. 2003. Efficiently inducing features
of Conditional Random Fields. In Proc. of Un-
certainty in Articifical Intelligence.
T. Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Technical report,
CMU, Department of Statistics, TR 758.
S. Pan and J. Hirschberg. 2001. Modeling local
context for pitch accent prediction. In Proc. of
ACL?01, Association for Computational Linguis-
tics.
S. Pan and K. McKeown. 1999. Word informa-
tiveness and automatic pitch accent modeling.
In Proc. of the Joint SIGDAT Conference on
EMNLP and VLC.
V. Punyakanok and D. Roth. 2000. The use of
classifiers in sequential inference. In Proc. of
Advances in Neural Information Processing Sys-
tems.
F. Sha and F. Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. of Human
Language Technology.
Xuejing Sun. 2002. Pitch accent prediction using
ensemble machine learning. In Proc. of the In-
ternational Conference on Spoken Language Pro-
cessing.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-
margin markov networks. In Proc. of Advances
in Neural Information Processing Systems.
P. Taylor. 2000. Analysis and synthesis of intona-
tion using the Tilt model. Journal of the Acousti-
cal Society of America.
C. W. Wightman, A. K. Syrdal, G. Stemmer,
A. Conkie, and M. Beutnagel. 2000. Percep-
tually Based Automatic Prosody Labeling and
Prosodically Enriched Unit Selection Improve
Concatenative Text-To-Speech Synthesis. vol-
ume 2, pages 71?74.
Reading Comprehension Programs in a 
Statistical-Language-Processing Class* 
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo Braz, 
Benjamin Garrett, Margaret Kosmala, Tomer Moscovich, Lixin Pang, 
Changhee Pyo, Ye Sun, Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa Zorn 
Brown University 
Abstract 
We present-some n w results for the reading 
comprehension task described in \[3\] that im- 
prove on the best published results - from 
36% in \[3\] to 41% (the best of the systems 
described herein). We discuss a variety of 
techniques that tend to give small improve- 
ments, ranging from the fairly simple (give 
verbs more weight in answer selection) to 
the fairly complex (use specific techniques 
for answering specific kinds of questions). 
1 Introduction 
CS241, the graduate course in statistical lan- 
guage processing at Brown University, had 
as its class project the creation of programs 
to answer reading-comprehension tests. In 
particular, we used the Remedia TM reading 
comprehension test data as annotated by a 
group at MITRE Corporation, henceforth 
called the Deep Read group \[3\]. The class di- 
vided itself into four groups with sizes rang- 
ing from two to four students. In the first 
half of the semester the goal was to repro- 
duce the results of Deep Read and of one 
aother. After this learning and debugging 
period the groups were encouraged to think 
of and implement new ideas. 
The Deep Read group provided us with 
an on-line version of the Remedia material 
along with several marked up versions of 
* This research was supported inpart by NSF grant 
LIS SBR 9720368. Thanks to Marc Light and the 
group at MITRE Corporation for providing the on- 
line versions of the reading comprehension material 
and the Brown Laboratory for Linguistic Informa- 
tion Processing (BLLIP) for providing the parsed 
and pronoun referenced versions. 
same. The material encompasses four grade 
levels - -  third through sixth. Each grade 
levels consists of thirty stories plus five ques- 
tions for each story. Each story has the form 
of a newspaper article, including a title and 
dateline. Following \[3\], we used grades three 
and six as our development corpus and four 
and five for testing. 
The questions on each story are typically 
one each of the "who, what, where, why, and 
when" varieties. The Deep Read group an- 
swered these questions by finding the sen- 
tence in the story that best answers the 
question. One of the marked up versions 
they provide indicates those sentences Titles 
and datelines are also considered possible an- 
swers to the questions. In about 10% of the 
cases Deep Read judged no sentence stand- 
ing on its own to be a good answer. In these 
cases no answer to the question is considered 
correct. In a few cases more than one answer 
is acceptable and all of them are so marked. 
Deep Read also provided a version with 
person/place/time arkings inserted auto- 
matically by the Alembic named-entity s s- 
tem \[4\]. Henceforth we refer to this as NE 
(named entity) material. As discussed be- 
low, these markings are quite useful. In addi- 
tion to the mark-ups provided by Deep Read, 
the groups were also g~ven a machine anno- 
tated version with full parse trees and pro- 
noun coreference. 
The Deep Read group suggests everal 
different metrics for judging the perfor- 
mance of reading-comprehension-question- 
answering programs. However, their data 
show that the performance of theii: programs 
goes up and down on all of the metrics in 
Methods  Resu l ts  
1 Best of Deep Read 36 
2 BOW Stem Coref Class 37 
3 BOV Stem NE Coref Tfidf Subj Why MainV 38 
4 BOV Stem NE Defaults Coref 38 
5 BOV Stem NE Defaults Qspecific 41 
BOW 
BOV 
Coref 
Class 
Defaults 
MainV 
NE 
Qspecific 
Subj 
Tfidf 
Why 
bag-of-words 
bag-of-verbs 
pronoun coreference 
Word-Net class membership 
Defaults from Figure 3 
Extra credit for main verb match 
named entity 
Specific techniques for all question types 
Prefer sentences with same subject 
term frequency times inverse document frequency 
Specific good words for "why" questions 
Figure 1: Some notable results 
tandem. We implemented several of those 
metrics ourselves, but to keep things sim- 
ple we only report results on one of them - 
how often (in percent) the program answers 
a question by choosing a correct sentence (as 
judged in the answer mark-ups). Following 
\[3\] we refer to this as the "humsent" (hu- 
man annotated sentence) metric. Note that 
if more than one sentence is marked as ac- 
ceptable, a program response of any of those 
sentences i considered correct. If no sen- 
tence is marked, the program cannot get the 
answer correct, so there is an upper bound of 
approximately 90% accuracy for this metric. 
The results were both en- and dis- 
couraging. On the encouraging side, three 
of the four groups were able to improve, at 
least somewhat, on the previous best results. 
On the other hand, the extra annotation 
we provided (machine-generated parses of all 
the sentences \[1\] and machine-generated pro- 
noun coreference information \[2\]) proved of 
limited utility. 
2 Resu l ts  
Figure 1 shows four of the results that bet- 
tered those of Deep Read. In the next sec- 
tion we discuss the techniques used in these 
programs. 
The performance of all the programs var- 
ied widely depending on the type of ques- 
tion to be answered. In particular, "why" 
questions proved the most difficult. (Deep 
Read observed the same phenomenon.) In 
Figure 2 we break down the results for sys- 
tem 3 in Figure 1 according to question type. 
This system was able to answer only 22% 
of the "why" questions correctly. Program 
5, which had the most complicated scheme 
for handling "why" questions, answered 26% 
correctly. 
3 D iscuss ion  
As noted above, the early phase of the 
project was concerned with replicating the 
Deep Read results. This we were able to 
do, although generally only to about 1.5 sig- 
nificant digits. It seems that one can get 
swings of several percentage points in per- 
formance just depending on, say, how one 
2 
Question Type Percent Correct 
When 32 
Where 50 
Who 57 
What 32 
Why 22 
Figure 2: Results by question type 
resolves ties in the bag-of-words cores, or 
whether one. considers capitalized and un- 
capitalized words the same. However, the 
numbers our groups got were in the same 
ballpark and, more importantly, the trends 
we found in the numbers were the same. For 
example, stemming helped a little, stop-lists 
actually hurt a very small amount, and the 
use of named-entity data gave the biggest 
single improvement of the various Deep Read 
techniques. 
We found two variations on bag-of-words 
that improved results both individually and 
when combined. The first of these is the 
"bag of verbs" (BOV) technique. In this 
scheme one first measures imilarity by do- 
ing bag-of-words, but looking only at verbs 
(obtained from the machine-generated parse 
trees we provided). If two sentences tied 
on BOV, then bag-of-words is used as a tie- 
breaker. As the usefulness of this technique 
was shown early in the project, all of the 
groups tried it. It seems to provide two or 
three percentage-point mprovement in a va- 
riety of circumstances. Most of our best re- 
sults were obtained when using this tech- 
nique. A further refinement of this tech- 
nique is to weight matching main verbs more 
highly. This is used in system 3. 
One group explored the idea of replacing 
bag-of-words with a scheme based upon the 
standard ocument-retrieval "tfidf" method. 
Document retrieval has long used a bag- 
of-words technique, in which the words are 
given different weights. So if our query has 
words wl...wn, the frequency of the word i in 
document in question is fi, and the number 
of documents that have word i is n, then the 
score for this document is 
L ~i (1) 
i=l  n i  
That is, we take the term frequency (tf = fi) 
times the inverse document frequency (idf = 
1/ni) and sum over the words in the query. 
Of course, our application is sentence re- 
trieval, not document retrieval, so we define 
term frequency as the number of times the 
word appears in the candidate sentence, and 
document frequency as the number of sen- 
tences in which this word appears. (If we 
use stemming, then this applies to stemmed 
words.) Replacing BOW (OR BOV) by 
tfidf gives a three to six percentage-point 
improvement, depending on the other tech- 
niques with which it is combined. This is 
somewhat surprising because, as stated ear- 
lier, stop-lists were observed both by Deep 
Read and ourselves to have a slight negative 
impact on performance. One might think 
that the tfidf scheme should have something 
like the same impact, as the words on the 
stop-list are exactly those that occur in many 
sentences on average, and thus ones whoes 
impact will be attenuated in tfidL That tfidf 
is nevertheless uccessful suggests (perhaps) 
that the words on the stop-lists are useful 
for settling ties, a situation where even the 
attenuated value provided in tfidf will work 
just fine. It may also be the case that it 
is useful to distinguish between those words 
that are more common and those that are 
less common, even though neither appear on 
the stop-list. 
The best results, however, were obtained 
by creating question-answering strategies for 
specific question types (who, what, where, 
why, when). For example, one simple strat- 
egy assigns a default answer to each ques- 
tion type (in case all of the other strategies 
produce a tie) and zero or more sentence lo- 
cations that should be eliminated from con- 
sideration (before any of the other strategies 
are used). The particulars of this "Defaults" 
strategy are shown in Figure 3. 
There were more complicated question- 
type strategies as well. As already noted, 
3 
Question Type Default Eliminate 
Who title dateline 
What 1st story line (none) 
When dateline (none) 
Where dateline title 
Why 1st story line title, 
dateline 
Figure 3: Default and eliminable sentences 
in the "Default" strategy 
"why" questions are the most difficult for 
bag-of-words. The reason is fairly intuitive. 
"Why" questions are of the form "Why did 
such-and-such happen?" Bag-of-words typ- 
ically finds a sentence of the form "Such 
and such happened." The following strategy 
makes use of the fact that the answer to the 
"why" question is often either the sentence 
preceding or following the sentence that de- 
scribes the event. 
If the first NP (noun-phrase) in the sen- 
tence following the match is a pronoun, 
choose that sentence: 
Q: Why did Chris write two books 
of his own? 
match: He has written two books 
of his own. 
A: They tell what it is like to be 
famous. 
If that rule does not apply, then if the first 
word of the matching sentence is "this", 
"that," "these" or "those", select the pre- 
vious sentence: 
Q: Why did Wang once get upset? 
A: When she was a little girl, her 
art teacher didn't like her paint- 
ings. 
match: This upset Wang. 
Finally, if neither of the above two rules ap- 
plies, look for sentences that have the follow- 
ing words and phrases (and morphological 
variants) which tend to answer why ques- 
tions: "show", "explain", "because", "no 
one knows", and "if so". If there is more 
than one such sentence, use bag-of-~words to 
decide between them: 
4 
Q: Why does Greenland have 
strange seasons? 
A: Because it is far north, it has 
four months of sunlight each year. 
A lot of the question-type-specific rules 
use the parse of the sentence to select key 
words that are more important matches than 
other words of the sentence. For example, 
"where" questions tended to come in two va- 
rieties: "Where AUX NP VP" (e.g., "Where 
did Fred find the dog?") and "Where AUX 
NP." (e.g:, "Where is the dog?"). In both 
cases the words of the NP are important o 
match, and in the first case the (stemmed) 
main verb of the VP is important. Also, sen- 
tences that have PPs (prepositional phrases) 
with a preposition that often indicates loca- 
tion (e.g., "in," "near," etc.) are given a 
boost by the weighting scheme. 
4 Conclusion 
We have briefly discussed several reading 
comprehension systems that are able to im- 
prove on the results of \[3\]. While these are 
positive results, many of the lessons learned 
in this exercise are more negative. In par- 
ticular, while the NE data clearly helped 
a few percent, most of the extra syntactic 
and semantic annotations (i.e., parsing and 
coreference) were either of very small utility, 
or their utility came about in idiosyncratic 
ways. For example, probably the biggest im- 
pact of the parsing data was that it allowed 
people to experiment with the bag-of-verbs 
technique. Also, the parse trees served as the 
language for describing very question spe- 
cific techniques, uch as the ones for "where" 
questions presented in the previous section. 
Thus our tentative conclusion is that we 
are still not at a point that a task like chil- 
dren's reading comprehension tests is a good 
testing ground for NLP techniques. To the 
extent that these standard techniques are 
useful, it seems to be only in conjunction 
with other methods that are more directly 
aimed at the task. 
Of course, this is not to say that some- 
one else will not come up with better syntac- 
tic/semantic annotations that more directly 
lead to improvements on such tests. We can 
only say that so far we have not been able 
to do so. 
References 
1. CHARNIAK, E. A maximum-entropy- 
inspired parser. In Proceedings of the 
2000 Conference of the North American 
Chapter of the Assocation for Computa- 
tional Linguistics. ACL, New Brunswick 
N J, 2000. 
2. GE, N.,-HALE, J. AND CHARNIAK, E. 
A statistical approach to anaphora reso- 
lution. In Proceedings of the Sixth Work- 
shop on Very Large Corpora. 1998, 161- 
171. 
3. HIRSCHMAN, L., LIGHT, M., BRECK, E. 
AND BURGER, J. D. Deep read: a read- 
ing comprehension system. In Proceedings 
of the ACL 1999. ACL, New Brunswick, 
N J, 1999, 325-332. 
4. VILAIN, M. AND DAY, D. Finite-state 
parsing by rule sequences. In Interna- 
tional Conferences on ComputationM Lin- 
guistics (COLING-96). The International 
Conmmittee on Computational Linguis- 
tics, 1996. 
5 
Investigating Loss Functions and Optimization Methods for Discriminative
Learning of Label Sequences
Yasemin Altun
Computer Science
Brown University
Providence, RI 02912
altun@cs.brown.edu
Mark Johnson
Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
Mark Johnson@brown.edu
Thomas Hofmann
Computer Science
Brown University
Providence, RI 02912
th@cs.brown.edu
Abstract
Discriminative models have been of inter-
est in the NLP community in recent years.
Previous research has shown that they
are advantageous over generative mod-
els. In this paper, we investigate how dif-
ferent objective functions and optimiza-
tion methods affect the performance of the
classifiers in the discriminative learning
framework. We focus on the sequence la-
belling problem, particularly POS tagging
and NER tasks. Our experiments show
that changing the objective function is not
as effective as changing the features in-
cluded in the model.
1 Introduction
Until recent years, generative models were the most
common approach for many NLP tasks. Recently,
there is a growing interest on discriminative mod-
els in the NLP community, and these models were
shown to be successful for different tasks(Lafferty
et al, 2001; Ratnaparkhi, 1999; Collins, 2000). Dis-
criminative models do not only have theoretical ad-
vantages over generative models, as we discuss in
Section 2, but they are also shown to be empirically
favorable over generative models when features and
objective functions are fixed (Klein and Manning,
2002).
In this paper, we use discriminative models to
investigate the optimization of different objective
functions by a variety of optimization methods. We
focus on label sequence learning tasks. Part-of-
Speech (POS) tagging and Named Entity Recogni-
tion (NER) are the most studied applications among
these tasks. However, there are many others, such
as chunking, pitch accent prediction and speech edit
detection. These tasks differ in many aspects, such
as the nature of the label sequences (chunks or indi-
vidual labels), their difficulty and evaluation meth-
ods. Given this variety, we think it is worthwhile to
investigate how optimizing different objective func-
tions affects performance. In this paper, we varied
the scale (exponential vs logarithmic) and the man-
ner of the optimization (sequential vs pointwise) and
using different combinations, we designed 4 differ-
ent objective functions. We optimized these func-
tions on NER and POS tagging tasks. Despite our
intuitions, our experiments show that optimizing ob-
jective functions that vary in scale and manner do
not affect accuracy much. Instead, the selection of
the features has a larger impact.
The choice of the optimization method is impor-
tant for many learning problems. We would like
to use optimization methods that can handle a large
number of features, converge fast and return sparse
classifiers. The importance of the features, and
therefore the importance of the ability to cope with
a larger number of features is well-known. Since
training discriminative models over large corpora
can be expensive, an optimization method that con-
verges fast might be advantageous over others. A
sparse classifier has a shorter test time than a denser
classifier. For applications in which the test time is
crucial, optimization methods that result in sparser
classifiers might be preferable over other methods
   
   
   
  
  
  



  
  
  



  
  
  
x(t+1)x(t)x(t?1) x(t+1)x(t)x(t?1)
y(t+1)y(t)y(t?1) y(t+1)y(t)y(t?1)
a) HMM b)CRF
Figure 1: Graphical representation of HMMs and
CRFs. Shaded areas indicate variables that the
model conditions on.
even if their training time is longer. In this paper we
investigate these aspects for different optimization
methods, i.e. the number of features, training time
and sparseness, as well as the accuracy. In some
cases, an approximate optimization that is more ef-
ficient in one of these aspects might be preferable to
the exact method, if they have similar accuracy. We
experiment with exact versus approximate as well
as parallel versus sequential optimization methods.
For the exact methods, we use an off-the-shelf gradi-
ent based optimization routine. For the approximate
methods, we use a perceptron and a boosting algo-
rithm for sequence labelling which update the fea-
ture weights parallel and sequentially respectively.
2 Discriminative Modeling of Label
Sequences Learning
Label sequence learning is, formally, the problem
of learning a function that maps a sequence of ob-
servations 	

Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 640?648, Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Markov Models for Sequence Segmentation
Qinfeng Shi
NICTA, Statistical Machine Learning
Australian National University
Canberra, 2601 ACT
qinfeng.shi@rsise.anu.edu.au
Yasemin Altun
Toyota Technological Institute
1427 E 60th St
Chicago, IL 60637
altun@tti-c.org
Alex Smola
NICTA, Statistical Machine Learning
Australian National University
Canberra, 2601 ACT
Alex.Smola@nicta.com.au
S. V. N. Vishwanathan
NICTA, Statistical Machine Learning
Australian National University
Canberra, 2601 ACT
SVN.Vishwanathan@nicta.com.au
Abstract
In this paper, we study the problem of auto-
matically segmenting written text into para-
graphs. This is inherently a sequence label-
ing problem, however, previous approaches
ignore this dependency. We propose a novel
approach for automatic paragraph segmen-
tation, namely training Semi-Markov mod-
els discriminatively using a Max-Margin
method. This method allows us to model
the sequential nature of the problem and to
incorporate features of a whole paragraph,
such as paragraph coherence which cannot
be used in previous models. Experimental
evaluation on four text corpora shows im-
provement over the previous state-of-the art
method on this task.
1 Introduction
In this paper, we study automatic paragraph segmen-
tation (APS). This task is closely related to some
well known problems such as text segmentation, dis-
course parsing, topic shift detection and is relevant
for various important applications in speech-to-text
and text-to-text tasks.
In speech-to-text applications, the output of a
speech recognition system, such as the output of sys-
tems creating memos and documents for the Parlia-
ment House, is usually raw text without any punc-
tuation or paragraph breaks. Clearly, such text
requires paragraph segmentations. In text-to-text
processing, such as summarization, the output text
does not necessarily retain the correct paragraph
structure and may require post-processing. There
is psycholinguistic evidence as cited by Sporleder
& Lapata (2004) showing that insertion of para-
graph breaks could improve the readability. More-
over, it has been shown that different languages may
have cross-linguistic variations in paragraph bound-
ary placement (Zhu, 1999), which indicates that ma-
chine translation can also benefit from APS. APS
can also recover the paragraph breaks that are often
lost in the OCR applications.
There has been growing interest within the NLP
community for APS in recent years. Previous meth-
ods such as Sporleder & Lapata (2004); Genzel
(2005); Filippova & Strube (2006) treat the problem
as a binary classification task, where each sentence
is labeled as the beginning of a paragraph or not.
They focus on the use of features, such as surface
features, language modeling features and syntactic
features. The effectiveness of features is investi-
gated across languages and/or domains. However,
these approaches ignore the inherent sequential na-
ture of APS. Clearly, consecutive sentences within
the same paragraph depend on each other. More-
over, paragraphs should exhibit certain properties
such as coherence, which should be explored within
an APS system. One cannot incorporate such prop-
erties/features when APS is treated as a binary clas-
sification problem. To overcome this limitation, we
cast APS as a sequence prediction problem, where
the performance can be significantly improved by
optimizing the choice of labeling over whole se-
quences of sentences, rather than individual sen-
tences.
Sequence prediction is one of the most promi-
640
Figure 1: Top: sequence (horizontal line) with seg-
ment boundaries (vertical lines). This corresponds
to a model where we estimate each segment bound-
ary independently of all other boundaries. Middle:
simple semi-Markov structure. The position of the
segment boundaries only depends on the position of
its neighbors, as denoted by the (red) dash arcs. Bot-
tom: a more sophisticated semi-Markov structure,
where each boundary depends on the position of two
of its neighbors. This may occur, e.g., when the de-
cision of where to place a boundary depends on the
content of two adjacent segments. The longer range
interaction is represented by the additional (blue)
arcs.
nent examples of structured prediction. This prob-
lem is generally formalized such that there exists
one variable for each observation in the sequence
and the variables form aMarkov chain (HMM). Seg-
mentation of a sequence has been studied as a class
of sequence prediction problems with common ap-
plications such as protein secondary structure pre-
diction, Named Entity Recognition and segmenta-
tion of FAQ?s. The exceptions to this approach
are Sarawagi & Cohen (2004); Raetsch & Sonnen-
burg (2006), which show that Semi-Markov mod-
els (SMMs) (Janssen & Limnois, 1999), which are
a variation of Markov models, are a natural formu-
lation for sequence segmentation. The advantage of
these models, depicted in Figure 1, is their ability
to encode features that capture properties of a seg-
ment as a whole, which is not possible in an HMM
model. In particular, these features can encode simi-
larities between two sequence segments of arbitrary
lengths, which can be very useful in tasks such as
APS.
In this paper, we present a Semi-Markov model
for APS and propose a max-margin training on these
methods. This training method is a generalization of
the Max-Margin methods for HMMs (Altun et al,
2003b) to SMMs. It follows the recent literature
on discriminative learning of structured prediction
(Lafferty et al, 2001; Collins, 2002; Altun et al,
2003a; Taskar et al, 2003). Our method inherits the
advantages of discriminative techniques, namely the
ability to encode arbitrary (overlapping) features and
not making implausible conditional independence
assumptions. It also has advantages of SMM mod-
els, namely the ability to encode features at seg-
ment level. We present a linear time inference al-
gorithm for SMMs and outline the learning method.
Experimental evaluation on datasets used previously
on this task (Sporleder & Lapata, 2004) shows im-
provement over the state-of-the art methods on APS.
2 Modeling Sequence Segmentation
In sequence segmentation, our goal is to solve the es-
timation problem of finding a segmentation y ? Y ,
given an observation sequence x ? X . For exam-
ple, in APS x can be a book which is a sequence
of sentences. In a Semi-Markov model, there ex-
ists one variable for each subsequence of observa-
tions (i. e. multiple observations) and these variables
form a Markov chain. This is opposed to an HMM
where there exists one variable for each observation.
More formally, in SMMs, y ? Y is a sequence of
segment labelings si = (bi, li) where bi is a non-
negative integer denoting the beginning of the ith
segment which ends at position bi+1 ? 1 and whose
label is given by li (Sarawagi & Cohen, 2004). Since
in APS the label of the segments is irrelevant, we
represent each segment simply by the beginning po-
sition y := {bi}
L?1
i=0 with the convention that b0 = 0
and bL = N where N is the number of observations
in x. Here, L denotes the number of segments in y.
So the first segment is [0, b1), and the last segment
is [bL?1, N), where [a, b) denotes all the sentences
from a to b inclusive a but exclusive b.
We cast this estimation problem as finding a dis-
criminant function F (x, y) such that for an obser-
vation sequence x we assign the segmentation that
receives the best score with respect to F ,
y?(x) := argmax
y?Y
F (x, y). (1)
641
As in many learning methods, we consider functions
that are linear in some feature representation ?,
F (x, y;w) = ?w,?(x, y)?. (2)
Here, ?(x, y) is a feature map defined over the joint
input/output space as detailed in Section 2.3.
2.1 Max-Margin Training
We now present a maximum margin training for
predicting structured output variables, of which se-
quence segmentation is an instance. One of the ad-
vantages of this method is its ability to incorporate
the cost function that the classifier is evaluated with.
Let ?(y, y?) be the cost of predicting y? instead of y.
For instance, ? is usually the 0-1 loss for binary and
multiclass classification. However, in segmentation,
this may be a more sophisticated function such as
the symmetric difference of y and y? as discussed in
Section 2.2. Then, one can argue that optimizing a
loss function that incorporates this cost can lead to
better generalization properties. One can find a the-
oretical analysis of this approach in Tsochantaridis
et al (2004).
We follow the general framework of Tsochan-
taridis et al (2004) and look for a hyperplane that
separates the correct labeling yi of each observa-
tion sequence xi in our training set from all the in-
correct labelings Y ?yi with some margin that de-
pends on ? additively 1. In order to allow some
outliers, we use slack variables ?i and maximize the
minimum margin, F (xi, yi)?maxy?Y ?yi F (xi, y),
across training instances i. Equivalently,
min
w,?
1
2
?w?2 + C
m?
i=1
?i (3a)
?i, y ?w,?(xi, yi)? ?(xi, y)? ? ?(yi, y)? ?i.
(3b)
To solve this optimization problem efficiently, one
1There is an alternative formulation that is multiplicative in
?. We prefer (3) due to computational efficiency reasons.
can investigate its dual given by
min
?
1
2
?
i,j,y,y?
?iy?jy?
?
?(xi, y),?(xj , y
?)
?
(4)
?
?
i,y
?(yi, y)?iy
?i, y
?
y
?iy ? C, ?iy ? 0.
Here, there exists one parameter ?iy for each train-
ing instance xi and its possible labeling y ?
Y . Solving this optimization problem presents a
formidable challenge since Y generally scales expo-
nentially with the number of variables within each
variable y. This essentially makes it impossible
to find an optimal solution via enumeration. In-
stead, one may use a column generation algorithm
(Tsochantaridis et al, 2005) to find an approximate
solution in polynomial time. The key idea is to find
the most violated constraints (3b) for the current set
of parameters and satisfy them up to some precision.
In order to do this, one needs to find
argmax
y?Y
?(yi, y) + ?w,?(xi, y)? , (5)
which can usually be done via dynamic program-
ming. As we shall see, this is an extension of the
Viterbi algorithm for Semi Markov models.
Note that one can express the optimization
and estimation problem in terms of kernels
k((x, y), (x?, y?)) := ??(x, y),?(x?, y?)?. We refer
the reader to Tsochantaridis et al (2005) for details.
To adapt the above framework to the segmenta-
tion setting, we need to address three issues: a) we
need to specify a loss function ? for segmentation,
b) we need a suitable feature map ? as defined in
Section 2.3, and c) we need to find an algorithm
to solve (5) efficiently. The max-margin training of
SMMs was also presented in Raetsch & Sonnenburg
(2006)
2.2 Cost Function
To measure the discrepancy between y and some al-
ternative sequence segmentation y?, we simply count
the number of segment boundaries that have a) been
missed and b) been wrongly added. Note that this
definition allows for errors exceeding 100% - for
642
Algorithm 1 Max-Margin Training Algorithm
Input: data xi, labels yi, sample sizem, tolerance

Initialize Si = ? for all i, and w = 0.
repeat
for i = 1 to m do
w =
?
i
?
y?Si
?iy?(xi, y)
y? = argmaxy?Y ?w,?(xi, y)?+ ?(yi, y)
? = max(0,maxy?Si ?w,?(xi, y)? +
?(yi, y))
if ?w,?(xi, y?)?+ ?(yi, y) > ? +  then
Increase constraint set Si ? Si ? y?
Optimize (4) wrt ?iy,?y ? Si.
end if
end for
until S has not changed in this iteration
instance, if we were to place considerably more
boundaries than can actually be found in a sequence.
The number of errors is given by the symmetric
difference between y and y?, when segmentations
are viewed as sets. This can be written as
?(y, y?) = |y|+ |y?| ? 2|y ? y?|
= |y|+
l??
i=1
[
1? 2
{
b?i ? y
}]
. (6)
Here | ? | denotes the cardinality of the set. Eq. (6)
plays a vital role in solving (5), since it allows us to
decompose the loss in y? into a constant and func-
tions depending on the segment boundaries b?i only.
Note that in the case where we want to segment and
label, we simply would need to check that the posi-
tions are accurate and that the labels of the segments
match.
2.3 Feature Representation
SMMs can extract three kinds of features from the
input/output pairs: a) node features, i. e. features that
encode interactions between attributes of the obser-
vation sequence and the (label of a) segment (rather
than the label of each observation as in HMM), b)
features that encode interactions between neighbor-
ing labels along the sequence and c) edge features,
i. e. features that encode properties of segments. The
first two types of features are commonly used in
other sequence models, such as HMMs and Con-
ditional Random Fields (CRFs). The third feature
type is specific to Semi-Markov models. In particu-
lar, these features can encode properties of a whole
segment or similarities between two sequence seg-
ments of arbitrary lengths. The cost of this express-
ibility is simply a constant factor of the complexity
of Markov models, if the maximum length of a seg-
ment is bounded. This type of features are particu-
larly useful in the face of sparse data.
As in HMMs, we assume stationarity in our model
and sum over the features of each segment to get
?(x, y). Then, ? corresponding to models of the
middle structure given in Figure 1 is given by
?(x, y?) := (?0,
l??1?
i=1
?1(n?i, x),
l??
i=1
?2(b?i?1, b?i, x)).
We let ?0 = l? ? 1, the number of segments. The
node features ?1 capture the dependency of the cur-
rent segment boundary to the observations, whereas
the edge features?2 represent the dependency of the
current segment to the observations. To model the
bottom structure in Figure 1, one can design features
that represent the dependency of the current segment
to its adjacent segments as well as the observations,
?3(x, bi?2, bi?1, bi). The specific choices of the fea-
ture map ? are presented in Section 3.
2.4 Column Generation on SMMs
Tractability of Algorithm 1 depends on the existence
of an efficient algorithm that finds the most violated
constraint (3b) via (5). Both the cost function of Sec-
tion 2.2 and the feature representation of Section 2.3
are defined over a short sequence of segment bound-
aries. Therefore, using the Markovian property, one
can perform the above maximization step efficiently
via a dynamic programming algorithm. This is a
simple extension of the Viterbi algorithm. The infer-
ence given by (1) can be performed using the same
algorithm, setting ? to a constant function.
We first state the dynamic programming recursion
for F + ? in its generality. We then give the pseu-
docode for ?3 = ?.
Denote by T (t?, t+;x) the largest value of
?(y, p)+F (x, p) for any partial segmentation p that
starts at position 0 and which ends with the segment
[t?, t+). Moreover, let M be a upper bound on the
643
Algorithm 2 Column Generation
Input: sequence x, segmentation y, max-length
of a segment M
Output: score s, segment boundaries y?
Initialize vectors T ? Rm and R ? Ym to 0
for i = 1 to l do
Ri = argmax
max(0,i?M)?j<i
Tj + g(j, i)
Ti = TRi + g(Ri, i)
end for
s = Tm + |y|
y? = {m}
repeat
i = y?first
y? ? {Ri, y?}
until i = 0
length of a segment. The recursive step of the dy-
namic program is given by
T (t?, t+;x) = max
max(0,t??M)?k<t?
T (k, t?;x)
+ g(k, t?, t+)
where we defined the increment g(k, t?, t+) as
??0(x),?1(x, t+),?2(x, t?, t+),?3(x, k, t?, t+), w?
+ 1? 2 {(t?, t+) ? y}
where by convention T (i, i?) = ?? if i < 0 for
all labels. Since T needs to be computed for all val-
ues of t+ ? M ? t? < t+, we need to compute
O(|x|M) many values, each of which requires an
optimization over M possible values. That is, stor-
age requirements are O(|x|M), whereas the com-
putation scales with O(|x|M2). If we have a good
bound on the maximal sequence length, this can be
dealt with efficiently. Finally, the recursion is set up
by T (0, 0, x) = |y|.
See Algorithm 2 for pseudocode, when ?3 = ?.
The segmentation corresponding to (5) is found by
constructing the path traversed by the argument of
the max operation generating T .
3 Features
We now specify the features described in Section 2.3
for APS. Note that the second type of features do
not exist for APS since we ignore the labelings of
segments.
3.1 Node Features ?1
Node features?1(bj , x) represent the information of
the current segment boundary and some attributes of
the observations around it (which we define as the
current, preceding and successive sentences). These
are sentence level features, which we adapt from
Genzel (2005) and Sporleder & Lapata (2004) 2. For
the bj th sentence, x(bj), we use the following fea-
tures
? Length of x(bj).
? Relative Position of x(bj).
? Final punctuation of x(bj).
? Number of capitalized words in x(bj).
? Word Overlap of x(bj) with the next one
Wover(x(bj), x(bj + 1)) =
2 | x(bj) ? x(bj + 1) |
| x(bj) | + | x(bj + 1) |
.
? First word of x(bj).
? Bag Of Words (BOW) features: Let the bag of
words of a set of sentences S be
B(S) = (c0, c1, ..., ci, ..., cN?1),
where N is the size of the dictionary and ci is
the frequency of word i in S.
? BOW of x(bj), B({x(bj)})
? BOW of x(bj) and the previous sentence
B({x(bj ? 1), x(bj)})
? BOW of x(bj) and the succeeding sen-
tence B({x(bj), x(bj + 1)})
? The inner product of the two items above
? Cosine Similarity of x(bj) and the previous
sentence
CS(x(bj ? 1), x(bj))
=
?B(x(bj ? 1)), B(x(bj))?
| B(x(bj ? 1)) | ? | B(x(bj)) |
2Due to space limitations, we omit the motivations for these
features and refer the reader to the literature cited above.
644
? Shannon?s Entropy of x(bj) computed by us-
ing a language model as described in Genzel &
Charniak (2003).
? Quotes(Qp, Qc, Qi). Qp andQc are the number
of pairs of quotes in the previous(Nump) and
current sentence (Numc), Qp = 0.5 ? Nump
and Qc = 0.5?Numc.
3.1.1 Edge Features ?2
Below is the set of features?2(bj , bj+1, x) encod-
ing information about the current segment. These
features represent the power of the Semi-Markov
models. Note that ?3 features also belong to edge
features category. In this paper, we did not use ?3
feature due to computational issues.
? Length of The Paragraph: This feature ex-
presses the assumption that one would want to
have a balance across the lengths of the para-
graphs assigned to a text. Very long and very
short paragraphs should be uncommon.
? Cosine Similarity of the current paragraph and
neighboring sentences: Ideally, one would like
to measure the similarity of two consecutive
paragraphs and search for a segmentation that
assigns low similarity scores (in order to fa-
cilitate changes in the content). This can be
encoded using ?3(x, bj?1, bj , bj+1) features.
When such features are computationally expen-
sive, one can measure the similarity of the cur-
rent paragraph with the preceding sentence as
CS(P, x(bj ? 1))
=
?BOW (P ), BOW (x(bj ? 1))?
| BOW (P ) | ? | BOW (x(bj ? 1)) |
where P is the set of sentences in the current
paragraph, [bj , bj+1). A similar feature is used
for CS(P, x(bj+1)).
? Shannon?s Entropy of the Paragraph: The mo-
tivation for including features encoding the en-
tropy of the sentences is the observation that the
entropy of paragraph initial sentences is lower
than the others (Genzel & Charniak, 2003).
The motivation for including features encod-
ing the entropy of the paragraphs, on the other
hand, is that the entropy rate should remain
more or less constant across paragraphs, es-
pecially for long texts like books. We ignore
the sentence boundaries and use the same tech-
nique that we use to compute the entropy of a
sentence.
3.2 Feature Rescaling
Most of the features described above are binary.
There are also some features such as the entropy
whose value could be very large. We rescale all the
non-binary valued features so that they do not over-
ride the effect of the binary features. The scaling is
performed as follows:
unew =
u?min(u)
max(u)?min(u)
where unew is the new feature and u is the old fea-
ture. min(u) is the minimum of u, and max(u)
is the maximum of u. An exception to this is the
rescaling of BOW features which is given by
B(x(bj))new = B(x(bj))/?B(x(bj)), B(x(bj))?
?., .? denotes the inner product.
4 Experiments
We collected four sets of data for our experiments.
The first corpus, which we call SB, consists of man-
ually annotated text from the book The Adventures
of Bruce-Partington Plans by Arthur Conan-Doyle.
The second corpus, which we call SA, again con-
sists of manually annotated text but from 10 differ-
ent books by Conan-Doyle. Our third corpus con-
sists of German (GER) and English (ENG) texts.
The German data consisting of 12 German novels
was used by Sporleder & Lapata (2006). This data
uses automatically assigned paragraph boundaries,
with the labeling error expected to be around 10%.
The English data contains 12 well known English
books from Project Gutenberg (http://www.
gutenberg.org/wiki/Main Page). For this
dataset the paragraph boundaries were marked man-
ually.
All corpora were approximately split into train-
ing (72%), development (21%), and test set (7%)
(see Table 1). The table also reports the accuracy of
the baseline classifier, denoted as BASE, which ei-
ther labels all sentences as paragraph boundaries or
645
Table 1: Number of sentences and % accuracy of the
baseline classifier (BASE) on various datasets used
in our experiments.
TOTAL TRAIN DEV TEST BASE
SB 59,870 43,678 12,174 3,839 53.70
SA 69,369 50,680 14,204 4,485 58.62
ENG 123,261 88,808 25,864 8,589 63.41
GER 370,990 340,416 98,610 31,964 62.10
non-boundaries, choosing whichever scheme yields
a better accuracy.
We evaluate our system using accuracy, precision,
recall, and the F1-score given by (2? Precision?
Recall)/(Precision+Recall) and compare our re-
sults to Sporleder & Lapata (2006) who used Boos-
Texter (Schapire & Singer, 2000) as a learning al-
gorithm. To the best of our knowledge, BoosTexter
(henceforth called BT) is the leading method pub-
lished for this task so far. In order to evaluate the im-
portance of the edge features and the resultant large-
margin constraint, we also compare against a stan-
dard binary Support Vector Machine (SVM) which
uses node features alone to predict whether each
sentence is the beginning of a paragraph or not. For
a fair comparison, all classifiers used the linear ker-
nel and the same set of node features.
We perform model selection for all three algo-
rithms by choosing the parameter values that achieve
the best F1-score on the development set. For
both the SVM as well as our algorithm, SMM, we
tune the parameter C (see (3a)) which measures the
trade-off between training error and margin. For BT,
we tune the number of Boosting iterations, denoted
by N .
4.1 Results
In our first experiment, we compare the perfor-
mance of our algorithm, SMM, on the English and
German corpus to a standard SVM and BoosTex-
ter. We report these result in Table 2. Our algo-
rithm achieves the best F1-score on the ENG cor-
pus. SMM performs very competitively on the GER
corpus, achieving accuracies close to those of BT.
We observed a large discrepancy between the per-
formance of our algorithm on the development and
Table 2: Test results on ENG and GER data after
model selection.
DATASET ALGO. ACC. REC. PREC. F1
ENG SMM 75.61 46.67 77.78 58.33
SVM 58.54 26.67 40.00 32.00
BT 65.85 33.33 55.56 41.67
GER SMM 70.56 46.81 65.67 54.66
SVM 39.92 100.00 38.68 55.79
BT 72.58 54.26 67.11 60.00
the test datasets. The situation is similar for both
SVM and BT. For instance, BT when trained on
the ENG corpora, achieves an optimal F1-score of
18.67% after N = 100 iterations. For the same N
value, the test performance is 41.67%. We conjec-
ture that this discrepancy is because the books that
we use for training and test are written by differ-
ent authors. While there is some generic informa-
tion about when to insert a paragraph break, it is
often subjective and part of the authors style. To
test this hypothesis, we performed experiments on
the SA and SB corpus, and present results in Table
3. Indeed, the F1-scores obtained on the develop-
ment and test corpus closely match for text drawn
from the same book (whilst exhibiting better over-
all performance), differs slightly for text drawn from
different books by the same author, and has a large
deviation for the GER and ENG corpus.
Table 3: Comparison on various ENG datasets.
DATASET ACC. REC. PREC. F1-SCORE
SB (DEV) 92.81 86.44 92.73 89.47
SB (TEST) 96.30 96.00 96.00 96.00
SA (DEV) 82.24 61.11 82.38 70.17
SA (TEST) 81.03 79.17 76.00 77.55
ENG (DEV) 69.84 18.46 78.63 29.90
ENG (TEST) 75.61 46.67 77.78 58.33
There is one extra degree of freedom that we can
optimize in our model, namely the offset, i. e. the
weight assigned to the constant feature ?0. After
fixing all the parameters as described above, we vary
the value of the offset parameter and pick the value
that gives the F1-score on the development data. We
choose to use F1-score, since it is the error measure
that we care about. Although this extra optimization
646
leads to better F1-score in German (69.35% as op-
posed to 54.66% where there is no extra tuning of
the offset), it results in a decrease of the F1-score in
English (52.28% as opposed to 58.33%). These re-
sults are reported in Table 4. We found that the dif-
ference of the F1-score of tuning and not tuning the
threshold on the development set was not a good in-
dicator on the usefulness of this extra parameter. We
are now investigating other properties, such as vari-
ance on the development data, to see if the tuning of
the threshold can be used for better APS systems.
Figure 2: Precision-recall curves
Figure 2 plots the precision-recall curve obtained
on various datasets. As can be seen the performance
of our algorithm on the SB dataset is close to opti-
mum, whilst it degrades slightly on the SA dataset,
and substantially on the ENG and GER datasets.
This further confirms our hypothesis that our algo-
rithm excels in capturing stylistic elements from a
single author, but suffers slightly when trained to
identify generic stylistic elements. We note that this
is not a weakness of our approach alone. In fact, all
the other learning algorithms also suffer from this
shortcoming.
Table 4: Performance on ENG test set tuning the
offset for best F1-score on ENG development set.
DATASET ACC. REC. PREC. F1-SCORE
ENG 75.61 46.67 77.78 58.33
ENG +?0 39.02 93.33 36.84 52.28
GER 70.56 46.81 65.67 54.66
GER + ?0 75.40 73.40 65.71 69.35
5 Conclusion
We presented a competitive algorithm for paragraph
segmentation which uses the ideas from large mar-
gin classifiers and graphical models to extend the
semi-Markov formalism to the large margin case.
We obtain an efficient dynamic programming for-
mulation for segmentation which works in linear
time in the length of the sequence. Experimental
evaluation shows that our algorithm is competitive
when compared to the state-of-the-art methods.
As future work, we plan on implementing ?3 fea-
tures in order to perform an accuracy/time analy-
sis. By defining appropriate features, we can use
our method immediately for text and discourse seg-
mentation. It would be interesting to compare this
method to Latent Semantic Analysis approaches for
text segmentation as studied for example in Bestgen
(2006) and the references thereof.
References
Altun, Y., Hofmann, T., & Johnson, M. (2003a).
Discriminative Learning for Label Sequences via
Boosting. In In Proceedings of NIPS 2003.
Altun, Y., Tsochantaridis, I., & Hofmann, T.
(2003b). Hidden markov support vector ma-
chines. In International Conference on Machine
Learning.
Bestgen, Y. (2006). Improving text segmentation us-
ing latent semantic analysis: A reanalysis of choi,
wiemer-hastings, and moore (2001). Computa-
tional Linguistics, 32, 5?12.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Filippova, K., & Strube, M. (2006). Using linguisti-
cally motivated features for paragraph segmenta-
tion. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Genzel, D. (2005). A paragraph boundary detec-
tion system. In Proceedings of the Conference
on Computational Linguistics and Intelligent Text
Processing.
Genzel, D., & Charniak, E. (2003). Variation of en-
tropy and parse tree of sentences as a function of
647
the sentence number. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing.
Janssen, J., & Limnois, N. (1999). Semi-markov
models and applications. Kluwer Academic.
Lafferty, J. D., McCallum, A., & Pereira, F. (2001).
Conditional random fields: Probabilistic model-
ing for segmenting and labeling sequence data. In
18th International Conference onMachine Learn-
ing ICML.
Raetsch, G., & Sonnenburg, S. (2006). Large scale
hidden Semi-Markov SVMs for gene structure
prediction. In In Proceedings of NIPS 2006.
Sarawagi, S., & Cohen, W. (2004). Semi-Markov
Conditional Random Fields for Information Ex-
traction. In Advances in Neural Information Pro-
cessing Systems (NIPS).
Schapire, R. E., & Singer, Y. (2000). Boostexter:
A boosting-based system for text categorization.
Machine Learning, 39(2/3), 135?168.
Sporleder, C., & Lapata, M. (2004). Automatic para-
graph identification: A study across languages
and domains. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Sporleder, C., & Lapata, M. (2006). Broad coverage
paragraph segmentation across languages and do-
mains. ACM Trans. Speech Lang. Process., 3(2),
1?35.
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-
margin markov networks. In S. Thrun, L. Saul, &
B. Scho?lkopf, eds., Advances in Neural Informa-
tion Processing Systems 16.
Tsochantaridis, I., Hofmann, T., Joachims, T., & Al-
tun, Y. (2004). Support vector machine learning
for interdependent and structured output spaces.
In ICML ?04: Twenty-first international confer-
ence on Machine learning. New York, NY, USA:
ACM Press. ISBN 1-58113-828-5.
Tsochantaridis, I., Joachims, T., Hofmann, T., & Al-
tun, Y. (2005). Large margin methods for struc-
tured and interdependent output variables. Jour-
nal of Machine Learning Research.
Zhu, C. (1999). Ut once more: The sentence as the
key functional unit of translation. Meta, 44(3),
429?447.
648
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 594?602,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Broad-Coverage Sense Disambiguation and Information Extraction with
a Supersense Sequence Tagger?
Massimiliano Ciaramita
Inst. of Cognitive Science and Technology
Italian National Research Council
m.ciaramita@istc.cnr.it
Yasemin Altun
Toyota Technological Institute
at Chicago
altun@tti-c.org
Abstract
In this paper we approach word sense
disambiguation and information extrac-
tion as a unified tagging problem. The
task consists of annotating text with the
tagset defined by the 41 Wordnet super-
sense classes for nouns and verbs. Since
the tagset is directly related to Wordnet
synsets, the tagger returns partial word
sense disambiguation. Furthermore, since
the noun tags include the standard named
entity detection classes ? person, location,
organization, time, etc. ? the tagger, as
a by-product, returns extended named en-
tity information. We cast the problem of
supersense tagging as a sequential label-
ing task and investigate it empirically with
a discriminatively-trained Hidden Markov
Model. Experimental evaluation on the
main sense-annotated datasets available,
i.e., Semcor and Senseval, shows consid-
erable improvements over the best known
?first-sense? baseline.
1 Introduction
Named entity recognition (NER) is the most stud-
ied information extraction (IE) task. NER typi-
cally focuses on detecting instances of ?person?,
?location?, ?organization? names and optionally
instances of ?miscellaneous? or ?time? categories.
The scalability of statistical NER allowed re-
searchers to apply it successfully on large col-
lections of newswire text, in several languages,
and biomedical literature. Newswire NER per-
formance, in terms of F-score, is in the upper
?The first author is now at Yahoo! Research. The tag-
ger described in this paper is free software and can be down-
loaded from http://www.loa-cnr.it/ciaramita.html.
80s (Carreras et al, 2002; Florian et al, 2003),
while Bio-NER accuracy ranges between the low
70s and 80s, depending on the data-set used for
training/evaluation (Dingare et al, 2005). One
shortcoming of NER is its over-simplified onto-
logical model, leaving instances of other poten-
tially informative categories unidentified. Hence,
the utility of named entity information is limited.
In addition, instances to be detected are mainly re-
stricted to (sequences of) proper nouns.
Word sense disambiguation (WSD) is the task
of deciding the intended sense for ambiguous
words in context. With respect to NER, WSD
lies at the other end of the semantic tagging spec-
trum, since the dictionary defines tens of thou-
sand of very specific word senses, including NER
categories. Wordnet (Fellbaum, 1998)1, possibly
the most used resource for WSD, defines word
senses for verbs, common and proper nouns. Word
sense disambiguation, at this level of granularity,
is a complex task which resisted all attempts of
robust broad-coverage solutions. Many distinc-
tions are too subtle to be captured automatically,
and the magnitude of the class space ? several
orders larger than NER?s ? makes it hard to ap-
proach the problem with sophisticated, but scal-
able, machine learning methods. Lastly, even if
the methods would scale up, there are not enough
manually tagged data, at the word sense level, for
training a model. The performance of state of
the art WSD systems on realistic evaluations is
only comparable to the ?first sense? baseline (cf.
Section 5.3). Notwithstanding much research, the
benefits of disambiguated lexical information for
language processing are still mostly speculative.
This paper presents a novel approach to broad-
1When referring to Wordnet, throughout the paper, we
mean Wordnet version 2.0.
594
NOUNS
SUPERSENSE NOUNS DENOTING SUPERSENSE NOUNS DENOTING
act acts or actions object natural objects (not man-made)
animal animals quantity quantities and units of measure
artifact man-made objects phenomenon natural phenomena
attribute attributes of people and objects plant plants
body body parts possession possession and transfer of possession
cognition cognitive processes and contents process natural processes
communication communicative processes and contents person people
event natural events relation relations between people or things or ideas
feeling feelings and emotions shape two and three dimensional shapes
food foods and drinks state stable states of affairs
group groupings of people or objects substance substances
location spatial position time time and temporal relations
motive goals Tops abstract terms for unique beginners
VERBS
SUPERSENSE VERBS OF SUPERSENSE VERBS OF
body grooming, dressing and bodily care emotion feeling
change size, temperature change, intensifying motion walking, flying, swimming
cognition thinking, judging, analyzing, doubting perception seeing, hearing, feeling
communication telling, asking, ordering, singing possession buying, selling, owning
competition fighting, athletic activities social political and social activities and events
consumption eating and drinking stative being, having, spatial relations
contact touching, hitting, tying, digging weather raining, snowing, thawing, thundering
creation sewing, baking, painting, performing
Table 1. Nouns and verbs supersense labels, and short description (from the Wordnet documentation).
coverage information extraction and word sense
disambiguation. Our goal is to simplify the disam-
biguation task, for both nouns and verbs, to a level
at which it can be approached as any other tagging
problem, and can be solved with state of the art
methods. As a by-product, this task includes and
extends NER. We define a tagset based on Word-
net?s lexicographers classes, or supersenses (Cia-
ramita and Johnson, 2003), cf. Table 1. The size
of the supersense tagset alows us to adopt a struc-
tured learning approach, which takes local depen-
dencies between labels into account. To this ex-
tent, we cast the supersense tagging problem as a
sequence labeling task and train a discriminative
Hidden Markov Model (HMM), based on that of
Collins (2002), on the manually annotated Semcor
corpus (Miller et al, 1993). In two experiments
we evaluate the accuracy of the tagger on the Sem-
cor corpus itself, and on the English ?all words?
Senseval 3 shared task data (Snyder and Palmer,
2004). The model outperforms remarkably the
best known baseline, the first sense heuristic ? to
the best of our knowledge, for the first time on the
most realistic ?all words? evaluation setting.
The paper is organized as follows. Section 2
introduces the tagset, Section 3 discusses related
work and Section 4 the learning model. Section 5
reports on experimental settings and results. In
Section 6 we summarize our contribution and con-
sider directions for further research.
2 Supersense tagset
Wordnet (Fellbaum, 1998) is a broad-coverage
machine-readable dictionary which includes
11,306 verbs mapped to 13,508 word senses,
called synsets, and 114,648 common and proper
nouns mapped to 79,689 synsets. Each noun or
verb synset is associated with one of 41 broad
semantic categories, in order to organize the
lexicographer?s work of updating and managing
the lexicon (see Table 1). Since each lexicog-
rapher category groups together many synsets
they have been also called supersenses (Ciaramita
and Johnson, 2003). There are 26 supersenses
for nouns, 15 for verbs. This coarse-grained
ontology has a number of attractive features, for
the purpose of natural language processing. First,
the small size of the set makes it possible to build
a single tagger which has positive consequences
on robustness. Second, classes, although fairly
general, are easily recognizable and not too
abstract or vague. More importantly, similar word
senses tend to be merged together.
As an example, Table 2 summarizes all senses
of the noun ?box?. The 10 synsets are mapped
to 6 supersenses: ?artifact?, ?quantity?, ?shape?,
?state?, ?plant?, and ?act?. Three similar senses
(2), (7) and (9), and the probably related (8), are
merged in the ?artifact? supersense. This process
can help disambiguation because it removes sub-
595
1. {box} (container) ?he rummaged through a box of
spare parts? - n.artifact
2. {box, loge} (private area in a theater or grandstand
where a small group can watch the performance) ?the
royal box was empty? - n.artifact
3. {box, boxful} (the quantity contained in a box) ?he
gave her a box of chocolates? - n.quantity
4. {corner, box} (a predicament from which a skillful or
graceful escape is impossible) ?his lying got him into a
tight corner? - n.state
5. {box} (a rectangular drawing) ?the flowchart contained
many boxes? - n.shape
6. {box, boxwood} (evergreen shrubs or small trees) -
n.plant
7. {box} (any one of several designated areas on a ball
field where the batter or catcher or coaches are posi-
tioned) ?the umpire warned the batter to stay in the bat-
ter?s box? - n.artifact
8. {box, box seat} (the driver?s seat on a coach) ?an armed
guard sat in the box with the driver? - n.artifact
9. {box} (separate partitioned area in a public place for a
few people) ?the sentry stayed in his box to avoid the
cold? - n.artifact
10. {box} (a blow with the hand (usually on the ear)) ?I
gave him a good box on the ear? - n.act
Table 2. The noun ?box? in Wordnet: each line lists one
synset, the set of synonyms, a definition, an optional
example sentence, and the supersense label.
tle distinctions, which are hard to discriminate and
increase the size of the class space. One possi-
ble drawback is that senses which one might want
to keep separate, e.g., the most common sense
box/container (1), can be collapsed with others.
One might argue that all ?artifact? senses share
semantic properties which differentiate them from
the other senses and can support useful semantic
inferences. Unfortunately, there are no general so-
lutions to the problem of sense granularity. How-
ever, major senses identified by Wordnet are main-
tained at the supersense level. Hence, supersense-
disambiguated words are also, at least partially,
synset-disambiguated.
Since Wordnet includes both proper and com-
mon nouns, the new tagset suggests an extended
notion of named entity. As well as the usual
NER categories, ?person?, ?group?, ?location?,
and ?time?2, supersenses include categories such
as artifacts, which can be fairly frequent, but usu-
ally neglected. To a greater extent than in stan-
dard NER, research in Bio-NER has focused on
the adoption of richer ontologies for information
extraction. Genia (Ohta et al, 2002), for exam-
ple, is an ontology of 46 classes ? with annotated
2The supersense category ?group? is rather a superordi-
nate of ?organization? and has wider scope.
corpus ? designed for supporting information ex-
traction in the molecular biology domain. In addi-
tion, there is growing interest for extracting rela-
tions between entities, as a more useful type of IE
(cf. (Rosario and Hearst, 2004)).
Supersense tagging is inspired by similar con-
siderations, but in a domain-independent setting;
e.g., verb supersenses can label semantic interac-
tions between nominal concepts. The following
sentence (Example 1), extracted from the data ?
further described in Section 5.1 ? shows the infor-
mation captured by the supersense tagset:
(1) Clara Harrisn.person, one of the
guestsn.person in the boxn.artifact, stood
upv.motion and demandedv.communication
watern.substance.
As Example 1 shows there is more information
that can be extracted from a sentence than just
the names; e.g. the fact that ?Clara Harris? and
the following ?guests? are both tagged as ?person?
might suggest some sort of co-referentiality, while
the coordination of verbs of motion and commu-
nication, as in ?stood up and demanded?, might be
useful for language modeling purposes. In such a
setting, structured learning methods, e.g., sequen-
tial, can help tagging by taking the senses of the
neighboring words into account.
3 Related Work
Sequential models are common in NER, POS tag-
ging, shallow parsing, etc.. Most of the work in
WSD, instead, has focused on labeling each word
individually, possibly revising the assignments of
senses at the document level; e.g., following the
?one sense per discourse? hypothesis (Gale et al,
1992). Although it seems reasonable to assume
that occurrences of word senses in a sentence can
be correlated, hence that structured learning meth-
ods could be successful, there has not been much
work on sequential WSD. Segond et al (1997) are
possibly the first to have applied an HMM tag-
ger to semantic disambiguation. Interestingly, to
make the method more tractable, they also used
the supersense tagset and estimated the model on
Semcor. By cross-validation they show a marked
improvement over the first sense baseline. How-
ever, in (Segond et al, 1997) the tagset is used dif-
ferently, by defining equivalence classes of words
with the same set of senses. From a similar per-
spective, de Loupy et al (de Loupy et al, 1998)
596
also investigated the potential advantages of using
HMMs for disambiguation. More recently, vari-
ants of the generative HMM have been applied to
WSD (Molina et al, 2002; Molina et al, 2004)
and evaluated also on Senseval data, showing per-
formance comparable to the first sense baseline.
Previous work on prediction at the supersense
level (Ciaramita and Johnson, 2003; Curran, 2005)
has focused on lexical acquisition (nouns exclu-
sively), thus aiming at word type classification
rather than tagging. As far as applications are con-
cerned, it has been shown that supersense infor-
mation can support supervised WSD, by provid-
ing a partial disambiguation step (Ciaramita et al,
2003). In syntactic parse re-ranking supersenses
have been used to build useful latent semantic fea-
tures (Koo and Collins, 2005). We believe that
supersense tagging has the potential to be useful,
in combination with other sources of information
such as part of speech, domain-specific NER mod-
els, chunking or shallow parsing, in tasks such
as question answering and information extraction
and retrieval, where large amounts of text need
to be processed. It is also possible that this kind
of shallow semantic information can help build-
ing more sophisticated linguistic analysis as in full
syntactic parsing and semantic role labeling.
4 Sequence Tagging
We take a sequence labeling approach to learn-
ing a model for supersense tagging. Our goal is
to learn a function from input vectors, the obser-
vations from labeled data, to response variables,
the supersense labels. POS tagging, shallow pars-
ing, NP-chunking and NER are all examples of
sequence labeling tasks in which performance can
be significantly improved by optimizing the choice
of labeling over whole sequences of words, rather
than individual words. The limitations of the gen-
erative approach to sequence tagging, i. e. Hidden
Markov Models, have been overcome by discrim-
inative approaches proposed in recent years (Mc-
Callum et al, 2000; Lafferty et al, 2001; Collins,
2002; Altun et al, 2003). In this paper we apply
perceptron trained HMMs originally proposed in
(Collins, 2002).
4.1 Perceptron-trained HMM
HMMs define a probabilistic model for observa-
tion/label sequences. The joint model of an obser-
vation/label sequence (x,y), is defined as:
P (y,x) =
?
i
P (yi|yi?1)P (xi|yi)), (2)
where yi is the ith label in the sequence and xi is
the ith word. In the NLP literature, a common ap-
proach is to model the conditional distribution of
label sequences given the label sequences. These
models have several advantages over generative
models, such as not requiring questionable inde-
pendence assumptions, optimizing the conditional
likelihood directly and employing richer feature
representations. This task can be represented as
learning a discriminant function F : X ?Y ? IR,
on a training data of observation/label sequences,
where F is linear in a feature representation ? de-
fined over the joint input/output space
F (x,y;w) = ?w,?(x,y)?. (3)
? is a global feature representation, mapping each
(x,y) pair to a vector of feature counts ?(x,y) ?
IRd, where d is the total number of features. This
vector is given by
?(x,y) =
d?
i=1
|y|?
j=1
?i(yj?1, yj ,x). (4)
Each individual feature ?i typically represents a
morphological, contextual, or syntactic property,
or also the inter-dependence of consecutive la-
bels. These features are described in detail in Sec-
tion 4.2. Given an observation sequence x, we
make a prediction by maximizing F over the re-
sponse variables:
fw(x) = argmax
y?Y
F (x,y;w). (5)
This involves computing the Viterbi decoding with
respect to the parameter vector w ? IRd. The
complexity of the Viterbi algorithm scales linearly
with the length of the sequence.
There are different ways of estimatingw for the
described model. We use the perceptron algorithm
for sequence tagging (Collins, 2002). The per-
ceptron algorithm focuses on minimizing the error
rate, without involving any normalization factors.
This property makes it very efficient which is a de-
sirable feature in a task dealing with a large tagset
such as ours. Additionally, the performance of
perceptron-trained HMMs is very competitive on
a number of tasks; e.g., in shallow parsing, where
597
Algorithm 1 Hidden Markov average perceptron
algorithm.
1: Initialize w0 = ~0
2: for t = 1...., T do
3: Choose xi
4: Compute y? = argmaxy?Y F (xi,y;w)
5: if yi 6= y? then
6: wt+1 ? wt + ?(xi,yi)? ?(xi, y?)
7: end if
8: w = 1T
?
twt
9: end for
10: return w
the perceptron performance is comparable to that
of Conditional Random Field models (Sha and
Pereira, 2003), The tendency to overfit of the per-
ceptron can be mitigated in a number of ways in-
cluding regularization and voting. Here we apply
averaging and straightforwardly extended Collins
algorithm, summarized in Algorithm 1.
4.2 Features
We used the following combination of
spelling/morphological and contextual fea-
tures. For each observed word xi in the data ?
extracts the following features:
1. Words: xi, xi?1, xi?2, xi+1, xi+2;
2. First sense: supersense baseline prediction
for xi, fs(xi), cf. Section 5.3;
3. Combined (1) and (2): xi + fs(xi);
4. Pos: posi (the POS of xi), posi?1, posi?2,
posi+1, posi+2, posi[0], posi?1[0], posi?2[0],
posi+1[0], posi+2[0], pos commi if xi?s POS
tags is ?NN? or ?NNS? (common nouns), and
pos propi if xi?s POS is ?NNP? or ?NNPS?
(proper nouns);
5. Word shape: sh(xi), sh(xi?1), sh(xi?2),
sh(xi+1), sh(xi+2), where sh(xi) is as
described below. In addition shi = low
if the first character of xi is lowercase,
shi = cap brk if the first character of xi is up-
percase and xi?1 is a full stop, question or
exclamation mark, or xi is the first word of
the sentence, shi = cap nobrk otherwise;
6. Previous label: supersense label yi?1.
Word features (1) are morphologically simplified
using the morphological functions of the Word-
net library. The first sense feature (2) is the label
predicted for xi by the baseline model, cf. Sec-
tion 5.3. POS labels (4) were generated using
Brants? TnT tagger (Brants, 2002). POS features
of the form posi[0] extract the first character from
the POS label, thus providing a simplified repre-
sentation of the POS tag. Finally, word shape fea-
tures (5) are regular expression-like transforma-
tion in which each character c of a string s is sub-
stituted with X if c is uppercase, if lowercase, c
is substituted with x, if c is a digit it is substituted
with d and left as it is otherwise. In addition each
sequence of two or more identical characters c is
substituted with c?. For example, for s = ?Merrill
Lynch& Co.?, sh(s) = Xx ? Xx ?&Xx..
Exploratory experiments with richer feature
sets, including syntactic information, affixes, and
topic labels associated with words, did not result
in improvements in terms of performance. While
more experiments are needed to investigate the
usefulness of other sources of information, the fea-
ture set described above, while basic, offers good
generalization properties.
5 Experiments
5.1 Data
We experimented with the following data-sets3.
The Semcor corpus (Miller et al, 1993), a frac-
tion of the Brown corpus (Kuc?era and Francis,
1967) which has been manually annotated with
Wordnet synset labels. Named entities of the cat-
egories ?person?, ?location? and ?group? are also
annotated. The original annotation with Wordnet
1.6 synset IDs has been converted to the most re-
cent version 2.0 of Wordnet. Semcor is divided
in three parts: ?brown1? and ?brown2?, here re-
ferred to as ?SEM?, in which nouns, verbs, adjec-
tives and adverbs are annotated. In addition, the
section ?brownv?, ?SEMv? here, contains annota-
tions only for verbs. We also experimented with
the Senseval-3 English all-words tasks data (Sny-
der and Palmer, 2004), here called ?SE3?. The
Senseval all-words task evaluates the performance
of WSD systems on all open class words in com-
plete documents. The Senseval-3 data consists of
two Wall Street Journal Articles, ?wsj 1778? and
3These datasets are available in a con-
sistent format and can be downloaded from
http://www.cs.unt.edu/ rada/downloads.html
598
Dataset
Counts SE3 SEM SEMv
Sentences 300 20,138 17,038
Tokens 5,630 434,774 385,546
Supersenses 1,617 135,135 40,911
Verbs 725 47,710 40,911
Nouns 892 87,425 0
Avg-poly-N-WS 4.66 4.41 4.33
Avg-poly-N-SS 2.86 2.75 2.66
Avg-poly-V-WS 11.17 10.87 11.05
Avg-poly-V-SS 4.20 4.11 4.16
Table 3. Statistics of the datasets. The row ?Super-
senses? lists the number of instances of supersense
labels, partitioned, in the following two rows, between
verb and noun supersense labels. The lowest four rows
summarize average polysemy figures at the synset and
supersense level for both nouns and verbs.
?wsj 1695?, and a fiction excerpt, ?cl 23?, from
the unannotated portion of the Brown corpus. Ta-
ble 3 summarizes a few statistics about the compo-
sition of the datasets. The four lower rows report
the average polysemy of nouns (?N?) and verbs
(?V?), in each dataset, both at the synset level
(?WS?) and supersense (?SS?) level. The average
number of senses decreases significantly when the
more general sense inventory is considered.
We substituted the corresponding supersense to
each noun and verb synset in all three data-sets:
SEM, SEMv and SE3. All other tokens were
labeled ?0?. The supersense label ?noun.Tops?
refers to 45 synsets which lie at the very top
of the Wordnet noun hierarchy. Some of these
synsets are expressed by very general nouns such
as ?biont?, ?benthos?, ?whole?, and ?nothing?.
However, others undoubtedly refer to other super-
senses, for which they provide the label, such as
?food?, ?person?, ?plant? or ?animal?. Since these
nouns tend to be fairly frequent, it is confusing
and inconsistent to label them ?noun.Tops?; e.g.,
nouns such as ?chowder? and ?Swedish meatball?
would be tagged as ?noun.food?, but the noun
?food? would be tagged as ?noun.Tops?. For this
reason, in all obvious cases, we substituted the
?noun.Tops? label with the more specific super-
sense label for the noun4.
The SEMv dataset only includes supersense la-
bels for verbs. In order to avoid unwanted false
negatives, that is, thousands of nouns labeled ?0?,
4The nouns which are left with the ?noun.Top? label are:
entity, thing, anything, something, nothing, object, living
thing, organism, benthos, heterotroph, life, and biont.
we applied the following procedure. Rather than
using the full sentences from the SEMv dataset,
from each sentence we generated the fragments in-
cluding a verb but no common or proper nouns;
e.g., from a sentence such as ?Karns? ruling per-
tainedverb.stative to eight of the 10 cases.? only the
fragment ?pertainedverb.stative to eight of the 10?
is extracted and used for training.
Sometimes more than one label is assigned to
a word, in all data-sets. In these cases we adopted
the heuristic of only using the first label in the data
as the correct synset/supersense. We leave the ex-
tension of the tagger to the multilabel case for fu-
ture research. As for now, we can expect that this
solution will simply lower, somewhat, both the
baseline and the tagger performance. Finally, we
adopted a beginning (B) and continuation of entity
(I) plus no label (0), encoding; i.e., the actual class
space defines 83 labels.
5.2 Setup
The supersense tagger was trained on the Semcor
datasets SEM and SEMv. The only free parame-
ter to set in evaluation is the number of iterations
to perform T (cf. Algorithm 1). We evaluated the
model?s accuracy on Semcor by splitting the SEM
data randomly in training, development and evalu-
ation. In a 5-fold cross-validation setup the tagger
was trained on 4/5 of the SEM data, the remain-
ing data was split in two halves, one used to fix T
the other for evaluating performance on test. The
full SEMv data was always added to the training
portion of SEM. We also evaluated the model on
the Senseval-3 data, using the same value for T set
by cross-validation on the SEM data5. The order-
ing of the training instances is randomized across
different runs, therefore the algorithm outputs dif-
ferent results after each run, even if the evaluation
set is fixed, as is the case for the Senseval evalu-
ation. The variance in the results on the SE3 data
was measured in this way.
5.3 Baseline tagger
The first sense baseline is the supersense of the
most frequent synset for a word, according to
Wordnet?s sense ranking. This baseline is very
competitive inWSD tasks, and it is extremely hard
to improve upon even slightly. In fact, the baseline
has been proposed as a good alternative to WSD
5On average T is equal to 12 times the size of the training
data.
599
Semcor Senseval-3
Method Recall Precision F-score [?] Recall Precision F-score [?]
Rand 42.99 38.17 40.44 42.09 35.84 38.70
Baseline 69.25 63.90 66.47 68.65 60.10 64.09
Supersense-Tagger 77.71 76.65 77.18 0.45 73.74 67.60 70.54 0.21
Table 4. Summary of results for random and first sense baselines and supersense tagger, ? is the standard error
computed on the five trials results.
altogether (cf. (McCarthy et al, 2004)). For this
reason we include the first sense prediction as one
of the features of our tagging model.
We apply the heuristic as follows. First, in each
sentence, we identify the longest sequence which
has an entry in Wordnet as either noun or verb.
We carry out this step using the Wordnet?s library
functions, which perform also morphological sim-
plification. Hence, in Example 1 the entry ?stand
up? is detected, although also ?stand? has an en-
try in Wordnet. Then, each word identified in
this way is assigned its most frequent sense ? the
only one available if the word is unambiguous. To
reduce the number of candidate supersenses we
distinguish between common and proper nouns;
e.g. ?Savannah? (city/river) is distinguished from
?savannah? (grassland). This method improves
slightly the accuracy of the baseline which does
not distinguish between different types of nouns.
5.4 Results
Table 4 summarizes overall performance6. The
first line shows the accuracy of a baseline which
assigns possible supersenses of identified words at
random. The second line shows the performance
of the first sense baseline (cf. Section 5.3), the
marked difference between the two is a measure of
the robustness of the first sense heuristic. On the
Semcor data the tagger improves over the base-
line by 10.71%, 31.19% error reduction, while
on Senseval-3 the tagger improves over the base-
line by 6.45%, 17.96% error reduction. We can
put these results in context, although indirectly,
by comparison with the results of the Senseval-
3 all words task systems. There, with a base-
line of 62.40%, only 4 out of 26 systems per-
formed above the baseline, with the two best sys-
tems (Mihalcea and Faruque, 2004; Decadt et al,
2004) achieving an F-score of 65.2% (2.8% im-
provement, 7.45% error reduction). The system
based on the HMM tagger (Molina et al, 2004),
6Scoring was performed with a re-implementation of the
?conlleval? script .
achieved an F-score of 60.9%. The supersense
tagger improves mostly on precision, while also
improving on recall. Overall the tagger achieves
F-scores between 70.5 and 77.2%. If we compare
these figures with the accuracy of NER taggers
the results are very encouraging. Given the con-
siderably larger ? one order of magnitude ? class
space some loss has to be expected. Experiments
with augmented tagsets in the biomedical domain
also show performance loss with respect to smaller
tagsets; e.g., Kazama et al (2002) report an F-
score of 56.2% on a tagset of 25 Genia classes,
compared to the 75.9% achieved on the simplest
binary case. The sequence fragments from SEMv
contribute about 1% F-score improvement.
Table 5 focuses on subsets of the evaluation.
The upper part summarizes the results on Sem-
cor for the classes comparable to standard NER?s:
?person?, ?group?, ?location? and ?time?. How-
ever, these categories here are composed of com-
mon nouns as well as proper names/named enti-
ties. On this four tags the tagger achieves an aver-
age 82.46% F-score, not too far from NER results.
The lower portion of Table 5 summarizes the re-
sults on the five most frequent noun and verb su-
persense labels on the Senseval-3 data, providing
more specific evidence for the supersense tagger?s
disambiguation accuracy. The tagger outperforms
the first sense baseline on all categories, with the
exception of ?verb.cognition? and ?noun.person?.
The latter case has a straightforward explanation,
named entities (e.g., ?Phil Haney?, ?Chevron? or
?Marina District?) are not annotated in the Sense-
val data, while they are in Semcor. Hence the tag-
ger learns a different model for nouns than the one
used to annotate the Senseval data. Because of this
discrepancy the tagger tends to return false posi-
tives for some categories. In fact, the other noun
categories on which the tagger performs poorly in
SE3 are ?group? and ?location? (baseline 52.10
tagger 44.72 and baseline 47.62% tagger 47.54%
F-score). Naturally, the lower performance on
Senseval is also explained by the fact that the eval-
600
NER supersenses in Semcor
Supersense-Tagger Baseline
Supersense # Supersenses R P F R P F
n.person 1526 92.04 87.94 89.94 56.29 77.35 65.16
n.group 665 75.38 79.56 77.40 62.42 66.81 64.54
n.location 459 77.21 75.37 76.25 67.88 63.33 65.53
n.time 412 88.36 84.30 86.27 78.26 83.88 80.98
5 most frequent verb supersenses in Senseval-3
Supersense # Supersenses R P F R P F
v.stative 184 80.33 81.30 80.81 72,83 63.81 68.02
v.communication 88 77.53 83.36 80.33 71.91 74.42 73.14
v.motion 81 69.63 64.54 66.98 58.02 60.26 59.12
v.cognition 61 73.44 67.91 70.56 75.41 71.87 73.60
v.change 60 68.33 67.47 67.89 56.67 57.63 57.14
5 most frequent noun supersenses in Senseval-3
Supersense # Supersenses R P F R P F
n.person 148 92.24 60.49 73.06 89.12 79.39 83.97
n.artifact 131 80.91 77.73 79.29 74.24 75.97 75.10
n.act 96 61.46 72.37 66.45 58.33 65.12 61.54
n.cognition 67 45.80 52.87 49.06 49.28 46.58 47.89
n.event 60 70.33 89.83 78.87 71.67 75.44 73.50
Table 5. Summary of results of baseline and tagger on selected subsets of labels: NER categories evaluated on
Semcor (upper section), and 5 most frequent verb (middle) and noun (bottom) categories evaluated on Senseval.
uation comes from different sources than training.
6 Conclusions
In this paper we presented a novel approach to
broad-coverage word sense disambiguation and
information extraction. We defined a tagset based
on Wordnet supersenses, a much simpler and gen-
eral semantic model than Wordnet which, how-
ever, preserves significant polysemy information
and includes standard named entity recognition
categories. We showed that in this framework it is
possible to perform accurate broad-coverage tag-
ging with state of the art sequence learning meth-
ods. The tagger considerably outperformed the
most competitive baseline on both Semcor and
Senseval data. To the best of our knowledge the re-
sults on Senseval data provide the first convincing
evidence of the possibility of improving by con-
siderable amounts over the first sense baseline.
We believe both the tagset and the structured
learning approach contribute to these results. The
simplified representation obviously helps by re-
ducing the number of possible senses for each
word (cf. Table 3). Interestingly, the relative im-
provement in performance is not as large as the
relative reduction in polysemy. This indicates that
sense granularity is only one of the problems in
WSD. More needs to be understood concerning
sources of information, and processes, that affect
word sense selection in context. As far as the tag-
ger is concerned, we applied the simplest feature
representation, more sophisticated features can be
used, e.g., based on kernels, which might con-
tribute significantly by allowing complex feature
combinations. These results also suggest new di-
rections of research within this model. In partic-
ular, the labels occurring in each sequence tend
to coincide with predicates (verbs) and arguments
(nouns and named entities). A sequential depen-
dency model might not be the most accurate at
capturing the grammatical dependencies between
these elements. Other conditional models, e.g.,
designed on head to head, or similar, dependen-
cies could prove more appropriate.
Another interesting issue is the granularity of
the tagset. Supersenses seem more practical then
synsets for investigating the impact of broad-
coverage semantic tagging, but they define a very
simplistic ontological model. A natural evolution
of this kind of approach might be one which starts
by defining a semantic model at an intermediate
level of abstraction (cf. (Ciaramita et al, 2005)).
601
References
Y. Altun, T. Hofmann, and M. Johnson. 2003.
Discriminative Learning for Label Sequences via
Boosting. In Proceedings of NIPS 2003.
T. Brants. 2002. TnT - A Statistical Part-of-Speech
Tagger. In Proceedings of ANLP 2000.
X. Carreras, L. Marquez, and L. Padro. 2002. Named
Entity Extraction Using AdaBoost. In Proceedings
of CONLL 2002.
M. Ciaramita and M. Johnson. 2003. Supersense Tag-
ging of Unknown Nouns in WordNet. In Proceed-
ings of EMNLP 2003.
M. Ciaramita, T. Hofmann, and M. Johnson. 2003. Hi-
erarchical Semantic Classification: Word Sense Dis-
ambiguation with World Knowledge. In Proceed-
ings of IJCAI 2003.
M. Ciaramita, S. Sloman, M. Johnson, and E. Upfal.
2005. Hierarchical Preferences in a Broad-Coverage
Lexical Taxonomy. In Proceedings of CogSci 2005.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings
of EMNLP 2002, pages 1?8.
J. Curran. 2005. Supersense Tagging of Unknown
Nouns Using Semantic Similarity. In Proceedings
of ACL 2005, pages 26?33.
C. de Loupy, M. El-Beze, and P.F. Marteau. 1998.
Word Sense Disambiguation Using HMM Tagger.
In Proceedings of LREC 1998, pages 1255?1258.
B. Decadt, V. Hoste, W. Daelemans, and A. van der
Bosch. 2004. GAMBL, Genetic Algorithm Opti-
mization of Memory-Based WSD. In Proceedings
of SENSEVAL-3/ACL 2004.
S. Dingare, M. Nissim, J. Finkel, C. Manning, and
C. Grover. 2005. A System for Identifying Named
Entities in Biomedical Text: How Results from
Two Evaluations Reflect on Both the System and
the Evaluations. Comparative and Functional Ge-
nomics, 6:77?85.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Extraction through Classifier Combi-
nation. In Proceedings of CONLL 2003.
W. Gale, K. Church, and D. Yarowsky. 1992. One
Sense per Discourse. In Proceedings of the DARPA
Workshop on Speech and Natural Language.
J. Kazama, T. Makino, Y. Ohta, and J. Tsujii. 2002.
Tuning Support Vector Machines for Biomedical
Named Entity Recognition. In Proceedings of the
Workshop on Natural Language Processing in the
Biomedical Domain (ACL 2002).
T. Koo and M. Collins. 2005. Hidden-Variable Mod-
els for Discriminative Reranking. In Proceedings of
EMNLP 2005.
H. Kuc?era and W. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown Uni-
versity Press, Providence, RI.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML 2001, pages 282?289.
A. McCallum, D. Freitag, and F. Pereira. 2000. Max-
imum Entropy Markov Models for Information Ex-
traction and Segmentation. In Proceedings of ICML
2000, pages 591?598.
D. McCarthy, R. Koeling, and J. Carroll. 2004. Find-
ing Predominant Senses in Untagged Text. In Pro-
ceedings of ACL 2004.
R. Mihalcea and E. Faruque. 2004. SenseLearner:
Minimally Supervised Word Sense Disambiguation
for All Words in Open Text. In Proceedings of
SENSEVAL-3/ACL 2004.
G.A. Miller, C. Leacock, T. Randee, and R. Bunker.
1993. A Semantic Concordance. In Proceedings of
the 3 DARPA Workshop on Human Language Tech-
nology, pages 303?308.
A. Molina, F. Pla, and E. Segarra. 2002. A Hid-
den Markov Model Approach to Word Sense Dsi-
ambiguation. In Proceedings of IBERAMIA 2002.
A. Molina, F. Pla, and E. Segarra. 2004. WSD System
Based on Specialized Hidden Markov Model (upv-
shmm-eaw). In Proceedings of SENSEVAL-3/ACL
2004.
Y. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.
2002. The GENIA Corpus: An Annotated Research
Abstract Corpus in the Molecular Biology Domain.
In Proceedings of HLT 2002.
B. Rosario and M. Hearst. 2004. Classifying Seman-
tic Relations in Bioscience Text. In Proceedings of
ACL 2004).
F. Segond, A. Schiller, G. Grefenstette, and J.P.
Chanod. 1997. An Experiment in Semantic Tagging
Using Hidden Markov Model. In Proceedings of the
Workshop on Automatic Information Extraction and
Building of Lexical Semantic Resources (ACL/EACL
1997), pages 78?81.
F. Sha and F. Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of HLT-
NAACL 2003, pages 213?220.
B. Snyder and M. Palmer. 2004. The english All-
Words Tasks. In Proceedings of SENSEVAL-3/ACL
2004.
602
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481?1491,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Overcoming the Lack of Parallel Data in Sentence Compression
Katja Filippova and Yasemin Altun
Google
Brandschenkestr. 110
Zu?rich, 8004 Switzerland
katjaf|altun@google.com
Abstract
A major challenge in supervised sentence
compression is making use of rich feature rep-
resentations because of very scarce parallel
data. We address this problem and present
a method to automatically build a compres-
sion corpus with hundreds of thousands of
instances on which deletion-based algorithms
can be trained. In our corpus, the syntactic
trees of the compressions are subtrees of their
uncompressed counterparts, and hence super-
vised systems which require a structural align-
ment between the input and output can be suc-
cessfully trained. We also extend an exist-
ing unsupervised compression method with a
learning module. The new system uses struc-
tured prediction to learn from lexical, syntac-
tic and other features. An evaluation with hu-
man raters shows that the presented data har-
vesting method indeed produces a parallel cor-
pus of high quality. Also, the supervised sys-
tem trained on this corpus gets high scores
both from human raters and in an automatic
evaluation setting, significantly outperforming
a strong baseline.
1 Introduction and related work
Sentence compression is a paraphrasing task where
the goal is to generate sentences shorter than given
while preserving the essential content. A robust
compression system would be useful for mobile de-
vices as well as a module in an extractive sum-
marization system (Mani, 2001). Although a com-
pression may differ lexically and structurally from
the source sentence, to date most systems are ex-
tractive and proceed by deleting words from the
input (Knight & Marcu, 2000; Dorr et al, 2003;
Turner & Charniak, 2005; Clarke & Lapata, 2008;
Berg-Kirkpatrick et al, 2011, inter alia). To de-
cide which words, dependencies or phrases can be
dropped, (i) rule-based approaches (Grefenstette,
1998; Jing & McKeown, 2000; Dorr et al, 2003;
Zajic et al, 2007), (ii) supervised models trained
on parallel data (Knight & Marcu, 2000; Turner &
Charniak, 2005; McDonald, 2006; Gillick & Favre,
2009; Galanis & Androutsopoulos, 2010, inter alia)
and (iii) unsupervised methods which make use of
statistics collected from non-parallel data (Hori &
Furui, 2004; Zajic et al, 2007; Clarke & Lapata,
2008; Filippova & Strube, 2008) have been investi-
gated. Since it is infeasible to manually devise a set
of accurate deletion rules with high coverage, recent
research has been devoted to developing statistical
methods and possibly augmenting them with a few
linguistic rules to improve output readability (Clarke
& Lapata, 2008; Nomoto, 2009).
Supervised models. A major problem for super-
vised deletion-based systems is very limited amount
of parallel data. Many approaches make use of a
small portion of the Ziff-Davis corpus which has
about 1K sentence-compression pairs1. Other main
sources of training data are the two manually crafted
compression corpora from the University of Edin-
burgh (?written? and ?spoken?, each approx. 1.4K
pairs). Galanis & Androutsopoulos (2011) attempt
at getting more parallel data by applying a deletion-
based compressor together with an automatic para-
1The method of Galley & McKeown (2007) could benefit
from a larger number of sentences.
1481
phraser and generating multiple alternative com-
pressions. To our knowledge, this extended data set
has not yet been used for successful training of com-
pression systems.
Scarce parallel data makes it hard to go beyond a
small set of features and explore lexicalization. For
example, Knight & Marcu (2000) only induce non-
lexicalized CFG rules, many of which occurred only
once in the training data. The features of McDon-
ald (2006) are formulated exclusively in terms of
syntactic categories. Berg-Kirkpatrick et al (2011)
have as few as 13 features to decide whether a con-
stituent can be dropped. Galanis & Androutsopou-
los (2010) use many features when deciding which
branches of the input dependency tree can be pruned
but require a reranker to select most fluent com-
pressions from a pool of candidates generated in the
pruning phase, many of which are ungrammatical.
Even further data limitations exist for the algo-
rithms which operate on syntactic trees and refor-
mulate the compression task as a tree pruning one
(Nomoto, 2008; Filippova & Strube, 2008; Cohn &
Lapata, 2009; Galanis & Androutsopoulos, 2010, in-
ter alia). These methods are sensitive to alignment
errors, their performance degrades if the syntactic
structure of the compression is very different from
that of the input. For example, see Nomoto?s 2009
analysis of the poor performance of the T3 system of
Cohn & Lapata (2009) when retrained on a corpus of
loosely similar RSS feeds and news.
Unsupervised models. Few approaches require
no training data at all. The model of Hori & Fu-
rui (2004) combines scores estimated from mono-
lingual corpora to generate compressions of tran-
scribed speech. Adopting an integer linear program-
ming (ILP) framework, Clarke & Lapata (2008) use
hand-crafted syntactic constraints and an ngram lan-
guage model, trained on uncompressed sentences, to
find best compressions. The model of Filippova &
Strube (2008) also uses ILP but the problem is for-
mulated over dependencies and not ngrams. Condi-
tional probabilities and word counts collected from
a large treebank are combined in an ad hoc man-
ner to assess grammatical importance and informa-
tiveness of dependencies. Similarly, Woodsend &
Lapata (2010) formulate an ILP problem to gener-
ate news story highlights using precomputed scores.
Again, an ad hoc combination of the scores learned
independently of the task is used in the objective
function.
Contributions of this paper. Our work is moti-
vated by the obvious need for a large parallel corpus
of sentences and compressions on which extractive
systems can be trained. Furthermore, we want the
compressions in the corpus to be structurally very
close to the input. Ideally, in every pair, the com-
pression should correspond to a subtree of the input.
To this end, our contributions are three-fold:
? We describe an automatic procedure of con-
structing a parallel corpus of 250,000 sentence-
compression pairs such that the dependency
tree of the compression is a subtree of the
source tree. An evaluation with human raters
demonstrates high quality of the parallel data
in terms of readability and informativeness.
? We successfully apply the acquired data to train
a novel supervised compression system which
produces readable and informative compres-
sions without employing a separate reranker.
In particular, we start with the unsupervised
method of Filippova & Strube (2008) and re-
place the ad hoc edge weighting with a lin-
ear function over a rich feature representation.
The parameter vector is learned from our cor-
pus specifically for the compression task us-
ing structured prediction (Collins, 2002). The
new system significantly outperforms the base-
line and hence provides further evidence for the
utility of the parallel data.
? We demonstrate that sparse lexical features are
very useful for sentence compression, and that
a large parallel corpus is a requirement for ap-
plying them successfully.
The compression framework we adopt and the un-
supervised baseline are introduced in Section 2, the
training algorithm for learning edge weights from
parallel data is described in Section 3. In Section
4 we explain how to obtain the data and present an
evaluation of its quality. In Section 5 we compare
the baseline with our system and report the results
of an experiment with humans as well as the results
of an automatic evaluation.
1482
2 Framework and baseline
We adopt the unsupervised compression framework
of Filippova & Strube (2008) as our baseline and ex-
tend it to a supervised structured prediction problem.
In the experiments reported by Filippova & Strube
(2008), the system was evaluated on the Edinburgh
corpora. It achieved an F-score (Riezler et al, 2003)
higher than reported by other systems on the same
data under an aggressive compression rate and thus
presents a competitive baseline.
Tree pruning as optimization. In this framework,
compressions are obtained by deleting edges of the
source dependency structure so that (1) the retained
edges form a valid syntactic tree, and (2) their to-
tal edge weight is maximized. The objective func-
tion is defined over set X = {xe, e ? E} of bi-
nary variables, corresponding to the set E of the
source edges, subject to the structural and length
constraints,
f(X) =
?
e?E
xe ? w(e) (1)
Here, w(e) denotes the weight of edge e. This con-
strained optimization problem is solved under the
tree structure and length constraints using ILP. If xe
is resolved to 1, the respective edge is retained, oth-
erwise it is deleted. The tree structure constraints en-
force at most one parent for every node and structure
connectivity (i.e., no disconnected subtrees). Given
that length(node(e)) denotes the length of the node
to which edge e points and ? is the maximum per-
mitted length for the compression, the length con-
straint is simply
?
e?E
xe ? length(node(e)) ? ? (2)
Word limit is used in the original paper, whereas we
use character length which is more appropriate for
system comparisons (Napoles et al, 2011). If uni-
form weights are used in Eq. (1), the optimal so-
lution would correspond to a subtree covering as
many edges as possible while keeping the compres-
sion length under given limit.
The solution to the surface realization problem
(Belz et al, 2011) is standard: the words in the com-
pression subtree are put in the same order they are
found in the source.
Due to space limitations, we refer the reader to
(Filippova & Strube, 2008) for a detailed descrip-
tion on the method. Essential for the present discus-
sion is that source dependency trees are transformed
to dependency graphs in that (1) auxiliary, deter-
miner, preposition, negation and possessive nodes
are collapsed with their heads; (2) prepositions re-
place labels on the edges to their arguments; (3) the
dummy root node is connected with every inflected
verb. Figures 1(a)-1(b) illustrate most of the trans-
formations. The transformations are deterministic
and reversible, they can be implemented in a single
top-down tree traversal2.
The set E of edges in Eq. (1) is thus the set of
edges of the transformed dependency graph, like in
Fig. 1(b). A benefit of the transformations is that
function words and negation appear in the compres-
sion if and only if their head words are present.
Hence no separate constraints are required to en-
sure that negation or a determiner is preserved. The
dummy root node makes constraint formulation eas-
ier and also allows for the generation of compres-
sions from any finite clause of the source.
The described pruning optimization framework
is used both for the unsupervised baseline and for
our supervised system. The difference between the
baseline and our system is in how edge weights,
w(e)?s in Eq. (1), are instantiated.
Baseline edge weights. The precomputed edge
weights reflect syntactic importance as well as infor-
mativeness of the nodes they point to. Given edge
e from head node h to node n, the edge weight is
the product of the syntactic and the informativeness
weights,
w(e) = wsynt(e)? winfo(e) (3)
The syntactic weight is defined as
wsynt(e) = P (label(e)|lemma(h)) (4)
For example, verb kill may have multiple argu-
ments realized with dependency labels subj, dobj, in,
etc. However, these argument labels are not equally
likely, e.g., P (subj|kill) > P (in|kill). When forced
to prune an edge, the system would prefer to keep
2Some of the transformations are comparable to what is im-
plemented in the Stanford parser (de Marneffe et al, 2006).
1483
Britain ?s Ministry of Defense says a British soldier was killed in a roadside blast in southern Afghanistan
ps
poss
prep pobj
nsubj
amod
det
auxpass
nsubj
ccomp
prep
pobj
amod
det
prep
pobj
amod
root
(a) Source dependency tree
root Britain?s Ministry of Defense says British a soldier was killed roadside in a blast southern in Afghanistan
of
subj
root
root
ccomp
subjamod
in
amod
in
amod
(b) Transformed graph
root British a soldier was killed in a blast in Afghanistan
subj
root
amod in in
(c) Tree of extracted headline A British soldier was killed in a blast in
Afghanistan
A British soldier was killed in a blast in Afghanistan
det
amod
subj
auxpass prep
pobj
det prep pobj
root
(d) Tree of extracted headline with transformations undone
Figure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistan
the subject edge over the preposition-in edge since it
contributes more weight to the objective function.
The informativeness score is inspired by Wood-
send & Lapata (2012) and is defined as
winfo(e) =
Pheadline(lemma(n))
Particle(lemma(n))
(5)
This weight tells us how likely it is that a word
from an article appears in the headline. For exam-
ple, given two edges one of which points to verb say
and another one to verb kill, the latter would be pre-
ferred over the former because kill is more ?head-
liny? than say. When collecting counts for the syn-
tactic and informativeness scores, we used 9M news
articles crawled from the Internet, much more than
Filippova & Strube (2008). As a result our estimates
are probably more accurate than theirs.
Although both wsynt and winfo have a meaning-
ful interpretation, there is no guarantee that product
is the best way to combine the two when assign-
ing edge weights. Also, it is unclear how to inte-
grate other signals, such as distance to the root, node
length or information about the siblings, which pre-
sumably all play a role in determining the overall
edge importance.
3 Learning edge weights
Our supervised system differs from the unsupervised
baseline in that instead of relying on precomputed
scores, we define edge weight w(e) in Eq. (1) with a
linear function over a feature representation,
w(e) = w ? f(e) (6)
Here f(e) is a vector of binary variables for every
feature from the set of all possible but very infre-
quent features in the training set. f(e) has 1 for every
feature extracted for edge e and zero otherwise.
Table 1 gives an overview of the feature types
we use (edge e points from head h to node n).
Note that syntactic, structural and semantic features
are closed-class. For all the structural features but
char length, seven is used as maximum possible
value; all possible character lengths are bucketed
into six classes. All the features are local ? for a
given edge, contextual information is included about
1484
syntactic label(e); for e* to h, label(e*); pos(h); pos(n)
structural depth(n); #children(n); #children(h); char length(n); #words in(n)
semantic NE tag(h); NE tag(n); is negated(n)
lexical lemma(n); lemma(h)-label(e); for e* to n?s siblings, lemma(h)-label(e*)
Table 1: Types of features extracted for edge e from h to n
the head and the target nodes, and the siblings as
well as the children of the latter. The negation fea-
ture is only applicable to verb nodes which contain
a negative particle, like not, after the tree transfor-
mations. Lexical features which combine lemmas
and syntactic labels are inspired by the unsupervised
baseline and are very sparse.
In what follows, our assumption is that we have a
compression corpus at our disposal where for every
input sentence there is a correct ?oracle? compres-
sion such that its transformed parse tree matches a
subtree of the transformed input graph. Given such
a corpus, we can apply structured prediction meth-
ods to learn the parameter vector w. In our study
we employ an averaged variant of online structured
perceptron (Collins, 2002). In the context of sen-
tence fusion, a similar dependency structure prun-
ing framework and a similar learning approach was
adopted by Elsner & Santhanam (2011).
At every iteration, for every input graph, we find
the optimal solution with ILP under the current pa-
rameter vector w. The maximum permitted com-
pression length is set to be the same as the length
of the oracle compression. Since the oracle com-
pression is a subtree of the input graph, it represents
a feasible solution for ILP. The parameter vector is
updated if there is a mismatch between the predicted
and the oracle sets of edges for all the features with
a non-zero net count. More formally, given an input
graph with the set of edges E, oracle compression
C ? E and compression Ct ? E predicted at itera-
tion t , the parameter update vector at t+ 1 is given
by
wt+1 = wt +
?
e?C\Ct
f(e)?
?
e?Ct\C
f(e) (7)
w is averaged over all the wt?s so that features
whose weight fluctuated a lot during training are pe-
nalized (Freund & Shapire, 1999).
Of course, training a model with a large number
of features, such as a lexicalized model, is only pos-
sible if there is a large compression corpus where
the dependency tree of the compression is a subtree
of the source sentence. In the next section we in-
troduce our method of getting a sufficient amount of
such data.
4 Acquiring parallel data automatically
In this section we explain how we obtained a parallel
corpus of sentences and compressions. The underly-
ing idea is to harvest news articles from the Internet
where the headline appears to be similar to the first
sentence and use it to find an extractive compression
of the sentence.
Collecting headline-sentence pairs. Using a
news crawler, we collected a corpus of news arti-
cles in English from the Internet. Similarly to previ-
ous work (Dolan et al, 2004; Wubben et al, 2009;
Bejan & Harabagiu, 2010, inter alia), the Google
News service3 was used to identify news. From ev-
ery article, the headline and the first sentence, which
are known to be semantically similar (Dorr et al,
2003), were extracted. Predictably, very few head-
lines are extractive compressions of the first sen-
tence, therefore simply looking for pairs where the
headline is a subsequence of the words from the first
sentence would not solve the problem of getting a
large amount of parallel data. Importantly, headlines
are syntactically quite different from ?normal? sen-
tences. For example, they may have no main verb,
omit determiners and appear incomplete, making it
hard for a supervised deletion-based system to learn
useful rules. Moreover, we observed poor parsing
accuracy for headlines which would make syntactic
annotations for headlines hardly useful.
Thus, instead of taking the headline as it is, we use
it to find a proper extractive compression of the sen-
3http://news.google.com, Jan-Dec 2012.
1485
tence by matching lemmas of content words (nouns,
verbs, adjectives, adverbs) and coreference IDs of
entities from the headline with those of the sentence.
The exact procedure is as follows (H, S and T stand
for headline, sentence and transformed graph of the
sentence):
PREPROCESSING H and S are preprocessed in a
standard way: tokenized, lemmatized, PoS and NE
tagged. Additionally, S is parsed with a dependency
parser (Nivre, 2006) and transformed as described in
Section 2 to obtain T. Finally, pronominal anaphora
is resolved in S. Recall that S is the first sentence,
so the antecedent must be located in a preceding,
higher-level clause.
FILTERING To restrict the corpus to grammatical
and informative headlines, we implemented a cas-
cade of filters. Pair (H, S) is discarded if any of the
questions in Table 2 is answered positively.
Is H a question?
Is H or S too short? (less than four word tokens)
Is H about as long as S? (min ratio: 1.5)
Does H lack a verb?
Does H begin with a verb?
Is there a noun, verb, adj, adv lemma from H
not found in S?
Are the noun, verb, adj, adv lemmas from H
found in S in a different order?
Table 2: Filters applied to candidate pair (H, S)
MATCHING Given the content words of H, a sub-
set of nodes in T is selected based on lemma or
coreference identity of the main (head) word in the
nodes. For example, the main word of a collapsed
node in T, which covers two words was killed, is
killed; was is its child attached with label aux in the
untransformed parse tree. This node is marked if H
contains word killed or killing because of the lemma
identity. In some cases there are multiple possible
matches. For example, given S Barack Obama said
he will attend G20 and H mentioning Obama, both
Barack Obama and he nodes are marked in T. Once
all the nodes in T which match content words and
entities from H are identified, a minimum subtree
covering these nodes is found such that every word
or entity from H occurs as many times in T as in
H. So if H mentions Obama only once, then either
Barack Obama or he must be covered by the subtree
but not both. This minimum subtree corresponds to
an extractive headline, H*, which we generate by
ordering the surface forms of all the words in the
subtree nodes by their offsets in S. Finally, the char-
acter length of H* is compared with the length of
H. If H* is much longer than H, the pair (H, S) is
discarded (max ratio 1.5).
As an illustration to the procedure, consider the
example from Figure 1 with the extracted headline
and its tree presented in Figure 1(c). Given the
headline British soldier killed in Afghanistan, the
extracted headline would be A British soldier was
killed in a blast in Afghanistan. The lemmas british,
soldier, kill, afghanistan from the headline match the
nodes British, a soldier, was killed, in Afghanistan
in the transformed graph. The node in a blast is
added because it is on the path from was killed to in
Afghanistan. Of course, it is possible to determinis-
tically undo the transformations in order to obtain a
standard dependency tree. In this case the extracted
headline would still correspond to a subtree of the
input (compare Fig. 1(d) with Fig. 1(a)). Also note
that a similar procedure can be implemented for con-
stituency parses.
The resulting corpus consists of 250K tuples (S,
T, H, H*), Appendix provides more examples of
source sentences, original headlines and extracted
headlines. We did not attempt to tune the values for
minimum/maximum length and ratio ? lower thresh-
olds may have produced comparable results.
Evaluating data quality. The described proce-
dure produces a comparatively large compression
corpus but how good are automatically constructed
compressions? To answer this question, we ran-
domly selected 50 tuples from the corpus and set up
an experiment with human raters to validate and as-
sess data quality in terms of readability4 and infor-
mativeness5 which are standard measures of com-
pression quality (Clarke & Lapata, 2006). Raters
were asked to read a sentence and a compression
(original H or extracted H* headline) and then rate
the compression on two five-point scales. Three rat-
ings were collected for every item. Table 3 gives
4Also called grammaticality and fluency.
5Also called importance and representativeness.
1486
average ratings with standard deviation.
AVG read AVG info
ORIG. HEADLINE 4.36 (0.75) 3.86 (0.79)
EXTR. HEADLINE 4.26 (1.01) 3.70 (1.04)
Table 3: Results for two kinds of headlines
In terms of readability and informativeness the
extracted headlines are comparable with human-
written ones: at 95% confidence there is no statis-
tically significant difference between the two.
Encouraged by the results of the validation exper-
iment we proceeded to our next question: Can a su-
pervised compression system be successfully trained
on this corpus?
5 System evaluation and discussion
From the corpus of 250K tuples we used 100K to
get pairs of extracted headlines and sentences for
training (on the development set we did not observe
much improvement from using more training data),
250 for development and the rest for testing. We
ran the learning algorithm for 20 iterations, checking
the performance on the development set. Features
which applied to less than 20 edges were pruned,
the size of the feature set is about 28K.
5.1 Evaluation with humans
50 pairs of original headlines and sentences (differ-
ent from the data validation set in Sec. 4) were ran-
domly selected for an evaluation with humans from
the test data. As in the data quality validation ex-
periment, we asked raters to assess the readability
and informativeness of proposed compressions for
the unsupervised system, our system and human-
written headlines. The latter provide us with upper
bounds on the evaluation criteria. Three ratings per
item per parameter were collected. To get compara-
ble results, the unsupervised and our systems used
the same compression rate: for both, the requested
maximum length was set to the length of the head-
line. Table 4 summarizes the results.
The results indicate that the trained model signifi-
cantly outperforms the unsupervised system, getting
particularly good marks for readability. The differ-
ence in readability between our system and original
headlines is not statistically significant. Note that
AVG read AVG info
ORIG. HEADLINE 4.66? 4.10??
OUR SYSTEM 4.30? 3.52?
UNSUP. SYSTEM 3.70 2.70
Table 4: Results for the systems and original headline: ?
and ? stand for significantly better than Unsupervised and
Our system at 95% confidence, respectively
the unsupervised baseline is also capable of generat-
ing readable compressions but does a much poorer
job in selecting most important information. Our
trained model successfully learned to optimize both
scores. We refer the reader to Appendix for input
and compression examples. Note that the ratings for
the human-written headlines in this experiment are
slightly different from the ratings in the data valida-
tion experiment because a different data sample was
used.
5.2 Automatic evaluation
Our automatic evaluation had the goal of explic-
itly addressing two relevant questions related to our
claims about (1) the benefits of having a large paral-
lel corpus and (2) employing a supervised approach
with a rich feature representation.
1. Our primary motivation for collecting parallel
data has been that having access to sparse lex-
ical features, which considerably increase the
feature space, would benefit compression sys-
tems. But is it really the case for sentence com-
pression? Can a comparable performance be
achieved with a closed, moderately sized set of
dense, non-lexical features? If yes, then a large
compression corpus is probably not needed.
Furthermore, to demonstrate that a large corpus
is not only sufficient but also necessary to learn
weights for thousands of features, we need to
compare the performance of the system when
trained on the full data set and a small portion
of it.
2. The syntactic and informativeness scores in Eq.
(3) were calculated over millions of news arti-
cles and do provide us with meaninful statis-
tics (see Sec. 2). Is there any benefit in re-
placing those scores with weights learned for
1487
their feature counterparts? Recall that one of
our feature types in Table 1 is the concate-
nation of lemma(h) (parent lemma) and la-
bel(e) which relies on the same information
as wsynt = P (label(e)|lemma(h)). The fea-
ture counterpart of winfo defined in Eq. (5) is
lemma(n)?the lemma of the node to which edge
points. How would the supervised system per-
form against the unsupervised one, if it only ex-
tracted features of these two types?
To answer these questions, we sampled 1,000 tu-
ples from the unused test data and measured F1
score (Riezler et al, 2003) by comparing the trees
of the generated compression and the ?correct?, ex-
tracted headline. The systems we compared are the
unsupervised baseline (UNSUP. SYSTEM) and the
supervised model trained on three kinds of feature
sets: (1) SYNT-INFO FEATURES, corresponding to
the supervised training of the unsupervised base-
line model (i.e., lemma(h)-label(e) and lemma(n));
(2) NON-LEX FEATURES, corresponding to a dense,
non-lexical feature representation (i.e., all the fea-
ture types from Table 1 excluding the three involv-
ing lemmas); (3) ALL FEATURES (same as OUR
SYSTEM). Additionally, we trained the system on
10% of the data?10K as opposed to 100K tuples,
ALL FEATURES (10K)?for 20 iterations ignoring
features which applied to less than three edges6. As
before, the same compression rate was used for all
the systems. The results are summarized in Table 5.
F1 score #features
UNSUP. SYSTEM 52.3 N.A.
SYNT-INFO FEATURES 75.0 12,490
NON-LEX FEATURES 79.6 330
ALL FEATURES 84.3 27,813
ALL FEATURES (10K) 81.4 22,529
Table 5: Results for the unsupervised baseline and the
supervised system trained on three kinds of feature sets
Clearly, having more features, lexicalized and un-
lexicalized, is important: there is a significant im-
6Recall from the beginning of the section that for the full
(100K) training set the threshold was set to 20 with no tuning.
For the 10K training set, we tried values of two, three, five and
varied the number of iterations. The result we report is the high-
est we could get for 10K.
provement in going beyond the closed set of 330
non-lexical features to all, from 79.6 to 84.3 points.
Moreover, successful training requires a large cor-
pus since the performance of the system degrades if
only 10K training instances are used. Note that this
number already exceeds all the existing compression
corpora taken together. Hence, sparse lexical fea-
tures are useful for compression and a large paral-
lel corpus is a requirement for successful supervised
training.
Concerning our second question, learning feature
weights from the data produces significantly better
results than the hand-crafted way of making use of
the same information, even if a much larger data
set is used to collect statistics. We observed a dra-
matic increase from 52.3 to 75.0 points. Thus, we
may conclude that training with dense and sparse
features directly from data definitely improves the
performance of the dependency pruning system.
5.3 Discussion
It is important to note that the data we used is chal-
lenging: first sentences in news articles tend to be
long, in fact longer than other news sentences, which
implies less reliable syntactic analysis and noisier
input to the syntax-based systems. In the test set
we used for the evaluation with humans, the mean
sentence length is 165 characters. The average com-
pression rate in characters is 0.46 ? 0.16 which is
quite aggressive7. Recall that we used the very same
framework for the unsupervised baseline and our
system as well as the same compression rate. All the
preprocessing errors affect both systems equally and
the comparison of the two is fair. Predictably, wrong
syntactic parses significantly increase chances of an
ungrammatical compression, and parser errors seem
to be a major source of readability deficiencies.
A property of the described compression frame-
work is that a desired compression length is ex-
pected to be provided by the user. This can be seen
both as a strength and as a weakness, depending on
the application. In a scenario where mobile devices
with a limited screen size are used, or in a summa-
rization scenario where a total summary length is
provided (see the DUC/TAC guidelines8), being able
7We follow the standard terminology where smaller values
imply shorter compressions.
8http://www.nist.gov/tac/
1488
to specify a length is definitely an advantage. How-
ever, one can also think of other applications where
the user does not have a strict length constraint but
wants the text to be somewhat shorter. In this case,
a reranker which compares compressions generated
for a range of possible lengths can be employed to
find a single compression (e.g., mean edge weight in
the solution or a language model-based score).
6 Conclusions
We have addressed a major problem for supervised
extractive compression models ? the lack of a large
parallel corpus. To this end, we presented a method
to automatically build such a corpus from web doc-
uments available on the Internet. An evaluation
with humans demonstrates that the quality of the
corpus is high ? the compressions are grammati-
cal and informative. We also significantly improved
a competitive unsupervised method achieving high
readability and informativeness scores by incorpo-
rating thousands of features and learning the feature
weights from our corpus. This result further con-
firms the practical utility of the automatically ob-
tained data. We have shown that employing lexi-
cal features is important for sentence compression,
and that our supervised module can successfully
learn their weights from the corpus. To our knowl-
edge, we are the first to empirically demonstrate that
sparse features are useful for compression and that a
large parallel corpus is a requirement for a success-
ful learning of their weights. We believe that other
supervised deletion-based systems can benefit from
our work.
Acknowledgements: The authors are thankful to
the EMNLP reviewers for their feedback and sug-
gestions.
Appendix
The appendix presents examples of source sentences
(S), original headlines (H), extracted headlines (H*),
unsupervised baseline (U) and our system (O) com-
pressions.
References
Bejan, C. & S. Harabagiu (2010). Unsupervised
event coreference resolution with rich linguistic
features. In Proc. of ACL-10, pp. 1412?1422.
Belz, A., M. White, D. Espinosa, E. Kow, D. Hogan
& A. Stent (2011). The first surface realization
shared task: Overview and evaluation results. In
Proc. of ENLG-11, pp. 217?226.
Berg-Kirkpatrick, T., D. Gillick & D. Klein (2011).
Jointly learning to extract and compress. In Proc.
of ACL-11.
Clarke, J. & M. Lapata (2006). Models for sen-
tence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proc. of COLING-ACL-06, pp. 377?385.
Clarke, J. & M. Lapata (2008). Global inference
for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Cohn, T. & M. Lapata (2009). Sentence compres-
sion as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Collins, M. (2002). Discriminative training methods
for Hidden Markov Models: Theory and exper-
iments with perceptron algorithms. In Proc. of
EMNLP-02, pp. 1?8.
de Marneffe, M.-C., B. MacCartney & C. D. Man-
ning (2006). Generating typed dependency parses
from phrase structure parses. In Proc. of LREC-
06, pp. 449?454.
Dolan, B., C. Quirk & C. Brokett (2004). Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzer-
land, 23?27 August 2004, pp. 350?356.
Dorr, B., D. Zajic & R. Schwartz (2003). Hedge
trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the Text Summa-
rization Workshop at HLT-NAACL-03, Edmonton,
Alberta, Canada, 2003, pp. 1?8.
1489
S Country star Sara Evans has married former University of Alabama quarterback Jay Barker.
H Country star Sara Evans marries
H* Country star Sara Evans has married
U Sara Evans has married Jay Barker
O Sara Evans has married Jay Barker
S Intel would be building car batteries, expanding its business beyond its core strength, the company said in a statement
H Intel to build car batteries
H* Intel would be building car batteries
U would be building the company said
O Intel would be building car batteries
S A New Orleans Saints team spokesman says tight end Jeremy Shockey was taken to a hospital but is doing fine.
H Spokesman: Shockey taken to hospital, doing fine
H* spokesman says Jeremy Shockey was taken to a hospital but is doing fine
U A New Orleans Saints team spokesman says Jeremy Shockey was taken
O tight end Jeremy Shockey was taken to a hospital but is doing fine
S President Obama declared a major disaster exists in the State of Florida and ordered Federal aid to supplement
State and local recovery efforts in the area struck by severe storms, flooding, tornadoes, and straight-line winds
beginning on May 17, 2009, and continuing.
H President Obama declares major disaster exists in the State of Florida
H* President Obama declared a major disaster exists in the State of Florida
U President Obama declared a major disaster exists and ordered Federal aid
O President Obama declared a major disaster exists in the State of Florida
S Regulators Friday shut down a small Florida bank, bringing to 119 the number of US bank failures this year amid
mounting loan defaults.
H Regulators shut down small Florida bank
H* Regulators shut down a small Florida bank
U shut down bringing the number of failures
O Regulators shut down a small Florida bank
S Three men were arrested Wednesday night and Dayton police said their arrests are in connection to a west Dayton
bank robbery.
H 3 men arrested in connection with Bank robbery
H* Three men were arrested are in connection to a bank robbery
U were arrested and Dayton police said their arrests are
O Three men were arrested and police said their arrests are
S The government and the social partners will resume the talks on the introduction of the so-called crisis tax,
which will be levied on all salaries, pensions and incomes over HRK 3,000.
H Government, social partners to resume talks on introduction of ?crisis? tax.
H* The government and the social partners will resume the talks on the introduction of the crisis tax
U The government will resume the talks on the introduction of the crisis tax which will be levied
O The government and the social partners will resume the talks on the introduction of the crisis tax
S England star David Beckham may have the chance to return to AC Milan after the Italian club?s coach said
he was open to his move on Sunday.
H Beckham has chance of returning to Milan
H* David Beckham may have the chance to return to AC Milan
U David Beckham may have the chance to return said star was
O David Beckham may have the chance to return to AC Milan
S Eastern Health and its insurance company have accepted liability for some patients involved in the breast cancer
testing scandal, according to a statement released Friday afternoon.
H Eastern Health accepts liability for some patients
H* Eastern Health have accepted liability for some patients
U Health have accepted liability according to a statement
O Eastern Health have accepted liability for some patients
S Frontier Communications Corp., a provider of phone, TV and Internet services, said Thursday
it has started a cash tender offer to purchase up to $700 million of its notes.
H Frontier Communications starts tender offer for up to $700 million of notes
H* Frontier Communications has started a tender offer to purchase $700 million of its notes
U Frontier Communications said Thursday a provider has started a tender offer
O Frontier Communications has started a tender offer to purchase $700 million of its notes
1490
Elsner, M. & D. Santhanam (2011). Learning to fuse
disparate sentences. In Proceedings of the Work-
shop on Monolingual Text-to-text Generation, Prt-
land, OR, June 24 2011, pp. 54?63.
Filippova, K. & M. Strube (2008). Dependency tree
based sentence compression. In Proc. of INLG-
08, pp. 25?32.
Freund, Y. & R. E. Shapire (1999). Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Galanis, D. & I. Androutsopoulos (2010). An ex-
tractive supervised two-stage method for sentence
compression. In Proc. of NAACL-HLT-10, pp.
885?893.
Galanis, D. & I. Androutsopoulos (2011). A new
sentence compression dataset and its use in an ab-
stractive generate-and-rank sentence compressor.
In Proc. of UCNLG+Eval-11, pp. 1?11.
Galley, M. & K. R. McKeown (2007). Lexicalized
Markov grammars for sentence compression. In
Proc. of NAACL-HLT-07, pp. 180?187.
Gillick, D. & B. Favre (2009). A scalable global
model for summarization. In ILP for NLP-09, pp.
10?18.
Grefenstette, G. (1998). Producing intelligent tele-
graphic text reduction to provide an audio scan-
ning service for the blind. In Working Notes of
the Workshop on Intelligent Text Summarization,
Palo Alto, Cal., 23 March 1998, pp. 111?117.
Hori, C. & S. Furui (2004). Speech summariza-
tion: An approach through word extraction and
a method for evaluation. IEEE Transactions on
Information and Systems, E87-D(1):15?25.
Jing, H. & K. McKeown (2000). Cut and paste based
text summarization. In Proc. of NAACL-00, pp.
178?185.
Knight, K. & D. Marcu (2000). Statistics-based
summarization ? step one: Sentence compression.
In Proc. of AAAI-00, pp. 703?711.
Mani, I. (2001). Automatic Summarization. Amster-
dam, Philadelphia: John Benjamins.
McDonald, R. (2006). Discriminative sentence com-
pression with soft syntactic evidence. In Proc. of
EACL-06, pp. 297?304.
Napoles, C., C. Callison-Burch, J. Ganitkevitch &
B. Van Durme (2011). Paraphrastic sentence com-
pression with a character-based metric: Tighten-
ing without deletion. In Proceedings of the Work-
shop on Monolingual Text-to-text Generation, Prt-
land, OR, June 24 2011, pp. 84?90.
Nivre, J. (2006). Inductive Dependency Parsing.
Springer.
Nomoto, T. (2008). A generic sentence trimmer with
CRFs. In Proc. of ACL-HLT-08, pp. 299?307.
Nomoto, T. (2009). A comparison of model free ver-
sus model intensive approaches to sentence com-
pression. In Proc. of EMNLP-09, pp. 391?399.
Riezler, S., T. H. King, R. Crouch & A. Zaenen
(2003). Statistical sentence condensation using
ambiguity packing and stochastic disambiguation
methods for Lexical-Functional Grammar. In
Proc. of HLT-NAACL-03, pp. 118?125.
Turner, J. & E. Charniak (2005). Supervised and
unsupervised learning for sentence compression.
In Proc. of ACL-05, pp. 290?297.
Woodsend, K. & M. Lapata (2010). Automatic gen-
eration of story highlights. In Proc. of ACL-10,
pp. 565?574.
Woodsend, K. & M. Lapata (2012). Multiple as-
pect summarization using Integer Linear Pro-
gramming. In Proc. of EMNLP-12, pp. 233?243.
Wubben, S., A. van den Bosch, E. Krahmer &
E. Marsi (2009). Clustering and matching head-
lines for automatic paraphrase acquisition. In
Proc. of ENLG-09, pp. 122?125.
Zajic, D., B. J. Dorr, J. Lin & R. Schwartz (2007).
Multi-candidate reduction: Sentence compression
as a tool for document summarization tasks. In-
formation Processing & Management, Special Is-
sue on Text Summarization, 43(6):1549?1570.
1491
