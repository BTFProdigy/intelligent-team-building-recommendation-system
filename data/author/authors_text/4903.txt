PENS: A Machine-aided English Writing System
for Chinese Users
Ting Liu1 Ming Zhou Jianfeng Gao Endong Xun Changning Huang
Natural Language Computing Group, Microsoft Research China, Microsoft Corporation
5F, Beijing Sigma Center
100080 Beijing, P.R.C.
{ i-liutin, mingzhou, jfgao, i-edxun, cnhuang@microsoft.com }
Abstract
Writing English is a big barrier for most
Chinese users. To build a computer-aided system
that helps Chinese users not only on spelling
checking and grammar checking but also on
writing in the way of native-English is a
challenging task. Although machine translation is
widely used for this purpose, how to find an
efficient way in which human collaborates with
computers remains an open issue. In this paper,
based on the comprehensive study of Chinese
users requirements, we propose an approach to
machine aided English writing system, which
consists of two components: 1) a statistical
approach to word spelling help, and 2) an
information retrieval based approach to
intelligent recommendation by providing
suggestive example sentences. Both components
work together in a unified way, and highly
improve the productivity of English writing. We
also developed a pilot system, namely PENS
(Perfect ENglish System). Preliminary
experiments show very promising results.
Introduction
With the rapid development of the Internet,
writing English becomes daily work for
computer users all over the world. However, for
Chinese users who have significantly different
culture and writing style, English writing is a big
barrier. Therefore, building a machine-aided
English writing system, which helps Chinese
users not only on spelling checking and grammar
checking but also on writing in the way of
native-English, is a very promising task.
Statistics shows that almost all Chinese
users who need to write in English1 have enough
knowledge of English that they can easily tell the
difference between two sentences written in
Chinese-English and native-English, respectively.
Thus, the machine-aided English writing system
should act as a consultant that provide various
kinds of help whenever necessary, and let users
play the major role during writing. These helps
include:
1) Spelling help: help users input hard-to-spell
words, and check the usage in a certain
context simultaneously;
2) Example sentence help: help users refine the
writing by providing perfect example
sentences.
Several machine-aided approaches have
been proposed recently. They basically fall into
two categories, 1) automatic translation, and 2)
translation memory. Both work at the sentence
level. While in the former, the translation is not
readable even after a lot of manually editing. The
latter works like a case-based system, in that,
given a sentence, the system retrieve similar
sentences from translation example database, the
user then translates his sentences by analogy. To
build a computer-aided English writing system
that helps Chinese users on writing in the way of
native-English is a challenging task. Machine
translation is widely used for this purpose, but
how to find an efficient way in which human
collaborates well with computers remains an
open issue. Although the quality of fully
automatic machine translation at the sentence
level is by no means satisfied, it is hopeful to
1 Now Ting Liu is an associate professor in Harbin
Institute of Technology, P.R.C.
provide relatively acceptable quality translations
at the word or short phrase level. Therefore, we
can expect that combining word/phrase level
automatic translation with translation memory
will achieve a better solution to machine-aided
English writing system [Zhou, 95].
In this paper, we propose an approach to
machine aided English writing system, which
consists of two components: 1) a statistical
approach to word spelling help, and 2) an
information retrieval based approach to
intelligent recommendation by providing
suggestive example sentences. Both components
work together in a unified way, and highly
improve the productivity of English writing. We
also develop a pilot system, namely PENS.
Preliminary experiments show very promising
results.
The rest of this paper is structured as follows.
In section 2 we give an overview of the system,
introduce the components of the system, and
describe the resources needed. In section 3, we
discuss the word spelling help, and focus the
discussion on Chinese pinyin to English word
translation. In addition, we describe various
kinds of word level help functions, such as
automatic translation of Chinese word in the form
of either pinyin or Chinese characters, and
synonym suggestion, etc. We also describe the
user interface briefly. In section 4, an effective
retrieval algorithm is proposed to implement the
so-called intelligent recommendation function. In
section 5, we present preliminary experimental
results. Finally, concluding remarks is given in
section 6.
1 System Overview
1.1 System Architecture
Figure 1 System Architecture
There are two modules in PENS. The first is
called the spelling help. Given an English word,
the spelling help performs two functions, 1)
retrieving its synonym, antonym, and thesaurus;
or 2) automatically giving the corresponding
translation of Chinese words in the form of
Chinese characters or pinyin. Statistical machine
translation techniques are used for this translation,
and therefore a Chinese-English bilingual
dictionary (MRD), an English language model,
and an English-Chinese word- translation model
(TM) are needed. The English language model is
a word trigram model, which consists of
247,238,396 trigrams, and the vocabulary used
contains 58541 words. The MRD dictionary
contains 115,200 Chinese entries as well as their
corresponding English translations, and other
information, such as part-of-speech, semantic
classification, etc. The TM is trained from a
word-aligned bilingual corpus, which occupies
approximately 96,362 bilingual sentence pairs.
The second module is an intelligent
recommendation system. It employs an effective
sentence retrieval algorithm on a large bilingual
corpus. The input is a sequence of keywords or a
short phrase given by users, and the output is
limited pairs bilingual sentences expressing
relevant meaning with users? query, or just a few
pairs of bilingual sentences with syntactical
relevance.
1.2 Bilingual Corpus Construction
We have collected bilingual texts extracted
from World Wide Web bilingual sites,
dictionaries, books, bilingual news and
magazines, and product manuals. The size of the
corpus is 96,362 sentence pairs. The corpus is
used in the following three cases:
1) Act as translation memory to support the
Intelligent Recommendation Function;
2) To be used to acquire English-Chinese
translation model to support translation at
word and phrase level;
3) To be used to extract bilingual terms to enrich
the Chinese-English MRD;
To construct a sentence aligned bilingual
corpus, we first use an alignment algorithm doing
the automatic alignment and then the alignment
result are corrected.
There have been quite a number of recent
papers on parallel text alignment. Lexically based
techniques use extensive online bilingual
lexicons to match sentences [Chen 93]. In
contrast, statistical techniques require almost no
prior knowledge and are based solely on the
lengths of sentences, i.e. length-based alignment
method. We use a novel method to incorporate
both approaches [Liu, 95]. First, the rough result
is obtained by using the length-based method.
Then anchors are identified in the text to reduce
the complexity. An anchor is defined as a block
that consists of n successive sentences. Our
experiments show best performance when n=3.
Finally, a small, restricted set of lexical cues is
applied to obtain for further improvement.
1.3 Translation Model Training
Chinese sentences must be segmented
before word translation training, because written
Chinese consists of a character stream without
space between words. Therefore, we use a
wordlist, which consists of 65502 words, in
conjunction with an optimization procedure
described in [Gao, 2000]. The bilingual training
process employs a variant of the model in [Brown,
1993] and as such is based on an iterative EM
(expectation-maximization) procedure for
maximizing the likelihood of generating the
English given the Chinese portion. The output of
the training process is a set of potential English
translations for each Chinese word, together with
the probability estimate for each translation.
1.4 Extraction of Bilingual
Domain-specific Terms
A domain-specific term is defined as a string
that consists of more than one successive word
and has certain occurrences in a text collection
within a specific domain. Such a string has a
complete meaning and lexical boundaries in
semantics; it might be a compound word, phrase
or linguistic template. We use two steps to extract
bilingual terms from sentence aligned corpus.
First we extract Chinese monolingual terms from
Chinese part of the corpus by a similar method
described in [Chien, 1998], then we extract the
English corresponding part by using the word
alignment information. A candidate list of the
Chinese-English bilingual terms can be obtained
as the result. Then we will check the list and add
the terms into the dictionary.
2 Spelling Help
The spelling help works on the word or
phrase level. Given an English word or phrase, it
performs two functions, 1) retrieving
corresponding synonyms, antonyms, and
thesaurus; and 2) automatically giving the
corresponding translation of Chinese words in
the form of Chinese characters or pinyin. We will
focus our discussion on the latter function in the
section.
To use the latter function, the user may input
Chinese characters or just input pinyin. It is not
very convenient for Chinese users to input
Chinese characters by an English keyboard.
Furthermore the user must switch between
English input model and Chinese input model
time and again. These operations will interrupt
his train of thought. To avoid this shortcoming,
our system allows the user to input pinyin instead
of Chinese characters. The pinyin can be
translated into English word directly.
Let us take a user scenario for an example to
show how the spelling help works. Suppose that a
user input a Chinese word ?? in the form of
pinyin, say ?wancheng?, as shown in figure1-1.
PENS is able to detect whether a string is a
pinyin string or an English string automatically.
For a pinyin string, PENS tries to translate it into
the corresponding English word or phrase
directly. The mapping from pinyin to Chinese
word is one-to-many, so does the mapping from
Chinese word to English words. Therefore, for
each pinyin string, there are alternative
translations. PENS employs a statistical approach
to determine the correct translation. PENS also
displays the corresponding Chinese word or
phrase for confirmation, as shown in figure 1-2.
Figure 1-1
Figure 1-2
If the user is not satisfied with the English
word determined by PENS, he can browse other
candidates as well as their bilingual example
sentences, and select a better one, as shown in
figure 1-3.
Figure 1-3
2.1 Word Translation Algorithm
based on Statistical LM and TM
Suppose that a user input two English words,
say EW1 and EW2, and then a pinyin string, say
PY. For PY, all candidate Chinese words are
determined by looking up a Pinyin-Chinese
dictionary. Then, a list of candidate English
translations is obtained according to a MRD.
These English translations are English words of
their original form, while they should be of
different forms in different contexts. We exploit
morphology for this purpose, and expand each
word to all possible forms. For instance,
inflections of ?go? may be ?went?, and ?gone?.
In what follows, we will describe how to
determine the proper translation among the
candidate list.
Figure 2-1: Word-level Pinyin-English
Translation
As shown in Figure 2-1, we assume that the
most proper translation of PY is the English word
with the highest conditional probability among
all leaf nodes, that is
According to Bayes? law, the conditional
probability is estimated by
),|(
),|(),,|(
),,|(
21
2121
21
EWEWPYP
EWEWEWPEWEWEWPYP
EWEWPYEWP
ijij
ij
?
= (2-1)
Since the denominator is independent of EWij, we
rewrite (2-1) as
),|(),,|(
),,|(
2121
21
EWEWEWPEWEWEWPYP
EWEWPYEWP
ijij
ij
?
? (2-2)
Since CWi is a bridge which connect the pinyin
and the English translation, we introduce Chinese
word CWi into
We get
),,,|(
),,,|(),,|(
),,|(
21
2121
21
EWEWEWPYCWP
EWEWEWCWPYPEWEWEWCWP
EWEWEWPYP
iji
ijiiji
ij
?
=
(2-3)
For simplicity, we assume that a Chinese word
doesn?t depends on the translation context, so we
can get the following approximate equation:
)|(),,|( 21 ijiiji EWCWPEWEWEWCWP ?
We can also assume that the pinyin of a Chinese
word is not concerned in the corresponding
English translation, namely:
)|(),,,|( 21 iiji CWPYPEWEWEWCWPYP ?
It is almost impossible that two Chinese words
correspond to the same pinyin and the same
English translation, so we can suppose that:
1),,,|( 21 ?EWEWEWPYCWP iji
Therefore, we get the approximation of (2-3) as
follows:
)|()|(
),,|( 21
iiji
ij
CWPYPEWCWP
EWEWEWPYP
?
= (2-4)
According to formula (2-2) and (2-4), we get:
),|()|()|(
),,|(
21
21
EWEWEWPCWPYPEWCWP
EWEWPYEWP
ijiiji
ij
??
= (2-5)
where P(CWi |EWij) is the translation model, and
can be got from bilingual corpus, and P(PY | CWi)
),,|( 21 EWEWEWPYP ij
is the polyphone model, here we suppose
P(PY|CWi) = 1, and P(EWij | EW1, EW2) is the
English trigram language model.
To sum up, as indicated in (2-6), the spelling help
find the most proper translation of PY by
retrieving the English word with the highest
conditional probability.
),|()|(maxarg
),,|(maxarg
21
21
EWEWEWPEWCWP
EWEWPYEWP
ijiji
EW
EW
ij
ij
?
=
(2-6)
3 Intelligent Recommendation
The intelligent recommendation works on
the sentence level. When a user input a sequence
of Chinese characters, the character string will be
firstly segmented into one or more words. The
segmented word string acts as the user query in
IR. After query expansion, the intelligent
recommendation employs an effective sentence
retrieval algorithm on a large bilingual corpus,
and retrieves a pair (or a set of pairs) of bilingual
sentences related to the query. All the retrieved
sentence pairs are ranked based on a scoring
strategy.
3.1 Query Expansion
Suppose that a user query is of the form CW1,
CW2, ? , CWm. We then list all synonyms for
each word of the queries based on a Chinese
thesaurus, as shown below.
mmnnn
m
m
CWCWCW
CWCWCW
CWCWCW
???
????????????
???
???
21 21
22212
12111
We can obtain an expanded query by
substituting a word in the query with its synonym.
To avoid over-generation, we restrict that only
one word is substituted at each time.
Let us take the query ?? for an example.
The synonyms list is as follows:
 =	??
 =
??.
The query consists of two words. By substituting
the first word, we get expanded queries, such as
??????, etc, and by
substituting the second word, we get other
expanded queries, such as ? 
??
???, etc.
Then we select the expanded query, which is
used for retrieving example sentence pairs, by
estimating the mutual information of words with
the query. It is indicated as follows
?
?
=
m
ik
k
ijk
ji
CWCWMI
1,
),(maxarg
where CWk is a the kth Chinese word in the query,
and CWij is the jth synonym of the i-th Chinese
word. In the above example, ? ? is
selected. The selection well meets the common
sense. Therefore, bilingual example sentences
containing ?? will be retrieved as well.
3.2 Ranking Algorithm
The input of the ranking algorithm is a
query Q, as described above, Q is a Chinese
word string, as shown below
Q= T1,T2,T3,?Tk
The output is a set of relevant bilingual
example sentence pairs in the form of,
S={(Chinsent, Engsent) | Relevance(Q,Chinsent)
>Relevance(Q,Engsent) >
where Chinsent is a Chinese sentence, and
Engsent is an English sentence, and 

For each sentence, the relevance score is
computed in two parts, 1) the bonus which
represents the similarity of input query and the
target sentence, and 2) the penalty, which
represents the dissimilarity of input query and the
target sentence.
The bonus is computed by the following formula:
WhereImproved-Edit-Distance Kernel for Chinese Relation Extraction
Wanxiang Che
School of Computer Sci. and Tech.
Harbin Institute of Technology
Harbin China, 150001
tliu@ir.hit.edu.cn
Jianmin Jiang, Zhong Su, Yue Pan
IBM CRL
Beijing China, 100085
{jiangjm, suzhong,
panyue}@cn.ibm.com
Ting Liu
School of Computer Sci. and Tech.
Harbin Institute of Technology
Harbin China, 150001
tliu@ir.hit.edu.cn
Abstract
In this paper, a novel kernel-based
method is presented for the problem
of relation extraction between named
entities from Chinese texts. The ker-
nel is defined over the original Chi-
nese string representations around par-
ticular entities. As a kernel func-
tion, the Improved-Edit-Distance (IED)
is used to calculate the similarity be-
tween two Chinese strings. By em-
ploying the Voted Perceptron and Sup-
port Vector Machine (SVM) kernel ma-
chines with the IED kernel as the clas-
sifiers, we tested the method by extract-
ing person-affiliation relation from Chi-
nese texts. By comparing with tradi-
tional feature-based learning methods,
we conclude that our method needs less
manual efforts in feature transformation
and achieves a better performance.
1 Introduction
Relation extraction (RE) is a basic and impor-
tant problem in information extraction field. It
extracts the relations among the named enti-
ties. Examples of relations are person-affiliation,
organization-location, and so on. For example, in
the Chinese sentence ?????IBM????
??? (Gerstner is the chairman of IBM Corpora-
tion.), the named entities are??? (people) and
IBM?? (organization). The relation between
them is person-affiliation.
Usually, we can regard RE as a classification
problem. All particular entity pairs are found
from a text and then decided whether they are a
relation which we need or not.
At the beginning, a number of manually en-
gineered systems were developed for RE prob-
lem (Aone and Ramos-Santacruz, 2000). The
automatic learning methods (Miller et al, 1998;
Soderland, 1999) are not necessary to have some-
one on hand with detailed knowledge of how the
RE system works, or how to write rules for it.
Usually, the machine learning method repre-
sents the NLP objects as feature vectors in the
feature extraction step. The methods are named
feature-based learning methods. But in many
cases, data cannot be easily represented explicitly
via feature vectors. For example, in most NLP
problems, the feature-based representations pro-
duce inherently local representations of objects,
for it is computationally infeasible to generate
features involving long-range dependencies. On
the other hand, finding the suitable features of a
particular problem is a heuristic work. Their ac-
quisition may waste a lot of time.
Different from the feature-based learning
methods, the kernel-based methods do not need
to extract the features from the original text, but
retain the original representation of objects and
use the objects in algorithms only via comput-
ing a kernel (similarity) function between a pair
of objects. Then the kernel-based methods use
existing learning algorithms with dual form, e.g.
the Voted Perceptron (Freund and Schapire, 1998)
or SVM (Cristianini and Shawe-Taylor, 2000), as
kernel machine to do the classification task.
132
Haussler (1999) and Watkins (1999) proposed
a new kernel method based on discrete structures
respectively. Lodhi et al (2002) used string ker-
nels to solve the text classification problem. Ze-
lenko et al (2003) used the kernel methods
for extracting relations from text. They defined
the kernel function over shallow parse represen-
tation of text. And the kernel method is used in
conjunction with the SVM and the Voted Percep-
tron learning algorithms for the task of extracting
person-affiliation and organization-location rela-
tions from text.
As mentioned above, the discrete structure ker-
nel methods are more suitable to RE problems
than the feature-based methods. But the string-
based kernel methods only consider the word
forms without their semantics. Shallow parser
based kernel methods need shallow parser sys-
tems. Because the performance of shallow parser
systems is not high enough until now, especially
for Chinese text, we cannot depend on it com-
pletely.
To cope with these problems, we propose the
Improved-Edit-Distance (IED) algorithm to cal-
culate the kernel (similarity) function. We con-
sider the semantic similarity between two words
in two strings and some structure information of
strings.
The rest of the paper is organized as follows. In
Section 2, we introduce the kernel-based machine
learning algorithms and their application in nat-
ural language processing problems. In Section 3,
we formalize the relation extraction problem as
a machine learning problem. In Section 4, we
give a novel kernel method, named the IED kernel
method. Section 5 describes the experiments and
results on a particular relation extraction problem.
In Section 6, we discuss the reason why the IED
based kernel method yields a better result than
other methods. Finally, in Section 7, we give the
conclusions and comments on the future work.
2 Kernel-based Machine Learning
Most machine learning methods represent an ob-
ject as a feature vector. They are well-known
feature-based learning methods.
Kernel methods (Cristianini and Shawe-Taylor,
2000) are an attractive alternative to feature-based
methods. The kernel methods retain the original
representation of objects and use the object only
via computing a kernel function between a pair
of objects. As we know, a kernel function is a
similarity function satisfying certain properties.
There are a number of learning algorithms that
can operate using only the dot product of exam-
ples. We call them kernel machines. For in-
stance, the Perceptron learning algorithm (Cris-
tianini and Shawe-Taylor, 2000), Support Vector
Machine (SVM) (Vapnik, 1998) and so on.
3 Relation Extraction Problem
We regard the RE problem as a classification
learning problem. We only consider the relation
between two entities in a sentence and no rela-
tions across sentences. For example, the sen-
tence ????????IBM??????
??? (President Bush met Gerstner, the chair-
man of IBM Corporation.) contains three enti-
ties,?? (people), ??? (people) and IBM?
? (organization). The three entities form two
candidate person-affiliation relation pairs: ??-
IBM?? and ???-IBM?? . The con-
texts of the entities pairs produce the examples
for the binary classification problem. Then, from
the context examples, a classifier can decide ??
?-IBM?? is a real person-affiliation relation
but ??-IBM?? is not.
3.1 Feature-based Methods
The feature-based methods have to transform the
context into features. Expert knowledge is re-
quired for deciding which elements or their com-
binations thereof are good features. Usually these
features? values are binary (0 or 1).
The feature-based methods will cost lots of la-
bor to find suitable features for a particular appli-
cation field. Another problem is that we can either
select only the local features with a small win-
dow or we will have to spend much more training
and test time. At the same time, the feature-based
methods will not use the combination of these fea-
tures.
3.2 Kernel-based Methods
Different from the feature-based methods, kernel-
based methods do not require much labor on ex-
tracting the suitable features. As explained in the
introduction to Section 2, we retain the original
133
string form of objects and consider the similarity
function between two objects. For the problem of
the person-affiliation relation extraction, the ob-
jects are the context around people and organiza-
tion with a fixed window size w. It means that
we get w words around each entity as the samples
in the classification problem. Again considering
the example ????????IBM?????
????, with w = 2, the object for the pair ?
?? (people) and IBM?? (organization) can
be written as ??? ? ORG ?? PEO ??
Through the objects transformed from the origi-
nal texts, we can calculate the similarity between
any two objects by using the kernel (similarity)
function.
For the Chinese relation extraction problem,
we must consider the semantic similarity between
words and the structure of strings while comput-
ing similarity. Therefore we must consider the
kernel function which has a good similarity mea-
sure. The methods for computing the similarity
between two strings are: the same-word based
method (Nirenburg et al, 1993), the thesaurus
based method (Qin et al, 2003), the Edit-Distance
method (Ristad and Yianilos, 1998) and the statis-
tical method (Chatterjee, 2001). We know that the
same-word based method cannot solve the prob-
lem of synonyms. The thesaurus based method
can overcome this difficulty but does not con-
sider the structure of the text. Although the Edit-
Distance method uses the structure of the text, it
also has the same problem of the replacement of
synonyms. As for the statistical method, it needs
large corpora of similarity text and thus is difficult
to use for realistic applications.
For the reasons described above, we propose a
novel Improved-Edit-Distance (IED) method for
calculating the similarity between two Chinese
strings.
4 IED Kernel Method
Like normal kernel methods, the new IED ker-
nel method includes two components: the ker-
nel function and the kernel machine. We use the
IED method to calculate the semantic similarity
between two Chinese strings as the kernel func-
tion. As for the kernel machine, we tested the
Voted Perceptron with dual form and SVM with a
customized kernel. In the following subsections,
(a) Edit-Distance (b) Improved-Edit-Distance
Figure 1: The comparison between the Edit-
Distance and the Improved-Edit-Distance
we will introduce the kernel function, the IED
method, and kernel machines.
4.1 Improved-Edit-Distance
Before the introduction to IED, we will give
a brief review of the classical Edit-Distance
method (Ristad and Yianilos, 1998).
The edit distance between two strings is de-
fined as: The minimum number of edit operations
necessary to transform one string into another.
There are three edit operations, Insert, Delete, and
Replace. For example, in Figure 1(a), the edit dis-
tance between ?????(like apples)? and ??
????(like bananas)? is 4, as indicated by the
four dotted lines.
As we see, the method of computing the edit
distance between two Chinese strings cannot re-
flect the actual situation. First, the Edit-Distance
method computes the similarity measured in Chi-
nese character. But in Chinese, most of the char-
acters have no concrete meanings, such as ???,
??? and so on. The single character cannot ex-
press the meanings of words. Second, the cost
of the Replace operation is different for different
words. For example, the operation of ??(love)?
being replace by ???(like)? should have a small
cost, because they are synonyms. At last, if there
are a few words being inserted into a string, the
meaning of it should not be changed too much.
Such as ?????(like apples)? and ?????
?(like sweet apples)? are very similar.
Based on the above idea, we provide the IED
method for computing the similarity between two
Chinese strings. It means that we will use Chinese
words as the basis of our measurement (instead of
characters). By using a thesaurus, the similarity
between two Chinese words can be computed. At
the same time, the cost of the Insert operation is
reduced.
Here, we use the CiLin (Mei et al, 1996) as
134
the thesaurus resource to compute the similarity
between two Chinese words. In CiLin, the se-
mantics of words are divided into High, Middle,
and Low classes to describe a semantic system
from general to special semantic. For example:
???(apple)? is Bh07, ???(banana)? is Bh07,
????(tomato)? is Bh06, and so on.
The semantic distance between word A and
word B can be defined as:
Dist(A, B) = min
a?A,b?B
dist(a, b)
where A and B are the semantic sets of word
A and word B respectively. The distance be-
tween semantic a and b is: dist(a, b) = 2 ?
(3 ? d), where d means that the semantic code
is different from the dth class. If the seman-
tic code is same, then the semantic distance is
0. Therefore, Dist(?????) = 0 and
Dist(??????) = 2.
Table 1 defines the variations of the edit dis-
tance on string ?AB? after doing various edit op-
erations. Where, ??? denotes one to four words,
?A? and ?B? are two words which user inputs. X?
denotes the synonyms of X.
Table 1: The Variations of Edit-Distance with AB
Rank Pattern
1 AB
2 A?B
3 AB?; A?B
4 A?B?; A??B
5 A?; B?
According to Table 1, we can define the cost of
various edit operations in IED. See Table 2, where
??? denotes the delete operation.
Table 2: The Cost of Edit Operation in IED
Edit Operation Cost
A?A 0
Insert 0.1
A?A? Dist(A, A?)/10 + 0.5
Others 1
By the redefinition of the cost of edit opera-
tions, the computation of IED between ????
?? and ??????? is as shown Figure 1(b),
where the Replace cost of ???????? is 0.5
and ????????? is 0.7. Thus the cost of IED
is 1.2. Compared with the cost of classical Edit-
Distance, the cost of IED is much more appropri-
ate in the actual situation.
We use dynamic programming to compute the
IED similar with the computing of edit distance.
In order to compute the similarity between two
strings, we should convert the distance value into
a similarity. Empirically, the maximal similarity
is set to be 10. The similarity is 10 minus the
improved edit distance of two Chinese strings.
4.2 Kernel Machines
We use the Voted Perceptron and SVM algorithms
as the kernel machines here.
The Voted Perceptron algorithm was described
in (Freund and Schapire, 1998). We used
SVMlight (Joachims, 1998) with custom kernel as
the implementation of the SVM method. In our
experiments, we just replaced the custom kernel
with the IED kernel function.
5 Experiments and Results
In this section, we show how to extract the
person-affiliation relation from text and give
some experimental results. It is relatively
straightforward to extend the IED kernel method
to other RE problems.
The corpus for our experiments comes from
Bejing Youth Daily1. We annotated about 500
news with named entities of PEO and ORG. We
selected 4,200 sentences (examples) with both
PEO and ORG pairs as described in Section 3.
There are about 1,200 positive examples and
3,000 negative examples. We took about 2,500
random examples as training data and the rest of
about 1,700 examples as test data.
5.1 Infection of Window Size in Kernel
Methods
The change of the performance of the IED kernel
method varying while the window size w is shown
in Table 3. Here the Voted Perceptron is used as
the kernel machine.
Our experimental results show that the IED
kernel method got the best performance with the
highest F -Score when the window size w =
1http://www.bjyouth.com/
135
2. As w grows, the Precision becomes higher.
With smaller w?s, the Recall becomes higher.
5.2 Comparison between Feature and
Kernel Methods
For the feature-based methods implementation,
we use the words which are around the PEO and
the ORG entities and their POS. The window size
is w (See Section 3). All examples can be trans-
formed into feature vectors. We used the regular-
ized winnow learning algorithm (Zhang, 2001) to
train on the training data and predict the test data.
From the experimental results, we find that when
w = 2, the performance of feature-based method
is highest.
The comparison of the performance between
the feature-based and the kernel-base methods is
shown in Table 4.
Figure 2 displays the change of F -Score for
different methods varying with the training data
size.
Figure 2: The learning curve (of F -Score) for the
person-affiliation relation, comparing IED kernel
with feature-based algorithms
Table 3: The Performance Effected by w
w Precision Recall F -Score
1 66.67% 92.68% 77.55%
2 93.55% 87.80% 90.85%
3 94.23% 74.36% 83.12%
Table 4: The Performance Comparison
Precision Recall F -Score
Regularized Winnow 75.90% 96.92% 85.14%
Voted Perceptron 93.55% 87.80% 90.85%
SVM 94.15% 88.38% 91.17%
From Table 4 and Figure 2, we can see
that the IED kernel methods perform better for
the person-affiliation relation extraction problem
than for the feature-based methods.
Figure 2 shows that the Voted Perceptron
method gets close to, but not as good as, the per-
formance to the SVM method on the RE problem.
But when using the method, we can save signifi-
cantly on computation time and programming ef-
fort.
6 Discussion
Our experimental results show that the kernel-
based and the feature-based methods can get the
best performance with the highest F -Score when
the window size w = 2. This shows that for re-
lation extraction problem, the two words around
entities are the most significant ones. On the other
hand, with w becoming bigger, the Precision be-
comes higher. And with w becoming smaller, the
Recall becomes higher.
From Table 4 and Figure 2, we can see that
the IED kernel methods perform very well for
the person-affiliation relation extraction. Further-
more, it does not need an expensive feature selec-
tion stage like feature-based methods. Because
the IED kernel method uses the semantic similar-
ity between words, it can get a better extension.
We can conclude that the IED kernel method re-
quires much fewer examples than feature-based
methods for achieving the same performance.
For example, there is a test sentence ???
? ?? ?? ? IBM?? ?? ? (Chairman
Hu Jintao met the CEO of IBM Corporation). The
feature-based method judges the ???-IBM?
? as a person-affiliation relation, because the
context around??? and IBM?? is similar
with the context of the person-affiliation relation.
However, the IED kernel method does the correct
judgment based on the structure information. For
this case the IED kernel method gets a higher pre-
cision. At the same time, because the IED kernel
method considers the extension of synonyms, its
recall does not decrease very much.
The speed is a practical problem in apply-
ing kernel-based methods. Kernel-based clas-
sifiers are relatively slow compared to feature-
based classifiers. The main reason is that the com-
puting of kernel (similarity) function takes much
136
time. Therefore, it becomes a key problem to im-
prove the efficiency of the computing of the ker-
nel function.
7 Conclusions
We presented a new approach for using kernel-
based machine learning methods for extracting re-
lations between named entities from Chinese text
sources. We define kernels over the original rep-
resentations of Chinese strings around the partic-
ular entities and use the IED method for comput-
ing the kernel function. The kernel-based meth-
ods need not transform the original expression of
objects into feature vectors, so the methods need
less manual efforts than the feature-based meth-
ods. We applied the Voted Perceptron and the
SVM learning method with custom kernels to ex-
tract the person-affiliation relations. The method
can be extended to extract other relations between
entities, such as organization-location, etc. We
also compared the performance of kernel-based
methods with that of feature-based methods, and
the experimental results show that kernel-based
methods are better than feature-based methods.
Acknowledgements
This research has been supported by National
Natural Science Foundation of China via grant
60435020 and IBM-HIT 2005 joint project.
References
Chinatsu Aone and Mila Ramos-Santacruz. 2000.
Rees: A large-scale relation and event extraction
system. In Proceedings of the 6th Applied Natural
Language Processing Conference, pages 76?83.
Niladri Chatterjee. 2001. A statistical approach
for similarity measurement between sentences for
EBMT. In Proceedings of Symposium on Transla-
tion Support Systems STRANS-2001, Indian Insti-
tute of Technology, Kanpur.
N. Cristianini and J. Shawe-Taylor. 2000. An Intro-
duction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algo-
rithm. In Computational Learning Theory, pages
209?217.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, 7,.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In Proceedings of ECML-98, number
1398, pages 137?142, Chemnitz, DE.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. J. Mach. Learn.
Res., 2:419?444.
Jiaju Mei, Yiming Lan, Yunqi Gao, and Hongxiang
Yin. 1996. Chinese Thesaurus Tongyici Cilin (2nd
Edtion). Shanghai Thesaurus Press, Shanghai.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract information?
BBN: Description of the SIFT system as used for
MUC. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7).
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993.
Two approaches to matching in example-based ma-
chine translation. In Proceedings of the Fifth In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 47?
57, Kyoto, Japan.
Bing Qin, Ting Liu, Yang Wang, Shifu Zheng, and
Sheng Li. 2003. Chinese question answering sys-
tem based on frequently asked questions. Jour-
nal of Harbin Institute of Technology, 10(35):1179?
1182.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, 1,.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Tong Zhang. 2001. Regularized winnow methods.
In Advances in Neural Information Processing Sys-
tems 13, pages 703?709.
137
Chinese Word Segmentation with Multiple Postprocessors
in HIT-IRLab
Huipeng Zhang     Ting Liu    Jinshan Ma    Xiantao Liao 
Information Retrieval Lab, Harbin Institute of Technology, Harbin, 150001 CHINA 
{zhp,tliu,mjs,taozi}@ir.hit.edu.cn
Abstract
This paper presents the results of the
system IRLAS1 from HIT-IRLab in the
Second International Chinese Word
Segmentation Bakeoff. IRLAS consists
of several basic components and multi-
ple postprocessors. The basic compo-
nents include basic segmentation,
factoid recognition, and named entity
recognition. These components main-
tain a segment graph together. The
postprocessors include merging of ad-
joining words, morphologically derived
word recognition, and new word identi-
fication. These postprocessors do some
modifications on the best word se-
quence which is selected from the seg-
ment graph. Our system participated in
the open and closed tracks of PK cor-
pus and ranked #4 and #3 respectively.
Our scores were very close to the high-
est level. It proves that our system has
reached the current state of the art.
1 Introduction
IRLAS participated in both the open and closed
tracks of PK corpus. The sections below descript
in detail the components of our system and the
tracks we participated in.
The structure of this paper is as follows. Sec-
tion 2 presents the system description. Section 3
describes in detail the tracks we participated in.
Section 4 gives some experiments and discus-
sions. Section 5 enumerates some external fac-
1 IRLAS is the abbreviation for ?Information Retrieval
Lab Lexical Analysis System?.
tors that affect our performance. Section 6 gives
our conclusion.
2 System Description 
2.1 Basic Segmentation
When a line is input into the system, it is first
split into sentences separated by period. The
reason to split a line into sentences is that in
named entity recognition, the processing of sev-
eral shorter sentences can reach a higher named
entity recall rate than that of a long sentence.
The reason to split the line only by period is for
the simplicity for programming, and the sen-
tences separated by period are short enough to
process.
Then every sentence is segmented into single
atoms. For example, a sentence like ?HIT-IRLab
?????? SIGHAN ?????? will be
segmented as ?HIT-IRLab/?/?/?/?/?/?
/SIGHAN/?/?/?/?/??.
After atom segmentation, a segment graph is
created. The number of nodes in the graph is the
number of atoms plus 1, and every atom corre-
sponds to an arc in the graph.
Then all the words in the dictionary2 that ap-
pear in the sentence will be added to the seg-
ment graph. The graph contains various
information such as the bigram possibility of
every word. Figure 1 shows the segment graph
of the above sentence after basic segmentation.
2.2 Factoid Recognition 
After basic segmentation, a graph with all the
atoms and all the words in the dictionary is set
up. On this basis, we find out all the factoids
2 The dictionary is trained with training corpus.
172
Figure 1: The segment graph 
Note: the probability of each word is not shown in the graph.
such as numbers, times and e-mails with a set of
rules. Then, we also add all these factoids to the
segment graph.
2.3 Named Entity Recognition 
Then we will recognize the named entities such
as persons and locations. First, we select N3 best
paths from the segment graph with Dijkstra al-
gorithm. Then for every path of the N+1 paths4
(N best paths and the atom path), we perform a
process of Roles Tagging with HMM model
(Zhang et al 2003). The process of it is much
like that of Part of Speech Tagging. Then with
the best role sequence of every path, we can find
out all the named entities and add them to the
segment graph as usual. Take the sentence ??
????????? for example. After basic
segmentation and factoid recognition, the N+1
paths are as follows:
?/?/?/?/?/?/??/?
?/?/?/?/?/?/?/?/?
Then for each path, the process of Roles
Tagging is performed and the following role
sequences are generated:
X/S/W/N/O/O/O/O5
X/S/W/N/O/O/O/O/O
From these role sequences, we can find out
that ?XSW? is a 3-character Chinese name. So
the word ????? is recognized as a person
name and be added to the segment graph. 
3 N is a constant which is 8 in our system.
4 It may be smaller than N+1 if the sentence is short
enough; exactly, N+1 is the upper bound of the path num-
ber.
5 X, S, W, N and O are all roles for person name recogni-
tion, X is surname, S is the first character of given name,
W is the second character of given name, N is the word
following a person name, and O is other remote context.
We defined 17 roles for person name recognition and 10
roles for location name recognition.
2.4 Merging of Adjoining Words
After the steps above, the segment graph is
completed and a best word sequence is gener-
ated with Dijkstra algorithm. This merging op-
eration and all the following operations are done
to the best word sequence.
There are many inconsistencies in the PK
corpus. For example, in PK training corpus, the
word ???? sometimes is considered as one
word, but sometimes is considered as two sepa-
rate words as ?? ??. The inconsistencies
lower the system?s performance to some extent.
To solve this problem, we first train from the
training corpus the probability of a word to be
one word and the probability to be two separate
words. Then we perform a process of merging:
if two adjoining words in the best word se-
quence are more likely to be one word, then we
just merge them together. 
2.5 Morphologically Derived Word Recog-
nition
To deal with the words with the postfix like
???, ???, ??? and so on, we perform the
process to merge the preceding word and the
postfix into one word. We train a list of post-
fixes from the training corpus. Then we scan the
best word sequence, if there is a single character
word that appears in the postfix list, we merge
the preceding word and this postfix into one
word. For example, a best word sequence like
??? ? ? ? ??? will be converted to
???? ? ? ??? after this operation.
2.6 New Word Identification
As for the words that are not in the dictionary
and cannot be identified with the steps above,
we perform a process of New Word Identifica-
tion (NWI). We train from the training corpus
the probability of a word to be independent and
the probability to be a special part of another
word. In our system, we only consider the words
that have one or two characters. Then we scan
173
the best word sequence, if the product of the
probabilities of two adjoining words exceed a
threshold, then we merge the two words into one
word.
Take the word ???? for example. It is
segmented as ?? ?? after all the above steps
since this word is not in the dictionary. We find
that the word ??? has a probability of 0.83 to
be the first character of a two character word,
and the word ??? has a probability of 0.94 to be
the last character of a two character word. The
product of them is 0.78 which is larger than 0.65,
which is the threshold in our system. So the
word ???? is recognized as a single word.
3 Tracks
3.1 Closed Track
As for the PK closed track, we first extract all
the common words and tokens from the training
corpus and set up a dictionary of 55,335 entries.
Then we extract every kind of named entity re-
spectively. With these named entities, we train
parameters for Roles Tagging. We also train all
the other parameters mentioned in Section 2
with the training corpus. 
3.2 Open Track
The PK open track is similar to closed one. In
open track, we use all the 6 months corpus of
People?s Daily and set up a dictionary of
107,749 entries. Additionally, we find 101 new
words from the Web and add them to the dic-
tionary. We train the parameters of named entity
recognition with a person list and a location list
in our laboratory. The training of other parame-
ters is the same with closed track.
4 Experiments and Discussions 
We do several experiments on PK test corpus to
see the contribution of each postprocessor. We
cut off one postprocessor at a time from the
complete system and record its F-score. The
evaluation results are shown in Table 1. In the
table, MDW represents Morphologically De-
rived Word Recognition, and NWI represents
New Word Identification.
PK open PK closed 
Complete
System 96.5% 94.9%
Without
Merging 96.3% 94.7%
Without
MDW 96.6% 94.4%
Without
NWI 96.5% 94.9%
Table 1: Evaluation results of IRLAS with each
postprocessor cut off at a time
From Table 1, we can come to some interest-
ing facts:
! The Merging of Adjoining Words has good
effect on both open and closed tracks. So
we can conclude that this module can solve
the problem of inconsistent training corpus
to some extent.
! Morphologically Derived Word Recogni-
tion does some harm in open track, but it
has a very good effect in closed track.
Maybe it is because that in open track, we
can make a comparatively larger dictionary
since we can use any resource we have. So
most MDWs6 are in the dictionary and the
MDWs that are not in the dictionary are
mostly difficult to recognize. So it does
more harm than good in many cases. But in
closed track, we have a small dictionary
and many common MDWs are not in the
dictionary. So it does much more good in
closed track.
! New Word Identification is minimal in both
open and closed tracks. Maybe it is because
that the above steps have recognized the
most OOV words and it is hard to recognize 
any more new words.
5 External Factors That Affect Our
Performance
The difference on the definition of words is the
main factor that affects our performance. In
many cases such as ??????, ????, ???
?? are all considered as one word in our system
but not so in the PK gold standard corpus. An-
other factor is the inconsistencies in training
corpus, although this problem has been solved to
some extent with the module of merging. But
6 It refers to Morphologically Derived Words.
174
because the inconsistencies also exist in test cor-
pus and there are some instances that a word is
more likely to be a single word in training cor-
pus but more likely to be separated into two
words in test corpus. For example, the word ??
?? is more likely to be a single word in training
corpus but is more likely to be separated into
two words in test corpus. There is another factor
that affects MDW, many postfixes in our system
are not considered as postfixes in PK gold stan-
dard corpus. For example, the word ????? is
recognized as a MDW in our system since ???
is a postfix, however, it is segmented into two
separate words as ??? ?? in PK gold stan-
dard corpus. 
6 Conclusion
Through the second SIGHAN bakeoff, we find
the segmentation model and the algorithm in our
system is effective and the multiple postproces-
sors we use can also enhance the performance of
our system. At the same time, we also find some
problems of us. It also has potential for us to
improve our system. Take MDW for example,
we can make use of more features such as the
POS and the length of the preceding word to
enhance the recall and precision rate. The bake-
off points out the direction for us to improve our
system.
References
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,
Shuo Bai, Chinese Named Entity Recognition Us-
ing Role Model. International Journal of Computa-
tional Linguistics and Chinese Language
Processing, 2003, Vol.8(2)
Andi Wu, Zixin Jiang, 2000. Statistically-Enhanced
New Word Identification in a Rule-Based Chinese
System. In Proceedings of the Second Chinese
Language Processing Workshop, pp. 46-51,
HKUST, Hong Kong.
Huaping Zhang, Hongkui Yu, Deyi Xiong, Qun Liu,
HHMM-based Chinese Lexical Analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, July 11-12,
2003, Sapporo, Japan.
Aitao Chen, Chinese Word Segmentation Using
Minimal Linguistics Knowledge. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing, July 11-12, 2003, Sapporo, Ja-
pan.
Andi Wu, Chinese Word Segmentation in MSR-NLP.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, July 11-12,
2003, Sapporo, Japan.
175
Automated Generalization of Phrasal Paraphrases from the Web*
Weigang Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lee@ir.hit.edu
.cn
Ting Liu 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
tliu@ir.hit.ed
u.cn
Yu Zhang
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
zhangyu@ir.hit
.edu.cn
Sheng Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lis@ir.hit.edu
.cn
Wei He 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
truman@ir.hit.
edu.cn
Abstract
Rather than creating and storing thou-
sands of paraphrase examples, para-
phrase templates have strong 
representation capacity and can be used
to generate many paraphrase examples.
This paper describes a new template
representation and generalization
method. Combing a semantic diction-
ary, it uses multiple semantic codes to
represent a paraphrase template. Using
an existing search engine to extend the
word clusters and generalize the exam-
ples.  We also design three metrics to
measure our generalized templates. The 
experimental results show that the rep-
resentation method is reasonable and 
the generalized templates have a higher 
precision and coverage.
1 Introduction
Paraphrases are alternative ways to convey the 
same information (Barzilay and McKeown,
2001) and they have been applied in many fields
of natural language processing. There are many
previous work on paraphrase examples extrac-
tion or combining them with some applications
such as information retrieval and question an-
swering (Agichtein et al, 2001; Florence et al, 
2003; Rinaldi et al, 2003; Tomuro, 2003; Lin
and Pantel, 2001;), information extraction 
(Shinyama et al, 2002; Shinyama and Sekine, 
2003), machine translation (Hiroshi et al, 2003;
Zhang and Yamamoto, 2003), multi-document
(Barzilay et al, 2003).
There is also some other research about 
paraphrase. (Wu and Zhou, 2003) just extract 
the synonymy collocation, such as <turn on, 
OBJ, light> and <switch on, OBJ, light> using
both monolingual corpora and bilingual corpora 
to get an optimal result, but do not generalize
them. (Glickman and Dagan, 2003) detects verb
paraphrases instances within a single corpus
without relying on any priori structure and in-
formation. Generation of paraphrase examples
was also investigated (Barzilay and Lee, 2003;
Quirk et al, 2004).
Rather than creating and storing thousands of 
paraphrases, paraphrase templates have strong 
representation capacity and can be used to gen-
erate many paraphrase examples. As (Hirst, 
2003) said, for each aspect of paraphrase there 
are two main challenges: representation of 
knowledge and acquisition of knowledge. Cor-
responding to the problem of generalization of 
paraphrase templates, there are also two prob-
lems: the first is the representation of paraphrase
templates and the second is acquisition of para-
phrase templates.
There are several methods about paraphrase
templates representation. The first method is 
using the Part-of-Speech (Barzilay and McKe-
own, 2001; Daum? and Marcu, 2003; Zhang and 
Yamamoto, 2003), the second uses name entity 
as the variable (Shinyama et al, 2002; Shinyama
and Sekine, 2003), the third method is similar to 
the second method which is called the inference 
rules extraction (Lin and Pantel, 2001).
A paraphrases template is a pair of natural
language phrases with variables standing in for
certain grammatical constructs in (Daum? and 
*: Supported by the Key Project of National Natural Sci-
ence Foundation of China under Grant No. 60435020
49
Marcu, 2003). He used Part-of-Speech to repre-
sent templates. But for some cases, the POS will 
be very limited and for some other cases will be 
over generalized. For example:
?????????
(In my view/mind ----I feel)
The above pair of phrases is a paraphrase, it 
can be generalized using POS information: 
? [pronoun]??
(In [pronoun] view/mind)
[pronoun]??
( [pronoun] feel)
But for this template many noun words will
be excluded. From this point of view, the tem-
plate representation capacity is limited. But for 
other examples, the POS information will be 
over generally. For example:
?????????
(What's the price for the apples?)
????????
(How much is the apples per Jin?)
Here, we just generalize one variable ????.
Then, the template becomes:
[noun]???????
(What's the price for the [noun]?)
[noun]??????
(How much is the [noun] per Jin?)
If there is a sentence ??????????
(What's the price for the notebook?)?, its? para-
phrase will be ?????????(How much 
is the notebook per Jin?)? according to this tem-
plate. Obviously, the result is unreasonable.
(Shinyama et al, 2002) tried to find para-
phrases assuming that two sentences sharing
many Named Entities and a similar structure are 
likely to be paraphrases of each other. But just 
name entities are limited, too. And (Lin and 
Pantel, 2001) present an unsupervised algorithm 
for discovering inference rules from text such as 
?X writes Y? and ?X is the author of Y?. This 
generalized method has good ability. But it also
has some limited aspect. For example:
[Jack] writes [his homework].
According to the paraphrase template, the
target sentence will be transformed into ?[Jack]
is the author of [his homework]?. It?s obviously
that the generated sentence is not standard.
So how to represent paraphrase templates
and generalize the paraphrase examples is a very 
interesting task. In this paper, we present a novel
approach to represent paraphrase template with 
semantic code of words and using an existing
search engine to get the paraphrase template.
The remainder of this paper is organized as 
follows. In the next section, we give the over-
view of our method. In section 3, we define the 
representation method in details. Section 4 pre-
sents the generalization method. Some experi-
ments and discussions are shown in Section 5. 
Finally, we draw a conclusion of this method
and give some suggestions about future work. 
2 Overview of Generalization Method
The origin input of our system is a seed phrasal
paraphrase example. And the output is the gen-
eralized paraphrase templates from the given 
examples. The overall architecture of our para-
phrase generalization is represented on figure 1. 
A seed phrasal
paraphrase examples
Getting the slot word
Extend the slot word
using Search Engine
on every example
Mapping two word
sets to their semantic
code sets
Intersection operation
on the two semantic
code sets
Generalizing a
template
Figure 1: Sketch Map of Paraphrase example
Generalization
We also use the example (1) to illustrate the 
representation. Here a semantic dictionary called 
?TongYiCiCiLin? (Extension Version)1 is used. 
The pair of phrases is a phrasal paraphrase. At
first, after preprocessing which includes word
segment, POS tagging and word sense disam-
biguation, we get the slot word in the paraphrase.
In this example, the slot word is ??(I)?. Then
we search the web using the context of the slot 
word. Every phrase in the phrasal pair derives a
set of sentences which include the original 
phrase context. A dependency parser on these 
sentences is used to extract the corresponding
word with the slot word. Two word sets can be 
obtained through the two sentence sets. Then,
we map word sets to their semantic code sets
1 TongYiCiCiLin (Extended Version) can be downloaded
from the website of HIT-IRLab (Http://ir.hit.edu.cn). In the 
past section, we abbreviate the TongYiCiCiLin (Extended
Version) to Cilin (EV) 
50
according to Cilin(EV). Then an intersection 
operation is conducted on the two sets. We use 
the intersection set to replace the slot word and 
generate the final paraphrase template. 
In order to verify the validation of the gener-
alized paraphrase template, we also design an 
automatic algorithm to confirm whether the 
template is reasonable using the existing search 
engine.
3 Representation of Template 
In the section of introduction, some representa-
tion methods of paraphrase template have been 
introduced. And we proposed a new method us-
ing word semantic codes to represent the vari-
able in a template. Before we introduce the 
representation method, Firstly, we give some 
general introduction about the semantic diction-
ary of Cilin(EV). 
3.1 TongYiCiCiLin (Extended Version) 
Cilin (EV) is derived from original TongY-
iCiCilin in which word senses are decomposed 
to 12 large categories, 94 middle categories, 
1,428 small categories. Cilin (EV) removes 
some outdated words and updates many new 
words. More fine-grained categories are added 
on the base of original classification system to 
satisfy the more complex natural language ap-
plications. The encoding criterion is shown in 
the table 1:
Table 1 Encoding table of dictionary
Encoding
bit 1 2 3 4 5 6 7 8
Example D a 1 5 B 0 2 =
Attribute Big Middle Small groups Atom groups
Layer 1 2 3 4 5
The encoding bits are arranged from left to 
right. The first three layers are same with Cilin. 
The fourth layer is represented by capital letters 
and the fifth layer is two-bit decimal digit. The 
last bit is some more detailed information about 
the atom groups. 
3.2 An Example of a Paraphrase Template 
For simplicity, we just select one slot word in 
every paraphrase. And we stipulated that only 
content word can be slot word. We also use the 
above paraphrase example (1). 
?????????
(In my view/mind ----I feel)
Here, we get the slot word ??(I)?. Through 
the Word Sense Disambiguation processing, we 
get its semantic code ?Aa02A01=? according to 
the fifth layer in Cilin(EV). If we just use the 
semantic code of the slot word, we can get a 
simple paraphrase template as follows:  
? [Aa02A01=] ??
(In [Aa02A01=]  view/mind)
[Aa02A01=] ??
([Aa02A01=]  feel)
But it is obviously that the template is very 
limited. Its? representation ability is also limited. 
So how to extend the ability of a paraphrase 
template is a challenging work.  
3.3 Extending the Template Abstract Ability 
According to the feature of Cilin(EV) architec-
ture, we can use the higher layer?s semantic 
code instead of the slot word to generalize the 
paraphrase template naturally. Of course it?s a 
very simple method to extend the template abil-
ity, but it also brings more redundancy of a 
paraphrase template and it will be proven in the 
later section. 
So we use multiple semantic codes of the dif-
ferent layer instead of only one semantic code of 
slot word in Cilin (EV). The later experimental 
results prove this representation has a good per-
formance with a good precision and coverage. 
4 Generalizing to Templates 
As mentioned above, we can use multiple se-
mantic codes to generalize paraphrase examples. 
So the problem of how to generalize paraphrase 
examples is transformed into the problem of 
how to get the multiple semantic codes set. We 
proposed a new method which uses the existing 
search engine to reach the target.  
4.1 Getting the Candidate Sentences 
After we removed the slot word in the para-
phrase examples, two phrasal contexts of the 
original paraphrase phrases were obtained. Each 
phrase without slot word is used as a search 
query for an existing search engine and achiev-
ing many sentences which include the query 
word. For this example, the two queries are ??
??(in?view)? and ???(feel)?. Each query 
gets one sentence set respectively. Part of the 
two result sentence sets are shown in figure 2 
and figure 3: 
51
Figure 2. Sentence Set 1 
Figure 3. Sentence Set 2 
From the above two sentence sets, we can
find that there is some noisy information in the 
sentences. In order to extend the correspondent
words of the slot word, it is not enough that we 
just use the position information or POS tagging
information of the slot word. Even if we extract
these words, many of them can?t be found in the
dictionary because they are not simple words.
Benefiting from the idea of (Lin and Pantel, 
2001), we use a dependency parser to determine
the correspondent extended words. 
4.2 Dependency Parser 
In this paper, we use a dependency parser (Ma et 
al., 2004) to extract the candidate slot word. For 
example, the dependency parsing result of the 
phrase of ?????? is shown in figure 4. 
Figure 4. Dependency parsing result 
The arcs in the figure represent dependency
relationships. The direction of an arc is from the 
head to the modifier in the relationship. Labels 
associated with the arcs represent types of de-
pendency relations. Table 2 lists a subset of the
dependency relations in the HIT-IRLab depend-
ency parser2.
Table 2. A subset of the dependency relations 
Relation Description
ATT ????(attribute)
HED ??(head)
SBJ ??(subject)
ADV ????(adverbial)
VOB ????(verb-object)
???????????????????
??????????????????
??????????????
,7 ????????????"
??????????????????
2 More information about the dependency parser can be got
from http://ir.hit.edu.cn/cuphelp.htm
4.3 Extracting the extended words 
We just use a very simple method to get the ex-
tended words from the parsed sentences. At first, 
we record the relations of the original parsed 
phrasal examples. And then we use these rela-
tions to matched similar part in the candidate 
parsed sentence except slot word. And we omit
these unseen relations and content words which
don?t appear in the original parsed phrasal ex-
amples. Then we can get the extended words. 
????????????
?????????
???????????????
??????????
???????????B720 ????
Figure 5. Dependency parsing result 
Figure 5 shows the dependency parsing result 
of the phrase of ???????????(In for-
eign capital fund manager view). We can easily
find that the extended word of the slot word
?? ?(I) is ??? ?(manager). Two extended
word sets can be extracted from two sentence
sets. Then we map each word to their semantic
code to get two semantic code sets. Intersection
operation is conducted on these two semantic
code sets to obtain their intersection set. Finally, 
we use the semantic code set instead of the slot 
word to generate the paraphrase template.
4.4 Some tricks 
Because the precision of the current dependency
parser on Chinese is not very high, we just ex-
tract a part of the candidate sentences to parse. 
There are three patterns to segment the long 
candidate sentences according to position of slot 
word in paraphrase examples. They are called
FRONT, MIDDLE and BACK. Here we use an
example to illustrate it as shown in table 3: 
Table 3 Examples of sentence segmentation
Pattern Origin Phrase Segment examples
FRONT (SW)?? ????????
?????
MIDDLE ?(SW)?? ????????
????????
????
The bold section in the sentence will be ex-
tracted to parse. Pattern type can be decided by 
52
the position relation between slot word and con-
text words. And these patterns can reduce the 
relative error rate of the dependency parser. That 
is to say, if the original phrase is parsed wrongly, 
the extracted segments may be parsed wrongly 
with the similar error. But according to our 
method, this kind of parser error has little influ-
ence on the final extracting result. 
5 Experiments and Discussions 
5.1 Setting 
We extract about 510 valid paraphrase examples 
from a Chinese paraphrase corpus (Li et al, 
2004). For simplicity, we just select those 
phrasal paraphrase examples which own same 
word. And we stipulate only content word can 
be as slot word. We just use four seed phrasal 
paraphrases as the original paraphrases in this 
paper. And the generalized paraphrase templates 
represented by semantic codes of the fifth layer 
in Cinlin (EV) are also shown in the Table 4: 
Table 4: Examples of the generalized template 
Origin 
Phrases 
Generalized Paraphrase  
templates 
??? [Aa01A01=,Aa01A05=,   
Aa01C03=,Aa02A01=,  ?]??
1 ? ? ?
?
?[Aa01A01=,Aa01A05=, 
  Aa01C03=,Aa02A01=,...  ]??
??? ? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]?2 ??? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]??
? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,? ]??
3 ? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,?  ]??
? ? ?
? ? ?
??
[Aa03A01=,Ac03A01=, 
Ba05A10#,Bb02A01=,?]???
???4
? ? ?
???
[Aa03A01=,Ac03A01=,Ba05A10#,
Bb02A01=,?]?????
5.2 Evaluation on Templates 
The goal of the evaluations is to confirm how 
reasonable this kind of representation method of 
paraphrase templates is and how well the tem-
plate is. We evaluated the generalized para-
phrase template in three ways. They are listed in 
the following three categories: 1) Reasonability; 
2) Precision; 3) Coverage. 
1) Reasonability 
The reasonability of a paraphrase template aims 
to measure the reasonable extent of the presenta-
tion method with multiple semantic codes. For 
example, if we use POS to generalize a para-
phrase template, its reasonability is very lower; 
that is to say, POS is not suitable to represent 
paraphrase template in some extent.  
We use an existing search engine to calcu-
late the reasonability of every paraphrase tem-
plate. Firstly, we instantiate all paraphrase 
examples from a template. Then all these exam-
ples are as the queries of the search engine. If 
two phrases in one paraphrase can be matched 
completely from the search engine, it also means 
that one or more examples are found on the Web 
via search engine, we then consider this para-
phrase is reasonable. Using this method we can 
get the approximate evaluation of all the exam-
ples. We define two metrics: 
Strict_Reasonability = S / N 
Loose_Reasonability = (L + S) / N 
Where N is the total number of the instanti-
ated examples; S is the number of the para-
phrase examples which two phrases in it can be 
matched all; L is the number of paraphrase ex-
amples only one phrase in a paraphrase can be 
matched.
2) Precision 
Every template is correspondent to the examples 
number with the semantic code of different layer 
in Cilin (EV) as shown in table 5.  
Table 5 Templates and their correspond exam-
ples number 
Instantiated examples 
number
Number of 
Paraphrase
templates Cilin3 Cilin4 Cilin5
1 2696 1815 478
2 13032 6354 3011
3 1057 587 177
4 3004 2229 429
From the above table, we can find that every 
template can instantiate many examples. If 
manually judging all of these examples will 
spend plenty of time. So we just sample part of 
all instantiate examples, 200 paraphrase exam-
ples for each template in this paper. For each 
53
phrase in a sample paraphrase example, it is as 
search query to get the first two matched sen-
tences. Evaluators would be asked whether it is
semantically okay to replace the query in the
sentence by the correspondent phrase in a para-
phrase. They were given only two options: Yes
or No. If search query have no matched results, 
we consider that this phrase cannot be replace 
with its correspondent paraphrase. According to 
the above regulations, we know that every para-
phrase examples correspondent to 4 sentences. If 
we sample n examples from a template, the pre-
cision of a paraphrase template can be calculated
by:
Precision = R / (4 * n) 
Where, R is the number of sentences which
is considered to be correct by the evaluator.
3) Coverage 
Evaluating directly the coverage of a paraphrase 
template is difficult because humans can?t enu-
merate all the words to be suitable to the tem-
plate. We use an approximate method to get the
coverage of a template. At first we use another 
search engine to get candidate sentences with 
similar method for generalization of a para-
phrase template. From these retrieved sentences
we can get many different words with the 
known generalized words because more than
85% of search results from different search en-
gine are different. Evaluators extract every sen-
tence which can be replaced with the 
correspondent phrase in a paraphrase and the
new sentences retain the origin meaning. We 
know each sentence is correspondent to a word. 
Then we define two metrics: 
Surface_Coverage = M / NS
Semantic_Coverage =
Map(K) / (Map(NS-M) + Map(K)) 
Where, NS is the number of all manually
tagged right words, M is the number of words 
which can be instantiated from a paraphrase
template, K is the number of all the words that 
generalized the template at the front. Map(X) is 
the total word number of the word clusters 
which derived from X word in the semantic dic-
tionary of Cilin(EV).
5.3 Result 
In order to exhibit the merit of our method, we 
conduct four groups of experiment. They are
POS-Tag, Cilin3, Cilin4 and Ciln5, respectively.
Especially, we just randomly select 400 words 
to satisfy the POS information.
Table 6: Experiment Results 
Reasonability
(%)
Coverage
(%)
St_R Lo_R Su_C Se_C
Preci-
sion
(%)
POS 10.50 17.00 90.00 ---- 11.75
Cilin3 45.57 84.50 27.55 38.71 45.75
Cilin4 46.89 84.54 23.87 44.48 64.13
Cilin5 46.24 83.12 20.39 39.47 69.88
Every value in table 6 is a average value of 
four values correspondent to four templates.
From the table we can find that the reasonability
of the Cilin-based representation template
changes little, and that of POS-based representa-
tion is very lower. We find that the longer origi-
nal phrases are, the lower the coverage of the
generalized template is. Although the average 
coverage of generalized template is relatively
low, we can draw a conclusion that using multi-
ple semantic codes to generalize phrasal para-
phrase examples is reasonable.
The column of the coverage shows that the 
coverage rates of Cilin-based templates are all
not more than 50%. And the POS-based tem-
plate has a very high coverage rate. And we 
know that the extended information is not
enough only depending on one search engine. 
We will combine several different search en-
gines with together to solve this problem in the 
future work. 
1.0 1.5 2.0 2.5 3.0 3.5 4.0
0
10
20
30
40
50
60
70
80
90
100
 strict_Reasonability  loose_Reasonability
 surface_Coverage  semantic_Coverage
 Precision
Va
lu
es
 o
f P
er
ce
nt
Different Template Representation Method
Figure 6. Experimental Results
The numbers from one to four on the X-axis
are correspondent to POS, Cilin3, Cilin4 and 
Cilin5 in figure 6. We can see the features
clearly of different representation methods of 
template from the figure 6. We can find that
54
Cilin5-based template has the highest precision, 
but its coverage is lower. And Cilin3-based 
template has opposite feature. This is because 
that one semantic code of Cilin3 includes more 
words than that of Cilin5. At the same time, 
more words bring more redundant information. 
And Cilin4-based template has a good tradeoff 
between coverage and precision. So we con-
clude that the semantic code of fourth layer in 
Cilin (EV) is more suitable to represent para-
phrase template.  
Some additional information can be extracted 
from the generalized template. Such as, the col-
location information between the slot word and 
the context words can be extract. For example, 
in the fourth template, we can get the informa-
tion about which words can be collocated with 
??(Jin)?.
Although this kind of representation of para-
phrase template has a good performance, it is 
weak for those words or structures that don?t 
exist in dictionary. Also, this method is not suit-
able to the named entities representation. 
6 Conclusion
In this paper, a novel method for automated 
generalization of paraphrase examples is pro-
posed. This method is not dependent on the tra-
ditional limited texts instead it is based on the 
richness of the Web. It uses the multiple seman-
tic codes to generalize a paraphrase example 
combing a semantic dictionary (Cilin (EV)). The 
experimental results proved that this representa-
tion method is reasonable and the generalized 
templates have a good precision and coverage.  
But this is just the beginning of the para-
phrase examples generalization. And we sim-
plify the problem in some aspects, such as we 
limited the number of the slot word in a para-
phrase example, and we stipulate only the same 
word can be slot word. Also, we find that our 
templates are weak for those words or structures 
that don?t exist in dictionary. Some methods in 
information extraction about named entities 
generalization can be used for reference in the 
future. Moreover, how to combine the semantic 
code with other representation forms together is 
also an interesting work. 
References
[1] Chris Quirk, Chris Brockett, and William Dolan. 
Monolingual Machine Translation for Para-
phrase Generation. editors, Dekang Lin and 
Dekai Wu, In Proceedings of EMNLP 2004, 
Barcelona, pages 142-149  
[2] Dekang Lin and Patrick Pantel. 2001. Discovery 
of Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-360 
[3] Dekang Lin and Patrick Pantel. Discovery of 
inference rules for question answering. Natural 
Language Engineering, 1, 2001.  
[4] E. Agichtein, S. Lawrence, and L. Gravano. 
Learning search engine specific query transfor-
mations for question answering. In Proceedings 
of the 10th International World-Wide Web Con-
ference (WWW10), 2001 
[5] Fabio Rinaldi, James Dowdall, Kaarel Kalju-
rand, Michael Hess, Diego Molla. 2003. Ex-
ploiting Paraphrases in a Question Answering 
System. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[6] Florence Duclaye France. Learning paraphrases 
to improve a question-answering system. In 
EACL Natural Language Processing for Ques-
tion Answering, 2003 
[7] Graeme Hirst. Paraphrasing Paraphrased. In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003 
[8] Hal Daum? III and Daniel Marcu. Acquiring 
paraphrase templates from document/abstract 
pairs. In NL Seminar in ISI, 2003 
[9] Hua Wu, Ming Zhou. Optimizing Synonym 
Extraction Using Monolingual and Bilingual 
Resources. In Proceedings of the Second Inter-
national Workshop on Paraphrasing, 2003 
[10] Hua Wu, Ming Zhou. Synonymous Collocation 
Extraction Using Translation Information. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics, 
2003 
[11] Jinshan Ma, Yu Zhang, Ting Liu, and Sheng Li. 
A Statistical Dependency Parser of Chinese un-
der Small Training Data. Workshop: Beyond 
shallow analyses - Formalisms and statistical 
modeling for deep analyses, IJCNLP-04, 4 2004. 
[12] Noriko Tomuro. 2003. Interrogative Reformula-
tion Patterns and Acquisition of Question Para-
phrases. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[13] Oren Glickman and Ido Dagan. Identifying 
lexical paraphrases from a single corpus: A case 
study for verbs. In Proceedings of Recent Ad-
vantages in Natural Language Processing, Sep-
tember 2003 
55
[14] Regina Barzilay and Kathleen McKeown. Ex-
tracting paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL, Toulouse, 2001 
[15] Regina Barzilay and Lillian Lee. Learning to 
Paraphrase: An Unsupervised Approach Using 
Multiple-Sequence Alignment. In Proceedings 
of HLT-NAACL 2003, pages 16-23  
[16] Regina Barzilay, Noemie Elhadad, Kathleen R. 
McKeown. 2003. Inferring Strategies for Sen-
tence Ordering in Multidocument News Sum-
marization. The Second International Workshop 
on Paraphrasing: Paraphrase Acquisition and 
Applications 
[17] Weigang Li, Ting Liu, Sheng Li. Combining 
Sentence Length with Location Information to 
Align Monolingual Parallel Texts. AIRS, 2004, 
pages 71-77 
[18] Yusuke Shinyama and Satoshi Sekine. Para-
phrase acquisition for information extraction. 
editors, Kentaro Inui and Ulf Hermjakob, In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003, pages 65-71 
[19] Yusuke Shinyama, Satoshi Sekine, Kiyoshi 
Sudo, and Ralph Grishman. Automatic para-
phrase acquisition from news articles, In Pro-
ceedings of Human Language Technology 
Conference (HLT2002), San Diego, USA, Mar. 
15, 2002 
[20] Zhang Yujie, Kazuhide Yamamoto. Automatic 
Paraphrasing of Chinese Utterances. Journal of 
Chinese Information Processing. Vol. 117 No. 
16: 31-38(Chinese) 
56
Fast Computing Grammar-driven Convolution Tree Kernel for
Semantic Role Labeling
Wanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li1
1School of Computer Science and Technology
Harbin Institute of Technology, China 150001
{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn
2Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{mzhang,aaiti}@i2r.a-star.edu.sg
3School of Computing
National University of Singapore, Singapore 117543
tancl@comp.nus.edu.sg
Abstract
Grammar-driven convolution tree kernel
(GTK) has shown promising results for se-
mantic role labeling (SRL). However, the
time complexity of computing the GTK is
exponential in theory. In order to speed
up the computing process, we design two
fast grammar-driven convolution tree kernel
(FGTK) algorithms, which can compute the
GTK in polynomial time. Experimental re-
sults on the CoNLL-2005 SRL data show
that our two FGTK algorithms are much
faster than the GTK.
1 Introduction
Given a sentence, the task of semantic role labeling
(SRL) is to analyze the propositions expressed by
some target verbs or nouns and some constituents
of the sentence. In previous work, data-driven tech-
niques, including feature-based and kernel-based
learning methods, have been extensively studied for
SRL (Carreras and Ma`rquez, 2005).
Although feature-based methods are regarded as
the state-of-the-art methods and achieve much suc-
cess in SRL, kernel-based methods are more effec-
tive in capturing structured features than feature-
based methods. In the meanwhile, the syntactic
structure features hidden in a parse tree have been
suggested as an important feature for SRL and need
to be further explored in SRL (Gildea and Palmer,
2002; Punyakanok et al, 2005). Moschitti (2004)
?The work was mainly done when the author was a visiting
student at I2R
and Che et al (2006) are two reported work to use
convolution tree kernel (TK) methods (Collins and
Duffy, 2001) for SRL and has shown promising re-
sults. However, as a general learning algorithm, the
TK only carries out hard matching between two sub-
trees without considering any linguistic knowledge
in kernel design. To solve the above issue, Zhang
et al (2007) proposed a grammar-driven convolu-
tion tree kernel (GTK) for SRL. The GTK can uti-
lize more grammatical structure features via two
grammar-driven approximate matching mechanisms
over substructures and nodes. Experimental results
show that the GTK significantly outperforms the
TK (Zhang et al, 2007). Theoretically, the GTK
method is applicable to any problem that uses syn-
tax structure features and can be solved by the TK
methods, such as parsing, relation extraction, and so
on. In this paper, we use SRL as an application to
test our proposed algorithms.
Although the GTK shows promising results for
SRL, one big issue for the kernel is that it needs ex-
ponential time to compute the kernel function since
it need to explicitly list all the possible variations
of two sub-trees in kernel calculation (Zhang et al,
2007). Therefore, this method only works efficiently
on such kinds of datasets where there are not too
many optional nodes in production rule set. In order
to solve this computation issue, we propose two fast
algorithms to compute the GTK in polynomial time.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the GTK. In Section 3,
we present our two fast algorithms for computing
the GTK. The experimental results are shown in Sec-
tion 4. Finally, we conclude our work in Section 5.
781
2 Grammar-driven Convolution Tree
Kernel
The GTK features with two grammar-driven ap-
proximate matching mechanisms over substructures
and nodes.
2.1 Grammar-driven Approximate Matching
Grammar-driven Approximate Substructure
Matching: the TK requires exact matching between
two phrase structures. For example, the two phrase
structures ?NP?DT JJ NN? (NP?a red car) and
?NP?DT NN? (NP?a car) are not identical, thus
they contribute nothing to the conventional kernel
although they share core syntactic structure property
and therefore should play the same semantic role
given a predicate. Zhang et al (2007) introduces
the concept of optional node to capture this phe-
nomenon. For example, in the production rule
?NP?DT [JJ] NP?, where [JJ] denotes an optional
node. Based on the concept of optional node, the
grammar-driven approximate substructure matching
mechanism is formulated as follows:
M(r1, r2) =
?
i,j
(IT (T ir1 , T jr2)? ?
ai+bj
1 ) (1)
where r1 is a production rule, representing a two-
layer sub-tree, and likewise for r2. T ir1 is the ith vari-
ation of the sub-tree r1 by removing one ore more
optional nodes, and likewise for T jr2 . IT (?, ?) is a bi-
nary function that is 1 iff the two sub-trees are iden-
tical and zero otherwise. ?1 (0 ? ?1 ? 1) is a small
penalty to penalize optional nodes. ai and bj stand
for the numbers of occurrence of removed optional
nodes in subtrees T ir1 and T jr2 , respectively.
M(r1, r2) returns the similarity (i.e., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees.
Grammar-driven Approximate Node Match-
ing: the TK needs an exact matching between two
nodes. But, some similar POSs may represent simi-
lar roles, such as NN (dog) and NNS (dogs). Zhang
et al (2007) define some equivalent nodes that can
match each other with a small penalty ?2 (0 ? ?2 ?
1). This case is called node feature mutation. The
approximate node matching can be formulated as:
M(f1, f2) =
?
i,j
(If (f i1, f j2 )? ?ai+bj2 ) (2)
where f1 is a node feature, f i1 is the ith mutation of
f1 and ai is 0 iff f i1 and f1 are identical and 1 oth-
erwise, and likewise for f2 and bj . If (?, ?) is a func-
tion that is 1 iff the two features are identical and
zero otherwise. Eq. (2) sums over all combinations
of feature mutations as the node feature similarity.
2.2 The GTK
Given these two approximate matching mecha-
nisms, the GTK is defined by beginning with the
feature vector representation of a parse tree T as:
??(T ) = (#subtree1(T ), . . . ,#subtreen(T ))
where #subtreei(T ) is the occurrence number of
the ith sub-tree type (subtreei) in T . Now the GTKis defined as follows:
KG(T1, T2) = ???(T1),??(T2)?
=?i #subtreei(T1) ?#subtreei(T2)=?i((
?
n1?N1 I
?
subtreei(n1))
? (?n2?N2 I
?
subtreei(n2)))
=?n1?N1
?
n2?N2 ?
?(n1, n2)
(3)
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively. I ?subtreei(n) is a function that
is ?a1 ??b2 iff there is a subtreei rooted at node n and
zero otherwise, where a and b are the numbers of
removed optional nodes and mutated node features,
respectively. ??(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
??(n1, n2) =
?
i
I ?subtreei(n1) ? I ?subtreei(n2) (4)
??(n1, n2) can be further computed by the follow-
ing recursive rules:
R-A: if n1 and n2 are pre-terminals, then:
??(n1, n2) = ??M(f1, f2) (5)
where f1 and f2 are features of nodes n1 and n2
respectively, and M(f1, f2) is defined in Eq. (2),
which can be computed in linear time O(n), where
n is the number of feature mutations.
R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-trees
of depth one rooted at n1 and n2 (denoted by Tn1
782
and Tn2 respectively) by removing different optionalnodes, then:
??(n1, n2) = ??
?
i,j IT (T in1 , T jn2)? ?
ai+bj
1
??nc(n1,i)k=1 (1 + ??(ch(n1, i, k), ch(n2, j, k)))
(6)
where T in1 , T jn2 , IT (?, ?), ai and bj have been ex-
plained in Eq. (1). nc(n1, i) returns the number
of children of n1 in its ith subtree variation T in1 .
ch(n1, i, k) is the kth child of node n1 in its ith vari-
ation subtree T in1 , and likewise for ch(n2, j, k). ?
(0 < ? < 1) is the decay factor.
R-C: else ??(n1, n2) = 0
3 Fast Computation of the GTK
Clearly, directly computing Eq. (6) requires expo-
nential time, since it needs to sum up all possible
variations of the sub-trees with and without optional
nodes. For example, supposing n1 = ?A?a [b] c
[d]?, n2 = ?A?a b c?. To compute the Eq. (6), we
have to list all possible variations of n1 and n2?s sub-
trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?a
c?; n2: ?A?a b c?. Unfortunately, Zhang et al
(2007) did not give any theoretical solution for the
issue of exponential computing time. In this paper,
we propose two algorithms to calculate it in polyno-
mial time. Firstly, we recast the issue of computing
Eq. (6) as a problem of finding common sub-trees
with and without optional nodes between two sub-
trees. Following this idea, we rewrite Eq. (6) as:
??(n1, n2) = ?? (1 +
lm?
p=lx
?p(cn1 , cn2)) (7)
where cn1 and cn2 are the child node sequences of
n1 and n2, ?p evaluates the number of common
sub-trees with exactly p children (at least including
all non-optional nodes) rooted at n1 and n2, lx =
max{np(cn1), np(cn2)} and np(?) is the number of
non-optional nodes, lm = min{l(cn1), l(cn2)}and
l(?) returns the number of children.
Now let?s study how to calculate ?p(cn1 , cn2) us-
ing dynamic programming algorithms. Here, we
present two dynamic programming algorithms to
compute it in polynomial time.
3.1 Fast Grammar-driven Convolution Tree
Kernel I (FGTK-I)
Our FGTK-I algorithm is motivated by the string
subsequence kernel (SSK) (Lodhi et al, 2002).
Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:
??0(s, t) = 1, for all s, t,
??p(s, t) = 0, ifmin(|s|, |t|) < p, (8)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)
??p(sx, t) = ????p(sx, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?|t|?j+2)),(10)
p = 1, . . . , n? 1,
?p(sx, t) = ?p(s, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?2)). (11)
where ??p is an auxiliary function since it is only
the interior gaps in the subsequences that are penal-
ized; ? is a decay factor only used in the SSK for
weighting each extra length unit. Lodhi et al (2002)
explained the correctness of the recursion defined
above.
Compared with the SSK kernel, the GTK has
three different features:
f1: In the GTK, only optional nodes can be
skipped while the SSK kernel allows any node skip-
ping;
f2: The GTK penalizes skipped optional nodes
only (including both interior and exterior skipped
nodes) while the SSK kernel weights the length of
subsequences (all interior skipped nodes are counted
in, but exterior nodes are ignored);
f3: The GTK needs to further calculate the num-
ber of common sub-trees rooted at each two match-
ing node pair x and t[j].
To reflect the three considerations, we modify the
SSK kernel as follows to calculate the GTK:
?0(s, t) = opt(s)? opt(t)? ?|s|+|t|1 , for all s, t, (12)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)
?p(sx, t) = ?1 ??p(sx, t)? opt(x)
+
?
j:tj=x
(?p?1(s, t[1 : j ? 1])? ?|t|?j (14)
?opt(t[j + 1 : |t|])???(x, t[j])).
where opt(w) is a binary function, which is 0 if
non-optional nodes are found in the node sequence
w and 1 otherwise (f1); ?1 is the penalty to penalize
skipped optional nodes and the power of ?1 is the
number of skipped optional nodes (f2); ??(x, t[j])
is defined in Eq. (7) (f3). Now let us compare
783
the FGTK-I and SSK kernel algorithms. Based on
Eqs. (8), (9), (10) and (11), we introduce the opt(?)
function and the penalty ?1 into Eqs. (12), (13) and
(14), respectively. opt(?) is to ensure that in the
GTK only optional nodes are allowed to be skipped.
And only those skipped optional nodes are penal-
ized with ?1. Please note that Eqs. (10) and (11)
are merged into Eq. (14) because of the different
meaning of ? and ?1. From Eq. (8), we can see
that the current path in the recursive call will stop
and its value becomes zero once non-optional node
is skipped (when opt(w) = 0).
Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =
?A?a b c? to exemplify how the FGTK-I algorithm
works. In Eq. (14)?s vocabulary, we have s = ?a [b]
c?, t = ?a b c?, x = ?[d]?, opt(x) = opt([d]) = 1,
p = 3. Then according to Eq (14), ?p(cn1 , cn2) can
be calculated recursively as Eq. (15) (Please refer to
the next page).
Finally, we have ?p(cn1 , cn2) = ?1 ???(a, a)?
??(b, b)???(c, c)
By means of the above algorithm, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |2) (Lodhi et
al., 2002). This means that the worst case complex-
ity of the FGTK-I is O(p?3|N1| ? |N2|2), where ? is
the maximum branching factor of the two trees.
3.2 Fast Grammar-driven Convolution Tree
Kernel II (FGTK-II)
Our FGTK-II algorithm is motivated by the partial
trees (PTs) kernel (Moschitti, 2006). The PT kernel
algorithm uses the following recursive formulas to
evaluate ?p(cn1 , cn2):
?p(cn1 , cn2) =
|cn1 |?
i=1
|cn2 |?
j=1
??p(cn1 [1 : i], cn2 [1 : j]) (16)
where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively. Given two child node sequences
s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b are
the last children), the PT kernel computes ??p(?, ?) as
follows:
??p(s1a, s2b) =
{
?2??(a, b)Dp(|s1|, |s2|) if a = b
0 else (17)
where ??(a, b) is defined in Eq. (7) and Dp is recur-
sively defined as follows:
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?Dp(k, l ? 1) + ?Dp(k ? 1, l) (18)
??2Dp(k ? 1, l ? 1)
D1(k, l) = 1, for all k, l (19)
where ? used in Eqs. (17) and (18) is a factor to
penalize the length of the child sequences.
Compared with the PT kernel, the GTK has two
different features which are the same as f1 and f2
when defining the FGTK-I.
To reflect the two considerations, based on the PT
kernel algorithm, we define another fast algorithm
of computing the GTK as follows:
?p(cn1 , cn2 ) =
? |cn1 |
i=1
? |cn2 |
j=1 ??p(cn1 [1 : i], cn2 [1 : j])
?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])
??|cn1 |?i+|cn2 |?j1
(20)
??p(s1a, s2b) =
{ ??(a, b)Dp(|s1|, |s2|) if a = b
0 else (21)
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?1Dp(k, l ? 1)? opt(s2[l]) (22)
+?1Dp(k ? 1, l)? opt(s1[k])
??21Dp(k ? 1, l ? 1)? opt(s1[k])? opt(s2[l])
D1(k, l) = ?k+l1 ? opt(s1[1 : k])? opt(s2[1 : l]), (23)
for all k, l
??p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)
where opt(w) and ?1 are the same as them in the
FGTK-I.
Now let us compare the FGTK-II and the PT al-
gorithms. Based on Eqs. (16), (18) and (19), we in-
troduce the opt(?) function and the penalty ?1 into
Eqs. (20), (22) and (23), respectively. This is to
ensure that in the GTK only optional nodes are al-
lowed to be skipped and only those skipped optional
nodes are penalized. In addition, compared with
Eq. (17), the penalty ?2 is removed in Eq. (21) in
view that our kernel only penalizes skipped nodes.
Moreover, Eq. (24) is only for fast computing. Fi-
nally, the same as the FGTK-I, in the FGTK-II the
current path in a recursive call will stop and its value
becomes zero once non-optional node is skipped
(when opt(w) = 0). Here, we still can use an ex-
ample to derivate the process of the algorithm step
by step as that for FGTK-I algorithm. Due to space
limitation, here, we do not illustrate it in detail.
By means of the above algorithms, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |) (Moschitti,
784
?p(cn1 , cn2 ) = ?p(?a [b] c [d]? , ?a b c?)
= ?1 ??p(?a [b] c?, ?a b c?) + 0 //Since x * t, the second term is 0
= ?1 ? (0 + ?p?1(?a [b]?, ?a b?)? ?3?31 ???(c, c)) //Since opt(?c?) = 0, the first term is 0
= ?1 ???(c, c)? (0 + ?p?2(?a?, ?a b?)? ?2?21 ???(b, b)) //Since p? 1 > |?a?|,?p?2(?a?, ?a b?) = 0
= ?1 ???(c, c)? (0 + ??(a, a)???(b, b)) //?p?2(?a?, ?a?) = ??(a, a)
(15)
2006). This means that the worst complexity of the
FGTK-II is O(p?2|N1| ? |N2|). It is faster than the
FGTK-I?s O(p?3|N1| ? |N2|2) in theory. Please note
that the average ? in natural language parse trees is
very small and the overall complexity of the FGTKs
can be further reduced by avoiding the computation
of node pairs with different labels (Moschitti, 2006).
4 Experiments
4.1 Experimental Setting
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Ma`rquez, 2005) as our experi-
mental corpus.
Classifier: SVM (Vapnik, 1998) is selected as our
classifier. In the FGTKs implementation, we mod-
ified the binary Tree Kernels in SVM-Light Tool
(SVM-Light-TK) (Moschitti, 2006) to a grammar-
driven one that encodes the GTK and the two fast dy-
namic algorithms inside the well-known SVM-Light
tool (Joachims, 2002). The parameters are the same
as Zhang et al (2007).
Kernel Setup: We use Che et al (2006)?s hybrid
convolution tree kernel (the best-reported method
for kernel-based SRL) as our baseline kernel. It is
defined as Khybrid = ?Kpath + (1 ? ?)Kcs (0 ?
? ? 1)1. Here, we use the GTK to compute the
Kpath and the Kcs.
In the training data (WSJ sections 02-21), we get
4,734 production rules which appear at least 5 times.
Finally, we use 1,404 rules with optional nodes for
the approximate structure matching. For the node
approximate matching, we use the same equivalent
node sets as Zhang et al (2007).
4.2 Experimental Results
We use 30,000 instances (a subset of the entire train-
ing set) as our training set to compare the different
kernel computing algorithms 2. All experiments are
1Kpath and Kcs are two TKs to describe predicate-
argument link features and argument syntactic structure fea-
tures, respectively. For details, please refer to (Che et al, 2006).
2There are about 450,000 identification instances are ex-
tracted from training data.
conducted on a PC with CPU 2.8GH and memory
1G. Fig. 1 reports the experimental results, where
training curves (time vs. # of instances) of five
kernels are illustrated, namely the TK, the FGTK-
I, the FGTK-II, the GTK and a polynomial kernel
(only for reference). It clearly demonstrates that our
FGTKs are faster than the GTK algorithm as ex-
pected. However, the improvement seems not so
significant. This is not surprising as there are only
30.4% rules (1,404 out of 4,734)3 that have optional
nodes and most of them have only one optional
node4. Therefore, in this case, it is not time con-
suming to list all the possible sub-tree variations and
sum them up. Let us study this issue from computa-
tional complexity viewpoint. Suppose all rules have
exactly one optional node. This means each rule can
only generate two variations. Therefore computing
Eq. (6) is only 4 times (2*2) slower than the GTK
in this case. In other words, we can say that given
the constraint that there is only one optional node
in one rule, the time complexity of the GTK is also
O(|N1| ? |N2|) 5, where N1 and N2 are the numbers
of tree nodes, the same as the TK.
12000
6000
8000
10000
Train
ing T
ime (
S) GTKFGTK-I
2000
4000Tra
ining
 Time
 (S)
FGTK-IITKPoly
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 1: Training time comparison among different
kernels with rule set having less optional nodes.
Moreover, Fig 1 shows that the FGTK-II is faster
than the FGTK-I. This is reasonable since as dis-
3The percentage is even smaller if we consider all produc-
tion (it becomes 14.4% (1,404 out of 9,700)).
4There are 1.6 optional nodes in each rule averagely.
5Indeed it is O(4 ? |N1| ? |N2|). The parameter 4 is omitted
when discussing time complexity.
785
cussed in Subsection 3.2, the FGTK-I?s time com-
plexity is O(p?3|N1| ? |N2|2) while the FGTK-II?s is
O(p?2|N1| ? |N2|).
40000
45000
20000
25000
30000
35000
Train
ing T
ime (
S) GTKFGTK-I
0
5000
10000
15000Trai
ning 
Time
 (S)
FGTK-IITKPoly
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 2: Training time comparison among different
kernels with rule set having more optional nodes.
To further verify the efficiency of our proposed
algorithm, we conduct another experiment. Here we
use the same setting as that in Fig 1 except that we
randomly add more optional nodes in more produc-
tion rules. Table 1 reports the statistics on the two
rule set. Similar to Fig 1, Fig 2 compares the train-
ing time of different algorithms. We can see that
Fig 2 convincingly justify that our algorithms are
much faster than the GTK when the experimental
data has more optional nodes and rules.
Table 1: The rule set comparison between two ex-
periments.
# rules # rule with at
least optional
nodes
# op-
tional
nodes
# average op-
tional nodes per
rule
Exp1 4,734 1,404 2,242 1.6
Exp2 4,734 4,520 10,451 2.3
5 Conclusion
The GTK is a generalization of the TK, which can
capture more linguistic grammar knowledge into the
later and thereby achieve better performance. How-
ever, a biggest issue for the GTK is its comput-
ing speed, which needs exponential time in the-
ory. Therefore, in this paper we design two fast
grammar-driven convolution tree kennel (FGTK-I
and II) algorithms which can compute the GTK in
polynomial time. The experimental results show that
the FGTKs are much faster than the GTK when data
set has more optional nodes. We conclude that our
fast algorithms enable the GTK kernel to easily scale
to larger dataset. Besides the GTK, the idea of our
fast algorithms can be easily used into other similar
problems.
To further our study, we will use the FGTK algo-
rithms for other natural language processing prob-
lems, such as word sense disambiguation, syntactic
parsing, and so on.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS-
2001.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL-2002, pages 239?246.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL-2004, pages 335?342.
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proceedings of the HHLT-NAACL-2006, June.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In Proceedings of IJCAI-2005.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proceedings of ACL-2007, pages
200?207.
786
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 457?464,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 An Equivalent Pseudoword Solution to Chinese  
Word Sense Disambiguation 
 
Zhimao Lu+    Haifeng Wang++    Jianmin Yao+++    Ting Liu+    Sheng Li+
+ Information Retrieval Laboratory, School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin, 150001, China 
{lzm, tliu, lisheng}@ir-lab.org 
++ Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No. 1, East Chang An Ave., Beijing, 100738, China 
wanghaifeng@rdc.toshiba.com.cn 
+++ School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
jyao@suda.edu.cn 
 
  
 
Abstract 
This paper presents a new approach 
based on Equivalent Pseudowords (EPs) 
to tackle Word Sense Disambiguation 
(WSD) in Chinese language. EPs are par-
ticular artificial ambiguous words, which 
can be used to realize unsupervised WSD. 
A Bayesian classifier is implemented to 
test the efficacy of the EP solution on 
Senseval-3 Chinese test set. The per-
formance is better than state-of-the-art 
results with an average F-measure of 0.80. 
The experiment verifies the value of EP 
for unsupervised WSD. 
1 Introduction 
Word sense disambiguation (WSD) has been a 
hot topic in natural language processing, which is 
to determine the sense of an ambiguous word in 
a specific context. It is an important technique 
for applications such as information retrieval, 
text mining, machine translation, text classifica-
tion, automatic text summarization, and so on. 
Statistical solutions to WSD acquire linguistic 
knowledge from the training corpus using ma-
chine learning technologies, and apply the 
knowledge to disambiguation. The first statistical 
model of WSD was built by Brown et al (1991). 
Since then, most machine learning methods have 
been applied to WSD, including decision tree, 
Bayesian model, neural network, SVM, maxi-
mum entropy, genetic algorithms, and so on. For 
different learning methods, supervised methods 
usually achieve good performance at a cost of 
human tagging of training corpus. The precision 
improves with larger size of training corpus. 
Compared with supervised methods, unsuper-
vised methods do not require tagged corpus, but 
the precision is usually lower than that of the 
supervised methods. Thus, knowledge acquisi-
tion is critical to WSD methods.  
This paper proposes an unsupervised method 
based on equivalent pseudowords, which ac-
quires WSD knowledge from raw corpus. This 
method first determines equivalent pseudowords 
for each ambiguous word, and then uses the 
equivalent pseudowords to replace the ambigu-
ous word in the corpus. The advantage of this 
method is that it does not need parallel corpus or 
seed corpus for training. Thus, it can use a large-
scale monolingual corpus for training to solve 
the data-sparseness problem. Experimental re-
sults show that our unsupervised method per-
forms better than the supervised method. 
The remainder of the paper is organized as fol-
lows. Section 2 summarizes the related work. 
Section 3 describes the conception of Equivalent 
Pseudoword. Section 4 describes EP-based Un-
supervised WSD Method and the evaluation re-
sult. The last section concludes our approach. 
2 Related Work 
For supervised WSD methods,  a knowledge ac-
quisition bottleneck is to prepare the manually 
457
tagged corpus. Unsupervised method is an alter-
native, which often involves automatic genera-
tion of tagged corpus, bilingual corpus alignment, 
etc. The value of unsupervised methods lies in 
the knowledge acquisition solutions they adopt. 
2.1 Automatic Generation of Training Corpus 
Automatic corpus tagging is a solution to WSD, 
which generates large-scale corpus from a small 
seed corpus. This is a weakly supervised learning 
or semi-supervised learning method. This rein-
forcement algorithm dates back to Gale et al 
(1992a). Their investigation was based on a 6-
word test set with 2 senses for each word. 
Yarowsky (1994 and 1995), Mihalcea and 
Moldovan (2000), and Mihalcea (2002) have 
made further research to obtain large corpus of 
higher quality from an initial seed corpus. A 
semi-supervised method proposed by Niu et al 
(2005) clustered untagged instances with tagged 
ones starting from a small seed corpus, which 
assumes that similar instances should have simi-
lar tags. Clustering was used instead of boot-
strapping and was proved more efficient.  
2.2 Method Based on Parallel Corpus 
Parallel corpus is a solution to the bottleneck of 
knowledge acquisition. Ide et al (2001 and 
2002), Ng et al (2003), and Diab (2003, 2004a, 
and 2004b) made research on the use of align-
ment for WSD.  
Diab and Resnik (2002) investigated the feasi-
bility of automatically annotating large amounts 
of data in parallel corpora using an unsupervised 
algorithm, making use of two languages simulta-
neously, only one of which has an available 
sense inventory. The results showed that word-
level translation correspondences are a valuable 
source of information for sense disambiguation. 
The method by Li and Li (2002) does not re-
quire parallel corpus. It avoids the alignment 
work and takes advantage of bilingual corpus. 
In short, technology of automatic corpus tag-
ging is based on the manually labeled corpus. 
That is to say, it still need human intervention 
and is not a completely unsupervised method. 
Large-scale parallel corpus; especially word-
aligned corpus is highly unobtainable, which has 
limited the WSD methods based on parallel cor-
pus.  
3 Equivalent Pseudoword 
This section describes how to obtain equivalent 
pseudowords without a seed corpus. 
Monosemous words are unambiguous priori 
knowledge. According to our statistics, they ac-
count for 86%~89% of the instances in a diction-
ary and 50% of the items in running corpus, they 
are potential knowledge source for WSD.  
A monosemous word is usually synonymous 
to some polysemous words. For example the 
words "?? , ?? , ?? ?? ?? ??, , , , 
??" has similar meaning as one of the senses 
of the ambiguous word "??", while "??, ?
?, ?? ?? ??, , ?? ?? ?? ??, , , , , 
?? ?? ?? ??, , , " are the same for "??". 
This is quite common in Chinese, which can be 
used as a knowledge source for WSD. 
3.1 Definition of Equivalent Pseudoword 
If the ambiguous words in the corpus are re-
placed with its synonymous monosemous word, 
then is it convenient to acquire knowledge from 
raw corpus? For example in table 1, the ambigu-
ous word "??" has three senses, whose syn-
onymous monosemous words are listed on the 
right column. These synonyms contain some in-
formation for disambiguation task. 
An artificial ambiguous word can be coined 
with the monosemous words in table 1. This 
process is similar to the use of general pseu-
dowords (Gale et al, 1992b; Gaustad, 2001; Na-
kov and Hearst, 2003), but has some essential 
differences. This artificial ambiguous word need 
to simulate the function of the real ambiguous 
word, and to acquire semantic knowledge as the 
real ambiguous word does. Thus, we call it an 
equivalent pseudoword (EP) for its equivalence 
with the real ambiguous word. It's apparent that 
the equivalent pseudoword has provided a new 
way to unsupervised WSD. 
S1 ??/??? 
S2 ??/??/??/??/????(ba3 wo4)
S3 ??/??/??/??/??
Table 1. Synonymous Monosemous Words for 
the Ambiguous Word "??" 
The equivalence of the EP with the real am-
biguous word is a kind of semantic synonym or 
similarity, which demands a maximum similarity 
between the two words. An ambiguous word has 
the same number of EPs as of senses. Each EP's 
sense maps to a sense of ambiguous word. 
The semantic equivalence demands further 
equivalence at each sense level. Every corre-
458
sponding sense should have the maximum simi-
larity, which is the strictest limit to the construc-
tion of an EP. 
The starting point of unsupervised WSD based 
on EP is that EP can substitute the original word 
for knowledge acquisition in model training. 
Every instance of each morpheme of the EP can 
be viewed as an instance of the ambiguous word, 
thus the training set can be enlarged easily. EP is 
a solution to data sparseness for lack of human 
tagging in WSD. 
3.2 Basic Assumption for EP-based WSD 
It is based on the following assumptions that EPs 
can substitute the original ambiguous word for 
knowledge acquisition in WSD model training. 
Assumption 1: Words of the same meaning 
play the same role in a language. The sense is an 
important attribute of a word. This plays as the 
basic assumption in this paper. 
Assumption 2: Words of the same meaning 
occur in similar context. This assumption is 
widely used in semantic analysis and plays as a 
basis for much related research. For example, 
some researchers cluster the contexts of ambigu-
ous words for WSD, which shows good perform-
ance (Schutze, 1998). 
Because an EP has a higher similarity with the 
ambiguous word in syntax and semantics, it is a 
useful knowledge source for WSD. 
3.3 Design and Construction of EPs 
Because of the special characteristics of EPs, it's 
more difficult to construct an EP than a general 
pseudo word. To ensure the maximum similarity 
between the EP and the original ambiguous word, 
the following principles should be followed. 
1) Every EP should map to one and only one 
original ambiguous word. 
2) The morphemes of an EP should map one 
by one to those of the original ambiguous word. 
3) The sense of the EP should be the same as 
the corresponding ambiguous word, or has the 
maximum similarity with the word. 
4) The morpheme of a pseudoword stands for 
a sense, while the sense should consist of one or 
more morphemes.  
5) The morpheme should be a monosemous 
word. 
The fourth principle above is the biggest dif-
ference between the EP and a general pseudo 
word. The sense of an EP is composed of one or 
several morphemes. This is a remarkable feature 
of the EP, which originates from its equivalent 
linguistic function with the original word. To 
construct the EP, it must be ensured that the 
sense of the EP maps to that of the original word. 
Usually, a candidate monosemous word for a 
morpheme stands for part of the linguistic func-
tion of the ambiguous word, thus we need to 
choose several morphemes to stand for one sense.  
The relatedness of the senses refers to the 
similarity of the contexts of the original ambigu-
ous word and its EP. The similarity between the 
words means that they serve as synonyms for 
each other. This principle demands that both se-
mantic and pragmatic information should be 
taken into account in choosing a morpheme word. 
3.4 Implementation of the EP-based Solution 
An appropriate machine-readable dictionary is 
needed for construction of the EPs. A Chinese 
thesaurus is adopted and revised to meet this de-
mand. 
Extended Version of TongYiCiCiLin 
To extend the TongYiCiCiLin (Cilin) to hold 
more words, several linguistic resources are 
adopted for manually adding new words. An ex-
tended version of the Cilin is achieved, which 
includes 77,343 items. 
A hierarchy of three levels is organized in the 
extended Cilin for all items. Each node in the 
lowest level, called a minor class, contains sev-
eral words of the same class. The words in one 
minor class are divided into several groups ac-
cording to their sense similarity and relatedness, 
and each group is further divided into several 
lines, which can be viewed as the fifth level of 
the thesaurus. The 5-level hierarchy of the ex-
tended Cilin is shown in figure 1. The lower the 
level is, the more specific the sense is. The fifth 
level often contains a few words or only one 
word, which is called an atom word group, an 
atom class or an atom node. The words in the 
same atom node hold the smallest semantic dis-
tance. 
From the root node to the leaf node, the sense 
is described more and more detailed, and the 
words in the same node are more and more re-
lated. Words in the same fifth level node have 
the same sense and linguistic function, which 
ensures that they can substitute for each other 
without leading to any change in the meaning of 
a sentence. 
 
 
459
 ?  ? 
?
?? ?? 
? ? ? ?
? ? ?
 
? ? ? ?
Level 1 
Level 2 
Level 3 
Level 4 
Level 5 
?  ? 
Figure 1. Organization of Cilin (extended) 
 
The extended version of extended Cilin is 
freely downloadable from the Internet and has 
been used by over 20 organizations in the world1. 
Construction of EPs 
According to the position of the ambiguous word, 
a proper word is selected as the morpheme of the 
EP. Almost every ambiguous word has its corre-
sponding EP constructed in this way. 
The first step is to decide the position of the 
ambiguous word starting from the leaf node of 
the tree structure. Words in the same leaf node 
are identical or similar in the linguistic function 
and word sense. Other words in the leaf node of 
the ambiguous word are called brother words of 
it. If there is a monosemous brother word, it can 
be taken as a candidate morpheme for the EP. If 
there does not exist such a brother word, trace to 
the fourth level. If there is still no monosemous 
brother word in the fourth level, trace to the third 
level. Because every node in the third level con-
tains many words, candidate morpheme for the 
ambiguous can usually be found. 
In most cases, candidate morphemes can be 
found at the fifth level. It is not often necessary 
to search to the fourth level, less to the third. Ac-
cording to our statistics, the extended Cilin con-
tains about monosemous words for 93% of the 
ambiguous words in the fifth level, and 97% in 
the fourth level. There are only 112 ambiguous 
words left, which account for the other 3% and 
mainly are functional words. Some of the 3% 
words are rarely used, which cannot be found in 
even a large corpus. And words that lead to se-
mantic misunderstanding are usually content 
words. In WSD research for English, only nouns, 
verbs, adjectives and adverbs are considered. 
                                                 
1 It is located at http://www.ir-lab.org/. 
From this aspect, the extended version of Cilin 
meets our demand for the construction of EPs. 
If many monosemous brother words are found 
in the fourth or third level, there are many candi-
date morphemes to choose from. A further selec-
tion is made based on calculation of sense simi-
larity. More similar brother words are chosen. 
Computing of EPs 
Generally, several morpheme words are needed 
for better construction of an EP. We assume that 
every morpheme word stands for a specific sense 
and does not influence each other. It is more 
complex to construct an EP than a common 
pseudo word, and the formulation and statistical 
information are also different. 
An EP is described as follows:  
 
iikiiii
k
k
WWWWS
WWWWS
WWWWS
L
MMMMMM
L
L
,,,:
,,,:
,,,:
321
22322212
11312111
2
1
 
WEP?????????? 
Where WEP is the EP word, Si is a sense of the 
ambiguous word, and Wik is a morpheme word of 
the EP. 
The statistical information of the EP is calcu-
lated as follows: 
1? stands for the frequency of the S)( iSC i : 
?=
k
iki WCSC )()(  
2? stands for the co-occurrence fre-
quency of S
),( fi WSC
i and the contextual word Wf : 
?=
k
fikfi WWCWSC ),(),(  
460
 Ambiguous word citation (Qin and Wang, 2005) Ours Ambiguous word 
citation (Qin and 
Wang, 2005) Ours 
??(ba3 wo4) 0.56 0.87 ??(mei2 you3) 0.75 0.68 
?(bao1) 0.59 0.75 ??(qi3 lai2) 0.82 0.54 
??(cai2 liao4) 0.67 0.79 ?(qian2) 0.75 0.62 
??(chong1 ji1) 0.62 0.69 ??(ri4 zi3) 0.75 0.68 
?(chuan1) 0.80 0.57 ?(shao3) 0.69 0.56 
??(di4 fang1) 0.65 0.65 ??(tu1 chu1) 0.82 0.86 
??(fen1 zi3) 0.91 0.81 ??(yan2 jiu1) 0.69 0.63 
??(yun4 dong4) 0.61 0.82 ??(huo2 dong4) 0.79 0.88 
?(lao3) 0.59 0.50 ?(zou3) 0.72 0.60 
?(lu4) 0.74 0.64 ?(zuo4) 0.90 0.73 
Average 0.72 0.69 Note: Average of the 20 words 
Table 2. The F-measure for the Supervised WSD 
 
4 EP-based Unsupervised WSD Method 
EP is a solution to the semantic knowledge ac-
quisition problem, and it does not limit the 
choice of statistical learning methods. All of the 
mathematical modeling methods can be applied 
to EP-based WSD methods. This section focuses 
on the application of the EP concept to WSD, 
and chooses Bayesian method for the classifier 
construction. 
4.1 A Sense Classifier Based on the Bayes-
ian Model 
Because the model acquires knowledge from the 
EPs but not from the original ambiguous word, 
the method introduced here does not need human 
tagging of training corpus. 
In the training stage for WSD, statistics of EPs 
and context words are obtained and stored in a 
database. Senseval-3 data set plus unsupervised 
learning method are adopted to investigate into 
the value of EP in WSD. To ensure the compara-
bility of experiment results, a Bayesian classifier 
is used in the experiments. 
Bayesian Classifier 
Although the Bayesian classifier is simple, it is 
quite efficient, and it shows good performance 
on WSD. 
The Bayesian classifier used in this paper is 
described in (1) 
???
?
???
?
+= ?
? ij
k
cv
kjkSi SvPSPwS )|(log)(logmaxarg)( (1)
Where wi is the ambiguous word,  is the 
occurrence probability of the sense S
)( kSP
k,  
is the conditional probability of the context word 
v
)|( kj SvP
j, and ci is the set of the context words. 
To simplify the experiment process, the Naive 
Bayesian modeling is adopted for the sense clas-
sifier. Feature selection and ensemble classifica-
tion are not applied, which is both to simplify the 
calculation and to prove the effect of EPs in 
WSD. 
Experiment Setup and Results  
The Senseval-3 Chinese ambiguous words are 
taken as the testing set, which includes 20 words, 
each with 2-8 senses. The data for the ambiguous 
words are divided into a training set and a testing 
set by a ratio of 2:1. There are 15-20 training 
instances for each sense of the words, and occurs 
by the same frequency in the training and test set. 
Supervised WSD is first implemented using 
the Bayesian model on the Senseval-3 data set. 
With a context window of (-10, +10), the open 
test results are shown in table 2. 
The F-measure in table 2 is defined in (2). 
RP
RP
F +
??= 2  (2) 
461
Where P and R refer to the precision and recall 
of the sense tagging respectively, which are cal-
culated as shown in (3) and (4) 
)tagged(
)correct(
C
C
P =  (3) 
)all(
)correct(
C
C
R =  (4) 
Where C(tagged) is the number of tagged in-
stances of senses, C(correct) is the number of 
correct tags, and C(all) is the number of tags in 
the gold standard set. Every sense of the am-
biguous word has a P value, a R value and a F 
value. The F value in table 2 is a weighted aver-
age of all the senses. 
In the EP-based unsupervised WSD experi-
ment, a 100M corpus (People's Daily for year 
1998) is used for the EP training instances. The 
Senseval-3 data is used for the test. In our ex-
periments, a context window of (-10, +10) is 
taken. The detailed results are shown in table 3. 
4.2 Experiment Analysis and Discussion 
Experiment Evaluation Method 
Two evaluation criteria are used in the experi-
ments, which are the F-measure and precision. 
Precision is a usual criterion in WSD perform-
ance analysis. Only in recent years, the precision, 
recall, and F-measure are all taken to evaluate 
the WSD performance. 
In this paper, we will only show the f-measure 
score because it is a combined score of precision 
and recall. 
Result Analysis on Bayesian Supervised WSD 
Experiment 
The experiment results in table 2 reveals that the 
results of supervised WSD and those of (Qin and 
Wang, 2005) are different. Although they are all 
based on the Bayesian model, Qin and Wang 
(2005) used an ensemble classifier. However, the 
difference of the average value is not remarkable. 
As introduced above, in the supervised WSD 
experiment, the various senses of the instances 
are evenly distributed. The lower bound as Gale 
et al (1992c) suggested should be very low and 
it is more difficult to disambiguate if there are 
more senses. The experiment verifies this reason-
ing, because the highest F-measure is less than 
90%, and the lowest is less than 60%, averaging 
about 70%. 
With the same number of senses and the same 
scale of training data, there is a big difference 
between the WSD results. This shows that other 
factors exist which influence the performance 
other than the number of senses and training data 
size. For example, the discriminability among the 
senses is an important factor. The WSD task be-
comes more difficult if the senses of the ambigu-
ous word are more similar to each other. 
Experiment Analysis of the EP-based WSD 
The EP-based unsupervised method takes the 
same open test set as the supervised method. The 
unsupervised method shows a better performance, 
with the highest F-measure score at 100%, low-
est at 59% and average at 80%. The results 
shows that EP is useful in unsupervised WSD. 
 
Sequence 
Number Ambiguous word F-measure
Sequence 
Number Ambiguous word 
F-measure 
(%) 
1 ??(ba3 wo4) 0.93 11 ??(mei2 you3) 1.00 
2 ?(bao1) 0.74 12 ??(qi3 lai2) 0.59 
3 ?(cai2 liao4) 0.80 13 ?(qian2) 0.71 
4 ??(chong1 ji1) 0.85 14 ??(ri4 zi3) 0.62 
5 ?(chuan1) 0.79 15 ?(shao3) 0.82 
6 ??(di4 fang1) 0.78 16 ??(tu1 chu1) 0.93 
7 ??(fen1 zi3) 0.94 17 ??(yan2 jiu1) 0.71 
8 ??(yun4 
dong4) 
0.94 18 ??(huo2 dong4) 0.89 
9 ?(lao3) 0.85 19 ?(zou3) 0.68 
10 ?(lu4) 0.81 20 ?(zuo4) 0.67 
Average 0.80 Note: Average of the 20 words 
Table 3. The Results for Unsupervised WSD based on EPs 
462
 
From the results in table 2 and table 3, it can 
be seen that 16 among the 20 ambiguous words 
show better WSD performance in unsupervised 
SWD than in supervised WSD, while only 2 of 
them shows similar results and 2 performs worse . 
The average F-measure of the unsupervised 
method is higher by more than 10%. The reason 
lies in the following aspects: 
1) Because there are several morpheme words 
for every sense of the word in construction of the 
EP, rich semantic information can be acquired in 
the training step and is an advantage for sense 
disambiguation. 
2) Senseval-3 has provided a small-scale train-
ing set, with 15-20 training instances for each 
sense, which is not enough for the WSD model-
ing. The lack of training information leads to a 
low performance of the supervised methods. 
3) With a large-scale training corpus, the un-
supervised WSD method has got plenty of train-
ing instances for a high performance in disam-
biguation. 
4) The discriminability of some ambiguous 
word may be low, but the corresponding EPs 
could be easier to disambiguate. For example, 
the ambiguous word "?" has two senses which 
are difficult to distinguish from each other, but 
its Eps' senses of "??/??/??" and "?/?/
?/?"can be easily disambiguated. It is the same 
for the word "??", whose Eps' senses are "?
?/?? /??" and "??/??". EP-based 
knowledge acquisition of these ambiguous words 
for WSD has helped a lot to achieve high per-
formance. 
5 Conclusion 
As discussed above, the supervised WSD method 
shows a low performance because of its depend-
ency on the size of the training data. This reveals 
its weakness in knowledge acquisition bottleneck. 
EP-based unsupervised method has overcame 
this weakness. It requires no manually tagged 
corpus to achieve a satisfactory performance on 
WSD. Experimental results show that EP-based 
method is a promising solution to the large-scale 
WSD task. In future work, we will examine the 
effectiveness of EP-based method in other WSD 
techniques. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
Sense Disambiguation Using Statistical Methods. 
In Proc. of the 29th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-1991), 
pages 264-270. 
Mona Talat Diab. 2003. Word Sense Disambiguation 
Within a Multilingual Framework. PhD thesis, 
University of Maryland College Park. 
Mona Diab. 2004a. Relieving the Data Acquisition 
Bottleneck in Word Sense Disambiguation. In Proc. 
of the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 303-
310. 
Mona T. Diab. 2004b. An Unsupervised Approach for 
Bootstrapping Arabic Sense Tagging. In Proc. of 
Arabic Script Based Languages Workshop at COL-
ING 2004, pages 43-50. 
Mona Diab and Philip Resnik. 2002. An Unsuper-
vised Method for Word Sense Tagging Using Par-
allel Corpora. In Proc. of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2002), pages 255-262. 
William Gale, Kenneth Church, and David Yarowsky. 
1992a. Using Bilingual Materials to Develop Word 
Sense Disambiguation Methods. In Proc. of the 4th 
International Conference on Theoretical and Meth-
odolgical Issues in Machine Translation(TMI-92), 
pages 101-112. 
William Gale, Kenneth Church, and David Yarowsky. 
1992b. Work on Statistical Methods for Word 
Sense Disambiguation. In Proc. of AAAI Fall Sym-
posium on Probabilistic Approaches to Natural 
Language, pages 54-60. 
William Gale, Kenneth Ward Church, and David 
Yarowsky. 1992c. Estimating Upper and Lower 
Bounds on the Performance of Word Sense Disam-
biguation Programs. In Proc. of the 30th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-1992), pages 249-256. 
Tanja Gaustad. 2001. Statistical Corpus-Based Word 
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words. In Proc. of the 39th ACL/EACL, 
Student Research Workshop, pages 61-66. 
Nancy Ide, Tomaz Erjavec, and Dan Tufi?. 2001. 
Automatic Sense Tagging Using Parallel Corpora. 
In Proc. of the Sixth Natural Language Processing 
Pacific Rim Symposium, pages 83-89. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. 
Sense Discrimination with Parallel Corpora. In 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, pages 54-60. 
Cong Li and Hang Li. 2002. Word Translation Dis-
ambiguation Using Bilingual Bootstrapping. In 
Proc. of the 40th Annual Meeting of the Association 
463
for Computational Linguistics (ACL-2002), pages 
343-351. 
Rada Mihalcea and Dan Moldovan. 2000. An Iterative 
Approach to Word Sense Disambiguation. In Proc. 
of Florida Artificial Intelligence Research Society 
Conference (FLAIRS 2000), pages 219-223. 
Rada F. Mihalcea. 2002. Bootstrapping Large Sense 
Tagged Corpora. In Proc. of the 3rd International 
Conference on Languages Resources and Evalua-
tions (LREC 2002), pages 1407-1411. 
Preslav I. Nakov and Marti A. Hearst. 2003. Cate-
gory-based Pseudowords. In Companion Volume to 
the Proceedings of HLT-NAACL 2003, Short Pa-
pers, pages 67-69. 
Hwee Tou. Ng, Bin Wang, and Yee Seng Chan. 2003. 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In Proc. of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 455-462. 
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 
2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. In 
Proc. of the 43th Annual Meeting of the Association 
for Computational Linguistics (ACL-2005), pages 
395-402. 
Ying Qin and Xiaojie Wang. 2005. A Track-based 
Method on Chinese WSD. In Proc. of Joint Sympo-
sium of Computational Linguistics of China (JSCL-
2005), pages 127-133. 
Hinrich. Schutze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1): 97-
123. 
David Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent Res-
toration in Spanish and French. In Proc. of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics(ACL-1994), pages 88-95. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196. 
 
464
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Convolution Tree Kernel for Semantic Role Labeling
Wanxiang Che
Harbin Inst. of Tech.
Harbin, China, 150001
car@ir.hit.edu.cn
Min Zhang
Inst. for Infocomm Research
Singapore, 119613
mzhang@i2r.a-star.edu.sg
Ting Liu, Sheng Li
Harbin Inst. of Tech.
Harbin, China, 150001
{tliu, ls}@ir.hit.edu.cn
Abstract
A hybrid convolution tree kernel is pro-
posed in this paper to effectively model
syntactic structures for semantic role la-
beling (SRL). The hybrid kernel consists
of two individual convolution kernels: a
Path kernel, which captures predicate-
argument link features, and a Constituent
Structure kernel, which captures the syn-
tactic structure features of arguments.
Evaluation on the datasets of CoNLL-
2005 SRL shared task shows that the
novel hybrid convolution tree kernel out-
performs the previous tree kernels. We
also combine our new hybrid tree ker-
nel based method with the standard rich
flat feature based method. The experi-
mental results show that the combinational
method can get better performance than
each of them individually.
1 Introduction
In the last few years there has been increasing in-
terest in Semantic Role Labeling (SRL). It is cur-
rently a well defined task with a substantial body
of work and comparative evaluation. Given a sen-
tence, the task consists of analyzing the proposi-
tions expressed by some target verbs and some
constituents of the sentence. In particular, for each
target verb (predicate) all the constituents in the
sentence which fill a semantic role (argument) of
the verb have to be recognized.
Figure 1 shows an example of a semantic role
labeling annotation in PropBank (Palmer et al,
2005). The PropBank defines 6 main arguments,
Arg0 is the Agent, Arg1 is Patient, etc. ArgM-
may indicate adjunct arguments, such as Locative,
Temporal.
Many researchers (Gildea and Jurafsky, 2002;
Pradhan et al, 2005a) use feature-based methods

 

 	

 

 



 

 	

Figure 1: Semantic role labeling in a phrase struc-
ture syntactic tree representation
for argument identification and classification in
building SRL systems and participating in eval-
uations, such as Senseval-3 1, CoNLL-2004 and
2005 shared tasks: SRL (Carreras and Ma`rquez,
2004; Carreras and Ma`rquez, 2005), where a
flat feature vector is usually used to represent a
predicate-argument structure. However, it?s hard
for this kind of representation method to explicitly
describe syntactic structure information by a vec-
tor of flat features. As an alternative, convolution
tree kernel methods (Collins and Duffy, 2001)
provide an elegant kernel-based solution to im-
plicitly explore tree structure features by directly
computing the similarity between two trees. In
addition, some machine learning algorithms with
dual form, such as Perceptron and Support Vector
Machines (SVM) (Cristianini and Shawe-Taylor,
2000), which do not need know the exact presen-
tation of objects and only need compute their ker-
nel functions during the process of learning and
prediction. They can be well used as learning al-
gorithms in the kernel-based methods. They are
named kernel machines.
In this paper, we decompose the Moschitti
(2004)?s predicate-argument feature (PAF) kernel
into a Path kernel and a Constituent Structure ker-
1http://www.cs.unt.edu/?rada/senseval/senseval3/
73
nel, and then compose them into a hybrid con-
volution tree kernel. Our hybrid kernel method
using Voted Perceptron kernel machine outper-
forms the PAF kernel in the development sets of
CoNLL-2005 SRL shared task. In addition, the fi-
nal composing kernel between hybrid convolution
tree kernel and standard features? polynomial ker-
nel outperforms each of them individually.
The remainder of the paper is organized as fol-
lows: In Section 2 we review the previous work.
In Section 3 we illustrate the state of the art
feature-based method for SRL. Section 4 discusses
our method. Section 5 shows the experimental re-
sults. We conclude our work in Section 6.
2 Related Work
Automatic semantic role labeling was first intro-
duced by Gildea and Jurafsky (2002). They used
a linear interpolation method and extract features
from a parse tree to identify and classify the con-
stituents in the FrameNet (Baker et al, 1998) with
syntactic parsing results. Here, the basic features
include Phrase Type, Parse Tree Path, Position.
Most of the following works focused on feature
engineering (Xue and Palmer, 2004; Jiang et al,
2005) and machine learning models (Nielsen and
Pradhan, 2004; Pradhan et al, 2005a). Some
other works paid much attention to the robust SRL
(Pradhan et al, 2005b) and post inference (Pun-
yakanok et al, 2004).
These feature-based methods are considered as
the state of the art method for SRL and achieved
much success. However, as we know, the standard
flat features are less effective to model the syntac-
tic structured information. It is sensitive to small
changes of the syntactic structure features. This
can give rise to a data sparseness problem and pre-
vent the learning algorithms from generalizing un-
seen data well.
As an alternative to the standard feature-based
methods, kernel-based methods have been pro-
posed to implicitly explore features in a high-
dimension space by directly calculating the sim-
ilarity between two objects using kernel function.
In particular, the kernel methods could be effective
in reducing the burden of feature engineering for
structured objects in NLP problems. This is be-
cause a kernel can measure the similarity between
two discrete structured objects directly using the
original representation of the objects instead of ex-
plicitly enumerating their features.
Many kernel functions have been proposed in
machine learning community and have been ap-
plied to NLP study. In particular, Haussler (1999)
and Watkins (1999) proposed the best-known con-
volution kernels for a discrete structure. In the
context of convolution kernels, more and more
kernels for restricted syntaxes or specific do-
mains, such as string kernel for text categoriza-
tion (Lodhi et al, 2002), tree kernel for syntactic
parsing (Collins and Duffy, 2001), kernel for re-
lation extraction (Zelenko et al, 2003; Culotta
and Sorensen, 2004) are proposed and explored
in NLP domain. Of special interest here, Mos-
chitti (2004) proposed Predicate Argument Fea-
ture (PAF) kernel under the framework of convo-
lution tree kernel for SRL. In this paper, we fol-
low the same framework and design a novel hybrid
convolution kernel for SRL.
3 Feature-based methods for SRL
Usually feature-based methods refer to the meth-
ods which use the flat features to represent in-
stances. At present, most of the successful SRL
systems use this method. Their features are usu-
ally extended from Gildea and Jurafsky (2002)?s
work, which uses flat information derived from
a parse tree. According to the literature, we
select the Constituent, Predicate, and Predicate-
Constituent related features shown in Table 1.
Feature Description
Constituent related features
Phrase Type syntactic category of the constituent
Head Word head word of the constituent
Last Word last word of the constituent
First Word first word of the constituent
Named Entity named entity type of the constituent?s head word
POS part of speech of the constituent
Previous Word sequence previous word of the constituent
Next Word sequence next word of the constituent
Predicate related features
Predicate predicate lemma
Voice grammatical voice of the predicate, either active or passive
SubCat Sub-category of the predicate?s parent node
Predicate POS part of speech of the predicate
Suffix suffix of the predicate
Predicate-Constituent related features
Path parse tree path from the predicate to the constituent
Position the relative position of the constituent and the predicate, before or after
Path Length the nodes number on the parse tree path
Partial Path some part on the parse tree path
Clause Layers the clause layers from the constituent to the predicate
Table 1: Standard flat features
However, to find relevant features is, as usual,
a complex task. In addition, according to the de-
scription of the standard features, we can see that
the syntactic features, such as Path, Path Length,
bulk large among all features. On the other hand,
the previous researches (Gildea and Palmer, 2002;
Punyakanok et al, 2005) have also recognized the
74
 

 	

 

 



 

 	

Figure 2: Predicate Argument Feature space
necessity of syntactic parsing for semantic role la-
beling. However, the standard flat features cannot
model the syntactic information well. A predicate-
argument pair has two different Path features even
if their paths differ only for a node in the parse
tree. This data sparseness problem prevents the
learning algorithms from generalizing unseen data
well. In order to address this problem, one method
is to list all sub-structures of the parse tree. How-
ever, both space complexity and time complexity
are too high for the algorithm to be realized.
4 Hybrid Convolution Tree Kernels for
SRL
In this section, we introduce the previous ker-
nel method for SRL in Subsection 4.1, discuss
our method in Subsection 4.2 and compare our
method with previous work in Subsection 4.3.
4.1 Convolution Tree Kernels for SRL
Moschitti (2004) proposed to apply convolution
tree kernels (Collins and Duffy, 2001) to SRL.
He selected portions of syntactic parse trees,
which include salient sub-structures of predicate-
arguments, to define convolution kernels for the
task of predicate argument classification. This por-
tions selection method of syntactic parse trees is
named as predicate-arguments feature (PAF) ker-
nel. Figure 2 illustrates the PAF kernel feature
space of the predicate buy and the argument Arg1
in the circled sub-structure.
The kind of convolution tree kernel is similar to
Collins and Duffy (2001)?s tree kernel except the
sub-structure selection strategy. Moschitti (2004)
only selected the relative portion between a predi-
cate and an argument.
Given a tree portion instance defined above, we
design a convolution tree kernel in a way similar
to the parse tree kernel (Collins and Duffy, 2001).
Firstly, a parse tree T can be represented by a vec-
tor of integer counts of each sub-tree type (regard-
less of its ancestors):
?(T ) = (# of sub-trees of type 1, . . . ,
# of sub-trees of type i, . . . ,
# of sub-trees of type n)
This results in a very high dimension since the
number of different subtrees is exponential to the
tree?s size. Thus it is computationally infeasible
to use the feature vector ?(T ) directly. To solve
this problem, we introduce the tree kernel function
which is able to calculate the dot product between
the above high-dimension vectors efficiently. The
kernel function is defined as following:
K(T1, T2) = ??(T1),?(T2)? =
?
i ?i(T1), ?i(T2)=?n1?N1
?
n2?N2
?
i Ii(n1) ? Ii(n2)
where N1 and N2 are the sets of all nodes in
trees T1 and T2, respectively, and Ii(n) is the in-
dicator function whose value is 1 if and only if
there is a sub-tree of type i rooted at node n and
0 otherwise. Collins and Duffy (2001) show that
K(T1, T2) is an instance of convolution kernels
over tree structures, which can be computed in
O(|N1| ? |N2|) by the following recursive defi-
nitions (Let ?(n1, n2) =
?
i Ii(n1) ? Ii(n2)):
(1) if the children of n1 and n2 are different then
?(n1, n2) = 0;
(2) else if their children are the same and they are
leaves, then ?(n1, n2) = ?;
(3) else ?(n1, n2) = ?
?nc(n1)
j=1 (1 +
?(ch(n1, j), ch(n2, j)))
where nc(n1) is the number of the children of
n1, ch(n, j) is the jth child of node n and ?(0 <
? < 1) is the decay factor in order to make the
kernel value less variable with respect to the tree
sizes.
4.2 Hybrid Convolution Tree Kernels
In the PAF kernel, the feature spaces are consid-
ered as an integral portion which includes a pred-
icate and one of its arguments. We note that the
PAF feature consists of two kinds of features: one
is the so-called parse tree Path feature and another
one is the so-called Constituent Structure feature.
These two kinds of feature spaces represent dif-
ferent information. The Path feature describes the
75
 

 	

 

 



 

 	

Figure 3: Path and Constituent Structure feature
spaces
linking information between a predicate and its ar-
guments while the Constituent Structure feature
captures the syntactic structure information of an
argument. We believe that it is more reasonable
to capture the two different kinds of features sepa-
rately since they contribute to SRL in different fea-
ture spaces and it is better to give different weights
to fuse them. Therefore, we propose two convo-
lution kernels to capture the two features, respec-
tively and combine them into one hybrid convolu-
tion kernel for SRL. Figure 3 is an example to il-
lustrate the two feature spaces, where the Path fea-
ture space is circled by solid curves and the Con-
stituent Structure feature spaces is circled by dot-
ted curves. We name them Path kernel and Con-
stituent Structure kernel respectively.
Figure 4 illustrates an example of the distinc-
tion between the PAF kernel and our kernel. In
the PAF kernel, the tree structures are equal when
considering constitutes NP and PRP, as shown in
Figure 4(a). However, the two constituents play
different roles in the sentence and should not be
looked as equal. Figure 4(b) shows the comput-
ing example with our kernel. During computing
the hybrid convolution tree kernel, the NP?PRP
substructure is not computed. Therefore, the two
trees are distinguished correctly.
On the other hand, the constituent structure fea-
ture space reserves the most part in the traditional
PAF feature space usually. Then the Constituent
Structure kernel plays the main role in PAF kernel
computation, as shown in Figure 5. Here, believes
is a predicate and A1 is a long sub-sentence. Ac-
cording to our experimental results in Section 5.2,
we can see that the Constituent Structure kernel
does not perform well. Affected by this, the PAF
kernel cannot perform well, either. However, in
our hybrid method, we can adjust the compromise

 
 	



 
 	


(a) PAF Kernel

 
	


	


 
	



	
 

(b) Hybrid Convolution Tree Kernel
Figure 4: Comparison between PAF and Hybrid
Convolution Tree Kernels
Figure 5: An example of Semantic Role Labeling
of the Path feature and the Constituent Structure
feature by tuning their weights to get an optimal
result.
Having defined two convolution tree kernels,
the Path kernel Kpath and the Constituent Struc-
ture kernel Kcs, we can define a new kernel to
compose and extend the individual kernels. Ac-
cording to Joachims et al (2001), the kernel func-
tion set is closed under linear combination. It
means that the following Khybrid is a valid kernel
if Kpath and Kcs are both valid.
Khybrid = ?Kpath + (1? ?)Kcs (1)
where 0 ? ? ? 1.
According to the definitions of the Path and the
Constituent Structure kernels, each kernel is ex-
plicit. They can be viewed as a matching of fea-
76
tures. Since the features are enumerable on the
given data, the kernels are all valid. Therefore, the
new kernel Khybrid is valid. We name the new ker-
nel hybrid convolution tree kernel, Khybrid.
Since the size of a parse tree is not con-
stant, we normalize K(T1, T2) by dividing it by?K(T1, T1) ?K(T2, T2)
4.3 Comparison with Previous Work
It would be interesting to investigate the differ-
ences between our method and the feature-based
methods. The basic difference between them lies
in the instance representation (parse tree vs. fea-
ture vector) and the similarity calculation mecha-
nism (kernel function vs. dot-product). The main
difference between them is that they belong to dif-
ferent feature spaces. In the kernel methods, we
implicitly represent a parse tree by a vector of in-
teger counts of each sub-tree type. That is to say,
we consider all the sub-tree types and their occur-
ring frequencies. In this way, on the one hand,
the predicate-argument related features, such as
Path, Position, in the flat feature set are embed-
ded in the Path feature space. Additionally, the
Predicate, Predicate POS features are embedded
in the Path feature space, too. The constituent re-
lated features, such as Phrase Type, Head Word,
Last Word, and POS, are embedded in the Con-
stituent Structure feature space. On the other hand,
the other features in the flat feature set, such as
Named Entity, Previous, and Next Word, Voice,
SubCat, Suffix, are not contained in our hybrid
convolution tree kernel. From the syntactic view-
point, the tree representation in our feature space
is more robust than the Parse Tree Path feature in
the flat feature set since the Path feature is sensi-
tive to small changes of the parse trees and it also
does not maintain the hierarchical information of
a parse tree.
It is also worth comparing our method with
the previous kernels. Our method is similar to
the Moschitti (2004)?s predicate-argument feature
(PAF) kernel. However, we differentiate the Path
feature and the Constituent Structure feature in our
hybrid kernel in order to more effectively capture
the syntactic structure information for SRL. In ad-
dition Moschitti (2004) only study the task of ar-
gument classification while in our experiment, we
report the experimental results on both identifica-
tion and classification.
5 Experiments and Discussion
The aim of our experiments is to verify the effec-
tiveness of our hybrid convolution tree kernel and
and its combination with the standard flat features.
5.1 Experimental Setting
5.1.1 Corpus
We use the benchmark corpus provided by
CoNLL-2005 SRL shared task (Carreras and
Ma`rquez, 2005) provided corpus as our training,
development, and test sets. The data consist of
sections of the Wall Street Journal (WSJ) part of
the Penn TreeBank (Marcus et al, 1993), with
information on predicate-argument structures ex-
tracted from the PropBank corpus (Palmer et al,
2005). We followed the standard partition used
in syntactic parsing: sections 02-21 for training,
section 24 for development, and section 23 for
test. In addition, the test set of the shared task
includes three sections of the Brown corpus. Ta-
ble 2 provides counts of sentences, tokens, anno-
tated propositions, and arguments in the four data
sets.
Train Devel tWSJ tBrown
Sentences 39,832 1,346 2,416 426
Tokens 950,028 32,853 56,684 7,159
Propositions 90,750 3,248 5,267 804
Arguments 239,858 8,346 14,077 2,177
Table 2: Counts on the data set
The preprocessing modules used in CONLL-
2005 include an SVM based POS tagger (Gime?nez
and Ma`rquez, 2003), Charniak (2000)?s full syn-
tactic parser, and Chieu and Ng (2003)?s Named
Entity recognizer.
5.1.2 Evaluation
The system is evaluated with respect to
precision, recall, and F?=1 of the predicted ar-
guments. Precision (p) is the proportion of ar-
guments predicted by a system which are cor-
rect. Recall (r) is the proportion of correct ar-
guments which are predicted by a system. F?=1
computes the harmonic mean of precision and
recall, which is the final measure to evaluate the
performances of systems. It is formulated as:
F?=1 = 2pr/(p + r). srl-eval.pl2 is the official
program of the CoNLL-2005 SRL shared task to
evaluate a system performance.
2http://www.lsi.upc.edu/?srlconll/srl-eval.pl
77
5.1.3 SRL Strategies
We use constituents as the labeling units to form
the labeled arguments. In order to speed up the
learning process, we use a four-stage learning ar-
chitecture:
Stage 1: To save time, we use a pruning
stage (Xue and Palmer, 2004) to filter out the
constituents that are clearly not semantic ar-
guments to the predicate.
Stage 2: We then identify the candidates derived
from Stage 1 as either arguments or non-
arguments.
Stage 3: A multi-category classifier is used to
classify the constituents that are labeled as ar-
guments in Stage 2 into one of the argument
classes plus NULL.
Stage 4: A rule-based post-processing stage (Liu
et al, 2005) is used to handle some un-
matched arguments with constituents, such as
AM-MOD, AM-NEG.
5.1.4 Classifier
We use the Voted Perceptron (Freund and
Schapire, 1998) algorithm as the kernel machine.
The performance of the Voted Perceptron is close
to, but not as good as, the performance of SVM on
the same problem, while saving computation time
and programming effort significantly. SVM is too
slow to finish our experiments for tuning parame-
ters.
The Voted Perceptron is a binary classifier. In
order to handle multi-classification problems, we
adopt the one vs. others strategy and select the
one with the largest margin as the final output. The
training parameters are chosen using development
data. After 5 iteration numbers, the best perfor-
mance is achieved. In addition, Moschitti (2004)?s
Tree Kernel Tool is used to compute the tree kernel
function.
5.2 Experimental Results
In order to speed up the training process, in the
following experiments, we ONLY use WSJ sec-
tions 02-05 as training data. The same as Mos-
chitti (2004), we also set the ? = 0.4 in the com-
putation of convolution tree kernels.
In order to study the impact of ? in hybrid con-
volution tree kernel in Eq. 1, we only use the hy-
brid kernel between Kpath and Kcs. The perfor-
mance curve on development set changing with ?
is shown in Figure 6.
Figure 6: The performance curve changing with ?
The performance curve shows that when ? =
0.5, the hybrid convolution tree kernel gets the
best performance. Either the Path kernel (? = 1,
F?=1 = 61.26) or the Constituent Structure kernel
(? = 0, F?=1 = 54.91) cannot perform better than
the hybrid one. It suggests that the two individual
kernels are complementary to each other. In ad-
dition, the Path kernel performs much better than
the Constituent Structure kernel. It indicates that
the predicate-constituent related features are more
effective than the constituent features for SRL.
Table 3 compares the performance comparison
among our Hybrid convolution tree kernel, Mos-
chitti (2004)?s PAF kernel, standard flat features
with Linear kernels, and Poly kernel (d = 2). We
can see that our hybrid convolution tree kernel out-
performs the PAF kernel. It empirically demon-
strates that the weight linear combination in our
hybrid kernel is more effective than PAF kernel for
SRL.
However, our hybrid kernel still performs worse
than the standard feature based system. This is
simple because our kernel only use the syntac-
tic structure information while the feature-based
method use a large number of hand-craft diverse
features, from word, POS, syntax and semantics,
NER, etc. The standard features with polynomial
kernel gets the best performance. The reason is
that the arbitrary binary combination among fea-
tures implicated by the polynomial kernel is useful
to SRL. We believe that combining the two meth-
ods can perform better.
In order to make full use of the syntactic
information and the standard flat features, we
present a composite kernel between hybrid kernel
(Khybrid) and standard features with polynomial
78
Hybrid PAF Linear Poly
Devel 66.01 64.38 68.71 70.25
Table 3: Performance (F?=1) comparison among
various kernels
kernel (Kpoly):
Kcomp = ?Khybrid + (1? ?)Kpoly (2)
where 0 ? ? ? 1.
The performance curve changing with ? in Eq. 2
on development set is shown in Figure 7.
Figure 7: The performance curve changing with ?
We can see that when ? = 0.5, the system
achieves the best performance and F?=1 = 70.78.
It?s statistically significant improvement (?2 test
with p = 0.1) than only using the standard features
with the polynomial kernel (? = 0, F?=1 = 70.25)
and much higher than only using the hybrid con-
volution tree kernel (? = 1, F?=1 = 66.01).
The main reason is that the convolution tree ker-
nel can represent more general syntactic features
than standard flat features, and the standard flat
features include the features that the convolution
tree kernel cannot represent, such as Voice, Sub-
Cat. The two kind features are complementary to
each other.
Finally, we train the composite method using
the above setting (Eq. 2 with when ? = 0.5) on the
entire training set. The final performance is shown
in Table 4.
6 Conclusions and Future Work
In this paper we proposed the hybrid convolu-
tion kernel to model syntactic structure informa-
tion for SRL. Different from the previous convo-
lution tree kernel based methods, our contribution
Precision Recall F?=1
Development 80.71% 68.49% 74.10
Test WSJ 82.46% 70.65% 76.10
Test Brown 73.39% 57.01% 64.17
Test WSJ Precision Recall F?=1
Overall 82.46% 70.65% 76.10
A0 87.97% 82.49% 85.14
A1 80.51% 71.69% 75.84
A2 75.79% 52.16% 61.79
A3 80.85% 43.93% 56.93
A4 83.56% 59.80% 69.71
A5 100.00% 20.00% 33.33
AM-ADV 66.27% 43.87% 52.79
AM-CAU 68.89% 42.47% 52.54
AM-DIR 56.82% 29.41% 38.76
AM-DIS 79.02% 75.31% 77.12
AM-EXT 73.68% 43.75% 54.90
AM-LOC 72.83% 50.96% 59.97
AM-MNR 68.54% 42.44% 52.42
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 49.32% 31.30% 38.30
AM-TMP 82.15% 68.17% 74.51
R-A0 86.28% 87.05% 86.67
R-A1 80.00% 74.36% 77.08
R-A2 100.00% 31.25% 47.62
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 50.00% 100.00% 66.67
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 20.00% 16.67% 18.18
R-AM-TMP 68.75% 63.46% 66.00
V 98.65% 98.65% 98.65
Table 4: Overall results (top) and detailed results
on the WSJ test (bottom).
is that we distinguish between the Path and the
Constituent Structure feature spaces. Evaluation
on the datasets of CoNLL-2005 SRL shared task,
shows that our novel hybrid convolution tree ker-
nel outperforms the PAF kernel method. Although
the hybrid kernel base method is not as good as
the standard rich flat feature based methods, it can
improve the state of the art feature-based methods
by implicating the more generalizing syntactic in-
formation.
Kernel-based methods provide a good frame-
work to use some features which are difficult to
model in the standard flat feature based methods.
For example the semantic similarity of words can
be used in kernels well. We can use general pur-
pose corpus to create clusters of similar words or
use available resources like WordNet. We can also
use the hybrid kernel method into other tasks, such
as relation extraction in the future.
79
Acknowledgements
The authors would like to thank the reviewers for
their helpful comments and Shiqi Zhao, Yanyan
Zhao for their suggestions and useful discussions.
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60435020, 60575042, and 60503072.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the ACL-Coling-1998, pages 86?90.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of CoNLL-2004, pages 89?
97.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, pages
152?164.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-2000.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proceedings of CoNLL-2003, pages 160?163.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proceedings
of NIPS-2001.
Nello Cristianini and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL-2004, pages 423?429.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Computational Learning Theory, pages 209?217.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of ACL-2002, pages 239?246.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and
accurate part-of-speech tagging: The svm approach
revisited. In Proceedings of RANLP-2003.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, July.
Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument
interdependence. In Proceedings of IJCAI-2005.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cat-
egorisation. In Proceedings of ICML-2001, pages
250?257.
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, pages 189?192.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of ACL-2004, pages 335?342.
Rodney D. Nielsen and Sameer Pradhan. 2004. Mix-
ing weak learners in semantic parsing. In Proceed-
ings of EMNLP-2004.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005a. Support vector learning for semantic
argument classification. Machine Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005, pages 581?588.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role
labeling. In Proceedings of IJCAI-2005, pages
1117?1123.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, Jan.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP 2004.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
Proceedings of ACL-08: HLT, pages 780?788,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora
Shiqi Zhao1, Haifeng Wang2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Toshiba (China) Research and Development Center, Beijing, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
Paraphrase patterns are useful in paraphrase
recognition and generation. In this paper, we
present a pivot approach for extracting para-
phrase patterns from bilingual parallel cor-
pora, whereby the English paraphrase patterns
are extracted using the sentences in a for-
eign language as pivots. We propose a log-
linear model to compute the paraphrase likeli-
hood of two patterns and exploit feature func-
tions based on maximum likelihood estima-
tion (MLE) and lexical weighting (LW). Us-
ing the presented method, we extract over
1,000,000 pairs of paraphrase patterns from
2M bilingual sentence pairs, the precision
of which exceeds 67%. The evaluation re-
sults show that: (1) The pivot approach is
effective in extracting paraphrase patterns,
which significantly outperforms the conven-
tional method DIRT. Especially, the log-linear
model with the proposed feature functions
achieves high performance. (2) The coverage
of the extracted paraphrase patterns is high,
which is above 84%. (3) The extracted para-
phrase patterns can be classified into 5 types,
which are useful in various applications.
1 Introduction
Paraphrases are different expressions that convey
the same meaning. Paraphrases are important in
plenty of natural language processing (NLP) ap-
plications, such as question answering (QA) (Lin
and Pantel, 2001; Ravichandran and Hovy, 2002),
machine translation (MT) (Kauchak and Barzilay,
2006; Callison-Burch et al, 2006), multi-document
summarization (McKeown et al, 2002), and natural
language generation (Iordanskaja et al, 1991).
Paraphrase patterns are sets of semantically
equivalent patterns, in which a pattern generally
contains two parts, i.e., the pattern words and slots.
For example, in the pattern ?X solves Y?, ?solves? is
the pattern word, while ?X? and ?Y? are slots. One
can generate a text unit (phrase or sentence) by fill-
ing the pattern slots with specific words. Paraphrase
patterns are useful in both paraphrase recognition
and generation. In paraphrase recognition, if two
text units match a pair of paraphrase patterns and the
corresponding slot-fillers are identical, they can be
identified as paraphrases. In paraphrase generation,
a text unit that matches a pattern P can be rewritten
using the paraphrase patterns of P.
A variety of methods have been proposed on para-
phrase patterns extraction (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Ibrahim et al, 2003;
Pang et al, 2003; Szpektor et al, 2004). However,
these methods have some shortcomings. Especially,
the precisions of the paraphrase patterns extracted
with these methods are relatively low.
In this paper, we extract paraphrase patterns from
bilingual parallel corpora based on a pivot approach.
We assume that if two English patterns are aligned
with the same pattern in another language, they are
likely to be paraphrase patterns. This assumption
is an extension of the one presented in (Bannard
and Callison-Burch, 2005), which was used for de-
riving phrasal paraphrases from bilingual corpora.
Our method involves three steps: (1) corpus prepro-
cessing, including English monolingual dependency
780
parsing and English-foreign language word align-
ment, (2) aligned patterns induction, which produces
English patterns along with the aligned pivot pat-
terns in the foreign language, (3) paraphrase pat-
terns extraction, in which paraphrase patterns are ex-
tracted based on a log-linear model.
Our contributions are as follows. Firstly, we are
the first to use a pivot approach to extract paraphrase
patterns from bilingual corpora, though similar
methods have been used for learning phrasal para-
phrases. Our experiments show that the pivot ap-
proach significantly outperforms conventional meth-
ods. Secondly, we propose a log-linear model for
computing the paraphrase likelihood. Besides, we
use feature functions based on maximum likeli-
hood estimation (MLE) and lexical weighting (LW),
which are effective in extracting paraphrase patterns.
Using the proposed approach, we extract over
1,000,000 pairs of paraphrase patterns from 2M
bilingual sentence pairs, the precision of which is
above 67%. Experimental results show that the pivot
approach evidently outperforms DIRT, a well known
method that extracts paraphrase patterns frommono-
lingual corpora (Lin and Pantel, 2001). Besides, the
log-linear model is more effective than the conven-
tional model presented in (Bannard and Callison-
Burch, 2005). In addition, the coverage of the ex-
tracted paraphrase patterns is high, which is above
84%. Further analysis shows that 5 types of para-
phrase patterns can be extracted with our method,
which can by used in multiple NLP applications.
The rest of this paper is structured as follows.
Section 2 reviews related work on paraphrase pat-
terns extraction. Section 3 presents our method in
detail. We evaluate the proposed method in Section
4, and finally conclude this paper in Section 5.
2 Related Work
Paraphrase patterns have been learned and used in
information extraction (IE) and answer extraction of
QA. For example, Lin and Pantel (2001) proposed a
method (DIRT), in which they obtained paraphrase
patterns from a parsed monolingual corpus based on
an extended distributional hypothesis, where if two
paths in dependency trees tend to occur in similar
contexts it is hypothesized that the meanings of the
paths are similar. The examples of obtained para-
(1) X solves Y
Y is solved by X
X finds a solution to Y
......
(2) born in <ANSWER> , <NAME>
<NAME> was born on <ANSWER> ,
<NAME> ( <ANSWER> -
......
(3) ORGANIZATION decides ?
ORGANIZATION confirms ?
......
Table 1: Examples of paraphrase patterns extracted with
the methods of Lin and Pantel (2001), Ravichandran and
Hovy (2002), and Shinyama et al (2002).
phrase patterns are shown in Table 1 (1).
Based on the same hypothesis as above, some
methods extracted paraphrase patterns from the web.
For instance, Ravichandran and Hovy (2002) de-
fined a question taxonomy for their QA system.
They then used hand-crafted examples of each ques-
tion type as queries to retrieve paraphrase patterns
from the web. For instance, for the question type
?BIRTHDAY?, The paraphrase patterns produced by
their method can be seen in Table 1 (2).
Similar methods have also been used by Ibrahim
et al (2003) and Szpektor et al (2004). The main
disadvantage of the above methods is that the pre-
cisions of the learned paraphrase patterns are rela-
tively low. For instance, the precisions of the para-
phrase patterns reported in (Lin and Pantel, 2001),
(Ibrahim et al, 2003), and (Szpektor et al, 2004)
are lower than 50%. Ravichandran and Hovy (2002)
did not directly evaluate the precision of the para-
phrase patterns extracted using their method. How-
ever, the performance of their method is dependent
on the hand-crafted queries for web mining.
Shinyama et al (2002) presented a method that
extracted paraphrase patterns from multiple news ar-
ticles about the same event. Their method was based
on the assumption that NEs are preserved across
paraphrases. Thus the method acquired paraphrase
patterns from sentence pairs that share comparable
NEs. Some examples can be seen in Table 1 (3).
The disadvantage of this method is that it greatly
relies on the number of NEs in sentences. The preci-
781
start Palestinian suicide bomber blew himself up in SLOT1 on SLOT2
killing SLOT3 other people and injuringwounding SLOT4 end
detroit the*e*
a
?s*e* buildingbuilding in detroit
flattened
groundlevelled
to
blastedleveled*e*wasreduced
razedleveled
to downrubble
into ashes*e*
to *e*
(1)
(2)
Figure 1: Examples of paraphrase patterns extracted by
Barzilay and Lee (2003) and Pang et al (2003).
sion of the extracted patterns may sharply decrease
if the sentences do not contain enough NEs.
Barzilay and Lee (2003) applied multi-sequence
alignment (MSA) to parallel news sentences and in-
duced paraphrase patterns for generating new sen-
tences (Figure 1 (1)). Pang et al (2003) built finite
state automata (FSA) from semantically equivalent
translation sets based on syntactic alignment. The
learned FSAs could be used in paraphrase represen-
tation and generation (Figure 1 (2)). Obviously, it
is difficult for a sentence to match such complicated
patterns, especially if the sentence is not from the
same domain in which the patterns are extracted.
Bannard and Callison-Burch (2005) first ex-
ploited bilingual corpora for phrasal paraphrase ex-
traction. They assumed that if two English phrases
e1 and e2 are aligned with the same phrase c in
another language, these two phrases may be para-
phrases. Specifically, they computed the paraphrase
probability in terms of the translation probabilities:
p(e2|e1) =
?
c
pMLE(c|e1)pMLE(e2|c) (1)
In Equation (1), pMLE(c|e1) and pMLE(e2|c) are
the probabilities of translating e1 to c and c to e2,
which are computed based on MLE:
pMLE(c|e1) =
count(c, e1)
?
c? count(c
?, e1)
(2)
where count(c, e1) is the frequency count that
phrases c and e1 are aligned in the corpus.
pMLE(e2|c) is computed in the same way.
This method proved effective in extracting high
quality phrasal paraphrases. As a result, we extend
it to paraphrase pattern extraction in this paper.
S T E (take)
should
We take
market
into
consideration
take
market
into
consideration
take
into
consideration
P S T E (take)
first
T E
demand
demand
Figure 2: Examples of a subtree and a partial subtree.
3 Proposed Method
3.1 Corpus Preprocessing
In this paper, we use English paraphrase patterns ex-
traction as a case study. An English-Chinese (E-
C) bilingual parallel corpus is employed for train-
ing. The Chinese part of the corpus is used as pivots
to extract English paraphrase patterns. We conduct
word alignment with Giza++ (Och and Ney, 2000) in
both directions and then apply the grow-diag heuris-
tic (Koehn et al, 2005) for symmetrization.
Since the paraphrase patterns are extracted from
dependency trees, we parse the English sentences
in the corpus with MaltParser (Nivre et al, 2007).
Let SE be an English sentence, TE the parse tree
of SE , e a word of SE , we define the subtree and
partial subtree following the definitions in (Ouan-
graoua et al, 2007). In detail, a subtree STE(e)
is a particular connected subgraph of the tree TE ,
which is rooted at e and includes all the descendants
of e. A partial subtree PSTE(e) is a connected sub-
graph of the subtree STE(e), which is rooted at e but
does not necessarily include all the descendants of e.
For instance, for the sentence ?We should first take
market demand into consideration?, STE(take) and
PSTE(take) are shown in Figure 21.
3.2 Aligned Patterns Induction
To induce the aligned patterns, we first induce the
English patterns using the subtrees and partial sub-
trees. Then, we extract the pivot Chinese patterns
aligning to the English patterns.
1Note that, a subtree may contain several partial subtrees. In
this paper, all the possible partial subtrees are considered when
extracting paraphrase patterns.
782
Algorithm 1: Inducing an English pattern
1: Input: words in STE(e) : wiwi+1...wj
2: Input: PE(e) = ?
3: For each wk (i ? k ? j)
4: If wk is in PSTE(e)
5: Append wk to the end of PE(e)
6: Else
7: Append POS(wk) to the end of PE(e)
8: End For
Algorithm 2: Inducing an aligned pivot pattern
1: Input: SC = t1t2...tn
2: Input: PC = ?
3: For each tl (1 ? l ? n)
4: If tl is aligned with wk in SE
5: If wk is a word in PE(e)
6: Append tl to the end of PC
7: If POS(wk) is a slot in PE(e)
8: Append POS(wk) to the end of PC
9: End For
Step-1 Inducing English patterns. In this paper, an
English pattern PE(e) is a string comprising words
and part-of-speech (POS) tags. Our intuition for
inducing an English pattern is that a partial sub-
tree PSTE(e) can be viewed as a unit that conveys
a definite meaning, though the words in PSTE(e)
may not be continuous. For example, PSTE(take)
in Figure 2 contains words ?take ... into consid-
eration?. Therefore, we may extract ?take X into
consideration? as a pattern. In addition, the words
that are in STE(e) but not in PSTE(e) (denoted as
STE(e)/PSTE(e)) are also useful for inducing pat-
terns, since they can constrain the pattern slots. In
the example in Figure 2, the word ?demand? indi-
cates that a noun can be filled in the slot X and the
pattern may have the form ?take NN into considera-
tion?. Based on this intuition, we induce an English
pattern PE(e) as in Algorithm 12.
For the example in Figure 2, the generated pat-
tern PE(take) is ?take NN NN into considera-
tion?. Note that the patterns induced in this way
are quite specific, since the POS of each word in
STE(e)/PSTE(e) forms a slot. Such patterns are
difficult to be matched in applications. We there-
2POS(wk) in Algorithm 1 denotes the POS tag of wk.
N N _1 ?? NN _2 NN_1 ?? N N _2
NN_1NN_2 considered byis NN_1 consider NN_2
Figure 3: Aligned patterns with numbered slots.
fore take an additional step to simplify the patterns.
Let ei and ej be two words in STE(e)/PSTE(e),
whose POS posi and posj are slots in PE(e). If ei
is a descendant of ej in the parse tree, we remove
posi from PE(e). For the example above, the POS
of ?market? is removed, since it is the descendant of
?demand?, whose POS also forms a slot. The sim-
plified pattern is ?take NN into consideration?.
Step-2 Extracting pivot patterns. For each En-
glish pattern PE(e), we extract an aligned Chinese
pivot pattern PC . Let a Chinese sentence SC be the
translation of the English sentence SE , PE(e) a pat-
tern induced from SE , we extract the pivot pattern
PC aligning to PE(e) as in Algorithm 2. Note that
the Chinese patterns are not extracted from parse
trees. They are only sequences of Chinese words
and POSes that are aligned with English patterns.
A pattern may contain two or more slots shar-
ing the same POS. To distinguish them, we assign
a number to each slot in the aligned E-C patterns. In
detail, the slots having identical POS in PC are num-
bered incrementally (i.e., 1,2,3...), while each slot in
PE(e) is assigned the same number as its aligned
slot in PC . The examples of the aligned patterns
with numbered slots are illustrated in Figure 3.
3.3 Paraphrase Patterns Extraction
As mentioned above, if patterns e1 and e2 are
aligned with the same pivot pattern c, e1 and e2 may
be paraphrase patterns. The paraphrase likelihood
can be computed using Equation (1). However, we
find that using only the MLE based probabilities can
suffer from data sparseness. In order to exploit more
and richer information to estimate the paraphrase
likelihood, we propose a log-linear model:
score(e2|e1) =
?
c
exp[
N?
i=1
?ihi(e1, e2, c)] (3)
where hi(e1, e2, c) is a feature function and ?i is the
783
weight. In this paper, 4 feature functions are used in
our log-linear model, which include:
h1(e1, e2, c) = scoreMLE(c|e1)
h2(e1, e2, c) = scoreMLE(e2|c)
h3(e1, e2, c) = scoreLW (c|e1)
h4(e1, e2, c) = scoreLW (e2|c)
Feature functions h1(e1, e2, c) and h2(e1, e2, c)
are based on MLE. scoreMLE(c|e) is computed as:
scoreMLE(c|e) = log pMLE(c|e) (4)
scoreMLE(e|c) is computed in the same way.
h3(e1, e2, c) and h4(e1, e2, c) are based on LW.
LW was originally used to validate the quality of a
phrase translation pair in MT (Koehn et al, 2003). It
checks how well the words of the phrases translate
to each other. This paper uses LW to measure the
quality of aligned patterns. We define scoreLW (c|e)
as the logarithm of the lexical weight3:
scoreLW (c|e) =
1
n
n?
i=1
log(
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(ci|ej)) (5)
where a denotes the word alignment between c and
e. n is the number of words in c. ci and ej are words
of c and e. w(ci|ej) is computed as follows:
w(ci|ej) =
count(ci, ej)
?
c?i
count(c?i, ej)
(6)
where count(ci, ej) is the frequency count of
the aligned word pair (ci, ej) in the corpus.
scoreLW (e|c) is computed in the same manner.
In our experiments, we set a threshold T . If the
score between e1 and e2 based on Equation (3) ex-
ceeds T , e2 is extracted as the paraphrase of e1.
3.4 Parameter Estimation
Five parameters need to be estimated, i.e., ?1, ?2,
?3, ?4 in Equation (3), and the threshold T . To
estimate the parameters, we first construct a devel-
opment set. In detail, we randomly sample 7,086
3The logarithm of the lexical weight is divided by n so as
not to penalize long patterns.
groups of aligned E-C patterns that are obtained as
described in Section 3.2. The English patterns in
each group are all aligned with the same Chinese
pivot pattern. We then extract paraphrase patterns
from the aligned patterns as described in Section 3.3.
In this process, we set ?i = 1 (i = 1, ..., 4) and as-
sign T a minimum value, so as to obtain all possible
paraphrase patterns.
A total of 4,162 pairs of paraphrase patterns have
been extracted and manually labeled as ?1? (correct
paraphrase patterns) or ?0? (incorrect). Here, two
patterns are regarded as paraphrase patterns if they
can generate paraphrase fragments by filling the cor-
responding slots with identical words. We use gra-
dient descent algorithm (Press et al, 1992) to esti-
mate the parameters. For each set of parameters, we
compute the precision P , recall R, and f-measure
F as: P = |set1?set2||set1| , R =
|set1?set2|
|set2| , F =
2PR
P+R ,
where set1 denotes the set of paraphrase patterns ex-
tracted under the current parameters. set2 denotes
the set of manually labeled correct paraphrase pat-
terns. We select the parameters that can maximize
the F-measure on the development set4.
4 Experiments
The E-C parallel corpus in our experiments was con-
structed using several LDC bilingual corpora5. After
filtering sentences that are too long (> 40 words) or
too short (< 5 words), 2,048,009 pairs of parallel
sentences were retained.
We used two constraints in the experiments to im-
prove the efficiency of computation. First, only sub-
trees containing no more than 10 words were used to
induce English patterns. Second, although any POS
tag can form a slot in the induced patterns, we only
focused on three kinds of POSes in the experiments,
i.e., nouns (tags include NN, NNS, NNP, NNPS),
verbs (VB, VBD, VBG, VBN, VBP, VBZ), and ad-
jectives (JJ, JJS, JJR). In addition, we constrained
that a pattern must contain at least one content word
4The parameters are: ?1 = 0.0594137, ?2 = 0.995936,
?3 = ?0.0048954, ?4 = 1.47816, T = ?10.002.
5The corpora include LDC2000T46, LDC2000T47,
LDC2002E18, LDC2002T01, LDC2003E07, LDC2003E14,
LDC2003T17, LDC2004E12, LDC2004T07, LDC2004T08,
LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T04,
LDC2007T02, LDC2007T09.
784
Method #PP (pairs) Precision
LL-Model 1,058,624 67.03%
MLE-Model 1,015,533 60.60%
DIRT top-1 1,179 19.67%
DIRT top-5 5,528 18.73%
Table 2: Comparison of paraphrasing methods.
so as to filter patterns like ?the [NN 1]?.
4.1 Evaluation of the Log-linear Model
As previously mentioned, in the log-linear model of
this paper, we use both MLE based and LW based
feature functions. In this section, we evaluate the
log-linear model (LL-Model) and compare it with
the MLE based model (MLE-Model) presented by
Bannard and Callison-Burch (2005)6.
We extracted paraphrase patterns using two mod-
els, respectively. From the results of each model,
we randomly picked 3,000 pairs of paraphrase pat-
terns to evaluate the precision. The 6,000 pairs of
paraphrase patterns were mixed and presented to the
human judges, so that the judges cannot know by
which model each pair was produced. The sampled
patterns were then manually labeled and the preci-
sion was computed as described in Section 3.4.
The number of the extracted paraphrase patterns
(#PP) and the precision are depicted in the first two
lines of Table 2. We can see that the numbers of
paraphrase patterns extracted using the two mod-
els are comparable. However, the precision of LL-
Model is significantly higher than MLE-Model.
Actually, MLE-Model is a special case of LL-
Model and the enhancement of the precision is
mainly due to the use of LW based features.
It is not surprising, since Bannard and Callison-
Burch (2005) have pointed out that word alignment
error is the major factor that influences the perfor-
mance of the methods learning paraphrases from
bilingual corpora. The LW based features validate
the quality of word alignment and assign low scores
to those aligned E-C pattern pairs with incorrect
alignment. Hence the precision can be enhanced.
6In this experiment, we also estimated a threshold T ? for
MLE-Model using the development set (T ? = ?5.1). The pat-
tern pairs whose score based on Equation (1) exceed T ? were
extracted as paraphrase patterns.
4.2 Comparison with DIRT
It is necessary to compare our method with another
paraphrase patterns extraction method. However, it
is difficult to find methods that are suitable for com-
parison. Some methods only extract paraphrase pat-
terns using news articles on certain topics (Shinyama
et al, 2002; Barzilay and Lee, 2003), while some
others need seeds as initial input (Ravichandran and
Hovy, 2002). In this paper, we compare our method
with DIRT (Lin and Pantel, 2001), which does not
need to specify topics or input seeds.
As mentioned in Section 2, DIRT learns para-
phrase patterns from a parsed monolingual corpus
based on an extended distributional hypothesis. In
our experiment, we implemented DIRT and ex-
tracted paraphrase patterns from the English part of
our bilingual parallel corpus. Our corpus is smaller
than that reported in (Lin and Pantel, 2001). To alle-
viate the data sparseness problem, we only kept pat-
terns appearing more than 10 times in the corpus for
extracting paraphrase patterns. Different from our
method, no threshold was set in DIRT. Instead, the
extracted paraphrase patterns were ranked accord-
ing to their scores. In our experiment, we kept top-5
paraphrase patterns for each target pattern.
From the extracted paraphrase patterns, we sam-
pled 600 groups for evaluation. Each group com-
prises a target pattern and its top-5 paraphrase pat-
terns. The sampled data were manually labeled and
the top-n precision was calculated as
PN
i=1 ni
N?n , where
N is the number of groups and ni is the number of
correct paraphrase patterns in the top-n paraphrase
patterns of the i-th group. The top-1 and top-5 re-
sults are shown in the last two lines of Table 2. Al-
though there are more correct patterns in the top-5
results, the precision drops sequentially from top-1
to top-5 since the denominator of top-5 is 4 times
larger than that of top-1.
Obviously, the number of the extracted para-
phrase patterns is much smaller than that extracted
using our method. Besides, the precision is also
much lower. We believe that there are two reasons.
First, the extended distributional hypothesis is not
strict enough. Patterns sharing similar slot-fillers do
not necessarily have the same meaning. They may
even have the opposite meanings. For example, ?X
worsens Y? and ?X solves Y? were extracted as para-
785
Type Count Example
trivial change 79 (e1) all the members of [NNPS 1] (e2) all members of [NNPS 1]
phrase replacement 267 (e1) [JJ 1] economic losses (e2) [JJ 1] financial losses
phrase reordering 56 (e1) [NN 1] definition (e2) the definition of [NN 1]
structural paraphrase 71 (e1) the admission of [NNP 1] to the wto (e2) the [NNP 1] ?s wto accession
information + or - 27 (e1) [NNS 1] are in fact women (e2) [NNS 1] are women
Table 3: The statistics and examples of each type of paraphrase patterns.
phrase patterns by DIRT. The other reason is that
DIRT can only be effective for patterns appearing
plenty of times in the corpus. In other words, it seri-
ously suffers from data sparseness. We believe that
DIRT can perform better on a larger corpus.
4.3 Pivot Pattern Constraints
As described in Section 3.2, we constrain that the
pattern words of an English pattern e must be ex-
tracted from a partial subtree. However, we do not
have such constraint on the Chinese pivot patterns.
Hence, it is interesting to investigate whether the
performance can be improved if we constrain that
the pattern words of a pivot pattern c must also be
extracted from a partial subtree.
To conduct the evaluation, we parsed the Chinese
sentences of the corpus with a Chinese dependency
parser (Liu et al, 2006). We then induced English
patterns and extracted aligned pivot patterns. For the
aligned patterns (e, c), if c?s pattern words were not
extracted from a partial subtree, the pair was filtered.
After that, we extracted paraphrase patterns, from
which we sampled 3,000 pairs for evaluation.
The results show that 736,161 pairs of paraphrase
patterns were extracted and the precision is 65.77%.
Compared with Table 2, the number of the extracted
paraphrase patterns gets smaller and the precision
also gets lower. The results suggest that the perfor-
mance of the method cannot be improved by con-
straining the extraction of pivot patterns.
4.4 Analysis of the Paraphrase Patterns
We sampled 500 pairs of correct paraphrase pat-
terns extracted using our method and analyzed the
types. We found that there are 5 types of para-
phrase patterns, which include: (1) trivial change,
such as changes of prepositions and articles, etc; (2)
phrase replacement; (3) phrase reordering; (4) struc-
tural paraphrase, which contain both phrase replace-
ments and phrase reordering; (5) adding or reducing
information that does not change the meaning. Some
statistics and examples are shown in Table 3.
The paraphrase patterns are useful in NLP appli-
cations. Firstly, over 50% of the paraphrase patterns
are in the type of phrase replacement, which can
be used in IE pattern reformulation and sentence-
level paraphrase generation. Compared with phrasal
paraphrases, the phrase replacements in patterns are
more accurate due to the constraints of the slots.
The paraphrase patterns in the type of phrase re-
ordering can also be used in IE pattern reformula-
tion and sentence paraphrase generation. Especially,
in sentence paraphrase generation, this type of para-
phrase patterns can reorder the phrases in a sentence,
which can hardly be achieved by the conventional
MT-based generation method (Quirk et al, 2004).
The structural paraphrase patterns have the advan-
tages of both phrase replacement and phrase reorder-
ing. More paraphrase sentences can be generated
using these patterns.
The paraphrase patterns in the type of ?informa-
tion + and -? are useful in sentence compression and
expansion. A sentence matching a long pattern can
be compressed by paraphrasing it using shorter pat-
terns. Similarly, a short sentence can be expanded
by paraphrasing it using longer patterns.
For the 3,000 pairs of test paraphrase patterns, we
also investigate the number and type of the pattern
slots. The results are summarized in Table 4 and 5.
From Table 4, we can see that more than 92%
of the paraphrase patterns contain only one slot,
just like the examples shown in Table 3. In addi-
tion, about 7% of the paraphrase patterns contain
two slots, such as ?give [NN 1] [NN 2]? vs. ?give
[NN 2] to [NN 1]?. This result suggests that our
method tends to extract short paraphrase patterns,
786
Slot No. #PP Percentage Precision
1-slot 2,780 92.67% 66.51%
2-slots 218 7.27% 73.85%
?3-slots 2 <1% 50.00%
Table 4: The statistics of the numbers of pattern slots.
Slot Type #PP Percentage Precision
N-slots 2,376 79.20% 66.71%
V-slots 273 9.10% 70.33%
J-slots 438 14.60% 70.32%
Table 5: The statistics of the type of pattern slots.
which is mainly because the data sparseness prob-
lem is more serious when extracting long patterns.
From Table 5, we can find that near 80% of the
paraphrase patterns contain noun slots, while about
9% and 15% contain verb slots and adjective slots7.
This result implies that nouns are the most typical
variables in paraphrase patterns.
4.5 Evaluation within Context Sentences
In Section 4.1, we have evaluated the precision of
the paraphrase patterns without considering context
information. In this section, we evaluate the para-
phrase patterns within specific context sentences.
The open test set includes 119 English sentences.
We parsed the sentences with MaltParser and in-
duced patterns as described in Section 3.2. For each
pattern e in sentence SE , we searched e?s paraphrase
patterns from the database of the extracted para-
phrase patterns. The result shows that 101 of the
119 sentences contain at least one pattern that can
be paraphrased using the extracted paraphrase pat-
terns, the coverage of which is 84.87%.
Furthermore, since a pattern may have several
paraphrase patterns, we exploited a method to au-
tomatically select the best one in the given context
sentence. In detail, a paraphrase pattern e? of e was
reranked based on a language model (LM):
score(e?|e, SE) =
?scoreLL(e
?|e) + (1 ? ?)scoreLM (e
?|SE) (7)
7Notice that, a pattern may contain more than one type of
slots, thus the sum of the percentages is larger than 1.
Here, scoreLL(e?|e) denotes the score based on
Equation (3). scoreLM (e?|SE) is the LM based
score: scoreLM (e?|SE) = 1n logPLM (S
?
E), where
S?E is the sentence generated by replacing e in SE
with e?. The language model in the experiment was
a tri-gram model trained using the English sentences
in the bilingual corpus. We empirically set ? = 0.7.
The selected best paraphrase patterns in context
sentences were manually labeled. The context infor-
mation was also considered by our judges. The re-
sult shows that the precision of the best paraphrase
patterns is 59.39%. To investigate the contribution
of the LM based score, we ran the experiment again
with ? = 1 (ignoring the LM based score) and found
that the precision is 57.09%. It indicates that the LM
based reranking can improve the precision. How-
ever, the improvement is small. Further analysis
shows that about 70% of the correct paraphrase sub-
stitutes are in the type of phrase replacement.
5 Conclusion
This paper proposes a pivot approach for extracting
paraphrase patterns from bilingual corpora. We use
a log-linear model to compute the paraphrase like-
lihood and exploit feature functions based on MLE
and LW. Experimental results show that the pivot ap-
proach is effective, which extracts over 1,000,000
pairs of paraphrase patterns from 2M bilingual sen-
tence pairs. The precision and coverage of the ex-
tracted paraphrase patterns exceed 67% and 84%,
respectively. In addition, the log-linear model with
the proposed feature functions significantly outper-
forms the conventional models. Analysis shows that
5 types of paraphrase patterns are extracted with our
method, which are useful in various applications.
In the future we wish to exploit more feature func-
tions in the log-linear model. In addition, we will try
to make better use of the context information when
replacing paraphrase patterns in context sentences.
Acknowledgments
This research was supported by National Nat-
ural Science Foundation of China (60503072,
60575042). We thank Lin Zhao, Xiaohang Qu, and
Zhenghua Li for their help in the experiments.
787
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolingual
Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in a
Meaning-Text Generation Model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann (Eds.):
Natural Language Generation in Artificial Intelligence
and Computational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li. 2006.
Dependency Parsing Based on Dynamic Local Opti-
mization. In Proceedings of CoNLL-X, pages 211-215.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Natural Language Engineering 13(2): 95-
135.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
A??da Ouangraoua, Pascal Ferraro, Laurent Tichit, and
Serge Dulucq. 2007. Local Similarity between Quo-
tiented Ordered Trees. In Journal of Discrete Algo-
rithms 5(1): 23-35.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP,
pages 41-48.
788
Proceedings of ACL-08: HLT, pages 843?851,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Entity-Mention Model for Coreference Resolution
with Inductive Logic Programming
Xiaofeng Yang1 Jian Su1 Jun Lang2
Chew Lim Tan3 Ting Liu2 Sheng Li2
1Institute for Infocomm Research
{xiaofengy,sujian}@i2r.a-star.edu.sg
2Harbin Institute of Technology
{bill lang,tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
3National University of Singapore,
tancl@comp.nus.edu.sg
Abstract
The traditional mention-pair model for coref-
erence resolution cannot capture information
beyond mention pairs for both learning and
testing. To deal with this problem, we present
an expressive entity-mention model that per-
forms coreference resolution at an entity level.
The model adopts the Inductive Logic Pro-
gramming (ILP) algorithm, which provides a
relational way to organize different knowledge
of entities and mentions. The solution can
explicitly express relations between an entity
and the contained mentions, and automatically
learn first-order rules important for corefer-
ence decision. The evaluation on the ACE data
set shows that the ILP based entity-mention
model is effective for the coreference resolu-
tion task.
1 Introduction
Coreference resolution is the process of linking mul-
tiple mentions that refer to the same entity. Most
of previous work adopts the mention-pair model,
which recasts coreference resolution to a binary
classification problem of determining whether or not
two mentions in a document are co-referring (e.g.
Aone and Bennett (1995); McCarthy and Lehnert
(1995); Soon et al (2001); Ng and Cardie (2002)).
Although having achieved reasonable success, the
mention-pair model has a limitation that informa-
tion beyond mention pairs is ignored for training and
testing. As an individual mention usually lacks ad-
equate descriptive information of the referred entity,
it is often difficult to judge whether or not two men-
tions are talking about the same entity simply from
the pair alone.
An alternative learning model that can overcome
this problem performs coreference resolution based
on entity-mention pairs (Luo et al, 2004; Yang et
al., 2004b). Compared with the traditional mention-
pair counterpart, the entity-mention model aims to
make coreference decision at an entity level. Classi-
fication is done to determine whether a mention is a
referent of a partially found entity. A mention to be
resolved (called active mention henceforth) is linked
to an appropriate entity chain (if any), based on clas-
sification results.
One problem that arises with the entity-mention
model is how to represent the knowledge related to
an entity. In a document, an entity may have more
than one mention. It is impractical to enumerate all
the mentions in an entity and record their informa-
tion in a single feature vector, as it would make the
feature space too large. Even worse, the number of
mentions in an entity is not fixed, which would re-
sult in variant-length feature vectors and make trou-
ble for normal machine learning algorithms. A solu-
tion seen in previous work (Luo et al, 2004; Culotta
et al, 2007) is to design a set of first-order features
summarizing the information of the mentions in an
entity, for example, ?whether the entity has any men-
tion that is a name alias of the active mention?? or
?whether most of the mentions in the entity have the
same head word as the active mention?? These fea-
tures, nevertheless, are designed in an ad-hoc man-
ner and lack the capability of describing each indi-
vidual mention in an entity.
In this paper, we present a more expressive entity-
843
mention model for coreference resolution. The
model employs Inductive Logic Programming (ILP)
to represent the relational knowledge of an active
mention, an entity, and the mentions in the entity. On
top of this, a set of first-order rules is automatically
learned, which can capture the information of each
individual mention in an entity, as well as the global
information of the entity, to make coreference deci-
sion. Hence, our model has a more powerful repre-
sentation capability than the traditional mention-pair
or entity-mention model. And our experimental re-
sults on the ACE data set shows the model is effec-
tive for coreference resolution.
2 Related Work
There are plenty of learning-based coreference reso-
lution systems that employ the mention-pair model.
A typical one of them is presented by Soon et al
(2001). In the system, a training or testing instance
is formed for two mentions in question, with a fea-
ture vector describing their properties and relation-
ships. At a testing time, an active mention is checked
against all its preceding mentions, and is linked with
the closest one that is classified as positive. The
work is further enhanced by Ng and Cardie (2002)
by expanding the feature set and adopting a ?best-
first? linking strategy.
Recent years have seen some work on the entity-
mention model. Luo et al (2004) propose a system
that performs coreference resolution by doing search
in a large space of entities. They train a classifier that
can determine the likelihood that an active mention
should belong to an entity. The entity-level features
are calculated with an ?Any-X? strategy: an entity-
mention pair would be assigned a feature X, if any
mention in the entity has the feature X with the ac-
tive mention.
Culotta et al (2007) present a system which uses
an online learning approach to train a classifier to
judge whether two entities are coreferential or not.
The features describing the relationships between
two entities are obtained based on the information
of every possible pair of mentions from the two en-
tities. Different from (Luo et al, 2004), the entity-
level features are computed using a ?Most-X? strat-
egy, that is, two given entities would have a feature
X, if most of the mention pairs from the two entities
have the feature X.
Yang et al (2004b) suggest an entity-based coref-
erence resolution system. The model adopted in the
system is similar to the mention-pair model, except
that the entity information (e.g., the global num-
ber/gender agreement) is considered as additional
features of a mention in the entity.
McCallum and Wellner (2003) propose several
graphical models for coreference analysis. These
models aim to overcome the limitation that pair-
wise coreference decisions are made independently
of each other. The simplest model conditions coref-
erence on mention pairs, but enforces dependency
by calculating the distance of a node to a partition
(i.e., the probability that an active mention belongs
to an entity) based on the sum of its distances to all
the nodes in the partition (i.e., the sum of the prob-
ability of the active mention co-referring with the
mentions in the entity).
Inductive Logic Programming (ILP) has been ap-
plied to some natural language processing tasks, in-
cluding parsing (Mooney, 1997), POS disambigua-
tion (Cussens, 1996), lexicon construction (Claveau
et al, 2003), WSD (Specia et al, 2007), and so on.
However, to our knowledge, our work is the first ef-
fort to adopt this technique for the coreference reso-
lution task.
3 Modelling Coreference Resolution
Suppose we have a document containing n mentions
{mj : 1 < j < n}, in which mj is the jth mention
occurring in the document. Let ei be the ith entity in
the document. We define
P (L|ei,mj), (1)
the probability that a mention belongs to an entity.
Here the random variable L takes a binary value and
is 1 if mj is a mention of ei.
By assuming that mentions occurring after mj
have no influence on the decision of linking mj to
an entity, we can approximate (1) as:
P (L|ei,mj)
? P (L|{mk ? ei, 1 ? k ? j ? 1},mj) (2)
? max
mk?ei,1?k?j?1
P (L|mk,mj) (3)
(3) further assumes that an entity-mention score
can be computed by using the maximum mention-
844
[ Microsoft Corp. ]11 announced [ [ its ]12 new CEO ]23
[ yesterday ]34. [ The company ]15 said [ he ]26 will . . .
Table 1: A sample text
pair score. Both (2) and (1) can be approximated
with a machine learning method, leading to the tra-
ditional mention-pair model and the entity-mention
model for coreference resolution, respectively.
The two models will be described in the next sub-
sections, with the sample text in Table 1 used for
demonstration. In the table, a mention m is high-
lighted as [ m ]eidmid, where mid and eid are the IDs
for the mention and the entity to which it belongs,
respectively. Three entity chains can be found in the
text, that is,
e1 : Microsoft Corp. - its - The company
e2 : its new CEO - he
e3 : yesterday
3.1 Mention-Pair Model
As a baseline, we first describe a learning framework
with the mention-pair model as adopted in the work
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{mk, mj}, in which mj is
an active mention and mk is a preceding mention.
An instance is associated with a vector of features,
which is used to describe the properties of the two
mentions as well as their relationships. Table 2 sum-
marizes the features used in our study.
For training, given each encountered anaphoric
mention mj in a document, one single positive train-
ing instance is created for mj and its closest an-
tecedent. And a group of negative training in-
stances is created for every intervening mentions
between mj and the antecedent. Consider the ex-
ample text in Table 1, for the pronoun ?he?, three
instances are generated: i(?The company?,?he?),
i(?yesterday?,?he?), and i(?its new CEO?,?he?).
Among them, the first two are labelled as negative
while the last one is labelled as positive.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm. During resolution, an input document is
processed from the first mention to the last. For each
encountered mention mj , a test instance is formed
for each preceding mention, mk. This instance is
presented to the classifier to determine the corefer-
ence relationship. mj is linked with the mention that
is classified as positive (if any) with the highest con-
fidence value.
3.2 Entity-Mention Model
The mention-based solution has a limitation that in-
formation beyond a mention pair cannot be captured.
As an individual mention usually lacks complete de-
scription about the referred entity, the coreference
relationship between two mentions may be not clear,
which would affect classifier learning. Consider
a document with three coreferential mentions ?Mr.
Powell?, ?he?, and ?Powell?, appearing in that or-
der. The positive training instance i(?he?, ?Powell?)
is not informative, as the pronoun ?he? itself dis-
closes nothing but the gender. However, if the whole
entity is considered instead of only one mention, we
can know that ?he? refers to a male person named
?Powell?. And consequently, the coreference rela-
tionships between the mentions would become more
obvious.
The mention-pair model would also cause errors
at a testing time. Suppose we have three mentions
?Mr. Powell?, ?Powell?, and ?she? in a document.
The model tends to link ?she? with ?Powell? be-
cause of their proximity. This error can be avoided,
if we know ?Powell? belongs to the entity starting
with ?Mr. Powell?, and therefore refers to a male
person and cannot co-refer with ?she?.
The entity-mention model based on Eq. (2) per-
forms coreference resolution at an entity-level. For
simplicity, the framework considered for the entity-
mention model adopts similar training and testing
procedures as for the mention-pair model. Specif-
ically, a training or testing instance has the form of
i{ei, mj}, in which mj is an active mention and ei
is a partial entity found before mj . During train-
ing, given each anaphoric mention mj , one single
positive training instance is created for the entity to
which mj belongs. And a group of negative train-
ing instances is created for every partial entity whose
last mention occurs between mj and the closest an-
tecedent of mj .
See the sample in Table 1 again. For the pronoun
?he?, the following three instances are generated for
845
Features describing an active mention, mj
defNP mj 1 if mj is a definite description; else 0
indefNP mj 1 if mj is an indefinite NP; else 0
nameNP mj 1 if mj is a named-entity; else 0
pron mj 1 if mj is a pronoun; else 0
bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0
Features describing a previous mention, mk
defNP mk 1 if mk is a definite description; else 0
indefNP mk 1 if mk is an indefinite NP; else 0
nameNP mk 1 if mk is a named-entity; else 0
pron mk 1 if mk is a pronoun; else 0
bareNP mk 1 if mk is a bare NP; else 0
subject mk 1 if mk is an NP in a subject position; else 0
Features describing the relationships between mk and mj
sentDist sentence distance between two mentions
numAgree 1 if two mentions match in the number agreement; else 0
genderAgree 1 if two mentions match in the gender agreement; else 0
parallelStruct 1 if two mentions have an identical collocation pattern; else 0
semAgree 1 if two mentions have the same semantic category; else 0
nameAlias 1 if two mentions are an alias of the other; else 0
apposition 1 if two mentions are in an appositive structure; else 0
predicative 1 if two mentions are in a predicative structure; else 0
strMatch Head 1 if two mentions have the same head string; else 0
strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0
strMatch Contain 1 if the string of mj is fully contained in that of mk ; else 0
Table 2: Feature set for coreference resolution
entity e1, e3 and e2:
i({?Microsoft Corp.?, ?its?, ?The company?},?he?),
i({?yesterday?},?he?),
i({?its new CEO?},?he?).
Among them, the first two are labelled as negative,
while the last one is positive.
The resolution is done using a greedy clustering
strategy. Given a test document, the mentions are
processed one by one. For each encountered men-
tion mj , a test instance is formed for each partial en-
tity found so far, ei. This instance is presented to the
classifier. mj is appended to the entity that is classi-
fied as positive (if any) with the highest confidence
value. If no positive entity exists, the active mention
is deemed as non-anaphoric and forms a new entity.
The process continues until the last mention of the
document is reached.
One potential problem with the entity-mention
model is how to represent the entity-level knowl-
edge. As an entity may contain more than one candi-
date and the number is not fixed, it is impractical to
enumerate all the mentions in an entity and put their
properties into a single feature vector. As a base-
line, we follow the solution proposed in (Luo et al,
2004) to design a set of first-order features. The fea-
tures are similar to those for the mention-pair model
as shown in Table 2, but their values are calculated
at an entity level. Specifically, the lexical and gram-
matical features are computed by testing any men-
tion1 in the entity against the active mention, for ex-
1Linguistically, pronouns usually have the most direct coref-
ample, the feature nameAlias is assigned value 1 if
at least one mention in the entity is a name alias of
the active mention. The distance feature (i.e., sent-
Dist) is the minimum distance between the mentions
in the entity and the active mention.
The above entity-level features are designed in an
ad-hoc way. They cannot capture the detailed infor-
mation of each individual mention in an entity. In
the next section, we will present a more expressive
entity-mention model by using ILP.
4 Entity-mention Model with ILP
4.1 Motivation
The entity-mention model based on Eq. (2) re-
quires relational knowledge that involves informa-
tion of an active mention (mj), an entity (ei), and
the mentions in the entity ({mk ? ei}). How-
ever, normal machine learning algorithms work on
attribute-value vectors, which only allows the repre-
sentation of atomic proposition. To learn from rela-
tional knowledge, we need an algorithm that can ex-
press first-order logic. This requirement motivates
our use of Inductive Logic Programming (ILP), a
learning algorithm capable of inferring logic pro-
grams. The relational nature of ILP makes it pos-
sible to explicitly represent relations between an en-
tity and its mentions, and thus provides a powerful
expressiveness for the coreference resolution task.
erence relationship with antecedents in a local discourse.
Hence, if an active mention is a pronoun, we only consider the
mentions in its previous two sentences for feature computation.
846
ILP uses logic programming as a uniform repre-
sentation for examples, background knowledge and
hypotheses. Given a set of positive and negative ex-
ample E = E+ ? E?, and a set of background
knowledge K of the domain, ILP tries to induce a
set of hypotheses h that covers most of E+ with no
E?, i.e., K ? h |= E+ and K ? h 6|= E?.
In our study, we choose ALEPH2, an ILP imple-
mentation by Srinivasan (2000) that has been proven
well suited to deal with a large amount of data in
multiple domains. For its routine use, ALEPH fol-
lows a simple procedure to induce rules. It first se-
lects an example and builds the most specific clause
that entertains the example. Next, it tries to search
for a clause more general than the bottom one. The
best clause is added to the current theory and all the
examples made redundant are removed. The proce-
dure repeats until all examples are processed.
4.2 Apply ILP to coreference resolution
Given a document, we encode a mention or a par-
tial entity with a unique constant. Specifically, mj
represents the jth mention (e.g., m6 for the pronoun
?he?). ei j represents the partial entity i before the
jth mention. For example, e1 6 denotes the part of
e1 before m6, i.e., {?Microsoft Corp.?, ?its?, ?the
company?}, while e1 5 denotes the part of e1 be-
fore m5 (?The company?), i.e., {?Microsoft Corp.?,
?its?}.
Training instances are created as described in Sec-
tion 3.2 for the entity-mention model. Each instance
is recorded with a predicate link(ei j , mj), where mj
is an active mention and ei j is a partial entity. For
example, the three training instances formed by the
pronoun ?he? are represented as follows:
link(e1 6,m6).
link(e3 6,m6).
link(e2 6,m6).
The first two predicates are put into E?, while the
last one is put to E+.
The background knowledge for an instance
link(ei j , mj) is also represented with predicates,
which are divided into the following types:
1. Predicates describing the information related to
ei j and mj . The properties of mj are pre-
2http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/aleph toc.html
sented with predicates like f (m, v), where f
corresponds to a feature in the first part of Ta-
ble 2 (removing the suffix mj), and v is its
value. For example, the pronoun ?he? can be
described by the following predicates:
defNP(m6, 0). indefNP(m6, 0).
nameNP(m6, 0). pron(m6, 1).
bareNP(m6, 0).
The predicates for the relationships between
ei j and mj take a form of f (e, m, v). In our
study, we consider the number agreement (ent-
NumAgree) and the gender agreement (entGen-
derAgree) between ei j and mj . v is 1 if all
of the mentions in ei j have consistent num-
ber/gender agreement with mj , e.g,
entNumAgree(e1 6,m6, 1).
2. Predicates describing the belonging relations
between ei j and its mentions. A predicate
has mention(e, m) is used for each mention in
e 3. For example, the partial entity e1 6 has
three mentions, m1, m2 and m5, which can be
described as follows:
has mention(e1 6,m1).
has mention(e1 6,m2).
has mention(e1 6,m5).
3. Predicates describing the information related to
mj and each mention mk in ei j . The predi-
cates for the properties of mk correspond to the
features in the second part of Table 2 (removing
the suffix mk), while the predicates for the re-
lationships between mj and mk correspond to
the features in the third part of Table 2. For ex-
ample, given the two mentions m1 (?Microsoft
Corp.) and m6 (?he), the following predicates
can be applied:
nameNP(m1, 1).
pron(m1, 0).
. . .
nameAlias(m1,m6, 0).
sentDist(m1,m6, 1).
. . .
the last two predicates represent that m1 and
3If an active mention mj is a pronoun, only the previous
mentions in two sentences apart are recorded by has mention,
while the farther ones are ignored as they have less impact on
the resolution of the pronoun.
847
m6 are not name alias, and are one sentence
apart.
By using the three types of predicates, the dif-
ferent knowledge related to entities and mentions
are integrated. The predicate has mention acts as
a bridge connecting the entity-mention knowledge
and the mention-pair knowledge. As a result, when
evaluating the coreference relationship between an
active mention and an entity, we can make use of
the ?global? information about the entity, as well as
the ?local? information of each individual mention
in the entity.
From the training instances and the associated
background knowledge, a set of hypotheses can be
automatically learned by ILP. Each hypothesis is
output as a rule that may look like:
link(A,B):-
predi1, predi2, . . . , has mention(A,C), . . . , prediN.
which corresponds to first-order logic
?A,B(predi1 ? predi2 ? . . .?
?C(has mention(A,C) ? . . . ? prediN)
? link(A,B))
Consider an example rule produced in our system:
link(A,B) :-
has mention(A,C), numAgree(B,C,1),
strMatch Head(B,C,1), bareNP(C,1).
Here, variables A and B stand for an entity and an
active mention in question. The first-order logic is
implemented by using non-instantiated arguments C
in the predicate has mention. This rule states that a
mention B should belong to an entity A, if there ex-
ists a mention C in A such that C is a bare noun
phrase with the same head string as B, and matches
in number with B. In this way, the detailed informa-
tion of each individual mention in an entity can be
captured for resolution.
A rule is applicable to an instance link(e, m), if
the background knowledge for the instance can be
described by the predicates in the body of the rule.
Each rule is associated with a score, which is the
accuracy that the rule can produce for the training
instances.
The learned rules are applied to resolution in a
similar way as described in Section 3.2. Given an
active mention m and a partial entity e, a test in-
stance link(e, m) is formed and tested against every
rule in the rule set. The confidence that m should
Train Test
#entity #mention #entity #mention
NWire 1678 9861 411 2304
NPaper 1528 10277 365 2290
BNews 1695 8986 468 2493
Table 3: statistics of entities (length > 1) and contained
mentions
belong to e is the maximal score of the applicable
rules. An active mention is linked to the entity with
the highest confidence value (above 0.5), if any.
5 Experiments and Results
5.1 Experimental Setup
In our study, we did evaluation on the ACE-2003
corpus, which contains two data sets, training and
devtest, used for training and testing respectively.
Each of these sets is further divided into three do-
mains: newswire (NWire), newspaper (NPaper), and
broadcast news (BNews). The number of entities
with more than one mention, as well as the number
of the contained mentions, is summarized in Table 3.
For both training and resolution, an input raw
document was processed by a pipeline of NLP
modules including Tokenizer, Part-of-Speech tag-
ger, NP Chunker and Named-Entity (NE) Recog-
nizer. Trained and tested on Penn WSJ TreeBank,
the POS tagger could obtain an accuracy of 97% and
the NP chunker could produce an F-measure above
94% (Zhou and Su, 2000). Evaluated for the MUC-
6 and MUC-7 Named-Entity task, the NER mod-
ule (Zhou and Su, 2002) could provide an F-measure
of 96.6% (MUC-6) and 94.1%(MUC-7). For evalu-
ation, Vilain et al (1995)?s scoring algorithm was
adopted to compute recall and precision rates.
By default, the ALEPH algorithm only generates
rules that have 100% accuracy for the training data.
And each rule contains at most three predicates. To
accommodate for coreference resolution, we loos-
ened the restrictions to allow rules that have above
50% accuracy and contain up to ten predicates. De-
fault parameters were applied for all the other set-
tings in ALEPH as well as other learning algorithms
used in the experiments.
5.2 Results and Discussions
Table 4 lists the performance of different corefer-
ence resolution systems. For comparison, we first
848
NWire NPaper BNews
R P F R P F R P F
C4.5
- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9
- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5
- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4
ILP
- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1
- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5
Table 4: Results of different systems for coreference resolution
examined the C4.5 algorithm4 which is widely used
for the coreference resolution task. The first line of
the table shows the baseline system that employs the
traditional mention-pair model (MP) as described in
Section 3.1. From the table, our baseline system
achieves a recall of around 66%-68% and a preci-
sion of around 50%-60%. The overall F-measure
for NWire, NPaper and BNews is 60.4%, 57.9% and
62.9% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure ranging in 50-60% for
the same data set. As our system relies only on sim-
ple and knowledge-poor features, the achieved F-
measure is around 2-4% lower than the state-of-the-
art systems do, like (Ng, 2007) and (Yang and Su,
2007) which utilized sophisticated semantic or real-
world knowledge. Since ILP has a strong capability
in knowledge management, our system could be fur-
ther improved if such helpful knowledge is incorpo-
rated, which will be explored in our future work.
The second line of Table 4 is for the system
that employs the entity-mention model (EM) with
?Any-X? based entity features, as described in Sec-
tion 3.2. We can find that the EM model does not
show superiority over the baseline MP model. It
achieves a higher precision (up to 2.6%), but a lower
recall (2.9%), than MP. As a result, we only see
?0.4% difference between the F-measure. The re-
sults are consistent with the reports by Luo et al
(2004) that the entity-mention model with the ?Any-
X? first-order features performs worse than the nor-
mal mention-pair model. In our study, we also tested
the ?Most-X? strategy for the first-order features as
in (Culotta et al, 2007), but got similar results with-
out much difference (?0.5% F-measure) in perfor-
4http://www.rulequest.com/see5-info.html
mance. Besides, as with our entity-mention predi-
cates described in Section 4.2, we also tried the ?All-
X? strategy for the entity-level agreement features,
that is, whether all mentions in a partial entity agree
in number and gender with an active mention. How-
ever, we found this bring no improvement against
the ?Any-X? strategy.
As described, given an active mention mj , the MP
model only considers the mentions between mj and
its closest antecedent. By contrast, the EM model
considers not only these mentions, but also their an-
tecedents in the same entity link. We were interested
in examining what if the MP model utilizes all the
mentions in an entity as the EM model does. As
shown in the third line of Table 4, such a solution
damages the performance; while the recall is at the
same level, the precision drops significantly (up to
12%) and as a result, the F-measure is even lower
than the original MP model. This should be because
a mention does not necessarily have direct corefer-
ence relationships with all of its antecedents. As the
MP model treats each mention-pair as an indepen-
dent instance, including all the antecedents would
produce many less-confident positive instances, and
thus adversely affect training.
The second block of the table summarizes the per-
formance of the systems with ILP. We were first con-
cerned with how well ILP works for the mention-
pair model, compared with the normally used algo-
rithm C4.5. From the results shown in the fourth
line of Table 4, ILP exhibits the same capability in
the resolution; it tends to produce a slightly higher
precision but a lower recall than C4.5 does. Overall,
it performs better in F-measure (1.8%) for Npaper,
while slightly worse (<1%) for Nwire and BNews.
These results demonstrate that ILP could be used as
849
link(A,B) :-
bareNP(B,0), has mention(A,C), appositive(C,1).
link(A,B) :-
has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).
link(A,B) :-
nameNP(B,0), has mention(A,C), predicative(C,1).
link(A,B) :-
has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).
link(A,B) :-
nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).
link(A,B) :-
pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),
subject(D, 1).
...
Figure 1: Examples of rules produced by ILP (entity-
mention model)
a good classifier learner for the mention-pair model.
The fifth line of Table 4 is for the ILP based entity-
mention model (described in Section 4.2). We can
observe that the model leads to a better performance
than all the other models. Compared with the sys-
tem with the MP model (under ILP), the EM version
is able to achieve a higher precision (up to 4.6% for
BNews). Although the recall drops slightly (up to
1.8% for BNews), the gain in the precision could
compensate it well; it beats the MP model in the
overall F-measure for all three domains (2.3% for
Nwire, 0.4% for Npaper, 1.4% for BNews). Es-
pecially, the improvement in NWire and BNews is
statistically significant under a 2-tailed t test (p <
0.05). Compared with the EM model with the man-
ually designed first-order feature (the second line),
the ILP-based EM solution also yields better perfor-
mance in precision (with a slightly lower recall) as
well as the overall F-measure (1.0% - 1.8%).
The improvement in precision against the
mention-pair model confirms that the global infor-
mation beyond a single mention pair, when being
considered for training, can make coreference rela-
tions clearer and help classifier learning. The bet-
ter performance against the EM model with heuristi-
cally designed features also suggests that ILP is able
to learn effective first-order rules for the coreference
resolution task.
In Figure 1, we illustrate part of the rules pro-
duced by ILP for the entity-mention model (NWire
domain), which shows how the relational knowledge
of entities and mentions is represented for decision
making. An interesting finding, as shown in the last
rule of the table, is that multiple non-instantiated ar-
guments (i.e. C and D) could possibly appear in
the same rule. According to this rule, a pronominal
mention should be linked with a partial entity which
contains a named-entity and contains an indefinite
NP in a subject position. This supports the claims
in (Yang et al, 2004a) that coreferential informa-
tion is an important factor to evaluate a candidate an-
tecedent in pronoun resolution. Such complex logic
makes it possible to capture information of multi-
ple mentions in an entity at the same time, which is
difficult to implemented in the mention-pair model
and the ordinary entity-mention model with heuris-
tic first-order features.
6 Conclusions
This paper presented an expressive entity-mention
model for coreference resolution by using Inductive
Logic Programming. In contrast to the traditional
mention-pair model, our model can capture infor-
mation beyond single mention pairs for both training
and testing. The relational nature of ILP enables our
model to explicitly express the relations between an
entity and its mentions, and to automatically learn
the first-order rules effective for the coreference res-
olution task. The evaluation on ACE data set shows
that the ILP based entity-model performs better than
the mention-pair model (with up to 2.3% increase in
F-measure), and also beats the entity-mention model
with heuristically designed first-order features.
Our current work focuses on the learning model
that calculates the probability of a mention be-
longing to an entity. For simplicity, we just use a
greedy clustering strategy for resolution, that is, a
mention is linked to the current best partial entity.
In our future work, we would like to investigate
more sophisticated clustering methods that would
lead to global optimization, e.g., by keeping a large
search space (Luo et al, 2004) or using integer
programming (Denis and Baldridge, 2007).
Acknowledgements This research is supported
by a Specific Targeted Research Project (STREP)
of the European Union?s 6th Framework Programme
within IST call 4, Bootstrapping Of Ontologies and
Terminologies STrategic REsearch Project (BOOT-
Strep).
850
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 122?129.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
4:493?525.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In Proceedings of the Annual Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL), pages 81?88.
J. Cussens. 1996. Part-of-speech disambiguation using
ilp. Technical report, Oxford University Computing
Laboratory.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North America Chapter of the Association for
Computational Linguistics (NAACL), pages 236?243.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
135?142.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proceedings of IJCAI-
03 Workshop on Information Integration on the Web,
pages 79?86.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of
the 14th International Conference on Artificial Intel-
ligences (IJCAI), pages 1050?1055.
R. Mooney. 1997. Inductive logic programming for nat-
ural language processing. In Proceedings of the sixth
International Inductive Logic Programming Work-
shop, pages 3?24.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 104?111,
Philadelphia.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
157?164.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 536?543.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learn-
ing expressive models for words sense disambiguation.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
41?48.
A. Srinivasan. 2000. The aleph manual. Technical re-
port, Oxford University Computing Laboratory.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann Publishers.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automati-
cally discovered patterns. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 528?535.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improv-
ing pronoun resolution by incorporating coreferential
information of candidates. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 127?134, Barcelona.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An
NP-cluster approach to coreference resolution. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 219?225, Geneva.
G. Zhou and J. Su. 2000. Error-driven HMM-based
chunk tagger with context-dependent lexicon. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 71?79, Hong Kong.
G. Zhou and J. Su. 2002. Named Entity recognition us-
ing a HMM-based chunk tagger. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 473?480, Philadel-
phia.
851
Proceedings of ACL-08: HLT, pages 1021?1029,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining Multiple Resources to Improve SMT-based Paraphrasing Model?
Shiqi Zhao1, Cheng Niu2, Ming Zhou2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Microsoft Research Asia, Beijing, China
{chengniu,mingzhou}@microsoft.com
Abstract
This paper proposes a novel method that ex-
ploits multiple resources to improve statisti-
cal machine translation (SMT) based para-
phrasing. In detail, a phrasal paraphrase ta-
ble and a feature function are derived from
each resource, which are then combined in a
log-linear SMT model for sentence-level para-
phrase generation. Experimental results show
that the SMT-based paraphrasing model can
be enhanced using multiple resources. The
phrase-level and sentence-level precision of
the generated paraphrases are above 60% and
55%, respectively. In addition, the contribu-
tion of each resource is evaluated, which indi-
cates that all the exploited resources are useful
for generating paraphrases of high quality.
1 Introduction
Paraphrases are alternative ways of conveying the
same meaning. Paraphrases are important in many
natural language processing (NLP) applications,
such as machine translation (MT), question an-
swering (QA), information extraction (IE), multi-
document summarization (MDS), and natural lan-
guage generation (NLG).
This paper addresses the problem of sentence-
level paraphrase generation, which aims at generat-
ing paraphrases for input sentences. An example of
sentence-level paraphrases can be seen below:
S1: The table was set up in the carriage shed.
S2: The table was laid under the cart-shed.
?This research was finished while the first author worked as
an intern in Microsoft Research Asia.
Paraphrase generation can be viewed as monolin-
gual machine translation (Quirk et al, 2004), which
typically includes a translation model and a lan-
guage model. The translation model can be trained
using monolingual parallel corpora. However, ac-
quiring such corpora is not easy. Hence, data sparse-
ness is a key problem for the SMT-based paraphras-
ing. On the other hand, various methods have been
presented to extract phrasal paraphrases from dif-
ferent resources, which include thesauri, monolin-
gual corpora, bilingual corpora, and the web. How-
ever, little work has been focused on using the ex-
tracted phrasal paraphrases in sentence-level para-
phrase generation.
In this paper, we exploit multiple resources to
improve the SMT-based paraphrase generation. In
detail, six kinds of resources are utilized, includ-
ing: (1) an automatically constructed thesaurus, (2)
a monolingual parallel corpus from novels, (3) a
monolingual comparable corpus from news articles,
(4) a bilingual phrase table, (5) word definitions
from Encarta dictionary, and (6) a corpus of simi-
lar user queries. Among the resources, (1), (2), (3),
and (4) have been investigated by other researchers,
while (5) and (6) are first used in this paper. From
those resources, six phrasal paraphrase tables are ex-
tracted, which are then used in a log-linear SMT-
based paraphrasing model.
Both phrase-level and sentence-level evaluations
were carried out in the experiments. In the former
one, phrase substitutes occurring in the paraphrase
sentences were evaluated. While in the latter one,
the acceptability of the paraphrase sentences was
evaluated. Experimental results show that: (1) The
1021
SMT-based paraphrasing is enhanced using multiple
resources. The phrase-level and sentence-level pre-
cision of the generated paraphrases exceed 60% and
55%, respectively. (2) Although the contributions of
the resources differ a lot, all the resources are useful.
(3) The performance of the method varies greatly on
different test sets and it performs best on the test set
of news sentences, which are from the same source
as most of the training data.
The rest of the paper is organized as follows: Sec-
tion 2 reviews related work. Section 3 introduces the
log-linear model for paraphrase generation. Section
4 describes the phrasal paraphrase extraction from
different resources. Section 5 presents the parameter
estimation method. Section 6 shows the experiments
and results. Section 7 draws the conclusion.
2 Related Work
Paraphrases have been used in many NLP applica-
tions. In MT, Callison-Burch et al (2006) utilized
paraphrases of unseen source phrases to alleviate
data sparseness. Kauchak and Barzilay (2006) used
paraphrases of the reference translations to improve
automatic MT evaluation. In QA, Lin and Pantel
(2001) and Ravichandran and Hovy (2002) para-
phrased the answer patterns to enhance the recall of
answer extraction. In IE, Shinyama et al (2002)
automatically learned paraphrases of IE patterns to
reduce the cost of creating IE patterns by hand. In
MDS, McKeown et al (2002) identified paraphrase
sentences across documents before generating sum-
marizations. In NLG, Iordanskaja et al (1991) used
paraphrases to generate more varied and fluent texts.
Previous work has examined various resources for
acquiring paraphrases, including thesauri, monolin-
gual corpora, bilingual corpora, and the web. The-
sauri, such as WordNet, have been widely used
for extracting paraphrases. Some researchers ex-
tract synonyms as paraphrases (Kauchak and Barzi-
lay, 2006), while some others use looser defini-
tions, such as hypernyms and holonyms (Barzilay
and Elhadad, 1997). Besides, the automatically
constructed thesauri can also be used. Lin (1998)
constructed a thesaurus by automatically clustering
words based on context similarity.
Barzilay andMcKeown (2001) used monolingual
parallel corpora for identifying paraphrases. They
exploited a corpus of multiple English translations
of the same source text written in a foreign language,
from which phrases in aligned sentences that appear
in similar contexts were extracted as paraphrases. In
addition, Finch et al (2005) applied MT evalua-
tion methods (BLEU, NIST,WER and PER) to build
classifiers for paraphrase identification.
Monolingual parallel corpora are difficult to find,
especially in non-literature domains. Alternatively,
some researchers utilized monolingual compara-
ble corpora for paraphrase extraction. Different
news articles reporting on the same event are com-
monly used as monolingual comparable corpora,
from which both paraphrase patterns and phrasal
paraphrases can be derived (Shinyama et al, 2002;
Barzilay and Lee, 2003; Quirk et al, 2004).
Lin and Pantel (2001) learned paraphrases from
a parsed monolingual corpus based on an extended
distributional hypothesis, where if two paths in de-
pendency trees tend to occur in similar contexts it is
hypothesized that the meanings of the paths are simi-
lar. The monolingual corpus used in their work is not
necessarily parallel or comparable. Thus it is easy
to obtain. However, since this resource is used to
extract paraphrase patterns other than phrasal para-
phrases, we do not use it in this paper.
Bannard and Callison-Burch (2005) learned
phrasal paraphrases using bilingual parallel cor-
pora. The basic idea is that if two phrases are
aligned to the same translation in a foreign language,
they may be paraphrases. This method has been
demonstrated effective in extracting large volume of
phrasal paraphrases. Besides, Wu and Zhou (2003)
exploited bilingual corpora and translation informa-
tion in learning synonymous collocations.
In addition, some researchers extracted para-
phrases from the web. For example, Ravichandran
and Hovy (2002) retrieved paraphrase patterns from
the web using hand-crafted queries. Pasca and Di-
enes (2005) extracted sentence fragments occurring
in identical contexts as paraphrases from one bil-
lion web documents. Since web mining is rather
time consuming, we do not exploit the web to ex-
tract paraphrases in this paper.
So far, two kinds of methods have been pro-
posed for sentence-level paraphrase generation, i.e.,
the pattern-based and SMT-based methods. Auto-
matically learned patterns have been used in para-
1022
phrase generation. For example, Barzilay and Lee
(2003) applied multiple-sequence alignment (MSA)
to parallel news sentences and induced paraphras-
ing patterns for generating new sentences. Pang et
al. (2003) built finite state automata (FSA) from se-
mantically equivalent translation sets based on syn-
tactic alignment and used the FSAs in paraphrase
generation. The pattern-based methods can generate
complex paraphrases that usually involve syntactic
variation. However, the methods were demonstrated
to be of limited generality (Quirk et al, 2004).
Quirk et al (2004) first recast paraphrase gener-
ation as monolingual SMT. They generated para-
phrases using a SMT system trained on parallel sen-
tences extracted from clustered news articles. In
addition, Madnani et al (2007) also generated
sentence-level paraphrases based on a SMT model.
The advantage of the SMT-based method is that
it achieves better coverage than the pattern-based
method. The main difference between their methods
and ours is that they only used bilingual parallel cor-
pora as paraphrase resource, while we exploit and
combine multiple resources.
3 SMT-based Paraphrasing Model
The SMT-based paraphrasing model used by Quirk
et al (2004) was the noisy channel model of Brown
et al (1993), which identified the optimal paraphrase
T ? of a sentence S by finding:
T ? = argmax
T
{P (T |S)}
= argmax
T
{P (S|T )P (T )} (1)
In contrast, we adopt a log-linear model (Och
and Ney, 2002) in this work, since multiple para-
phrase tables can be easily combined in the log-
linear model. Specifically, feature functions are de-
rived from each paraphrase resource and then com-
bined with the language model feature1:
T ? = argmax
T
{
N?
i=1
?TM ihTM i(T, S)+
?LMhLM (T, S)} (2)
where N is the number of paraphrase tables.
hTM i(T, S) is the feature function based on the i-
th paraphrase table PTi. hLM (T, S) is the language
1The reordering model is not considered in our model.
model feature. ?TM i and ?LM are the weights of
the feature functions. hTM i(T, S) is defined as:
hTM i(T, S) = log
Ki?
k=1
Scorei(Tk, Sk) (3)
where Ki is the number of phrase substitutes from
S to T based on PTi. Tk in T and Sk in S are
phrasal paraphrases in PTi. Scorei(Tk, Sk) is the
paraphrase likelihood according to PTi2. A 5-gram
language model is used, therefore:
hLM (T, S) = log
J?
j=1
p(tj |tj?4, ..., tj?1) (4)
where J is the length of T , tj is the j-th word of T .
4 Exploiting Multiple Resources
This section describes the extraction of phrasal
paraphrases using various resources. Similar to
Pharaoh (Koehn, 2004), our decoder3 uses top 20
paraphrase options for each input phrase in the de-
fault setting. Therefore, we keep at most 20 para-
phrases for a phrase when extracting phrasal para-
phrases using each resource.
1 - Thesaurus: The thesaurus4 used in this work
was automatically constructed by Lin (1998). The
similarity of two words e1 and e2 was calculated
through the surrounding context words that have de-
pendency relations with the investigated words:
Sim(e1, e2)
=
P
(r,e)?Tr(e1)?Tr(e2)
(I(e1, r, e) + I(e2, r, e))
P
(r,e)?Tr(e1)
I(e1, r, e) +
P
(r,e)?Tr(e2)
I(e2, r, e)
(5)
where Tr(ei) denotes the set of words that have de-
pendency relation r with word ei. I(ei, r, e) is the
mutual information between ei, r and e.
For each word, we keep 20 most similar words as
paraphrases. In this way, we extract 502,305 pairs of
paraphrases. The paraphrasing score Score1(p1, p2)
used in Equation (3) is defined as the similarity
based on Equation (5).
2If none of the phrase substitutes from S to T is from PTi
(i.e., Ki = 0), we cannot compute hTM i(T, S) as in Equation
(3). In this case, we assign hTM i(T, S) a minimum value.
3The decoder used here is a re-implementation of Pharaoh.
4http://www.cs.ualberta.ca/ lindek/downloads.htm.
1023
2 - Monolingual parallel corpus: Following Barzi-
lay and McKeown (2001), we exploit a corpus
of multiple English translations of foreign nov-
els, which contains 25,804 parallel sentence pairs.
We find that most paraphrases extracted using the
method of Barzilay and McKeown (2001) are quite
short. Thus we employ a new approach for para-
phrase extraction. Specifically, we parse the sen-
tences with CollinsParser5 and extract the chunks
from the parsing results. Let S1 and S2 be a pair
of parallel sentences, p1 and p2 two chunks from S1
and S2, we compute the similarity of p1 and p2 as:
Sim(p1, p2) = ?Simcontent(p1, p2)+
(1 ? ?)Simcontext(p1, p2) (6)
where, Simcontent(p1, p2) is the content similarity,
which is the word overlapping rate of p1 and p2.
Simcontext(p1, p2) is the context similarity, which is
the word overlapping rate of the contexts of p1 and
p26. If the similarity of p1 and p2 exceeds a thresh-
old Th1, they are identified as paraphrases. We ex-
tract 18,698 pairs of phrasal paraphrases from this
resource. The paraphrasing score Score2(p1, p2) is
defined as the similarity in Equation (6). For the
paraphrases occurring more than once, we use their
maximum similarity as the paraphrasing score.
3 - Monolingual comparable corpus: Similar to
the methods in (Shinyama et al, 2002; Barzilay and
Lee, 2003), we construct a corpus of comparable
documents from a large corpus D of news articles.
The corpusD contains 612,549 news articles. Given
articles d1 and d2 from D, if their publication date
interval is less than 2 days and their similarity7 ex-
ceeds a threshold Th2, they are recognized as com-
parable documents. In this way, a corpus containing
5,672,864 pairs of comparable documents is con-
structed. From the comparable corpus, parallel sen-
tences are extracted. Let s1 and s2 be two sentences
from comparable documents d1 and d2, if their sim-
ilarity based on word overlapping rate is above a
threshold Th3, s1 and s2 are identified as parallel
sentences. In this way, 872,330 parallel sentence
pairs are extracted.
5http://people.csail.mit.edu/mcollins/code.html
6The context of a chunk is made up of 6 words around the
chunk, 3 to the left and 3 to the right.
7The similarity of two documents is computed using the vec-
tor space model and the word weights are based on tf?idf.
We run Giza++ (Och and Ney, 2000) on the paral-
lel sentences and then extract aligned phrases as de-
scribed in (Koehn, 2004). The generated paraphrase
table is pruned by keeping the top 20 paraphrases for
each phrase. After pruning, 100,621 pairs of para-
phrases are extracted. Given phrase p1 and its para-
phrase p2, we compute Score3(p1, p2) by relative
frequency (Koehn et al, 2003):
Score3(p1, p2) = p(p2|p1) =
count(p2, p1)
P
p? count(p
?, p1)
(7)
People may wonder why we do not use the same
method on the monolingual parallel and comparable
corpora. This is mainly because the volumes of the
two corpora differ a lot. In detail, the monolingual
parallel corpus is fairly small, thus automatical word
alignment tool like Giza++ may not work well on
it. In contrast, the monolingual comparable corpus
is quite large, hence we cannot conduct the time-
consuming syntactic parsing on it as we do on the
monolingual parallel corpus.
4 - Bilingual phrase table: We first construct
a bilingual phrase table that contains 15,352,469
phrase pairs from an English-Chinese parallel cor-
pus. We extract paraphrases from the bilingual
phrase table and compute the paraphrasing score
of phrases p1 and p2 as in (Bannard and Callison-
Burch, 2005):
Score4(p1, p2) =
?
f
p(f |p1)p(p2|f) (8)
where f denotes a Chinese translation of both p1 and
p2. p(f |p1) and p(p2|f) are the translation probabil-
ities provided by the bilingual phrase table. For each
phrase, the top 20 paraphrases are kept according
to the score in Equation (8). As a result, 3,177,600
pairs of phrasal paraphrases are extracted.
5 - Encarta dictionary definitions: Words and their
definitions can be regarded as paraphrases. Here
are some examples from Encarta dictionary: ?hur-
ricane: severe storm?, ?clever: intelligent?, ?travel:
go on journey?. In this work, we extract words? def-
initions from Encarta dictionary web pages8. If a
word has more than one definition, all of them are
extracted. Note that the words and definitions in the
8http://encarta.msn.com/encnet/features/dictionary/diction-
aryhome.aspx
1024
dictionary are lemmatized, but words in sentences
are usually inflected. Hence, we expand the word
- definition pairs by providing the inflected forms.
Here we use an inflection list and some rules for in-
flection. After expanding, 159,456 pairs of phrasal
paraphrases are extracted. Let < p1, p2 > be a word
- definition pair, the paraphrasing score is defined
according to the rank of p2 in all of p1?s definitions:
Score5(p1, p2) = ?
i?1 (9)
where ? is a constant (we empirically set ? = 0.9)
and i is the rank of p2 in p1?s definitions.
6 - Similar user queries: Clusters of similar user
queries have been used for query expansion and sug-
gestion (Gao et al, 2007). Since most queries are at
the phrase level, we exploit similar user queries as
phrasal paraphrases. In our experiment, we use the
corpus of clustered similar MSN queries constructed
by Gao et al (2007). The similarity of two queries
p1 and p2 is computed as:
Sim(p1, p2) = ?Simcontent(p1, p2)+
(1 ? ?)Simclick?through(p1, p2) (10)
where Simcontent(p1, p2) is the content similarity,
which is computed as the word overlapping rate of
p1 and p2. Simclick?through(p1, p2) is the click
through similarity, which is the overlapping rate of
the user clicked documents for p1 and p2. For each
query q, we keep the top 20 similar queries, whose
similarity with q exceeds a threshold Th4. As a re-
sult, 395,284 pairs of paraphrases are extracted. The
score Score6(p1, p2) is defined as the similarity in
Equation (10).
7 - Self-paraphrase: In addition to the six resources
introduced above, a special paraphrase table is used,
which is made up of pairs of identical words. The
reason why this paraphrase table is necessary is that
a word should be allowed to keep unchanged in para-
phrasing. This is a difference between paraphras-
ing and MT, since all words should be translated in
MT. In our experiments, all the words that occur in
the six paraphrase table extracted above are gath-
ered to form the self-paraphrase table, which con-
tains 110,403 word pairs. The score Score7(p1, p2)
is set 1 for each identical word pair.
5 Parameter Estimation
The weights of the feature functions, namely ?TM i
(i = 1, 2, ..., 7) and ?LM , need estimation9. In MT,
the max-BLEU algorithm is widely used to estimate
parameters. However, it may not work in our case,
since it is more difficult to create a reference set of
paraphrases.
We propose a new technique to estimate parame-
ters in paraphrasing. The assumption is that, since a
SMT-based paraphrase is generated through phrase
substitution, we can measure the quality of a gener-
ated paraphrase by measuring its phrase substitutes.
Generally, the paraphrases containing more correct
phrase substitutes are judged as better paraphrases10.
We therefore present the phrase substitution error
rate (PSER) to score a generated paraphrase T :
PSER(T ) = ?PS0(T )?/?PS(T )? (11)
where PS(T ) is the set of phrase substitutes in T
and PS0(T ) is the set of incorrect substitutes.
In practice, we keep top n paraphrases for each
sentence S. Thus we calculate the PSER for each
source sentence S as:
PSER(S) = ?
n[
i=1
PS0(Ti)?/?
n[
i=1
PS(Ti)? (12)
where Ti is the i-th generated paraphrase of S.
Suppose there are N sentences in the develop-
ment set, the overall PSER is computed as:
PSER =
NX
j=1
PSER(Sj) (13)
where Sj is the j-th sentence in the development set.
Our development set contains 75 sentences (de-
scribed in detail in Section 6). For each sentence,
all possible phrase substitutes are extracted from the
six paraphrase tables above. The extracted phrase
substitutes are then manually labeled as ?correct? or
?incorrect?. A phrase substitute is considered as cor-
rect only if the two phrases have the same meaning
in the given sentence and the sentence generated by
9Note that, we also use some other parameters when extract-
ing phrasal paraphrases from different resources, such as the
thresholds Th1, Th2, Th3, Th4, as well as ? and ? in Equa-
tion (6) and (10). These parameters are estimated using differ-
ent development sets from the investigated resources. We do
not describe the estimation of them due to space limitation.
10Paraphrasing a word to itself (based on the 7-th paraphrase
table above) is not regarded as a substitute.
1025
substituting the source phrase with the target phrase
remains grammatical. In decoding, the phrase sub-
stitutes are printed out and then the PSER is com-
puted based on the labeled data.
Using each set of parameters, we generate para-
phrases for the sentences in the development set
based on Equation (2). PSER is then computed as
in Equation (13). We use the gradient descent algo-
rithm (Press et al, 1992) to minimize PSER on the
development set and get the optimal parameters.
6 Experiments
To evaluate the performance of the method on dif-
ferent types of test data, we used three kinds of sen-
tences for testing, which were randomly extracted
from Google news, free online novels, and forums,
respectively. For each type, 50 sentences were ex-
tracted as test data and another 25 were extracted as
development data. For each test sentence, top 10 of
the generated paraphrases were kept for evaluation.
6.1 Phrase-level Evaluation
The phrase-level evaluation was carried out to in-
vestigate the contributions of the paraphrase tables.
For each test sentence, all possible phrase substitutes
were first extracted from the paraphrase tables and
manually labeled as ?correct? or ?incorrect?. Here,
the criterion for identifying paraphrases is the same
as that described in Section 5. Then, in the stage
of decoding, the phrase substitutes were printed out
and evaluated using the labeled data.
Two metrics were used here. The first is the
number of distinct correct substitutes (#DCS). Ob-
viously, the more distinct correct phrase substitutes
a paraphrase table can provide, the more valuable it
is. The second is the accuracy of the phrase substi-
tutes, which is computed as:
Accuracy =
#correct phrase substitutes
#all phrase substitutes
(14)
To evaluate the PTs learned from different re-
sources, we first used each PT (from 1 to 6) along
with PT-7 in decoding. The results are shown in Ta-
ble 1. It can be seen that PT-4 is the most useful, as
it provides the most correct substitutes and the ac-
curacy is the highest. We believe that it is because
PT-4 is much larger than the other PTs. Compared
with PT-4, the accuracies of the other PTs are fairly
PT combination #DCS Accuracy
1+7 178 14.61%
2+7 94 25.06%
3+7 202 18.35%
4+7 553 56.93%
5+7 231 20.48%
6+7 21 14.42%
Table 1: Contributions of the paraphrase tables.
PT-1: from the thesaurus; PT-2: from the monolingual
parallel corpus; PT-3: from the monolingual comparable
corpus; PT-4: from the bilingual parallel corpus; PT-5:
from the Encarta dictionary definitions; PT-6: from the
similar MSN user queries; PT-7: self-paraphrases.
low. This is because those PTs are smaller, thus they
can provide fewer correct phrase substitutes. As a
result, plenty of incorrect substitutes were included
in the top 10 generated paraphrases.
PT-6 provides the least correct phrase substitutes
and the accuracy is the lowest. There are several
reasons. First, many phrases in PT-6 are not real
phrases but only sets of keywords (e.g., ?lottery re-
sults ny?), which may not appear in sentences. Sec-
ond, many words in this table have spelling mis-
takes (e.g., ?widows vista?). Third, some phrase
pairs in PT-6 are not paraphrases but only ?related
queries? (e.g., ?back tattoo? vs. ?butterfly tattoo?).
Fourth, many phrases of PT-6 contain proper names
or out-of-vocabulary words, which are difficult to be
matched. The accuracy based on PT-1 is also quite
low. We found that it is mainly because the phrase
pairs in PT-1 are automatically clustered, many of
which are merely ?similar? words rather than syn-
onyms (e.g., ?borrow? vs. ?buy?).
Next, we try to find out whether it is necessary to
combine all PTs. Thus we conducted several runs,
each of which added the most useful PT from the
left ones. The results are shown in Table 2. We can
see that all the PTs are useful, as each PT provides
some new correct phrase substitutes and the accu-
racy increases when adding each PT except PT-1.
Since the PTs are extracted from different re-
sources, they have different contributions. Here we
only discuss the contributions of PT-5 and PT-6,
which are first used in paraphrasing in this paper.
PT-5 is useful for paraphrasing uncommon concepts
since it can ?explain? concepts with their definitions.
1026
PT combination #DCS Accuracy
4+7 553 56.93%
4+5+7 581 58.97%
4+5+3+7 638 59.42%
4+5+3+2+7 649 60.15%
4+5+3+2+1+7 699 60.14%
4+5+3+2+1+6+7 711 60.16%
Table 2: Performances of different combinations of para-
phrase tables.
For instance, in the following test sentence S1, the
word ?amnesia? is a relatively uncommon word, es-
pecially for the people using English as the second
language. Based on PT-5, S1 can be paraphrased
into T1, which is much easier to understand.
S1: I was suffering from amnesia.
T1: I was suffering from memory loss.
The disadvantage of PT-5 is that substituting
words with the definitions sometimes leads to gram-
matical errors. For instance, substituting ?heat
shield? in the sentence S2 with ?protective barrier
against heat? keeps the meaning unchanged. How-
ever, the paraphrased sentence T2 is ungrammatical.
S2: The U.S. space agency has been cautious
about heat shield damage.
T2: The U.S. space administration has been
cautious about protective barrier against heat
damage.
As previously mentioned, PT-6 is less effective
compared with the other PTs. However, it is use-
ful for paraphrasing some special phrases, such as
digital products, computer software, etc, since these
phrases often appear in user queries. For example,
S3 below can be paraphrased into T3 using PT-6.
S3: I have a canon powershot S230 that uses
CF memory cards.
T3: I have a canon digital camera S230 that
uses CF memory cards.
The phrase ?canon powershot? can hardly be
paraphrased using the other PTs. It suggests that PT-
6 is useful for paraphrasing new emerging concepts
and expressions.
Test sentences Top-1 Top-5 Top-10
All 150 55.33% 45.20% 39.28%
50 from news 70.00% 62.00% 57.03%
50 from novel 56.00% 46.00% 37.42%
50 from forum 40.00% 27.60% 23.34%
Table 3: Top-n accuracy on different test sentences.
6.2 Sentence-level Evaluation
In this section, we evaluated the sentence-level qual-
ity of the generated paraphrases11. In detail, each
generated paraphrase was manually labeled as ?ac-
ceptable? or ?unacceptable?. Here, the criterion for
counting a sentence T as an acceptable paraphrase of
sentence S is that T is understandable and its mean-
ing is not evidently changed compared with S. For
example, for the sentence S4, T4 is an acceptable
paraphrase generated using our method.
S4: The strain on US forces of fighting in Iraq
and Afghanistan was exposed yesterday when
the Pentagon published a report showing that
the number of suicides among US troops is at
its highest level since the 1991 Gulf war.
T4: The pressure on US troops of fighting in
Iraq and Afghanistan was revealed yesterday
when the Pentagon released a report showing
that the amount of suicides among US forces
is at its top since the 1991 Gulf conflict.
We carried out sentence-level evaluation using the
top-1, top-5, and top-10 results of each test sentence.
The accuracy of the top-n results was computed as:
Accuracytop?n =
?N
i=1 ni
N ? n
(15)
where N is the number of test sentences. ni is the
number of acceptable paraphrases in the top-n para-
phrases of the i-th test sentence.
We computed the accuracy on the whole test set
(150 sentences) as well as on the three subsets, i.e.,
the 50 news sentences, 50 novel sentences, and 50
forum sentences. The results are shown in table 3.
It can be seen that the accuracy varies greatly on
different test sets. The accuracy on the news sen-
tences is the highest, while that on the forum sen-
tences is the lowest. There are several reasons. First,
11The evaluation was based on the paraphrasing results using
the combination of all seven PTs.
1027
the largest PT used in the experiments is extracted
using the bilingual parallel data, which are mostly
from news documents. Thus, the test set of news
sentences is more similar to the training data.
Second, the news sentences are formal while the
novel and forum sentences are less formal. Espe-
cially, some of the forum sentences contain spelling
mistakes and grammar mistakes.
Third, we find in the results that, most phrases
paraphrased in the novel and forum sentences are
commonly used phrases or words, such as ?food?,
?good?, ?find?, etc. These phrases are more dif-
ficult to paraphrase than the less common phrases,
since they usually have much more paraphrases in
the PTs. Therefore, it is more difficult to choose the
right paraphrase from all the candidates when con-
ducting sentence-level paraphrase generation.
Fourth, the forum sentences contain plenty of
words such as ?board (means computer board)?,
?site (means web site)?, ?mouse (means computer
mouse)?, etc. These words are polysemous and have
particular meanings in the domains of computer sci-
ence and internet. Our method performs poor when
paraphrasing these words since the domain of a con-
text sentence is hard to identify.
After observing the results, we find that there are
three types of errors: (1) syntactic errors: the gener-
ated sentences are ungrammatical. About 32% of the
unacceptable results are due to syntactic errors. (2)
semantic errors: the generated sentences are incom-
prehensible. Nearly 60% of the unacceptable para-
phrases have semantic errors. (3) non-paraphrase:
the generated sentences are well formed and com-
prehensible but are not paraphrases of the input sen-
tences. 8% of the unacceptable results are of this
type. We believe that many of the errors above can
be avoided by applying syntactic constraints and by
making better use of context information in decod-
ing, which is left as our future work.
7 Conclusion
This paper proposes a method that improves the
SMT-based sentence-level paraphrase generation
using phrasal paraphrases automatically extracted
from different resources. Our contribution is that
we combine multiple resources in the framework of
SMT for paraphrase generation, in which the dic-
tionary definitions and similar user queries are first
used as phrasal paraphrases. In addition, we analyze
and compare the contributions of different resources.
Experimental results indicate that although the
contributions of the exploited resources differ a lot,
they are all useful to sentence-level paraphrase gen-
eration. Especially, the dictionary definitions and
similar user queries are effective for paraphrasing
some certain types of phrases.
In the future work, we will try to use syntactic
and context constraints in paraphrase generation to
enhance the acceptability of the paraphrases. In ad-
dition, we will extract paraphrase patterns that con-
tain more structural variation and try to combine the
SMT-based and pattern-based systems for sentence-
level paraphrase generation.
Acknowledgments
We would like to thank Mu Li for providing us with
the SMT decoder. We are also grateful to Dongdong
Zhang for his help in the experiments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Michael Elhadad. 1997. Using Lex-
ical Chains for Text Summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10-17.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL, pages 50-57.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. In Computational Linguistics 19(2): 263-311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using Machine Translation Evalua-
tion Techniques to Determine Sentence-level Semantic
Equivalence. In Proceedings of IWP, pages 17-24.
1028
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
Lingual Query Suggestion Using Query Logs of Dif-
ferent Languages. In Proceedings of SIGIR, pages
463-470.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in
a Meaning-Text Generation Model. In Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Version
1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING/ACL,
pages 768-774.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using Paraphrases for Parame-
ter Tuning in Statistical Machine Translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 120-127.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proceedings of ACL,
pages 295-302.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
Marius Pasca and Pe?ter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, pages 119-130.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Hua Wu and Ming Zhou. 2003. Synonymous Collo-
cation Extraction Using Translation Information. In
Proceedings of ACL, pages 120-127.
1029
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809?816,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dependency Based Chinese Sentence Realization 
 
 
Wei He1, Haifeng Wang2, Yuqing Guo2, Ting Liu1 
1Information Retrieval Lab, Harbin Institute of Technology, Harbin, China 
{whe,tliu}@ir.hit.edu.cn 
2Toshiba (China) Research and Development Center, Beijing, China 
{wanghaifeng,guoyuqing}@rdc.toshiba.com.cn 
 
  
Abstract 
This paper describes log-linear models for a 
general-purpose sentence realizer based on de-
pendency structures. Unlike traditional realiz-
ers using grammar rules, our method realizes 
sentences by linearizing dependency relations 
directly in two steps. First, the relative order 
between head and each dependent is deter-
mined by their dependency relation. Then the 
best linearizations compatible with the relative 
order are selected by log-linear models. The 
log-linear models incorporate three types of 
feature functions, including dependency rela-
tions, surface words and headwords. Our ap-
proach to sentence realization provides sim-
plicity, efficiency and competitive accuracy. 
Trained on 8,975 dependency structures of a 
Chinese Dependency Treebank, the realizer 
achieves a BLEU score of 0.8874. 
1 Introduction 
Sentence realization can be described as the 
process of converting the semantic and syntactic 
representation of a sentence or series of sen-
tences into meaningful, grammatically correct 
and fluent text of a particular language. 
Most previous general-purpose realization sys-
tems are developed via the application of a set of 
grammar rules based on particular linguistic 
theories, e.g. Lexical Functional Grammar (LFG), 
Head Driven Phrase Structure Grammar (HPSG), 
Combinatory Categorical Grammar (CCG), Tree 
Adjoining Grammar (TAG) etc. The grammar 
rules are either developed by hand, such as those 
used in LinGo (Carroll et al, 1999), OpenCCG 
(White, 2004) and XLE (Crouch et al, 2007), or 
extracted automatically from annotated corpora, 
like the HPSG (Nakanishi et al, 2005), LFG 
(Cahill and van Genabith, 2006; Hogan et al, 
2007) and CCG (White et al, 2007) resources 
derived from the Penn-II Treebank. 
Over the last decade, there has been a lot of in-
terest in a generate-and-select paradigm for sur-
face realization. The paradigm is characterized 
by a separation between realization and selection, 
in which rule-based methods are used to generate 
a space of possible paraphrases, and statistical 
methods are used to select the most likely reali-
zation from the space. Usually, two statistical 
models are used to rank the output candidates. 
One is n-gram model over different units, such as 
word-level bigram/trigram models (Bangalore 
and Rambow, 2000; Langkilde, 2000), or fac-
tored language models integrated with syntactic 
tags (White et al 2007). The other is log-linear 
model with different syntactic and semantic fea-
tures (Velldal and Oepen, 2005; Nakanishi et al, 
2005; Cahill et al, 2007). 
However, little work has been done on proba-
bilistic models learning direct mapping from in-
put to surface strings, without the effort to con-
struct a grammar. Guo et al (2008) develop a 
general-purpose realizer couched in the frame-
work of Lexical Functional Grammar based on 
simple n-gram models. Wan et al (2009) present 
a dependency-spanning tree algorithm for word 
ordering, which first builds dependency trees to 
decide linear precedence between heads and 
modifiers then uses an n-gram language model to 
order siblings. Compared with n-gram model, 
log-linear model is more powerful in that it is 
easy to integrate a variety of features, and to tune 
feature weights to maximize the probability. A 
few papers have presented maximum entropy 
models for word or phrase ordering (Ratnaparkhi, 
2000; Filippova and Strube, 2007). However, 
those attempts have been limited to specialized 
applications, such as air travel reservation or or-
dering constituents of a main clause in German.  
This paper presents a general-purpose realizer 
based on log-linear models for directly lineariz-
ing dependency relations given dependency 
structures. We reduce the generation space by 
809
two techniques: the first is dividing the entire 
dependency tree into one-depth sub-trees and 
solving linearization in sub-trees; the second is 
the determination of relative positions between 
dependents and heads according to dependency 
relations. Then the best linearization for each 
sub-tree is selected by the log-linear model that 
incorporates three types of feature functions, in-
cluding dependency relations, surface words and 
headwords. The evaluation shows that our realiz-
er achieves competitive generation accuracy. 
The paper is structured as follows. In Section 
2, we describe the idea of dividing the realization 
procedure for an entire dependency tree into a 
series of sub-procedures for sub-trees. We de-
scribe how to determine the relative positions 
between dependents and heads according to de-
pendency relations in Section 3. Section 4 gives 
details of the log-linear model and the feature 
functions used for sentence realization. Section 5 
explains the experiments and provides the results. 
2 Sentence Realization from Dependen-
cy Structure  
2.1 The Dependency Input  
The input to our sentence realizer is a dependen-
cy structure as represented in the HIT Chinese 
Dependency Treebank (HIT-CDT)1. In our de-
pendency tree representations, dependency rela-
tions are represented as arcs pointing from a head 
to a dependent. The types of dependency arcs 
indicate the semantic or grammatical relation-
ships between the heads and the dependents, 
which are recorded in the dependent nodes. Fig-
ure 1 gives an example of dependency tree repre-
sentation for the sentence: 
 
(1) ? ? ?? ?? 
 this is Wuhan Airline 
 ?? ?? ?? ?? 
 first time buy Boeing airliner 
?This is the first time for Airline Wuhan to buy 
Boeing airliners.? 
In a dependency structure, dependents are un-
ordered, i.e. the string position of each node is 
not recorded in the representation. Our sentence 
realizer takes such an unordered dependency tree 
as input, determines the linear order of the words 
                                                 
1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences 
and 215,334 words, which are manually annotated with 
part-of-speech tags and dependency labels. (Liu et al, 
2006a) 
as encoded in the nodes of the dependency struc-
ture and produces a grammatical sentence. As the 
dependency structures input to our realizer have 
been lexicalized, lexical selection is not involved 
during the surface realization. 
2.2 Divide and Conquer Strategy for Linea-
rization 
For determining the linear order of words 
represented by nodes of the given dependency 
structure, in principle, the sentence realizer has 
to produce all possible sequences of the nodes 
from the input tree and selects the most likely 
linearization among them. If the dependency tree 
consists of a considerable number of nodes, this 
procedure would be very time-consuming.  To 
reduce the number of possible realizations, our 
generation algorithm adopts a divide-and-
conquer strategy, which divides the whole tree 
into a set of sub-trees of depth one and recursive-
ly linearizes the sub-trees in a bottom-up fashion. 
As illustrated in Figure 2, sub-trees c and d, 
which are at the bottom of the tree, are linearized 
first, then sub-tree b is processed, and finally 
sub-tree a.  
The procedure imposes a projective constraint 
on the dependency structures, viz. each head 
dominates a continuous substring of the sentence 
realization. This assumption is feasible in the 
application of the dependency-based generation, 
because: (i) it has long been observed that the 
dependency structures of a vast majority of sen-
tences in the languages of the world are projec-
tive (Igor, 1988) and (ii) non-projective depen-
dencies in Chinese, for the most part, are used to 
account for non-local dependency phenomena. 
Figure 1: The dependency tree for the sentence 
???????????????? 
??(HED)
is 
??(SBV)
this 
???(VOB) 
buy 
???(ADV)
first time 
???(VOB) 
airliner 
???(SBV)
airline 
???(ATT)
Wuhan 
???(ATT) 
Boeing 
810
Though non-local dependencies are important for 
accurate semantic analysis, they can be easily 
converted to local dependencies conforming to 
the projective constraint. In fact, we find that the 
10, 000 manually-build dependency trees of the 
HIT-CDT do not contain any non-projective de-
pendencies. 
3 Relative Position Determination 
In dependency structures, the semantic or gram-
matical roles of the nodes are indicated by types 
of dependency relations. For example, the VOB 
dependency relation, which stands for the verb-
object structure, means that the head is a verb 
and the dependent is an object of the verb; the 
ATT relation, means that the dependent is an 
attribute of the head. In languages with fairly 
rigid word order, the relative position between 
the head and dependent of a certain relation is in 
a fixed order. For example in Chinese, the object 
almost always occurs behind its dominating verb; 
the attribute modifier always occurs in front of 
its head word. Therefore, we can draw a conclu-
sion that the relative positions between head and 
dependent of VOB and ATT can be determined 
by the types of dependency relations. 
We make a statistic on the relative positions 
between head and dependent for each dependen-
cy relation type. Following (Covington, 2001), 
we call a dependent that precedes its head prede-
pendent, a dependent that follows its head post-
dependent. The corpus used to gather appropriate 
statistics is HIT-CDT. Table 1 gives the numbers 
??(HED) 
is 
??(SBV) 
this 
?   ?   ???????????? 
?  
???(VOB) 
buy 
???(ADV)
first time 
?  ?  
????   ??   ??   ???? 
???(VOB)
airliner 
???(ATT)
Boeing 
??   ??
???(SBV) 
Airline 
???(ATT) 
Wuhan 
??   ?? 
sub-tree a 
sub-tree b 
sub-tree c sub-tree d 
Figure 2: Illustration of the linearization procedure 
Relation Description Postdep. Predep.
ADV adverbial 1 25977
APP appositive 807 0
ATT attribute 0 47040
CMP complement 2931 3
CNJ conjunctive 0 2124
COO coordinate 6818 0
DC dep. clause 197 0
DE DE phrase 0 10973
DEI DEI phrase 131 3
DI DI phrase 0 400
IC indep.clause 3230 0
IS indep.structure 125 794
LAD left adjunct 0 2644
MT mood-tense 3203 0
POB prep-obj 7513 0
QUN quantity 0 6092
RAD right adjunct 1332 1
SBV subject-verb 6 16016
SIM similarity 0 44
VOB verb-object 23487 21
VV verb-verb 6570 2
Table 1: Numbers of pre/post-dependents for each 
dependency relation 
811
of predependent/postdependent for each type of 
dependency relations and its descriptions. 
Table 1 shows that 100% dependents of ATT 
relation are predependents and 23,487(99.9%) 
against 21(0.1%) VOB dependents are postde-
pendents. Almost all the dependency relations 
have a dominant dependent type?predependent 
or postdependent. Although some dependency 
relations have exceptional cases (e.g. VOB), the 
number is so small that it can be ignored. The 
only exception is the IS relation, which has 
794(86.4%) predependents and 125(13.6%) 
postdependents. The IS label is an abbreviation 
for independent structure. This type of depen-
dency relation is usually used to represent inter-
jections or comments set off by brackets, which 
usually has little grammatical connection with 
the head. Figure 3 gives an example of indepen-
dent structure. This example is from a news re-
port, and the phrase ??????? (set apart by 
brackets in the original text) is a supplementary 
explanation for the source of the news. The con-
nection between this phrase and the main clause 
is so weak that either it precedes or follows the 
head verb is acceptable in grammar. However, 
this kind of news-source-explanation is customa-
ry to place at the beginning of a sentence in Chi-
nese. This can probably explain the majority of 
the IS-tagged dependents are predependents. 
If we simply treat all the IS dependents as pre-
dependents, we can assume that every dependen-
cy relation has only one type of dependent, either 
predependent or postdependent. Therefore, the 
relative position between head and dependent 
can be determined just by the types of dependen-
cy relations. 
In the light of this assumption, all dependents 
in a sub-tree can be classified into two groups?
predependents and postdependents. The prede-
pendents must precede the head, and the postde-
pendents must follow the head. This classifica-
tion not only reduces the number of possible se-
quences, but also solves the linearization of a 
sub-tree if the sub-tree contains only one depen-
dent, or two dependents of different types, viz. 
one predependent and one postdependent. In sub-
tree c of Figure 2, the dependency relation be-
tween the only dependent and the head is ATT, 
which indicates that the dependent is a prede-
pendent. Therefore, node 7 is bound to precede 
node 5, and the only linearization result is ???
???. In sub-tree a of the same figure, the clas-
sification for SBV is predependent, and for VOB 
is postdependent, so the only linearization is 
<node 2, node 1, node 3>.  
In HIT-CDT, there are 108,086 sub-trees in 
the 10,000 sentences, 65% sub-trees have only 
one dependent, and 7% sub-trees have two de-
pendents of different types (one predependent 
and one postdependent). This means that the 
relative position classification can deterministi-
cally linearize 72% sub-trees, and only the rest 
28% sub-trees with more than one predependent 
or postdependent need to be further determined. 
4 Log-linear Models 
We use log-linear models for selecting the se-
quence with the highest probability from all the 
possible linearizations of a sub-tree. 
4.1 The Log-linear Model 
Log-linear models employ a set of feature func-
tions to describe properties of the data, and a set 
of learned weights to determine the contribution 
of each feature. In this framework, we have a set 
of M feature functions Mmtrhm ,...,1),,( = . 
For each feature function, there exists a model 
parameter Mmtrm ,...,1),,( =?  that is fitted to 
optimize the likelihood of the training data. A 
conditional log-linear model for the probability 
of a realization r given the dependency tree t, has 
the general parametric form 
)],(exp[
)(
1
)|(
1
trh
tZ
trp m
M
m
m?
=
= ?
?
?            (1) 
where )(tZ?  is a normalization factor defined as 
? ?
? =
=
)(' 1
)],'(exp[)(
tYr
m
M
m
m trhtZ ??                    (2) 
And Y(t) gives the set of all possible realizations 
of the dependency tree t. 
4.2 Feature Functions 
We use three types of feature functions for cap-
turing relations among nodes on the dependency 
tree. In order to better illustrate the feature func-
tions used in the log-linear model, we redraw 
sub-tree b of Figure 2 in Figure 4. Here we as-
sume the linearizations of sub-tree c and d have 
Figure 3: Example of independent structure 
???(HED) 
serious 
??????(IS) 
Xinhua news 
?????(SBV) 
southern snowstorm 
812
been finished, and the strings of linearizing re-
sults are recorded in nodes 5 and 6. 
The sub-tree in Figure 4 has two predepen-
dents (SBV and ADV) and one postdependent 
(VOB). As a result of this classification, the only 
two possible linearizations of the sub-tree are 
<node 4, node 6, node 3, node 5> and <node 6, 
node 4, node 3, node 5>. Then the log-linear 
model that incorporates three types of feature 
functions is used to make further selection. 
Dependency Relation Model: For a particular 
sub-tree structure, the task of generating a string 
covered by the nodes on the sub-tree is equiva-
lent to linearizing all the dependency relations in 
that sub-tree. We linearize the dependency rela-
tions by computing n-gram models, similar to 
traditional word-based language models, except 
using the names of dependency relations instead 
of words. For the two linearizations of Figure 4, 
the corresponding dependency relation sequences 
are ?ADV SBV VOB VOB? and ?SBV ADV 
VOB VOB?. The dependency relation model 
calculates the probability of dependency relation 
n-gram P(DR) according to Eq.(3). The probabil-
ity score is integrated into the log-linear model as 
a feature.  
)...()( 11 m
m DRDRPDRP =  (3) 
       )|( 1 1
1
?
+?
=
?= k nkm
k
k DRDRP  
Word Model: We integrate an n-gram word 
model into the log-linear model for capturing the 
relation between adjacent words. For a string of 
words generated from a possible sequence of 
sub-tree nodes, the word models calculate word-
based n-gram probabilities of the string. For ex-
ample, in Figure 4, the strings generated by the 
two possible sequences are ????? ?? ?
? ????? and ??? ???? ?? ???
??. The word model takes these two strings as 
input, and calculates the n-gram probabilities. 
Headword Model: 2 In dependency representa-
tions, heads usually play more important roles 
than dependents. The headword model calculates 
the n-gram probabilities of headwords, without 
regard to the words occurring at dependent nodes, 
in that dependent words are usually less impor-
tant than headwords. In Figure 4, the two possi-
ble sequences of headwords are ??? ?? ?
?  ??? and ???  ??  ??  ???. The 
headword strings are usually more generic than 
the strings including all words, and thus the 
headword model is more likely to relax the data 
sparseness. 
   Table 2 gives some examples of all the features 
used in the log-linear model. The examples listed 
in the table are features of the linearization 
<node 6, node 4, node 3, node 5>, extracted from 
the sub-tree in Figure 4. 
In this paper, all the feature functions used in 
the log-linear model are n-gram probabilities. 
However, the log-linear framework has great 
potential for including other types of features. 
4.3 Parameter Estimation 
BLEU score, a method originally proposed to 
automatically evaluate machine translation quali-
ty (Papineni et al, 2002), has been widely used 
as a metric to evaluate general-purpose sentence 
generation (Langkilde, 2002; White et al, 2007; 
Guo et al 2008, Wan et al 2009). The BLEU 
measure computes the geometric mean of the 
precision of n-grams of various lengths between 
a sentence realization and a (set of) reference(s).  
To estimate the parameters ),...,( 1 M??  for the 
feature functions ),...,( 1 Mhh , we use BLEU
3 as 
optimization objective function and adopt the 
approach of minimum error rate training 
                                                 
2 Here the term ?headword? is used to describe the word 
that occurs at head nodes in dependency trees.  
3 The BLEU scoring script is supplied by NIST Open Ma-
chine Translation Evaluation at 
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 
Feature function Examples of features 
Dependency Relation ?SBV ADV VOB?  ?ADV VOB VOB? 
Word Model ???????? ???????? ????????????????
Headword Model ?????? ?????? ?????? 
Table 2: Examples of feature functions 
???(VOB) 
buy 
???(ADV) 
first time 
???(VOB) 
airliner 
?????? 
airliners of Boeing 
???(SBV)
Airline 
?????? 
Airline Wuhan
Figure 4: Sub-tree with multiple predependents
813
(MERT), which is popular in statistical machine 
translation (Och, 2003).   
4.4 The Realization Algorithm 
The realization algorithm is a recursive proce-
dure that starts from the root node of the depen-
dency tree, and traverses the tree by depth-first 
search. The pseudo code of the realization algo-
rithm is shown in Figure 5. 
5 Experiments 
5.1 Experimental Design 
Our experiments are carried out on HIT-CDT. 
We randomly select 526 sentences as the test set, 
and 499 sentences as the development set for 
optimizing the model parameters. The rest 8,975 
sentences of the HIT-CDT are used for training 
of the dependency relation model. For training of 
word models, we use the Xinhua News part 
(6,879,644 words) of Chinese Gigaword Second 
Edition (LDC2005T14), segmented by the Lan-
guage Technology Platform (LTP) 4 . And for 
training the headword model, we use both the 
HIT-CDT and the HIT Chinese Skeletal Depen-
dency Treebank (HIT-CSDT). HIT-CSDT is a 
                                                 
4 http://ir.hit.edu.cn/demo/ltp  
component of LTP and contains 49,991 sen-
tences in dependency structure representation 
(without dependency relation labels). 
As the input dependency representation does 
not contain punctuation information, we simply 
remove all punctuation marks in the test and de-
velopment sets. 
5.2 Evaluation Metrics 
In addition to BLEU score, percentage of exactly 
matched sentences and average NIST simple 
string accuracy (SSA) are adopted as evaluation 
metrics. The exact match measure is percentage 
of the generated string that exactly matches the 
corresponding reference sentence. The average 
NIST simple string accuracy score reflects the 
average number of insertion (I), deletion (D), and 
substitution (S) errors between the output sen-
tence and the reference sentence. Formally, SSA 
= 1 ? (I + D + S) / R, where R is the number of 
tokens in the reference sentence. 
5.3 Experimental Results 
All the evaluation results are shown in Table 3. 
The first experiment, which is a baseline experi-
ment, ignores the tree structure and randomly 
chooses position for every word. From the 
second experiment, we begin to utilize the tree 
structure and apply the realization algorithm de-
scribed in Section 4.4. In the second experiment, 
predependents are distinguished from postdepen-
dents by the relative position determination me-
thod (RPD), then the orders inside predependents 
and postdependents are chosen randomly. From 
the third experiments, the log-linear models are 
used for scoring the generated sequences, with 
the aid of three types of feature functions as de-
scribed in Section 4.2. First, the feature functions 
of trigram dependency relation model (DR), bi-
gram word model (Bi-WM), trigram word model 
(Tri-WM) (with Katz backoff) and trigram 
headword model (HW) are used separately in 
experiments 3-6. Then we combine the feature 
1:procedure SEARCH 
2:input: sub-tree T {head:H dep.:D1?Dn} 
3:  if n = 0 then return 
4:  for i := 1 to n 
5:    SEARCH(Di) 
6:  Apre := {} 
7:  Apost := {} 
8:  for i := 1 to n 
9:    if PRE-DEP(Di) then Apre:=Apre?{Di} 
10:   if POST-DEP(Di) then Apost:=Apost?{Di} 
11: for all permutations p1 of Apre 
12:   for all permutations p2 of Apost 
13:     sequence s := JOIN(p1,H,p2) 
14:     score r := LOG-LINEAR(s) 
15:     if best-score(r) then RECORD(r,s) 
Figure 5: The algorithm for linearizations of sub-
trees 
 Model BLEU ExMatch SSA 
1 Random 0.1478 0.0038 0.2044 
2 RPD + Random 0.5943 0.1274 0.6369 
3 RPD + DR 0.7204 0.2167 0.7683 
4 RPD + Bi-WM 0.8289 0.4125 0.8270 
5 RPD + Tri-WM 0.8508 0.4715 0.8415 
6 RPD + HW 0.7592 0.2909 0.7638 
7 RPD + DR + Bi-WM 0.8615 0.4810 0.8723 
8 RPD + DR + Tri-WM 0.8772 0.5247 0.8817 
9 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920 
Table 3: BLEU, ExMatch and SSA scores on the test set 
814
functions incrementally based on the RPD and 
DR model. 
The relative position determination plays an 
important role in the realization algorithm. We 
observe that the BLEU score is boosted from 
0.1478 to 0.5943 by using the RPD method. This 
can be explained by the reason that the lineariza-
tions of 72% sub-trees can be definitely deter-
mined by the RPD method. All of the four fea-
ture functions we have tested achieve considera-
ble improvement in BLEU scores. The depen-
dency relation model achieves 0.7204, the bi-
gram word model 0.8289, the trigram word mod-
el 0.8508 and the headword model achieves 
0.7592. While the combined models perform bet-
ter than any of their individual component mod-
els. On the foundation of relative position deter-
mination method, the combination of dependen-
cy relation and bigram word model achieves a 
BLEU score of 0.8615, and the combination of 
dependency relation and trigram word model 
achieves a BLEU score of 0.8772. Finally the 
combination of dependency relation model, tri-
gram word model and headword model achieves 
the best result 0.8874.  
5.4 Discussion 
We first inspected the errors made by the relative 
position determination method. In the treebank-
tree test set, there are 7 predependents classified 
as postdependents and 3 postdependents classi-
fied as predependents by error. Among the 9,384 
dependents, the error rate of the relative position 
determination method is very small (0.1%). 
Then we make a classification on the errors in 
the experiment of dependency relation model 
(with relative position determination method). 
Table 4 shows the distribution of the errors. 
The first type of errors is caused by duplicate 
dependency relations, i.e. a head with two or 
more dependents that have the same dependency 
relations. In this situation, only using the depen-
dency relation model cannot generate the right 
linearization. However, word models, which util-
ize the word information, can make distinctions 
between the dependencies. The reason for the 
errors of SBV-ADV and ATT-QUN is probably 
because the order of these pairs of grammar roles 
is somewhat flexible. For example, the strings of 
???(ADV)/today ?(SBV)/I? and ??(SBV)/I 
??(ADV)/today? are both very common and 
acceptable in Chinese. 
The word models tend to combine the nodes 
that have strong correlation together. For exam-
ple in Figure 6, node 2 is more likely to precede 
node 3 because the words ??? /protect? and 
??? /future? have strong correlation, but the 
correct order is <node 3, node 2>. 
Headword model only consider the words oc-
cur at head nodes, which is helpful in the situa-
tion like Figure 6. In our experiments, the head-
word model gets a relatively low performance by 
itself, however, the addition of headword model 
to the combination of the other two feature func-
tions improves the result from 0.8772 to 0.8874. 
This indicates that the headword model is com-
plementary to the other feature functions. 
6 Conclusions 
We have presented a general-purpose realizer 
based on log-linear models, which directly maps 
dependency relations into surface strings. The 
linearization of a whole dependency tree is di-
vided into a series of sub-procedures on sub-trees. 
The dependents in the sub-trees are classified 
into two groups, predependents or postdepen-
dents, according to their dependency relations. 
The evaluation shows that this relative position 
determination method achieves a considerable 
result. The log-linear model, which incorporates 
three types of feature functions, including de-
pendency relation, surface words and headwords, 
successfully captures factors in sentence realiza-
tion and demonstrates competitive performance.  
 
References  
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for 
Generation. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics, 
pages 42-48. Saarbr?cken, Germany. 
 Error types Proportion
1 Duplicate dependency relations 60.0% 
2 SBV-ADV 20.3% 
3 ATT-QUN 6.3% 
4 Other 13.4% 
Table 4: Error types in the RPD+DR experiment 
Figure 6: Sub-tree for ??????????? 
??? 
work
???(ATT) 
protect 
??? ??? 
?birds protecting?
??(SBV) 
of 
??? ?? 
future 
815
Aoife Cahill and Josef van Genabith. 2006. Robust 
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033-
1040. Sydney, Australia. 
Aoife Cahill, Martin Forst and Christian Rohrer. 2007. 
Stochastic Realisation Ranking for a Free Word 
Order language. In Proceedings of 11th European 
Workshop on Natural Language Generation, pages 
17-24. Schloss Dagstuhl, Germany. 
John Carroll, Ann Copestake, Dan Flickinger, and 
Victor Poznanski. 1999. An Efficient Chart Gene-
rator for (Semi-)Lexicalist Grammars. In Proceed-
ings of the 7th European Workshop on Natural 
Language Generation, pages 86-95, Toulouse. 
Michael A. Covington. 2001. A Fundamental Algo-
rithm for Dependency Parsing. In Proceedings of 
the 39th Annual ACM Southeast Conference, pages 
95?102. 
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy 
King, John Maxwell, and Paula Newman. 2007. 
XLE documentation. Palo Alto Research Center, 
CA. 
Katja Filippova and Michael Strube. 2007. Generating 
Constituent Order in German Clauses. In Proceed-
ings of the 45th Annual Meeting of the Association 
of Computational Linguistics, pages 320-327. Pra-
gue, Czech Republic. 
Yuqing Guo, Haifeng Wang and Josef van Genabith. 
2008. Dependency-Based N-Gram Models for 
General Purpose Sentence Realisation. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 297-304. Manchester, 
UK. 
Deirdre Hogan, Conor Cafferkey, Aoife Cahill and 
Josef van Genabith. 2007. Exploiting Multi-Word 
Units in History-Based Probabilistic Generation. In 
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and CoNLL, pages 267-276. Prague, Czech Repub-
lic. 
Mel'?uk Igor. 1988. Dependency syntax: Theory and 
practice. In Suny Series in Linguistics. State Uni-
versity of New York Press, New York, USA. 
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. In Proceedings of 1st Meeting of 
the North American Chapter of the Association for 
Computational Linguistics, pages 170-177. Seattle, 
WA. 
Irene Langkilde. 2002. An Empirical Verification of 
Coverage and Correctness for a General-Purpose 
Sentence Generator. In Proceedings of the Second 
International Conference on Natural Language 
Generation, pages 17-24. New York, USA. 
Ting Liu, Jinshan Ma, and Sheng Li. 2006a. Building 
a Dependency Treebank for Improving Chinese 
Parser. Journal of Chinese Language and Compu-
ting, 16(4): 207-224. 
Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li. 
2006b. Dependency Parsing Based on Dynamic 
Local Optimization. In Proceedings of CoNLL-X, 
pages 211-215, New York, USA. 
Hiroko Nakanishi, Yusuke Miyao and Jun?ichi Tsujii. 
2005. Probabilistic Models for Disambiguation of 
an HPSG-Based Chart Generator. In Proceedings 
of the 9th International Workshop on Parsing 
Technology, pages 93-102. Vancouver, British Co-
lumbia. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160-167, Sappo-
ro, Japan. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-
318. Philadelphia, PA. 
Adwait Ratnaparkhi. 2000. Trainable Methods for 
Natural Language Generation. In Proceedings of 
North American Chapter of the Association for 
Computational Linguistics, pages 194-201. Seattle, 
WA. 
Erik Velldal and Stephan Oepen. 2005. Maximum 
Entropy Models for Realization Ranking. In Pro-
ceedings of the 10th Machine Translation Summit, 
pages 109-116. Phuket, Thailand,  
Stephen Wan, Mark Dras, Robert Dale, C?cile Paris. 
2009. Improving Grammaticality in Statistical Sen-
tence Generation: Introducing a Dependency Span-
ning Tree Algorithm with an Argument Satisfac-
tion Model. In Proceedings of the 12th Conference 
of the European Chapter of the ACL, pages 852-
860. Athens, Greece. 
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proceedings of the third International Nat-
ural Language Generation Conference, pages 182-
191. Hampshire, UK. 
Michael White, Rajakrishnan Rajkumar and Scott 
Martin. 2007. Towards Broad Coverage Surface 
Realization with CCG. In Proceedings of the Ma-
chine Translation Summit XI Workshop, pages 22-
30. Copenhagen, Danmark. 
816
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834?842,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Application-driven Statistical Paraphrase Generation
Shiqi Zhao, Xiang Lan, Ting Liu, Sheng Li
Information Retrieval Lab, Harbin Institute of Technology
6F Aoxiao Building, No.27 Jiaohua Street, Nangang District
Harbin, 150001, China
{zhaosq,xlan,tliu,lisheng}@ir.hit.edu.cn
Abstract
Paraphrase generation (PG) is important
in plenty of NLP applications. However,
the research of PG is far from enough. In
this paper, we propose a novel method for
statistical paraphrase generation (SPG),
which can (1) achieve various applications
based on a uniform statistical model, and
(2) naturally combine multiple resources
to enhance the PG performance. In our
experiments, we use the proposed method
to generate paraphrases for three differ-
ent applications. The results show that
the method can be easily transformed from
one application to another and generate
valuable and interesting paraphrases.
1 Introduction
Paraphrases are alternative ways that convey the
same meaning. There are two main threads in the
research of paraphrasing, i.e., paraphrase recogni-
tion and paraphrase generation (PG). Paraphrase
generation aims to generate a paraphrase for a
source sentence in a certain application. PG shows
its importance in many areas, such as question
expansion in question answering (QA) (Duboue
and Chu-Carroll, 2006), text polishing in natu-
ral language generation (NLG) (Iordanskaja et al,
1991), text simplification in computer-aided read-
ing (Carroll et al, 1999), and sentence similarity
computation in the automatic evaluation of ma-
chine translation (MT) (Kauchak and Barzilay,
2006) and summarization (Zhou et al, 2006).
This paper presents a method for statistical
paraphrase generation (SPG). As far as we know,
this is the first statistical model specially designed
for paraphrase generation. It?s distinguishing fea-
ture is that it achieves various applications with a
uniform model. In addition, it exploits multiple
resources, including paraphrase phrases, patterns,
and collocations, to resolve the data shortage prob-
lem and generate more varied paraphrases.
We consider three paraphrase applications in
our experiments, including sentence compression,
sentence simplification, and sentence similarity
computation. The proposed method generates
paraphrases for the input sentences in each appli-
cation. The generated paraphrases are then man-
ually scored based on adequacy, fluency, and us-
ability. The results show that the proposed method
is promising, which generates useful paraphrases
for the given applications. In addition, comparison
experiments show that our method outperforms a
conventional SMT-based PG method.
2 Related Work
Conventional methods for paraphrase generation
can be classified as follows:
Rule-based methods: Rule-based PG methods
build on a set of paraphrase rules or patterns,
which are either hand crafted or automatically
collected. In the early rule-based PG research,
the paraphrase rules are generally manually writ-
ten (McKeown, 1979; Zong et al, 2001), which
is expensive and arduous. Some researchers then
tried to automatically extract paraphrase rules (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Zhao
et al, 2008b), which facilitates the rule-based PG
methods. However, it has been shown that the
coverage of the paraphrase patterns is not high
enough, especially when the used paraphrase pat-
terns are long or complicated (Quirk et al, 2004).
Thesaurus-based methods: The thesaurus-based
methods generate a paraphrase t for a source sen-
tence s by substituting some words in s with
their synonyms (Bolshakov and Gelbukh, 2004;
834
Kauchak and Barzilay, 2006). This kind of method
usually involves two phases, i.e., candidate extrac-
tion and paraphrase validation. In the first phase,
it extracts all synonyms from a thesaurus, such as
WordNet, for the words to be substituted. In the
second phase, it selects an optimal substitute for
each given word from the synonyms according to
the context in s. This kind of method is simple,
since the thesaurus synonyms are easy to access.
However, it cannot generate other types of para-
phrases but only synonym substitution.
NLG-based methods: NLG-based methods (Ko-
zlowski et al, 2003; Power and Scott, 2005) gen-
erally involve two stages. In the first one, the
source sentence s is transformed into its semantic
representation r by undertaking a series of NLP
processing, including morphology analyzing, syn-
tactic parsing, semantic role labeling, etc. In the
second stage, a NLG system is employed to gen-
erate a sentence t from r. s and t are paraphrases
as they are both derived from r. The NLG-based
methods simulate human paraphrasing behavior,
i.e., understanding a sentence and presenting the
meaning in another way. However, deep analysis
of sentences is a big challenge. Moreover, devel-
oping a NLG system is also not trivial.
SMT-based methods: SMT-based methods
viewed PG as monolingual MT, i.e., translating s
into t that are in the same language. Researchers
employ the existing SMT models for PG (Quirk
et al, 2004). Similar to typical SMT, a large
parallel corpus is needed as training data in the
SMT-based PG. However, such data are difficult
to acquire compared with the SMT data. There-
fore, data shortage becomes the major limitation
of the method. To address this problem, we have
tried combining multiple resources to improve the
SMT-based PG model (Zhao et al, 2008a).
There have been researchers trying to propose
uniform PG methods for multiple applications.
But they are either rule-based (Murata and Isa-
hara, 2001; Takahashi et al, 2001) or thesaurus-
based (Bolshakov and Gelbukh, 2004), thus they
have some limitations as stated above. Further-
more, few of them conducted formal experiments
to evaluate the proposed methods.
3 Statistical Paraphrase Generation
3.1 Differences between SPG and SMT
Despite the similarity between PG and MT, the
statistical model used in SMT cannot be directly
applied in SPG, since there are some clear differ-
ences between them:
1. SMT has a unique purpose, i.e., producing
high-quality translations for the inputs. On
the contrary, SPG has distinct purposes in
different applications, such as sentence com-
pression, sentence simplification, etc. The
usability of the paraphrases have to be as-
sessed in each application.
2. In SMT, words of an input sentence should
be totally translated, whereas in SPG, not all
words of an input sentence need to be para-
phrased. Therefore, a SPG model should be
able to decide which part of a sentence needs
to be paraphrased.
3. The bilingual parallel data for SMT are easy
to collect. In contrast, the monolingual paral-
lel data for SPG are not so common (Quirk
et al, 2004). Thus the SPG model should
be able to easily combine different resources
and thereby solve the data shortage problem
(Zhao et al, 2008a).
4. Methods have been proposed for automatic
evaluation in MT (e.g., BLEU (Papineni et
al., 2002)). The basic idea is that a translation
should be scored based on their similarity to
the human references. However, they cannot
be adopted in SPG. The main reason is that it
is more difficult to provide human references
in SPG. Lin and Pantel (2001) have demon-
strated that the overlapping between the au-
tomatically acquired paraphrases and hand-
crafted ones is very small. Thus the human
references cannot properly assess the quality
of the generated paraphrases.
3.2 Method Overview
The SPG method proposed in this work contains
three components, i.e., sentence preprocessing,
paraphrase planning, and paraphrase generation
(Figure 1). Sentence preprocessing mainly in-
cludes POS tagging and dependency parsing for
the input sentences, as POS tags and dependency
information are necessary for matching the para-
phrase pattern and collocation resources in the
following stages. Paraphrase planning (Section
3.3) aims to select the units to be paraphrased
(called source units henceforth) in an input sen-
tence and the candidate paraphrases for the source
835
Multiple Paraphrase Tables
PT1 ??
Paraphrase 
Planning
Paraphrase 
Generation t
Sentence 
Preprocessings
A
PT2 PTn
Figure 1: Overview of the proposed SPG method.
units (called target units) from multiple resources
according to the given application A. Paraphrase
generation (Section 3.4) is designed to generate
paraphrases for the input sentences by selecting
the optimal target units with a statistical model.
3.3 Paraphrase Planning
In this work, the multiple paraphrase resources are
stored in paraphrase tables (PTs). A paraphrase ta-
ble is similar to a phrase table in SMT, which con-
tains fine-grained paraphrases, such as paraphrase
phrases, patterns, or collocations. The PTs used in
this work are constructed using different corpora
and different score functions (Section 3.5).
If the applications are not considered, all units
of an input sentence that can be paraphrased us-
ing the PTs will be extracted as source units. Ac-
cordingly, all paraphrases for the source units will
be extracted as target units. However, when a cer-
tain application is given, only the source and target
units that can achieve the application will be kept.
We call this process paraphrase planning, which is
formally defined as in Figure 2.
An example is depicted in Figure 3. The ap-
plication in this example is sentence compression.
All source and target units are listed below the in-
put sentence, in which the first two source units
are phrases, while the third and fourth are a pattern
and a collocation, respectively. As can be seen, the
first and fourth source units are filtered in para-
phrase planning, since none of their paraphrases
achieve the application (i.e., shorter in bytes than
the source). The second and third source units are
kept, but some of their paraphrases are filtered.
3.4 Paraphrase Generation
Our SPG model contains three sub-models: a
paraphrase model, a language model, and a usabil-
ity model, which control the adequacy, fluency,
Input: source sentence s
Input: paraphrase application A
Input: paraphrase tables PTs
Output: set of source units SU
Output: set of target units TU
Extract source units of s from PTs: SU={su1, ?, sun}
For each source unit sui
Extract its target units TUi={tui1, ?, tuim}
For each target unit tuij
If tuij cannot achieve the application A
Delete tuij from TUi
End If
End For
If TUi is empty
Delete sui from SU
End If
End for
Figure 2: The paraphrase planning algorithm.
and usability of the paraphrases, respectively1.
Paraphrase Model: Paraphrase generation is a
decoding process. The input sentence s is first
segmented into a sequence of I units s?I1, which
are then paraphrased to a sequence of units t?I1.
Let (s?i, t?i) be a pair of paraphrase units, their
paraphrase likelihood is computed using a score
function ?pm(s?i, t?i). Thus the paraphrase score
ppm(s?I1, t?I1) between s and t is decomposed into:
ppm(s?I1, t?I1) =
I?
i=1
?pm(s?i, t?i)?pm (1)
where ?pm is the weight of the paraphrase model.
Actually, it is defined similarly to the translation
model in SMT (Koehn et al, 2003).
In practice, the units of a sentence may be para-
phrased using different PTs. Suppose we have K
PTs, (s?ki , t?ki) is a pair of paraphrase units from
the k-th PT with the score function ?k(s?ki , t?ki),
then Equation (1) can be rewritten as:
ppm(s?I1, t?I1) =
K?
k=1
(
?
ki
?k(s?ki , t?ki)?k) (2)
where ?k is the weight for ?k(s?ki , t?ki).
Equation (2) assumes that a pair of paraphrase
units is from only one paraphrase table. However,
1The SPG model applies monotone decoding, which does
not contain a reordering sub-model that is often used in SMT.
Instead, we use the paraphrase patterns to achieve word re-
ordering in paraphrase generation.
836
The US government should take the overall situation into consideration and actively promote bilateral high-tech trades.
The US government
The US administration
The US government on
overall situation 
overall interest
overall picture
overview
situation as a whole
whole situation
take [NN_1] into consideration  
consider [NN_1]
take into account [NN_1]
take account of [NN_1]
take [NN_1] into account
take into consideration [NN_1]
<promote, OBJ, trades>  
<sanction, OBJ, trades>
<stimulate, OBJ, trades>
<strengthen, OBJ, trades>
<support, OBJ, trades>
<sustain, OBJ, trades>
Paraphrase application: sentence compression
Figure 3: An example of paraphrase planning.
we find that about 2% of the paraphrase units ap-
pear in two or more PTs. In this case, we only
count the PT that provides the largest paraphrase
score, i.e., k? = argmaxk{?k(s?i, t?i)?k}.
In addition, note that there may be some units
that cannot be paraphrased or prefer to keep un-
changed during paraphrasing. Therefore, we have
a self-paraphrase table in the K PTs, which para-
phrases any separate word w into itself with a con-
stant score c: ?self (w,w) = c (we set c = e?1).
Language Model: We use a tri-gram language
model in this work. The language model based
score for the paraphrase t is computed as:
plm(t) =
J?
j=1
p(tj |tj?2tj?1)?lm (3)
where J is the length of t, tj is the j-th word of t,
and ?lm is the weight for the language model.
Usability Model: The usability model prefers
paraphrase units that can better achieve the ap-
plication. The usability of t depends on para-
phrase units it contains. Hence the usability model
pum(s?I1, t?I1) is decomposed into:
pum(s?I1, t?I1) =
I?
i=1
pum(s?i, t?i)?um (4)
where ?um is the weight for the usability model
and pum(s?i, t?i) is defined as follows:
pum(s?i, t?i) = e?(s?i,t?i) (5)
We consider three applications, including sentence
compression, simplification, and similarity com-
putation. ?(s?i, t?i) is defined separately for each:
? Sentence compression: Sentence compres-
sion2 is important for summarization, subti-
tle generation, and displaying texts in small
screens such as cell phones. In this appli-
cation, only the target units shorter than the
sources are kept in paraphrase planning. We
define ?(s?i, t?i) = len(s?i) ? len(t?i), where
len(?) denotes the length of a unit in bytes.
? Sentence simplification: Sentence simplifi-
cation requires using common expressions in
sentences so that readers can easily under-
stand the meaning. Therefore, only the target
units more frequent than the sources are kept
in paraphrase planning. Here, the frequency
of a unit is measured using the language
model mentioned above3. Specifically, the
langauge model assigns a score scorelm(?)
for each unit and the unit with larger score
is viewed as more frequent. We define
?(s?i, t?i) = 1 iff scorelm(t?i) > scorelm(s?i).
? Sentence similarity computation: Given a
reference sentence s?, this application aims to
paraphrase s into t, so that t is more similar
(closer in wording) with s? than s. This ap-
plication is important for the automatic eval-
uation of machine translation and summa-
rization, since we can paraphrase the human
translations/summaries to make them more
similar to the system outputs, which can re-
fine the accuracy of the evaluation (Kauchak
and Barzilay, 2006). For this application,
2This work defines compression as the shortening of sen-
tence length in bytes rather than in words.
3To compute the language model based score, the
matched patterns are instantiated and the matched colloca-
tions are connected with words between them.
837
only the target units that can enhance the sim-
ilarity to the reference sentence are kept in
planning. We define ?(s?i, t?i) = sim(t?i, s?)?
sim(s?i, s?), where sim(?, ?) is simply com-
puted as the count of overlapping words.
We combine the three sub-models based on a
log-linear framework and get the SPG model:
p(t|s) =
K?
k=1
(?k
?
ki
log ?k(s?ki , t?ki))
+ ?lm
J?
j=1
log p(tj |tj?2tj?1)
+ ?um
I?
i=1
?(s?i, t?i) (6)
3.5 Paraphrase Resources
We use five PTs in this work (except the self-
paraphrase table), in which each pair of paraphrase
units has a score assigned by the score function of
the corresponding method.
Paraphrase phrases (PT-1 to PT-3): Para-
phrase phrases are extracted from three corpora:
(1) Corp-1: bilingual parallel corpus, (2) Corp-
2: monolingual comparable corpus (comparable
news articles reporting on the same event), and
(3) Corp-3: monolingual parallel corpus (paral-
lel translations of the same foreign novel). The
details of the corpora, methods, and score func-
tions are presented in (Zhao et al, 2008a). In
our experiments, PT-1 is the largest, which con-
tains 3,041,822 pairs of paraphrase phrases. PT-2
and PT-3 contain 92,358, and 17,668 pairs of para-
phrase phrases, respectively.
Paraphrase patterns (PT-4): Paraphrase patterns
are also extracted from Corp-1. We applied the ap-
proach proposed in (Zhao et al, 2008b). Its basic
assumption is that if two English patterns e1 and e2
are aligned with the same foreign pattern f , then
e1 and e2 are possible paraphrases. One can refer
to (Zhao et al, 2008b) for the details. PT-4 con-
tains 1,018,371 pairs of paraphrase patterns.
Paraphrase collocations (PT-5): Collocations4
can cover long distance dependencies in sen-
tences. Thus paraphrase collocations are useful for
SPG. We extract collocations from a monolingual
4A collocation is a lexically restricted word pair with a
certain syntactic relation. This work only considers verb-
object collocations, e.g., <promote, OBJ, trades>.
corpus and use a binary classifier to recognize if
any two collocations are paraphrases. Due to the
space limit, we cannot introduce the detail of the
approach. We assign the score ?1? for any pair
of paraphrase collocations. PT-5 contains 238,882
pairs of paraphrase collocations.
3.6 Parameter Estimation
To estimate parameters ?k(1 ? k ? K), ?lm,
and ?um, we adopt the approach of minimum error
rate training (MERT) that is popular in SMT (Och,
2003). In SMT, however, the optimization objec-
tive function in MERT is the MT evaluation cri-
teria, such as BLEU. As we analyzed above, the
BLEU-style criteria cannot be adapted in SPG. We
therefore introduce a new optimization objective
function in this paper. The basic assumption is that
a paraphrase should contain as many correct unit
replacements as possible. Accordingly, we design
the following criteria:
Replacement precision (rp): rp assesses the pre-
cision of the unit replacements, which is defined
as rp = cdev(+r)/cdev(r), where cdev(r) is the
total number of unit replacements in the generated
paraphrases on the development set. cdev(+r) is
the number of the correct replacements.
Replacement rate (rr): rr measures the para-
phrase degree on the development set, i.e., the per-
centage of words that are paraphrased. We define
rr as: rr = wdev(r)/wdev(s), where wdev(r) is
the total number of words in the replaced units on
the development set, and wdev(s) is the number of
words of all sentences on the development set.
Replacement f-measure (rf): We use rf as the
optimization objective function in MERT, which
is similar to the conventional f-measure and lever-
ages rp and rr: rf = (2? rp? rr)/(rp+ rr).
We estimate parameters for each paraphrase ap-
plication separately. For each application, we first
ask two raters to manually label all possible unit
replacements on the development set as correct or
incorrect, so that rp, rr, and rf can be automati-
cally computed under each set of parameters. The
parameters that result in the highest rf on the de-
velopment set are finally selected.
4 Experimental Setup
Our SPG decoder is developed by remodeling
Moses that is widely used in SMT (Hoang and
Koehn, 2008). The POS tagger and depen-
dency parser for sentence preprocessing are SVM-
838
Tool (Gimenez and Marquez, 2004) and MST-
Parser (McDonald et al, 2006). The language
model is trained using a 9 GB English corpus.
4.1 Experimental Data
Our method is not restricted in domain or sentence
style. Thus any sentence can be used in develop-
ment and test. However, for the sentence similarity
computation purpose in our experiments, we want
to evaluate if the method can enhance the string-
level similarity between two paraphrase sentences.
Therefore, for each input sentence s, we need a
reference sentence s? for similarity computation.
Based on the above consideration, we acquire
experiment data from the human references of
the MT evaluation, which provide several human
translations for each foreign sentence. In detail,
we use the first translation of a foreign sentence
as the source s and the second translation as the
reference s? for similarity computation. In our ex-
periments, the development set contains 200 sen-
tences and the test set contains 500 sentences, both
of which are randomly selected from the human
translations of 2008 NIST Open Machine Transla-
tion Evaluation: Chinese to English Task.
4.2 Evaluation Metrics
The evaluation metrics for SPG are similar to the
human evaluation for MT (Callison-Burch et al,
2007). The generated paraphrases are manually
evaluated based on three criteria, i.e., adequacy,
fluency, and usability, each of which has three
scales from 1 to 3. Here is a brief description of
the different scales for the criteria:
Adequacy 1: The meaning is evidently changed.
2: The meaning is generally preserved.
3: The meaning is completely preserved.
Fluency 1: The paraphrase t is incomprehensible.
2: t is comprehensible.
3: t is a flawless sentence.
Usability 1: t is opposite to the application purpose.
2: t does not achieve the application.
3: t achieves the application.
5 Results and Analysis
We use our method to generate paraphrases for the
three applications. Results show that the percent-
ages of test sentences that can be paraphrased are
97.2%, 95.4%, and 56.8% for the applications of
sentence compression, simplification, and similar-
ity computation, respectively. The reason why the
last percentage is much lower than the first two
is that, for sentence similarity computation, many
sentences cannot find unit replacements from the
PTs that improve the similarity to the reference
sentences. For the other applications, only some
very short sentences cannot be paraphrased.
Further results show that the average number of
unit replacements in each sentence is 5.36, 4.47,
and 1.87 for sentence compression, simplification,
and similarity computation. It also indicates that
sentence similarity computation is more difficult
than the other two applications.
5.1 Evaluation of the Proposed Method
We ask two raters to label the paraphrases based
on the criteria defined in Section 4.2. The labeling
results are shown in the upper part of Table 1. We
can see that for adequacy and fluency, the para-
phrases in sentence similarity computation get the
highest scores. About 70% of the paraphrases are
labeled ?3?. This is because in sentence similar-
ity computation, only the target units appearing
in the reference sentences are kept in paraphrase
planning. This constraint filters most of the noise.
The adequacy and fluency scores of the other two
applications are not high. The percentages of la-
bel ?3? are around 30%. The main reason is that
the average numbers of unit replacements for these
two applications are much larger than sentence
similarity computation. It is thus more likely to
bring in incorrect unit replacements, which influ-
ence the quality of the generated paraphrases.
The usability is needed to be manually labeled
only for sentence simplification, since it can be
automatically labeled in the other two applica-
tions. As shown in Table 1, for sentence simplifi-
cation, most paraphrases are labeled ?2? in usabil-
ity, while merely less than 20% are labeled ?3?.
We conjecture that it is because the raters are not
sensitive to the slight change of the simplification
degree. Thus they labeled ?2? in most cases.
We compute the kappa statistic between the
raters. Kappa is defined as K = P (A)?P (E)1?P (E) (Car-
letta, 1996), where P (A) is the proportion of times
that the labels agree, and P (E) is the proportion
of times that they may agree by chance. We define
P (E) = 13 , as the labeling is based on three point
scales. The results show that the kappa statistics
for adequacy and fluency are 0.6560 and 0.6500,
which indicates a substantial agreement (K: 0.61-
0.8) according to (Landis and Koch, 1977). The
839
Adequacy (%) Fluency (%) Usability (%)
1 2 3 1 2 3 1 2 3
Sentence rater1 32.92 44.44 22.63 21.60 47.53 30.86 0 0 100
compression rater2 40.54 34.98 24.49 25.51 43.83 30.66 0 0 100
Sentence rater1 29.77 44.03 26.21 22.01 42.77 35.22 25.37 61.84 12.79
simplification rater2 33.33 35.43 31.24 24.32 39.83 35.85 30.19 51.99 17.82
Sentence rater1 7.75 24.30 67.96 7.75 22.54 69.72 0 0 100
similarity rater2 7.75 19.01 73.24 6.69 21.48 71.83 0 0 100
Baseline-1 rater1 47.31 30.75 21.94 43.01 33.12 23.87 - - -
rater2 47.10 30.11 22.80 34.41 41.51 24.09 - - -
Baseline-2 rater1 29.45 52.76 17.79 25.15 52.76 22.09 - - -
rater2 33.95 46.01 20.04 27.61 48.06 24.34 - - -
Table 1: The evaluation results of the proposed method and two baseline methods.
kappa statistic for usability is 0.5849, which is
only moderate (K: 0.41-0.6).
Table 2 shows an example of the generated para-
phrases. A source sentence s is paraphrased in
each application and we can see that: (1) for sen-
tence compression, the paraphrase t is 8 bytes
shorter than s; (2) for sentence simplification, the
words wealth and part in t are easier than their
sources asset and proportion, especially for the
non-native speakers; (3) for sentence similarity
computation, the reference sentence s? is listed be-
low t, in which the words appearing in t but not in
s are highlighted in blue.
5.2 Comparison with Baseline Methods
In our experiments, we implement two baseline
methods for comparison:
Baseline-1: Baseline-1 follows the method pro-
posed in (Quirk et al, 2004), which generates
paraphrases using typical SMT tools. Similar to
Quirk et al?s method, we extract a paraphrase ta-
ble for the SMT model from a monolingual com-
parable corpus (PT-2 described above). The SMT
decoder used in Baseline-1 is Moses.
Baseline-2: Baseline-2 extends Baseline-1 by
combining multiple resources. It exploits all PTs
introduced above in the same way as our pro-
posed method. The difference from our method is
that Baseline-2 does not take different applications
into consideration. Thus it contains no paraphrase
planning stage or the usability sub-model.
We tune the parameters for the two baselines
using the development data as described in Sec-
tion 3.6 and evaluate them with the test data. Since
paraphrase applications are not considered by the
baselines, each baseline method outputs a single
best paraphrase for each test sentence. The gener-
ation results show that 93% and 97.8% of the test
sentences can be paraphrased by Baseline-1 and
Baseline-2. The average number of unit replace-
ments per sentence is 4.23 and 5.95, respectively.
This result suggests that Baseline-1 is less capa-
ble than Baseline-2, which is mainly because its
paraphrase resource is limited.
The generated paraphrases are also labeled by
our two raters and the labeling results can be found
in the lower part of Table 1. As can be seen,
Baseline-1 performs poorly compared with our
method and Baseline-2, as the percentage of la-
bel ?1? is the highest for both adequacy and flu-
ency. This result demonstrates that it is necessary
to combine multiple paraphrase resources to im-
prove the paraphrase generation performance.
Table 1 also shows that Baseline-2 performs
comparably with our method except that it does
not consider paraphrase applications. However,
we are interested how many paraphrases gener-
ated by Baseline-2 can achieve the given applica-
tions by chance. After analyzing the results, we
find that 24.95%, 8.79%, and 7.16% of the para-
phrases achieve sentence compression, simplifi-
cation, and similarity computation, respectively,
which are much lower than our method.
5.3 Informal Comparison with Application
Specific Methods
Previous research regarded sentence compression,
simplification, and similarity computation as to-
tally different problems and proposed distinct
method for each one. Therefore, it is interesting
to compare our method to the application-specific
methods. However, it is really difficult for us to
840
Source
sentence
Liu Lefei says that in the long term, in terms of asset alocation, overseas investment should occupy a
certain proportion of an insurance company?s overall allocation.
Sentence
compression
Liu Lefei says that in [the long run]phr , [in area of [asset alocation][NN 1]]pat, overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
simplification
Liu Lefei says that in [the long run]phr , in terms of [wealth]phr [distribution]phr , overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
similarity
Liu Lefei says that in [the long run]phr , in terms [of capital]phr allocation, overseas investment should
occupy [the [certain][JJ 1] ratio of [an insurance company?s overall allocation][NN 1]]pat.
(reference sentence: Liu Lefei said that in terms of capital allocation, outbound investment should make
up a certain ratio of overall allocations for insurance companies in the long run .)
Table 2: The generated paraphrases of a source sentence for different applications. The target units after
replacement are shown in blue and the pattern slot fillers are in cyan. [?]phr denotes that the unit is a
phrase, while [?]pat denotes that the unit is a pattern. There is no collocation replacement in this example.
reimplement the methods purposely designed for
these applications. Thus here we just conduct an
informal comparison with these methods.
Sentence compression: Sentence compression
is widely studied, which is mostly reviewed as a
word deletion task. Different from prior research,
Cohn and Lapata (2008) achieved sentence com-
pression using a combination of several opera-
tions including word deletion, substitution, inser-
tion, and reordering based on a statistical model,
which is similar to our paraphrase generation pro-
cess. Besides, they also used paraphrase patterns
extracted from bilingual parallel corpora (like our
PT-4) as a kind of rewriting resource. However,
as most other sentence compression methods, their
method allows information loss after compression,
which means that the generated sentences are not
necessarily paraphrases of the source sentences.
Sentence Simplification: Carroll et al (1999)
has proposed an automatic text simplification
method for language-impaired readers. Their
method contains two main parts, namely the lex-
ical simplifier and syntactic simplifier. The for-
mer one focuses on replacing words with simpler
synonyms, while the latter is designed to transfer
complex syntactic structures into easy ones (e.g.,
replacing passive sentences with active forms).
Our method is, to some extent, simpler than Car-
roll et al?s, since our method does not contain syn-
tactic simplification strategies. We will try to ad-
dress sentence restructuring in our future work.
Sentence Similarity computation: Kauchak
and Barzilay (2006) have tried paraphrasing-based
sentence similarity computation. They paraphrase
a sentence s by replacing its words with Word-
Net synonyms, so that s can be more similar in
wording to another sentence s?. A similar method
has also been proposed in (Zhou et al, 2006),
which uses paraphrase phrases like our PT-1 in-
stead of WordNet synonyms. These methods can
be roughly viewed as special cases of ours, which
only focus on the sentence similarity computation
application and only use one kind of paraphrase
resource.
6 Conclusions and Future Work
This paper proposes a method for statistical para-
phrase generation. The contributions are as fol-
lows. (1) It is the first statistical model spe-
cially designed for paraphrase generation, which
is based on the analysis of the differences between
paraphrase generation and other researches, espe-
cially machine translation. (2) It generates para-
phrases for different applications with a uniform
model, rather than presenting distinct methods for
each application. (3) It uses multiple resources,
including paraphrase phrases, patterns, and collo-
cations, to relieve data shortage and generate more
varied and interesting paraphrases.
Our future work will be carried out along two
directions. First, we will improve the components
of the method, especially the paraphrase planning
algorithm. The algorithm currently used is sim-
ple but greedy, which may miss some useful para-
phrase units. Second, we will extend the method to
other applications, We hope it can serve as a uni-
versal framework for most if not all applications.
Acknowledgements
The research was supported by NSFC (60803093,
60675034) and 863 Program (2008AA01Z144).
Special thanks to Wanxiang Che, Ruifang He,
Yanyan Zhao, Yuhang Guo and the anonymous re-
viewers for insightful comments and suggestions.
841
References
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation, pages 136-158.
Jean Carletta. 1996. Assessing Agreement on Clas-
sification Tasks: The Kappa Statistic. In Computa-
tional Linguistics, 22(2): 249-254.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, John Tait. 1999. Simpli-
fying Text for Language-Impaired Readers. In Pro-
ceedings of EACL, pages 269-270.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion In Proceed-
ings of COLING, pages 137-144.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had Asked:
The impact of paraphrasing for Question Answer-
ing. In Proceedings of HLT-NAACL, pages 33-36.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, pages
43-46.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL, pages 455-462.
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure us-
ing lexico-grammatical resources. In Proceedings
of IWP, pages 1-8.
J. R. Landis and G. G. Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data.
In Biometrics 33(1): 159-174.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Parsing with a
Two-Stage Discriminative Parser. In Proceedings of
CoNLL.
Kathleen R. McKeown. 1979. Paraphrasing Using
Given and New Information in a Question-Answer
System. In Proceedings of ACL, pages 67-72.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal Model for Paraphrasing - Using Transformation
Based on a Defined Criteria. In Proceedings of NL-
PRS, pages 47-54.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic gen-
eration of large-scale paraphrases. In Proceedings of
IWP, pages 73-79.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Tetsuro Takahashi, Tomoyam Iwakura, Ryu Iida, At-
sushi Fujita, Kentaro Inui. 2001. KURA: A
Transfer-based Lexico-structural Paraphrasing En-
gine. In Proceedings of NLPRS, pages 37-46.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. Combining Multiple Resources
to Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. ParaEval: Using Para-
phrases to Evaluate Summaries Automatically. In
Proceedings of HLT-NAACL, pages 447-454.
Chengqing Zong, Yujie Zhang, Kazuhide Yamamoto,
Masashi Sakamoto, Satoshi Shirai. 2001. Approach
to Spoken Chinese Paraphrasing Based on Feature
Extraction. In Proceedings of NLPRS, pages 551-
556.
842
Combining Neural Networks and Statistics for 
 Chinese Word sense disambiguation 
Zhimao Lu  Ting Liu  Sheng Li  
Information Retrieval Laboratory of Computer Science & Technology School,  
Harbin Institute of Technology 
Harbin, China, 150001 
{lzm, tliu}@ir.hit.edu.cn 
 
Abstract 
The input of network is the key problem for 
Chinese Word sense disambiguation utilizing 
the Neural Network. This paper presents an 
input model of Neural Network that calculates 
the Mutual Information between contextual 
words and ambiguous word by using statistical 
method and taking the contextual words to 
certain number beside the ambiguous word 
according to (-M, +N). The experiment adopts 
triple-layer BP Neural Network model and 
proves how the size of training set and the 
value of M and N affect the performance of 
Neural Network model. The experimental 
objects are six pseudowords owning three 
word-senses constructed according to certain 
principles. Tested accuracy of our approach on 
a close-corpus reaches 90.31%,, and 89.62% on 
a open-corpus. The experiment proves that the 
Neural Network model has good performance 
on Word sense disambiguation. 
1 Introduction 
It is general that one word with many senses in 
natural language. According statistics, there are 
about 42% ambiguous words in Chinese corpus (Lu, 
2001). Word sense disambiguation (WSD) is a 
method to determine the sense of ambiguous word 
given the context circumstance.  
WSD, a long-standing problem in NLP, has been 
a very active research topic,, which can be well 
applied in many NLP systems, such as Information 
Retrieval, Text Mining, Machine Translation, Text 
Categorization, Text Summarization, Speech 
Recognition, Text to Speech, and so on. 
With rising of Corpus linguistics, the machine 
learning methods based on statistics are booming 
(Yarowsky, 1992). These methods draw the support 
from the high-powered computers, get the statistics of 
large real-world corpus, find and acquire knowledge 
of linguistics automatically. They deal with all change 
by invariability, thus it is easy to trace the evaluation 
and development of natural language. So the statistic 
methods of NLP has attracted the attention of 
professional researchers and become the mainstream 
bit by bit. Corpus-based Statistical approaches are  
Decision Tree (Pedersen, 2001), Decision List, 
Genetic Algorithm, Naive-Bayesian Classifier 
(Escudero, 2000)?Maximum Entropy Model (Adam, 
1996; Li, 1999), and so on.  
Corpus-based statistical approaches can be divided 
into supervised and unsupervised according to whether 
training corpus is sense-labeled text. Supervised 
learning methods have the good learning ability and 
can get better accuracy in WSD experiments (Sch?tze, 
1998). Obviously the data sparseness problem is a 
bottleneck for supervised learning algorithm. If you 
want to get better learning and disambiguating effect, 
you can enlarge the size and smooth the data of 
training corpus. According to practical demand, it 
would spend much more time and manpower to 
enlarge the size of training corpus. Smoothing data is 
merely a subsidiary measure. The sufficient large size 
of training corpus is still the foundation to get a 
satisfied effect in WSD experiment. 
Unsupervised WSD never depend on tagged 
corpus and could realize the training of large real 
corpus coming from all kinds of applying field. So 
researchers begin to pay attention to this kind of 
methods (Lu, 2002). The kind of methods can 
overcome the sparseness problem in a degree. 
It is obvious that the two kinds of methods based 
on statistic have their own advantages and 
disadvantages, and cannot supersede each other.  
This paper researches the Chinese WSD using the 
model of artificial neural network and investigates 
the effect on WSD from input model of neural 
network constructed by the context words and the 
size of training corpus.  
2 BP Neural Network 
At the moment, there are about more than 30 kinds of 
artificial neural network (ANN) in the domain of 
research and application. Especially, BP neural 
network is a most popular model of ANN nowadays.  
2.1 The structure of BP Neural Network 
The BP model provides a simple method to 
calculate the variation of network performance 
cased by variation of single weight. This model 
contains not only input nodes and output nodes, but 
also multi-layer or mono-layer hidden nodes. Fig1.1 
is a construction chart of triple-layer BP neural 
network. As it is including the weights modifying 
process from the output layer to the input layer 
resulting from the total errors, the BP neural 
network is called Error Back Propagation network. 
 
 
 
 
 
 
 
 
 
Fig. 1.1 BP Network 
Fig.1.1 The structure of BP neural network 
Except for the nodes of input layer, all nodes of 
other layers are non-linear input and output. So the 
feature function should be differential on every part 
of function. General speaking, we can choose the 
sigmoid, tangent inspired, or linear function as the 
feature function because they are convenient for 
searching and solving by gradient technique. 
Formula (1) is a sigmoid function.  
                         ?1? 
The output of sigmoid function ranges between 0 
and 1, increasing monotonically with its input. 
Because it maps a very large input domain to a 
small range of outputs, it is often referred to as the 
squashing function of the unit. The output layer and 
hidden layer should adopt the sigmoid inspired 
function under the condition of intervention on the 
output, such as confining the output between 0 and 
1.  
2.2 Back Propagation function of BP 
neural network 
The joint weights should be revised many times 
during the progress of the error propagating back in 
BP networks. The variation of joint weights every 
time is solved by the method of gradient descent. 
Because there is no objective output in hidden layer, 
the variation of joint weight in hidden layer is 
solved under the help of error back propagation in 
output layer. If there are many hidden layers, this 
method can reason out the rest to the first layer by 
analogy. 
1) the variation of joint weights in output layer 
To calculate the variation of joint weights from 
input i?th to output k?th is as following: 
?wik = ??      = ??               (2) 
 = ?(tk - Ok )f2? O?i = ? ?ik O?i 
?ik = (tk ? Ok )f2?                    (3) 
?bki = ??       = ??               (4) 
= ?(tk - Ok )f2? = ? ?ik 
 
2) the variation of joint weights in hidden layer 
To calculate the variation of joint weights from 
input j?th to output i?th is as following: 
?w?ij = -?       = -?                   (5) 
1
1 + e-xf (x)  = ???
?E ????wik
?E ??? ?Ok 
?Ok ??? ? wik 
?E ???? w?ij
n?
k=1 
?E ????bki 
?E ??? ?Ok 
?Ok ??? ?bki 
?? ??
?? 
??
x1  x3 xn x2 
y1  y3 ym y2 
Outputs 
Hidden 
Inputs 
?E ??? ?Ok 
?Ok ????O?i 
?O?i???
? w?ij
n?
k=1 
= ?    (tk - Ok )f2? wik f1?pj  
= ? ?ij pj 
where:  ?ij = ei f1??ei =    ?ik wik          (6) 
     ?b?ki = ? ?ij                          (7) 
3. The construction of WSD model 
Under the consideration of fact that only 
numerical data can be accepted by the input and 
output of neural network, if BP neural network is 
used on WSD, the prerequisite is to vector the part of 
semantic meaning (words or phrases) and sense. 
In the event of training BP model, the input vector 
P and objective vector O of WSD should be 
determined firstly. And then we should choose the 
construction of neural network that needs to be 
designed, say, how many layers is network, how 
many neural nodes are in every layer, and the 
inspired function of hidden layer and output layer. 
The training of model still needs the vector added 
weight, output, and error vector. The training is over 
when the sum of square errors is less than the 
objection of error. Or the errors of output very to 
adjust the joint weight back and repeat the training. 
3.1 To vector the vocabulary 
WSD depends on the context to judge the meaning 
of ambiguous words. So the input of model should 
be the ambiguous words and the contextual words 
round them. In order to vector the words in the 
context, the Mutual Information (MI) of ambiguous 
words and context should be calculated. So MI can 
show the opposite distance of ambiguous words and 
contextual words. MI can replace every contextual 
word. That is suitable to as the input model. The 
function of MI is as follow: 
(8) 
  P(w1) and P(w2) are the probability of word w1 
and w2 to appear in the corpus separately. While 
P(w1, w2) is the probability of word w1 and w2 to 
appear together.  
The experimental corpus in this article stems 
from the People Daily of 1998. The extent is 
123,882 lines (10,000,000 words), including 
121,400 words and phrases.  
3.2 The pretreatment of BP network model 
The supervised WSD need artificial mark of 
meaning. But it is time consuming to mark artificially. 
So it is difficult to get the large scope and high quality 
training linguistic corpus. In order to overcome this 
difficulty and get large enough experimental linguistic 
corpuses, we should turn to seek the new way. 
We use pseudoword in place of the real word. That 
can get the arbitrary large experimental corpus 
according to the real demand.  
3.2.1 The construction of Pseudoword 
Pseudoword is the artificial combination of 
several real words on the basis of experimental 
demand to form an unreal word that possesses 
many features of real words and instead of real 
word as the experimental object in natural language 
research.  
In the real world, one word has many meanings 
derives from the variation and flexible application 
of words. That needs a long-term natural evolution. 
Frankly speaking, that evolution never ceases at all 
times. For example, the word ?? ?(da3) extends 
some new uses in recent years. Actually, in the 
endless history river of human beings, the 
development and variation of words meaning are 
rapid so far as to be more rapid than the 
replacement of dictionaries sometimes. Usually that 
makes an awkward position when you use 
dictionary to define the words meanings. Definitely, 
it is inconvenient for the research of natural 
linguistics based on dictionary.  
But the meaning of pseudoword (Sch?tze, 1992) 
need not defined with the aid of dictionary and 
simulates the real ambiguous word to survey the 
effect of various algorithms of classified meanings.  
To form a pseudoword need the single meaning 
word as a morpheme.  
Set:   Wp =  w1 / w2 / ? / wi 
Wp is a pseudoword formed with wi which 
contains i algorithms and meanings for every 
n ?
k=1 
MI(w1, w2) = 
P(w1, w2) ?????P(w1)P(w2) log 
n ?
k=1 
algorithm of pseudoword is single meaning and 
every living example is about equal to a 
pseudoword marked meaning in corpus. That is 
similar to the effect of artificial marked meaning. 
But the effect is more stable and reliable than 
artificial marked meaning. What?s more, the scope 
of corpus can enlarge endless according to the 
demand to avoid the phenomenon of sparse data. 
To define the number of algorithm, we count the 
average number of meanings according to the 
large-sized Chinese dictionaries (Table 3.1). Table 
3.2 show the overall number of ambiguous word 
and percentage of ambiguous word having 2~4 
meanings in all ambiguous word. These two charts 
indicate that verb is most active in Chinese and its 
average number of meanings is most, about 2.56. 
The percentage of ambiguous word having 2~4 
meanings is most in all ambiguous word.  
part of 
speech 
Average 
sense 
?including 
single-sense 
word? 
Average sense
?only 
ambiguous 
word? 
noun 1.136452 2.361200 
verb 1.220816 2.558158 
adjective 1.144717 2.300774 
adverb 1.059524 2.078431 
Table 3.1 the average number of a Chinese 
word?s sense 
3.2.2 Define the input vector  
It should be based on context to determine the 
sense of ambiguous word. The model?s input should 
be the vector of the ambiguous word and context 
words. It is well-known that the number of context  
ambiguous 
word 7955 / 
Bi-senses 
word 5799 72.80% 
Tri-senses 
word 1154 14.51% 
Four-senses 
word 450 5.66% 
Table 3.2 the distributing of ambiguous word 
words showing on the both sides of ambiguous 
word is not fixed in different sentences. But the 
number of vectors needed by BP network is fixed. 
In other words, the number of neural nodes of input 
model is fixed in the training. If the extracting 
method of feature vector is (-M, +N) in context, in 
other words there are M vectors on the left of 
ambiguous word and N vectors on the right, the 
extraction of feature vectors must span the limit of 
sentences. If the number of feature vectors is not 
enough, the ambiguous words on the left and right 
boundaries of whole corpus do not participate in the 
training.  
According to the extracting method of feature 
vector (-M, +N), the vector of model input is as 
following: 
V ?? = {MI11?MI 12???MI1i?MI 11??MI 12????
MI 1j??MI21?MI 22???MI2i?MI 21??MI 22????
MI2j??MI31?MI 32???MI3i?MI 31??MI 32????
MI 3j?}?1?i?M?1?j?N. 
Where, MI1i , MI1j? are the MI of context and the 
first meaning of ambiguous word?MI2i , MI2j? are 
the MI of context and the second meaning of 
ambiguous word?MI3i ?MI3j? are the MI of context 
and the third meaning of ambiguous word. MI1i, 
MI2i and MI3i are the feature words of ambiguous 
word on the left and MI of ambiguous word. MI1j?, 
MI2j? and MI2j? are the feature words of ambiguous 
word on the right and MI of ambiguous word.  
pseudo-
words
word
ID 
sample 
number 
pseudo-
words 
word 
ID 
sample
number
34466 5550 84323 3773
71345 3715 12751 2284
31796 12098 52915 3900W1 
total 21363
W4 
total 9957
71072 9296 53333 1362
78031 6024 29053 6135
48469 1509 75941 1205W2 
total 16829
W5 
total 8702
7464 25925 39945 2346
77375 2478 71335 1640
23077 4704 51491 1012W3 
total 33107
W6 
total 4998
Table 3.3 the total number of the feature -vector 
sample of ambiguous word 
Training corpus are 105,000 lines, and each line 
is a paragraph, totally about 10,000,000 words. 
Table 3.3 shows the number of collected feature 
vector samples (the frequency of ambiguous word).  
3.3 The definition of output model 
Every ambiguous word has three meanings, 
totally eighteen meanings for six ambiguous words. 
Every ambiguous word trains a model and every 
model has three outputs showed by three-bit integer 
of binary system, such as the three meanings of 
ambiguous word W are showed as followed: 
si1 = 100    si2 = 010   si3 = 001 
3.4 The definition of network structure 
According to statistics, when (-M, +N) are (-8, 
+9) using the method of feature extraction, the 
cover percentage of effective information is more 
than 87% (Lu, 2001). However, if the sentence is 
very short, collecting the contextual feature words 
on the basis of (-8, +9) can include much useless 
information to the input model. Undoubtedly, that 
will increase more noisy effect and deduce the 
meaning-distinguish ability of verve network.  
This article makes an on-the-spot investigation of 
experimental corpus, a fairly integrated meaning 
unit (the marks of border including comma, 
semicolon, ellipsis, period, question mark, 
exclamation mark, and the like), which average 
length is between 9~10 words. So this article 
collects the contextual feature words on the basis of 
(-5, +5) in the experiments, 10 feature words 
available that calculate MI with each meaning of 
ambiguous word separately to get 30 vectors. All 
punctuation marks should be filtered while the 
feature words are collected. The input layer of 
neural network model is regarded as 30 neural 
nodes. The triple-layer neural network adopts the 
inspired S function. From that, the number of neural 
nodes in hidden layer is defined as 12 on the basis 
of experimental contrast, and 3 neural nodes in 
output layer. Hence, the structure of model is 30 ? 
12 ? 3, and the precision of differential training is 
defined as 0.3 based on the experimental contrast. 
3.5 The test and training of model 
The experimental corpus appeared in front are 
123,882 lines. It is divided to three parts according 
to the demand of experiment, C1 (15,000 lines), C2 
(60,000 lines), and C3 (105,000 lines). The open 
test corpus is 18,882 lines. 
Table 3.3 tells us that there is a great disparity 
between the sample numbers of different 
ambiguous words in the experimental corpus of the 
same class. And the distribution of different 
meanings is not even for same ambiguous word. 
For the trained neural network has the good ability 
of differentiation for each word, the number of 
training sample should be about equal to each other 
for each meaning. So this experiment selects the 
least training samples. For example, there are 200 
samples of the first meaning in training corpus, the 
second 400, and the third 500. To balance the input, 
each meaning merely has 200 samples to be elected 
for training. 
Three groups of training corpus can train 3 neural 
networks possessing different vectors for every 
ambiguous word and make the unclose and open 
test for these networks separately.  
4 The result of experiment 
In order to analyze the effect that the extent of 
training corpus influences the meaning distinguish 
ability of neural network, this article trains the 
model of neural network using the experimental 
corpus individually, C1, C2 and C3, and makes the 
close and open test for 6 ambiguities separately.  
The close test means the corpus are same in test 
and training.  
The experiment is divided into two groups 
according to the extracting method of contextual 
feature words.  
4.1 The first experiment one 
Table 4.1 shows the result of the first experiment  
which extracts the contextual feature words using 
the method of ??5?+ 5?. 
In addition, the first experiment investigates that 
the extent of training corpus (the number of training 
samples big or small) influences the ability to 
distinguish the models. The result of test for 6  
close-test open-test pseudo- 
words accuracy Training set accuracy 
Training
set 
W1 0.8800 C2 0.8951 C3 
W2 0.8867 C2 0.8775 C2 
W3 0.8652 C3 0.8574 C3 
W4 0.8532 C3 0.8687 C3 
W5 0.8769 C3 0.8745 C3 
W6 0.8868 C2 0.8951 C3 
Table 4.1 The contrast chat of experimental result 
for six ambiguities 
ambiguities is showed in table 4.2 (close test), table 
4.3 (open test), and table 4.4. Considering the 
length of this article, table 4.2 and table 4.3 shows 
the detailed data, and table 4.4 is brief.  
Training set pseudo- 
words C1 C2 C3 
sense 1 0.9226 0.8169 0.8991
sense 2 0.5513 0.8017 0.6872
sense 3 0.8027 0.9564 0.9510W1 
average 0.7589 0.8800 0.8720
sense 1 0.8121 0.8780 0.9377
sense 2 0.8389 0.8968 0.8804
sense 3 0.7248 0.8856 0.8370
W6 
average 0.7919 0.8868 0.8850
Table 4.2 The result of W1 and W6 in close test  
under the different training corpus 
4.2 The second experiment 
The second experiment investigates emphatically 
the effect that the method to collect the feature 
words influences the ability to distinguish BP 
model. 
Training set pseudo- 
words C1 C2 C3 
sense 1 0.9019 0.7827 0.8942
sense 2 0.4607 0.8097 0.7175
sense 3 0.7792 0.9500 0.9515W1 
average 0.7573 0.8798 0.8951
sense 1 0.8233 0.9093 0.9535
sense 2 0.8799 0.8182 0.8604
sense 3 0.7278 0.8544 0.8038W6 
average 0.8259 0.8683 0.8951
Table 4.3 The result of W1 and W6 in open test 
 under the different training corpus 
There are many methods adopted in this 
experiment, including (?10?+ 10),??3?+ 3?,??3?
+ 7?,??7?+ 3?,??4?+ 6?and??6?+ 4?. Merely 
the ambiguous words W1 and W6 are regarded as the 
Training set pseudo- 
words C1 C2 C3 
W2 0.6628 0.8867 0.8772
W3 0.6695 0.8453 0.8652
W4 0.7414 0.8452 0.8532
W5
close
0.8283 0.8537 0.8769
W2 0.7287 0.8613 0.8700
W3 0.8085 0.8384 0.8574
W4 0.7920 0.8655 0.8687
W5
open
0.8288 0.8775 0.8745
Table 4.4 The contrast chart of experimental 
 result for four ambiguities 
Table 4.5 the experimental result under different 
feature collecting method 
experimental objects in this group experiment. See 
table 4.5 for the correct percentage of WSD.  
5. Analysis and discussion  
See table 5.1 for the number of experimental 
corpus samples in experiment.  
According to the table 3.3 and 5.1, the frequency of 
the each meaning (morpheme) of ambiguous word 
showing in corpus is quite different. That accords 
with the distribution of the every meanings of 
ambiguous word. However, there is one different 
point that the frequency of the each meaning of 
ambiguous word is rather high (that is the outcome 
selected by morpheme.). In other words, there are 
many examples showing for the each meaning of 
ambiguous word in training and test corpus. On the 
contrast, the difference of frequency is quite  
accuracy pseudo-
words
feature 
collecting 
method close-test open-test
Training
set 
??10?+10? 0.8897 0.8685 C1 
??3?+3? 0.7917 0.7176 C2 
??4?+6? 0.8600 0.8888 C3 
??6?+4? 0.8797 0.8938 C2 
??3?+7? 0.8514 0.8827 C3 
W1 
??7?+3? 0.8431 0.8825 C3 
??10?+10? 0.9031 0.8962 C2 
??3?+3? 0.8487 0.8460 C2 
??4?+6? 0.8982 0.8873 C3 
??6?+4? 0.8480 0.8772 C2 
??3?+7? 0.8669 0.8359 C3 
W6 
??7?+3? 0.8982 0.8895 C3 
pseudo- 
words 
Morpheme 
ID 
sample 
number 
pseudo- 
words 
Morpheme
ID 
sample
number
34466 1040 84323 591
71345 662 12751 484W1 
31796 2101 
W4 
52915 829
71072 1296 53333 274
78031 1043 29053 1153W2 
48469 315 
W5 
75941 238
7464 4389 39945 430
77375 469 71335 308W3 
23077 865 
W6 
51491 158
Table 5.1 The number of experimental samples 
obvious for the each meaning of real ambiguous 
word, because some meanings are used in oral 
language. But that never or seldom appears in 
experimental corpus.  
The statistics can uncover this linguistic 
phenomenon. We find that the meaning of the most 
percentage of ambiguous word showing in the 
corpus is 83.54% on the whole percentage of each 
meaning. That illustrates the distribution of each 
meaning has a great disparity in real ambiguous 
word. Seeing that condition, to differentiate the 
meaning of ambiguous word is harder than that of 
real ambiguous word absolutely.  
5.1 The analysis and discussion of the first 
experiment 
Table 4.1 records the results of close and open 
tests in detail and the training materials to get these 
results.  
Seeing from the experimental results, the correct 
percentage reaches 89.51% most (ambiguous word 
W1 and W6) in open test of WSD, and 85.74% the 
least (ambiguous word W3). 
The relationship of correct percentage and the 
extent of training corpus can be deduced from the 
experimental results of table 4.2, 4.3 and 4.4. 
The larger the extent of training corpus (the 
number of training sample?, the larger the result of 
close test. It is obvious to see that from C1 to C2. 
From C2 to C3 one or two experimental results 
fluctuate more or less.  
With the growing of training sample, the 
experimental results of open test increase steadily, 
except ambiguous word W2 (a little bit difference).  
The experimental data prove the growing of 
training samples rise the correct percentage. 
However, when the rising reaches to a certain 
degree, more rising is not good for the improvement 
of model. What?s more, the effect of noise is more 
and more remarkable. That decreases the model?s 
ability of differentiation in a certain degree. On the 
other hand, after the growing of training corpus, the 
linguistic phenomenon around ambiguities is richer 
and richer, more and more complex. That makes it 
harder to determine the meaning.  
5.2 The analysis and discussion of the second 
experiment 
This article emphasizes on the collecting method 
of contextual feature words in experiment two, in 
other words, the effect that the different values of M 
and N influence the model of BP network. The 
experimental results (table 4.1 and 4.5) tell us that 
the context windows influence the correct 
percentage heavily. The correct percentage 
increases almost by leaps and bounds from (?3?+ 3) 
to??5?+5?. The discrepancy is obvious despite 
close test or open test. The correct percentage 
increase again to??10?+ 10?, in which the close test 
of ambiguous word W6 is more than 90% and 
89.62% the close test, with the exception of W1 
which open test is slightly special. That illustrates 
the more widely the context windows open, the 
more the effective information is caught to benefit 
the WSD more. 
Comparing the four feature methods of collection, 
including ??3?+ 7?,??7?+ 3?,??4?+ 6?
and??6?+ 4? with??5?+ 5?, the number of feature 
words besides the ambiguous word is various and 
the experimental results (table 4.1 and 4.5) are not 
same, although the windows are same. Among them, 
the correct percentage of ??5?+ 5?is the highest. 
And that of??4?+ 6?and??6?+ 4?is better than 
that of??3?+ 7?and??7?+ 3?a bit. That shows 
the more balanceable the feature words besides 
ambiguous word, the more advantageous to judge 
meaning, and the better the experimental results.  
In addition, some experimental results of open 
test are better than that of close test. The main 
reason is the experimental corpus of open test is 
smaller than training corpus. So the contextual 
meanings of ambiguous word in experimental 
corpus are rather explicit. Thereby, that explains 
why should be this kind of experimental result.  
5.3 Conclusions 
Considering the analysis of experimental data, 
the conclusions are as following: 
First, the artificial model of neural network 
established in this article has good ability of 
differentiation for Chinese meaning.  
Next, higher correct percentage of WSD stems 
from the large enough corpus. 
At last, the larger the windows of contextual 
feature words, the more the effective information. 
At the same time, the more balanceable the number 
of feature words beside the ambiguous word, the 
more beneficial that for WSD.  
6 Concluding remarks 
Although the BP network is a classified model 
applied extensively, the report of research on WSD 
about it is seldom. Especially the report about the 
Chinese WSD is less, and only one report (Zhang, 
2001) is available in internal reports.  
Zhang (2001) uses 96 semantic classes to instead 
the all words in training corpus according to the 
TongyiciCilin. The input model is the codes of 
semantic class of contextual words and ambiguities. 
The experiment of WSD merely makes for one 
phrase ? ?(cai2liao4) in this document and the 
correct percentage of open test is 80.4%. ? ? 
has 3 meanings and that is similar to the 
ambiguities structured in my article.  
Using BP for Chinese WSD, the key point and 
difficulty are on the determination of input model. 
The performance of input model may influence the 
construction of BP network and the output result 
directly.   
We make the experiment on the input of BP 
network many times and finally find the input 
model introduced as above (table 3.1) which test 
result is satisfied.  
Acknowledgements This work was supported by 
the National Natural Science Foundation of China 
(Grant No. 60203020). 
 
Reference 
Lu Song, Bai Shuo, et al 2001. Supervised word 
sense disambiguation bassed on Vector Space 
Model, Journal of Comouter Research & 
Development, 38(6): 662-667. 
Pedersen. 2001. Lexical semantic ambiguous word 
resolution with bigram-based decision trees, In 
Proceedings of the Second International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 157-168, Mexico City, February. 
Escudero, G., Marquez,L., et al 2000. Naive Bayes 
and examplar based approaches to word sense    
disambiguation revisited. In Proceedings of the 14th 
Europear Conference on Artificial Intelligence, 
ECAI. 
Adam,L.B. 1996. A maximum entropy approach to 
natural language proceeding. Computational 
Linguistics, 22(1):39-71. 
Li, J. Z. 1999. An improved maximum language 
and its application. Journal of software, 3:257-263. 
Yarowsky, D. Word sense disambiguation using 
statistical models of Roget?s categories trained on 
large corpora. In: Zampolli, A., ed. Computation 
Linguistic?92. Nantas: Association for 
Computational Linguistics, 1992. 454~460.  
Hinrich Sch?tze, 1998. Automatic word sense 
discrimination. Computational Linguistics, 24(1): 
97-124. 
Lu Song., Bai Shuo. 2002. An unsupervised 
approach to word sense disambiguation based on 
sense-word in vector space model. Journal of 
Software. 13(06):1082-08 
Hinrich Sch?tze. 1992. Context space. In AAAI 
Fall Symposium on Probabilistic Approaches to 
Natural Language, pages 113?120, Cambridge, 
MA. 
Lu Song Bai Shuo. 2001. Quantitative Analysis of 
Context Field. In Natural Language Processing, 
CHINESEJ.COMPUTERS, 24(7) , 742-747 
Zhang Guoqing, Zhang Yongkui. 2001. A 
Neural-network Based Word Sense Disambiguation 
Method. Computer Engineering, 27(12). 
A New Chinese Natural Language Understanding Architecture
Based on Multilayer Search Mechanism
Wanxiang Che Ting Liu Sheng Li
School of Computer Science and Technology
Harbin Institute of Technology
P.O. Box 321, HIT
Harbin China, 150001
{car, tliu, ls}@ir.hit.edu.cn
Abstract
A classical Chinese Natural Language Under-
standing (NLU) architecture usually includes
several NLU components which are executed
with some mechanism. A new Multilayer Search
Mechanism (MSM) which integrates and quan-
tifies these components into a uniform multi-
layer treelike architecture is presented in this
paper. The mechanism gets the optimal re-
sult with search algorithms. The components
in MSM affect each other. At last, the per-
formance of each component is enhanced. We
built a practical system ? CUP (Chinese Under-
standing Platform) based on MSM with three
layers. By the experiments on Word Segmen-
tation, a better performance was achieved. In
theory the normal cascade and feedback mech-
anism are just some special cases of MSM.
1 Introduction
At present a classical Chinese NLU architec-
ture usually includes several components, such
as Word Segmentation (Word-Seg), POS Tag-
ging, Phrase Analysis, Parsing, Word Sense Dis-
ambiguation (WSD) and so on. These compo-
nents are executed one by one from lower layers
(such as Word-Seg, POS Tagging) to higher lay-
ers (such as Parsing, WSD) to form a kind of
cascade mechanism. But when people build a
NLU system based on these complex language
analysis, it is a very serious problem since the
errors of each layer component are multiplied.
With more and more analysis components, the
final result becomes too bad to be applicable.
Another problem is that the components in
the system affect each other when people build a
practical but toy NLU system. Here the toy sys-
tem means that each component is ideal enough
with perfect input. But in fact, on the one hand
the lower layer components need the informa-
tion of higher layer components; on the other
hand the incorrect analysis of lower layers must
reduce the accuracy of higher layers. In Chinese
Word-Seg component, many segmentation am-
biguities which cannot be solved using only lexi-
cal information. In order to improve the perfor-
mance of Word-Seg, we have to use some syntax
and even semantic information. Without cor-
rect Word-Seg results, however the syntax and
semantic parser cannot obtain a correct analy-
sis. It is a chain debts problem.
People have tried to solve the error-multiplied
problem by integrating multi-layers into a uni-
form model (Gao et al, 2001; Nagata, 1994).
But with the increasing number of integrated
layers, the model becomes too complex to build
or solve.
The feedback mechanism (Wu and Jiang,
1998) helps to use the information of high lay-
ers to control the final result. If the analysis
at feedback point cannot be passed, the whole
analysis will be denied. This mechanism places
too much burden on the function of feedback
point. This leads to the problems that a correct
lower layer result may be rejected or an error
result may be accepted.
We propose a new Multilayer Search Mecha-
nism (MSM) to solve the problems mentioned
above. Based on the mechanism, we build a
practical Chinese NLU platform ? CUP (Chi-
nese Understanding Platform). Section 2 intro-
duces the background and architecture of the
new mechanism and how to build it up. Exper-
imental results with CUP is given in Section 3.
In Section 4, we discuss why the new mechanism
gets better results than the old ones. Conclu-
sions and the some future work follow in Sec-
tion 5.
2 Multilayer Search Mechanism
The novel Multilayer Search Mechanism (MSM)
integrates and quantifies NLU components into
a uniform multilayer treelike platform, such as
Word-Seg, POS Tagging, Parsing and so on.
These components affect each other by comput-
ing the final score and then get better results.
2.1 Background
Considering a Chinese sentence, the sen-
tence analysis task can be formally defined
as finding a set of word segmentation se-
quence (W ), a POS tagging sequence (POS),
a syntax dependency parsing tree (DP ) and
so on which maximize their joint probability
P (W,POS,DP, ? ? ?). In this paper, we assume
that there are only three layers W , POS and
DP in MSM. It is relatively straightforward,
however, to extend the method to the case for
which there are more than three layers. There-
fore, the sentence analysis task can be described
as finding a triple < W,POS,DP > that max-
imize the joint probability P (W,POS,DP ).
< W,POS,DP >= arg max
W,POS,DP
P (W,POS,DP )
The joint probability distribution
P (W,POS,DP ) can be written in the fol-
lowing form using the chain rule of probability:
P (W,POS,DP )
=P (W )P (POS|W )P (DP |W,POS)
Where P (W ) is considered as the probabil-
ity of the word segmentation layer, P (POS|W )
is the conditional probability of POS Tag-
ging with a given word segmentation result,
P (DP |W,POS) is the conditional probability
of a dependency parsing tree with a given word
segmentation and POS Tagging result similarly.
So the form of < W,POS,DP > can be trans-
formed into:
< W,POS,DP >
= arg max
W,POS,DP
P (W,POS,DP )
= arg max
W,POS,DP
P (W )P (POS|W )P (DP |W,POS)
= arg max
W,POS,DP
logP (W ) + logP (POS|W )
+ logP (DP |W,POS)
= arg min
W,POS,DP
? logP (W )? logP (POS|W )
? logP (DP |W,POS)
We consider that each inversion of probability?s
logarithm at the last step of the above equation
is a score given by a component (Such as Word-
Seg, POS Tagging and so on). So at last, we find
an n-tuple < W,POS,DP, ? ? ? > that minimizes
the last score Sn of a sentence analysis result
with n layers. Sn is defined as:
Sn = s1 + s2 + ? ? ?+ sn (1)
si denotes the score of the ith layer compo-
nent.
2.2 The Architecture of Multilayer
Search Mechanism
Because there are lots of analysis results at each
layer, it?s a combinatorial explosion problem to
find the optimal result. Assuming that each
component produces m results for an input on
average and there are n layers in a NLU system,
the final search space is mn. With the increas-
ing of n, it?s impossible for a system to find the
optimal result in the huge search space.
The classical cascade mechanism uses a
greedy algorithm to solve the problem. It only
keeps the optimal result at each layer. But if
it?s a fault analysis result for the optimal result
at a layer, it?s impossible for this mechanism to
find the final correct analysis result.
To overcome the difficulty, we build a new
Multilayer Search Mechanism (MSM). Different
from the cascade mechanism, MSM maintains a
number of results at each component, so that
the correct analysis should be included in these
results with high probability. Then MSM tries
to use the information of all layer components
to find out the correct analysis result. Different
from the feedback mechanism, the acceptance
of an analysis is not based on a higher layer
components alone. The lower layer components
provide some information to help to find the
correct analysis result as well.
According to the above idea, we design the
architecture of MSM with multilayer treelike
structure. The original input is root and the
several analysis results of the input become
branches. Iterating this progress, we get a big-
ger analysis tree. Figure 1 gives an analysis ex-
ample of a Chinese sentence ????????
??? (He likes beautiful flowers). For the in-
put sentence, there are several Word-Seg results
with scores (the lower the better). Then for each
of Word-Seg results, there are several POS Tag-
ging results, too. And for each of POS Tagging
result, the same thing happens. So we get a big
tree structure and the correct analysis result is a
path in the tree from the root to the leaf except
for there is no correct analysis result in some
analysis components.
A search algorithm can be used to find out the
correct analysis result among the lowest score in
the tree. But because each layer cannot give the
exact score in Equation 1 as the standard score
and the ability of analysis are different with
different layers, we should weight every score.
Then the last score is the linear weighted sum
(Equation 2).
Sn = w1s1 + w2s2 + ? ? ?+ wnsn (2)
si denotes the score of the ith layer compo-
nent which we will introduce in Section 3; wi
denotes the weight of the ith layer components
which we will introduce in the next section.
In order to get the optimal result, all kinds
of tree search algorithms can be used. Here
the BEST-FIRST SEARCH Algorithm (Rus-
sell and Norvig, 1995) is used. Figure 2 shows
the main algorithm steps.
POS Tag
Word-Seg
??
?????????
56.2? ?? ?? ? ??? 87.3? ? ?? ? ? ???
108.3? ?? ?? ? ???  r      v        a      u      n
187.4? ?? ?? ? ???  b      v        a      u      n
Figure 1: An Example of Multilayer Search
Mechanism
1. Add the initial node (starting point) to the
queue.
2. Compare the front node to the goal state. If
they match then the solution is found.
3. If they do not match then expand the front
node by adding all the nodes from its links.
4. If all nodes in the queue are expanded then the
goal state is not found (e.g.there is no solution).
Stop.
5. According to Equation 2 evaluate the score of
expanded nodes and reorder the nodes in the
queue.
6. Go to step 2.
Figure 2: BEST-FIRST SEARCH Algorithm
2.3 Layer Weight
We should find out a group of appropriate
w1, w2, ? ? ? , wn in Equation 2 to maximize the
number of the optimal paths in MSM which can
get the correct results. They are expressed by
W ?.
W ? = arg max
W
ObjFun(minSn) (3)
Here W ? is named as Whole Layer Weight.
ObjFun(?) denotes a function to value the re-
sult that a group ofW can get. Here we can con-
sider that the performance of each layer is pro-
portional to the last performance of the whole
system in MSM. So it maybe the F-Score of
Word-Seg, precision of POS Tagging and so on.
minSn returns the optimal analysis results with
the lowest score.
Here, the F-Score of Word-Seg can be defined
as the harmonic mean of recall and precision of
Word-Seg. That is to say:
Seg.F -Score = 2 ? Seg.Pre ? Seg.RecSeg.Pre+ Seg.Rec
Seg.Pre = #words correctly segmented#words segmented
Seg.Rec = #words correctly segmented#words in input texts
Finding out the most suitable group of W
is an optimization problem. Genetic Algo-
rithms (GAs) (Mitchell, 1996) is just an adap-
tive heuristic search algorithm based on the evo-
lutionary ideas of natural selection and genetics
to solve optimization problems. It exploits his-
torical information to direct the search into the
region of better performance within the search
space.
To use GAs to solve optimization prob-
lems (Wall, 1996) the following three questions
should be answered:
1. How to describ genome?
2. What is the objective function?
3. Which controlling parameters to be se-
lected?
A solution to a problem is represented as a
genome. The genetic algorithm then creates a
population of solutions and applies genetic op-
erators such as mutation and crossover to evolve
the solutions in order to find the best one(s) af-
ter several generations. The numbers of popula-
tion and generation are given by controlling pa-
rameters. The objective function decides which
solution is better than others.
In MSM, the genome is just the group of W
which can be denoted by real numbers between
0 and 1. Because the result is a linear weighted
sum, we should normalize the weights to let w1+
w2+ ? ? ?+wn = 1. The objective function is just
ObjFun(?) in Equation 3. Here the F-Score
of Word-Seg is used to describe it. We set the
genetic generations as 10 and the populations in
one generation as 30. The Whole Layer Weight
shows in the row of WLW in Table 4. The F-
Score of Word-Seg shows as Table 3.
We can see that the Word-Seg layer gets an
obviously large weight. So the final result is
inclined to the result of Word-Seg.
2.4 Self Confidence
Our analysis indicates that the method of
weighting a whole layer uniformly cannot re-
flect the individual information of each sen-
tence to some component. So the F-Score of
Word-Seg drops somewhat comparing with us-
ing Only Word-Seg. For example, the most
sentences which have ambiguities in Word-Seg
component are still weighted high with Word-
Seg layer weight. Then the final result may still
be the same as the result of Word-Seg compo-
nent. It is ambiguous, too. So we must use a
parameter to decrease the weight of a compo-
nent with ambiguity. It is used to describe the
analysis ability of a component for an input. We
name it as Self Confident (SC) of a component.
It is described by the difference between the first
and the second score of a component. Then the
bigger SC of a component, the larger weight of
it.
There are lots of methods to value the differ-
ence between two numbers. So there are many
kinds of definitions of SC. We use A and B to
denote the first and the second score of a compo-
nent respectively. Then the SC can be defined
as B?A, BA and so on. We must select the bet-ter one to represent SC. The better means that
a method which gets a lower Error Rate with a
threshold t? which gets the Minimal Error Rate.
t? = arg min
t
ErrRate(t)
ErrRate(t) denotes the Error Rate with the
threshold t. An error has two definitions:
? SC is higher than t but the first result is
fault
? SC is lower than t but the first result is
right
Then the Error Rate is the ratio between the
error number and the total number of sentences.
Table 2 is the comparison list between differ-
ent definitions of SC and their Minimal Error
Rate of Word-Seg. By this table we select B?A
as the last SC because it gets the minimal Min-
imal Error Rate within the different definitions
of SC.
SC is added into Equation 2 to describe the
individual information of each sentence inten-
sively. Equation 4 shows the new score method
of a path.
Sn = w1sc1s1 + w2sc2s2 + ? ? ?+ wnscnsn (4)
sci denotes the SC of a component in the ith
layer.
3 Experimental Results
3.1 Score of Components
We build a practical system CUP (Chinese
Understanding Platform) based on MSM with
three layers ? Word-Seg, POS Tagging and
Parsing. Each component not only provides the
n-best analysis result, but also the score of each
result.
In the Word-Seg component, we use the uni-
gram model (Liu et al, 1998) to value different
results of Word-Seg. So the score of a result is:
ScoreWord?Seg = ? logP (W ) = ?
?
logP (wi)
wi denotes the ith word in the Word-Seg re-
sult of a sentence.
In the POS Tagging component the classical
Markov Model (Manning and Schu?tze, 1999) is
used to select the n-best POS results of each
Word-Seg result. So the score of a result is:
ScorePOS =? logP (POS|W )
=? log P (W |POS)P (POS)P (W )
=?
?
logP (wi|ti)?
?
logP (ti|ti?1)
+ logP (W )
ti denotes the POS of the ith word in a Word-
Seg result of a sentence.
In the Parsing component, we use a Chinese
Dependency Parser System developed by HIT-
IRLab1. The score of a result is:
ScoreParsing =? logP (DP |W,POS)
=? log P (W,POS,DP )P (W,POS)
=?
?
logP (lij)
+ logP (W,POS)
lij denotes a link between the ith and jth
word in a Word-Seg and POS Tagging result
of a sentence.
Table 1 gives the one and five-best results of
each component with a correct input. The test
data comes from Beijing Univ. and Fujitsu Chi-
nese corpus (Huiming et al, 2000). The F-Score
is used to value the performance of the Word-
Seg, Precision to POS Tagging and the correct
rate of links to Parsing.
Table 1: The five-best results of each compo-
nent
1-best 5-best
Word-Seg 87.83% 94.45%
POS Tag 85.34% 93.28%
Parsing 80.25% 82.13%
3.2 Self Confidence Selection
In order to select a better SC, we test all kinds of
definition form to calculate their Minimal Error
Rate. For example B?A, BA and so on. A and Bdenote the first and the second score of a com-
ponent respectively. Table 2 shows the relation-
ship between definition forms of SC and their
Minimal Error Rate. Here, we experimented
with the first and the second Word-Seg results
of more than 7100 Chinese sentences.
3.3 F-Score of Word-Seg
The result of Word-Seg is used to test our
system?s performance, which means that the
ObjFun(?) returns the F-Score of Word-Seg.
There are 1,500 sentences as training data
and 500 sentences as test data. Among these
data about 10% sentences have ambiguities and
the others come from Beijing Univ. and Fujitsu
1The Parser has not been published still.
Chinese corpus (Huiming et al, 2000). In CUP
the five-best results of each component are se-
lected. Table 3 lists the F-Score of Word-Seg.
They use Only Word-Seg (OWS), Whole Layer
Weight (WLW), SC (SC) and FeedBack mecha-
nism (FB) separately. Using the feedback mech-
anism means that the last analysis result of a
sentence is decided by the Parsing. We select
the result which has the lowest score of Pars-
ing. Table 4 shows the weight distributions in
WLW and SC weighting methods.
3.4 The Efficiency of CUP
The efficiency test of CUP was done with 7112
sentences with 20 Chinese characters averagely.
It costs 58.97 seconds on a PC with PIV 2.0
CPU and 512M memory. The average cost of a
sentence is 0.0083 second.
4 Discussions
According to Table 1, we can see that the per-
formance of each component improved with the
increasing of the number of results. But at the
same time, the processing time must increase.
So we should balance the efficiency and effec-
tiveness with an appropriate number of results.
Thus, it?s more possible for CUP to find out the
correct analysis than the original cascade mech-
anism if we can invent an appropriate method.
We define SC as B ? A which gets the mini-
mal Minimal Error Rate with the analysis of the
Table 2: SC and Minimal Error Rate
Definition Form of SC Minimal
Error Rate
1
A ? 1B 23.85%B ?A 21.07%
B
A 23.98%B
A ? AB 23.98%B?A
length of a sentence 24.12%
B?A
length of a sentence+100 23.71%
Table 3: F-Score of Word-Seg
OWS WLW SC FB
F-Score 86.99% 85.80% 88.13% 80.72%
Table 4: Layer Weight
1-layer 2-layer 3-layer
In WLW 0.84 0.12 0.04
In SC 0.44 0.40 0.16
Table 2. Take the case of Word Segmentation:
B ?A =
?
i
logP (wAi )?
?
j
logP (wBj )
It?s just the difference between logarithms of
different word results? probability of the first
and the second result of Word Segmentation.
Table 3 shows that MSM using SC gets a bet-
ter performance than other methods. For a Chi-
nese sentence ???????????. (There
are some drinks under the table). The CUP
gets the correct analysis ? ???/n ?/nd ?/v
?/u ?/m ?/q ?/n ?/w?. But the cascade
and feedback mechanism?s result is ???/n ?
?/v ?/u ?/m ?/q ?/n ?/w?.
The cascade mechanism uses the Only Word-
Seg result. In this method P (??) is more
than P (?) ? P (?). At the same time, the
wrong analysis is a grammatical sentence and
is accepted by Parsing. These create that these
two mechanisms cannot get the correct result.
But the MSM synthesizes all the information of
Word-Seg, POS Tagging and Parsing. Finally
it gets the correct analysis result.
Now, CUP integrates three layers and its effi-
ciency is high enough for practical applications.
5 Conclusions and Future Work
A new Chinese NLU architecture based on Mul-
tilayer Search Mechanism (MSM) integrates al-
most all of NLU components into a uniform
multilayer treelike platform and quantifies these
components to use the search algorithm to find
out the optimal result. Thus any component
can be added into MSM conveniently. They
only need to accept an input and give several
outputs with scores. By experiments we can see
that a practical system ? CUP based on MSM
improves the performance of Word-Seg to a cer-
tain extent. And its efficiency is high enough for
most practical applications.
The cascade and the feedback mechanism are
JUST the special cases of MSM. If greedy algo-
rithm is used at each layer to expand the result
with the lowest score, MSM becomes the cas-
cade mechanism. If the weight of each layer
except the feedback point is set 0, the MSM be-
comes the feedback mechanism.
In the future we are going to add the Phrase
Analysis, WSD (Word Sense Disambiguation)
and Semantic Analysis components into CUP,
because it is impossible to analyze some sen-
tences correctly without semantic understand-
ing and the Phrase Analysis helps to en-
hance the performance of Parsing. At last,
CUP becomes a whole Chinese NLU platform
with Word-Seg, POS Tagging, Phrase Analy-
sis, Parsing, WSD and Semantic Analysis, six
components from lower layers to higher layers.
Under the framework of MSM, it becomes very
easy to add these components.
With the increasing of layers the handle speed
must decrease. So some heuristic search algo-
rithms will be used to improve the speed of
searching while enhancing the speed of each
component. Under the MSM framework, we can
do these easily.
The performance of each component should
be improved in the future. At least, it is impos-
sible for MSM to find out the correct analysis
result if there is a component which cannot give
a correct result within n-best results with a cor-
rect input. In addition, we are going to evalu-
ate the performance of each component not just
Word-Seg only.
6 Acknowledgements
We thank Liqi Gao and Zhuoran Wang provide
the Word Segmentation tool, Wei He provide
the POS Tagging tool and Jinshan Ma provide
the Parser tool for us. We acknowledge Dekang
Lin for his valuable comments on the earlier ver-
sions of this paper. This work was supported by
NSFC 60203020.
References
Shan Gao, Yan Zhang, Bo Xu, ChengQing
Zong, ZhaoBing Han, and RangShen Zhang.
2001. The research on integrated chinese
words segmentation and labeling based on tri-
gram statistic model. In Proceedings of IJCL-
2001, Tai Yuan, Shan Xi, China.
Duan Huiming, Song Jing, Xu Guowei,
Hu Guoxin, and Yu Shiwen. 2000. The de-
velopment of a large-scale tagged chinese cor-
pus and its applications. Applied Linguistics,
(2):72?77.
Ting Liu, Yan Wu, and Kaizhu Wang. 1998.
The problem and algorithm of maximal prob-
ability word segmentation. Journal of Harbin
Institute of Technology, 30(6):37?41.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
Melanie Mitchell. 1996. An Introduction to
Genetic Algorithms. The MIT Press, Cam-
bridge, Massachusetts.
Masaaki Nagata. 1994. A stochastic japanese
morphological analyzer using a forward-dp
backward-A* n-best search algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics, pages
201?207.
Stuart Russell and Peter Norvig. 1995. Artifi-
cial Intelligence: A Modern Approach. Pren-
tice Hall Series in Artificial Intelligence, En-
glewood Cliffs, NJ, USA.
Matthew Wall. 1996. GAlib: A C++ Li-
brary of Genetic Algorithms components.
http://lancet.mit.edu/ga/.
Andi Wu and Zixin Jiang. 1998. Word segmen-
tation in sentence analysis. In Proceedings
of the 1998 International Conference on Chi-
nese Information Processing, pages 169?180,
Beijing, China.
Aligning Bilingual Corpora Using Sentences Location Information* 
Li Weigang   Liu Ting   Wang Zhen   Li Sheng 
Information Retrieval Lab, Computer Science & Technology School, 
Harbin Institute of Technology 321# 
Harbin, China, 150001 
{LEE, tliu, wangzhen, lis}@ir.hit.edu.cn 
 
 
Abstract 
Large amounts of bilingual resource on the Internet 
provide us with the probability of building a large 
scale of bilingual corpus. The irregular characteris-
tics of the real texts, especially without the strictly 
aligned paragraph boundaries, bring a challenge to 
alignment technology. The traditional alignment 
methods have some difficulties in competency for 
doing this. This paper describes a new method for 
aligning real bilingual texts using sentence pair 
location information. The model was motivated by 
the observation that the location of a sentence pair 
with certain length is distributed in the whole text 
similarly. It uses (1:1) sentence beads instead of 
high frequency words as the candidate anchors. 
The method was developed and evaluated through 
many different test data. The results show that it 
can achieve good aligned performance and be ro-
bust and language independent. It can resolve the 
alignment problem on real bilingual text. 
1 Introduction 
There have been a number of papers on aligning 
parallel texts at the sentence level in the last cen-
tury, e.g., (Brown et al 1991; Gale and Church, 
1993; Simard et al 1992; Wu DeKai 1994). On 
clean inputs, such as the Canadian Hansards and 
the Hong Kang Hansards, these methods have been 
very successful.  
(Church, Kenneth W, 1993; Chen, Stanley, 1993) 
proposed some methods to resolve the problem in 
noisy bilingual texts. Cognate information between 
Indo-European languages pairs are used to align n- 
 
 
oisy texts. But these methods are limited when 
aligning the language pairs which are not in the 
same genre or have no cognate information. (Fung, 
1994) proposed a new algorithm to resolve this 
problem to some extent. The algorithm uses fre-
quency, position and recency information as fea-
tures for pattern matching. (W. Bin, 2000) adapted 
the similar idea with (Fung, 1994) to align special 
domain bilingual texts. Their algorithms need 
some high frequency word pairs as anchor points. 
When processing the texts that include less high-
frequency words, these methods will perform 
weakly and with less precision because of the scar-
city of the data problem.  
 (Haruno and Yamazaki, 1996) tried to align 
short texts without enough repeated words in struc-
turally different languages, such as English and 
Japanese. They applied the POS information of 
content words and an online dictionary to find 
matching word pairs. But this is only suitable for 
the short texts. 
The real text always includes some noisy infor-
mation. It has the following characteristics as fol-
lows: 
1) There are no strict aligned paragraph bounda-
ries in real bilingual text;  
2) Some paragraphs may be merged into a larger 
paragraph because of the translator?s individual 
idea;  
3) There are many complex translation patterns 
in real text; 
4)  There exist different styles and themes; 
5) Different genres have different inherent char-
acteristics.  
The tradition approaches to alignment fall into 
two main classes: lexical and length. All these 
methods have limitations when facing the real text 
according to the characteristics mentioned above. 
    * This research was supported by National Natural 
Science Foundation (60203020) and Science Founda-
tion of Harbin Institute of technology (hit.2002.73). 
We proposed a new alignment method based on 
the sentences location information. Its basic idea is 
that the location of a sentence pair with certain 
length is distributed in the whole text similarly. 
The local and global location information of a sen-
tence pair is fully combined together to determine 
the probability with which the sentence pair is a 
sentence bead. 
In the first of the following sections, we describe 
several concepts. The subsequent section reports 
the mathematical model of our alignment approach. 
Section 4 presents the process of anchors selection 
and algorithm implementation is shown in section 
5. The experiment results and discussion are shown 
in section 6. In the final section, we conclude with 
a discussion of future work. 
2 Several conceptions 
1) Alignment anchors: (Brown, 1991) firstly in-
troduced the concept of alignment anchors when 
he aligned Hansard corpus. He considered that the 
whole texts were divided into some small frag-
ments by these alignment anchors. Anchors are 
some aligned sentence pairs. 
2) Sentence bead:  and at the same time, (Brown, 
1991) called each aligned sentence pair a sentence 
bead. Sentence bead has some different styles, 
such as (0:1), (1:0), (1:1), (1:2), (1: more), (2:1), 
(2:2), (2: more), (more: 1), (more: 2), (more: more).  
3) Sentence pair: Any two sentences in the bilin-
gual text can construct a sentence pair. 
4) Candidate anchors: Candidate anchors are 
those that can be possible alignment anchors. In 
this paper, all (1:1) sentence beads are categorized 
as candidate anchors. 
3 Mathematical Model of Alignment 
The alignment process has two steps: the first 
step is to integrate all the origin paragraphs into 
one large paragraph. This can eliminate the prob-
lem induced by the vague paragraph boundaries. 
The second step is the alignment process. After 
alignment, the bilingual text becomes sequences of 
translated fragments. The unit of a fragment can be 
one sentence, two sentences or several sentences. 
The traditional alignment method can be used with 
the fragment with several sentences to improve the 
alignment granularity. In this paper the formal de-
scription of the alignment task was given by ex-
tending the concepts of bipartite graph and 
matching in graph theory. 
3.1 Bipartite graph 
Bipartite graph: Here, we assumed G to be an 
undirected graph, then it could be defined as G=<V, 
E>. The vertex+ set of V has two finite subsets: V1 
and V2, also V1 ? V2?V, V1?V2??. Let E be a 
collection of pairs, when e?E, then e={vi, vj}, 
where vi?V1,vj?V2. The triple G was described 
as, G=<V1, E, V2>, called bipartite graph. In a bi-
partite graph G, if each vertex of V1 is joined with 
each vertex of V2, or vice versa, here an edge 
represents a sentence pair. The collection E is the 
set of all the edges. The triple G=<V1, E, V2> is 
called complete bipartite graph. We considered 
that: |V1|?m, |V2|?n, where the parameters m and 
n are respectively the elements numbers of V1 and 
V2. The complete bipartite graph was usually ab-
breviated as Km, n (as shown in figure 1). 
 
 
 
 
 
 
 
3.2 Matching 
Matching: Assuming G?<V1, E, V2> was a bi-
partite graph. A matching of G was defined as M, a 
subset of E with the property that no two edges of 
M have a common vertex. 
3.3 Best Alignment Matching 
The procedure of alignment using sentence loca-
tion information can be seen as a special matching. 
We defined this problem as ?Best Alignment 
Matching? (BAM). 
BAM: If M=<S, EM, T> is a best alignment 
matching of G=<S, E, T>, then M must meet the 
following conditions:  
1) All the vertexes in the complete bipartite 
graph are ordered; 
2) The weight of any edges in EM d(si, tj) has: 
d(si, tj)< D (where D is alignment threshold); at the 
same time, there are no edges {sk, tr} which made 
k<i and r>j, or k>i and r<j; 
Figure 1 K3,3 complete bipartite graph 
3) If we consider: |S|=m and |T|=n, then the edge 
{sm, tn} belonged to EM; 
Best alignment matching can be attained by 
searching for the smallest weight of edge in collec-
tion E, until the weight of every edge d(si, tj) is 
equal or more than the alignment threshold D. 
Generally, the alignment threshold D is determined 
according to experience because different texts 
have different styles. 
 
 
 
 
 
 
 
 
 
If each sentence in the text S (or T) corre-
sponds with a vertex in V1(or V2), the text S or T 
can be denoted by S(s1, s2, s3,?si, ?sj, ?sm) or 
T(t1, t2, t3?ti, ?tj, ?tn). Considering the form 
merely, each element in S combined with any ele-
ment in T can create a complete bipartite graph. 
Thus the alignment task can be seen as the process 
of searching for the BAM in the complete bipartite 
graph. As shown in figure 2, the edge e = {si, tj} 
belongs to M; this means that the i-th sentence in 
text S and the j-th sentence in text T can make an 
alignment anchor. Each edge is corresponding to 
an alignment value. In order to ensure the bilingual 
texts are divided with the same fragment number, 
we default that the last sentence in the bilingual 
text is aligned. That is to say, {sm, tn} E? M was 
correct, if |S|=m and |T|=n in the BAM mathemati-
cal model.  
We stipulated the smaller the alignment value is, 
the more similar the sentence pair is to a candidate 
anchor. The smallest value of the sentence pair is 
found from the complete bipartite graph. That 
means the selected sentence pair is the most prob-
able aligned (1:1) sentence bead. Alignment proc-
ess is completed until the alignment anchors 
become saturated under alignment threshold value. 
Sentence pairs extracted from all sentence pairs 
are seen as alignment anchors. These anchors di-
vide the whole texts into short aligned fragments. 
The definition of BAM ensures that the selected 
sentence pairs cannot produce cross-alignment er-
rors, and some cases of (1: more) or (more: 1) 
alignment fragments can be attained by the frag-
ments pairs between two selected alignment an-
chors. 
4 Anchors Selection during Alignment 
All (1:1) sentence beads are extracted from dif-
ferent styles of bilingual texts. The distribution 
states that all of them are similar as presented in 
figure 3. The horizontal axis denotes the sentence 
number in Chinese text, and the vertical axis de-
notes the sentence number in English text. 
-20 0 20 40 60 80 100 120 140 160 180
-20
0
20
40
60
80
100
120
140
160
180
200
 
Se
nte
nc
e N
um
be
r in
 E
ng
lis
h T
ex
t
Sentence Number in Chinese Text
Beads
 
 
 
 
Statistical results show that more than 85% sen-
tence beads are (1:1) sentence beads in bilingual 
texts and their distributions obey an obvious law 
well. (DeKai Wu, 1994) offered that (1:1) sentence 
beads occupied 89% in English-Chinese as well. If 
we select these style sentence beads as candidate 
anchors, the alignment method will be general on 
any other language pairs. The main points of our 
alignment method using sentences location infor-
mation are: locating by the whole text, collocating 
by sentence length and checking by a bilingual 
dictionary. Location information of any sentence 
pair is used fully. Three lengths are used: are sen-
tence length, upper context length above the sen-
tence pair and nether context length below the 
sentence. All this information is considered to cal-
culate the alignment weight of each sentence pair. 
Finally, the sentence pair with high weight will be 
checked by a English-Chinese bilingual dictionary. 
In order to study the relationship between every 
sentence pair of {si, tj}, four parameters are defined: 
Whole text length ratio: P0 = Ls / Lt; 
Upper context length ratio: Pu[i, j] = Usi / Utj; 
Nether context length ratio: Pd[i, j] = Dsi / Dtj 
Sentence length ratio: Pl[i, j] = Lsi / Ltj; 
Figure 2 Sketch map of Km, n BAM under 
alignment threshold D 
t1  t2   t3  t4  t5  t6  t7      ti     tj                      tn-2   tn-1  tn  
s1  s2  s3   s4  s5   s6  s7      si   sj                    sm-2  sm-1 sm  
????
????
Figure 3 Distribution of (1:1) sentence beads 
in bilingual texts 
Where  
si    the i-th sentence of S; 
tj    the j-th sentence of T; 
Ls  the length of source language text S; 
Lt   the length of target language text T; 
Lsi  the length of si; 
Ltj  the length of tj; 
Usi  the upper context length above sentence si; 
Utj  the upper context length above sentence tj; 
Dsi  the nether context length below sentence si; 
Dtj  the nether context length below sentence tj; 
Figure 4 illustrates clearly the relationship of all 
variables.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
If si and tj can construct a (1:1) alignment anchor, 
P[i, j] must be less than the alignment threshold, 
where P[i,j] denotes the integrated alignment value 
between si and tj. We assume that the weight coef-
ficient of Pl[i, j] is 1. Only considering the form, 
Pu[i, j] and Pd[i, j] must have the same weight co-
efficient. Here the weight coefficient is set ?. We 
constructed a formal alignment function on every 
sentence pair: 
P[i,j] = 
?(Pu[i, j]-P0)? + (Pl[i, j] -P0)? +?(Pd[i, j] -P0)? 
Where, the parameter ? is the weight coefficient, 
if can adjust the weight of sentence pair length and 
the weight of context lengths well. The longer the 
text is, the more insensitive the effect of the con-
text-length is. So ??s value should increase in order 
to balance the whole proportion. The short text is 
vice versa. In this paper we define: 
?= (Ls/Lsi + Lt/Ltj)/2 
According to the definition of BAM, the smaller 
the alignment function value of P[i, j] is, the more 
the probability of sentence pair {si, tj} being a (1:1) 
sentence bead is. In this paper, we adopt a greedy 
algorithm to select alignment anchors according to 
all the alignment function values of P[i, j] which 
are less than the alignment threshold. This proce-
dure can be implemented with a time complexity 
of O(m*n).  
To obtain further improvement in alignment ac-
curacy requires calculation of the similarity of the 
sentence pairs. An English-Chinese bilingual dic-
tionary is adopted to calculate the semantic simi-
larity between the two sentences in a sentence pair. 
The similarity formula based on a bilingual dic-
tionary is followed: 
 
 
 
Where L| | is the bytes number of all elements, 
Match (T) is (according to English-Chinese dic-
tionary) the English words which have Chinese 
translation in the Chinese sentence, Match (S) is 
the matched Chinese fragments. 
According to the above dictionary check, align-
ment precision is improved greatly. We take a sta-
tistic on all the errors and find that most errors are 
partial alignment errors. Partial alignment means 
that the alignment location is correct, but a half 
pair of the alignment pairs is not integrated. It is 
very difficult to avoid these errors when only tak-
ing into account the sentence location and length 
information. Thus in order to reduce this kind of 
error, we check the semantic similarity of the con-
text-adjacent sentence pairs also. Because these 
pairs could be other alignment patterns, such as 
(1:2) or (2:1), the similarity formulas have some 
difference from the (1:1) sentence pair formula. 
Here, a simple judgement is performed. It is  
shown as: 
If?Lsi-1 * P0 > Ltj-1? 
 
   
 
else 
 
   
 
Here, those alignment anchors whose similari-
ties exceed the similarity threshold based on the 
bilingual dictionary will become the final align-
ment anchors. These final anchors divide the whole 
bilingual texts into aligned fragments. 
Figure 4 Sketch map of variables relationship 
si tjLsi Ltj
Usi Utj
Dsi Dtj
Ls Lt
Chinese Text English Text
s1 t1
tnsm
| ( ) | | ( ) |
| | | |
L Match S L Match TH L S L T
+= +
adjacent
adjacent *)01(
|)(||)(|
LsP
TMatchLSMatchLH +
+=
adjacent
adjacent *)0/11(
|)(||)(|
LtP
TMatchLSMatchLH +
+=
5 Algorithm Implementation 
According to the definition of BAM, the first se-
lected anchor will divide the whole bilingual texts 
into two parts.  We stipulated that the sentences in 
the upper part of source text cannot match any sen-
tence in the nether part of target text. As shown in 
Fig 5, after the first alignment anchors were se-
lected, the second candidate anchors must be se-
lected in the first quadrant or the third quadrant 
and exclusive from the boundary. It is obvious that 
if the candidate anchors exist in the second quad-
rant or fourth quadrant, the cross alignment will 
happen. For example, if the (i, j) is the first se-
lected alignment anchor, and the (i-1, j+1) is the 
second selected alignment anchor, the cross align-
ment appears. We can limit the anchors selection 
field to prevent the cross-alignment errors. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In addition, in order to resolve the problem that 
the first sentence pair is not a (1:1) sentence bead, 
we use a virtual sentence length as the origin 
alignment sentence bead when we initialize the 
alignment process. 
The implementation of alignment algorithm is 
described as followed: 
1) Load the bilingual text and English-Chinese 
dictionary; 
2) Identify the English and Chinese sentences 
boundaries and number each sentence; 
3) Default the last sentence pair to be aligned 
and calculate every sentence pair?s alignment value; 
4) Search for sentence pair that is corresponding 
to the smallest alignment function value; 
5) If the smallest alignment function value is 
less than the alignment threshold and the go to step 
6), and if the smallest value is equal to or more 
than the threshold, then go to step 7); 
6) If the similarity of the sentence pair is more 
than a certain threshold, the sentence pair will be-
come an alignment anchor and divide the bilingual 
text into two parts respectively, then limit the 
search field of the next candidate anchors and go to 
the step 4) 
7) Output the aligned texts, and go to the end. 
6 Results and Discussion 
We use the real bilingual texts of the seven-
teenth chapter in the literary masterpiece ?Wuther-
ing Heights? as our test data. The basic 
information of the data is shown in the table 1. 
 English text size 38.1K 
Chinese text size  25.1K 
English sentence number 273 
Chinese sentence number 277 
Table 1 Basic information of the test data  
In order to verify the validity of our algorithm, 
we implement the classic length-based sentence 
alignment method using dynamic programming. 
The precision is defined:  
Precision = The correct aligned sentence pairs / 
All alignment sentence pairs in bilingual texts 
The comparison results are presented in table 2.  
Method Precision (%) 
Length-based 
alignment method  20.3 
Location-based 
alignment method 87.8 
Table 2 Comparison results between two methods 
Because the origin bilingual texts have no obvi-
ous aligned paragraph boundaries, the error exten-
sion phenomena happen easily in the length-based 
alignment method if the paragraphs are not strictly 
aligned correctly. Its alignment results are so 
weaker that it cannot be used. If we omit all of the 
origin paragraphs information, we merge all the 
paragraphs in the bilingual text into one larger 
paragraph respectively. The length-based align-
ment method rated the precision of 25.4%. This is 
mainly because the English and Chinese languages 
don?t belong to the same genre and have large dif-
ference between the language pairs. But our 
Figure 5 Anchors selection in Bilingual Texts 
j+1
second 
quadrant
first 
quadrant
third 
quadrant
fourth 
quadrant
0 m
n
i
j
i-1
method rated 129 (1:1) sentence pairs as alignment 
anchors which divide the bilingual text into aligned 
fragments. The length-based classic method was 
applied to these aligned fragments and got a high 
precision. Fig 6 shows 129 selected anchors distri-
bution which is in the same trend with all the (1:1) 
sentence beads. Their only difference is the sparse 
extent of the aligned pairs. 
0 50 100 150 200 250 300
0
50
100
150
200
250
300
 
En
gli
sh
 S
en
ten
ce
 N
um
be
r
Chinese Sentence Number
 A
 
Figure 6 Distribution of alignment anchors 
In order to evaluate the adaptability of our 
method, we select texts with different themes and 
styles as the test set. We merge two news bilingual 
texts and two novel texts. The data information is 
show in Table 3. 
Our method is applied on the fixed data and re-
ceives the precision rating of 86.9%. The result 
shows that this alignment method is theme inde-
pendent.  
English text size 63.9K  
Chinese text size  41.5K  
English sentence number 510 
Chinese sentence number 526 
 Table 3 Basic information of the fixed test data 
 (Haruno and Yamazaki, 1996) tried to align 
short texts in structurally different languages, such 
as English and Japanese. In this paper the aligned 
language pairs of English and Chinese belongs to 
structurally different languages as well. Our 
method gets the highest precision in aligning short 
texts. A bilingual news text is selected to be test 
data. The result is shown in table 4. There are two 
aligned sentence error pairs which are induced by 
the lack the corresponding translation.  
English text size 5.6K  
Chinese text size  3.4K  
English sentence number 40 
Chinese sentence number 38 
Precision (%) 94.4 
Table 4 Alignment results of short test data 
It is difficult to attain large test set because do-
ing so need more manual work. We construct the 
test set by merging the aligned sentence pairs in 
the existing sentence aligned bilingual corpus into 
two files. Then the two translated files can be as 
test set. Here we merge 2000 aligned sentence 
pairs. The file information is as follows:  
English text size 200.3K  
Chinese text size  144.2K  
English sentence number 2069 
Chinese sentence number 2033 
Table 5 Basic information of the large test data 
From the table 4, it is evident that there are 
many different styles of sentence beads. The 
method is developed on this large test set and gets 
the precision of 90.5%. The reason of the slight 
precision increase is that the last test set is rela-
tively clean and the sentence length distribution 
relatively average. But overall, our method per-
forms very well to align the real bilingual texts. It 
shows the high robustness and is not related to the 
languages, text themes, text length. This method 
can resolve the alignment problem of the real text. 
7 Conclusion 
This paper proposed a new method for fully 
aligning real bilingual texts using sentence location 
information, described concretely in section 3 and 
4. The model was motivated by the observation 
that the location of a sentence pair with certain 
length is distributed in the whole text similarly. It 
uses the (1:1) sentence beads instead of the high 
frequency words as the candidate anchors. Local 
and global location characteristics of sentence pairs 
are involved to determine the probability which the 
sentence pair is an alignment anchors. 
Every sentence pair corresponds to an alignment 
value which is calculated according to the formal 
alignment function. Then the process of BAM is 
performed to get the alignment anchors. This 
alignment method can restrain the errors extension 
effectively in comparison to the traditional align-
ment method. Furthermore, it has shown strong 
robustness, even if when it meets ill-quality texts 
that include incorrect sentences. To obtain further 
improvement in alignment accuracy sentence simi-
larity based on an English-Chinese dictionary was 
performed. It need not segment the Chinese sen-
tence. The whole procedure requires little cost to 
implement. 
Additionally, we can adjust the alignment and 
similarity thresholds dynamically to get high preci-
sion alignment anchors, for example, applying the 
first test set, even if we get only 105 (1:1) sentence 
beads but the precision is 100%. We found that this 
method can perform the function of paragraph 
alignment very well and ensure simultaneous the 
alignment precision.  
Of these pairs about half of total number of (1:1) 
sentence beads can be even extracted from the bi-
lingual text directly to build a large scale bilingual 
corpus if the original bilingual text is abundant. 
And the rest bilingual text can be used as spare 
resource. Now, we have obtained about 500,000 
English-Chinese aligned sentence pairs with high 
quality. 
In the future, we hope to do further alignment on 
the basis of current work and extend the method to 
align other language pairs. 
References 
Wu, DeKai. 1994. Aligning a parallel English-
Chinese corpus statistically with lexical criteria. 
In Proceedings of the 32nd Annual Conference 
of the Association for Computational Linguistics, 
80--87, Las Cruces, New Mexico 
Simard, M., Foster, G., and Isabelle, P. 1992. Us-
ing Cognates to Align Sentences in Bilingual 
Corpora.Fourth International Conference on 
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-92), Montreal, Canada 
Brown, P., Lai, J. and Mercer, R. 1991. Aligning 
Sentences in Parallel Corpora. ACL-91 
Fung Pascale and Kathleen Mckeown. 1994. Align-
ing noisy parallel corpora across language 
groups: Word pair feature matching by dynamic 
time warping. In AMTA-94, Association for 
Machine Translation in the Americas, 81--88, 
Columbia, Maryland 
Wang Bin, Liu Qin, Zhang Xiang. 2000. Auto-
matic Chinese-English Paragraph Segmentation 
and Alignment. Journal of Software, 
11(11):1547-1553 (Chinese) 
Church, Kenneth W. 1993. Char_align: A Pro-
gram for Aligning Parallel Texts at the Charac-
ter Level. Proceedings of ACL-93, Columbus 
OH  
Chen, Stanley. 1993. Aligning Sentences in Bilin-
gual Corpora Using Lexical Information. In Pro-
ceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics 
(ACL-1993) 
Gale, W.A. Church, K.W. 1993. A Program for 
Aligning Sentences in Bilingual Corpora. Com-
putational Linguistics, 19(2): 75-102 
Haruno, Masahiko & Takefumi Yamazaki (1996), 
High-performance bilingual text alignment using 
statistical and dictionary information, In Pro-
ceedings of ACL '96, Santa Cruz, California, 
USA, pp. 131-138 
M. Kay & M. Roscheisen. 1993. Text-Translation 
Alignment. Computational Linguistics 19:1 
 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 189?192, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Lableing System using Maximum Entropy Classier ?
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu and Huaijun Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology
China, 150001
{tliu, car, ls, yxhu, hjliu}@ir.hit.edu.cn
Abstract
A maximum entropy classifier is used in
our semantic role labeling system, which
takes syntactic constituents as the labeling
units. The maximum entropy classifier is
trained to identify and classify the predi-
cates? semantic arguments together. Only
the constituents with the largest probabil-
ity among embedding ones are kept. Af-
ter predicting all arguments which have
matching constituents in full parsing trees,
a simple rule-based post-processing is ap-
plied to correct the arguments which have
no matching constituents in these trees.
Some useful features and their combina-
tions are evaluated.
1 Introduction
The semantic role labeling (SRL) is to assign syn-
tactic constituents with semantic roles (arguments)
of predicates (most frequently verbs) in sentences.
A semantic role is the relationship that a syntactic
constituent has with a predicate. Typical semantic
arguments include Agent, Patient, Instrument, etc.
and also adjunctive arguments indicating Locative,
Temporal, Manner, Cause, etc. It can be used in
lots of natural language processing application sys-
tems in which some kind of semantic interpretation
is needed, such as question and answering, informa-
tion extraction, machine translation, paraphrasing,
and so on.
?This research was supported by National Natural Science
Foundation of China via grant 60435020
Last year, CoNLL-2004 hold a semantic role la-
beling shared task (Carreras and Ma`rquez, 2004)
to test the participant systems? performance based
on shallow syntactic parser results. In 2005, SRL
shared task is continued (Carreras and Ma`rquez,
2005), because it is a complex task and now it is
far from desired performance.
In our SRL system, we select maximum en-
tropy (Berger et al, 1996) as a classifier to im-
plement the semantic role labeling system. Dif-
ferent from the best classifier reported in litera-
tures (Pradhan et al, 2005) ? support vector ma-
chines (SVMs) (Vapnik, 1995), it is much eas-
ier for maximum entropy classifier to handle the
multi-class classification problem without additional
post-processing steps. The classifier is much faster
than training SVMs classifiers. In addition, max-
imum entropy classifier can be tuned to minimize
over-fitting by adjusting gaussian prior. Xue and
Palmer (2004; 2005) and Kwon et al (2004) have
applied the maximum entropy classifier to semantic
role labeling task successfully.
In the following sections, we will describe our
system and report our results on development and
test sets.
2 System Description
2.1 Constituent-by-Constituent
We use syntactic constituent as the unit of labeling.
However, it is impossible for each argument to find
its matching constituent in all auto parsing trees. Ac-
cording to statistics, about 10% arguments have no
matching constituents in the training set of 245,353
189
constituents. The top five arguments with no match-
ing constituents are shown in Table 1. Here, Char-
niak parser got 10.08% no matching arguments and
Collins parser got 11.89%.
Table 1: The top five arguments with no matching
constituents.
Args Cha parser Col parser Both
AM-MOD 9179 9205 9153
A1 5496 7273 3822
AM-NEG 3200 3217 3185
AM-DIS 1451 1482 1404
A0 1416 2811 925
Therefore, we can see that Charniak parser got a
better result than Collins parser in the task of SRL.
So we use the full analysis results created by Char-
niak parser as our classifier?s inputs. Assume that
we could label all AM-MOD and AM-NEG arguments
correctly with simple post processing rules, the up-
per bound of performance could achieve about 95%
recall.
At the same time, we can see that for some ar-
guments, both parsers got lots of no matchings such
as AM-MOD, AM-NEG, and so on. After analyzing
the training data, we can recognize that the perfor-
mance of these arguments can improve a lot after
using some simple post processing rules only, how-
ever other arguments? no matching are caused pri-
marily by parsing errors. The comparison between
using and not using post processing rules is shown
in Section 3.2.
Because of the high speed and no affection in the
number of classes with efficiency of maximum en-
tropy classifier, we just use one stage to label all ar-
guments of predicates. It means that the ?NULL?
tag of constituents is regarded as a class like ?ArgN?
and ?ArgM?.
2.2 Features
The following features, which we refer to as the
basic features modified lightly from Pradhan et
al. (2005), are provided in the shared task data for
each constituent.
? Predicate lemma
? Path: The syntactic path through the parse tree from the
parse constituent to the predicate.
? Phrase type
? Position: The position of the constituent with respect to
its predicate. It has two values, ?before? and ?after?,
for the predicate. For the situation of ?cover?, we use
a heuristic rule to ignore all of them because there is no
chance for them to become an argument of the predicate.
? Voice: Whether the predicate is realized as an active or
passive construction. We use a simple rule to recognize
passive voiced predicates which are labeled with part of
speech ? VBN and sequences with AUX.
? Head word stem: The stemming result of the con-
stituent?s syntactic head. A rule based stemming algo-
rithm (Porter, 1980) is used. Collins Ph.D thesis (Collins,
1999)[Appendix. A] describs some rules to identify the
head word of a constituent. Especially for prepositional
phrase (PP) constituent, the normal head words are not
very discriminative. So we use the last noun in the PP
replacing the traditional head word.
? Sub-categorization
We also use the following additional features.
? Predicate POS
? Predicate suffix: The suffix of the predicate. Here, we
use the last 3 characters as the feature.
? Named entity: The named entity?s type in the constituent
if it ends with a named entity. There are four types: LOC,
ORG, PER and MISC.
? Path length: The length of the path between a constituent
and its predicate.
? Partial path: The part of the path from the constituent
to the lowest common ancestor of the predicate and the
constituent.
? Clause layer: The number of clauses on the path between
a constituent and its predicate.
? Head word POS
? Last word stem: The stemming result of the last word of
the constituent.
? Last word POS
We also use some combinations of the above fea-
tures to build some combinational features. Lots of
combinational features which were supposed to con-
tribute the SRL task of added one by one. At the
same time, we removed ones which made the per-
formance decrease in practical experiments. At last,
we keep the following combinations:
? Position + Voice
? Path length + Clause layer
? Predicate + Path
? Path + Position + Voice
? Path + Position + Voice + Predicate
? Head word stem + Predicate
? Head word stem + Predicate + Path
? Head word stem + Phrase
? Clause layer + Position + Predicate
All of the features and their combinations are used
without feature filtering strategy.
190
2.3 Classifier
Le Zhang?s Maximum Entropy Modeling Toolkit 1,
and the L-BFGS parameter estimation algorithm
with gaussian prior smoothing (Chen and Rosenfeld,
1999) are used as the maximum entropy classifier.
We set gaussian prior to be 2 and use 1,000 itera-
tions in the toolkit to get an optimal result through
some comparative experiments.
2.4 No Embedding
The system described above might label two con-
stituents even if one embeds in another, which is not
allowed by the SRL rule. So we keep only one ar-
gument when more arguments embedding happens.
Because it is easy for maximum entropy classifier to
output each prediction?s probability, we can label the
constituent which has the largest probability among
the embedding ones.
2.5 Post Processing Stage
After labeling the arguments which are matched
with constituents exactly, we have to handle the ar-
guments, such as AM-MOD, AM-NEG and AM-DIS,
which have few matching with the constituents de-
scribed in Section 2.1. So a post processing is given
by using some simply rules:
? Tag target verb and successive particles as V.
? Tag ?not? and ?n?t? in target verb chunk as AM-NEG.
? Tag modal verbs in target verb chunk, such as words with
POS of ?MD?, ?going to?, and so on, as AM-MOD.
? Tag the words with POS of ?CC? and ?RB? at the start of
a clause which include the target verb as AM-DIS.
3 Experiments
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of
PropBank corpus. It consists of the sections from
the Wall Street Journal part of Penn Treebank. Sec-
tions 02-21 are training sets, and Section 24 is devel-
opment set. The results are evaluated for precision,
recall and F?=1 numbers using the srl-eval.pl script
provided by the shared task organizers.
3.2 Post Processing
After using post processing rules, the final F?=1 is
improved from 71.02% to 75.27%.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
3.3 Performance Curve
Because the training corpus is substantially en-
larged, this allows us to test the scalability of
learning-based SRL systems to large data set and
compute learning curves to see how many data are
necessary to train. We divide the training set, 20
sections Penn Treebank into 5 parts with 4 sections
in each part. There are about 8,000 sentences in each
part. Figure 1 shows the change of performance as
a function of training set size. When all of training
data are used, we get the best system performance as
described in Section 3.4.
Figure 1: Our SRL system performance curve (of
F?=1) effecting of the training set size.
We can see that as the training set becomes larger
and larger, so does the performance of SRL system.
However, the rate of increase slackens. So we can
say that at present state, the larger training data has
favorable effect on the improvement of SRL system
performance.
3.4 Best System Results
In all the experiments, all of the features and their
combinations described above are used in our sys-
tem. Table 2 presents our best system performance
on the development and test sets.
From the test results, we can see that our system
gets much worse performance on Brown corpus than
WSJ corpus. The reason is easy to be understood
for the dropping of automatic syntactic parser per-
formance on new corpus but WSJ corpus.
The training time on PIV 2.4G CPU and 1G Mem
machine is about 20 hours on all 20 sections, 39,832-
191
Precision Recall F?=1
Development 79.65% 71.34% 75.27
Test WSJ 80.48% 72.79% 76.44
Test Brown 71.13% 59.99% 65.09
Test WSJ+Brown 79.30% 71.08% 74.97
Test WSJ Precision Recall F?=1
Overall 80.48% 72.79% 76.44
A0 88.14% 83.61% 85.81
A1 79.62% 72.88% 76.10
A2 73.67% 65.05% 69.09
A3 76.03% 53.18% 62.59
A4 78.02% 69.61% 73.58
A5 100.00% 40.00% 57.14
AM-ADV 59.85% 48.02% 53.29
AM-CAU 68.18% 41.10% 51.28
AM-DIR 56.60% 35.29% 43.48
AM-DIS 76.32% 72.50% 74.36
AM-EXT 83.33% 46.88% 60.00
AM-LOC 65.31% 52.89% 58.45
AM-MNR 58.28% 51.16% 54.49
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 43.68% 33.04% 37.62
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.38% 66.70% 72.07
R-A0 81.70% 85.71% 83.66
R-A1 77.62% 71.15% 74.25
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 83.33% 47.62% 60.61
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 77.27% 65.38% 70.83
V 98.71% 98.71% 98.71
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
sentences training set with 1,000 iterations and more
than 1.5 million samples and 2 million features.
The predicting time is about 160 seconds on 1,346-
sentences development set.
4 Conclusions
We have described a maximum entropy classifier
is our semantic role labeling system, which takes
syntactic constituents as the labeling units. The
fast training speed of the maximum entropy clas-
sifier allows us just use one stage of arguments
identification and classification to build the system.
Some useful features and their combinations are
evaluated. Only the constituents with the largest
probability among embedding ones are kept. Af-
ter predicting all arguments which have matching
constituents in full parsing trees, a simple rule-
based post-processing is applied to correct the ar-
guments which have no matching constituents. The
constituent-based method depends much on the syn-
tactic parsing performance. The comparison be-
tween WSJ and Brown test sets results fully demon-
strates the point of view.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the conll-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL-2004, pages 89?97, Boston,
MA, USA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Pennsyl-
vania University.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. Framenet-based semantic parsing using maxi-
mum entropy models. In Proc. Coling 2004.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Vladamir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proc. EMNLP
2004.
Nianwen Xue and Martha Palmer. 2005. Automatic se-
mantic role labeling for chinese verbs. In Proc. IJCAI
2005.
192
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 211?215, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing Based on Dynamic Local Optimization
Ting Liu Jinshan Ma Huijia Zhu Sheng Li
Information Retrieval Lab
Harbin Institute of Technology
Harbin, 150001, China
{tliu,mjs,hjzhu,ls}@ir.hit.edu.cn
Abstract
This paper presents a deterministic pars-
ing algorithm for projective dependency
grammar. In a bottom-up way the al-
gorithm finds the local optimum dynam-
ically. A constraint procedure is made
to use more structure information. The
algorithm parses sentences in linear time
and labeling is integrated with the parsing.
This parser achieves 63.29% labeled at-
tachment score on the average in CoNLL-
X Shared Task.
1 Introduction
Recently, dependency grammar has gained renewed
attention in the parsing community. Good results
have been achieved in some dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al, 2004).
With the availability of many dependency treebanks
(van der Beek et al, 2002; Hajic? et al, 2004;
Bo?hmova? et al, 2003; Kromann, 2003; Dz?eroski et
al., 2006) and more other treebanks which can be
converted to dependency annotation (Brants et al,
2002; Nilsson et al, 2005; Chen et al, 2003; Kawata
and Bartels, 2000), multi-lingual dependency pars-
ing is proposed in CoNLL shared task (Buchholz et
al., 2006).
Many previous works focus on unlabeled parsing,
in which exhaustive methods are often used (Eis-
ner, 1996). Their global searching performs well
in the unlabeled dependency parsing. But with the
increase of parameters, efficiency has to be consid-
ered in labeled dependency parsing. Thus determin-
istic parsing was proposed as a robust and efficient
method in recent years. Such method breaks the
construction of dependency tree into a series of ac-
tions. A classifier is often used to choose the most
probable action to assemble the dependency tree.
(Yamada and Matsumoto, 2003) defined three ac-
tions and used a SVM classifier to choose one of
them in a bottom-up way. The algorithm in (Nivre
et al, 2004) is a blend of bottom-up and top-down
processing. Its classifier is trained by memory-based
learning.
Deterministic parsing derives an analysis without
redundancy or backtracking, and linear time can be
achieved. But when searching the local optimum in
the order of left-to-right, some wrong reduce may
prevent next analysis with more possibility. (Jin et
al., 2005) used a two-phase shift-reduce to decrease
such errors, and improved the accuracy of long dis-
tance dependencies.
In this paper a deterministic parsing based on dy-
namic local optimization is proposed. According to
the probabilities of dependency arcs, the algorithm
dynamically finds the one with the highest probabil-
ities instead of dealing with the sentence in order.
A procedure of constraint which can integrate more
structure information is made to check the rational-
ity of the reduce. Finally our results and error anal-
ysis are presented.
2 Dependency Probabilities
An example of Chinese dependency tree is showed
in Figure1. The tree can be represented as a directed
graph with nodes representing word tokens and arcs
211
Figure 1: A Chinese dependency tree
representing dependency relations. The assumption
that the arcs are independent on each other often is
made so that parsing can be handled easily. On the
other side the independence assumption will result
in the loss of information because dependencies are
interrelated on each other actually. Therefore, two
kinds of probabilities are used in our parser. One is
arc probabilities which are the possibility that two
nodes form an arc, and the other is structure proba-
bilities which are used to describe some specific syn-
tactic structures.
2.1 Arc Probabilities
A dependency arc A
i
can be expressed as a 4-tuple
A
i
= <Node
i
, Node
j
, D, R>. Node
i
and Node
j
are
nodes that constitute the directed arc. D is the direc-
tion of the arc, which can be left or right. R is rela-
tion type labeled on the arc. Under the independence
assumption that an arc depends on its two nodes we
can calculate arc probability given two nodes. In our
paper the arc probabilities are calculated as follows:
P
1
= P(R,D|CTag
i
, CTag
j
, Dist)
P
2
= P(R,D|FTag
i
, FTag
j
)
P
3
= P(R,D|CTag
i
, Word
j
)
P
4
= P(R,D|Word
i
, CTag
j
)
P
5
= P(R,D|Word
i
,CTag
i
, Word
j
,CTag
j
)
P
6
= P(R,D|CTag
i?1
, CTag
i
, CTag
j
, CTag
j+1
)
Where CTag is coarse-grained part of speech tag
and FTag is fine-grained tag. As to Word we choose
its lemma if it exists. Dist is the distance between
Node
i
and Node
j
. It is divided into four parts:
Dist = 1 if j-i = 1
Dist = 2 if j-i = 2
Dist = 3 if 3?j-i?6
Dist = 4 if j-i > 6
All the probabilities are obtained by maximum
likelihood estimation from the training data. Then
interpolation smoothing is made to get the final arc
probabilities.
2.2 Structure Probabilities
Structure information plays the critical role in syn-
tactic analysis. Nevertheless the flexibility of syn-
tactic structures and data sparseness pose obstacles
to us. Especially some structures are related to spe-
cific language and cannot be employed in multi-
lingual parsing. We have to find those language-
independent features.
In valency theory ?valence? represents the num-
ber of arguments that a verb is able to govern. In
this paper we extend the range of verbs and argu-
ments to all the words. We call the new ?valence?
Governing Degree (GD), which means the ability of
one node governing other nodes. In Figure1, the GD
of node ???? is 2 and the GDs of two other nodes
are 0. The governing degree of nodes in dependency
tree often shows directionality. For example, Chi-
nese token ??? always governs one left node. Fur-
thermore, we subdivide the GD into Left Governing
Degree (LGD) and Right Governing Degree (RGD),
which are the ability of words governing their left
children or right children. In Figure 1 the LGD and
RGD of verb ???? are both 1.
In the paper we use the probabilities of GD
over the fine-grained tags. The probabilities of
P(LDG|FTag) and P(RGD|FTag) are calculated
from training data. Then we only reserve the FTags
with large probability because their GDs are stable
and helpful to syntactic analysis. Other FTags with
small probabilities are unstable in GDs and cannot
provide efficient information for syntactic analysis.
If their probabilities are less than 0.65 they will be
ignored in our dependency parsing.
3 Dynamic local optimization
Many previous methods are based on history-based
models. Despite many obvious advantages, these
methods can be awkward to encode some constrains
within their framework (Collins, 2000). Classifiers
are good at encoding more features in the determin-
istic parsing (Yamada and Matsumoto, 2003; Nivre
et al, 2004). However, such algorithm often make
more probable dependencies be prevented by pre-
ceding errors. An example is showed in Figure 2.
Arc a is a frequent dependency and b is an arc with
more probability. Arc b will be prevented by a if the
reduce is carried out in order.
212
Figure 2: A common error in deterministic parsing
3.1 Our algorithm
Our deterministic parsing is based on dynamic local
optimization. The algorithm calculates the arc prob-
abilities of two continuous nodes, and then reduces
the most probable arc. The construction of depen-
dency tree includes four actions: Check, Reduce,
Delete, and Insert. Before a node is reduced, the
Check procedure is made to validate its correctness.
Only if the arc passes the Check procedure it can
be reduced. Otherwise the Reduce will be delayed.
Delete and Insert are then carried out to adjust the
changed arcs. The complete algorithm is depicted
as follows:
Input Sentence: S = (w
1
, w
2
,l, w
n
)
Initialize:
for i = 1 to n
R
i
= GetArcProb(w
i
,w
i+1
);
Push(R
i
) onto Stack;
Sort(Stack);
Start:
i = 0;
While Stack.empty = false
R = Stack.top+i;
if Check(R) = true
Reduce(R);
Delete(R?);
Insert(R?);
i = 0;
else
i++;
The algorithm has following advantages:
? Projectivity can be guaranteed. The node is
only reduced with its neighboring node. If a
node is reduced as a leaf it will be removed
from the sentence and doesn?t take part in next
Reduce. So no cross arc will occur.
? After n-1 pass a projective dependency tree is
complete. Algorithm is finished in linear time.
? The algorithm always reduces the node with the
Figure 3: Adjustment
highest probability if it passes the Check. No
any limitation on order thus the spread of errors
can be mitigated effectively.
? Check is an open process. Various constrains
can be encoded in this process. Structural con-
strains, partial parsed information or language-
dependent knowledge can be added.
Adjustment is illustrated in Figure 3, where ??
?? is reduced and arc R? is deleted. Then the algo-
rithm computes the arc probability of R? and inserts
it to the Stack.
3.2 Checking
The information in parsing falls into two kinds:
static and dynamic. The arc probabilities in 2.1 de-
scribe the static information which is not changed in
parsing. They are obtained from the training data in
advance. The structure probabilities in 2.2 describe
the dynamic information which varies in the process
of parsing. The use of dynamic information often
depends on what current dependency tree is.
Besides the governing degree, Check procedure
also uses another dynamic information?Sequential
Dependency. Whether current arc can be reduced is
relating to previous arc. In Figure 3 the reduce of the
arc R depends on the arc R?. If R? has been delayed
or its probability is little less than that of R, arc R
will be delayed.
If the arc doesn?t pass the Check it will be de-
layed. The delayed time ranges from 1 to Length
which is the length of sentence. If the arc is delayed
Length times it will be blocked. The Reduce will be
delayed in the following cases:
?
?
GD(Node
i
) > 0 and its probability is P. If
GD(Node
i
) = 0 and Node
i
is made as child
in the Reduce, the Node
i
will be delayed
Length*P times.
?
?
GD(Node
i
) ? m (m > 0) and its probability
is P. If GD(Node
i
) = m and Node
i
is made as
parent in the Reduce, the Node
i
will be delayed
Length*P times.
213
Figure 4: Token score with size of training data
Figure 5: Token score with sentence length
? P(R?) > ?P(R), the current arc R will be de-
layed Length*(P(R?)/P(R)) times. R? is the pre-
ceding arc and ? = 0.60.
? If arc R? is blocking, the arc R will be delayed.
?
GD is empirical value and GD is current value.
4 Experiments and analysis
Our parsing results and average results are listed
in the Table 1. It can be seen that the attachment
scores vary greatly with different languages. A gen-
eral analysis and a specific analysis are made respec-
tively in this section.
4.1 General analysis
We try to find the properties that make the differ-
ence to parsing results in multi-lingual parsing. The
properties of all the training data can be found in
(Buchholz et al, 2006). Intuitively the size of train-
ing data and average length of per sentence would
be influential on dependency parsing. The relation
of these properties and scores are showed in the Fig-
ure 4 and 5.
From the charts we cannot assuredly find the
properties that are proportional to score. Whether
Czech language with the largest size of training data
or Chinese with the shortest sentence length, don?t
achieve the best results. It seems that no any factor is
determining to parsing results but all the properties
exert influence on the dependency parsing together.
Another factor that maybe explain the difference
of scores in multi-lingual parsing is the characteris-
tics of language. For example, the number of tokens
with HEAD=0 in a sentence is not one for some lan-
guages. Table 1 shows the range of governing de-
gree of head. This statistics is somewhat different
with that from organizers because we don?t distin-
guish the scoring tokens and non-scoring tokens.
Another characteristic is the directionality of de-
pendency relations. As Table 1 showed, many rela-
tions in treebanks are bi-directional, which increases
the number of the relation actually. Furthermore, the
flexibility of some grammatical structures poses dif-
ficulties to language model. For instance, subject
can appear in both sides of the predicates in some
treebanks which tends to cause the confusion with
the object (Kromann, 2003; Afonso et al, 2002;
Civit Torruella and Mart?? Anton??n, 2002; Oflazer et
al., 2003; Atalay et al, 2003).
As to our parsing results, which are lower than all
the average results except for Danish. That can be
explained from the following aspects:
(1) Our parser uses a projective parsing algorithm
and cannot deal with the non-projective tokens,
which exist in all the languages except for Chinese.
(2) The information provided by training data is not
fully employed. Only POS and lemma are used. The
morphological and syntactic features may be helpful
to parsing.
(3) We haven?t explored syntactic structures in depth
for multi-lingual parsing and more structural fea-
tures need to be used in the Check procedure.
4.2 Specific analysis
Specifically we make error analysis to Chinese and
Turkish. In Chinese result we found many errors
occurred near the auxiliary word ???(DE). We call
the noun phrases with ??? DE Structure. The word
??? appears 355 times in the all 4970 dependencies
of the test data. In Table 2 the second row shows the
frequencies of ?DE? as the parent of dependencies.
The third row shows the frequencies while it is as
child. Its error rate is 33.1% and 43.4% in our re-
sults respectively. Furthermore, each head error will
result in more than one errors, so the errors from DE
Structures are nearly 9% in our results.
214
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu
our 50.74 75.29 58.52 77.70 59.36 68.11 70.84 71.13 57.21 65.08 63.83 41.72
ave 59.94 78.32 67.17 76.16 70.73 78.58 85.86 80.63 65.16 73.52 76.44 55.95
NH 17 1 28 4 9 1 14 1 11 1 1 5
BD 27/24 78/55 82/72 54/24 26/17 46/40 7/2 55/40 26/23 21/19 64/54 26/23
Table 1: The second and third rows are our scores and average scores. The fourth row lists the maximal
number of tokens with HEAD=0 in a sentence. The last row lists the number of relations/the number of
bi-directional relations of them (Our statistics are slightly different from that of organizers).
gold system error headerr
parent 320 354 106 106
child 355 355 154 74
Table 2: Chinese DE Structure Errors
The high error rate is due to the flexibility of DE
Structure. The children of DE can be nouns and
verbs, thus the ambiguities will occur. For example,
the sequence ?V N1 DE N2? is a common ambigu-
ious structure in Chinese. It needs to be solved with
semantic knowledge to some extent. The errors of
DE being child are mostly from noun compounds.
For example, the string ????????? results
in the error: ?DE? as the child of ????. It will be
better that noun compounds are processed specially.
Our results and average results achieve the low-
est score on Turkish. We try to find some reasons
through the following analysis. Turkish is a typi-
cal head-final language and 81.1% of dependencies
are right-headed. The monotone of directionality in-
creases the difficulties of identification. Another dif-
ficulty is the diversity of the same pair. Taking noun
and pronoun as example, which only achieve the ac-
curacy of 25% and 28% in our results, there are 14
relations in the noun-verb pairs and 11 relations in
the pronoun-verb pairs. Table 3 illustrates the distri-
bution of some common relations in the test data.
The similarity of these dependencies makes our
parser only recognize 23.3% noun-verb structures
and 21.8% pronoun-verb structures. The syntactic
or semantic knowledge maybe helpful to distinguish
these similar structures.
5 Conclusion
This paper has applied a deterministic algorithm
based on dynamic local optimization to multi-
total obj sub mod D.A L.A
Noun-V 1300 494 319 156 102 78
Pron-V 215 91 60 9 37 3
Table 3: The distribution of some common relations
lingual dependency parsing. Through the error
analysis for some languages, we think that the dif-
ference between languages is a main obstacle posed
on multi-lingual dependency parsing. Adopting
different learners according to the type of languages
may be helpful to multi-lingual dependency parsing.
Acknowledgement This work was supported
by the National Natural Science Foundation of
China under Grant No. 60435020?60575042 and
60503072.
References
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
M.X. Jin, M.Y. Kim, and J.H. Lee. 2005. Two-phase
shift-reduce deterministic dependency parser of chi-
nese. In Proc. of IJCNLP: Companion Volume includ-
ing Posters/Demos and tutorial abstracts.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of the Eighth Conf. on
Computational Natural Language Learning (CoNLL),
pages 49?56.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of the 8th Intern. Workshop on Parsing Technologies
(IWPT).
215
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 165?168,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-IR-WSD: A WSD System for English Lexical Sample Task 
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang and Ting Liu 
Information Retrieval Lab 
Harbin Institute of technology 
Harbin, China, 150001 
{yhguo,wxche}@ir.hit.edu.cn 
 
 
Abstract 
HIT-IR-WSD is a word sense disambigua-
tion (WSD) system developed for English 
lexical sample task (Task 11) of Semeval 
2007 by Information Retrieval Lab, Harbin 
Institute of Technology. The system is 
based on a supervised method using an 
SVM classifier. Multi-resources including 
words in the surrounding context, the part-
of-speech of neighboring words, colloca-
tions and syntactic relations are used. The 
final micro-avg raw score achieves 81.9% 
on the test set, the best one among partici-
pating runs. 
1 Introduction 
Lexical sample task is a kind of WSD evaluation 
task providing training and test data in which a 
small pre-selected set of target words is chosen and 
the target words are marked up. In the training data 
the target words? senses are given, but in the test 
data are not and need to be predicted by task par-
ticipants. 
HIT-IR-WSD regards the lexical sample task 
as a classification problem, and devotes to extract 
effective features from the instances. We didn?t use 
any additional training data besides the official 
ones the task organizers provided. Section 2 gives 
the architecture of this system. As the task pro-
vides correct word sense for each instance, a su-
pervised learning approach is used. In this system, 
we choose Support Vector Machine (SVM) as 
classifier. SVM is introduced in section 3. Know-
ledge sources are presented in section 4. The last 
section discusses the experimental results and 
present the main conclusion of the work performed. 
2 The Architecture of the System 
HIT-IR-WSD system consists of 2 parts: feature 
extraction and classification. Figure 1 portrays the 
architecture of the system. 
 
Figure?1:?The?architecture?of?HIT?IR?WSD?
165
Features are extracted from original instances 
and are made into digitized features to feed the 
SVM classifier. The classifier gets the features of 
training data to make a model of the target word. 
Then it uses the model to predict the sense of target 
word in the test data. 
3 Learning Algorithm 
SVM is an effective learning algorithm to WSD 
(Lee and Ng, 2002). The SVM tries to find a 
hyperplane with the largest margin separating the 
training samples into two classes. The instances in 
the same side of the hyperplane have the same 
class label. A test instance?s feature decides the 
position where the sample is in the feature space 
and which side of the hyperplane it is. In this way, 
it leads to get a prediction. SVM could be extended 
to tackle multi-classes problems by using one-
against-one or one-against-rest strategy. 
In the WSD problem, input of SVM is the fea-
ture vector of the instance. Features that appear in 
all the training samples are arranged as a vector 
space. Every instance is mapped to a feature vector. 
If the feature of a certain dimension exists in a 
sample, assign this dimension 1 to this sample, else 
assign it 0. For example, assume the feature vector 
space is <x1, x2, x3, x4, x5, x6, x7>; the instance is 
?x2 x6 x5 x7?. The feature vector of this sample 
should be <0, 1, 0, 0, 1, 1, 1>.  
The implementation of SVM here is libsvm 1 
(Chang and Lin, 2001) for multi-classes. 
4 Knowledge Sources 
We used 4 kinds of features of the target word and 
its context as shown in Table 1. 
Part of the original text of an example is ?? 
This is the <head>age</head> of new media , the 
era of ??. 
Name Extraction Tools Example 
Surrounding 
words 
WordNet 
(morph)2 
?, this, be, age, new, 
medium, ,, era, ? 
Part-of-
speech SVMTool
3 
DT_0, VBZ_0, DT_0, 
NN_t, IN_1, JJ_1, 
NNS_1 
                                                 
1?http://www.csie.ntu.edu.tw/~cjlin/libsvm/?
2?http://wordnet.princeton.edu/man/morph.3WN.html?
3?http://www.lsi.upc.es/~nlp/SVMTool/?
Collocation  
this_0, be_0, the_0, 
age_t, of_1, new_1, 
medium_1, ,_1, the_1 
Syntactic 
relation MaltParser
4 
SYN_HEAD_is 
SYN_HEADPOS_VBZ 
SYN_RELATION_PRD 
SYN_HEADRIGHT 
Table?1:?Features?the?system?extracted?
The next 4 subsections elaborate these features. 
4.1 Words in the Surrounding Context 
We take the neighboring words in the context of 
the target word as a kind of features ignoring their 
exact position information, which is called bag-of-
words approach. 
Mostly, a certain sense of a word is tend to ap-
pear in a certain kind of context, so the context 
words could contain some helpful information to 
disambiguate the sense of the target word. 
Because there would be too many context words 
to be added into the feature vector space, data 
sparseness problem is inevitable. We need to re-
duce the sparseness as possible as we can. A sim-
ple way is to use the words? morphological root 
forms. In addition, we filter the tokens which con-
tain no alphabet character (including punctuation 
symbols) and stop words. The stop words are 
tested separately, and only the effective ones 
would be added into the stop words list. All re-
maining words in the instance are gathered, con-
verted to lower case and replaced by their morpho-
logical root forms. The implementation for getting 
the morphological root forms is WordNet (morph). 
4.2 Part-of-Speechs of Neighboring Words 
As mentioned above, the data sparseness is a se-
rious problem in WSD. Besides changing tokens to 
their morphological root forms, part-of-speech is a 
good choice too. The size of POS tag set is much 
smaller than the size of surrounding words set. 
And the neighboring words? part-of-speeches also 
contain useful information for WSD. In this part, 
we use a POS tagger (Gim?nez and M?rquez, 2004) 
to assign POS tags to those tokens.  
We get the left and right 3 words? POS tags to-
gether with their position information in the target 
words? sentence.  
For example, the word age is to be disambi-
guated in the sentence of ?? This is the 
                                                 
4?http://w3.msi.vxu.se/~nivre/research/MaltParser.html?
166
<head>age</head> of new media , the era of ??. 
The features then will be added to the feature vec-
tor are ?DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, 
NNS_1?, in which _0/_1 stands for the word with 
current POS tag is in the left/right side of the target 
word. The POS tag set in use here is Penn Tree-
bank Tagset5. 
4.3 Collocations 
Different from bag-of-words, collocation feature 
contains the position information of the target 
words? neighboring words. To make this feature in 
the same form with the bag-of-words, we appended 
a symbol to each of the neighboring words? mor-
phological root forms to mark whether this word is 
in the left or in the right of the target word. Like 
POS feature, collocation was extracted in the sen-
tence where the target word belongs to. The win-
dow size of this feature is 5 to the left and 5 to the 
right of the target word, which is attained by em-
pirical value. In this part, punctuation symbol and 
stop words are not removed. 
Take the same instance last subsection has men-
tioned as example. The features we extracted are 
?this_0, be_0, the_0, age_t, of_1, new_1, me-
dium_1?. Like POS, _0/_1 stands for the word is 
in the left/right side of the target word. Then the 
features were added to the feature vector space. 
4.4 Syntactic Relations 
Many effective context words are not in a short 
distance to the target word, but we shouldn?t en-
large the window size too much in case of includ-
ing too many noises. A solution to this problem is 
to use the syntactic relations of the target word and 
its parent head word. 
We use Nivre et al, (2006)?s dependency parser. 
In this part, we get 4 features from every instance: 
head word of the target word, the head word?s POS, 
the head word?s dependency relation with the tar-
get word and the relative position of the head word 
to the target word. 
Still take the same instance which has been 
mentioned in the las subsection as example. The 
features we extracted are ?SYN_HEAD_is, 
SYN_HEADPOS_VBZ, SYN_RELATION_PRD, 
SYN_HEADRIGHT?, in which SYN_HEAD_is 
stands for is is the head word of age; 
SYN_HEADPOS_VBZ stands for the POS of the 
                                                 
5?http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html?
head word is is VBZ; SYN_RELATION_PRD 
stands for the relationship between the head word 
is and target word age is PRD; and 
SYN_HEADRIGHT stands for the target word age 
is in the right side of the head word is. 
5 Data Set and Results 
This English lexical sample task: Semeval 2007 
task 116 provides two tracks of the data set for par-
ticipants. The first one is from LDC and the second 
from web. 
We took part in this evaluation in the second 
track. The corpus is from web. In this track the task 
organizers provide a training data and test data set 
for 20 nouns and 20 adjectives. 
In order to develop our system, we divided the 
training data into 2 parts: training and development 
sets. The size of the training set is about 2 times of 
the development set. The development set contains 
1,781 instances. 
4 kinds of features were merged into 15 combi-
nations. Here we use a vector (V) to express which 
features are used. The four dimensions stand for 
syntactic relations, POS, surrounding words and 
collocations, respectively. For example, 1010 
means that the syntactic relations feature and the 
surrounding words feature are used. 
V Precision V Precision
0001 78.6% 1001 78.2% 
0010 80.3% 1010 81.9% 
0011 82.0% 1011 82.8% 
0100 70.4% 1100 73.3% 
0101 79.0% 1101 79.1% 
0110 82.1% 1110 82.5% 
0111 82.9% 1111 82.9% 
1000 72.6%   
Table?2:?Results?of?Combinations?of?Features?
From Table 2, we can conclude that the sur-
rounding words feature is the most useful kind of 
features. It obtains much better performance than 
other kinds of features individually. In other words, 
without it, the performance drops a lot. Among 
these features, syntactic relations feature is the 
most unstable one (the improvement with it is un-
stable), partly because the performance of the de-
pendency parser is not good enough. As the ones 
with the vector 0111 and 1111 get the best perfor-
                                                 
6http://nlp.cs.swarthmore.edu/semeval/tasks/task11/descript
ion.shtml?
167
mance, we chose all of these kinds of features for 
our final system. 
A trade-off parameter C in SVM is tuned, and 
the result is shown in Figure 2. We have also tried 
4 types of kernels of the SVM classifier (parame-
ters are set by default). The experimental results 
show that the linear kernel is the most effective as 
Table 3 shows. 
 
Figure?2:?Accuracy?with?different?C?parameters?
Kernel 
Function 
Type 
Linear Poly-nomial RBF
Sig-
moid
Accuracy 82.9% 68.3% 68.3% 68.3%
Table?3:?Accuracy?with?different?kernel?function?
types?
Another experiment (as shown in Figure 3) also 
validate that the linear kernel is the most suitable 
one. We tried using polynomial function. Unlike 
the parameters set by default above (g=1/k, d=3), 
here we set its Gama parameter as 1 (g=1) but oth-
er parameters excepting degree parameter are still 
set by default. The performance gets better when 
the degree parameter is tuned towards 1. That 
means the closer the kernel function to linear func-
tion the better the system performs. 
 
Figure?3:?Accuracy?with?different?degree? in?po?
lynomial?function?
In order to get the relation between the system 
performance and the size of training data, we made 
several groups of training-test data set from the 
training data the organizers provided. Each of them 
has the same test data but different size of training 
data which are 2, 3, 4 and 5 times of the test data 
respectively. Figure 4 shows the performance 
curve with the training data size. Indicated in Fig-
ure 4, the accuracy increases as the size of training 
data enlarge, from which we can infer that we 
could raise the performance by using more training 
data potentially. 
 
Figure?4:?Accuracy?s?trend?with?the?training?da?
ta?size?
Feature extraction is the most time-consuming 
part of the system, especially POS tagging and 
parsing which take 2 hours approximately on the 
training and test data. The classification part (using 
libsvm) takes no more than 5 minutes on the train-
ing and test data. We did our experiment on a PC 
with 2.0GHz CPU and 960 MB system memory. 
Our official result of HIT-IR-WSD is: micro-
avg raw score 81.9% on the test set, the top one 
among the participating runs. 
Acknowledgement 
We gratefully acknowledge the support for this 
study provided by the National Natural Science 
Foundation of China (NSFC) via grant 60435020, 
60575042, 60575042 and 60675034. 
References 
Lee, Y. K., and Ng, H. T. 2002. An empirical evaluation 
of knowledge sources and learning algorithms for 
word sense disambiguation. In Proceedings of 
EMNLP02, 41?48. 
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a 
library for support vector machines. 
Jes?s Gim?nez and Llu?s M?rquez. 2004. SVMTool: A 
general POS tagger generator based on Support Vec-
tor Machines. Proceedings of the 4th International 
Conference on Language Resources and Evaluation 
(LREC'04). Lisbon, Portugal. 
Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 
2006. Labeled Pseudo-Projective Dependency Pars-
ing with Support Vector Machines. In Proceedings of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
168
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT: Web based Scoring Method for English Lexical Substitution 
Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng Li 
Information Retrieval Laboratory, School of Computer Science and Technology, 
Box 321, Harbin Institute of Technology 
Harbin, P.R. China, 150001 
{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cn 
 
 
Abstract 
This paper describes the HIT system and its 
participation in SemEval-2007 English 
Lexical Substitution Task. Two main steps 
are included in our method: candidate sub-
stitute extraction and candidate scoring. In 
the first step, candidate substitutes for each 
target word in a given sentence are ex-
tracted from WordNet. In the second step, 
the extracted candidates are scored and 
ranked using a web-based scoring method. 
The substitute ranked first is selected as the 
best substitute. For the multiword subtask, 
a simple WordNet-based approach is em-
ployed. 
1 Introduction 
Lexical substitution aims to find alternative words 
that can occur in given contexts. It is important in 
many applications, such as query reformulation in 
question answering, sentence generation, and 
paraphrasing. There are two key problems in the 
lexical substitution task, the first of which is 
candidate substitute extraction. Generally speaking, 
synonyms can be regarded as candidate substitutes 
of words. However, some looser lexical 
relationships can also be considered, such as 
Hypernyms and Hyponyms defined in WordNet 
(Fellbaum, 1998). In addition, since lexical 
substitution is context dependent, some words 
which do not have similar meanings in general 
may also be substituted in some certain contexts 
(Zhao et al, 2007). As a result, finding a lexical 
knowledge base for substitute extraction is a 
challenging task. 
The other problem is candidate scoring and 
ranking according to given contexts. In the lexical 
substitution task of SemEval-2007, context is con-
strained as a sentence. The system therefore has to 
score the candidate substitutes of each target word 
using the given sentence. The following questions 
should be considered here: (1) What words in the 
given sentence are ?useful? context? (2) How to 
combine the context words and use them in rank-
ing candidate substitutes? For the first question, we 
can use all words of the sentence, words in a win-
dow, or words having syntactic relations with the 
target word. For the second question, we can re-
gard the context words as ?bag of words?, n-grams, 
or syntactic structures. 
In HIT, we extract candidate substitutes from 
WordNet, in which both synonyms and hypernyms 
are investigated (Section 3.1). After that, we score 
the candidates using a web-based scoring method 
(Section 3.2). In this method, we first select frag-
ments containing the target word from the given 
sentence. Then we construct queries by replacing 
the target word in the fragments with the candidate 
substitute. Finally, we search Google using the 
constructed queries and score each candidate based 
on the counts of retrieved snippets. 
The rest of this paper is organized as follows: 
Section 2 reviews some related work on lexical 
substitution. Section 3 describes our system, espe-
cially the web-based scoring method. Section 4 
presents the results and analysis. 
2 Related Work 
Synonyms defined in WordNet have been widely 
used in lexical substitution and expansion (Smea-
ton et al, 1994; Langkilde and Knight, 1998; Bol-
173
shakov and Gelbukh, 2004). In addition, a lot of 
methods have been proposed to automatically con-
struct thesauri of synonyms. For example, Lin 
(1998) clustered words with similar meanings by 
calculating the dependency similarity. Barzilay and 
McKeown (2001) extracted paraphrases using mul-
tiple translations of literature works. Wu and Zhou 
(2003) extracted synonyms with multiple resources, 
including a monolingual dictionary, a bilingual 
corpus, and a monolingual corpus. Besides the 
handcrafted and automatic synonym resources, the 
web has been exploited as a resource for lexical 
substitute extraction (Zhao et al, 2007). 
As for substitute scoring, various methods have 
been investigated, among which the classification 
method is the most widely used (Dagan et al, 2006; 
Kauchak and Barzilay, 2006). In detail, a binary 
classifier is trained for each candidate substitute, 
using the contexts of the substitute as features. 
Then a new contextual sentence containing the tar-
get word can be classified as 1 (the candidate is a 
correct substitute in the given sentence) or 0 (oth-
erwise). The features used in the classification are 
usually similar with that in word sense disam-
biguation (WSD), including bag of word lemmas 
in the sentence, n-grams and parts of speech (POS) 
in a window, etc. There are other models presented 
for candidate substitute scoring. Glickman et al 
(2006) proposed a Bayesian model and a Neural 
Network model, which estimate the probability of 
a word may occur in a given context. 
3 HIT System 
3.1 Candidate Substitute Extraction 
In HIT, candidate substitutes are extracted from 
WordNet. Both synonyms and hypernyms defined 
in WordNet are investigated. Let w be a target 
word, pos the specified POS of w. n the number of 
w?s synsets defined in WordNet. Then the system 
extracts w?s candidate substitutes as follows: 
z Extracts all the synonyms in each synset 
under pos1 as candidate substitutes. 
z If w has no synonym for the i-th synset 
(1?i?n), then extracts the synonyms of its 
nearest hypernym. 
z If pos is r (or a), and no candidate substi-
tute can be extracted as described above, 
                                                 
1 In this task, four kinds of POS are specified: n - noun, v - 
verb, a - adjective, r - adverb.  
then extracts candidate substitutes under the 
POS a (or r). 
3.2 Candidate Substitute Scoring 
As mentioned above, all words in the given sen-
tence can be used as contextual information in the 
scoring of candidate substitutes. However, it is ob-
vious that not all context words are really useful 
when determining a word?s substitutes. An exam-
ple can be seen from Figure 1. 
 
 
She turns eyes <head>bright</head> with 
excitement towards Fiona , still tugging on the 
string of the minitiature airship-cum-dance 
card she has just received at the door . 
Figure 1. An example of a context sentence. 
 
In the example above, words turns, eyes, with, 
and excitement are useful context words, while the 
others are not. The useless contexts may even be 
noise if they are used in the scoring. As a result, it 
is important to select context words carefully. 
In HIT, we select context words based on the 
following assumption: useful context words for 
lexical substitute are those near the target word in 
the given sentence. In other words, the words that 
are far from the target word are not taken into con-
sideration. Obviously, this assumption is not al-
ways true. However, considering only the 
neighboring words can reduce the risk of bringing 
in noise. Besides, Edmonds (1997) has also dem-
onstrated in his paper that short-distance colloca-
tions with neighboring words are more useful in 
lexical choice than long ones. 
Let w be the target word, t a candidate substitute, 
S the context sentence. Our basic idea is that: One 
can substitute w in S with t, which generates a new 
sentence S?. If S? can be found on the web, then the 
substitute is admissible. The more times S? occurs 
on the web, the more probable the substitute is. In 
practice, however, it is difficult to find a whole 
sentence S? on the web due to sparseness. Instead, 
we use fragments of S? which contains t and sev-
eral neighboring context words (based on the as-
sumption above). Then the question is how to ob-
tain one (or more) fragment of S?. 
A window with fixed size can be used here. Su-
ppose p is the position of t in S?, for instance, we 
can construct a fragment using words from posi-
tion p-r to p+r, where r is the radius of window. 
174
However, a fixed r is difficult to set, since it may 
be too large for some sentences, which makes the 
fragments too specific, while too small for some 
other sentences, which makes the fragments too 
loose. An example can be seen in Table 1. 
 
1(a) But when Daniel turned <head>blue</head> 
one time and he totally stopped breathing. 
1(b) Daniel turned t one time 
2(a) We recommend that you <head>check</head> 
with us beforehand. 
2(b) that you t with us 
Table 1. Examples of fragments with fixed size. 
 
In Table1, 1(a) and 2(a) are two sentences from 
the test data of SemEval-2007Task10. 1(b) and 2(b) 
are fragments constructed according to 1(a) and 
2(a), where the window radius is 2 and t denotes 
any candidate substitute of the target word. It is 
obvious that 1(b) is a rather strict fragment, which 
makes it difficult to find sentences containing it on 
the web, while 2(b) is quite loose, which can 
hardly constrain the semantics of t. 
Having considered the problem above, we pro-
pose a rule-based method that constructs fragments 
with varied lengths. Let Ft be a fragment contain-
ing t, the construction rules are as follows: 
Rule-1: Ft must contain at least two words be-
sides t, at least one of which is non-stop word. 
Rule-2: Ft does not cross sub-sentence boundary 
(?,?). 
Rule-3: Ft should be the shortest fragment that 
satisfies Rule-1 and Rule-2. 
According to the rules above, we construct at 
most three fragments for each S?: (1) t occurs at the 
beginning of Ft, (2) t occurs in the middle of Ft, 
and (3) t occurs at the end of Ft. Here we have an-
other constraint: if one constructed fragment F1 is 
the substring of F2, then F2 is removed. Please 
note that the morphology is not taken into account 
when we construct queries. 
For the sentence 1(a) and 2(a) in Table 1, the 
constructed fragments are as follows: 
 
For 1(a): Daniel turned t; t one time; turned t 
one 
For 2(a): recommend that you t; t with us be-
forehand 
Table 2. Examples of the constructed fragments 
 
To score a candidate substitute, we replace ?t? in 
the fragments with each candidate substitute and 
use them as queries, which are then fed to Google. 
The score of t is computed according to the counts 
of retrieved snippets: 
?
=
=
n
i
tWebMining iFSnippetcountn
tScore
1
))((
1
)(     (1) 
where n is the number of constructed fragments, 
Fti is the i-th fragment (query) corresponding to t, 
and count(Snippet(Fti)) is the count of snippets 
retrieved by Fti. 
All candidate substitutes with scores larger than 
0 are ranked and the first 10 substitutes are re-
tained for the oot subtask. If the number of candi-
dates whose scores are larger than 0 is less than 10, 
the system ranks the rest of the candidates by their 
frequencies using a word frequency list. The spare 
capacity is filled with those candidates with largest 
frequencies. For the best subtask, we simply output 
the substitute that ranks first in oot. 
3.3 Detection of Multiwords 
The method used to detect multiword in the HIT 
system is quite similar to that employed in the 
baseline system. We also use WordNet to detect if 
a multiword that includes the target word occurs 
within a window of 2 words before and 2 words 
after the target word.  
A difference from the baseline system lies in 
that our system looks up WordNet using longer 
multiword candidates first. If a longer one is found 
in WordNet, then its substrings will be ignored. 
For example, if we find ?get alng with? in Word-
Net, we will output it as a multiword and will not 
check ?get alng? any more. 
4 Results 
Our system is the only one that participates all the 
three subtasks of Task10, i.e., best, oot, and mw. 
The evaluation results of our system can be found 
in Table 3 to Table 5. Our system ranks the fourth 
in the best subtask and seventh in the oot subtask. 
We have analyzed the results from two aspects, 
i.e., the ability of the system to extract candidate 
substitutes and the ability to rank the correct sub-
stitutes in front. There are a total of 6,873 manual 
substitutes for all the 1,710 items in the gold stan-
dard, only 2,168 (31.54%) of which have been ex-
tracted as candidate substitutes by our system. This 
result suggests that WordNet is not an appropriate 
175
source for lexical substitute extraction. In the fu-
ture work, we will try some other lexical resources, 
such as the Oxford American Writer Thesaurus 
and Encarta. In addition, we will also try the 
method that automatically constructs lexical re-
sources, such as the automatic clustering method. 
Further analysis shows that, 1,388 (64.02%) out 
of the 2,168 extracted correct candidates are 
ranked in the first 10 in the oot output of our sys-
tem. This suggests that there is a big space for our 
system to improve the candidate scoring method. 
In the future work, we will consider more and 
richer features, such as the syntactic features, in 
candidate substitute scoring. Furthermore, A dis-
advantage of this method is that the web mining 
process is quite inefficient. Therefore, we will try 
to use the Web 1T 5-gram Version 1 from Google 
(LDC2006T13) in the future. 
 
 P R ModeP ModeR
OVERALL 11.35 11.35 18.86 18.86 
Further Analysis 
NMWT 11.97 11.97 19.81 19.81 
NMWS 12.55 12.38 19.93 19.65 
RAND 11.81 11.81 20.03 20.03 
MAN 10.81 10.81 17.53 17.53 
Baselines 
WORDNET 9.95 9.95 15.58 15.58 
LIN 8.84 8.53 14.69 14.23 
Table 3. best results. 
 
 P R ModeP ModeR
OVERALL 33.88 33.88 46.91 46.91 
Further Analysis 
NMWT 35.60 35.60 48.48 48.48 
NMWS 36.63 36.63 49.33 49.33 
RAND 33.95 33.95 47.25 47.25 
MAN 33.81 33.81 46.53 46.53 
Baselines 
WORDNET 29.70 29.35 40.57 40.57 
LIN 27.70 26.72 40.47 39.19 
Table 4. oot results. 
 
 Our System WordNet BL 
 P R P R 
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 5. mw results. 
 
Acknowledgements 
This research was supported by National Natural 
Science Foundation of China (60575042, 
60503072, 60675034). 
References 
Barzilay Regina and McKeown Kathleen R. 2001. Ex-
tracting paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL/EACL. 
Bolshakov Igor A. and Gelbukh Alexander. 2004. Syn-
onymous Paraphrasing Using WordNet and Internet. 
In Proceedings of NLDB. 
Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-
shtein Efrat, Strapparava Carlo. 2006. Direct Word 
Sense Matching for Lexical Substitution. In Proceed-
ings of ACL. 
Edmonds Philip. 1997. Choosing the Word Most Typi-
cal in Context Using a Lexical Co-occurrence Net-
work. In Proceedings of ACL. 
Fellbaum Christiane. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Cambridge, MA. 
Glickman Oren, Dagan Ido, Keller Mikaela, Bengio 
Samy. 2006. Investigating Lexical Substitution Scor-
ing for Subtitle Generation. In Proceedings of 
CoNLL. 
Kauchak David and Barzilay Regina. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of 
HLT-NAACL. 
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of the COLING-ACL. 
Lin Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING-ACL. 
Smeaton Alan F., Kelledy Fergus, and O?Donell Ruari. 
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding Posting Lists, Query Expansion 
with WordNet and POS Tagging of Spanish. In Pro-
ceedings of TREC-4. 
Wu Hua and Zhou Ming. 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proceedings of IWP. 
Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and 
Zhang Yu. 2007. Automatic Acquisition of Context-
Specific Lexical Paraphrases. In Proceedings of 
IJCAI-07. 
176
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency-based Syntactic and Semantic Parsing
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cn
Abstract
Our CoNLL 2009 Shared Task system in-
cludes three cascaded components: syntactic
parsing, predicate classification, and semantic
role labeling. A pseudo-projective high-order
graph-based model is used in our syntactic de-
pendency parser. A support vector machine
(SVM) model is used to classify predicate
senses. Semantic role labeling is achieved us-
ing maximum entropy (MaxEnt) model based
semantic role classification and integer linear
programming (ILP) based post inference. Fi-
nally, we win the first place in the joint task,
including both the closed and open challenges.
1 System Architecture
Our CoNLL 2009 Shared Task (Hajic? et al, 2009):
multilingual syntactic and semantic dependencies
system includes three cascaded components: syn-
tactic parsing, predicate classification, and semantic
role labeling.
2 Syntactic Dependency Parsing
We extend our CoNLL 2008 graph-based
model (Che et al, 2008) in four ways:
1. We use bigram features to choose multiple pos-
sible syntactic labels for one arc, and decide the op-
timal label during decoding.
2. We extend the model with sibling features (Mc-
Donald, 2006).
3. We extend the model with grandchildren fea-
tures. Rather than only using the left-most and right-
most grandchildren as Carreras (2007) and Johans-
son and Nugues (2008) did, we use all left and right
grandchildren in our model.
4. We adopt the pseudo-projective approach in-
troduced in (Nivre and Nilsson, 2005) to handle the
non-projective languages including Czech, German
and English.
2.1 Syntactic Label Determining
The model of (Che et al, 2008) decided one la-
bel for each arc before decoding according to uni-
gram features, which caused lower labeled attach-
ment score (LAS). On the other hand, keeping all
possible labels for each arc made the decoding in-
efficient. Therefore, in the system of this year, we
adopt approximate techniques to compromise, as
shown in the following formulas.
f lbluni(h, c, l) = f lbl1 (h, 1, d, l) ? f lbl1 (c, 0, d, l)
L1(h, c) = arg maxK1l?L(w ? f lbluni(h, c, l))
f lblbi (h, c, l) = f lbl2 (h, c, l)
L2(h, c) = arg maxK2l?L1(h,c)(w ? {f lbluni ? f lblbi })
For each arc, we firstly use unigram features to
choose the K1-best labels. The second parameter of
f lbl1 (?) indicates whether the node is the head of the
arc, and the third parameter indicates the direction.
L denotes the whole label set. Then we re-rank the
labels by combining the bigram features, and choose
K2-best labels. During decoding, we only use the
K2 labels chosen for each arc (K2 ? K1 < |L|).
2.2 High-order Model and Algorithm
Following the Eisner (2000) algorithm, we use spans
as the basic unit. A span is defined as a substring
of the input sentence whose sub-tree is already pro-
duced. Only the start or end words of a span can link
with other spans. In this way, the algorithm parses
the left and the right dependence of a word indepen-
dently, and combines them in the later stage.
We follow McDonald (2006)?s implementation of
first-order Eisner parsing algorithm by modifying its
scoring method to incorporate high-order features.
Our extended algorithm is shown in Algorithm 1.
There are four different span-combining opera-
tions. Here we explain two of them that correspond
to right-arc (s < t), as shown in Figure 1 and 2. We
49
Algorithm 1 High-order Eisner Parsing Algorithm
1: C[s][s][c] = 0, 0 ? s ? N , c ? cp, icp # cp: complete; icp: incomplete
2: for j = 1 to N do
3: for s = 0 to N do
4: t = s+ jL
5: if t > N then
6: break
7: end if
# Create incomplete spans
8: C[s][t][icp] = maxs?r<t;l?L2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))
9: C[t][s][icp] = maxs?r<t;l?L2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))
# Create complete spans
10: C[s][t][cp] = maxs<r?t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))
11: C[t][s][cp] = maxs?r<t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))
12: end for
13: end for
follow the way of (McDonald, 2006) and (Carreras,
2007) to represent spans. The other two operations
corresponding to left-arc are similar.
 
Figure 1: Combining two spans into an incomplete span
Figure 1 illustrates line 8 of the algorithm in Al-
gorithm 1, which combines two complete spans into
an incomplete span. A complete span means that
only the head word can link with other words fur-
ther, noted as ??? or ???. An incomplete span
indicates that both the start and end words of the
span will link with other spans in the future, noted as
?99K? or ?L99?. In this operation, we combine two
smaller spans, sps?r and spr+1?t, into sps99Kt with
adding arcs?t. As shown in the following formu-
las, the score of sps99Kt is composed of three parts:
the score of sps?r, the score of spr+1?t, and the
score of adding arcs?t. The score of arcs?t is
determined by four different feature sets: unigram
features, bigram features, sibling features and left
grandchildren features (or inside grandchildren fea-
tures, meaning that the grandchildren lie between s
and t). Note that the sibling features are only related
to the nearest sibling node of t, which is denoted as
sck here. And the inside grandchildren features are
related to all the children of t. This is different from
the models used by Carreras (2007) and Johansson
and Nugues (2008). They only used the left-most
child of t, which is tck? here.
ficp(s, r, t, l) = funi(s, t, l) ? fbi(s, t, l)
? fsib(s, sck, t) ? {?k?i=1 fgrand(s, t, tci, l)}
Sicp(s, r, t, l) = w ? ficp(s, r, t, l)
S(sps99Kt) = S(sps?r) + S(spr+1?t)
+ Sicp(s, r, t, l)
In Figure 2 we combine sps99Kr and spr?t into
sps?t, which explains line 10 in Algorithm 1. The
score of sps?t also includes three parts, as shown
in the following formulas. Although there is no new
arc added in this operation, the third part is neces-
sary because it reflects the right (or called outside)
grandchildren information of arcs?r.
r trc1 rcks r s tr rc1 rck
l l
 
Figure 2: Combining two spans into a complete span
fcp(s, r, t, l) = ?ki=1 fgrand(s, r, rci, l)
Scp(s, r, t, l) = w ? fcp(s, r, t, l)
S(sps?t) = S(sps99Kr)
+ S(spr?t) + Scp(s, r, t, l)
50
2.3 Features
As shown above, features used in our model can be
decomposed into four parts: unigram features, bi-
gram features, sibling features, and grandchildren
features. Each part can be seen as two different sets:
arc-related and label-related features, except sibling
features, because we do not consider labels when us-
ing sibling features. Arc-related features can be un-
derstood as back-off of label-related features. Actu-
ally, label-related features are gained by simply at-
taching the label to the arc-features.
The unigram and bigram features used in our
model are similar to those of (Che et al, 2008), ex-
cept that we use bigram label-related features. The
sibling features we use are similar to those of (Mc-
Donald, 2006), and the grandchildren features are
similar to those of (Carreras, 2007).
3 Predicate Classification
The predicate classification is regarded as a super-
vised word sense disambiguation (WSD) task here.
The task is divided into four steps:
1. Target words selection: predicates with multi-
ple senses appearing in the training data are selected
as target words.
2. Feature extraction: features in the context
around these target words are extracted as shown in
Table 4. The detailed explanation about these fea-
tures can be found from (Che et al, 2008).
3. Classification: for each target word, a Support
Vector Machine (SVM) classifier is used to classify
its sense. As reported by Lee and Ng (2002) and
Guo et al (2007), SVM shows good performance on
the WSD task. Here libsvm (Chang and Lin, 2001)
is used. The linear kernel function is used and the
trade off parameter C is 1.
4. Post processing: for each predicate in the test
data which does not appear in the training data, its
first sense in the frame files is used.
4 Semantic Role Labeling
The semantic role labeling (SRL) can be divided
into two separate stages: semantic role classification
(SRC) and post inference (PI).
During the SRC stage, a Maximum en-
tropy (Berger et al, 1996) classifier is used to
predict the probabilities of a word in the sentence
Language No-duplicated-roles
Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc
Chinese A0, A1, A2, A3, A4, A5,
Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND
English A0, A1, A2, A3, A4, A5,
German A0, A1, A2, A3, A4, A5,
Japanese DE, GA, TMP, WO
Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,
arg2-loc, arg2-null, arg4-des, argL-null, argM-
cau, argM-ext, argM-fin
Table 1: No-duplicated-roles for different languages
to be each semantic role. We add a virtual role
?NULL? (presenting none of roles is assigned)
to the roles set, so we do not need semantic role
identification stage anymore. For a predicate
of each language, two classifiers (one for noun
predicates, and the other for verb predicates) predict
probabilities of each word in a sentence to be each
semantic role (including virtual role ?NULL?). The
features used in this stage are listed in Table 4.
The probability of each word to be a semantic role
for a predicate is given by the SRC stage. The re-
sults generated by selecting the roles with the largest
probabilities, however, do not satisfy some con-
strains. As we did in the last year?s system (Che et
al., 2008), we use the ILP (Integer Linear Program-
ming) (Punyakanok et al, 2004) to get the global op-
timization, which is satisfied with three constrains:
C1: Each word should be labeled with one and
only one label (including the virtual label ?NULL?).
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?). The
threshold we use in our system is 0.3.
C3: Statistics show that some roles (except for
the virtual role ?NULL?) usually appear once for
a predicate. We impose a no-duplicate-roles con-
straint with a no-duplicate-roles list, which is con-
structed according to the times of semantic roles?
duplication for each single predicate. Table 1 shows
the no-duplicate-roles for different languages.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit1. The
classifier parameters are tuned with the development
data for different languages respectively. lp solve
5.52 is chosen as our ILP problem solver.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
2http://sourceforge.net/projects/lpsolve
51
5 Experiments
5.1 Experimental Setup
We participate in the CoNLL 2009 shared task
with all 7 languages: Catalan (Taule? et al, 2008),
Chinese (Palmer and Xue, 2009), Czech (Hajic? et
al., 2006), English (Surdeanu et al, 2008), Ger-
man (Burchardt et al, 2006), Japanese (Kawahara
et al, 2002), and Spanish (Taule? et al, 2008). Be-
sides the closed challenge, we also submitted the
open challenge results. Our open challenge strategy
is very simple. We add the SRL development data
of each language into their training data. The pur-
pose is to examine the effect of the additional data,
especially for out-of-domain (ood) data.
Three machines (with 2.5GHz Xeon CPU and
16G memory) were used to train our models. Dur-
ing the peak time, Amazon?s EC2 (Elastic Com-
pute Cloud)3 was used, too. Our system requires
15G memory at most and the longest training time
is about 36 hours.
During training the predicate classification (PC)
and the semantic role labeling (SRL) models, golden
syntactic dependency parsing results are used. Pre-
vious experiments show that the PC and SRL test re-
sults based on golden parse trees are slightly worse
than that based on cross trained parse trees. It is,
however, a pity that we have no enough time and ma-
chines to do cross training for so many languages.
5.2 Results and Discussion
In order to examine the performance of the ILP
based post inference (PI) for different languages, we
adopt a simple PI strategy as baseline, which se-
lects the most likely label (including the virtual la-
bel ?NULL?) except for those duplicate non-virtual
labels with lower probabilities (lower than 0.5). Ta-
ble 2 shows their performance on development data.
We can see that the ILP based post inference can
improve the precision but decrease the recall. Ex-
cept for Czech, almost all languages are improved.
Among them, English benefits most.
The final system results are shown in Table 3.
Comparing with our CoNLL 2008 (Che et al, 2008)
syntactic parsing results on English4, we can see that
our new high-order model improves about 1%.
3http://aws.amazon.com/ec2/
4devel: 85.94%, test: 87.51% and ood: 80.73%
Precision Recall F1
Catalan simple 78.68 77.14 77.90
Catalan ILP 79.42 76.49 77.93
Chinese simple 80.74 74.36 77.42
Chinese ILP 81.97 73.92 77.74
Czech simple 88.54 84.68 86.57
Czech ILP 89.23 84.05 86.56
English simple 83.03 83.55 83.29
English ILP 85.63 83.03 84.31
German simple 78.88 75.87 77.34
German ILP 82.04 74.10 77.87
Japanese simple 88.04 70.68 78.41
Japanese ILP 89.23 70.16 78.56
Spanish simple 76.73 75.92 76.33
Spanish ILP 77.71 75.34 76.51
Table 2: Comparison between different PI strategies
For the open challenge, because we did not mod-
ify the syntactic training data, its results are the same
as the closed ones. We can, therefore, examine the
effect of the additional training data on SRL. We can
see that along with the development data are added
into the training data, the performance on the in-
domain test data is increased. However, it is inter-
esting that the additional data is harmful to the ood
test.
6 Conclusion and Future Work
Our CoNLL 2009 Shared Task system is com-
posed of three cascaded components. The pseudo-
projective high-order syntactic dependency model
outperforms our CoNLL 2008 model (in English).
The additional in-domain (devel) SRL data can help
the in-domain test. However, it is harmful to the ood
test. Our final system achieves promising results. In
the future, we will study how to solve the domain
adaptive problem and how to do joint learning be-
tween syntactic and semantic parsing.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60675034, and the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144.
52
Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score
devel test ood devel test ood devel test ood
Catalan closed 86.65 86.56 ?? 77.93 77.10 ?? 82.30 81.84 ??open ?? ?? 77.36 ?? 81.97
Chinese closed 75.73 75.49 ?? 77.74 77.15 ?? 76.79 76.38 ??open ?? ?? 77.23 ?? 76.42
Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66open ?? ?? 86.57 85.21 ?? 83.31 80.63
English closed 87.09 88.48 81.57 84.30 85.51 73.82 85.70 87.00 77.71open ?? ?? 85.61 73.66 ?? 87.05 77.63
German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19open ?? ?? 78.61 70.09 ?? 82.44 73.20
Japanese closed 92.55 92.57 ?? 78.56 78.26 ?? 85.86 85.65 ??open ?? ?? 78.35 ?? 85.70
Spanish closed 87.22 87.33 ?? 76.51 76.47 ?? 81.87 81.90 ??open ?? ?? 76.66 ?? 82.00
Average closed ?? 85.23 77.90 ?? 79.94 76.38 ?? 82.64 77.19open 80.06 76.32 82.70 77.15
Table 3: Final system results
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In LREC-2006.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP/CoNLL-
2007.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded
syntactic and semantic dependency parsing system. In
CoNLL-2008.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies.
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. HIT-IR-WSD: A wsd system for
english lexical sample task. In SemEval-2007.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP-2008.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In LREC-2002.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP-
2002.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In ACL-2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1).
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Coling-2004.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In LREC-2008.
53
Catalan Chinese Czech English German Japanese Spanish
ChildrenPOS ? ? ??
ChildrenPOSNoDup ? ? ? ?
ConstituentPOSPattern ? ? ? ? ? ? ? ? ? ? ? ? ? ?
ConstituentPOSPattern+DepRelation ? ? ? ? ? ?
ConstituentPOSPattern+DepwordLemma ? ? ? ? ? ?
ConstituentPOSPattern+HeadwordLemma ? ? ? ? ? ? ? ? ? ?
DepRelation N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? N M ? ?
DepRelation+DepwordLemma ? ? ? ?
DepRelation+Headword N M N M N N M N M N
DepRelation+HeadwordLemma ? ? ? ? ? ? ? ?
DepRelation+HeadwordLemma+DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepRelation+HeadwordPOS N M N M N M N M N M N
Depword ? ? ? ?
DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepwordLemma+HeadwordLemma ? ? ? ? ? ?
DepwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ?
DepwordPOS N M N M N M ? ? N M N M ? ? N M
DepwordPOS+HeadwordPOS ? ? ? ?
DownPathLength ? ? ? ?
FirstLemma ? ? ? ? ? ? ? ? ? ? ? ?
FirstPOS ? ? ? ?
FirstPOS+DepwordPOS ? ? ? ? ? ?
FirstWord ? ? ? ?
Headword N M N M N M N M N M ? ? N
HeadwordLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N
HeadwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ? ? ?
HeadwordPOS N M N M N M ? ? N M ? ? N M ? ? N M
LastLemma ? ? ? ? ? ? ? ? ? ?
LastPOS ? ? ? ?
LastWord ? ?
Path ? ? ? ? ? ? ? ? ? ? ? ?
Path+RelationPath ? ? ? ? ? ? ? ? ? ?
PathLength ? ? ? ? ? ? ? ? ? ? ? ?
PFEAT N M N M N M
PFEATSplit N M ? ? N M ? ? N M ? ? N M ? ?
PFEATSplitRemoveNULL N M N M N M
PositionWithPredicate ? ? ? ? ? ? ? ? ? ?
Predicate N M ? ? N M N M ? ? N M N M N M ? ?
Predicate+PredicateFamilyship ? ? ? ? ? ? ? ? ? ?
PredicateBagOfPOSNumbered M N M N M N M
PredicateBagOfPOSNumberedWindow5 N M N M N M N M N M
PredicateBagOfPOSOrdered N M N M N M N M N
PredicateBagOfPOSOrderedWindow5 N M N M N M N M N M N M
PredicateBagOfPOSWindow5 N N M N M N M N M N
PredicateBagOfWords M N M N N M N M
PredicateBagOfWordsAndIsDesOfPRED N M N M M N M N M
PredicateBagOfWordsOrdered M N M N M M N M N M
PredicateChildrenPOS N M ? ? N M N M N M N M N M ? ?
PredicateChildrenPOSNoDup N M N M N M N M N M N M
PredicateChildrenREL N M ? ? N M N M N M N M ? ? N M
PredicateChildrenRELNoDup N M ? ? N M N M N M N M ? ? N M
PredicateFamilyship ? ?
PredicateLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N M ? ?
PredicateLemma+PredicateFamilyship ? ? ? ? ? ?
PredicateSense ? ? ? ? ? ? ? ? ? ? ? ? ? ?
PredicateSense+DepRelation ? ? ? ?
PredicateSense+DepwordLemma ? ? ? ?
PredicateSense+DepwordPOS ? ? ? ?
PredicateSiblingsPOS N M N M N N M N M N M
PredicateSiblingsPOSNoDup N M ? ? N M N M N M N M N M ? ?
PredicateSiblingsREL N M ? ? N M N M N M N M N M
PredicateSiblingsRELNoDup N M N M ? ? M N M N M ? ? N M ? ?
PredicateVoiceEn N M
PredicateWindow5Bigram N M N M N M N M
PredicateWindow5BigramPOS N M N M N M N M N M N M
RelationPath ? ? ? ? ? ? ? ? ? ? ? ? ? ?
SiblingsPOS ? ? ? ?
SiblingsREL ?
SiblingsRELNoDup ? ? ? ?
UpPath ? ? ? ? ? ? ?
UpPathLength ? ?
UpRelationPath ? ? ? ? ? ?
UpRelationPath+HeadwordLemma ? ? ? ? ? ? ? ?
Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). N: noun predicate
PC, M: verb predicate PC, ?: noun predicate SRL, ?: verb predicate SRL.
54
HITIQA: Towards Analytical Question Answering 
Sharon Small1, Tomek Strzalkowski1, Ting Liu1, Sean Ryan1, Robert Salkin1,  
Nobuyuki Shimizu1, Paul Kantor2, Diane Kelly2, Robert Rittman2, Nina Wacholder2 
 
1The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tl7612,seanryan, 
rs6021,ns3202}@albany.edu 
2Rutgers University 
4 Huntington Street 
New Brunswick, NJ 08904 
{kantor,diane,hitiqa, 
wacholder}@scils.rutgers.edu
 
Abstract 
In this paper we describe the analytic 
question answering system HITIQA (High-
Quality Interactive Question Answering) 
which has been developed over the last 2 years 
as an advanced research tool for information 
analysts. HITIQA is an interactive open-
domain question answering technology 
designed to allow analysts to pose complex 
exploratory questions in natural language and 
obtain relevant information units to prepare 
their briefing reports. The system uses novel 
data-driven semantics to conduct a 
clarification dialogue with the user that 
explores the scope and the context of the 
desired answer space. The system has 
undergone extensive hands-on evaluations by 
a group of intelligence analysts. This 
evaluation validated the overall approach in 
HITIQA but also exposed limitations of the 
current prototype.  
1 Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as 
?What has been Russia?s reaction to the U.S. 
bombing of Kosovo?? The distinguishing property 
of such questions is that one cannot generally 
anticipate what might constitute the answer. While 
certain types of things may be expected (e.g., 
diplomatic statements), the answer is heavily 
conditioned by what information is in fact 
available on the topic. From a practical viewpoint, 
analytical questions are often underspecified, thus 
casting a broad net on a space of possible answers. 
Questions posed by professional analysts are 
aimed to probe the available data along certain 
dimensions. The results of these probes determine 
follow up questions, if necessary. Furthermore, at 
any stage clarifications may be needed to adjust 
the scope and intent of each question. Figure 1 
shows a fragment of an analytical session with 
HITIQA; note that these questions are not aimed at 
factoids, despite their simple form. 
User: What is the history of the nuclear arms 
program linking Iraq and other countries in the 
region? 
HITIQA: [responses and clarifications] 
User: Who financed the nuclear arms program 
in Iraq? 
HITIQA:? 
User: Has Iraq been able to import uranium? 
HITIQA:? 
User: What type of debt does exist between Iraq 
and her trading partners in the region? 
FIGURE 1: A fragment of an analyst?s session 
with HITIQA 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question 
answering.  In this paper we focus on three aspects 
of our work: 
1. Question Semantics: how the system 
?understands? user requests 
2. Human-Computer Dialogue: how the user and 
the system negotiate this understanding 
3. User Evaluations and Results 
2 Factoid vs. Analytical QA 
There are significant differences between 
factoid, or fact-finding, and analytical question 
answering.  A factoid question is normally 
understood to seek a piece of information that 
would make a corresponding statement true (i.e., it 
becomes a fact): ?How many states are in the 
U.S.?? / ?There are X states in the U.S.? In this 
sense, a factoid question usually has just one 
correct answer that can generally be judged for its 
truthfulness with respect to some information 
source.  
As noted by Harabagiu et al (1999), factoid 
questions display a distinctive ?answer type?, 
which is the type of the information item needed 
for the answer, e.g., ?person? or ?country?, etc. 
Most existing factoid QA systems deduct this 
expected answer type from the form of the 
question using a finite list of possible answer 
types. For example, ?Who was the first man in 
space? expects a ?person? as the answer type. This 
is generally a very good strategy that has been 
exploited successfully in a number of automated 
QA systems, especially in the context of TREC 
QA1 evaluations. Given the excellent results posted 
by the best systems and an adequate performance 
attained even by some entry-level system, we 
believe that the process of factoid question 
answering is now fairly well understood 
(Harabagiu et al, 2002; Hovy et al, 2000; Prager 
at al., 2001, Wu et al, 2003). 
   In contrast to a factoid question, an analytical 
question has a virtually unlimited variety of 
syntactic forms with only a loose connection 
between their syntax and the expected answer. 
Given the many possible forms of analytical 
questions, it would be counter-productive to 
restrict them to a predefined number of 
question/answer types. Therefore, the formation of 
an answer in analytical QA should instead be 
guided by the user?s intended interest expressed in 
the question, as well as through any follow up 
dialogue with the system. This clearly involves 
user's intentions (the speech acts) and how they 
evolve with respect to the overall information 
strategy they are pursuing. 
In this paper we argue that the semantics 
(though not necessarily the intent) of an analytical 
question is more likely to be deduced from the 
information that is considered relevant to the 
question than through a detailed analysis of its 
particular form. We noted that the questions 
analysts ask, while clearly part of a strategy, are 
generally quite flexible and ?forgiving?, in the 
sense that there is always a strong possibility that 
the answer may not arrive in the expected form, 
and thus a change of strategy, and even the initial 
expectations, may be warranted. This suggests 
strongly that a solution to analytic QA must 
involve a dialogue that combines information 
seeking and problem solving strategies. 
3 Document Retrieval 
HITIQA works with unstructured text data, 
which means that a document retrieval step is 
required to detect any information that may be 
relevant to the user question. It has to be noted that 
determining ?relevant? information is not the same 
as finding an answer; indeed we can use relatively 
simple information retrieval methods (keyword 
matching, etc.) to obtain perhaps 200 ?relevant? 
                                                     
1 TREC QA is the annual Question Answering evaluation 
sponsored by the U.S. National Institute of Standards and Technology 
www.trec.nist.gov 
documents from a database. This gives us an initial 
information space to work on in order to determine 
the scope and complexity of the answer, but we are 
nowhere near the answer yet. The current version 
of HITIQA uses the INQUERY system (Callan et 
al., 1992), although we have also used SMART 
(Buckley, 1985) and other IR systems (such as 
Google).   
4 Text Framing 
In HITIQA we use a text framing technique to 
delineate the gap between the possible meaning of 
the user?s question and the system ?understanding? 
of this question. We can approximate the meaning 
of the question by extracting references to known 
concepts in it, including named entities. The 
information retrieved from the database may well 
lead to other interpretations of the question, and we 
need to determine which of these are ?correct?.  
The framing process imposes a partial structure 
on the text passages that allows the system to 
systematically compare different passages against 
each other and against the question. Framing is not 
attempting to capture the entire meaning of the 
passage; it needs to be just sufficient enough to 
communicate with the user about the differences in 
their question and the returned text. In particular, 
the framing process may uncover topics or aspects 
within the answer space which the user has not 
explicitly asked for, and thus may be unaware of 
their existence. If these topics or aspects align 
closely with the user?s question, (i.e., matching 
many of the salient attributes) we may want to 
make the user aware of them and let him/her 
decide if they should be included in the answer.   
Frames are built from the retrieved data, after 
clustering it into several topical groups. Passages 
are clustered using a combination of hierarchical 
clustering and n-bin classification (Hardy et al, 
2002a). Each cluster represents a topic theme 
within the retrieved set: usually an alternative or 
complimentary interpretation of the user?s 
question. Since clusters are built out of small text 
passages, we initially associate a frame with each 
passage that serves as a seed of a cluster. We 
subsequently merge passages and their associated 
frames to arrive at one or more combined frames 
for the cluster. 
HITIQA starts text framing by building a 
general frame on the seed passages of the clusters 
and any of the top N (currently N=10) scored 
passages that are not already in a cluster. The 
general frame represents an event or a relation 
involving any number of entities, which make up 
the frame?s attributes, such as LOCATION, PERSON, 
ORGANIZATION, DATE, etc. Attributes are extracted 
from text passages by BBN?s Identifinder, which 
tags 24 types of named entities. The event/relation 
itself could be pretty much anything, e.g., accident, 
pollution, trade, etc. and it is captured into the 
TOPIC attribute from the central verb or noun 
phrase of the passage. In the general frame, 
attributes have no assigned roles; they are loosely 
grouped around the TOPIC (Figure 2).  
We have also defined three slightly more 
specialized typed frames by assigning roles to 
selected attributes in the general frame. These 
three ?specialized? frames are: (1) a Transfer 
frame with three roles including FROM, TO and 
OBJECT; (2) a two-role Relation frame with AGENT 
and OBJECT roles; and (3) an one-role Property 
frame. These typed frames represent certain 
generic events/relationships, which then map into 
more specific event types in each domain. Other 
frame types may be defined if needed, but we do 
not anticipate there will be more than a handful all 
together.2 For example, another 3-role frame may 
be State-Change frame with AGENT, OBJECT and 
INSTRUMENT roles, etc.3  
FRAME TYPE: General 
TOPIC: imported 
LOCATION: Iraq, France, Israel 
ORGANIZATION: IAEA [missed: Nukem] 
PERSON: Leonard Spector 
WEAPON: uranium, nuclear bomb 
DATES: 1981, 30 November 1990, .. 
FIGURE 2: A general frame obtained from the 
text passage in Figure 3 (not all attributes shown). 
 
Where the general frame is little more than just 
a ?bag of attributes?, the typed frames capture 
some internal structure of an event, but only to the 
extent required to enable an efficient dialogue with 
the user. Typed frames are ?triggered? by 
appearance of specific words in text, for example 
the word export may trigger a Transfer frame. A 
single text passage may invoke one or more typed 
frames, or none at all. When no typed frame is 
invoked, the general frame is used as default. If a 
typed frame is invoked, HITIQA will attempt to 
identify the roles, e.g. FROM, TO, OBJECT, etc. This 
is done by mapping general frame attributes 
selected from text onto the typed attributes in the 
frames. In any given domain, e.g., weapon non-
proliferation, both the trigger words and the role 
identification rules can be specialized from a 
                                                     
2 Scalability is certainly an outstanding issue here, and we are 
working on effective frame acquisition methods, which is outside of 
the scope of this paper. While classifications such as (Levin, 1993) or 
FrameNet (Fillmore, 2001) are relevant, we are currently aiming at a 
less detailed system. 
3 A more detailed discussion of possible frame types is beyond the 
scope of the current paper. 
training corpus of typical documents and 
questions. For example, the role-id rules rely both 
on syntactic cues and the expected entity types, 
which are domain adaptable.  
Domain adaptation is desirable for obtaining 
more focused dialogue, but it is not necessary for 
HITIQA to work. We used both setups under 
different conditions: the generic frames were used 
with TREC document collection to measure impact 
of IR precision on QA accuracy (Small et al, 
2004). The domain-adapted frames were used for 
sessions with intelligence analysts working with 
the WMD Domain (see below). Currently, the 
adaptation process includes manual tuning 
followed by corpus bootstrapping using an 
unsupervised learning method (Strzalkowski & 
Wang, 1996). We generally rely on BBN?s 
Identifinder for extraction of basic entities, and use 
bootstrapping to define additional entity types as 
well as to assign roles to attributes. 
The version of HITIQA reported here and used 
by analysts during the evaluation has been adapted 
to the Weapons of Mass Destruction Non-
Proliferation domain (WMD domain, henceforth).  
Figure 3 contains an example passage from this 
data set. In the WMD domain, the typed frames 
were mapped onto WMDTransfer 3-role frame, 
and two 2-role frames WMDTreaty  and 
WMDDevelop. Adapting the frames to the WMD 
domain required very minimal modification, such 
as adding the WEAPON entity to augment the 
Identifinder entity set, generating a list of 
international weapon control treaties, etc. 
The Bush Administration claimed that Iraq was 
within one year of producing a nuclear bomb. On 
30 November 1990... Leonard Spector said that 
Iraq possesses 200 tons of natural uranium 
imported and smuggled from several countries. 
Iraq possesses a few working centrifuges and the 
blueprints to build them. Iraq imported centrifuge 
materials from Nukem of the FRG and from other 
sources. One decade ago, Iraq imported 27 pounds 
of weapons-grade uranium from France, for Osirak 
nuclear research center. In 1981, Israel destroyed 
the Osirak nuclear reactor. In November 1990, the 
IAEA inspected Iraq and found all material 
accounted for....  
FIGURE 3: A text passage from the WMD 
domain data    
 
HITIQA frames define top-down constraints on 
how to interpret a given text passage, which is 
quite different from MUC4 template filling task 
                                                     
4 MUC, the Message Understanding Conference, funded by 
DARPA, involved the evaluation of information extraction systems 
applied to a common task. 
(Humphreys et al, 1998). What we?re trying to do 
here is to ?fit? a frame over a text passage. This 
also means that multiple frames can be associated 
with a text passage, or to be exact, with a cluster of 
passages. Since most of the passages that undergo 
the framing process are part of some cluster of 
very similar passages, the added redundancy helps 
to reinforce the most salient features for extraction. 
This makes the framing process potentially less 
error-prone than MUC-style template filling. 
A very similar framing process is applied to the 
user?s question, resulting in one or more Goal 
frames, which are subsequently compared to the 
data frames obtained from retrieved text passages. 
A Goal frame can be a general frame or any of the 
typed frames. Goal frames generated from the 
question, ?Has Iraq been able to import 
uranium?? are shown in Figures 4 and 5. 
FRAME TYPE: General 
TOPIC: import 
WEAPON:  uranium 
LOCATION: Iraq 
FIGURE 4: A general goal frame from the Iraq 
question 
The frame in Figure 4 is simply a General 
frame which is invoked first. HITIQA then 
discovers that TOPIC=import denotes a Transfer-
event in the WMD domain, so it creates a 
WMDTransfer frame that replaces the general 
frame. This new frame, shown in Figure 5, has 
three role attributes TRF_TO, TRF_FROM and 
TRF_OBJECT, plus the relation type (TRF_TYPE). 
Each role attribute is defined over an underlying 
general frame attribute (given in parentheses), 
which are used to compare frames of different 
types.  The role-id rules rely both on syntactic cues 
and the expected entity types, which are domain 
adaptable. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): import 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION, ORGANIZATION): ? 
TRF_OBJECT (WEAPON): uranium 
FIGURE 5: A typed goal frame from the Iraq 
question 
HITIQA automatically judges a particular data 
frame as relevant, and subsequently the 
corresponding segment of text as relevant, by 
comparison to the Goal frame. The data frames are 
scored based on the number of conflicts found with 
the Goal frame. The conflicts are mismatches on 
values of corresponding attributes, specifically 
when the data frame attribute list does not contain 
any of the entities in the corresponding Goal 
Frame attribute list.  If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.   
All other data frames are scored with an 
increasing value based on the number of conflicts, 
score 1 for frames with one conflict with the Goal 
frame, score 2 for two conflicts etc. Frames that 
conflict with all information found in the query are 
given the score 99 indicating the lowest rank. 
Currently, frames with a conflict score 99 are 
excluded from further processing as outliers. The 
frame in Figure 6 is scored as relevant to the user?s 
query and included in the answer space. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): imported 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION): France 
TRF_OBJECT (WEAPON): uranium 
CONFLICT SCORE: 0 
FIGURE 6: A typed frame obtained from the 
text passage in Figure 3, in response to the Iraq 
question 
5 Enabling Dialogue with the User 
Framed information allows HITIQA to 
automatically judge text passages as fully or 
partially relevant and to conduct a meaningful 
dialogue with the user about their content. The 
purpose of the dialogue is to help the user navigate 
the answer space and to negotiate more precisely 
what information he or she is seeking. The main 
principle here is that the dialogue is primarily 
content oriented. Thus, it is okay to ask the user 
whether information about the AIDS conference in 
Cape Town should be included in the answer to a 
question about combating AIDS in Africa. 
However, the user should never be asked if a 
particular keyword is useful or not, or if a 
document is relevant or not.  
Our approach to dialogue in HITIQA is 
modeled to some degree upon the mixed-initiative 
dialogue management adopted in the AMITIES 
project (Hardy et al, 2002b). The main advantage 
of the AMITIES model is its reliance on data-
driven semantics which allows for spontaneous 
and mixed initiative dialogue to occur. By contrast, 
the major approaches to implementation of 
dialogue systems to date rely on systems of 
functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial 
systems, such as in various telephony products, as 
well as in practically oriented research prototypes 
(e.g., DARPA Communicator; Seneff and Polifoni, 
2000; Ferguson and Allen, 1998), a complete 
dialogue transition graph is designed to guide the 
conversation and predict user responses, which is 
suitable for closed domains only. In the statistical 
variation of this approach, a transition graph is 
derived from a large body of annotated 
conversations (e.g., Walker, 2000; Litman and Pan, 
2002). This latter approach is facilitated through a 
dialogue annotation process, e.g., using Dialogue 
Act Markup in Several Layers (DAMSL) (Allen 
and Core, 1997), which is a system of functional 
dialogue acts.  
Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be 
considered. In this paper we are concentrating 
exclusively on the semantic layer. 
6 Clarification Dialogue 
The clarification dialogue is when the user and 
the system negotiate the information task that 
needs to be performed. Data frames with a conflict 
score of 0 form the initial kernel answer space and 
HITIQA proceeds by generating an answer from 
this space. Depending upon the presence of other 
frames outside of this set, the system may initiate a 
dialogue with the user. When the Goal frame is a 
general frame HITIQA first initiates a clarification 
dialogue on existing general data frames that have 
one conflict. All of these 1-conflict general frames 
are first grouped on their common conflict 
attribute. HITIQA begins asking the user questions 
on these near-miss frame groups, with the largest 
group first. The groups must be at least groups of 
size N, where N is a user controlled setting.  This 
setting restricts of all HITIQA?s generated 
dialogue. HITIQA then check for the existence of 
any data frames that are one of the three typed 
frames. Clarification dialogue will be initiated on 
these, when all of their general attributes agree 
with the general attributes of the Goal frame 
respectively. Alternatively, if the Goal frame is one 
of the three type specific frames, a clarification 
dialogue is first initiated on groups of one conflict 
data frames that are the same type as the Goal 
frame. The clarification dialogue will then 
continue to the remaining two type specific frames 
if any exist, and finally on to any General data 
frames. 
A 1-conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a 
mismatch on any of the general frame attributes, 
for example, LOCATION, ORGANIZATION, TIME, 
etc., or in one of the role-assigned attributes, TO, 
FROM, OBJECT, etc.  A special case arises when the 
conflict occurs on the TOPIC attribute, which 
indicated the event type. Since all other attributes 
match, we may be looking at potentially different 
events (though similar type events) involving the 
same entities, occurring at the same location or 
time. The purpose of the clarification dialogue in 
this case is to probe which of these additional 
events may be of interest to the user.  
Another special case arises when the Goal 
frame has a different value of the TYPE attribute 
than a data frame. The purpose of the clarification 
dialogue here is to see if the user wishes to expand 
the answer space to include events of a different 
type. This situation is illustrated in the exchange 
shown in Figure 7.  
The clarification question asked by HITIQA in 
Figure 7 was obtained by comparing the Goal 
frame (Figure 5) to a partly matching frame 
(Figure 8) generated from another text passage. 
We note first that the Goal frame for this example 
is WMDTransfer type, while the data frame in 
Figure 8 is WMDDevelop type. Nonetheless, both 
frames match on their general-frame attributes 
WEAPON and LOCATION. Therefore, HITIQA asks 
the user if it should expand the answer space to 
include development of uranium in Iraq as well. 
User: ?Has Iraq been able to import uranium?? 
HITIQA: IRAQ REPORTED TO HAVE 
IMPORTED MISSILES AND URANIUM  
Supporting passage:  
...that Iraq possesses 200 tons of natural 
uranium imported and smuggled from several 
countries. Iraq possesses a few working 
centrifuges and the blueprints to build them. Iraq 
imported centrifuge materials from Nukem of the 
FRG and from other sources. One decade ago, 
Iraq imported 27 pounds of weapons-grade 
uranium from France, for Osirak nuclear research 
center... 
HITIQA: ?Are you also interested in 
background information on the uranium 
development program in Iraq?? 
User: ? 
 
FIGURE 7:  The clarification dialogue detail 
During the dialogue, as new information is 
obtained from the user, the Goal frame is updated 
and the scores of all the data frames are 
reevaluated.  If the user responds the equivalent of 
?yes? to the system clarification question in the 
dialogue in Figure 7, a corresponding 
WMDDevelop frame will be added to the set of 
active Goal frames and all WMDDevelop frames 
obtained from text passages will be re-scored for 
possible inclusion in the answer. 
FRAME TYPE: WMDDevelop    
DEV_TYPE (TOPIC): development, produced 
DEV_OBJ (WEAPON): nuc. weapons, uranium 
DEV_AGENT (LOCATION): Iraq, Tuwaitha 
CONFLICT SCORE: 2 
Conflicts with FRAME_TYPE and TOPIC  
FIGURE 8: A 2-conflict frame against the 
Iraq/uranium question that generated the dialogue 
in Figure 7. 
The user may end the dialogue at any point using 
the generated answer given the current state of the 
frames. Currently, the answer is simply composed 
of text passages from the zero conflict frames. In 
addition, HITIQA will generate a ?headline? for 
the text passages in the answer space.  This is done 
using a combination of text templates and simple 
grammar rules applied to the attributes of the 
passage frame. Figure 7 shows a portion of the 
answer generated by HITIQA for the Iraq query. 
7 HITIQA Preliminary Evaluations 
We have evaluated HITIQA in a series of 
workshops with professional analysts in order to 
obtain an in-depth and comprehensive assessment 
of the system usability and performance. In 
addition to evaluating our research progress, the 
purpose of these workshops was to test several 
evaluation instruments to see if they can be 
meaningfully applied to a complex information 
system such as HITIQA. 
     For the participating analysts, the primary 
activity at these workshops involved preparation of 
reports in response to ?scenarios? ? complex 
questions that often encompass multiple sub-
questions, aspects and hypotheses. For example, in 
one scenario, analysts were asked ti locate 
information about the al Qaeda terorist group: its 
membership, sources of funding and activities. In 
another scenario, the analysts were requested to 
find information on the chemical weapon Sarin. 
Figure 9 shows one of the analytical scenarios used 
in these workshops. We prepared a database of 
over 1GByte of text documents; it included articles 
from the Center for Non-proliferation (CNS) data 
collected for the AQUAINT program and similar 
data retrieved from the web using Google. The 
analysts? task was to prepare a report ?as much like 
what you would do in your normal work 
environment as possible.? Over the six days of the 
workshops, each analyst prepared five such reports 
in sessions of one to three hours. Each session 
involved multiple questions posed to the system, as 
well as clarification dialogue, visual browsing and 
report construction. Figure 10 shows an abridged 
transcript from another analytical session with 
HITIQA.  
 Figure 9: A scenario level analytic task  
One of our primary concerns was to design 
tasks that were similar in scope and difficulty to 
those that the analysts are used to performing at 
work and to ensure that they felt comfortable using 
the system. 5 questions in the scenario evaluation 
dealt with this issue; for example, one question 
asked how the scenarios compared in difficulty 
with the tasks the analysts normally perform at 
work. The mean score for these five questions was 
3.75 on a 5 point scale (five is the best score). The 
lowest score (M=2.88) was received on the 
question ?How did the scenario compare in 
difficulty to tasks that you normally perform at 
work??; this slightly above average rating of 
difficulty of the tasks was quite satisfactory for our 
purposes.  
    In the final evaluation, analysts were asked to 
rate their agreement with statements such as 
?Having HITIQA helps me find important 
information? (score 4.50), ?Having Hitiqa at work 
would help me find information faster than I can 
currently find it? (score 4.33), and ?Hitiqa would 
be a useful addition to the tools that I already have 
at work? (score 4.25). The mean normalized score 
for the combined final evaluation of Workshop I 
was 3.75 on the 5 point scale; this means that the 
system received many more ratings of 4 and 5 than 
of 1 and 2. Comments made by the analysts in the 
group discussion and in the individual interviews 
confirmed that analysts liked the interactive 
dialogue and were very pleased with the results. 
For example, one analyst said ?I learned more 
about Sarin gas in 30 minutes than I probably 
would have at work in a half a day.? As desired, 
the analysts also made many suggestions for 
improving the interface and the interoperation of 
The department chief has requested a report by the 
close of business today on the nuclear arms program in 
Iraq and how it was influenced by the neighboring 
countries. List the extent of the nuclear program in each 
involved country including funding, capabilities, quantity, 
etc. Your report should also include key figures in Iraq 
nuclear program as well as in other countries in the region, 
and,any travels that these key figures have made to other 
countries in regards to a nuclear program, any weapons 
that have been used in the past by either country, any 
purchases or trades that have been made relevant to 
weapons of mass destruction (possibly oil trade, etc.), any 
ingredients and chemicals that have been used, any 
potential weapons that could be under development, 
countries that are involved or have close ties to Iraq or her 
trade partners, possible locations of development sites, and 
possible companies or organizations that these countries 
work with for their nuclear arms program. Add any other 
information relating to the Iraqi Nuclear Arms Programs.  
the visual and text display. For a research system 
undergoing its first rigorous evaluation, these 
results are very satisfactory ? they support the 
value of the design of the HITIQA system, 
including the interactive mode and the visual 
display and encourage us to move forward with 
this approach. 
 FIGURE 10: Fragment of an analytical session 
8 Future work 
The AQUAINT Program has entered its second 
phase in May 2004. Over the next 2 years our 
focus will be on augmenting HITIQA to provide 
more advanced dialogue capabilities, including 
problem solving dialogue related to hypothesis 
formation and verification. This implies building 
up system?s knowledge acquisition capabilities by 
exploiting diverse data sources, including 
structured databases and the internet. 
9 Acknowledgements 
This paper is based on work supported in part by 
the Advanced Research and Development Activity 
(ARDA)?s Advanced Question Answering for 
Intelligence (AQUAINT) Program. Special thanks 
to Heather McCallum-Bayliss and John Rogers for 
helping to arrange the analyst workshops. 
Additional thanks for Google for extending their 
license for this experiment, to Ralph Weischedel of 
BBN/Verizon for the use of IdentiFinder, to Chuck 
Messenger and Peter LaMonica for assistance in 
development of the analytical scenarios, and to 
Bruce Croft at University of Massachusetts for the 
use of INQUERY system. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  
Dialog Act Markup in Several Layers. 
www.cs.rochester.edu/research/cisd.  
Buckley, C. 1985. Implementation of the Smart 
information retrieval system. TR85-686, 
Computer Science, Cornell University. 
Ferguson, G. and J. Allen. 1998. TRIPS: An 
Intelligent Integrated Problem-Solving Assistant. 
AAAI-98 Conf., pp. 567-573. 
Fillmore, C. & C. F. Baker. 2001. Frame semantics 
for text understanding. WordNet Workshop at 
NAACL. 
Hardy, H., et al 2002a. Cross-Document 
Summarization by Concept Classification. 
Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., et al 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual 
Customer Service. ISLE Workshop, UK. 
Harabagiu, S., et. al. 2002. Answering Complex, 
List and Context questions with LCC?s Question 
Answering Server.   TREC-10. 
Hovy, E., et al 2000. Question Answering in 
Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference TREC-9. 
Humphreys, R. et al 1998. Description of the 
LaSIE-II System as Used for MUC-7. Proc. of  
7th Message Under. Conf. (MUC-7.). 
Levin, B. 1993. English Verb Class and 
Alternations: A Preliminary Investigation. 
Chicago: University of Chicago Press. 
Litman, Diane J. and Shimei Pan. 2002. Designing 
and Evaluating an Adaptive Spoken Dialogue 
System. User Modeling and User-Adapted 
Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Prager, J. et al 2003. In Question-Answering Two 
Heads are Better Than One. Proceedings of 
HLT-NAACL 2003, pp 24-31.  
Seneff, S. and J. Polifroni. 2000. Dialogue 
Management in the MERCURY Flight 
Reservation System. ANLP-NAACL 2000. 
Small et al 2004. A Data Driven Approach to 
Interactive Question Answering. In M. Maybury 
(ed). Future Directions in Automated Question 
Answering. MIT Press (to appear). 
Strzalkowski, T and J. Wang. 1996. A self-learning 
Universal Concept Spotter. Proceedings of 
COLING-96, pp. 931-936. 
Walker, M. A. 2002. An Application of 
Reinforcement Learning to Dialogue Strategy 
Selection in a Spoken Dialogue System for 
Email. Journal of AI Research, vol 12., pp. 387-
416. 
Wu, M. et al 2003. Question Answering by 
Pattern Matching, Web-Proofing, Semantic 
Form Proofing. TREC-12.Notebook. 
 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 60 minutes  
Copying 24 passages to report 
 Visual Panel Browsing: 5 minutes 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 26 minutes 
 Copying 6 passages to report 
 Visual Panel browsing: 1 minute 
 Adding 1 passage to report 
User: How was South Africa's CBW program 
financed?  
 Clarification Dialogue: 40 seconds 
 Studying Answer Panel: 11 minutes 
 Copying 3 passages to report 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 977?984
Manchester, August 2008
Investigating the Portability of Corpus-Derived Cue Phrases for Dialogue
Act Classification
Nick Webb and Ting Liu
ILS Institute
University at Albany, SUNY
Albany, NY, USA
{nwebb|tl7612}@albany.edu
Abstract
We present recent work in the area of
Cross-Domain Dialogue Act tagging. Our
experiments investigate the use of a sim-
ple dialogue act classifier based on purely
intra-utterance features - principally in-
volving word n-gram cue phrases. We ap-
ply automatically extracted cues from one
corpus to a new annotated data set, to de-
termine the portability and generality of
the cues we learn. We show that our auto-
matically acquired cues are general enough
to serve as a cross-domain classification
mechanism.
1 Introduction
A number of researchers (Hirschberg and Litman,
1993; Grosz and Sidner, 1986) speak of cue or key
phrases in utterances that can serve as useful indi-
cators of discourse structure. We have previously
investigated the use of such cue phrases to predict
dialogue acts or DAs (functional tags which rep-
resent the communicative intentions behind each
user utterance) (Webb et al, 2005a). We devel-
oped an approach, in common with the work of
Samuel et al (1999), where word n-grams that
might serve as cue phrases are automatically de-
tected in a corpus and we have previously reported
the results of experiments evaluating this approach
on the SWITCHBOARD corpus, where our results
rival the best reported over that data (Stolcke et al,
2000), although our method adopts a significantly
less complex algorithm.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
An interesting by-product of our approach is the
ranked list of cue phrases derived from the source
corpus. Visual inspection of these cues reveals
that, as one might expect, there is a high degree of
correlation between phrases such as ?can you? and
the DA <yes/no question>, ?where is? and ?who
is? with the DA <wh-question> and ?right? or
?ok? with DA <agree/accept>. These cues appear
to be of a general nature, unrelated to the source
domain or application. Therefore, despite being
automatically acquired from one domain specific
corpus, these cues should be equally applicable to
new corpora, from a different domain and it is this
hypothesis we test. This paper presents our work
on dialogue act classification using cues automat-
ically extracted from a corpus from one domain,
and applying these cues directly as a classifier over
a new corpus from a different domain.
The material is presented as follows: Previous
work with dialogue act modelling is outlined in
Section 2. An overview of the corpora used for
the experiments we report can be seen in Section
3. A brief overview of our classification method
is given in Section 4. Our experiments evaluating
the cue-based dialogue act classifier tagging new,
out-of-domain data are given in Section 5. Finally
we end with some discussion and an outline of in-
tended further work.
2 Related Work
Dialogue Acts (DAs) (Bunt, 1994), also known as
speech acts or dialogue moves, represent the func-
tional performance of a speaker?s utterance, such
as a greeting ?Hello there?, asking a question like
?How is your mother?? or making a request ?Can
you move your foot??.
There are two broad categories of computational
model used to interpret these acts. The first, in-
977
Corpus Availability
Utterance
count
Dialogue
count
Word
count
Distinct
words
Dialogue
type
SWITCHBOARD public 223606 1155 1431725 21715 Conversational
AMITI
?
ES GE restricted 30206 1000 228165 7841 Task-oriented
Figure 1: Summary data for the dialogue corpora
cluding the work of Cohen and Perrault (1979) re-
lies on processing belief logics, centring on the im-
pact each utterance has on the hearer - what the
hearer believes the speaker intended to commu-
nicate. These models can be very accurate, but
often are complex, and require significant world-
knowledge to create.
The second model type is cue-based, and cen-
tres on the notion of repeated, predictive cues -
subsections of language which are strong indica-
tors of specific DAs. In this second category, much
of the work is cast as a probabilistic classification
task, solved by training approaches on labelled ex-
amples of dialogue acts. As an example of these
probabilistic methods, Stolcke et al (2000) apply
a HMM method to the SWITCHBOARD corpus, one
that exploits both the order of words within ut-
terances and the order of dialogue acts over ut-
terances. They use a single split of the data for
their experiments, with 198k utterances for train-
ing and 4k utterances for testing, achieving a DA
tagging accuracy of 71.0% on word transcripts.
Another learning approach by Samuel et al (1998)
uses transformation-based learning over a number
of utterance features, including utterance length,
speaker turn and the dialogue act tags of adja-
cent utterances. They achieved an average score
of 75.12% tagging accuracy over the VERBMO-
BIL corpus. A significant aspect of this work is
the automatic identification of word sequences that
might serve as useful dialogue act cues (Samuel et
al., 1999). A number of statistical criteria are ap-
plied to identify potentially useful word n-grams
that are then supplied to the transformation-based
learning method as ?features?.
What has been less explored is the portabil-
ity or adaptability of these models to new cor-
pora and new domains. Prasad and Walker (2002)
look at applying models generated from a Human-
Computer corpus to a Human-Human corpus in the
same domain, that of travel planning, and score a
very low 36.72% accuracy using their model. The
work of Tur et al (2006) is closer to the work re-
ported here - they apply models derived from the
SWITCHBOARD corpus to the ICSI-MRDA corpus
(Shriberg et al, 2004) using boosting, applied to
a high level of representation (comprising only 5
DA categories, one of which they exclude), where
they achieve 57.37% tagging accuracy. This seems
to indicate that cross-domain application of mod-
els is possible, although the level of accuracy as
presently reported is low.
3 Experimental Corpora
Our work as described here applies to two corpora
- the DA-tagged portion of the SWITCHBOARD cor-
pus (Jurafsky et al, 1998), and the AMITI
?
ES GE
corpus (Hardy et al, 2002; Hardy et al, 2003), cre-
ated as part of the AMITI
?
ES European 5th Frame-
work program project (Hardy et al, 2005). A sum-
mary of the two corpora can be seen in Figure 1.
3.1 Switchboard
The annotated portion of the SWITCHBOARD cor-
pus comprises 1155 annotated conversations be-
tween two human participants, where the dia-
logues are of an unstructured, non-directed char-
acter. Participants do not know each other, and
are provided only with a set of topics which they
may wish to discuss. The SWITCHBOARD corpus
is annotated using an elaboration of the DAMSL
tag set. In 1998 the Discourse Resource Initia-
tive finalised a task-independent set of DAs, called
DAMSL (Dialogue Act Markup in Several Layers),
for use across different domains. DAMSL has been
used to mark-up several dialogue corpora, such as
TRAINS (Core and Allen, 1997), and the SWITCH-
BOARD corpus (Jurafsky et al, 1998).
The annotation over the SWITCHBOARD corpus
involves 50 major classes, together with a num-
ber of diacritic marks, which combine to generate
220 distinct labels. Jurafsky et al (1998) propose
a clustering of these 220 tags into 42 larger classes
and it is this clustered set that was used both in
our experiments and those of Stolcke et al (2000).
In measuring the agreement between annotators in
labelling this data, Jurafsky et al (1998) report
an average pair-wise kappa of .80 (Carletta et al,
978
<Turn Id="utt3" Speaker="A" DA-Type="Open-question"> what do you think was different ten
years ago from now?</Turn>
<Turn Id="utt4" Speaker="B" DA-Type="Statement-opinion"> Well I would say as far as social
changes go I think families were more together.</Turn>
<Turn Id="utt5" Speaker="B" DA-Type="Statement-opinion"> They did more things
together</Turn>
<Turn Id="utt6" Speaker="A" DA-Type="Acknowledge"> Uh-huh</Turn>
Figure 2: Excerpt of dialogue from the SWITCHBOARD corpus
1997). An excerpt of dialogue from the SWITCH-
BOARD corpus can be seen in Figure 2.
3.2 AMITI
?
ES
The AMITI
?
ES project (Hardy et al, 2005) collected
1000 English human-human dialogues from Euro-
pean GE call centres. These calls are of an in-
formation seeking or transactional type, in which
customers interact with their financial accounts
by phone to check balances, make payments and
report lost credit cards. The resulting data has
been sanitised, to replace identifying features such
as names, addresses and account numbers with
generic information (?John Doe?, ?1 The Street?)
and the corpus is annotated with DAs using XDML,
combining slight variant of the 42-class DAMSL
(Hardy et al, 2002) with domain specific seman-
tic information such as account numbers and credit
card details (Hardy et al, 2003).
The most frequent tag in the AMITI
?
ES corpus
is Influence-on-listener=?Information-request?,
which occurs 20% of the time. For this corpus, the
average pair-wise kappa score of .59 was signifi-
cantly lower than the SWITCHBOARD corpus. For
the major categories (questions, answers), average
pair-wise kappa scores were around .70. Again,
according to the work of Carletta et al (1997), a
minimum kappa score of 0.67 is required to draw
tentative conclusions. An excerpt of dialogue
from the AMITI
?
ES corpus can be seen in Figure 3.
4 DA Classification
In this section we briefly describe our approach to
DA classification, based solely on intra-utterance
features. A key aspect of the approach is the se-
lection of the word n-grams to use as cue phrases.
Samuel et al (1999) investigate a series of different
statistical criteria for use in automatically selecting
cue phrases, but we use a criterion of predictivity,
described below, which is one that Samuel et al
(1999) do not consider.
4.1 Cue Phrase Selection
For our experiments, the word n-grams used as po-
tential cue phrases during classification are com-
puted from the training data. All word n-grams of
length 1?4 within the data are considered as can-
didates. The phrases chosen as cue phrases are
selected principally using a criterion of predictiv-
ity, which is the extent to which the presence of
a certain n-gram in an utterance is predictive of it
having a certain dialogue act category. For an n-
gram n and dialogue act d, this corresponds to the
conditional probability: P (d | n), a value that can
be straightforwardly computed. For each n-gram,
we are interested in its maximal predictivity, i.e.
the highest predictivity value found for it with any
DA category. This set of n-grams is then reduced
by applying thresholds of predictivity and occur-
rence, i.e. eliminating any n-gram whose maxi-
mal predictivity is below some minimum require-
ment, or whose maximal number of occurrences
in any category falls below some threshold value.
This thresholding removes some low frequency,
high predictivity n-grams that skew classification
performance. The n-grams that remain are identi-
fied as our cue phrases. The threshold values that
are used in all experiments were arrived at empiri-
cally, using a validation set to automatically set the
threshold levels independently of the test data, as
described in Webb et al (2005b).
4.2 Using Cue Phrases in Classification
To classify an utterance, we identify all the word
n-grams it contains, and determine which of these
has the highest predictivity of some dialogue act
category (i.e. is performing as some cue). If mul-
tiple cue phrases share the same maximal predic-
tivity, but predict different categories, we select the
979
<Turn Id="2.1" Speaker="Operator" Info-level="Communication-mgt"
Conventional="Opening">good morning customer services sam speaking</Turn>
<Turn Id="3.1" Speaker="Customer" Info-level="Communication-mgt"
Conventional="Opening">erm good morning</Turn>
<Turn Id="3.2" Speaker="Customer" Info-level="Task"
Forward-function="Explanation">erm I was away for about two months and i came back
and my card i don?t know whether i have lost it or it is stolen</Turn>
<Turn Id="4.1" Speaker="Operator" Understanding="Backchannel"
Response-to="T3.2">right okay</Turn>
<Turn Id="4.2" Speaker="Operator" Info-level="Task"
Influence-on-listener="Info-request-explicit">can you confirm your name
for me please</Turn>
Figure 3: Excerpt of dialogue from the AMITI
?
ES GE corpus
DA for the phrase with the highest frequency. If the
combination of predicitivity and occurrence count
is insufficient to determine a single DA, then a ran-
dom choice is made amongst the remaining can-
didate DAs. If no cue phrases are present, then a
default tag is assigned, corresponding to the most
frequent tag within the training corpus.
Our best reported figures on the 202k utterance
SWITCHBOARD corpus are a cross-validated score
of 69.09%, with a single high score of 71.29%,
which compares very favourably with the (not
cross-validated) 71% reported in Stolcke et al
(2000) for the same corpus. We also presented in-
formation that shows that adding a sequence model
of DA progressions - an n-gram model of DAs -
results in no significant increase in performance
(Webb et al, 2005a). This is surprising consid-
ering that Stolcke et al (2000) report their best fig-
ures when combining a HMM model of the words
inside utterances with a tri-gram model of the Di-
alogue Act sequence, as in the work of Reithinger
and Klesen (1997). When Stolcke et al (2000) add
the sequence model to the HMM language model, it
adds around 20% points to the final accuracy score
over the SWITCHBOARD data.
However, our observation is confirmed by both
Serafin and Eugenio (2004) and Ries (March
1999). On the basis of this result, we hypothe-
sise that our cues are highly predictive of dialogue
structure, and that much dialogue processing may
take place at a very shallow level.
5 Cross-Domain Classification
The central purpose of this paper is to examine
the use of automatically extracted cues to tag data
other than the corpus from which they are de-
rived. The hypothesis we wish to test is that these
cues are sufficiently general to work as a classi-
fication device on a corpus from a different do-
main, even containing interactions of a different
conversational style. Specifically, SWITCHBOARD
is an open domain spoken human-human conver-
sational corpus and we have shown state-of-the-art
tagging performance over this data using our cue-
based model. We now wish to see how well these
same cues perform over the AMITI
?
ES GE corpus
of spoken task-based dialogues. The dialogues in
the AMITI
?
ES GE corpus are far more goal directed,
and contain domain specific cues not found in the
general conversational SWITCHBOARD corpus.
The ability to apply cues extracted from one cor-
pus to new data is an interesting challenge. It could
confirm work which indicates the prominence of
such word cues in language (Hirschberg and Lit-
man, 1993). A tag mechanism that can operate
across domains presents a range of benefits - for
example it can be used to annotate or partially an-
notate new data collections.
5.1 DA Mapping
Cross-corpus classification would be simplified if
both corpora were annotated with identical DA tax-
onomies. In actuality, the SWITCHBOARD corpus
and the AMITI
?
ES GE corpus are annotated with
variants of the DAMSL DA annotation scheme. In
the SWITCHBOARD corpus, the hierarchical nature
of the DAMSL schema has been flattened and clus-
tered, to produce 42 major classes. In the AMITI
?
ES
GE corpus, the dialogue level schema has been left
largely untouched from the DAMSL original. In or-
980
der to be able to compare automatic classification
performance across the two corpora, a mapping is
required between the 42-class schema of SWITCH-
BOARD and the DAMSL-like XDML schema of
the AMITI
?
ES GE corpus. In their work, Juraf-
sky et al (1998) include such a mapping between
SWITCHBOARD and DAMSL that covers approxi-
mately 80% of the labels in the SWITCHBOARD
corpus. We have adapted this slightly to cover
minor differences between the XDML used in the
AMITI
?
ES GE corpus and the original DAMSL, al-
though this leaves us with two issues that we need
to address.
First there are differences in granularity on both
sides. Importantly, in many instances we may
identify the most salient role of the utterance, but
miss modification information which may make
little interpretative difference. For example, mark-
up in the AMITI
?
ES GE corpus makes the distinc-
tion between <Forward-function=?Assert?> and
<Forward-function=?Reassert?>, whereas mark-
up in the SWITCHBOARD corpus ignores such a
distinction, and annotates both as type <Forward-
function=?Assert?> - although the SWITCH-
BOARD corpus captures the difference between as-
sertions that are opinions, and those that are not,
whereas the original DAMSL does not capture this
distinction. To address this mismatch we create
a set of super classes by relating the annotations
of SWITCHBOARD-DAMSL and the AMITI
?
ES GE-
XDML corpora at the most salient level, according
to the mapping contained in Jurafsky et al (1998).
Whilst the majority of tags have a one-to-one cor-
relation, there are elements of both the Forward-
Looking Function (see Figure 4) and Backward-
Looking Function (Figure 5) that require mapping
in both directions.
Secondly, there are a number of AMITI
?
ES GE
tags that we know a-priori we have little or no
chance to recognise. For example, the AMITI
?
ES
GE corpus is meticulously annotated to include that
certain utterances are perceived as answers to prior
utterances. Our approach to DA tagging is purely
intra-utterance, taking no account of the wider dis-
course structure, so will not recognise these dis-
tinctions. Although such a model of discourse
structure should be trivial, based for example on
an adjacency pair approach, this will be evaluated
further in future work.
5.2 Evaluation Criteria
These issues require that we create two evaluation
criteria for our subsequent experiments - strict and
lenient. With strict evaluation, we are required to
match all elements of the AMITI
?
ES GE corpus an-
notation - despite knowing in advance that this is
not possible for a range of utterances. We use our
strict evaluation criteria to establish a lower bound
of performance for our classifier. Our lenient ap-
proach is a back-off model, where we require that
we correctly identify the most critical part of the
multi-part annotation - those that are identified as
the most salient.
We?ll use the dialogue excerpt shown in Fig-
ure 3 as an example of how these two scor-
ing mechanisms work. The first utterance (2.1)
is marked as <Info-level=?Communication-mgt?
Conventional=?Opening?>. This has a one-to-
one correlation with the SWITCHBOARD-DAMSL
tag <conventional-opening>. In the case of this
example, and in all instances in the AMITI
?
ES
GE corpus, utterances are marked as <Info-
level=?Task?>, unless they are from a small
set of exceptions, including openings, closings
or backchannels, that are annotated as <Info-
level=?Communication-mgt?>. Once an utterance
is tagged as one of these exceptions, we know to
change the <Info-level> assignment accordingly.
There will be no difference between our strict
and lenient evaluation models for the interpreta-
tion of this utterance. The same is true for the sec-
ond (3.1) utterance annotation, which has a direct
correlation with SWITCHBOARD-DAMSL annota-
tions. However, the fourth utterance (4.1) includes
a <Response-to=?T3.2?> annotation that we will
not be able to identify using our intra-utterance
model. This utterance will be judged correct us-
ing the lenient model, and incorrect using the strict
metric.
The third utterance (3.2) is marked as
<Forward-function=?Explanation?>. Using
the Forward-function map shown in Figure
4, we see that this maps to the super class
<Forward-function=?Assert?>, that in turn maps
to the SWITCHBOARD-DAMSL tags <statement-
non-opinion> and <statement-opinion>. This
means that any utterance identified by the
presence of a cue phrase as either <statement-
non-opinion> or <statement-opinion> will in
fact be tagged as <Info-level=?Task? Forward-
function=?Assert?>. Whilst this annotation
981
Forward? function = ?Assert?
Forward? function = ?Reassert?
Forward? function = ?Explanation?
Forward? function = ?Rexplanation?
Forward? function = ?Expression?
?
?
?
?
?
?
?
Forward? function = ?Assert?
{
statement? non? opinion
statement? opinion
Figure 4: Partial Forward-Looking Function mapping table (XDML } SUPERCLASS { SWITCHBOARD-
DAMSL)
Inf ? on? list = ?Info? req ? explicit?
Inf ? on? list = ?Info? req ? implicit?
Inf ? on? list = ?Conf ? req ? implicit?
Inf ? on? list = ?Conf ? req ? explicit?
?
?
?
?
?
Influence? on? listener =
?Information? request?
?
?
?
?
?
?
?
?
?
?
?
yes? no? question
wh? questions
open? questions
or ? clause
declarative? question
tag ? question
Figure 5: Partial Backward-Looking Function mapping table (XDML } SUPERCLASS { SWITCHBOARD-
DAMSL)
captures the salient behaviour of the utterance,
it is not an exact match to the original AMITI
?
ES
GE corpus annotation and correspondingly when
scoring the lenient model will score this as correct,
whereas the exact model will not.
The same is true with the fifth utterance
(4.2), annotated in this case as <Influence-on-
listener=?Info-request-explicit?>. A classifier
trained over the SWITCHBOARD corpus would
identify this (through the mapping see in Fig-
ure 5) as <Influence-on-listener=?Information-
request?>, which would be scored as correct using
the lenient measure, and incorrect using the exact.
5.3 Classification Experiments
The results of our experiments are summarised
in Figure 6. First, to establish our baseline tag-
ging performance, we take the classification al-
gorithm outlined earlier in Section 4, and apply
it to the SWITCHBOARD corpus for both training
and testing, replicating the work reported in Webb
et al (2005a). In this case, 198,000 utterances
are used for training, and a separate 4,000 utter-
ances are used for testing. We achieve a cross-
validated score of 69.6%, where the most frequent
tag in SWITCHBOARD, <statement-non-opinion>,
occurs 36% of the time. This is a confirmation
of the work reported in Webb et al (2005a), and
demonstrates that this simple model works excep-
tionally well for this task.
For the first of the new experiments to test our
hypothesis, we substitute the AMITI
?
ES GE corpus
for the SWITCHBOARD corpus in both steps - train-
ing and testing - which will give us an upper bound
of performance of this particular classification al-
gorithm over this data. In this experiment, we used
10% of the corpus for testing - giving us a total of
27,000 utterances for training and 3,000 utterances
for testing. For all experiments where AMITI
?
ES GE
data is used as a test corpus, both strict and lenient
scoring will be used. Strict scoring sets a lower
bound for this exercise, and should be greater than
chance, which corresponds to the distribution of
the most frequent DA tag in each corpus. For strict
scoring, where we are required to match all the el-
ements of the AMITI
?
ES GE XDML tag, we score
65.9% accuracy in this experiment. For lenient,
where we must match only the most salient fea-
tures, we score 70.8% accuracy. Whilst there is
no direct comparison to other work on this cor-
pus, Hardy et al (2005) show partial results for DA
classification on this task, looking only at a few
major classes, and achieve a score of 86%. How-
ever, this includes only the 5 most frequent DA
categories, and considers utterances shorter than a
certain number of words.
Finally, we attempt cross-domain classification:
First, we train our classifier using SWITCHBOARD
data, and test using AMITI
?
ES GE data. We recorded
a strict evaluation score of 39.8% tagging accu-
racy. Using the lenient score, we achieve around
55.7% accuracy. This can be considered a very
good result, given the lower bound score of 20%
- that is the count of the most frequent tag.
982
Training
corpus
Training
utterances
Testing
corpus
Test
utterances
Common
tag (%)
Lenient
score
Strict
score
SWITCHBOARD 198,000 SWITCHBOARD 4,000 36% n/a 69%
AMITI
?
ES GE 27,000 AMITI
?
ES GE 3,000 20% 70.8% 65.9%
SWITCHBOARD 198,000 AMITI
?
ES GE 30,000 20% 55.7% 39.8%
AMITI
?
ES GE 27,000 SWITCHBOARD 198,000 36% 48.3% 40%
SWITCHBOARD 27,000 AMITI
?
ES GE 3,000 20% 53.2% 38%
Figure 6: Experimental Results
Then we apply the classification in reverse -
we train on AMITI
?
ES GE data, and test on the
SWITCHBOARD corpus, using all available data
in both cases. Using the strict evaluation met-
ric, we achieve a score of 40.0%, and a lenient
score of 48.3%. This compares to a baseline of
36%, so is not a drastic improvement over our
lower bound. Some inspection of the data in-
formed us that the AMITI
?
ES GE data did not include
many <backchannel> utterances, so subsequently
most of these instances in the SWITCHBOARD cor-
pus were missed by our classifier. By changing
the default tag to be <backchannel>, rather than
the most frequent tag for the training corpus, we
achieve a performance gain to 47.7% with strict
scoring, and 56.0% with the lenient metric.
For the last experiment, we also wanted to study
the effect of limiting the training data on cross-
domain classification, by reducing the SWITCH-
BOARD data to match that of the AMITI
?
ES GE train-
ing set - that is, to use only 27,000 utterances of
the SWITCHBOARD corpus as training data to ex-
tract cues, which are then applied both to itself (for
reference), and to the AMITI
?
ES GE corpus. On a
related note, part of the work conducted in Webb
et al (2005a) studied the impact of different size
training models when classifying SWITCHBOARD
data, using models of 4k, 50k and 202k utterances.
Whilst substantial improvement was seen when
moving from 4k utterances to 50k utterances, the
subsequent increase from 50k to 202k utterances
had a negligible impact on classification accuracy.
With the reduced SWITCHBOARD training set, we
score 53.2% with the lenient metric, and 38% with
strict, indicating that the reduction is size of the
training data has some effect on classification ac-
curacy.
6 Discussion, Future Work
We have shown that the cues extracted from the
SWITCHBOARD corpus can be used to success-
fully classify utterances in the AMITI
?
ES GE cor-
pus. We achieve almost 80% of the upper baseline
performance over the AMITI
?
ES GE corpus, when
judged using our lenient scoring mechanism - scor-
ing 55.7% using the cross-domain cues, compared
to the 70.8% when using in-domain cues. When
using the strict measure we still achieve around
60% of the upper bound performance, both results
being a substantial improvement over the baseline
measure of 20%, corresponding to the most fre-
quent tag in the AMITI
?
ES GE corpus. This is a sig-
nificant result, which confirms the idea that cues
can be sufficiently general across domains to be
used in classification.
However, whilst the experiment using SWITCH-
BOARD corpus derived cues to classify AMITI
?
ES
GE data works well, the same is not true in re-
verse. There are two possible explanations for this
result. It could be related to the size of data avail-
able for training, although our experiments in this
area seem to suggest otherwise and so we believe
that the composition of the training data is a more
crucial element. Although the DA distribution in
the SWITCHBOARD corpus is uneven, there is suf-
ficient data for the major classes to be effective on
new data that also contains these classes. Although
the AMITI
?
ES GE contains a lot of questions and
statements, there is very little of the other signif-
icant categories, such as <backchannels>, a key
DA in the SWITCHBOARD corpus and conversa-
tional speech in general. Correspondingly, the cues
derived from the AMITI
?
ES GE data perform well
on a selection of utterances in the SWITCHBOARD
corpus, but very poorly on others. We want to per-
form an in-depth error analysis to see if the errors
we obtain in classification accuracy are consistent.
We can also compare our list of automatically de-
rived cues phrases, particularly those that overlap
between the two corpora, to those reported in prior
literature. It might be interesting to see if more
complex models, derived using state-of-the art ma-
983
chine learning approaches, could demonstrate sim-
ilar portability - i.e is it the simplicity of our model
that allows for the observed robust portability?
Finally, we wish to combine SWITCHBOARD
and AMITI
?
ES corpora in the cue learning phase, to
see how this effects classification, and apply the
results to a range of other corpora, including the
ICSI-MRDA corpus (Shriberg et al, 2004).
References
Bunt, Harry. 1994. Context and dialogue control.
THINK, 3:19?31.
Carletta, J. C., A. Isard, S. Isard, J. Kowtko,
G. Doherty-Sneddon, and A. Anderson. 1997. The
Reliability of a Dialogue Structure Coding Scheme.
Computational Linguistics, 23:13?31.
Cohen, P. R. and C. R. Perrault. 1979. Elements of a
plan based theory of speech acts. Cognitive Science,
3.
Core, Mark G. and James Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, MIT, Cambridge, MA.
Grosz, Barbara and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. Compu-
tational Linguistics, 19(3).
Hardy, Hilda, Kirk Baker, Laurence Devillers, Lori
Lamel, Sophie Rosset, Tomek Strzalkowski, Cristian
Ursu, and Nick Webb. 2002. Multi-layered dialogue
annotation for automated multilingual customer ser-
vice. In Proceedings of the ISLE workshop on Dia-
logue Tagging for Multimodal Human Computer In-
teraction, Edinburgh.
Hardy, H., K. Baker, H. Bonneau-Maynard, L. Dev-
illers, S. Rosset, and T. Strzalkowski. 2003. Se-
mantic and dialogic annotation for automated mul-
tilingual customer service. In Eurospeech, Geneva,
Switzerland.
Hardy, H., A. Biermann, R. Bryce Inouye, A. McKen-
zie, T. Strzalkowski, C. Ursu, N. Webb, and M. Wu.
2005. The AMITIES System: Data-Driven Tech-
niques for Automated Dialogue. Speech Communi-
cation, 48:354?373.
Hirschberg, Julia and Diane Litman. 1993. Empiri-
cal Studies on the Disambiguation of Cue Phrases.
Computational Linguistics, 19(3):501?530.
Jurafsky, Daniel, Rebecca Bates, Noah Coccaro,
Rachel Martin, Marie Meteer, Klaus Ries, Eliza-
beth Shriberg, Andreas Stolcke, Paul Taylor, and
Carol Van Ess-Dykema. 1998. Switchboad dis-
course language modeling project final report. Re-
search Note 30, Center for Language and Speech
Processing, Johns Hopkins University, Baltimore.
Prasad, Rashmi and Marilyn Walker. 2002. Train-
ing a Dialogue Act Tagger for Humna-Human and
Human-Computer Travel Dialogues. In Proceedings
of the 3rd SIGdial workshop on Discourse and Dia-
logue, Philadelphia, Pennsylvania.
Reithinger, Norbert and Martin Klesen. 1997. Dia-
logue act classification using language models. In
Proceedings of EuroSpeech-97.
Ries, Klaus. March, 1999. Hmm and neural network
based speech act classification. In Proceddings of
the IEEE Conference on Acoustics, Speech and Sig-
nal Processing, volume 1, pages 497?500, Phoenix,
AZ.
Samuel, Ken, Sandra Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-
based learning. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Montreal.
Samuel, Ken, Sandra Carberry, and K. Vijay-Shanker.
1999. Automatically selecting useful phrases for di-
alogue act tagging. In Proceedings of the Fourth
Conference of the Pacific Association for Computa-
tional Linguistics, Waterloo, Ontario, Canada.
Serafin, Riccardo and Barbara Di Eugenio. 2004.
FLSA: Extending Latent Semantic Analysis with
features for dialogue act classification. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, Barcelona, Spain.
Shriberg, E., R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Special Interest Group on Dis-
course and Dialogue (SIGdial), Boston, USA.
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue act model-
ing for automatic tagging and recognition of conver-
sational speech. In Computational Linguistics 26(3),
339?373.
Tur, Gokhan, Umit Guz, and Dilek Hakkani-Tur. 2006.
Model Adaptation for Dialogue Act Tagging. In
IEEE Spoken Language Technology Workshop.
Webb, Nick, Mark Hepple, and Yorick Wilks.
2005a. Dialogue Act Classification Based on Intra-
Utterance Features. In Proceedings of the AAAI
Workshop on Spoken Language Understanding.
Webb, Nick, Mark Hepple, and Yorick Wilks. 2005b.
Empirical determination of thresholds for optimal di-
alogue act classification. In Proceedings of the Ninth
Workshop on the Semantics and Pragmatics of Dia-
logue.
984
HITIQA:  An Interactive Question Answering System  
A Preliminary Report 
 
Sharon Small, Ting Liu, Nobuyuki Shimizu, and Tomek Strzalkowski  
 
ILS Institute 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tl7612,ns3203,tomek}@albany.edu 
 
 
Abstract
HITIQA is an interactive question answering 
technology designed to allow intelligence analysts 
and other users of information systems to pose 
questions in natural language and obtain relevant 
answers, or the assistance they require in order to 
perform their tasks. Our objective in HITIQA is to 
allow the user to submit exploratory, analytical, 
non-factual questions, such as ?What has been 
Russia?s reaction to U.S. bombing of Kosovo?? 
The distinguishing property of such questions is 
that one cannot generally anticipate what might 
constitute the answer. While certain types of things 
may be expected (e.g., diplomatic statements), the 
answer is heavily conditioned by what information 
is in fact available on the topic. From a practical 
viewpoint, analytical questions are often under-
specified, thus casting a broad net on a space of 
possible answers. Therefore, clarification dialogue 
is often needed to negotiate with the user the exact 
scope and intent of the question. 
 
1   Introduction 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question answer-
ing.  In this paper we focus on two aspects of our 
work: 
1. Question Semantics: how the system ?un-
derstands? user requests. 
2. Human-Computer Dialogue: how the user 
and the system negotiate this understand-
ing. 
     We will also discuss very preliminary evalua-
tion results from a series of pilot tests of the system 
conducted by intelligence analysts via a remote 
internet link.  
   
2   Factual vs. Analytical 
The objective in HITIQA is to allow the user to 
submit and obtain answers to exploratory, analyti-
cal, non-factual questions.  There are very signifi-
cant differences between factual, or fact-finding, 
and analytical question answering. A factual ques-
tion seeks pieces of information that would make a 
corresponding statement true (i.e., they become 
facts): ?How many states are in the U.S.?? / ?There 
are X states in the U.S.? In this sense, a factual 
question usually has just one correct answer that 
can generally, be judged for its truthfulness. By 
contrast, an analytical question is when the ?truth? 
of the answer is more a matter of opinion and may 
depend upon the context in which the question is 
asked. Answers to analytical questions are rarely 
unilateral, indeed, a mere ?correct? answer may 
have limited value, and in some cases may not 
even be determinate (?Which college is the best??, 
?How do I stop my baby?s crying??). Instead, an-
swers to analytical questions are often judged as 
helpful, or useful, or satisfactory, etc. ?Technically 
correct? answers (e.g., ?feed the baby milk?) may 
be considered as irrelevant or at best unresponsive.   
     The distinction between factual and analytical 
questions depends primarily on the intention of the 
person who is asking, however, the form of a ques-
tion is often indicative of which of the two classes 
it is more likely to belong to.  Factual questions 
can be classified into a number of syntactic formats 
(?question typology?) that aids in automatic proc-
essing. 
     Factual questions display a fairly distinctive 
?answer type?, which is the type of the information 
piece needed to fulfill the statement.  Recent auto-
mated systems for answering factual questions  
deduct  this expected answer type from the form of 
the question and a finite list of possible answer 
 types. For example, ?Who was the first man in 
space? expects a ?person? as the answer, while 
?How long was the Titanic?? expects some length 
measure as an answer, probably in yards and feet, 
or meters.  This is generally a very good strategy, 
that has been exploited successfully in a number of 
automated QA systems that appeared in recent 
years, especially in the context of TREC QA1 
evaluations (Harabagiu et al, 2000; Hovy et al, 
2000; Prager at al., 2001).     
     This process is not easily applied to analytical 
questions. This is because the type of an answer for 
analytical questions cannot always be anticipated 
due to their inherently exploratory character.  In 
contrast to a factual question, an analytical ques-
tion has an unlimited variety of syntactic forms 
with only a loose connection between their syntax 
and the expected answer.  Given the unlimited po-
tential of the formation of analytical questions, it 
would be counter-productive to restrict them to a 
limited number of question/answer types. Even 
finding a non-strictly factual answer to an other-
wise simple question about Titanic length (e.g., 
?two football fields?) would push the limits of the 
answer-typing approach. Therefore, the formation 
of an answer should instead be guided by the top-
ics the user is interested in, as recognized in the 
query and/or through the interactive dialogue, 
rather than by a single type as inferred from the 
query in a factual system.   
     This paper argues that the semantics of an ana-
lytical question is more likely to be deduced from 
the information that is considered relevant to the 
question than through a detailed analysis of their 
particular form. While this may sound circular, it 
needs not be. Determining ?relevant? information 
is not the same as finding an answer; indeed we 
can use relatively simple information retrieval 
methods (keyword matching, etc.) to obtain per-
haps 50 or 100 ?relevant? documents from a data-
base. This gives us an initial answer space to work 
on in order to determine the scope and complexity 
of the answer. In our project, we use structured 
templates, which we call frames to map out the 
content of pre-retrieved documents, and subse-
quently to delineate the possible meaning of the 
question (Section 6). 
                                                 
1 TREC QA is the annual Question Answering evalua-
tion sponsored by the U.S. National Institute of Stan-
dards and Technology www.trec.nist.gov. 
 
3   Document Retrieval 
When the user poses a question to a system sitting 
atop a huge database of unstructured data (text 
files), the first order of business is to reduce that 
pile to perhaps a handful of documents where the 
answer is likely to be found. This means, most of-
ten, document retrieval, using fast but non-exact 
selection methods.  Questions are tokenized and 
sent to a document retrieval engine, such as Smart 
(Buckley, 1985) or InQuery (Callan et al, 1992).  
Noun phrases and verb phrases are extracted from 
the question to give us a list of potential topics that 
the user may be interested in.   
    In the experiments with the HITIQA prototype, 
see Figure 1, we are retrieving the top fifty docu-
ments from three gigabytes of newswire 
(AQUAINT corpus plus web-harvested docu-
ments).  
 
Document 
Retrieval
Document 
Retrieval
Build
Frames
Build
Frames
Process
Frames
Process
Frames
Dialogue
Manager
Dialogue
anager
Segment/
Filter
Segment/
Filter
Cluster
Paragraphs
Cluster
Paragraphs
Answer
Generator
Answer
Generator
answer
Tokenized 
question
top 50 
documents
distinct 
paragraphs
clusters
framed text 
segments
candidate 
answer topics
relevant text 
segments
system 
clarification 
question/
user response
DB
Gate
Wordnet
Figure 1: HITIQA preliminary architecture 
 
4   Data Driven Semantics of Questions 
The set of documents and text passages returned 
from the initial search is not just a random subset 
of the database. Depending upon the quality (recall 
and precision) of the text retrieval system avail-
 able, this set can be considered as a first stab at 
understanding the user?s question by the machine.  
Again, given the available resources, this is the 
best the system can do under the circumstances. 
Therefore, we may as well consider this collection 
of retrieved texts (the Retrieved Set) as the mean-
ing of the question as understood by the system. 
This is a fair assessment: the better our search ca-
pabilities, the closer this set would be to what the 
user may accept as an answer to the question.  
     We can do better, however. We can perform 
automatic analysis of the retrieved set, attempting 
to uncover if it is a fairly homogenous bunch (i.e., 
all texts have very similar content), or whether 
there are a number of diverse topics represented 
there, somehow tied together by a common thread. 
In the former case, we may be reasonably confi-
dent that we have the answer, modulo the retriev-
able information. In the latter case, we know that 
the question is more complex than the user may 
have intended, and a negotiation process is needed. 
     We can do better still. We can measure how 
well each of the topical groups within the retrieved 
set is ?matching up? against the question. This is 
accomplished through a framing process described 
later in this paper. The outcome of the framing 
process is twofold: firstly, the alternative interpre-
tations of the question are ranked within 3 broad 
categories: on-target, near-misses and outliers. 
Secondly, salient concepts and attributes for each 
topical group are extracted into topic frames. This 
enables the system to conduct a meaningful dia-
logue with the user, a dialogue which is wholly 
content oriented, and thus entirely data driven.  
ON-TARGET
OUTLIERS
NEAR-MISSES
 
Figure 2: Answer Space Topology.  The goal of interac-
tive QA it to optimize the ON-TARGET middle zone. 
 
5   Clustering 
We use n-gram-based clustering of text passages 
and concept extraction  to uncover the main topics, 
themes and entities in this set.  
     Retrieved documents are first broken into natu-
rally occurring paragraphs.  Duplicate paragraphs 
are filtered out and the remaining passages are 
clustered using a combination of hierarchical clus-
tering and n-bin classification (details of the clus-
tering algorithm can be found in Hardy et al, 
2002a).  Typically three to six clusters are gener-
ated out of the top 50 documents, which may yield 
as many as 1000 passages.  Each cluster represents 
a topic theme within the retrieved set: usually an 
alternative or complimentary interpretation of the 
user?s question. 
    A list of topic labels is assigned to each cluster. 
A topic label may come from one of two places:  
First, the texts in the cluster are compared against 
the list of key phrases extracted from the user?s 
query.  For each match found, the matching phrase 
is used as a topic label for the cluster. If a match 
with the key phrases from the question cannot be 
obtained, Wordnet is consulted to see if a common 
ancestor can be found. For example, ?rifle? and 
?machine gun? are kinds of ?weaponry? in Word-
net, which allows an indirect match between a 
question about weapon inspectors and a text re-
porting a discovery by the authorities of a cache of 
?rifles? and ?machine guns?.  
 
6   Framing 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s 
question and the system ?understanding? of this 
question. The framing is an attempt to impose a 
partial structure on the text that would allow the 
system to systematically compare different text 
pieces against each other and against the question, 
and also to communicate with the user about this. 
In particular, the framing process may uncover 
topics and themes within the retrieved set which 
the user has not explicitly asked for, and thus may 
be unaware of their existence. Nonetheless these 
may carry important information ? the NEAR-
MISSES in Figure 2. 
     In the current version of the system, frames are 
fairly generic templates, consisting of a small 
number of attributes, such as LOCATION, PERSON, 
COUNTRY, ORGANIZATION, etc.  Future versions of 
HITIQA will add domain specialized frames, for 
example, we are currently constructing frames for 
the Weapons Non-proliferation Domain. Most of 
the frame attributes are defined in advance, how-
 ever, dynamic frame expansion is also possible. 
Each of the attributes in a frame is equipped with 
an extractor function which specializes in locating 
and extracting instances of this attribute in the run-
ning text. The extractors are implemented using 
information extraction utilities which form the ker-
nel of Sheffield?s GATE2 system.  We have modi-
fied GATE to separate organizations into compa-
nies and other organizations, and we have also ex-
panded by adding new concepts such as industries.  
Therefore, the framing process resembles strongly 
the template filling task in information extraction 
(cf. MUC3 evaluations), with one significant ex-
ception: while the MUC task was to fill in a tem-
plate using potentially any amount of source text 
(Humphreys et al, 1998), the framing is essentially 
an inverse process. In framing, potentially multiple 
frames can be associated with a small chunk of text 
(a passage or a short paragraph). Furthermore, this 
chunk of text is part of a cluster of very similar text 
chunks that further reinforce some of the most sali-
ent features of these texts. This makes the frame 
filling a significantly less error-prone task ? our 
experience has been far more positive than the 
MUC evaluation results may indicate. This is be-
cause, rather than trying to find the most appropri-
ate values for attributes from among many poten-
tial candidates, we in essence fit the frames over 
small passages4.  
Therefore, data frames are built from the re-
trieved data, after clustering it into several topical 
groups. Since clusters are built out of small text 
passages, we associate a frame with each passage 
that serves as a seed of a cluster. We subsequently 
merge passages, and their associated frames when-
ever anaphoric and other cohesive links are de-
tected.   
     A very similar process is applied to the user?s 
question, resulting in a Goal Frame which can be 
subsequently compared to the data frames obtained 
from retrieved data. For example, the Goal Frame 
generated from the question, ?How has pollution in 
the Black Sea affected the fishing industry, and 
                                                 
2 GATE is Generalized Architecture for Text Engineering, an 
information extraction system developed at the University of 
Sheffield (Cunningham, 2000). 
3 MUC, the Message Understanding Conference, funded by 
ARPA, involved the evaluation of information extraction sys-
tems applied to a common task. 
4 We should note that selecting the right frame type for a pas-
sage is an important pre-condition to ?understanding?. 
what are the sources of this pollution?? is shown 
in Figure 3 below. 
 
TOPIC:[pollution, industry, sources] 
LOCATION: [Black Sea] 
INDUSTRY:[fishing] 
Figure 3: HITIQA generated Goal Frame 
 
            
TOPIC: pollution 
SUB-TOPIC: [sources] 
LOCATION: [Black Sea] 
INDUSTRY :[fisheries, tourism] 
TEXT: [In a period of only three decades (1960's-1980's), 
the Black Sea has suffered the catastrophic degradation 
of a major part of its natural resources. Particularly acute 
problems have arisen as a result of pollution (notably 
from nutrients, fecal material, solid waste and oil), a 
catastrophic decline in commercial fish stocks, a severe 
decrease in tourism and an uncoordinated approach to-
wards coastal zone management. Increased loads of nutri-
ents from rivers and coastal sources caused an overpro-
duction of phytoplankton leading to extensive eutrophica-
tion and often extremely low dissolved oxygen concentra-
tions. The entire ecosystem began to collapse. This prob-
lem, coupled with pollution and irrational exploitation of 
fish stocks, started a sharp decline in fisheries resources.] 
RELEVANCE: Matches on all elements found in goalframe  
Figure 4: A HITIQA generated data frame.  Words in 
bold were used to fill the Frame. 
 
     The data frames are then compared to the Goal 
Frame. We pay particular attention to matching the 
topic attributes, before any other attributes are con-
sidered. If there is an exact match between a Goal 
Frame topic and the text being used to build the 
data frame, then this becomes the data frame?s 
topic as well.  If more than one match is found, the 
subsequent matches become the sub-topics of the 
data frame. On the other hand, if no match is pos-
sible against the Goal Frame topic, we choose the 
topic from the list of the Wordnet generated hy-
pernyms. An example data frame generated from 
the text retrieved in response to the query about the 
Black Sea is shown in Figure 4. After the initial 
framing is done, frames judged to be related to the 
same concept or event, are merged together and 
values of their attributes are combined. 
 
7   Judging Frame Relevance 
We judge a particular data frame as relevant, and 
subsequently the corresponding segment of text as 
relevant, by comparison to the Goal Frame. The 
 data frames are scored based on the number of 
conflicts found between them and the Goal Frame. 
The conflicts are mismatches on values of corre-
sponding attributes. If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.  All other data 
frames are scored with an incrementing conflict 
value, one for frames with one conflict with the 
Goal Frame, two for two conflicts etc.  Frames that 
conflict with all information found in the query are 
given a score of 99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of 99  
are excluded from further processing. The frame in 
Figure 4 is scored as fully relevant to the question 
(0 conflicts). 
 
8   Enabling Dialogue with the User 
Framed information allows HITIQA to automati-
cally judge some text as relevant and to conduct a 
meaningful dialogue with the user as needed on 
other text. The purpose of the dialogue is to help 
the user to navigate the answer space and to solicit 
from the user more details as to what information 
he or she is seeking. The main principle here is that 
the dialogue is at the information semantic level, 
not at the information organization level. Thus, it is 
okay to ask the user whether information about the 
AIDS conference in Cape Town should be in-
cluded in the answer to a question about combating 
AIDS in Africa. However, the user should never be 
asked if a particular keyword is useful or not, or if 
a document is relevant or not. We have developed 
a 3-pronged strategy: 
1. Narrowing dialogue: ask questions that 
would allow the system to reduce the size 
of the answer set.  
2. Expanding dialogue: ask questions that 
would allow the system to decide if the an-
swer set needs to be expanded by informa-
tion just outside of it (near-misses). 
3. Fact seeking dialogue: allow the user to 
ask questions seeking additional facts and 
specific examples, or similar situations. 
Of the above, we have thus far implemented the 
first two options as part of the preliminary clarifi-
cation dialogue. The clarification dialogue is when 
the user and the system negotiate the task that 
needs to be performed. We can call this a ?triaging 
stage?, as opposed to the actual problem solving 
stage (point 3 above). In practice, these two stages 
are not necessarily separated and may be overlap-
ping throughout the entire interaction. Nonetheless, 
these two have decidedly distinct character and 
require different dialogue strategies on the part of 
the system. 
     Our approach to dialogue in HITIQA is mod-
eled to some degree upon the mixed-initiative dia-
logue management adopted in the AMITIES pro-
ject (Hardy et al, 2002b). The main advantage of 
the AMITIES model is its reliance on data-driven 
semantics which allows for spontaneous and mixed 
initiative dialogue to occur.  
     By contrast, the major approaches to implemen-
tation of dialogue systems to date rely on systems 
of functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial sys-
tems, such as in various telephony products, as 
well as in practically oriented research prototypes5, 
(e.g., DARPA, 2002; Seneff and Polifoni, 2000; 
Ferguson and Allen, 1998) a complete dialogue 
transition graph is designed to guide the conversa-
tion and predict user responses, which is suitable 
for closed domains only. In the statistical variation 
of this approach, a transition graph is derived from 
a large body of annotated conversations (e.g., 
Walker, 2000; Litman and Pan, 2002). This latter 
approach is facilitated through a dialogue annota-
tion process, e.g., using Dialogue Act Markup in 
Several Layers (DAMSL) (Allen and Core, 1997), 
which is a system of functional dialogue acts.  
     Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be consid-
ered. In this paper we are concentrating exclusively 
on the semantic layer. 
 
9   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the 
size of this set and the presence of other frames 
outside of it, the system either proceeds to generate 
the answer or initiates a dialogue with the user. For 
                                                 
5 A notable exception is CU Communicator developed 
at University of Colorado (Ward and Pellom, 1999) 
 example, if the answer space appears too large or 
varied, e.g. consists of many different topics, the 
system may ask the user how to narrow it. Alterna-
tively, the presence of large groups of texts frames 
with near-miss frames assigned to them (i.e., 
frames with 1 or 2 conflicts with the Goal Frame) 
may indicate that the answer space is actually lar-
ger, and the user will be consulted about a  possi-
ble broadening of the question. Currently, we only 
initiate a clarification dialogue for 1-conflict 
frames.  
     A 1-conflict frame has only a single attribute 
mismatch with the Goal Frame. This could be a 
mismatch on any attribute, for example, LOCA-
TION, or ORGANIZATION, or TIME, etc. A special 
case arises when the conflict occurs on the TOPIC 
attribute.  Since all other attributes match, we may 
be looking at potentially different events or situa-
tions involving the same entities, or occurring at 
the same location or time. The purpose of the clari-
fication dialogue in this case is to probe which of 
these topics may be of interest to the user.  This is 
illustrated in the exchange below recorded during 
an evaluation session with an intelligence analyst: 
 
User: ?Who is Elizardo Sanchez?? 
HITIQA: ?Are you interested in seeing information 
about civil rights as it is related to Elizardo Sanchez? 
 
ONE-CONFLICT FRAME 
TOPIC: civil_rights  
SUB-TOPIC: []  
LOCATION: [Cuba, Cuba, Cuba]  
COMPANIES: []  
PEOPLE: [Sanchez, Sanchez]  
DOCUMENT DATE: [2000, 1, 11]  
SOURCE: HAVANA (AP) ?  
TEXT: [``I consider that the situation for civil and political 
rights in Cuba has worsened over the past year... owing to 
that Cuba continues to be the only closed society in this hemi-
sphere,'' Sanchez said. ``There have been no significant re-
lease of prisoners, the number of people sanctioned or proc-
essed for political motives increased. Sanchez, who himself 
spent many years in Cuban prisons, is among the communist 
island's best known opposition activists. The commission he 
heads issues a report on civil rights every six months, along 
with a list of people it considers to be imprisoned for political 
motives. ] 
Figure 5: One of the Frames that were used in generat-
ing Sanchez  dialogue.  Words in bold were used to fill 
the Frame. 
 
    In order to understand what happened here, we 
need to note first that the Goal Frame for the user 
question does not have any specific value assigned 
to its TOPIC attribute. This of course is as we would 
expect it: the question does not give us a hint as to 
what information we need to look for or may be 
hoping to find about Sanchez. This also means that 
all the text frames obtained from the retrieved set 
for this question will have at least one conflict, 
near-misses. One such text frame is shown in Fig-
ure 5: its topic is ?civil rights? and it about San-
chez. HITIQA thus asks if ?civil rights? is a topic 
of interest to the user. If the user responds posi-
tively, this topic will be added to the answer space.    
     The above dialogue strategy is applicable to 
other attribute mismatch cases, and produces intel-
ligent-sounding responses from the system. During 
the dialogue, as new information is obtained from 
the user, the Goal Frame is updated and the scores 
of all the data frames are reevaluated. The system 
may interpret the new information as a positive or 
negative. Positives are added to the Goal Frame. 
Negatives are stored in a Negative-Goal Frame and 
will also be used in the re-scoring of the data 
frames, possibly causing conflict scores to in-
crease. The Negative-Goal Frame is created when 
HITIQA receives a negative response from the 
user. The Negative-Goal Frame includes informa-
tion that HITIQA has identified as being of no in-
terest to the user.  If the user responds the equiva-
lent of ?yes? to the system clarification question  in 
the Sanchez dialogue, civil_rights will be added to 
the topic list in the Goal Frame and all one-conflict 
frames with a civil_rights topic will be re-scored to 
Zero conflicts, two-conflict frames with 
civil_rights as a topic will be rescored to one, etc.  
If the user responds ?no?, the Negative-Goal 
Frame will be generated and all frames with 
civil_rights as a topic will be rescored to 99 in or-
der to remove them from further processing. 
     The clarification dialogue will continue on the 
topic level until all the significant sets of NEAR-
MISS frames are either included in the answer 
space (through user broadening the scope of the 
question that removes the initial conflicts) or dis-
missed as not relevant. When HITIQA reaches this 
point it will re-evaluate the data frames in its an-
swer space.  If there are too many answer frames 
now (more than a pre-determined upper threshold), 
the dialogue manager will offer to the user to nar-
row the question using another frame attribute. If 
the size of the new answer space is still too small 
(i.e., there are many unresolved near-miss  frames), 
 the dialogue manager will suggest to the user ways 
of further broadening the question, thus making 
more data frames relevant, or possibly retrieving 
new documents by adding terms acquired through  
the clarification dialogue.  When the number of 
frames is within the acceptable range, HITIQA will 
generate the answer using the text from the frames 
in the current answer space.  The user may end the 
dialogue at any point and have an answer gener-
ated given the current state of the frames. 
 
9.1   Narrowing Dialogue 
HITIQA attempts to reduce the number of frames 
judged to be relevant through a Narrowing Dia-
logue. This is done when the answer space con-
tains too many elements to form a succinct answer. 
This typically happens when the initial question 
turns out to be too vague or unspecific, with re-
spect to the available data. 
 
9.2   Broadening Dialogue 
As explained before, the system may attempt to 
increase the number of frames judged relevant 
through a Broadening Dialogue (BD), whenever 
the answer space appears too narrow, i.e., contains 
too few zero-conflict frames.  We are conducting 
further experiments to define this situation more 
precisely. Currently, the BD will only occur if 
there are one-conflict frames, or near misses. 
Broadening questions can be asked about any of 
the attributes which have values in the Goal Frame. 
 
10   Answer Generation 
Currently, the answer is simply composed of text 
passages from the zero conflict frames. The text of 
these frames are ordered by date and outputted to 
the user.  Typically the answer to these analytical 
type questions will require many pages of informa-
tion.  Example 1 below shows the first portion of 
the answer generated by HITIQA for the Black Sea 
query. Current work is focusing on answer genera-
tion. 
 
2002:  
The Black Sea is widely recognized as one of the re-
gional seas most damaged by human activity. Almost 
one third of the entire land area of continental Europe 
drains into this sea? major European rivers, the Da-
nube, Dnieper and Don, discharge into this sea while its 
only connection to the world's oceans is the narrow 
Bosphorus Strait. The Bosphorus is as little as 70 me-
ters deep and 700 meters wide but the depth of the 
Black Sea itself exceeds two kilometers in places. Con-
taminants and nutrients enter the Black Sea via river 
run-off mainly and by direct discharge from land-based 
sources. The management of the Black Sea itself is the 
shared responsibility of the six coastal countries: Bul-
garia, Georgia, Romania, Russian Federation, Turkey, 
and Ukraine? 
Example 1: Partial answer generated by HITIQA to the 
Black Sea query. 
 
11   Evaluations 
We have just completed the first round of a pilot 
evaluation for testing the interactive dialogue com-
ponent of HITIQA. The purpose of this first stage 
of evaluation is to determine what kind of dialogue 
is acceptable/tolerable to the user and whether an 
efficient navigation though the answer space is 
possible.  HITIQA was blindly tested by two dif-
ferent analysts on eleven different topics.  Five 
different groups participated, but no analyst tested 
more than one system, as system comparison was 
not a goal.  The analysts were given complete free-
dom in forming their queries and responses to 
HITIQA?s questions.  They were only provided 
with descriptions of the eleven topics the systems 
would be tested on.  The analysts were given 15 
minutes for each topic to arrive at what they be-
lieved to be an acceptable answer. During testing a 
Wizard (human) was allowed to intervene if 
HITIQA generated a dialogue question/response 
that was felt inappropriate. The Wizard was able to 
override the system and send a Wizard generated 
question/response to the analyst.  The HITIQA 
Wizard intervened an average of 13% of the time. 
     These results are for information purposes only 
as it was not a formal evaluation.  HITIQA earned 
an average score of 5.8 from both Analysts for dia-
logue, where 1 was ?extremely dissatisfied? and 7 
was ?completely satisfied?.  The highest score pos-
sible was a 7 for each dialogue.  The Analysts were 
asked to grade each scenario for success or failure.  
We divide the failures from both analysts into three 
categories: 
1) the user gives up on the system for the 
given scenario(9%) 
2) the 15 minute time limit was up(13%) 
3) the data was not in the database(9%) 
HITIQA had a 63% success rate for Analyst 1 and 
a 73% success rate for Analyst 2. It is unclear how 
 these results should be interpreted, if at all, as the 
evaluation was a mere pilot, mostly to test the me-
chanics of the setup. We know only that a human 
Wizard equipped with all necessary information 
can easily achieve 100% success in this test. What 
is still needed is a baseline performance, perhaps 
based on using an ordinary keyword-based search 
engine.  
12   Future Work 
This paper describes a work in progress. We ex-
pect that the initial specification of content frame 
will evolve as we subject the initial system to more 
demanding evaluations. Currently, the frames are 
not topically specialized, and this appears the most 
logical next refinement, i.e., develop several (10-
30) types of frames covering different classes of 
events, from politics to medicine to science to in-
ternational economics, etc. This is expected to in-
crease the accuracy of the dialogue as is the inter-
active visualization which is also under develop-
ment. Answer generation will involve fusion of 
information on the frame level, and is currently in 
an initial phase of implementation. 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 2002-
H790400-000. 
References  
J. Allen.  and Core. 1997. Draft of DAMSL:  Dialog Act 
Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/ resources/damsl/    
Bagga, A., T. Strzalkowski, and G.B. Wise. 2000. PartsID: A 
Dialog-Based System for Identifying Parts for Medical 
Systems. Proc. of the ANLP-NAACL-2.  
Chris Buckley. May 1985. Implementation of the Smart in-
formation retrieval system. Technical Report TR85-686, 
Department of Computer Science, Cornell University, 
Ithaca, NY.  
James P. Callan, W. Bruce Croft, Stephen M. Harding 1992. 
The INQUERY Retrieval System.  Proc. of DEXA-92, 3rd 
International Conference on Database and Expert Systems 
Applications. 78-83.  
Cunningham, H., D. Maynard, K. Bontcheva, V. Tablan and 
Y. Wilks. 2000 Experience of using GATE for NLP R&D. 
In Coling 2000 Workshop on Using Toolsets and Architec-
tures To Build NLP Systems.  
DARPA Communicator Program. 2002. 
http://www.darpa.mil/iao/communicator   
 Grinstein, G.G., Levkowitz, H., Pickett, R.M., Smith, S. 1993. 
?Visualization alternatives: non-pixel based images,? 
Proc. of IS&T 46th Annual Conf. 132-133.  
George Ferguson and James Allen. 1998. "TRIPS: An Intelli-
gent Integrated Problem-Solving Assistant," in Proc. of 
the Fifteenth National Conference on Artificial Intelli-
gence (AAAI-98), Madison, WI. 567-573.  
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and 
X. Zhang 2002a. Cross-Document Summarization by 
Concept Classification. Proceedings of SIGIR-2002, Tam-
pere, Finland.  
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb.  2002b.  Multi-layer 
Dialogue Annotation for Automated Multilingual Cus-
tomer Service. ISLE Workshop, Edinburgh, Scotland.  
Harabagiu, S., M. Pasca and S. Maiorano. 2000. Experiments 
with Open-Domain Textual Question Answering. In Proc. 
of COLING-2000. 292-298.  
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, 
H. Cunningham, Y. Wilks. 1998. Description of the 
LaSIE-II System as Used for MUC-7. In Proceedings of 
the Seventh Message Understanding Conference (MUC-
7.)  
Judith Hochberg, Nanda Kambhatla and Salim Roukos. 2002. 
A Flexible Framework for Developing Mixed-Initiative 
Dialog Systems. Proc. of 3rd SIGDIAL Workshop on Dis-
course and Dialogue, Philadelphia.  
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. 
Question Answering in Webclopedia. Notebook Proceed-
ings of Text Retrieval Conference (TREC-9).  
Johnston, M., Ehlen, P., Bangalore, S., Walker., M., Stent, A., 
Maloor, P., and Whittaker, S. 2002. MATCH: An Archi-
tecture for Multimodal Dialogue Systems. In Meeting of 
the Association for Computational Linguistics , 2002.  
Diane J. Litman and Shimei Pan. Designing and Evaluating an 
Adaptive Spoken Dialogue System. 2002. User Modeling 
and User-Adapted Interaction. 12(2/3):111-137.  
Miller, G.A. 1995. WordNet: A Lexical Database. Comm. of 
the ACM, 38(11):39-41. 
John Prager, Dragomir R. Radev, and Krzysztof Czuba. An-
swering what-is questions by virtual annotation. In Human 
Language Technology Conference, Demonstrations Sec-
tion, San Diego, CA, 2001.  
S. Seneff and J. Polifroni, ``Dialogue Management in the 
MERCURY Flight Reservation System,'' Proc. ANLP-
NAACL 2000, Satellite Workshop, 1-6, Seattle, WA, 2000.   
Marilyn A. Walker. An Application of Reinforcement Learn-
ing to Dialogue Strategy Selection in a Spoken Dialogue 
System for Email . Journal of Artificial Intelligence Re-
search.12:387-416.  
W. Ward and B. Pellom.  1999.  The CU Communicator Sys-
tem.  IEEE ASRU. 341-344.  
HITIQA: Scenario Based Question Answering 
Sharon Small, Tomek Strzalkowski, Tracy Janack, Ting Liu,  
Sean Ryan, Robert Salkin, Nobuyuki Shimizu 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tj5550,tl7612,seanryan,rs6021,ns3203}@albany.edu 
 
Paul Kantor, Diane Kelly, Robert Rittman, Nina Wacholder 
Rutgers University 
New Brunswick, New Jersey 08903 
{kantor, nina, diane, rritt}@scils.rutgers.edu 
 
Boris Yamrom 
Lehman College of the City University of New York 
Bronx, New York 10468 
byamrom@lehman.cuny.edy 
 
 
 
Abstract 
In this paper we describe some preliminary 
results of qualitative evaluation of the answer-
ing system HITIQA (High-Quality Interactive 
Question Answering) which has been devel-
oped over the last 2 years as an advanced re-
search tool for information analysts. HITIQA 
is an interactive open-domain question an-
swering technology designed to allow analysts 
to pose complex exploratory questions in natu-
ral language and obtain relevant information 
units to prepare their briefing reports in order 
to satisfy a given scenario. The system uses 
novel data-driven semantics to conduct a clari-
fication dialogue with the user that explores 
the scope and the context of the desired answer 
space. The system has undergone extensive 
hands-on evaluations by a group of intelli-
gence analysts representing various foreign in-
telligence services. This evaluation validated 
the overall approach in HITIQA but also ex-
posed limitations of the current prototype.  
1   Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as ?What 
has been Russia?s reaction to U.S. bombing of Kos-
ovo?? The distinguishing property of such questions is 
that one cannot generally anticipate what might consti-
tute the answer. While certain types of things may be 
expected (e.g., diplomatic statements), the answer is 
heavily conditioned by what information is in fact avail-
able on the topic, background knowledge of the user, 
context in the scenario, intended audience, etc. From a 
practical viewpoint, analytical questions are often un-
derspecified, thus casting a broad net on a space of pos-
sible answers. Therefore, clarification dialogue is often 
needed to negotiate with the user the exact scope and 
intent of the question, and clarify whether similar topics 
found might also be of interest to the user in order to 
complete their scenario report. This paper will present 
results from a series of evaluations conducted in a series 
of workshops with the intended end users of HITIQA 
(professional intelligence analysts) using the system to 
solve realistic analytic problems. 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in the 
state of the art of automated question answering.  In this 
paper we focus on our approach to analytical question 
answering in order to produce a report in response to a 
given scenario.  We also report on the user evaluations 
we conducted and their results with respect to our 
unique approach. 
2   Analytical QA Scenarios 
Analytical scenarios are information task directives 
assigned to analysts to support a larger foreign policy 
process. Scenarios thus contain the information need 
specifications at various levels of detail,  the type, for-
mat and timing of the response required (an intelligence 
report) as well as the primary recipient of the report 
(e.g., the Secretary of State). A hypothetical, but realis-
tic scenario is shown in Figure 1 below. This scenario, 
along with several others like it, was used in evaluating 
 HITIQA performance and fitness for supporting the 
analytical process.  
As can be readily assessed from the directives in 
Figure 1, scenarios are not merely tough questions; they 
are far too complex to be considered as a single question 
at all. It is equally clear that no simple answer can be 
expected and that preparing a report would mean find-
ing answers to a series of interlocking questions or vari-
ous granularities.  
 
Scenario: The al-Qaida Terrorist Group 
 
As an employee of the Central Intelligence Agency, your pro-
fession entails knowledge of the al-Qaida terrorist group.  
Your division chief has ordered a detailed report on the al-
Qaida Terrorist Group due in three weeks. Provide as much 
information as possible on this militant organization. Eventu-
ally, this report should present information regarding the most 
essential concerns, including who are the key figures involved 
with al-Qaida along with other organizations, countries, and 
members that are affiliated, any trades that al-Qaida has made 
with organizations or countries, what facilities they possess, 
where they receive their financial support, what capabilities 
they have (CBW program, other weapons, etc.) and how have 
they acquired them, what is their possible future activity, how 
their training program operates, who their new members are. 
Also, include any other relevant information to your report as 
you see fit.  
 FIGURE 1: Scenario used during user evaluations 
  
We have organized a series of usability evaluations 
with active duty intelligence analysts to find out how 
they approach the problem of solving a scenario. The 
prerequisites for this were are follows: 
1. A robust, broadly functional analytical QA sys-
tem capable of sustaining realistic analytic 
tasks. 
2. A realistic corpus of ?raw intelligence? in form 
of varying quality and verity new-like reports. 
3. A set of realistic, average complexity analytic 
tasks or scenarios to be used. 
HITIQA has been developed over the past two years as 
an open-ended highly flexible interactive QA system to 
allow just this type of evaluation. The system supports a 
variety of information gathering functions without 
straight jacketing the user into any particular mode or 
interaction style. The system does not produce cut and 
dry ?answers?; instead it allows the analysts to build the 
answers the way they want them. While this open-
endedness may seem like unfinished business, we be-
lieve that further development must take into account 
the needs of analysts if they were ever to adopt this 
technology in their work. 
Our main hypothesis is that analysts employ a range 
of strategies to find the required information and that 
these strategies depend significantly upon the nature of 
the task and the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. Our experience with interactive systems also indi-
cated that real users are unlikely to follow any single 
information exploration strategy, but instead would use 
multiple, parallel, even overlapping approaches in order 
to maximize the returns and their confidence in the re-
sults. As a corollary we may expect that the scenario 
tasks are unlikely to be systematically decomposed into 
a series of smaller tasks ahead of actual search. In other 
words, the analytical process is a dialogue, not a se-
quence of commands. Moreover, questions actually 
submitted to the system during the analytical process 
seldom seek just the exact answer, instead they are often 
considered as ?light beams? through the data: focusing 
on the answer but also illuminating adjacent, related 
information which may prove just as valuable.  
AFRL, NIST, CNS and ARDA collaborated in the 
development of scenarios used in our evaluation ses-
sions.  
3   Data Driven Semantics of Questions 
When the user poses a question to a system having 
access to a huge database of unstructured data (text 
files), we need to first reduce the big pile to perhaps a 
handful of documents where the answer is likely to be 
found. The easiest way to do it is to convert the question 
into a search query (by removing stopwords and stem-
ming and tokenizing other words) and submitting this 
query to a fast but non-exact document retrieval system, 
e.g.,   Smart (Buckley, 1985) or InQuery (Callan et al, 
1992), or if you are on the web, Google, etc.   
In the current prototype of HITIQA, we use a com-
bination of Google and InQuery to retrieve the top 50 to 
200 documents from a large document database, con-
sisting of several smaller collections such as newspaper 
stories, documents from the Center of Nonproliferation 
Studies, as well as web mined files.  The retrieved 
documents are then broken down into passages, mostly 
exploiting the naturally occurring paragraph structure of 
the original sources. 
The set of text passages returned from the initial 
search is the first (very crude) approximation of the An-
swer Space for the user?s first question. In order to de-
termine what this answer space consists of we perform 
automatic analysis (a combination of hierarchical clus-
tering and classification) to uncover if what we got is a 
fairly homogenous collection (i.e., all texts have very 
similar content), or whether there are a number of di-
verse topics or aspects represented in there, somehow 
tied together by a common thread. In the former case, 
we may be reasonably confident that we have the an-
swer, modulo the retrievable information. In the latter 
case, we know that the question is more complex than 
the user may have intended, and a negotiation process is 
needed to clarify topics of interest for the scenario re-
port. 
 The next step is to measure how well each of the as-
pects within the answer space is ?matching up? against 
the original question. This is accomplished through the 
framing process described later in this paper. The out-
come of the framing process is twofold: first, the alter-
native interpretations of the question are ranked within 3 
broad categories: on-target, near-misses and outliers. 
Second, salient concepts and attributes for each topi-
cal/aspectual group are extracted into topic frames. This 
enables the system to conduct a meaningful dialogue 
with the user, a dialogue which is wholly content ori-
ented, and entirely data driven.  
4   Partial structuring of text data 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s ques-
tion and the system ?understanding? of this question. 
The framing is an attempt to impose a partial structure 
on the text that would allow the system to systemati-
cally compare different text pieces against each other 
and against the question, and also to communicate with 
the user about this. In particular, the framing process 
may uncover topics or aspects within the answer space 
which the user has not explicitly asked for, and thus 
may be unaware of their existence.  This approach is 
particularly beneficial to the needs of the scenario prob-
lem, where these similar aspects frequently are needed 
in completely ?answering? the scenario, with the sce-
nario report.   
In the current version of HITIQA, frames are pre-
defined structures representing various event types. We 
started with the General frame, which can represent any 
event or relation involving any number of entities such 
as people, locations, organizations, time, and so forth.  
In a specialized domain, or if the user interests are 
known to be limited to a particular set of topics, we de-
fine domain-specific frames. Current HITIQA prototype 
has three broad domain-specific frames, related to the 
Weapon of Mass Destruction proliferation domain 
(which was one of the domains of interest to our users). 
These frames are: WMDTransfer, WMDDevelop, 
WMDTreaty, and of course we keep the General frame.  
Obviously, these three frames do not cover the domain 
represented by our data set; they merely capture the 
most commonly occurring types of events. All frames 
contain a small number of core attributes, such as LO-
CATION, PERSON, COUNTRY, ORGANIZATION, ETC., which 
are extracted using BBN?s Identifinder software, which 
extracts 24 types of entities.  Domain-specific frames 
add event specific attributes, which may require extract-
ing additional items from text, or assigning roles to ex-
isting attributes, or both.  For example, WMDTransfer?s 
attributes TRANSFER_TO and TRANSFER_FROM define 
roles of some COUNTRY or ORGANIZATION, while the 
TRANSFER_TYPE attribute scans the text for keywords 
that may indicate the type of transfer, e.g., export, sale, 
etc.  
HITIQA creates a Goal frame for the user?s ques-
tion, which can be subsequently compared to the data 
frames obtained from retrieved data. A Goal frame can 
be a General frame or any of the domain specific frames 
available in HITIQA.  For example, the Goal frame 
generated from the question, ?Where does al-Qaida 
have training facilities?? is a General frame as shown in 
Figure 2.  This was the first question generated by one 
of our analysts during the first evaluation while working 
on the al-Qaida scenario shown in Figure 1. 
 
FRAME TYPE: General 
TOPIC: training facilities 
ORGANIZATION: al-Qaida 
FIGURE 2: HITIQA generated General-type Goal frame from 
the al-Qaida training facilities question 
 
FRAME TYPE: General 
CONFLICT SCORE: 1 
TRANSFER TYPE: provided 
TRANSFER TO: al-Qaida 
TRANSFER FROM: Iraq 
TOPIC: provided 
SUB-TOPIC: imported 
LOCATION: Iraq 
PEOPLE: Abu Musab al-Zarqawi, Bush, George 
Tenet, Saddam Hussein 
ORGANIZATION:CIA, Administration, al-Qaida 
DOCUMENT: web_283330  
PARAGRAPHS:  ["CIA chief George Tenet seems to 
have gone a long way to back the Bush Administrations dec-
larations that the long split between Islamic fundamentalist 
terrorist organizations like Al-Qaida and secular Iraqi ruler 
Saddam Hussein is healed.   
He has testified that the CIA has evidence of Iraqi provid-
ing Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members 
of a terrorist network led by Abu Musab al-Zarqawi, a close 
Al Qaida associate. "]  
RELEVANCE:  Conflict: [Topic] 
FIGURE 3: A HITIQA generated data frame and the un-
derlying text passage. Words in bold were used to fill the 
Frame.   
 
HTIQA automatically judges a particular data frame 
as relevant, and subsequently the corresponding seg-
ment of text as relevant, by comparison to the Goal 
frame. The data frames are scored based on the number 
of conflicts found between them and the Goal frame. 
The conflicts are mismatches on values of correspond-
ing attributes. If a data frame is found to have no con-
flicts, it is given the highest relevance rank, and a con-
flict score of zero.  All other data frames are scored with 
 a decreasing value based on the number of conflicts, 
negative one for frames with one conflict with the Goal 
frame, negative two for two conflicts etc.  Frames that 
conflict with all information found in the question are 
given a score of -99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of -99 are 
excluded from further processing as outliers. The frame 
in Figure 2 is scored as a near miss and will generate 
dialogue, where the user will decide whether or not it 
should be included in the answer space. 
5   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the pres-
ence of other frames outside of this set, the system ei-
ther proceeds to generate the answer or initiates a dia-
logue with the user.  HITIQA begins asking the user 
questions on these near-miss frame groups, with the 
largest group first.  The groups must be at least groups 
of size N, where N is a user controlled setting.  This 
setting restricts all of HITIQA?s generated dialogue.   
A one conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a mis-
match on any of the General attributes, for example, 
LOCATION, or ORGANIZATION, or TIME, etc., or in one of 
the domain specific attributes, TRANSFER_TO, or TRANS-
FER_TYPE, etc.  A special case arises when the conflict 
occurs on the TOPIC attribute.  Since all other attributes 
match, we may be looking at potentially different events 
or situations involving the same entities, or occurring at 
the same location or time. The purpose of the clarifica-
tion dialogue in this case is to probe which of these top-
ics may be of interest to the user.  Another special case 
arises when the Goal frame is of a different type than a 
data frame.  The purpose of the clarification dialogue in 
this case is to expand the user?s answer space into a 
different but possibly related event.  A combination of 
both of these cases is illustrated in the exchange in Fig-
ure 4 below.   
User: ?Where does al-Qaida have training facili-
ties?? 
HITIQA: ?Do you want to see material on the trans-
fer of weapons and intelligence to al-Qaida?? 
FIGURE 4: Dialogue generated by HITIQA for the al-Qaida 
training facilities question 
 
In order to understand what happened here, we need 
to note first that the Goal frame for this example is a 
General Frame, from Figure 2.  One of the data frames 
that caused this dialogue to be generated is shown in 
Figure 3 above.  While this frame is of a different frame 
type than the Goal frame, namely WMD Transfer, it 
matches on all of the General attributes except TOPIC, so 
HITIQA asks the user if they would like to expand their 
answer space to this other domain, namely to include 
the transfer of weapons involving this organization as 
well.   
 
ANSWER REPORT:  
 
The New York Times said the Mindanao had become the 
training center for the Jemaah Islamiah network, believed by 
many Western governments to be affiliated to the al-Qaida 
movement of Osama bin Laden 
DocName: A-web_283305 ParaId: 2  
 
? 
IRAQ REPORTED TO HAVE PROVIDED MATERIALS 
TO AL QAIDA  
2003  
[CIA chief George Tenet seems to have gone a long way to 
back the Bush Administrations declarations that the long split 
between Islamic fundamentalist terrorist organizations like Al 
Qiada and secular Iraqi ruler Saddam Hussein is healed. 
DocName: A-web_283330 ParaId: 6  
He has testified that the CIA has evidence of Iraqi providing 
Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members of 
a terrorist network led by Abu Musab al-Zarqawi, a close Al 
Qaida associate. The Bush Administration and the press has 
carelessly shorthanded this to mean, a senior Al Qaida mem-
ber, ignoring the real ambiguities that surround the true nature 
of that association, and whether Zarqawi shares Al Qaidas 
ends, or is receiving anything more than lodging inside Iraq. ] 
DocName: A-web_283330 ParaId: 7  
FIGURE 5: Partial answer generated by HITIQA to the al-
Qaida training facilities question 
 
During the dialogue, as new information is obtained 
from the user, the Goal frame is updated and the scores 
of all the data frames are reevaluated. The system may 
interpret the new information as a positive or negative. 
Positives are added to the Goal frame. Negatives are 
stored in a Negative-Goal frame and will also be used in 
the re-scoring of the data frames, possibly causing con-
flict scores to increase. If the user responds the equiva-
lent of ?yes? to the system clarification question in Fig-
ure 4, a corresponding WMD Transfer frame would be 
added to the Goal frame and all WMD Transfer frames 
will be re-scored.  If the user responds ?no?, the Nega-
tive-Goal frame will be generated and all WMD Trans-
fer frames will be rescored to 99 in order to remove 
them from further processing.  The user may end the 
dialogue, at any point and have an answer generated 
given the current state of the frames.   
Currently, the answer is simply composed of text 
passages from the zero conflict frames. In addition, 
HITIQA will generate a ?headline? for the text passages 
in all the Frames in the answer space.  This is done us-
ing grammar rules and the attributes of a frame.  Figure 
 5 shows a portion of the answer generated by HITIQA 
for the al-Qaida training facilities question. 
 
6   HITIQA Interface 
There are two distinct ways for the user to interact 
with HITIQA to explore their answer space.  The An-
swer Panel displays the user?s current answer at any 
given time during the interaction for a single question.  
Through this panel the user can read the paragraphs that 
are currently in their answer.  There are links on this 
panel so the user is able to view the full original source 
document from which the passage(s) were extracted. 
 The Visual panel offers the user an alternative to 
reading text by providing a tool for visually browsing 
the entire answer space.  Figure 6 shows a typical view 
of the visualization panel. The spheres are representa-
tive of single frames and groups of frames.  The user?s 
attention may be drawn to particular frames by the color 
coding or the attribute spikes.  The colors represent the 
frame?s score, so the user can quickly see what is in 
their answer, blue, and what is not, all other colors.  The 
attribute spikes may also be used as a navigation tool.  
The active attribute is chosen by the user through radio 
buttons. The current active attribute in Figure 6, is Lo-
cation.  This displays all instances of locations men-
tioned in the corresponding text. 
 
 
        Figure 6: Frame Level Display 
 
The underlying text that was used to build the frame 
may be displayed in the lower right hand window.  In 
this text display window there is a hyperlink that takes 
the user directly to the full source document. The user is 
able to interact with this panel by adding and removing 
information from their generated answer. Moving from 
the visualization to the textual dialogue, the generated 
answer, and back is seamless in a sense that any 
changes to the frame scores in one modality are imme-
diately accessible to the user in another modality. Users 
can add and remove frames from the answer space and 
HITIQA will always seamlessly pickup a new dialogue 
or generate a new answer.  
 
7   HITIQA Qualitative Evaluations 
In order to assess our progress thus far, and to also 
develop metrics to guide future evaluation, we invited a 
group of analysts employed by the US government to 
participate in two three-day workshops held in Septem-
ber and October 2003.  
The two basic objectives of the workshops were: 
1. To perform a realistic assessment of the useful-
ness and usability of HITIQA as an end-to-end system, 
from the information seeker's initial questions to com-
pletion of a draft report.  
2. To develop metrics to compare the answers ob-
tained by different analysts and evaluate the quality of 
the support that HITIQA provides.     
Each of these objectives entails a particular chal-
lenge. Performing a realistic assessment of HITIQA is 
difficult because many of the resources that the analysts 
use, as well as the reports they produce, are classified 
and therefore inaccessible to researchers.  
Assessing the quality of the support that the system 
provides is not easy because analytical questions rarely 
have a single right answer. It is not obvious how to de-
fine, for example, the precision of the system. We there-
fore conducted an 'information unit' exercise, whose 
purpose was to determine whether the analysts could 
identify information building blocks in their reports, so 
that we could compare and contrast different reports.  
To obtain an adequate supply of appropriate text 
data to support extensive question answering sessions 
(1, 2, 3 and 4 hours long), we prepared a new corpus of 
approximately 1.2 Gbytes. This new corpus consists of 
the reports from the Center for Non-Proliferation Stud-
ies (CNS) collected for the AQUAINT Program, aug-
mented with a much larger collection of texts on similar 
subject matter mined from the web using Google1. The 
final corpus proved to be sufficient to support about 
three hours of use of HITIQA to ?solve? each of the 
scenarios. 
The first day of the first workshop was devoted to 
training, including a two-part proficiency test. HITIQA 
is a fairly complex system, that includes multiple layers 
of data processing and user interaction, and it was criti-
cal that the users are sufficiently ?fluent? if we were to 
measure their productivity. The analysts' primary task 
on the second day was preparation of reports in re-
sponse to the scenarios. 
                                                 
1 Google has kindly agreed to temporarily extend our 
usage license so we could collect the data over a short 
time. 
  The third day was devoted to quantitative and quali-
tative evaluation, discussed later. In addition, we asked 
the analysts to score each others reports, as well as to 
identify key information units in them. These informa-
tion units could be later compared across different re-
ports in order to determine their completeness.  
8   Workshop Results 
The results of the quantitative evaluations strongly vali-
date the approach that we have taken. These conclusions 
are confirmed by analysts comments gleaned both from 
the formal qualitative assessment and from informal 
discussion. As one analyst said, ?the system as it stands 
now, in my mind, gave me enough information to try to 
put together a 80% solution but ?I don't think you're 
ever gonna reach that 100% state.? At the same time, we 
learned a great deal about how analysts work. 
It is important to determine the realism of the sce-
narios used during the workshop relative to the analysts? 
current work tasks in order for any results to be mean-
ingful. Each analyst was asked a series of five questions 
such as, ?How realistic was the scenario?  In other 
words did it resemble tasks you could imagine perform-
ing at work?? These 5 questions were all relative to the 
realism and difficulty of the scenario tasks.  Analysts 
used a scale of 1 to 5 based on their agreement with the 
statements, where 5 was complete agreement.  Our 
mean score was 3.84, indicating our scenarios were real-
istic and of about average difficulty when compared to 
the work they normally perform.   
We have classified the type of passages that an ana-
lyst copied to their report into two categories, answer 
passages and additional information passages, see Fig-
ure 7 below.  The answer passages either exactly an-
swered the user?s initial question or supplied supporting 
information.  The additional passages do not answer the 
original question posed, but may have been added to the 
answer through dialogue, or through the user?s explora-
tion of document links offered.  This could be a piece of 
information needed to satisfy some other aspect of the 
scenario that they had not asked about yet, or possibly a 
topic the user had not even considered but found rele-
vant when it was presented to them. As can be seen 
there was a very large amount of ?additional? informa-
tion that the user copied to their report.  The amounts 
reported here are the averages for all of the analysts for 
both workshops.  This supports our hypothesis that ana-
lysts seldom seek just the exact answer, but they are 
also looking at adjacent, related information, much of 
which they retain for their report.  Note that there were a 
small number of passages that contained a combination 
of answer and additional information; these were added 
to answer.   
 
Average Number of Passages Copied to Report
2.83
13.63
1.54
5.06
0.00
2.00
4.00
6.00
8.00
10.00
12.00
14.00
16.00
answ er additional
Passage Type
N
um
be
r o
f P
as
sa
ge
s
copied f rom link
copied f rom answ er
 Figure 7: Average Number of Passages Copied 
 
Total Passages Copied and Viewed: Analyst 2
37
8 16
28 27
230
50
242
352
152
16 11 4 7 4
49
27 26 34
44
0
50
100
150
200
250
300
350
400
1 2 3 4 5
Scenario
N
um
be
r o
f P
as
sa
ge
s
passages copied from links
passages viewed from links
passages copied from answer
passages viewed on answer
 
 
Figure 8: Number of Passages Copied Vs. Those Viewed 
 
We should now establish the number of passages 
copied versus those viewed, relative to links and the 
answer.  Figure 8 above shows the total number of pas-
sages copied versus the total number of passages 
viewed.  It is seen that many more passages need to be 
viewed through full document links before a useful pas-
sage is found.  In comparison a much smaller number of 
answer passages need to be viewed from the Answer 
panel in order to find useful passages.   
All of the analysts? sessions were recorded using 
Camtasia.  Figure 9 shows an annotation created for a 
typical session.  Analysts were observed to utilize a 
range of varying strategies as they worked different 
scenarios and even while working different queries of 
the same scenario.  Figure 10 shows the statistics for 
each Analyst?s use of HITIQA while working on the 
scenarios during the two workshops (note that Analyst-4 
was only able to attend the first workshop and Analyst-1 
did not create a report for Scenario 2).  Some of the 
variations in strategies among the analysts while work-
ing the same scenario are quite striking.  For example, 
Scenario 4 was  worked quite  differently  by  Analyst-1  
 versus Analyst-2.  While Analyst-1 spent almost all of 
his/her  time in  the Visual Panel, Analyst-2 spent virtu-
ally all of his/her time in the Answer panel.  Analyst-1 
produced his/her report copying 52 paragraphs while 
Analyst 2 copied only 35.  There are also large varia-
tions in the number of questions asked for the same sce-
nario.  Examine scenario 5, where Analyst-3 asked a 
total of 11 questions and Analyst-2 only needed to ask 2 
questions.  Relative to this, Analyst-3, who asked a 
much larger number of questions, copied only 28 pas-
sages, whereas Analyst-2 copied 31.  These variations, 
as stated earlier in the paper, could be due to the nature 
of the task, the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. For example, the difference in the number of 
questions asked between Analyst-2 and Analyst-3 for 
scenario 5 may be due to difference in search strategies 
employed, but may also reflect the amount of back-
ground knowledge of the topic.   
 
      
        FIGURE 9: Fragment of an analytical session 
 
Variation of Strategies: Analyst 1
8
0
5 5 5
18
0
20
52
4248
0
41.5
86
60.5
12
0 1
6.67
26
1
10
100
1 2 3 4 5
Scenario
Variation of Strategies: Analyst 2
5
4
3 3
2
53
19 20
35 3135 29
47
5
17
61
20
33
100
72
1
10
100
1 2 3 4 5
Scenario
 
101 115
Variation of Strategies: Analyst 3
12
3
4
6
11
58
17
25 24
28
21
7
19
70
54
65
1
26
1
10
100
1 2 3 4 5
Scenario
 
Variation of Strategies: Analyst 4
2
3
0 0 0
35
0 0 0
37
0 0 0 0
36 40
0 0 0
34
1
10
100
1 2 3 4 5
Scenario
# questions asked
# passages copied
time in visual
time in answer
 
Figure 10: Varying Strategies Employed 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
          Clarification Dialogue: 1 minute 
? 6 questions generated by HITIQA 
? replied ?Yes? to 5 and ?No? to 1 
? 5+ passages added to answer 
           Studying Answer Panel: 60 minutes  
? Copying 24 passages to report 
? 10 from Answer 
? 14 from Links to Full Document 
? Visual Panel Browsing: 5 minutes 
? Nothing copied 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
          Clarification Dialogue: 1 minute 
? 5 questions generated by HITIQA 
? replied ?Yes? to 2 and ?No? to 3 
? 2+ passages added to answer 
           Studying Answer Panel: 26 minutes 
? Copying 6 passages to report 
? 6 from Links to Full Document 
            Visual Panel browsing: 1 minute 
? Copying 1 passage to report 
? 1 from Links to Full Document 
User: How was South Africa's CBW program fi-
nanced?  
         Clarification Dialogue: 40 seconds 
? 7 questions generated by HITIQA 
? replied ?Yes? to 3 and ?No? to 4 
? 3+ passages added to answer 
            Studying Answer Panel: 11 minutes 
? Copying 3 passages to report 
? 1 from Answer 
      2 from Links to full Document 
  
There is, however, some consistency across the ana-
lysts in the amount of information retained per scenario. 
The charts are drawn in logarithmic scale, but it should 
be visible that scenarios 2 and 3 produced less interac-
tion and required less information to fulfill than scenar-
ios 4 and 5. It is also visible that scenario 1 required 
more questions to be asked and more exploration to be 
done in visual panel than other scenarios. 
Finally, it is important to provide some metric re-
garding the user?s overall satisfaction with their use of 
HITIQA.  At the end of each workshop Analysts were 
given a series of 17 questions, such as ?HITIQA helps 
me find important information?, shown in Figure 11, to 
assess their overall experience with the system.  Many 
of these questions were designed for the user to com-
pare HITIQA to the current tools they are using for this 
type of task.   Analysts again used a scale of 1 to 5 
based on their agreement with the statements.  The re-
sults were then converted, where 5 would always denote 
the best, and are shown in Figure 11 below.  It is impor-
tant to note that we scored highly overall, but addition-
ally we scored highly in the majority of questions rela-
tive to comparison of their current tools.  For example, 
for Question 14: ?Having HITIQA at work would help 
me find information faster than I can currently find it?, 
our mean score was 3.83.  
 
3.7215702092Total
3.00311117
4.141616
2.8614215
3.8314114
3.1632113
4.1424112
4.0024111
4.00710
3.71529
3.29258
3.143317
3.711516
4.43345
4.293314
4.14163
3.7114112
3.71611
ScoreScore5Score4Score3Score2Score1Question
MeanFrequency of Analyst's Scores of Overall Workshop I & II
1                  2                 3                 4    5 score
frequency
 
      FIGURE 11: Final Evaluation Results, Workshop 1 & 2 
 
In summary, the results from these two evaluations 
indicate that HITIQA, in its current state, is already 
competitive with the tools that the analysts are currently 
using in their work, supporting our overall approach to 
Analytical Question Answering.  HTIQA provides the 
user with a tool to find the passages needed to complete 
a report for a given scenario.  While working on a sce-
nario HITIQA has been shown to provide information 
which exactly answers the user?s question, and addi-
tionally HITIQA?s method brings to light other related 
information that the analyst retains in order to complete 
their report. 
 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Advanced 
Question Answering for Intelligence (AQUAINT) Program 
under contract number 2002-H790400-000. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  Dialog Act Markup in 
Several Layers. www.cs.rochester.edu/research/cisd/   
Baeza-Yates and Ribeiro-Neto. 1999. Modern Information Retrieval. 
Addison Wesley. 
Chris Buckley. 1985. Implementation of the Smart information re-
trieval system. Technical Report TR85-686, Department of Com-
puter Science, Cornell University, Ithaca, NY. 
Ferguson, George and James Allen. 1998. TRIPS: An Intelligent Inte-
grated Problem-Solving Assistant, in Proceedings of the 15th 
AAAI Conference (AAAI-98), Madison, WI, pp. 567-573. 
Hardy, H., N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and X. 
Zhang. 2002a. Cross-Document Summarization by Concept Clas-
sification. Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb. 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual Customer Service. ISLE 
Workshop, Edinburgh, Scotland. 
Harabagiu, S., et. al. 2002. Answering Complex, List and Context 
questions with LCC?s Question Answering Server.   In Proceedings 
of Text Retrieval Conference (TREC-10). 
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. Ques-
tion Answering in Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference (TREC-9). 
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. 
Cunningham, Y. Wilks. 1998. Description of the LaSIE-II System 
as Used for MUC-7. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7.) 
Litman, Diane J. and Shimei Pan. 2002. Designing and Evaluating an 
Adaptive Spoken Dialogue System. User Modeling and User-
Adapted Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Seneff, S. and J. Polifroni. 2000. Dialogue Management in the MER-
CURY Flight Reservation System. Proc. ANLP-NAACL 2000, 
Satellite Workshop, pp. 1-6, Seattle, WA. 
Small, Sharon, Nobuyuki Shimizu, Tomek Strzalkowski and Liu Ting 
(2003). HITIQA: A Data Driven Approach to Interactive Question 
Answering: A Preliminary Report. AAAI Spring Symposium on 
New Directions in Question Answering, Stanford University, 
March 24-26, 2003. pp. 94?104. 
Tang, Rong, K.B. Ng, Tomek Strzalkowski and Paul Kantor (2003). 
Automatic Prediction of Information Quality in News Documents. 
Proceedings of HLT-NAACL 2003, Edmonton, May 27-June 1 
Walker, Marilyn A. 2002. An Application of Reinforcement Learning 
to Dialogue Strategy Selection in a Spoken Dialogue System for 
Email . Journal of AI Research, vol 12., pp. 387-416. 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161?169,
Beijing, August 2010
Jointly Modeling WSD and SRL with Markov Logic
Wanxiang Che and Ting Liu
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
{car, tliu}@ir.hit.edu.cn
Abstract
Semantic role labeling (SRL) and word
sense disambiguation (WSD) are two fun-
damental tasks in natural language pro-
cessing to find a sentence-level seman-
tic representation. To date, they have
mostly been modeled in isolation. How-
ever, this approach neglects logical con-
straints between them. We therefore ex-
ploit some pipeline systems which verify
the automatic all word sense disambigua-
tion could help the semantic role label-
ing and vice versa. We further propose a
Markov logic model that jointly labels se-
mantic roles and disambiguates all word
senses. By evaluating our model on the
OntoNotes 3.0 data, we show that this
joint approach leads to a higher perfor-
mance for word sense disambiguation and
semantic role labeling than those pipeline
approaches.
1 Introduction
Semantic role labeling (SRL) and word sense dis-
ambiguation (WSD) are two fundamental tasks in
natural language processing to find a sentence-
level semantic representation. Semantic role la-
beling aims at identifying the relations between
predicates in a sentence and their associated ar-
guments. Word sense disambiguation is the pro-
cess of identifying the correct meaning, or sense
of a word in a given context. For example, for
the sentence in Figure 1, we can find out that the
predicate token ?hitting? at position 3 has sense
?cause to move by striking? and the sense label is
?hit.01?. The argument headed by the token ?cat?
at position 1 with sense ?feline mammal? (cat.01)
is referring to the player (A0), and the argument
headed by the token ?ball? at position 5 with sense
Figure 1: A sample of word sense disambiguation
and semantic role labeling.
?round object that is hit in games? (ball.01) is re-
ferring to the game object (A1) being hit.
Normally, semantic role labeling and word
sense disambiguation are regarded as two inde-
pendent tasks, i.e., the word sense information
is rarely used in a semantic role labeling system
and vice versa. A few researchers have used se-
mantic roles to help the verb sense disambigua-
tion (Dang and Palmer, 2005). More people used
predicate senses in semantic role labeling (Hajic?
et al, 2009; Surdeanu et al, 2008). However, both
of the pipeline methods ignore possible dependen-
cies between the word senses and semantic roles,
and can result in the error propagation problem.
The same problem also appears in other natural
language processing tasks.
In order to make different natural language pro-
cessing tasks be able to help each other, jointly
modeling methods become popular recently, such
as joint Chinese word segmentation and part-of-
speech tagging (Kruengkrai et al, 2009; Zhang
and Clark, 2008; Jiang et al, 2008), joint lemma-
tization and part-of-speech prediction (Toutanova
and Cherry, 2009), joint morphological segmenta-
tion and syntactic parsing (Goldberg and Tsarfaty,
2008), joint text and aspect ratings for sentiment
summarization (Titov and McDonald, 2008), and
joint parsing and named entity recognition (Finkel
and Manning, 2009). For semantic role label-
ing, Dahlmeier et al (2009) proposed a method
to maximize the joint probability of the seman-
161
tic role of preposition phrases and the preposition
sense.
In order to do better joint learning, a novel
statistical relational learning framework, Markov
logic (Domingos and Lowd, 2009) was intro-
duced to join semantic role labeling and predicate
senses (Meza-Ruiz and Riedel, 2009). Markov
logic combines the first order logic and Markov
networks, to develop a joint probability model
over all related rules. Global constraints (intro-
duced by Punyakanok et al (2008)) among se-
mantic roles can be easily added into Markov
logic. And the more important, the jointly model-
ing can be realized using Markov logic naturally.
Besides predicates and prepositions, other word
senses are also important information for recog-
nizing semantic roles. For example, if we know
?cat? is an ?agent? of the predicate ?hit? in a sen-
tence, we can guess that ?dog? can also be an
?agent? of ?hit?, though it does not appear in the
training data. Similarly, the semantic role infor-
mation can also help to disambiguate word senses.
In addition, the predicate sense and the argument
sense can also help each other. In the sentence
?The cat is hitting a ball.?, if we know ?hit? here
has a game related sense, we can guess that the
?ball? should have the sense ?is a round object in
games?. In the same way, the correct ?ball? sense
can help to disambiguate the sense of ?hit?. The
joint probability, that they are disambiguated cor-
rectly simultaneously will be larger than other ab-
normalities.
The release of OntoNotes (Hovy et al, 2006)
provides us an opportunity to jointly model all
word senses disambiguation and semantic role la-
beling. OntoNotes is a large corpus annotated
with constituency trees (based on Penn Tree-
bank), predicate argument structures (based on
Penn PropBank), all word senses, etc. It has been
used in some natural language processing tasks,
such as joint parsing and named entity recogni-
tion (Finkel and Manning, 2009), and word sense
disambiguation (Zhong et al, 2008).
In this paper, we first propose some pipeline
systems which exploit automatic all word sense
disambiguation into semantic role labeling task
and vice versa. Then we present a Markov logic
model which can easily express useful global con-
straints and jointly disambiguate all word senses
and label semantic roles.
Experiments on the OntoNotes 3.0 corpus show
that (1) the automatic all word sense disambigua-
tion and semantic role labeling tasks can help each
other when using pipeline approaches, and more
important, (2) the joint approach using Markov
logic leads to higher accuracy for word sense dis-
ambiguation and performance (F1) for semantic
role labeling than pipeline approaches.
2 Related Work
Joint models were often used in semantic role la-
beling community. Toutanova et al (2008) and
Punyakanok et al (2008) presented a re-ranking
model and an integer linear programming model
respectively to jointly learn a global optimal se-
mantic roles assignment. Besides jointly learning
semantic role assignment of different constituents
for one task (semantic role labeling), their meth-
ods have been used to jointly learn for two tasks
(semantic role labeling and syntactic parsing).
However, it is easy for the re-ranking model to
loss the optimal result, if it is not included in the
top n results. In addition, the integer linear pro-
gramming model can only use hard constraints. A
lot of engineering work is also required in both
models.
Recently, Markov logic (Domingos and Lowd,
2009) became a hot framework for joint model.
It has been successfully used in temporal relations
recognition (Yoshikawa et al, 2009), co-reference
resolution (Poon and Domingos, 2008), etc. It
is very easy to do joint modeling using Markov
logic. The only work is to define relevant formu-
las. Meza-Ruiz and Riedel (2009) have joined se-
mantic role labeling and predicate senses disam-
biguation with Markov logic.
The above idea, that the predicate senses and
the semantic role labeling can help each other,
may be inspired by Hajic? et al (2009), Surdeanu
et al (2008), and Dang and Palmer (2005). They
have shown that semantic role features are helpful
to disambiguate verb senses and vice versa.
Besides predicate senses, Dahlmeier et al
(2009) proposed a joint model to maximize prob-
ability of the preposition senses and the semantic
role of prepositional phrases.
162
Except for predicate and preposition senses,
Che et al (2010) explored all word senses for se-
mantic role labeling. They showed that all word
senses can improve the semantic role labeling per-
formance significantly. However, the golden word
senses were used in their experiments. The results
are still unknown when an automatic word sense
disambiguation system is used.
In this paper, we not only use all word senses
disambiguated by an automatic system, but also
make the semantic role labeling results to help
word sense disambiguation synchronously with a
joint model.
3 Markov Logic
Markov logic can be understood as a knowledge
representation with a weight attached to a first-
order logic formula. Let us describe Markov
logic in the case of the semantic role labeling
task. We can model this task by first introduc-
ing a set of logical predicates such as role(p, a, r)
and lemma(i, l), which means that the argument
at position a has the role r with respect to the
predicate at position p and token at position i has
lemma l respectively. Then we specify a set of
weighted first order formulas that define a distri-
bution over sets of ground atoms of these predi-
cates (or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulas assigns high probability to
possible worlds where semantic role labeling is
correct and a low probability to worlds where this
is not the case. For instance, for the sentence
in Figure 1, a suitable set of weighted formulas
would assign a high probability to the world:
lemma(1, cat), lemma(3, hit), lemma(5, ball)
role(3, 1, A0), role(3, 5, A1)
and low probabilities to other cases.
A Markov logic network (MLN) M is a set
of weighted formulas, i.e., a set of pairs (?, ?),
where ? is a first order formula and ? is the real
weight of the formula. M defines a probability
distribution over possible worlds:
p(y) = 1Z exp(
?
(?,?)?M
?
?
c?C?
f?c (y))
where each c is a binding of free variables in ?
to constants. Each f?c is a binary feature function
that returns 1 if the possible world y includes the
ground formula by replacing the free variables in
? with the constants in c is true, and 0 otherwise.
C? is the set of all bindings for the variables in ?.
Z is a normalization constant.
4 Model
We divide our system into two stages: word sense
disambiguation and semantic role labeling. For
comparison, we can process them with pipeline
strategy, i.e., the word sense disambiguation re-
sults are used in semantic role labeling or the se-
mantic role labeling results are used in word sense
disambiguation. Of course, we can jointly process
them with Markov logic easily.
We define two hidden predicates for the two
stages respectively. For word sense disambigua-
tion, we define the predicate sense(w, s) which
indicates that the word at position w has the
sense s. For semantic role labeling, the predicate
role(p, a, r) is defined as mentioned in above.
Different from Meza-Ruiz and Riedel (2009),
which only used sense number as word sense
representation, we use a triple (lemma, part-of-
speech, sense num) to represent the word sense
s. For example, (hit, v, 01) denotes that the verb
?hit? has sense number 01. Obviously, our rep-
resentation can distinguish different word senses
which have the identical sense number. In ad-
dition, we use one argument classification stage
with predicate role to label semantic roles as Che
et al (2009). Similarly, no argument identifica-
tion stage is used in our model. The approach can
improve the recall of the system.
In addition to the hidden predicates, we define
observable predicates to represent the information
available in the corpus. Table 1 presents these
predicates.
4.1 Local Formula
A local formula means that its groundings relate
any number of observed ground atoms to exactly
one hidden ground atom. For example
lemma(p,+l1)?lemma(a,+l2)? role(p, a,+r)
163
Predicates Description
word(i, w) Token i has word w
pos(i, t) Token i has part-of-speech t
lemma(i, l) Token i has lemma l
chdpos(i, t) The part-of-speech string of to-
ken i?s all children is t
chddep(i, d) The dependency relation string
of token i?s all children is t
firstLemma(i, l) The leftmost lemma of a sub-
tree rooted by token i is l
lastLemma(i, l) The rightmost lemma of a sub-
tree rooted by token i is l
posFrame(i, fr) fr is a part-of-speech frame at
token i
dep(h, a, de) The dependency relation be-
tween an argument a and its
head h is de
isPredicate(p) Token p is a predicate
posPath(p, a, pa) The part-of-speech path be-
tween a predicate p and an ar-
gument a is pa
depPath(p, a, pa) The dependency relation path
between a predicate p and an ar-
gument a is pa
pathLen(p, a, le) The path length between a pred-
icate p and an argument a is le
position(p, a, po) The relative position between a
predicate p and an argument a
is po
family(p, a, fa) The family relation between a
predicate p and an argument a
is fa
wsdCand(i, t) Token i is a word sense disam-
biguation candidate, here t is
?v? or ?n?
uniqe(r) For a predicate, semantic role r
can only appear once
Table 1: Observable Predicates.
means that if the predicate lemma at position p
is l1 and the argument lemma at position a is l2,
then the semantic role between the predicate and
the argument is r with some possibility.
The + notation signifies that Markov logic gen-
erates a separate formula and a separate weight for
each constant of the appropriate type, such as each
possible pair of lemmas (l1, l2, r). This type of
?template-based? formula generation can be per-
formed automatically by a Markov logic engine,
such as the thebeast1 system.
The local formulas are based on features em-
ployed in the state-of-the-art systems. For word
sense disambiguation, we use the basic features
mentioned by Zhong et al (2008). The semantic
role labeling features are from Che et al (2009),
1http://code.google.com/p/thebeast/
Features SRL WSD
Lemma ? ?
POS ? ?
FirstwordLemma ?
HeadwordLemma ?
HeadwordPOS ?
LastwordLemma ?
POSPath ?
PathLength ?
Position ?
PredicateLemma ?
PredicatePOS ?
RelationPath ?
DepRelation ?
POSUpPath ?
POSFrame ?
FamilyShip ?
BagOfWords ?
Window3OrderedWords ?
Window3OrderedPOSs ?
Table 2: Local Features.
the best system of the CoNLL 2009 shared task.
The final features are listed in Table 2.
What follows are some simple examples in or-
der to explain how we implement each feature as
a formula (or a set of formulas).
Consider the ?Position? feature. We first intro-
duce a predicate position(p, a, po) that denotes
the relative position between predicate p and ar-
gument a is po. Then we add a formula
position(p, a,+po)? role(p, a,+r)
for all possible combinations of position and role
relations.
The ?BagOfWords? feature means that the
sense of a word w is determined by all of lemmas
in a sentence. Then, we add the following formula
set:
wsdCand(w,+tw) ? lemma(w,+lw) ? lemma(1,+l1) ? sense(w,+s)
. . .
wsdCand(w,+tw) ? lemma(w,+lw) ? lemma(2,+li) ? sense(w,+s)
. . .
wsdCand(w,+tw) ? lemma(w,+lw) ? lemma(n,+ln) ? sense(w,+s)
where, the w is the position of current word and
tw is its part-of-speech tag, lw is its lemma. li
is the lemma of token i. There are n tokens in a
sentence totally.
4.2 Global Formula
Global formulas relate more than one hidden
ground atoms. We use this type of formula for
two purposes:
164
1. To capture the global constraints among dif-
ferent semantic roles;
2. To reflect the joint relation between word
sense disambiguation and semantic role labeling.
Punyakanok et al (2008) proposed an integer
linear programming (ILP) model to get the global
optimization for semantic role labeling, which sat-
isfies some constraints. This approach has been
successfully transferred into dependency parse
tree based semantic role labeling system by Che
et al (2009). The final results must satisfy two
constraints which can be described with Markov
logic formulas as follows:
C1: Each word should be labeled with one and
only one label.
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
The same unique constraint also happens on the
word sense disambiguation, i.e.,
sense(w, s1) ? s1 6= s2 ? ?sense(w, s2)
C2: Some roles (A0?A5) appear only once fora predicate.
role(p, a1, r) ? uniqe(r) ? a1 6= a2 ? ?role(p, a2, r)
It is also easy to express the joint relation be-
tween word sense disambiguation and semantic
role labeling with Markov logic. What we need
to do is just adding some global formulas. The
relation between them can be shown in Figure 2.
Inspired by CoNLL 2008 (Surdeanu et al, 2008)
and 2009 (Hajic? et al, 2009) shared tasks, where
most of successful participant systems used pred-
icate senses for semantic role labeling, we also
model that the word sense disambiguation impli-
cates the semantic role labeling.
Here, we divide the all word sense disambigua-
tion task into two subtasks: predicate sense dis-
ambiguation and argument sense disambiguation.
The advantages of the division method approach
lie in two aspects. First, it makes us distinguish
the contributions of predicate and argument word
sense disambiguation respectively. Second, as
previous discussed, the predicate and argument
sense disambiguation can help each other. There-
fore, we can reflect the help with the division and
use Markov logic to represent it.
Figure 2: Global model between word sense dis-
ambiguation and semantic role labeling.
Finally, we use three global formulas to imple-
ment the three lines with direction in Figure 2.
They are:
sense(p,+s) ? role(p, a,+r)
sense(a,+s) ? role(p, a,+r)
sense(p,+s) ? sense(a,+s)
5 Experiments
5.1 Experimental Setting
In our experiments, we use the OntoNotes
Release 3.02 corpus, the latest version of
OntoNotes (Hovy et al, 2006). The OntoNotes
project leaders describe it as ?a large, multilingual
richly-annotated corpus constructed at 90% inter-
nanotator agreement.? The corpus has been an-
notated with multiple levels of annotation, includ-
ing constituency trees, predicate argument struc-
ture, word senses, co-reference, and named enti-
ties. For this work, we focus on the constituency
trees, word senses, and predicate argument struc-
tures. The corpus has English, Chinese, and Ara-
bic portions, and we just use the English portion,
which has been split into four sections: broad-
cast conversation (bc), broadcast news (bn), mag-
azine (mz), and newswire (nw). There are several
datasets in each section, such as cnn and voa.
We will do our experiments on all of the
OntoNotes 3.0 English datasets. For each dataset,
we aimed for roughly a 60% train / 20% develop-
ment / 20% test split. See Table 3 for the detailed
statistics. Here, we use the human annotated part-
of-speech and parse trees provided by OntoNotes.
The lemma of each word is extracted using Word-
Net tool3.
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2009T24
3http://wordnet.princeton.edu/
165
Training Developing Testing
bc
cctv 1,042 (0000-0003) 328 (0004-0004) 333 (0005-0005)
cnn 2,927 (0000-0004) 963 (0005-0006) 880 (0007-0008)
msnbc 2,472 (0000-0003) 1,209 (0004-0005) 1,315 (0006-0007)
phoenix 590 (0000-0001) 240 (0002-0002) 322 (0003-0003)
bn
abc 594 (0001-0040) 146 (0041-0054) 126 (0057-0069)
cnn 1,610 (0001-0234) 835 (0235-0329) 1,068 (0330-0437)
mnb 309 (0001-0015) 111 (0016-0020) 114 (0021-0025)
nbc 281 (0001-0023) 128 (0024-0031) 78 (0032-0039)
pri 1,104 (0001-0068) 399 (0069-0090) 366 (0091-0112)
voa 1,159 (0001-0159) 315 (0160-0212) 315 (0213-0265)
mz sinorama 5,051 (1001-1048) 1,262 (1049-1063) 1,456 (1064-1078)
nw wsj 8,138 (0020-1446) 2,549 (1447-1705) 3,133 (1730-2454)xinhua 2,285 (0001-0195) 724 (0196-0260) 670 (0261-0325)
All 27,562 9,209 10,176
Table 3: Training, developing and testing set sizes for the datasets in sentences. The file ranges (in
parenthesis) refer to the numbers within the names of the original OntoNotes 3.0 files. Here, we remove
4,873 sentences without semantic role labeling annotation.
Because we used semantic role labeling sys-
tem which is based on dependence syntactic trees,
we convert the constituency trees into dependence
trees with an Constituent-to-Dependency Conver-
sion Tool4.
The thebeast system is used in our experiment
as Markov logic engine. It uses cutting planes in-
ference technique (Riedel, 2008) with integer lin-
ear programming. The weights are learned with
MIRA (Crammer and Singer, 2003) online learn-
ing algorithm.
To our knowledge, this is the first word sense
disambiguation and semantic role labeling exper-
iment on OntoNotes 3.0 corpus. In order to com-
pare our joint model with previous work, we build
several systems:
Baseline: There are two independent baseline
systems: word sense disambiguation and seman-
tic role labeling. In each of baseline systems,
we only use the local formulas (Section 4.1) and
the global formulas which only express the global
constraints (Section 4.2).
Pipeline: In a pipeline system, we use ad-
ditional features outputted by preceded stages.
Such as in semantic role labeling pipeline sys-
tem, we use word sense as features, i.e., we set
sense(w, s) as an observable predicate and add
sense(p, s) ? role(p, a, r) and sense(a, s) ?
role(p, a, r) formulas into semantic role label-
ing task. As for word sense disambiguation
4http://nlp.cs.lth.se/software/treebank converter/
task, we add a set of formulas role(p, ai, r) ?
sense(p, s), where ai is the ith argument of
the predicate at position p, and a formula
role(p, a, r) ? sense(p, s) for the argument at
position a respectively.
Jointly: We use all global formulas mentioned
in Section 4.2. With Markov logic, we can add
global constraints and get the word sense disam-
biguation and the semantic role labeling results si-
multaneously.
5.2 Results and Discussion
The performance of these systems on test set is
shown in Table 4. All of the parameters are fine
tuned on the development set.
Here, we only consider the noun and verb word
sense disambiguation, which cover most of multi-
sense words. Therefore, the word sense disam-
biguation performance means the accuracy of all
nouns and verbs in the test set. The performance
of semantic role labeling is calculated using the
semantic evaluation metric of the CoNLL 2009
shared task scorer5. It measures the precision, re-
call, and F1 score of the recovered semantic de-
pendencies. The F1 score is used as the final per-
formance metric. A semantic dependency is cre-
ated for each predicate and its arguments. The la-
bel of such dependency is the role of the argument.
The same with the CoNLL 2009 shared task, we
assume that the predicates have been identified
5http://ufal.mff.cuni.cz/conll2009-st/eval09.pl
166
WSD SRL
Most Frequent Sense 85.58 ?
Baseline 89.37 83.97
PS 89.53 84.17
Pipeline AS 89.41 83.94
PS + AS ? 84.24
Jointly
PS? SRL 89.53 84.27
AS? SRL 89.49 84.16
PS? AS 89.45 ?
PS + AS? SRL 89.54 84.34
Fully 89.55 84.36
Table 4: The results of different systems. Here, PS
means predicate senses and AS means argument
senses.
correctly.
The first row of Table 4 gives the word sense
disambiguation result with the most frequent
sense, i.e., the #01 sense of each candidate word
which normally is the most frequent one in a bal-
ance corpus.
The second row shows the baseline perfor-
mances. Here, we note that the 89.37 word sense
disambiguation accuracy and the 83.97 semantic
role labeling F1 we obtained are comparable to
the state-of-the-art systems, such as the 89.1 word
sense disambiguation accuracy given by Zhong et
al. (2008) and 85.48 semantic role labeling perfor-
mance given by Che et al (2010) on OntoNotes
2.0 respectively, although the corpus used in our
experiments is upgraded version of theirs6. Ad-
ditionally, the performance of word sense dis-
ambiguation is higher than that of the most fre-
quent sense significantly (z-test7 with ? < 0.01).
Therefore, the experimental results show that the
Markov logic can achieve considerable perfor-
mances for word sense disambiguation and se-
mantic role labeling on the latest OntoNotes 3.0
corpus.
There are two kinds of pipeline systems: word
sense disambiguation (WSD) based on semantic
role labeling and semantic role labeling (SRL)
based on word sense disambiguation. For the us-
ing method of word senses, we first only exploit
predicate senses (PS) as mentioned by Surdeanu
et al (2008) and Hajic? et al (2009). Then, in or-
6Compared with OntoNotes 2.0, the version 3.0 incorpo-
rates more corpus.
7http://www.dimensionresearch.com/resources/
calculators/ztest.html
der to examine the contribution of word senses ex-
cept for predicates, we use argument senses (AS)
in isolation. Finally, all word senses (PS + AS)
were considered.
We can see that when the predicate senses (PS)
are used to label semantic role, the performance
of semantic role labeling can be improved from
83.97 to 84.17. The conclusion, that the predi-
cate sense can improve semantic role labeling per-
formance, is similar with CoNLL 2008 (Surdeanu
et al, 2008) and 2009 (Hajic? et al, 2009) shared
tasks. However, the improvement is not signifi-
cant (?2-test8 with ? < 0.1). Additionally, the
semantic role labeling can improve the predicate
sense disambiguation significantly from 89.37 to
89.53 (z-test with ? < 0.1). The same conclusion
was obtained by Dang and Palmer (2005).
However, when we only use argument senses
(AS), both of the word sense disambiguation and
semantic role labeling performances are almost
unchanged (from 89.37 to 89.41 and from 83.97
to 83.94 respectively). For the semantic role la-
beling task, the reason is that the original lemma
and part-of-speech features have been able to de-
scribe the argument related information. This kind
of sense features is just reduplicate. On the other
hand, the argument senses cannot be determined
only by the semantic roles. For example, the
semantic role ?A1? cannot predict the argument
sense of ?ball? exactly. The predicates must be
considered simultaneously.
Therefore, we use the last strategy (PS + AS),
which combines the predicate sense and the ar-
gument sense together to predict semantic roles.
The results show that the performance can be
improved significantly (?2-test with ? < 0.05)
from 83.97 to 84.24. Accordingly, the experi-
ment proves that automatic all word sense disam-
biguation can further improve the semantic role
labeling performance. Different from Che et al
(2010), where the semantic role labeling can be
improved with correct word senses about F1 = 1,
our improvement is much lower. The main reason
is that the performance of our word sense disam-
biguation with the most basic features is not high
enough. Another limitation of the pipeline strat-
8http://graphpad.com/quickcalcs/chisquared1.cfm
167
egy is that it is difficult to predict the combination
between predicate and argument senses. This is
an obvious shortcoming of the pipeline method.
With Markov logic, we can easily join different
tasks with global formulas. As shown in Table 4,
we use five joint strategies:
1. PS ? SRL: means that we jointly disam-
biguate predicate senses and label semantic roles.
Compared with the pipeline PS system, word
sense disambiguation performance is unchanged.
However, the semantic role labeling performance
is improved from 84.17 to 84.27. Compared with
the baseline?s 83.97, the improvement is signifi-
cant (?2-test with ? < 0.05).
2. AS ? SRL: means that we jointly disam-
biguate argument senses and label semantic roles.
Compared with the pipeline AS system, both of
word sense disambiguation and semantic role la-
beling performances are improved (from 89.41 to
89.49 and from 83.94 to 84.16 respectively). Al-
though, the improvement is not significant, it is
observed that the joint model has the capacity to
improve the performance, especially for semantic
role labeling, if we could have a more accurate
word sense disambiguation.
3. PS ? AS: means that we jointly dis-
ambiguate predicate word senses and argument
senses. This kind of joint model does not influ-
ence the performance of semantic role labeling.
The word sense disambiguation outperforms the
baseline system from 89.37 to 89.45. The result
verifies our assumption that the predicate and ar-
gument senses can help each other.
4. PS + AS ? SRL: means that we jointly
disambiguate all word senses and label semantic
roles. Compared with the pipeline method which
uses the PS + AS strategy, the joint method can
further improve the semantic role labeling (from
84.24 to 84.34). Additionally, it can obtain the
predicate and argument senses together. The all
word sense disambiguation performance (89.54)
is higher than the baseline (89.37) significantly (z-
test with ? < 0.1).
5. Fully: finally, we use all of the three global
formulas together, i.e., we jointly disambiguate
predicate senses, argument senses, and label se-
mantic roles. It fully joins all of the tasks. Both of
all word sense disambiguation and semantic role
labeling performances can be further improved.
Although the improvements are not significant
compared with the best pipeline system, they sig-
nificantly (z-test with ? < 0.1 and ?2-test with
? < 0.01 respectively) outperform the baseline
system. Additionally, the performance of the fully
joint system does not outperform partly joint sys-
tems significantly. The reason seems to be that
there is some overlap among the contributions of
the three joint systems.
6 Conclusion
In this paper, we presented a Markov logic model
that jointly models all word sense disambiguation
and semantic role labeling. We got the following
conclusions:
1. The baseline systems with Markov logic is
competitive to the state-of-the-art word sense dis-
ambiguation and semantic role labeling systems
on OntoNotes 3.0 corpus.
2. The predicate sense disambiguation is ben-
eficial to semantic role labeling. However, the
automatic argument sense disambiguation itself is
harmful to the task. It must be combined with the
predicate sense disambiguation.
3. The semantic role labeling not only can help
predicate sense disambiguation, but also argument
sense disambiguation (a little). In contrast, be-
cause of the limitation of the pipeline model, it
is difficult to make semantic role labeling to help
predicate and argument sense disambiguation si-
multaneously.
4. It is easy to implement the joint model of
all word sense disambiguation and semantic role
labeling with Markov logic. More important, the
joint model can further improve the performance
of the all word sense disambiguation and semantic
role labeling than pipeline systems.
Acknowledgement
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
168
References
Che, Wanxiang, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing.
In Proceedings of CoNLL 2009: Shared Task, pages
49?54, June.
Che, Wanxiang, Ting Liu, and Yongqiang Li. 2010.
Improving semantic role labeling with word sense.
In NAACL 2010, pages 246?249, June.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Dahlmeier, Daniel, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and se-
mantic roles of prepositional phrases. In Proceed-
ings of EMNLP 2009, pages 450?458, August.
Dang, Hoa Trang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL 2005, pages 42?49, Morris-
town, NJ, USA.
Domingos, Pedro and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and
Machine Learning. Morgan & Claypool Publishers.
Finkel, Jenny Rose and Christopher D. Manning.
2009. Joint parsing and named entity recognition.
In Proceedings of NAACL 2009, pages 326?334,
June.
Goldberg, Yoav and Reut Tsarfaty. 2008. A single
generative model for joint morphological segmenta-
tion and syntactic parsing. In Proceedings of ACL
2008, pages 371?379, June.
Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of CoNLL
2009: Shared Task, pages 1?18, June.
Hovy, Eduard, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
Ontonotes: The 90% solution. In Proceedings of
NAACL 2006, pages 57?60, June.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL 2008, pages 897?904, June.
Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of ACL-IJCNLP 2009,
pages 513?521, August.
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Proceedings of NAACL 2009,
pages 155?163, June.
Poon, Hoifung and Pedro Domingos. 2008. Joint
unsupervised coreference resolution with Markov
Logic. In Proceedings of EMNLP 2008, pages 650?
659, October.
Punyakanok, Vasin, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Riedel, Sebastian. 2008. Improving the accuracy and
efficiency of map inference for markov logic. In
Proceedings of UAI 2008, pages 468?475. AUAI
Press.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of CoNLL
2008, pages 159?177, August.
Titov, Ivan and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL 2008, pages 308?316,
June.
Toutanova, Kristina and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech
prediction. In Proceedings of ACL-IJCNLP 2009,
pages 486?494, August.
Toutanova, Kristina, Aria Haghighi, and Christo-
pher D. Manning. 2008. A global joint model for
semantic role labeling. Computational Linguistics,
34(2).
Yoshikawa, Katsumasa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly iden-
tifying temporal relations with markov logic. In
Proceedings of ACL-IJCNLP 2009, pages 405?413,
August.
Zhang, Yue and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL 2008, pages 888?896,
June.
Zhong, Zhi, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of EMNLP 2008,
pages 1002?1010, October.
169
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1317?1325,
Beijing, August 2010
Paraphrasing with Search Engine Query Logs
Shiqi Zhao??, Haifeng Wang?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com, tliu@ir.hit.edu.cn
Abstract
This paper proposes a method that extracts
paraphrases from search engine query
logs. The method first extracts paraphrase
query-title pairs based on an assumption
that a search query and its correspond-
ing clicked document titles may mean the
same thing. It then extracts paraphrase
query-query and title-title pairs from the
query-title paraphrases with a pivot ap-
proach. Paraphrases extracted in each step
are validated with a binary classifier. We
evaluate the method using a query log
from Baidu1, a Chinese search engine.
Experimental results show that the pro-
posed method is effective, which extracts
more than 3.5 million pairs of paraphrases
with a precision of over 70%. The results
also show that the extracted paraphrases
can be used to generate high-quality para-
phrase patterns.
1 Introduction
The use of paraphrases is ubiquitous in hu-
man languages, which also presents a challenge
for natural language processing (NLP). Previous
studies have shown that paraphrasing can play im-
portant roles in plenty of areas, such as machine
translation (MT) (Callison-Burch et al, 2006;
Kauchak and Barzilay, 2006), question answer-
ing (QA) (Duboue and Chu-Carroll, 2006; Riezler
et al, 2007), natural language generation (NLG)
(Iordanskaja et al, 1991), and so on. As a result,
the research on paraphrasing and its applications
have attracted significant interest.
1www.baidu.com
This paper proposes a method that uses search
engine query logs for extracting paraphrases,
which is illustrated in Figure 1. Specifically, three
kinds of paraphrases can be extracted with our
method, which include (1) query-title (Q-T): a
query and a document title that users clicked on;
(2) query-query (Q-Q): two queries, for which
users clicked on the same document title; (3) title-
title (T-T): two titles that users clicked on for the
same query. We train a classifier for each kind to
filter incorrect pairs and refine the paraphrases.
Extracting paraphrases using query logs has
many advantages. First, query logs keep growing,
which have no scale limitation. Second, query
logs reflect web users? real needs, hence the ex-
tracted paraphrases may be more useful than that
from other kinds of corpora. Third, paraphrases
extracted from query logs can be directed applied
in search engines for query suggestion and doc-
ument reranking. In addition, we find that both
queries and titles contain a good many question
sentences, which can be useful in developing QA
systems.
We conduct experiments using a query log of
a commercial Chinese search engine Baidu, from
which we extracted about 2.7 million pairs of
paraphrase Q-T, 0.4 million pairs of paraphrase Q-
Q, and 0.4 million pairs of paraphrase T-T. The
precision of the paraphrases is above 70%. In
addition, we generate paraphrase patterns using
the extracted paraphrases. The results show that
73,484 pairs of paraphrase patterns have been gen-
erated, with a precision of over 78%.
In the rest of the paper, we first review related
work in Section 2. Section 3 describes our method
in detail. Section 4 presents the evaluation and re-
1317
paraphrase Q-T extraction
query title both query and title
paraphrase Q-Q extraction paraphrase T-T extraction
paraphrase relation
Figure 1: Illustration of the proposed method.
sults. Section 5 concludes the paper and discusses
future directions.
2 Related Work
In this section, we briefly review previous studies
on paraphrase extraction and query log mining in
information retrieval (IR).
2.1 Paraphrase Extraction
A variety of data resources have been exploited
for paraphrase extraction. For example, some re-
searchers extract paraphrases from multiple trans-
lations of the same foreign novel (Barzilay and
McKeown, 2001; Ibrahim et al, 2003), while
some others make use of comparable news arti-
cles that report on the same event within a small
time interval (Shinyama et al, 2002; Barzilay and
Lee, 2003; Dolan et al, 2004). Besides the mono-
lingual corpora, bilingual parallel corpora have
also been used for extracting paraphrases (Ban-
nard and Callison-Burch, 2005; Callison-Burch,
2008; Zhao et al, 2008). Their basic assumption
is that phrases that align with the same foreign
phrase may have the same meaning.
The above methods have achieved promising
results. However, their performances are usually
constrained due to the scale and domain limita-
tion. As an alternative, researchers have tried
to acquire paraphrases from large-scale web cor-
pora (Lin and Pantel, 2001; Pas?ca and Dienes,
2005; Bhagat and Ravichandran, 2008) or directly
based on web mining (Ravichandran and Hovy,
2002). These methods are guided by an extended
version of distributional hypothesis, namely, if
two phrases often occur in similar contexts, their
meanings tend to be similar. The disadvantage
of these methods is that the underlying assump-
tion does not always hold. Phrases with opposite
meanings can also occur in similar contexts, such
as ?X solves Y? and ?X worsens Y? (Lin and Pan-
tel, 2001). In addition, the extracted paraphrases
are generally short fragments with two slots (vari-
ables) at both ends.
2.2 Query Log Mining in IR
Query logs are widely used in the IR commu-
nity, especially for mining similar queries. For ex-
ample, Wen et al (2002) clustered queries based
on user click information. Their basic idea is
that if some queries result in similar user clicks,
the meanings of these queries should be similar.
Such methods have also been investigated in (Gao
et al, 2007) for cross-lingual query suggestion
and (Zhao et al, 2007) for synonymous questions
identification. This paper is partly inspired by
their studies. However, we do not simply use click
information as clues for mining similar queries.
Instead, we mine paraphrases across queries and
clicked document titles.
In addition, query logs can be used for query
expansion. For instance, Cui et al (2002)
extract probabilistic correlations between query
terms and document terms by analyzing query
logs, which are then used to select high-quality
1318
H1: If a query q hits a title t, then q and
t are likely to be paraphrases.
H2: If queries q1 and q2 hit the same title t,
q1 and q2 are likely to be paraphrases.
H3: If a query q hits titles t1 and t2, then
t1 and t2 are likely to be paraphrases.
Table 1: Hypotheses for extracting paraphrases.
expansion terms for new queries. Note that the
expansion terms are merely related terms of the
queries, not necessarily paraphrases.
There are other studies that use query logs
for constructing ontologies (Sekine and Suzuki,
2007), learning named entities (Pas?ca, 2007),
building user profiles (Richardson, 2008), correct-
ing spelling errors (Ahmad and Kondrak, 2005),
and so forth.
3 The Proposed Method
3.1 Basic Idea
Nowadays, more and more users tend to search
long queries with search engines. Many users
even directly search questions to get exact an-
swers. By analyzing our query log that records
rich information including user queries, clicked
urls, titles, etc., we find that most titles of clicked
documents are highly related with search queries.
Especially, paraphrases can be easily found from
long queries and the corresponding clicked ti-
tles. This motivates us to extract paraphrases from
query-title pairs. Here we introduce a concept hit
that will be frequently used: given a query q, a
web document d, and d?s title t, if there exist some
users that click on d when searching q, then we
say q hits t.
The hypothesis for extracting paraphrase Q-T
is shown in Table 1 (H1). In addition, we find
that when several queries hit the same title, the
queries are likely to be paraphrases of each other.
The other way round, when a query hits several
titles, paraphrases can also be found among the ti-
tles. We therefore further extract paraphrase Q-Q
and T-T from the paraphrase Q-T. The underly-
ing hypotheses can be found in Table 1 (H2 and
INPUT: Q: query space, T : title space
OUTPUT: Pqt: the set of paraphrase Q-T,
Pqq: the set of paraphrase Q-Q,
Ptt: the set of paraphrase T-T,
ParaSet: the set of paraphrases
1. FOR any q ? Q and t ? T
2. IF q hits t
3. IF IsParaphrase(q, t)
4. Add ?q, t? to Pqt
5. END IF
6. END IF
7. END FOR
8. FOR any q1, q2 ? Q and t ? T
9. IF ?q1, t? ? Pqt and ?q2, t? ? Pqt
10. IF IsParaphrase(q1, q2)
11. Add ?q1, q2? to Pqq
12. END IF
13. END IF
14. END FOR
15. FOR any t1, t2 ? T and q ? Q
16. IF ?q, t1? ? Pqt and ?q, t2? ? Pqt
17. IF IsParaphrase(t1, t2)
18. Add ?t1, t2? to Ptt
19. END IF
20. END IF
21. END FOR
22. RETURN ParaSet = Pqt ? Pqq ? Ptt
Table 2: Algorithm for extracting paraphrases.
H3). Note that, based on H2 and H3, paraphrase
Q-Q and T-T can be directly extracted from raw
Q-T pairs. However, in consideration of preci-
sion, we extract them from paraphrase Q-T. We
call our paraphrase Q-Q and T-T extraction ap-
proach as a pivot approach, since we use titles as
pivots (queries as targets) when extracting para-
phrase Q-Q and use queries as pivots (titles as tar-
gets) when extracting paraphrase T-T.
3.2 Algorithm
Our paraphrase extraction algorithm is shown in
Table 2. In particular, lines 1?7 extract para-
1319
phrase Q-T from the query log. Lines 8?14 and
15?21 extract paraphrase Q-Q and T-T, respec-
tively. Line 22 combines the paraphrase Q-T, Q-
Q, and T-T together. To filter noise, the extracted
Q-T, Q-Q, and T-T pairs are all validated using
a function IsParaphrase(s1, s2). In this work,
we recast paraphrase validation as a binary clas-
sification problem. Any pair of ?s1, s2? is classi-
fied as 1 (paraphrase) or 0 (non-paraphrase) with
a support vector machine (SVM) classifier. The
features used for classification will be detailed in
Section 3.3.
In practice, we exploit a query log that contains
287 million Q-T pairs, which are then filtered us-
ing the following constraints: (1) exclude Q-T
pairs that are too short, i.e., either query q or tittle
t contains less than three terms; (2) exclude Q-T
pairs where q subsumes t or vice versa, e.g., ??
? (beef)? and ?????? (cooking method of
beef)?; (3) exclude Q-T pairs in which the similar-
ity between q and t is below a predefined threshold
T 2; (4) exclude Q-T pairs whose t contains fre-
quent internet terms, such as ??? (home page)?,
??? (web site)?, ??? (online)?, since such ti-
tles are mostly organization home pages, online
videos, downloadable resources, etc., which are
useless for our purpose of paraphrase extraction.
3.3 Features for Paraphrase Validation
Given a pair of candidate paraphrases ?s1, s2?, in
which s1 and s2 can be either a query or a title, we
exploit the following features in the classification-
based paraphrase validation.
? Frequency Feature FF . FF is defined based
on each ?s1, s2??s frequency. We expect that more
frequent ?s1, s2? should be more reliable.
FF (s1, s2) = {
c(s1,s2)
C if c(s1, s2) < C
1 if c(s1, s2) ? C
(1)
where c(s1, s2) denotes the number of times that
the ?s1, s2? pair occurs in the corpus. C is a nor-
malizing factor (C = 10 in our experiments).
2The similarity is computed based on word overlap rate,
which will be described in detail in section 3.3. We set T =
0.6 in the experiments.
? Length Rate Feature FLR:
FLR(s1, s2) =
min{cw(s1), cw(s2)}
max{cw(s1), cw(s2)}
(2)
where cw(s) denotes the number of words in s.
? Word Overlap Rate Feature FWOR:
FWOR(s1, s2) =
cw(s1 ? s2)
max{cw(s1), cw(s2)}
(3)
where ?s1 ? s2? is the intersection of s1 and s2.
? Character Overlap Rate Feature FCOR. Chi-
nese words are composed of characters. It is quite
often that words with similar characters share
similar meanings, such as ??? (comfortable)?
and ??? (comfortable)?, ??? (sell)? and ??
? (sell)?. Here we use FCOR to measure the sim-
ilarity between s1 and s2 at the character level.
Detailedly, we segment s1 and s2 into sets of
characters and compute the overlap rate based on
Equation (3)3.
? Cosine Similarity Feature FCS . In FCS , both
s1 and s2 are represented as vectors and their co-
sine similarity is computed as:
FCS(s1, s2) =
vecw(s1) ? vecw(s2)
?vecw(s1)? ? ?vecw(s2)?
(4)
where vecw(s) is the vector of words in s, ??? de-
notes the dot product of two vectors, ?vecw(s)?
is the norm of a vector. Here, the weight of each
word w in a vector is computed using a heuristic
similar to tf-idf:
W (w) = tf(w)? log( Nc(w) + 0.1) (5)
where tf(w) is the frequency of w in the given s,
c(w) is the number of times that w occurs in the
corpus, N = maxw c(w).
? Edit Distance Feature FED. Let ED(s1, s2)
be the edit distance at the word level between s1
and s2, we compute FED as follows:
FED(s1, s2) = 1?
ED(s1, s2)
max{cw(s1), cw(s2)}
(6)
3In FCOR, cw(s) of Equation (3) denotes the number of
characters in s.
1320
? Named Entity (NE) Similarity Feature FNE .
NE information is critical in paraphrase identifica-
tion (Shinyama et al, 2002). We therefore com-
pute the NE similarity between s1 and s2 and take
it as a feature. We employ a Chinese NE recog-
nition tool that can recognize person names, loca-
tions, organizations, and numerals. The NE simi-
larity is computed as:
FNE(s1, s2) =
cne(s1 ? s2) + 1
max{cne(s1), cne(s2)}+ 1
(7)
where cne(s) denotes the number of NEs in s.
Equation (7) guarantees FNE = 1 if there are no
NEs in either s1 or s2.
? Pivot Fertility Feature FPF : FPF is a fea-
ture specially designed for paraphrase Q-Q and
T-T extraction, which are based on the pivot ap-
proach4. Specifically, we define fertility of a pivot
as the number of targets it corresponds to. Our ob-
servation indicates that the larger the fertility of a
pivot is, the more noisy the targets are. Hence we
define FPF as:
FPF (s1, s2) = maxp
1
f(p) (8)
where s1 = q1, s2 = q2, p = t when classifying
Q-Q, while s1 = t1, s2 = t2, p = q when classi-
fying T-T. f(p) denotes the fertility of the pivot p.
The value is maximized over p if s1 and s2 can be
extracted with multiple pivots.
3.4 Generating Paraphrase Patterns
A key feature of our method is that the extracted
paraphrases are particularly suitable for generat-
ing paraphrase patterns, especially for the hot do-
mains that are frequently searched. For example,
there are quite a few paraphrases concerning the
therapy of various diseases, from which we can
easily induce patterns expressing the meaning of
?How to treat [X] disease?, such as ?[X] ? ?
? ???, ??? ?? [X] ??, and ?[X] ? ?
?? ???. Therefore, in this work, we try to
generate paraphrase patterns using the extracted
paraphrases.
In our preliminary experiments, we only induce
paraphrase patterns from paraphrases that contain
4FPF is not used in paraphrase Q-T validation.
SAME RELA DIFF
percent (%) 55.92 44.08 -
Table 3: Human labeling of candidate Q-T.
no more than 6 words. In addition, only one slot
is allowed in each pair of paraphrase patterns. Let
s1 and s2 be a pair of paraphrases extracted above.
If there exist words w ? s1 and v ? s2 that satisfy
(1) w = v, (2) w and v are not stop words, then
we can induce a pair of paraphrase patterns by re-
placing w in s1 and v in s2 with a slot ?[X]?. It is
obvious that several pairs of paraphrase patterns
may be induced from one pair of paraphrases.
4 Experiments
We experiment with a query log that contains a
total of 284,316,659 queries. Statistics reveal that
170,315,807 queries (59.90%) lead to at least one
user click, each having 1.69 clicks on average. We
extract 287,129,850 raw Q-T pairs using the query
log, from which 4,448,347 pairs of candidate Q-
T are left after filtering as described in Section
3.2. Almost all queries and titles are written in
Chinese, though some of them contain English or
Japanese words. The preprocessing of candidate
Q-T includes Chinese word segmentation (WSeg)
and NE recognition (NER). Our WSeg tool is im-
plemented based on forward maximum matching,
while the NER tool is based on a NE dictionary
mined from the web.
4.1 Evaluation of Candidate Q-T
We first evaluate candidate Q-T without valida-
tion. To this end, we randomly sampled 5000
pairs of candidate Q-T and labeled them manu-
ally. Each pair is labeled into one of the 3 classes:
SAME - q and t have the same meaning; RELA - q
and t have related meanings; DIFF - q and t have
clearly different meanings. The labeling results
are listed in Table 3. We can see that no candidate
Q-T is in the DIFF class. This is not surprising,
since users are unlikely to click on web pages un-
related to their queries.
To gain a better insight into the data, we ana-
lyzed the subtle types of candidate Q-T in both
SAME and RELA classes. In detail, we sampled
1321
1000 pairs of candidate Q-T from the 5000 pairs
labeled above, in which 563 are in the SAME
class, while the other 437 are in the RELA class.
Our analysis suggests that candidate Q-T in the
SAME class can be divided into 4 subtle types:
? Trivial change (12.61%): changes of punctu-
ation or stop words, such as ??? ?? ?
??? and ??????????.
? Word or phrase replacement (68.38%): re-
placements of synonymous words or phrases,
such as ??? ? ? ?? ?? ? (how
mach is ...)? and ??? ? ? ?? ??
??? (what is the price of ...)?.
? Structure change (7.10%): changes of both
words and word orders, such as ?????
? ?? ? ?? (what fruit can I eat on a
diet)? and ?? ?? ?? ?? ?? (what
fruit can help loss weight)?.
? Others (11.90%): candidate Q-T that cannot
be classified into the 3 types above.
The above analysis reveals that more than two
thirds of candidate Q-T in the SAME class are in
the ?word or phrase replacement? type, while the
ones with structure changes are slightly more than
7%. We believe this is mainly because queries
and titles are relatively short and their structures
are simple. Thus structure rewriting can hardly be
conducted. This distribution is in line with that
reported in (Zhao et al, 2008).
As for the RELA class, we find that 42.33% of
such candidate Q-T share a problem of named en-
tity mismatch, such as ??? (US) ?? ??
??? and ??? (China) ?? ?? ?? ?
??. This indicates that the NE similarity feature
is necessary in paraphrase validation.
4.2 Evaluation of Paraphrase Q-T
The candidate Q-T extracted above are classified
with a SVM classifier5 under its default setting.
To evaluate the classifier, we run 5-fold cross val-
idation with the 5000 human annotated data, in
which we use 4000 for training and the rest 1000
for testing in each run. The evaluation criteria are
5We use libsvm-2.82 toolkit, which can be downloaded
from http://www.csie.ntu.edu.tw/ cjlin/libsvm/
precision (P), recall (R), and f-measure (F), which
are defined as follows:
P = ?Sa ? Sm??Sa?
(9)
R = ?Sa ? Sm??Sm?
(10)
F = 2? P ?RP +R (11)
where Sa is the set of paraphrases automatically
recognized with the classifier, Sm is the set of
paraphrases manually annotated. Precision, re-
call, and f-measure are averaged over 5 runs in
the 5-fold cross validation.
Figure 2 (a) shows the classification results
(dark bars). For comparison, we also show the
precision, recall6, and f-measure of the candidate
Q-T (light bars). As can be seen, the precision is
improved from 0.5592 to 0.7444 after classifica-
tion. F-measure is also evidently enhanced. This
result indicates that the classification-based para-
phrase validation is effective. We then use all of
the 5000 annotated data to train a classifier and
classify all the candidate Q-T. Results show that
2,762,291 out of 4,448,347 pairs of candidate Q-
T are classified as paraphrases.
4.3 Evaluation of Paraphrase Q-Q and T-T
From the paraphrase Q-T, we further extracted
934,758 pairs of candidate Q-Q and 438,954 pairs
of candidate T-T (without validation). We ran-
domly sampled 5000 from each for human an-
notation. The results show that the precisions of
candidate Q-Q and T-T are 0.4672 and 0.6860, re-
spectively. As can be seen, the precision of can-
didate Q-Q is much lower than that of candidate
T-T. Our analysis reveals that it is mainly because
candidate Q-Q are more noisy, since user queries
contain quite a lot of spelling mistakes and infor-
mal expressions.
The candidate Q-Q and T-T are also refined
based on classification. We first evaluate the clas-
sification performance using the 5000 human la-
beled data. The experimental setups for Q-Q and
6We assume all possible paraphrases are included in the
candidates, thus its recall is 100%.
1322
(a) Q-T classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.5592 1 0.7173
para. 0.7444 0.8391 0.7887
P R F
(b) Q-Q classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.4672 1 0.6369
para. 0.7345 0.6575 0.6938
P R F
(c) T-T classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.686 1 0.8138
para. 0.7056 0.9776 0.8196
P R F
Figure 2: Classification precision (P), recall (R), and f-measure (F).
T-T classification are the same as that of Q-T clas-
sification, in which we run 5-fold cross validation
with a SVM classifier using its default parameters.
Figure 2 (b) and (c) give the classification results
(dark bars) as well as the precision, recall, and f-
measure of the candidates (light bars).
We can see that the precision of Q-Q is signifi-
cantly enhanced from 0.4672 to 0.7345 after clas-
sification, which suggests that a substantial part
of errors and noise are removed. The increase of
f-measure demonstrates the effectiveness of clas-
sification despite the decrease of recall. Mean-
while, the quality of candidate T-T is not clearly
improved after classification. The reason should
be that the precision of candidate T-T is already
pretty high. We then use all 5000 human labeled
data to train a classifier for Q-Q and T-T respec-
tively and classify all candidate Q-Q and T-T. Re-
sults show that 390,920 pairs of paraphrase Q-Q
and 415,539 pairs of paraphrase T-T are extracted
after classification.
4.4 Evaluation of Paraphrase Patterns
Using the method introduced in Section 3.4, we
have generated 73,484 pairs of paraphrase pat-
terns that appear at least two times in the cor-
pus. We randomly selected 500 pairs and labeled
them manually. The results show that the preci-
sion is 78.4%. Two examples are shown in Ta-
ble 4, in which p1 and p2 are paraphrase patterns.
Some slot fillers are also listed below. We real-
p1 [X]??????
p2 ???? [X]??
(how to open [X] file)
slot 7z; ashx; aspx; bib; cda; cdfs; cmp;
cpi; csf; csv; cur; dat; dek...
p1 ?? [X]???
p2 ?? [X]???
(poems about [X])
slot ?? (prairies);?? (Yangtze River);
?? (Mount Tai);?? (nostalgia)...
Table 4: Examples of paraphrase patterns.
ize that the method currently used for inducing
paraphrase patterns is simple. Hence we will im-
prove the method in our following experiments.
Specifically, multiple slots will be allowed in a
pair of patterns. In addition, we will try to ap-
ply the alignment techniques in the generation of
paraphrase patterns, as Zhao et al (2008) did.
4.5 Analysis
Feature Contribution. To investigate the contri-
butions of different features used in classification,
we tried different feature combinations for each of
our three classifiers. The results are shown in Ta-
ble 5, in which ?+? means the feature has contri-
bution to the corresponding classifier. As can be
seen, the character overlap rate feature (FCOR),
cosine similarity feature (FCS), and NE similarity
1323
Feature Q-T Q-Q T-T
FF +
FLR +
FWOR
FCOR + + +
FCS + + +
FED +
FNE + + +
FPF +
Table 5: Feature contribution.
feature (FNE) are the most useful, which play im-
portant roles in all the three classifiers. The other
features are useful in some of the classifiers ex-
cept the word overlap rate feature (FWOR). The
classification results reported in prior sections are
all achieved with the optimal feature combination.
Analysis of the Paraphrases. We combine the
extracted paraphrase Q-T, Q-Q and T-T and get
a total of 3,560,257 pairs of unique paraphrases.
Statistics show that only 8380 pairs (0.24%) are
from more than one source, which indicates that
the intersection among the three sets is very small.
Further statistics show that the average length of
the queries and titles in the paraphrases is 6.69
(words).
To have a detailed analysis of the extracted
paraphrases, we randomly selected 1000 pairs and
manually labeled the precision, types, and do-
mains. It is found that more than 43% of the para-
phrases are paraphrase questions, in which how
(36%), what (19%), and yes/no (14%) questions
are the most common. In addition, we find that
the precision of paraphrase questions (84.26%)
is evidently higher than non-question paraphrases
(65.14%). Those paraphrase questions are useful
in question analysis and expansion in QA, which
can hardly be extracted from other kinds of cor-
pora.
As expected, the paraphrases we extract cover
a variety of domains. However, around 50% of
them are in the 7 most popular domains7, includ-
ing: (1) health and medicine, (2) documentary
download, (3) entertainment, (4) software, (5) ed-
7Note that pornographic queries have been filtered from
the query log beforehand.
ucation and study, (6) computer game, (7) econ-
omy and finance. This analysis reflects what web
users are most concerned about. These domains,
especially (4) and (6), are not well covered by the
parallel and comparable corpora previously used
for paraphrase extraction.
5 Conclusions and Future Directions
In this paper, we put forward a novel method that
extracts paraphrases from search engine query
logs. Our contribution is that we, for the first
time, propose to extract paraphrases from user
queries and the corresponding clicked document
titles. Specifically, three kinds of paraphrases
are extracted, which can be (1) a query and a
hit title, (2) two queries that hit the same title,
and (3) two titles hit by the same query. The
extracted paraphrases are refined based on clas-
sification. Using the proposed method, we ex-
tracted over 3.5 million pairs of paraphrases from
a query log of Baidu. Human evaluation results
show that the precision of the paraphrases is above
70%. The results also show that we can gener-
ate high-quality paraphrase patterns from the ex-
tracted paraphrases.
Our future research will be conducted along the
following directions. Firstly, we will use a much
larger query log for paraphrase extraction, so as to
enhance the coverage of paraphrases. Secondly,
we plan to have a deeper study of the transitivity
of paraphrasing. Simply speaking, we want to find
out whether we can extract ?s1, s3? as paraphrases
given that ?s1, s2? and ?s2, s3? are paraphrases.
6 Acknowledgments
We would like to thank Wanxiang Che, Hua Wu,
and the anonymous reviewers for their useful
comments on this paper.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a Spelling Error Model from Search Query
Logs. In Proceedings of HLT/EMNLP, pages 955-
962.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
1324
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of ACL/EACL, pages 50-57.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Sur-
face Patterns. In Proceedings of ACL-08: HLT,
pages 674-682.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, pages 196-205.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma.
2002. Probabilistic Query Expansion Using Query
Logs In Proceedings of WWW, pages 325-332.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News
Sources. In Proceedings of COLING, pages 350-
356.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian
Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007.
Cross-Lingual Query Suggestion Using Query Logs
of Different Languages. In Proceedings of SIGIR,
pages 463-470.
Ali Ibrahim, Boris Katz, Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natu-
ral Language Engineering 7(4): 343-360.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across
the Web. In Proceedings of IJCNLP, pages 119-
130.
Marius Pas?ca. 2007. Weakly-supervised Discovery
of Named Entities using Web Search Queries. In
Proceedings of CIKM, pages 683-690.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Matthew Richardson. 2008. Learning about the World
through Long-Term Query Logs. In ACM Transac-
tions on the Web 2(4): 1-27.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring
Ontological Knowledge from Query Logs. In Pro-
ceedings of WWW, pages 1223-1224.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from
News Articles. In Proceedings of HLT, pages 40-
46.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query Clustering Using User Logs. In ACM
Transactions on Information Systems 20(1): 59-81,
2002.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
Question Paraphrases for QA from Encarta Logs. In
Proceedings of IJCAI, pages 1795-1800.
1325
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326?1334,
Beijing, August 2010
Leveraging Multiple MT Engines for Paraphrase Generation
Shiqi Zhao??, Haifeng Wang?, Xiang Lan?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com,
{xlan, tliu}@ir.hit.edu.cn
Abstract
This paper proposes a method that lever-
ages multiple machine translation (MT)
engines for paraphrase generation (PG).
The method includes two stages. Firstly,
we use a multi-pivot approach to acquire
a set of candidate paraphrases for a source
sentence S. Then, we employ two kinds
of techniques, namely the selection-based
technique and the decoding-based tech-
nique, to produce a best paraphrase T for
S using the candidates acquired in the first
stage. Experimental results show that:
(1) The multi-pivot approach is effective
for obtaining plenty of valuable candi-
date paraphrases. (2) Both the selection-
based and decoding-based techniques can
make good use of the candidates and pro-
duce high-quality paraphrases. Moreover,
these two techniques are complementary.
(3) The proposed method outperforms a
state-of-the-art paraphrase generation ap-
proach.
1 Introduction
This paper addresses the problem of paraphrase
generation (PG), which seeks to generate para-
phrases for sentences. PG is important in many
natural language processing (NLP) applications.
For example, in machine translation (MT), a
sentence can be paraphrased so as to make it
more translatable (Zhang and Yamamoto, 2002;
Callison-Burch et al, 2006). In question answer-
ing (QA), a question can be paraphrased to im-
prove the coverage of answer extraction (Duboue
and Chu-Carroll, 2006; Riezler et al, 2007). In
natural language generation (NLG), paraphrasing
can help to increase the expressive power of the
NLG systems (Iordanskaja et al, 1991).
In this paper, we propose a novel PG method.
For an English sentence S, the method first ac-
quires a set of candidate paraphrases with a multi-
pivot approach, which uses MT engines to auto-
matically translate S into multiple pivot languages
and then translate them back into English. Fur-
thermore, the method employs two kinds of tech-
niques to produce a best paraphrase T for S us-
ing the candidates, i.e., the selection-based and
decoding-based techniques. The former selects
a best paraphrase from the candidates based on
Minimum Bayes Risk (MBR), while the latter
trains a MT model using the candidates and gen-
erates paraphrases with a MT decoder.
We evaluate our method on a set of 1182 En-
glish sentences. The results show that: (1) al-
though the candidate paraphrases acquired by MT
engines are noisy, they provide good raw ma-
terials for further paraphrase generation; (2) the
selection-based technique is effective, which re-
sults in the best performance; (3) the decoding-
based technique is promising, which can generate
paraphrases that are different from the candidates;
(4) both the selection-based and decoding-based
techniques outperform a state-of-the-art approach
SPG (Zhao et al, 2009).
2 Related Work
2.1 Methods for Paraphrase Generation
MT-based method is the mainstream method on
PG. It regards PG as a monolingual machine trans-
lation problem, i.e., ?translating? a sentence S
into another sentence T in the same language.
1326
Quirk et al (2004) first presented the MT-based
method. They trained a statistical MT (SMT)
model on a monolingual parallel corpus extracted
from comparable news articles and applied the
model to generate paraphrases. Their work shows
that SMT techniques can be extended to PG. How-
ever, its usefulness is limited by the scarcity of
monolingual parallel data.
To overcome the data sparseness problem, Zhao
et al (2008a) improved the MT-based PG method
by training the paraphrase model using multi-
ple resources, including monolingual parallel cor-
pora, monolingual comparable corpora, bilingual
parallel corpora, etc. Their results show that bilin-
gual parallel corpora are the most useful among
the exploited resources. Zhao et al (2009) further
improved the method by introducing a usability
sub-model into the paraphrase model so as to gen-
erate varied paraphrases for different applications.
The main disadvantage of the MT-based
method is that its performance heavily depends on
the fine-grained paraphrases, such as paraphrase
phrases and patterns, which provide paraphrase
options in decoding. Hence one has to first ex-
tract fine-grained paraphrases from various cor-
pora with different methods (Zhao et al, 2008a;
Zhao et al, 2009), which is difficult and time-
consuming.
In addition to the MT-based method, re-
searchers have also investigated other methods for
paraphrase generation, such as the pattern-based
methods (Barzilay and Lee, 2003; Pang et al,
2003), thesaurus-based methods (Bolshakov and
Gelbukh, 2004; Kauchak and Barzilay, 2006),
and NLG-based methods (Kozlowski et al, 2003;
Power and Scott, 2005).
2.2 Pivot Approach for Paraphrasing
Bannard and Callison-Burch (2005) introduced
the pivot approach to extracting paraphrase
phrases from bilingual parallel corpora. Their ba-
sic assumption is that two English phrases aligned
with the same phrase in a foreign language (also
called a pivot language) are potential paraphrases.
Zhao et al (2008b) extended the approach and
used it to extract paraphrase patterns. Both of the
above works have proved the effectiveness of the
pivot approach in paraphrase extraction.
Pivot approach can also be used in paraphrase
generation. It generates paraphrases by translating
sentences from a source language to one (single-
pivot) or more (multi-pivot) pivot languages and
then translating them back to the source language.
Duboue et al (2006) first proposed the multi-
pivot approach for paraphrase generation, which
was specially designed for question expansion in
QA. In addition, Max (2009) presented a single-
pivot approach for generating sub-sentential para-
phrases. A clear difference between our method
and the above works is that we propose selection-
based and decoding-based techniques to gener-
ate high-quality paraphrases using the candidates
yielded from the pivot approach.
3 Multi-pivot Approach for Acquiring
Candidate Paraphrases
A single-pivot PG approach paraphrases a sen-
tence S by translating it into a pivot language
PL with a MT engine MT1 and then translat-
ing it back into the source language with MT2.
In this paper, a single-pivot PG system is repre-
sented as a triple (MT1, PL, MT2). A multi-
pivot PG system is made up of a set of single-pivot
systems with various pivot languages and MT en-
gines. Given m pivot languages and n MT en-
gines, we can build a multi-pivot PG system con-
sisting of N (N ? n ? m ? n) single-pivot ones,
where N = n ? m ? n iff all the n MT engines
can perform bidirectional translation between the
source and each pivot language.
In this work, we experiment with 6 pivot lan-
guages (Table 1) and 3 MT engines (Table 2) in
the multi-pivot approach. All the 3 MT engines
are off-the-shelf systems, in which Google and
Microsoft translators are SMT engines, while Sys-
tran translator is a rule-based MT engine. Each
MT engine can translate English to all the 6 pivot
languages and back to English. We thereby con-
struct a multi-pivot PG system consisting of 54
(3*6*3) single-pivot systems.
The advantages of the multi-pivot PG approach
lie in two aspects. First, it effectively makes use
of the vast bilingual data and translation rules un-
derlying the MT engines. Second, the approach is
simple, which just sends sentences to the online
MT engines and gets the translations back.
1327
Source Sentence he said there will be major cuts in the salaries of high-level civil servants .
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials .
(GG, F , GG) he said there will be significant cuts in the salaries of top civil level .
(MS, C, MS) he said that there will be a major senior civil service pay cut .
(MS, F , ST ) he said there will be great cuts in the wages of the high level civils servant .
(ST , G, GG) he said that there are major cuts in the salaries of senior government officials .
Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.
1 French (F) 4 Italian (I)
2 German (G) 5 Portuguese (P)
3 Spanish (S) 6 Chinese (C)
Table 1: Pivot languages used in the approach.
1 Google Translate (GG)
(translate.google.com)
2 Microsoft Translator (MS)
(www.microsofttranslator.com)
3 Systran Online Translation (ST)
(www.systransoft.com)
Table 2: MT engines utilized in the approach.
4 Producing High-quality Paraphrases
using the Candidates
Table 3 shows some examples of candidate para-
phrases for a sentence. As can be seen, the can-
didates do provide some correct and useful para-
phrase substitutes (in bold) for the source sen-
tence. However, they also contain quite a few er-
rors (in italic) due to the limited translation qual-
ity of the MT engines. The problem is even
worse when the source sentences get longer and
more complicated. Therefore, we need to com-
bine the outputs of the multiple single-pivot PG
systems and produce high-quality paraphrases out
of them. To this end, we investigate two tech-
niques, namely, the selection-based and decoding-
based techniques.
4.1 Selection-based Technique
Given a source sentence S along with a set D of
candidate paraphrases {T1, T2, ..., Ti, ...TN}, the
goal of the selection-based technique is to select
the best paraphrase T?i for S from D. The para-
phrase selection technique we propose is based on
Minimum Bayes Risk (MBR). In detail, the MBR
based technique first measures the quality of each
candidate paraphrase Ti ? D in terms of Bayes
risk (BR), and then selects the one with the min-
imum BR as the best paraphrase. In detail, given
S, a candidate Ti ? D, a reference paraphrase
T 1, and a loss function L(T, Ti) that measures the
quality of Ti relative to T , we define the Bayes
risk as follows:
BR(Ti) = EP (T,S)[L(T, Ti)], (1)
where the expectation is taken under the true dis-
tribution P (T, S) of the paraphrases. According
to (Bickel and Doksum, 1977), the candidate para-
phrase that minimizes the Bayes risk can be found
as follows:
T?i = arg minTi?D
?
T?T
L(T, Ti)P (T |S), (2)
where T represents the space of reference para-
phrases. In practice, however, the collection of
reference paraphrases is not available. We thus
construct a set D? = D?{S} to approximate T 2.
In addition, we cannot estimate P (T |S) in Equa-
tion (2), either. Therefore, we make a simplifica-
tion by assigning a constant c to P (T |S) for each
T ? D?, which can then be removed:
T?i = arg minTi?D
?
T?D?
L(T, Ti). (3)
Equation (3) can be further rewritten using a gain
function G(T, Ti) instead of the loss function:
1Here we assume that we have the collection of all possi-
ble paraphrases of S, which are used as references.
2The source sentence S is included in D? based on the
consideration that a sentence is allowed to keep unchanged
during paraphrasing.
1328
T?i = arg maxTi?D
?
T?D?
G(T, Ti). (4)
We define the gain function based on BLEU:
G(T, Ti) = BLEU(T, Ti). BLEU is a
widely used metric in the automatic evaluation of
MT (Papineni et al, 2002). It measures the sim-
ilarity of two sentences by counting the overlap-
ping n-grams (n=1,2,3,4 in our experiments):
BLEU(T, Ti) = BP ?exp(
4?
n=1
wn log pn(T, Ti)),
where pn(T, Ti) is the n-gram precision of Ti and
wn = 1/4. BP (? 1) is a brevity penalty that
penalizes Ti if it is shorter than T .
In summary, for each sentence S, the MBR
based technique selects a paraphrase that is the
most similar to all candidates and the source sen-
tence. The underlying assumption is that correct
paraphrase substitutes should be common among
the candidates, while errors committed by the
single-pivot PG systems should be all different.
We denote this approach as S-1 hereafter.
Approaches for comparison. In the experiments,
we also design another two paraphrase selection
approaches S-2 and S-3 for comparison with S-1.
S-2: S-2 selects the best single-pivot PG
system from all the 54 ones. The selection
is also based on MBR and BLEU. For each
single-pivot PG system, we sum up its gain
function values over a set of source sentences
(i.e., ?S
?
TS?D?S G(TS , TSi)). Then we se-lect the one with the maximum gain value as
the best single-pivot system. In our experi-
ments, the selected best single-pivot PG system is
(ST, P,GG), the candidate paraphrases acquired
by which are then returned as the best paraphrases
in S-2.
S-3: S-3 is a simple baseline, which just ran-
domly selects a paraphrase from the 54 candidates
for each source sentence S.
4.2 Decoding-based Technique
The selection-based technique introduced above
has an inherent limitation that it can only select
a paraphrase from the candidates. That is to say, it
major cuts high-level civil servants
significant cuts senior officials
major cuts* high-level officials
important cuts senior civil servants
big cuts
great cuts
Table 4: Extracted phrase pairs. (*This is called
a self-paraphrase of the source phrase, which
is generated when a phrase keeps unchanged in
some of the candidate paraphrases.)
can never produce a perfect paraphrase if all the
candidates have some tiny flaws. To solve this
problem, we propose the decoding-based tech-
nique, which trains a MT model using the can-
didate paraphrases of each source sentence S and
generates a new paraphrase T for S with a MT
decoder.
In this work, we implement the decoding-based
technique using Giza++ (Och and Ney, 2000) and
Moses (Hoang and Koehn, 2008), both of which
are commonly used SMT tools. For a sentence
S, we first construct a set of parallel sentences
by pairing S with each of its candidate para-
phrases: {(S,T1),(S,T2),...,(S,TN )} (N = 54).
We then run word alignment on the set using
Giza++ and extract aligned phrase pairs as de-
scribed in (Koehn, 2004). Here we only keep the
phrase pairs that are aligned ?3 times on the set,
so as to filter errors brought by the noisy sentence
pairs. The extracted phrase pairs are stored in a
phrase table. Table 4 shows some extracted phrase
pairs.
Note that Giza++ is sensitive to the data size.
Hence it is interesting to examine if the alignment
can be improved by augmenting the parallel sen-
tence pairs. To this end, we have tried augmenting
the parallel set for each sentence S by pairing any
two candidate paraphrases. In this manner, C2N
sentence pairs are augmented for each S. We con-
duct word alignment using the (N+C2N ) sentence
pairs and extract aligned phrases from the original
N pairs. However, we have not found clear im-
provement after observing the results. Therefore,
we do not adopt the augmentation strategy in our
experiments.
1329
Using the extracted phrasal paraphrases, we
conduct decoding for the sentence S with Moses,
which is based on a log-linear model. The default
setting of Moses is used, except that the distortion
model for phrase reordering is turned off3. The
language model in Moses is trained using a 9 GB
English corpus. We denote the above approach as
D-1 in what follows.
Approach for comparison. The main advantage
of the decoding-based technique is that it allows
us to customize the paraphrases for different re-
quirements through tailoring the phrase table or
tuning the model parameters. As a case study,
this paper shows how to generate paraphrases with
varied paraphrase rates4.
D-2: The extracted phrasal paraphrases (in-
cluding self-paraphrases) are stored in a phrase ta-
ble, in which each phrase pair has 4 scores mea-
suring their alignment confidence (Koehn et al,
2003). Our basic idea is to control the paraphrase
rate by tuning the scores of the self-paraphrases.
We thus extend D-1 to D-2, which assigns a
weight ? (? > 0) to the scores of the self-
paraphrase pairs. Obviously, if we set ? < 1,
the self-paraphrases will be penalized and the de-
coder will prefer to generate a paraphrase with
more changes. If we set ? > 1, the decoder will
tend to generate a paraphrase that is more similar
to the source sentence. In our experiments, we set
? = 0.1 in D-2.
5 Experimental Setup
Our test sentences are extracted from the paral-
lel reference translations of a Chinese-to-English
MT evaluation5, in which each Chinese sentence
c has 4 English reference translations, namely e1,
e2, e3, and e4. We use e1 as a test sentence to para-
phrase and e2, e3, e4 as human paraphrases of e1
for comparison with the automatically generated
paraphrases. We process the test set by manually
filtering ill-formed sentences, such as the ungram-
matical or incomplete ones. 1182 out of 1357
3We conduct monotone decoding as previous work
(Quirk et al, 2004; Zhao et al, 2008a, Zhao et al, 2009).
4The paraphrase rate reflects how different a paraphrase
is from the source sentence.
52008 NIST Open Machine Translation Evaluation: Chi-
nese to English Task.
Score Adequacy Fluency
5 All Flawless English
4 Most Good English
3 Much Non-native English
2 Little Disfluent English
1 None Incomprehensible
Table 5: Five point scale for human evaluation.
test sentences are retained after filtering. Statistics
show that about half of the test sentences are from
news and the other half are from essays. The aver-
age length of the test sentences is 34.12 (words).
Manual evaluation is used in this work. A para-
phrase T of a sentence S is manually scored based
on a five point scale, which measures both the ?ad-
equacy? (i.e., how much of the meaning of S is
preserved in T ) and ?fluency? of T (See Table 5).
The five point scale used here is similar to that in
the human evaluation of MT (Callison-Burch et
al., 2007). In MT, adequacy and fluency are eval-
uated separately. However, we find that there is a
high correlation between the two aspects, which
makes it difficult to separate them. Thus we com-
bine them in this paper.
We compare our method with a state-of-the-
art approach SPG6 (Zhao et al, 2009), which
is a statistical approach specially designed for
PG. The approach first collects a large volume of
fine-grained paraphrase resources, including para-
phrase phrases, patterns, and collocations, from
various corpora using different methods. Then it
generates paraphrases using these resources with
a statistical model7.
6 Experimental Results
We evaluate six approaches, i.e., S-1, S-2, S-3, D-
1, D-2 and SPG, in the experiments. Each ap-
proach generates a 1-best paraphrase for a test
sentence S. We randomize the order of the 6 para-
phrases of each S to avoid bias of the raters.
6SPG: Statistical Paraphrase Generation.
7We ran SPG under the setting of baseline-2 as described
in (Zhao et al, 2009).
1330
00.5
1
1.52
2.53
3.54
4.5
score 3.92 3.52 2.78 3.62 3.36 3.47
S-1 S-2 S-3 D-1 D-2 SPG
Figure 1: Evaluation results of the approaches.
6.1 Human Evaluation Results
We have 6 raters in the evaluation, all of whom
are postgraduate students. In particular, 3 raters
major in English, while the other 3 major in com-
puter science. Each rater scores the paraphrases
of 1/6 test sentences, whose results are then com-
bined to form the final scoring result. The av-
erage scores of the six approaches are shown in
Figure 1. We can find that among the selection-
based approaches, the performance of S-3 is the
worst, which indicates that randomly selecting a
paraphrase from the candidates works badly. S-
2 performs much better than S-3, suggesting that
the quality of the paraphrases acquired with the
best single-pivot PG system are much higher than
the randomly selected ones. S-1 performs the best
in all the six approaches, which demonstrates the
effectiveness of the MBR-based selection tech-
nique. Additionally, the fact that S-1 evidently
outperforms S-2 suggests that it is necessary to ex-
tend a single-pivot approach to a multi-pivot one.
To get a deeper insight of S-1, we randomly
sample 100 test sentences and manually score all
of their candidates. We find that S-1 successfully
picks out a paraphrase with the highest score for
72 test sentences. We further analyze the remain-
ing 28 sentences for which S-1 fails and find that
the failures are mainly due to the BLEU-based
gain function. For example, S-1 sometimes se-
lects paraphrases that have correct phrases but in-
correct phrase orders, since BLEU is weak in eval-
uating phrase orders and sentence structures. In
the next step we shall improve the gain function
by investigating other features besides BLEU.
In the decoding-based approaches, D-1 ranks
the second in the six approaches only behind S-1.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
S-1 S-2 S-3 D-1 D-2 SPG
r1 r2 r3 r4 r5 r6
Figure 2: Evaluation results from each rater.
We will further improve D-1 in the future rather
than simply use Moses in decoding with the de-
fault setting. However, the value of D-1 lies in
that it enables us to break down the candidates
and generate new paraphrases flexibly. The per-
formance decreases when we extend D-1 to D-2
to achieve a larger paraphrase rate. This is mainly
because more errors are brought in when more
parts of a sentence are paraphrased.
We can also find from Figure 1 that S-1, S-2,
and D-1 all get higher scores than SPG, which
shows that our method outperforms this state-of-
the-art approach. This is more important if we
consider that our method is lightweight, which
makes no effort to collect fine-grained paraphrase
resources beforehand. After observing the results,
we believe that the outperformance of our method
can be mainly ascribed to the selection-based and
decoding-based techniques, since we avoid many
errors by voting among the candidates. For in-
stance, an ambiguous phrase may be incorrectly
paraphrased by some of the single-pivot PG sys-
tems or the SPG approach. However, our method
may obtain the correct paraphrase through statis-
tics over all candidates and selecting the most
credible one.
The human evaluation of paraphrases is subjec-
tive. Hence it is necessary to examine the coher-
ence among the raters. The scoring results from
the six raters are depicted in Figure 2. As it can be
seen, they show similar trends though the raters
have different degrees of strictness.
1331
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
PR1 0.116 0.138 0.232 0.149 0.206 0.139 0.366 0.379 0.386PR2 0.211 0.272 0.427 0.22 0.3 0.234 0.607 0.602 0.694
S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3
Figure 3: Paraphrase rates of the approaches.
6.2 Paraphrase Rate
Human evaluation assesses the quality of para-
phrases. However, the paraphrase rates cannot be
reflected. A paraphrase that is totally transformed
from the source sentence and another that is al-
most unchanged may get the same score. There-
fore, we propose two strategies, i.e., PR1 and PR2,
to compute the paraphrase rate:
PR1(T ) = 1? OL(S, T )L(S) ; PR2(T ) =
ED(S, T )
L(S) .
Here, PR1 is defined based on word overlapping
rate, in which OL(S, T ) denotes the number of
overlapping words between a paraphrase T and its
source sentence S, L(S) denotes the number of
words in S. PR2 is defined based on edit distance,
in which ED(S, T ) denotes the edit distance be-
tween T and S. Obviously, PR1 only measures
the percentage of words that are changed from
S to T , whereas PR2 further takes word order
changes into consideration. It should be noted that
PR1 and PR2 not only count the correct changes
between S and T , but also count the incorrect
ones. We compute the paraphrase rate for each
of the six approaches by averaging the paraphrase
rates over the whole test set. The results are shown
in the left part of Figure 3.
On the whole, the paraphrase rates of the ap-
proaches are not high. In particular, we can see
that the paraphrase rate of D-2 is clearly higher
than D-1, which is in line with our intention of de-
signing D-2. We can also see that the paraphrase
rate of S-3 is the highest among the approaches.
We find it is mainly because the paraphrases gen-
erated with S-3 contain quite a lot of errors, which
contribute most of the changes.
7 Analysis
7.1 Effectiveness of the Proposed Method
Our analysis starts from the candidate paraphrases
acquired with the multi-pivot approach. Actu-
ally, the results of S-3 reflect the average qual-
ity of the candidate paraphrases. A score of 2.78
(See Figure 1) indicates that the candidates are
unacceptable according to the human evaluation
metrics. This is in line with our expectation that
the automatically acquired paraphrases through a
two-way translation are noisy. However, the re-
sults of S-1 and D-1 demonstrate that, using the
selection-based and decoding-based techniques,
we can produce paraphrases of good quality. Es-
pecially, S-1 gets a score of nearly 4, which sug-
gests that the paraphrases are pretty good accord-
ing to our metrics. Moreover, our method out-
performs SPG built on pre-extracted fine-grained
paraphrases. It shows that our method makes good
use of the paraphrase knowledge from the large
volume of bilingual data underlying the multiple
MT engines.
7.2 How to Choose Pivot Languages and MT
Engines in the Multi-pivot Approach
In our experiments, besides the six pivot lan-
guages used in the multi-pivot system, we have
also tried another five pivot languages, including
Arabic, Japanese, Korean, Russian, and Dutch.
They are finally abandoned since we find that they
perform badly. Our experience on choosing pivot
languages is that: (1) a pivot language should be
a language whose translation quality can be well
guaranteed by the MT engines; (2) it is better to
choose a pivot language similar to the source lan-
guage (e.g., French - English), which is easier to
translate; (3) the translation quality of a pivot lan-
guage should not vary a lot among the MT en-
gines. On the other hand, it is better to choose
MT engines built on diverse models and corpora,
which can provide different paraphrase options.
We plan to employ a syntax-based MT engine in
our further experiments besides the currently used
phrase-based SMT and rule-based MT engines.
1332
S he said there will be major cuts in the salaries of high-level civil servants .
S-1 he said that there will be significant cuts in the salaries of senior officials .
S-2 he said there will be major cuts in salaries of civil servants high level .
S-3 he said that there will be significant cuts in the salaries of senior officials .
D-1 he said , there will be significant cuts in salaries of senior civil servants .
D-2 he said , there will be significant cuts in salaries of senior officials .
SPG he said that there will be the main cuts in the wages of high-level civil servants .
HP1 he said there will be a big salary cut for high-level government employees .
HP2 he said salaries of senior public servants would be slashed .
HP3 he claimed to implement huge salary cut to senior civil servants .
Table 6: Comparing the automatically generated paraphrases with the human paraphrases.
7.3 Comparing the Selection-based and
Decoding-based Techniques
It is necessary to compare the paraphrases gener-
ated via the selection-based and decoding-based
techniques. As stated above, the selection-based
technique can only select a paraphrase from the
candidates, while the decoding-based technique
can generate a paraphrase different from all can-
didates. In our experiments, we find that for
about 90% test sentences, the paraphrases gener-
ated by the decoding-based approach D-1 are out-
side the candidates. In particular, we compare the
paraphrases generated by S-1 and D-1 and find
that, for about 40% test sentences, S-1 gets higher
scores than D-1, while for another 21% test sen-
tences, D-1 gets higher scores than S-18. This
indicates that the selection-based and decoding-
based techniques are complementary. In addition,
we find examples in which the decoding-based
technique can generate a perfect paraphrase for
the source sentence, even if all the candidate para-
phrases have obvious errors. This also shows that
the decoding-based technique is promising.
7.4 Comparing Automatically Generated
Paraphrases with Human Paraphrases
We also analyze the characteristics of the gener-
ated paraphrases and compare them with the hu-
man paraphrases (i.e., the other 3 reference trans-
lations in the MT evaluation, see Section 5, which
are denoted as HP1, HP2, and HP3). We find that,
compared with the automatically generated para-
phrases, the human paraphrases are more com-
8For the rest 39%, S-1 and D-1 get identical scores.
plicated, which involve not only phrase replace-
ments, but also structure reformulations and even
inferences. Their paraphrase rates are also much
higher, which can be seen in the right part of Fig-
ure 3. We show the automatic and human para-
phrases for the example sentence of this paper in
Table 6. To narrow the gap between the automatic
and human paraphrases, it is necessary to learn
structural paraphrase knowledge from the candi-
dates in the future work.
8 Conclusions and Future Work
We put forward an effective method for para-
phrase generation, which has the following con-
tributions. First, it acquires a rich fund of para-
phrase knowledge through the use of multiple MT
engines and pivot languages. Second, it presents
a MBR-based technique that effectively selects
high-quality paraphrases from the noisy candi-
dates. Third, it proposes a decoding-based tech-
nique, which can generate paraphrases that are
different from the candidates. Experimental re-
sults show that the proposed method outperforms
a state-of-the-art approach SPG.
In the future work, we plan to improve the
selection-based and decoding-based techniques.
We will try some standard system combination
strategies, like confusion networks and consensus
decoding. In addition, we will refine our evalu-
ation metrics. In the current experiments, para-
phrase correctness (adequacy and fluency) and
paraphrase rate are evaluated separately, which
seem to be incompatible. We plan to combine
them together and propose a uniform metric.
1333
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics: Basic Ideas and Selected Topics.
Holden-Day Inc., Oakland, CA, USA.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL-2007 Workshop on Statistical Ma-
chine Translation, pages 136-158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Ver-
sion 1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-
sentence paraphrases from predicate/argument
structure using lexico-grammatical resources. In
Proceedings of IWP, pages 1-8.
Aure?lien Max. 2009. Sub-sentential Paraphrasing by
Contextual Pivot Translation. In Proceedings of the
2009 Workshop on Applied Textual Inference, ACL-
IJCNLP 2009, pages 18-26.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL, pages 440-447.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT-NAACL, pages 102-
109.
Kishore Papineni, Salim Roukos, ToddWard, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic
generation of large-scale paraphrases. In Proceed-
ings of IWP, pages 73-79.
Chris Quirk, Chris Brockett, andWilliamDolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Yujie Zhang and Kazuhide Yamamoto. 2002. Para-
phrasing of Chinese Utterances. In Proceedings of
COLING, pages 1163-1169.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Genera-
tion. In Proceedings of ACL-IJCNLP 2009, pages
834-842.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. CombiningMultiple Resources to
Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
1334
Coling 2010: Poster Volume, pages 1167?1175,
Beijing, August 2010
Bridging Topic Modeling and Personalized Search
Wei Song Yu Zhang Ting Liu Sheng Li
School of Computer Science
Harbin Institute of Technology
{wsong, yzhang, tliu, lisheng}@ir.hit.edu.cn
Abstract
This work presents a study to bridge topic
modeling and personalized search. A
probabilistic topic model is used to extract
topics from user search history. These
topics can be seen as a roughly summary
of user preferences and further treated as
feedback within the KL-Divergence re-
trieval model to estimate a more accurate
query model. The topics more relevant
to current query contribute more in updat-
ing the query model which helps to dis-
tinguish between relevant and irrelevant
parts and filter out noise in user search
history. We designed task oriented user
study and the results show that: (1) The
extracted topics can be used to cluster
queries according to topics. (2) The pro-
posed approach improves ranking qual-
ity consistently for queries matching user
past interests and is robust for queries not
matching past interests.
1 Introduction
The majority of queries submitted to search en-
gines are short and ambiguous and the users of
search engines often have different search intents
even when they submit the same query (Janse and
Saracevic, 2000)(Silverstein and Moricz, 1999).
The ?one size fits all? approach fails to optimize
each individual?s specific information need. Per-
sonalized search has be viewed as a promising
direction to solve the ?data overload? problem,
and aims to provide different search results ac-
cording to the specific preference of an individ-
ual(Pitkow and Breuel, 2002). Information re-
trieval (IR) communities have developed models
for context sensitive search and related applica-
tions (Shen and Zhai, 2005a)(White and Chen,
2009).
The search context includes a broad range of in-
formation types such as a user?s background, his
personal desktop index, browser history and even
the context information of a group of similar users
(Teevan, 2009). In this paper, we exploit the user
search history of an individual which contains the
past submitted queries, results returned and the
click through information. As described in (Tan
and Zhai, 2006), search history is one of the most
important forms of search context. When dealing
with search history, distinguishing between rele-
vant and irrelevant parts is important. The search
history may contain a lot of noisy information
which can harm the performance of personaliza-
tion (Dou and Wen, 2007). Hence, we need to
sort out relevant and irrelevant parts to optimize
search personalization.
In this paper, we propose a topic model based
approach to study users? preferences. The main
contribution of this work is modeling user search
history with topics for personalized search. Our
approach mainly consists of two steps: topic ex-
traction and relevance feedback. We assume that
a user?s search history is governed by the underly-
ing hidden properties and apply probabilistic La-
tent Semantic Indexing (pLSI) (Hofmann, 1999)
to extract topics from user search history. Each
topic indexes a unigram language model. We
model these extracted topics as feedback in the
KL-Divergence retrieval framework. The task is
to estimate a more accurate query model based
on the evidence from user feedback. We distin-
1167
guish relevant parts from irrelevant parts in search
history by focusing on the relevance between top-
ics and query. The closer a topic is to the cur-
rent query, the more it contributes in updating the
query model, which in turn is used to rerank the
documents in results set.
2 Related Work
2.1 Personalized IR
Personalized search is an active ongoing research
direction. Based on different representations of
user profile, we classify approaches as follows:
Taxonomy based methods: this approach
maps user interests to an existing taxonomy.
ODP1 is widely used for this purpose. For
example, by exploiting the user search history,
(Speretta and Gauch, 2005) modeled user interest
as a weighted concept hierarchy created from the
top 3 level of ODP. (Havelivala, 2002) proposed
the ?topic sensitive pagerank? algorithm by cal-
culating a set of PageRanks for each web page on
the top 16 ODP categories. (Qiu and Cho, 2006)
further improved this approach by building user
models from user click history. In recent stud-
ies, (Xu S. and Yu, 2008) used ODP categories
for exploring folksonomy for personalized search.
(Dou and Wen, 2007) proposed a method that rep-
resent user profile as a weighting vector of 67 pre-
defined topic categories provided by KDD Cup-
2005. Taxonomy based methods rely on a pre-
defined taxonomy and may suffer from the granu-
larity problem.
Content based methods: this category of
methods use traditional text presentation model
such as vector space model and language model
to express user preference. Rich content infor-
mation such as user search history, browser his-
tory and indexes of desktop documents are ex-
plored. The user profiles are built in the forms of
term vectors or term probability distributions. For
example, (Sugiyama and M., 2004) represented
user profiles as vectors of distinct terms and ac-
cumulated past preferences. (Teevan and Horvitz,
2005) constructed a rich user model based on both
search-related information, such as previously is-
sued queries, and other information such as doc-
1Open Directory Project, http://dmoz.org/
uments and emails a user had read and created.
(Shen and Zhai, 2005b) used browsing histories
and query sessions to construct short term indi-
vidual models for personalized search.
Learning to rank methods: (Eugene and Su-
san, 2005) and (Eugene and Zheng, 2006) incor-
porated user feedback into the ranking process in a
learning to rank framework. They leveraged mil-
lions of past user interaction with web search en-
gine to construct implicit feedback features. How-
ever, this approach aims to satisfy majority of
users rather than individuals.
2.2 Probabilistic Topic Models
Probabilistic topic models have become popular
tools for unsupervised analysis of document col-
lection. Topic models are based upon the idea
that documents are mixtures of topics, where
a topic is a probability distribution over words
(Steyvers and Griffiths, 2007). These topics are
interpretable to a certain degree. In fact, one of
the most important applications of topic models
is to find out semantic lexicons from a corpus.
One of the most popular topic models, the prob-
abilistic Latent Semantic Indexing Model (pLSI),
was introduced by Hofmann (Hofmann, 1999)
and quickly gained acceptance in a number of text
modeling applications. In this study, pLSI is used
to discover the underlying topics in user search
history. Though pLSI is argued that it is not a
complete generative model, we used it because it
does not need to generate unseen documents in
our case and the model is much easier to be es-
timated compared with sophisticated models such
as LDA(David M. Blei and Jordan, 2003).
2.3 Model based Relevance Feedback
Our work is also related to language model based
(pseudo) relevance feedback (Zhai and Lafferty,
2001b) and shares the similar idea with (Tan B.
and Zhai, 2007). The differences are: (1) The
feedback source is user search history rather than
top ranked documents for a query. (2) We make
use of user implicit feedback rather than explicit
feedback. (3) The topics in search history could
be extracted offline and updated periodically. Ad-
ditionally, these topics provide an informative pic-
ture of user search history.
1168
Table 1: An illustration of topics extracted from a
user?s search history. Terms with highest proba-
bilities are listed below each topic.
Topic 2 Topic 3 Topic 9 Topic 16
climb movie swim cup
0.032 0.091 0.044 0.027
setup download ticket world
0.022 0.078 0.032 0.022
equipment dvd notice team
0.020 0.061 0.019 0.016
practice watch travel brazil
0.009 0.060 0.016 0.011
player cinema hotel storm
0.006 0.038 0.008 0.007
3 Proposed Approach
3.1 Main Idea
A user?s search history usually covers multiple
topics. It is crucial to distinguish between rele-
vant and irrelevant parts for optimizing personal-
ization. We propose a topic model based method
to achieve that goal. First, we construct a doc-
ument collection revealing user intents according
to the user?s past activities. A probabilistic topic
model is applied on this collection to extract la-
tent topics. Then the extracted topics are used as
feedback. The query model is updated by high-
lighting the topics highly relevant to current query.
Finally, the search results are reranked according
to the relevance to the updated query model. Ta-
ble 1 shows 4 topics extracted from a user?s search
history. Each topic is a unigram language model.
The terms with higher probabilities belonging to
each topic are listed. We can predict that the user
has interests in both movie and football. However,
when the user submits a query about world cup,
the topic 16 is given higher preference for esti-
mating a more accurate query model.
3.2 Topic Extraction from Search History
Individual?s search history consists of all the past
query units. Each query unit includes query text,
returned search results (with title, snippets and
URLs) and click through information. Here, we
concatenated the title and snippet of each search
result to form a document being considered as a
whole. The whole search history can be seen as a
collection of documents. Obviously, many doc-
uments in the collection may fail to satisfy the
user?s information need and are uncertain for dis-
covering the user?s preferences. Therefore, the
first task is to select proper documents in search
history as the preference collection for topic dis-
covery.
3.2.1 Preference Collection
An intuitive solution is to use the documents
that are clicked by the user. The assumption is
that a user clicks on a result only if he is interested
in the document. However, user click is sparse in
real search environments and the documents not
clicked by the user may also be relevant to the
user?s information need. We assumed that the user
had only one search intent for a submitted query.
To enhance this coherence within a query unit, we
created only one super-document for a query unit
as follows: if a query unit had clicked documents,
then we concatenated these document to form a
preferred document. Otherwise, we selected the
top n documents from the search results and con-
catenated them as a preferred document. That is
motivated by the idea of pseudo relevance feed-
back (Lavrenko and Croft, 2001) and used here for
alleviating data sparsity. Pseudo relevance feed-
back is sensitive to the number of feedback docu-
ments. In this work, n is set to 3, because the aver-
age clicks for a query is not more than 3. By this
way, we got a preference collection whose size is
the same as the number of past queries.
3.2.2 Topic Extraction
Given the collection of preferred documents,
we applied pLSI on this collection to extract
underlying topics. We define the collection as
C={d1,d2,. . . ,dM}, where di corresponds to the
ith query unit, and M is the size of the collection.
Each query unit is viewed as a mixture of differ-
ent topics. It is reasonable in reality. For exam-
ple, a news document about ?play basketball with
obama? might be seen as a mixture of topics ?pol-
itics? and ?sports?.
Modeling: The basic idea of pLSI is to treat
the words in each document as being generated
from a mixture model where the component mod-
els are topic word distributions. Let k be the num-
1169
ber of topics which is assumed known and fixed.
?j is the word distribution for topic j. We extract
topics from collection C using a simple proba-
bilistic mixture model as described in (Zhai and
Yu, 2004). A word w within document d can be
viewed as generated from a mixture model:
pd(w) = ?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)
(1)
where ?B is the background model for all the doc-
uments. The background model is used to draw
common words across all the documents and lead
to more discriminative and informative topic mod-
els, since ?B gives high weights to non-topical
words. ?B is the probability that a term is gen-
erated from the background model which is set to
be a constant. To draw more discriminative topic
models, we set ?B to 0.95. Parameter pid,j indi-
cates the probability that topic j is assigned to the
specific document d, where?kj=1 pid,j=1.Parameter estimation: The parameters we
have to estimate including the background model
?B , {?j} and {pid,j}. ?B is maximum likelihood
estimated (MLE) using all available text in our
data set so that it is a fixed distribution. The other
parameters to be estimated are {?j} and {pid,j}.
The log-likelihood of document d is:
log p(d) =
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(2)
The log-likelihood of the whole collection C is:
log(C) =
?
d?C
?
w?V
c(w, d) log[?Bp(w|?B)
+(1 ? ?B)
k?
j=1
pid,jp(w|?j)]
(3)
The Expectation-Maximization (EM) algorithm
(Dempster and Rubin, 1977) is used to find a
group of parameters maximizing equation (3).
The updating formulas are:
,
,
1
( ) ( )
,
,
( ) ( )
, ' '
' 1
, ,( 1)
,
, ,
' 1
(
E-Step:
( | )
( )
( | ) (1 ) ( | )
( | )
( )
( | )
M-Step:
( , )(1 ( )) ( )
( , )(1 ( )) ( ')
B B
d w k
B B B d j j
j
m m
d j j
d w k
m m
d j j
j
d w d wm w V
d j k
d w d ww V
j
p w
p z B
p w p w
p w
p z j
p w
c w d p z B p z j
c w d p z B p z j
p
? ?
? ? ? ? ?
? ?
? ?
?
=
=
+ ?
?=
= =
+ ?
= =
? = ==
? = =
?
?
?
??
, ,
1)
, ' , '
'
( , )(1 ( )) ( )
( | )
( ', )(1 ( )) ( )
d w d w
m d C
j
d w d w
d C w V
c w d p z B p z j
w
c w d p z B p z j
?+ ?
? ?
? = =
= ? = =
?
? ?
 
where c(w, d) denotes the number of times w
occurs in d. A hidden variable zd,w is introduced
for the identity of each word. p(zd,w = B) is
the probability that the word w in document d is
generated by the background model. p(zd,w = j)
denotes the probability that the word w in docu-
ment d is generated using topic j given that w is
not generated from the background model. Infor-
mally, the EM algorithm starts with randomly as-
signing values to the parameters to be estimated
and then alternates between E-Step and M-Step
iteratively until it yields a local maximum of the
log likelihood.
Interpretation: As shown in equation (1), a
word can be viewed as a mixture of topics. From
the updating formulas, we can see that the domi-
nant topic of a word depends on both itself and the
context. The word tends to have the same topic
with the document containing it. While the prob-
ability of assigning topic j to document d is es-
timated by aggregating all the fractions of words
generated by topic j in document d. We can ex-
plain it in a more intuitive way with in our applica-
tion. As we know, the queries are usually ambigu-
ous. A classic example is ?apple? which may re-
fer to a kind of fruit, apple Inc, apple electric prod-
ucts, etc. Therefore, it is reasonable to assume
that each word belongs to multiple latent seman-
tic properties. If a returned result contains ?ap-
ple? and other words like ?computer?, ?ipod? ,
etc. The word ?apple? in this result tends to have
the same topic distributions with ?computer? and
1170
?ipod?. If the user clicks the result, we can predict
that the user?s real preference about query ?ap-
ple? is related to electric products having a high
probability. Further, if ?apple? occurs frequently
in many documents related to electric products,
it obtains a higher probability in this topic. As
a result, we not only know user?s interest in elec-
tric products, but also find a preference to ?apple?
brand.
Since a document?s topic depends on the words
it contains, two documents with similar word dis-
tributions have similar topic distributions. In other
words, each topic is like a bridge connecting
queries with similar intents. In summary, the topic
extraction process plays a role in our application
for finding user preference, highlighting discrimi-
native words and connecting queries with similar
intents.
3.3 Topics as Feedback
The topics extracted from search history are con-
sidered as a kind of feedback. Since topic mod-
els actually are extensions of language models,
we use such feedback within the KL-Divergence
retrieval model (Xu and Croft, 1999)(Zhai and
Lafferty, 2001b) that is a principled framework
to model feedback in the language modeling ap-
proach. In this framework, feedback is treated as
updating the query language model based on extra
evidence obtained from the feedback sources. The
information retrieval task is to rank documents ac-
cording to the KL divergence D(?q||?d) between
a query language model ?q and a document lan-
guage model ?d. The KL divergence is defined as:
D(?q||?d) =
?
w?V
p(w|?q) log
p(w|?q)
p(w|?d)
(4)
where V denotes the vocabulary. We estimate
the document model ?d using Dirichlet estimation
(Zhai and Lafferty, 2001a):
p(w|?d) =
c(w, d) + ?p(w|?C)
|d| + ? (5)
where |d| is document length, p(w|?C) is collec-
tion language model which is estimated using the
whole data collection. ? is the Dirichlet prior that
is set to 20 in this work. The updated query model
is defined as:
p(w|?q) = ?pml(w|?q)
+(1 ? ?)
k?
j=1
p(w|?j)p(z = j|q)
(6)
where pml(w|?q) is the MLE query model. {?j}
represents a set of extracted topics each of which
is a unigram language model. ? is used to bal-
ance the two components. z is a hidden variable
over topics. The task is to estimate the multino-
mial topic distribution p(z|q) for query q. Since
pLSI does not properly provide a prior, we esti-
mate p(z = j|q) as:
p(z = j|q) = p(q, z = j)?k
j?=1 p(q, z = j?)
? sim(?q, ?j)?k
j?=1 sim(?q, ?j?)
(7)
Since the query text is usually very short, it is
not easy to make a decision based on query text
alone. Instead, we concatenate all the available
documents in returned result set to form a super-
document. A language model is estimated for it.
We convert both the document language model
and topic models into weighted term vectors and
use cosine similarity as the sim function. p(z|q)
plays an import role here as it determines the con-
tribution of topics. The topics with higher similar-
ity with current query contributes more in updat-
ing query model. This scheme helps to filter out
noisy information in search history.
4 Evaluation and Discussion
4.1 Data Collection
To the best of our knowledge, there is no public
collection with enough content information and
user implicit feedback. We decided to carry out
a data collection. Due to the difficulty to de-
scribe and evaluate user interests implicitly, we
predefined some user interests and implemented
a search system to collect user interactions.
The predefined interests belong to 5 big cate-
gories namely Entertainment, Computer & Inter-
net, Sports, Health and Social life. Each inter-
est is a kind of user preference such as ?movies?
1171
Table 2: An example of predefined user interests
and tasks
category Enterntainment
interest movies
task1 search for a brief introductionof your favorite movie
task2 search for an introduction ofan actor or actress you like
task3 search for movies about?artificial intelligence?
Table 3: Statistics of the data collection
user 1 2 3 4 5
#queries 218 256 177 206 311
#big category 5 5 5 5 5
#interest 25 25 25 25 25
#tasks 100 100 100 100 100
avg.#relevant 4.17 4.22 3.89 4.12 3.24results
avg.#clicked 2.37 2.21 2.71 1.98 2.42results
and ?outdoor sports?. For each interest, we de-
signed several tasks each of which had a goal. Ta-
ble 2 illustrates an example of a predefined user
interest and related tasks. The volunteers were
asked to find out the information need according
to the tasks. Though we defined these interests
and tasks, we did not impose any constraint on
the queries. The volunteers could choose and re-
formulate any query they thought good for find-
ing the desired information. But we did try to in-
crease the possibility that a user might issue am-
biguous queries by designing tasks like ?search
for movies about artificial intelligence? which was
categorized to interest ?movies?, but also related
to computer science.
To collect the user interaction with search en-
gine, we implemented a Lucene based search sys-
tem on Tianwang terabyte corpus(Yan and Peng,
2005). Five volunteers were asked to submit
queries to this system to find information satisfy-
ing the tasks of each interest. The system recorded
users? activities including submitted queries, re-
turned search results (with title, snippet and URL)
and users? click through information. When the
user finished a task, he clicked a button to tell the
system termination of the session containing all
the queries and activities related to this task. After
finishing all the tasks, the volunteers were asked to
judge the top 20 results? relevance (relevant or not
relevant) for each query according to the search
target. Each volunteer submitted 233 queries on
average. Table 3 presents some statistics of this
collection.
4.2 Evaluating Topic Extraction
It is not easy to assess the quality of topics, be-
cause topic extraction is an unsupervised process
and difficult to give a standard answer. Therefore,
we view the topic extraction as a clustering prob-
lem that is to organize queries into clusters. To
group queries into clusters through extracted top-
ics, we use j? = argmax
j
pid,j to assign a query to
the j?th topic. Each topic corresponds to a cluster.
All the queries are divided into k clusters. Based
on the data collection, we setup the golden an-
swers according to the predefined interests. We
view all the queries belonging to a predefined in-
terest(which includes multiple tasks) form a clus-
ter which helps us to build a golden answer with
25 clusters in tatal.
One purpose of making use of topics in search
history is to find more relevant parts and reduce
the noise. We hope that the extracted topics are
coherent. That is, a cluster should contain as many
queries as possible belonging to a single inter-
est. To evaluate coherence, we adopt purity (Zhao
and Karypis, 2001), a commonly used metric for
evaluating clustering. The higher the purity is,
the better the system performs. We compare our
method (denoted as PLSI) against the k-means al-
gorithm(denoted as K-Means) on the preference
collection.
Figure 1 shows the overall purity with differ-
ent number of topics. Our method gained better
performance than k-means algorithm consistently.
It is effective to discover and organize user inter-
ests. Besides, as illustrated in Table 1, our method
is able to give higher probability to discriminative
words of each topic that provides a clear picture
of user search history. This leads to an emergence
of novel approaches for personalized browsing.
1172
0.2
0.3
0.4
0.5
0.6
0.7
0.8
10 20 30 40 50 60 70 80 90 100
Number of topics
Pu
ri
ty
PLSI K-Means
 
Figure 1: Average purity over 5 users gained by
both PLSI and K-Means with different number of
topics(clusters).
4.3 Evaluating Result Reranking
4.3.1 Metric
To quantify the ranking quality, the Dis-
counted Cumulative Gain (DCG) (Jarvelin and
Kekakainen, 2000) is used. DCG is a metric that
gives higher weights to highly ranked documents
and incorporates different relevance levels by giv-
ing them different gain values.
DCG(i) =
{
G(1), if i = 1
DCG(i? 1) + G(i)log(i) , otherwise
In our work, we use G(i) = 1 for the results la-
beled as relevant by a user and G(i) = 0 for the
results that are not relevant. The average normal-
ized DCG (NDCG)over all the test queries is se-
lected to show the performance.
4.3.2 Systems
We evaluated the performance of following sys-
tems:
PLSI: The proposed method. The history model
was a weighted interpolation over topics extracted
from the preference collection described in ses-
sion 3.2.1.
PSEUDO: From each query unit, we selected
top n documents as pseudo feedback. The lan-
guage history model was estimated on all these
documents.
PLSI-PSEUDO: Top n documents from each
query unit were concatenated to form a preferred
document. The history model was constructed
based on topics extracted from these preferred
documents.
HISTORY: The history language model was es-
timated based on all the documents in search his-
tory.
TB: It was based on(Tan and Zhai, 2006)which
built a unit language model for every past query
and the history model was a weighted interpola-
tion of past unit language models.
ORIGINAL: The default search system.
The first 5 systems provided schemes to smooth
the query model. They estimated the query mod-
els by utilizing different types of feedback (im-
plicit feedback or pseudo feedback) and weight-
ing methods (topic modeling or simple language
modeling). The updated query model was an in-
terpolation between MLE query model and his-
tory language model. The interpolation parameter
was set to 0.5, and n was set to 3.
4.3.3 Performance Comparison
To evaluate the performance on a test query, we
focus on two conditions:
1. the test query matches some past interests.
We want to check the ability of systems to
find relevant information from noisy data.
2. the test query does not match any of past in-
terests. We are interested in the robustness of
the systems.
For the first case, the users were asked to se-
lect at most 2 queries they submitted for each
task. These queries were used as test queries.
The other queries were used to simulate the users?
search history. In total we got 400 queries for
testing. Figure 2 demonstrates the performance
of these systems over all test queries. PLSI
outperformed all other systems consistently that
shows topic model based methods help to esti-
mate a more accurate query model and the user
implicit feedback is better evidence. The PLSI-
PSEUDO also performed well that indicates the
top documents is useful for revealing the topic
of queries, even though they do not satisfy user
need on occasion. TB also gained better perfor-
mance than PSEUDO and HISTORY. It indicates
1173
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 2: The overall average performance of sys-
tems, when each test query matches some user
past interests
highlighting relevant parts in search history helps
to improve the retrieval performance, when the
query matches some of user past interests. Com-
pared with default system, both HISTORY and
PSEUDO improved a lot which proves that the
context in search history is reliable feedback.
For the second case, each user was asked to
hold out 5 interests from his collection for test-
ing and the other interests were used as search
history. The users selected queries from the held
out interests as test queries. These queries did
not match each user?s past interests. We got 244
test queries. As figure 3 shows, though systems
still performed better against ORIGINAL, the im-
provements were not significant. PLSI still gained
the best performance. It has better ability to al-
leviate the effect of noise. HISTORY and PLSI
are more robust than PLSI-PSEUDOwhich seems
sensitive to the number of topics in this case.
In both cases, HISTORY gained moderate per-
formance but quite robust. It is still a very strong
baseline, though noisy information is not filtered
out. PLSI performed best in both cases. PLSI-
PSEUDO outperformed PSEUDO when the test
queries matched user past interests and gained
comparable results in second case. It shows that
modeling user search history as a mixture of top-
ics and weighting topics according to relevance
between topics and query help to update a better
query model. However, it is necessary to deter-
mine if a query matches past interests that helps
to optimize personalized search strategies.
0.49
0.5
0.51
0.52
0.53
0.54
0.55
0.56
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
0.51
0.514
0.518
0.522
0.526
0.53
10 20 30 40 50 60 70 80 90 100
Number of topics
ND
CG
PLSI PLSI-PSEUDO PSEUDO
HISTORY TB ORIGINAL
 
Figure 3: The overall average performance of sys-
tems, when each test query does not match any
user past interest.
5 Conclusion and Future Work
In this paper, we have proposed a topic model
based method for personalized search. This ap-
proach has some advantages: first, it provides a
principled way to combine topic modeling and
personalized search; second, it is able to find user
preferences in an unsupervised way and gives an
informative summary of user search history; third,
it explores the underlying relationship between
different query units via topics that helps to filter
out the noise and improve ranking quality.
In future, we plan to do a large scale study by
leveraging the already built search system or busi-
ness search engines. Also, we will try to add more
information to extend the existing model. Besides,
it is necessary to design methods for determin-
ing whether a submitted query matches the user
past interests that is crucial to apply our algorithm
adaptively and selectively.
Acknowledgements
This research is supported by the National Nat-
ural Science Foundation of China under Grant
No. 60736044, by the National High Technol-
ogy Research and Development Program of China
No. 2008AA01Z144, by Key Laboratory Opening
Funding of MOE-Microsoft Key Laboratory of
Natural Language Processing and Speech, Harbin
Institute of Technology, HIT.KLOF.2009020. We
thank the anonymous reviewers and Fikadu
Gemechu for their useful comments and help.
1174
References
David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Dempster, A.P., Laird N.M. and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of Royal Statist. Soc. B,
39:1?38.
Dou, Z., Su R. and J. Wen. 2007. A large-scale evalu-
ation and analysis of personalized search strategies.
Proc. WWW, pages 581?590.
Eugene, A., Eric B. and D. Susan. 2005. Improving
web search ranking by incorporating user behavior
information. Proc.SIGIR, pages 19?26.
Eugene, A. and Zijian Zheng. 2006. Identifying best
bet web search results by mining past user behavior.
Proc.SIGKDD, pages 902?908.
Havelivala, T.H. 2002. Topic-sensitive pagerank.
Proc. WWW, pages 517?526.
Hofmann, T. 1999. Probabilistic latent semantic in-
dexing. Proc.SIGIR, pages 50?57.
Janse, B.J., Spink A. Bateman J. and T. Saracevic.
2000. Real life, real users, and real needs: a study
and analysis of user queries on the web. Information
Processing and Management, 26(2):207?222.
Jarvelin, K. and J. Kekakainen. 2000. Ir evaluation
methods for retrieving highly relevant documents.
Proc.SIGIR, pages 41?48.
Lavrenko, V. and W. Croft. 2001. Relevance based
language models. Proc.SIGIR, pages 120?127.
Pitkow, J., Schutze H. Cass T. Cooley R. Turnbull D.
Edmonds A. Adar E. and T. Breuel. 2002. Person-
alized search. Commun,ACM, 45(9):50?55.
Qiu, F. and J. Cho. 2006. Automatic identification of
user interest for personalized search. Proc.WWW,
pages 727?736.
Shen, X., Tan B. and C. Zhai. 2005a. Context-
sensitive information retrieval using implicit feed-
back. Proc. SIGIR, pages 43?50.
Shen, X., Tan B. and C. Zhai. 2005b. Implicit user
modeling for personalized search. Proc. CIKM,
pages 824?831.
Silverstein, C., Marais H. Henzinger M. and
M. Moricz. 1999. Analysis of a very large web
search engine query log. SIGIR Forum, 33(1):6?12.
Speretta, M. and S. Gauch. 2005. Personalized search
based on user search histories. Proc. WI?05, pages
622?628.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. Handbook of Latent Semantic Analysis.
Erlbaum, Hillsdale, NJ.
Sugiyama, K., Hatano K. and Yoshkawa. M. 2004.
Personalized search based on user search histories.
Proc. WWW, pages 675?684.
Tan, B., Shen X. and C. Zhai. 2006. Mining long-
term search history to improve search accuracy.
Proc.SIGKDD, pages 718?723.
Tan B., Atulya Velivelli, Fang H. and C. Zhai. 2007.
Term feedback for information retrieval with lan-
guage models. Proc.SIGIR, pages 263?270.
Teevan, J., Dumais S.T. and E. Horvitz. 2005. Per-
sonalizing search via automated analysis of interests
and activities. Proc.SIGKDD, pages 449?456.
Teevan, J., Morris M.R. Bush S. 2009. Discover-
ing and using groups to improve personalization.
Proc.WSDM, pages 15?24.
White, R.W., Bailey P. and L. Chen. 2009. Pre-
dicting user interest from contextual information.
Proc.SIGIR, pages 363?370.
Xu, Jinxi and W. Croft. 1999. Cluster-based language
models for distributed retrieval. Proc.SIGIR, pages
254?261.
Xu S., Bao, S. Fei B. Su Z. and Y. Yu. 2008. Exploring
folksonomy for personalized search. Proc.SIGIR,
pages 155?162.
Yan, H., Li J. Zhu j. and B. Peng. 2005. Tian-
wang search engine at trec 2005: Terabyte track.
Proc.TREC.
Zhai, C. and J. Lafferty. 2001a. A study of smooth-
ing methods for language models applied to ad hoc
information retrieval. Proc.SIGIR, pages 334?342.
Zhai, Chengxiang and John Lafferty. 2001b. Model-
based feedback in the language modeling approach
to information retrieval. Proc.CIKM, pages 403?
410.
Zhai, C., Velivelli A. and B. Yu. 2004. A cross-
collection mixture model for comparative text min-
ing. Proc.SIGKDD, pages 743?748.
Zhao, Y. and G. Karypis. 2001. Criterion functions
for document clustering: Experiments and analysis.
Technical Report TR #01?40, Department of Com-
puter Science, University of Minnesota, Minneapo-
lis, MN.
1175
Coling 2010: Demonstration Volume, pages 13?16,
Beijing, August 2010
LTP: A Chinese Language Technology Platform
Wanxiang Che, Zhenghua Li, Ting Liu
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
Harbin Institute of Technology
{car, lzh, tliu}@ir.hit.edu.cn
Abstract
LTP (Language Technology Platform) is
an integrated Chinese processing platform
which includes a suite of high perfor-
mance natural language processing (NLP)
modules and relevant corpora. Espe-
cially for the syntactic and semantic pars-
ing modules, we achieved good results
in some relevant evaluations, such as
CoNLL and SemEval. Based on XML in-
ternal data representation, users can easily
use these modules and corpora by invok-
ing DLL (Dynamic Link Library) or Web
service APIs (Application Program Inter-
face), and view the processing results di-
rectly by the visualization tool.
1 Introduction
A Chinese natural language processing (NLP)
platform always includes lexical analysis (word
segmentation, part-of-speech tagging, named en-
tity recognition), syntactic parsing and seman-
tic parsing (word sense disambiguation, semantic
role labeling) modules. It is a laborious and time-
consuming work for researchers to develop a full
NLP platform, especially for Chinese, which has
fewer existing NLP tools. Therefore, it should be
of particular concern to build an integrated Chi-
nese processing platform. There are some key
problems for such a platform: providing high per-
formance language processing modules, integrat-
ing these modules smoothly, using processing re-
sults conveniently, and showing processing results
directly.
LTP (Language Technology Platform), a Chi-
nese processing platform, is built to solve the
above mentioned problems. It uses XML to trans-
fer data through modules and provides all sorts
? 
? 	

 ? 



? 




Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 172?182, Dublin, Ireland, August 23-29 2014.
Building Large-Scale Twitter-Specific Sentiment Lexicon :
A Representation Learning Approach
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\?
, Ming Zhou
?
, Ting Liu
\
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach. We cast sentiment lexicon learning as a phrase-level sentiment classification
task. The challenges are developing effective feature representation of phrases and obtaining
training data with minor manual annotations for building the sentiment classifier. Specifical-
ly, we develop a dedicated neural architecture and integrate the sentiment information of tex-
t (e.g. sentences or tweets) into its hybrid loss function for learning sentiment-specific phrase
embedding (SSPE). The neural network is trained from massive tweets collected with positive
and negative emoticons, without any manual annotation. Furthermore, we introduce the Urban
Dictionary to expand a small number of sentiment seeds to obtain more training data for building
the phrase-level sentiment classifier. We evaluate our sentiment lexicon (TS-Lex) by applying
it in a supervised learning framework for Twitter sentiment classification. Experiment results
on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than
previously introduced sentiment lexicons.
1 Introduction
A sentiment lexicon is a list of words and phrases, such as ?excellent?, ?awful? and ?not bad?, each
of which is assigned with a positive or negative score reflecting its sentiment polarity and strength.
Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment in-
formation and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012;
Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to
estimate the sentiment score of each phrase. These methods typically employ parsing results, syntac-
tic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between
phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et
al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit
the dependency relations between sentiment words and aspect words. However, parsing information and
the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon
from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and
it is hard to have reliable tweet parsers due to the informal language style.
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level classi-
fication task. Our method contains two part: (1) a representation learning algorithm to effectively learn
the continuous representation of phrases, which are used as features for phrase-level sentiment classifica-
tion, (2) a seed expansion algorithm that enlarge a small list of sentiment seeds to collect training data for
building the phrase-level classifier. Specifically, we learn sentiment-specific phrase embedding (SSPE),
which is a low-dimensional, dense and real-valued vector, by encoding the sentiment information and
?
This work was partly done when the first author was visiting Microsoft Research.
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
172
Sentiment 
Classifier 
Sentiment 
Lexicon 
Phrase Embedding 
NEG: goon looser 
Sentiment Seeds 
Tweets with Emoticons 
Soooo nice~ :) 
It?s horrible :( 
Seed 
Expansion 
Representation 
Learning 
POS: good :) 
NEG: poor :( 
NEU: when he 
Training Data 
POS: wanted fave 
NEU: again place 
[1.31,0.97] good: 
[0.99,1.17] coool: 
[-0.81,-0.7] bad: 
[-0.8,-0.72] mess: 
Learning 
Algorithm 
Figure 1: The representation learning approach for building Twitter-specific sentiment lexicon.
syntactic contexts into the continuous representation of phrases
1
. As a result, the nearest neighbors in the
embedding space of SSPE are favored to have similar semantic usage as well as the same sentiment po-
larity. To this end, we extend the existing phrase embedding learning algorithm (Mikolov et al., 2013b),
and develop a dedicated neural architecture with hybrid loss function to incorporate the supervision from
sentiment polarity of text (e.g. tweets). We learn SSPE from tweets, leveraging massive tweets con-
taining positive and negative emoticons as training set without any manual annotation. To obtain more
training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban
Dictionary
2
, which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we
utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in
the sentiment lexicon.
We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learn-
ing framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the bench-
mark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced
lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the top-
performed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by
regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu
and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding
learning algorithms. The main contributions of this work are as follows:
? To our best knowledge, this is the first work that leverages the continuous representation of phrases
for building large-scale sentiment lexicon from Twitter;
? We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from
massive tweets selected with positive and negative emoticons;
? We report the results that our lexicon outperforms existing sentiment lexicons by applying them in
a supervised learning framework for Twitter sentiment classification.
2 Related Work
In this section, we give a brief review about building sentiment lexicon and learning continuous repre-
sentation of words and phrases.
2.1 Sentiment Lexicon Learning
Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das
and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Tur-
ney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods
1
Word/unigram is also regarded as phrase in this paper.
2
http://www.urbandictionary.com/
173
and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by re-
garding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation
algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran,
2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score
of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are
mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples
from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between senti-
ment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet.
Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al.
(2010) represent words and phrases with their syntactic contexts within a window size from the web
documents. Unlike the dominated propagation based methods, we explore the classification framework
based on representation learning for building large-scale sentiment lexicon from Twitter.
To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual
information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and
:(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expres-
sions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and
sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase
embedding and propose a representation learning approach to build sentiment lexicon.
2.2 Learning Continuous Representation of Word and Phrase
Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al.,
2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al.,
2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et
al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al.,
2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend
the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the
embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing
embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentiment-
specific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike
previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike
Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we
integrate the sentiment information of text into our method. It is worth noting that we focus on learning
the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013)
that learn the compositionality of sentences.
3 Methodology
In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a
classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases
as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed
expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment
distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode
the sentiment information into the continuous representation of phrases, we extend an existing phrase
embedding learning algorithm (Mikolov et al., 2013b) and develop a tailored neural architecture to learn
sentiment-specific phrase embedding (SSPE), as described in subsection 3.1. To automatically obtain
more training data for building the phrase-level sentiment classifier, we use the similar words from Urban
Dictionary to expand a small list of sentiment seeds, as described in subsection 3.2.
3.1 Sentiment-Specific Phrase Embedding
Mikolov et al. (2013b) introduce Skip-Gram to learn phrase embedding based on the context words of
phrases, as illustrated in Figure 2(a).
Given a phrase w
i
, Skip-Gram maps it into its continuous representation e
i
. Then, Skip-Gram utilizes
174
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
polj 
(a) Skip-Gram (b) Our Model 
sej 
sj 
Figure 2: The traditional Skip-Gram model and our neural architecture for learning sentiment-specific
phrase embedding (SSPE).
e
i
to predict the context words of w
i
, namely w
i?2
, w
i?1
, w
i+1
, w
i+2
, et al. Hierarchical softmax (Morin
and Bengio, 2005) is leveraged to accelerate the training procedure because the vocabulary size of phrase
table is typically huge. The objective of Skip-Gram is to maximize the average log probability:
f
syntactic
=
1
T
T
?
i=1
?
?c?j?c,j 6=0
log p(w
i+j
|e
i
) (1)
where T is the occurrence of each phrase in the corpus, c is the window size, e
i
is the embedding of the
current phrase w
i
, w
i+j
is the context words of w
i
, p(w
i+j
|e
i
) is calculated with hierarchical softmax.
The basic softmax unit is calculated as softmax
i
= exp(z
i
)/
?
k
exp(z
k
). We leave out the details
of hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013b) due to the page limit. It is
worth noting that, Skip-Gram is capable to learn continuous representation of words and phrases with
the identical model (Mikolov et al., 2013b).
To integrate sentiment information into the continuous representation of phrases, we develop a tailored
neural architecture to learn SSPE, as illustrated in Figure 2(b). Given a triple ?w
i
, s
j
, pol
j
? as input,
where w
i
is a phrase contained in the sentence s
j
whose gold sentiment polarity is pol
j
, our training
objective is to (1) utilize the embedding of w
i
to predict its context words, and (2) use the sentence
representation se
j
to predict the gold sentiment polarity of s
j
, namely pol
j
. We simply average the
embedding of phrases contained in a sentence as its continuous representation (Huang et al., 2012). The
objective of the sentiment part is to maximize the average of log sentiment probability:
f
sentiment
=
1
S
S
?
j=1
log p(pol
j
|se
j
) (2)
where S is the occurrence of each sentence in the corpus,
?
k
pol
jk
= 1. For binary classification
between positive and negative, the distribution of [0,1] is for positive and [1,0] is for negative. Our final
training objective is to maximize the linear combination of the syntactic and sentiment parts:
f = ? ? f
syntactic
+ (1? ?) ? f
sentiment
(3)
where ? weights the two parts. Accordingly, the nearest neighbors in the embedding space of SSPE are
favored to have similar semantic usage as well as the same sentiment polarity.
We train our neural model with stochastic gradient descent and use AdaGrad (Duchi et al., 2011) to
update the parameters. We empirically set embedding length as 50, window size as 3 and the learning
rate of AdaGrad as 0.1. Hyper-parameter ? is tuned on the development set. To obtain large-scale
training corpus, we collect tweets from April, 2013 through TwitterAPI. After filtering the tweets that
are too short (< 5 words) and removing @user and URLs, we collect 10M tweets (5M positive and 5M
negative) with positive and negative emoticons
3
, which is are utilized as the training data to train our
neural model. The vocabulary size is 750,000 after filtering the 1?4 grams through frequency.
3
We use the emoticons selected by Hu et al. (2013), namely :) : ) :-) :D =) as positive and :( : ( :-( as negative ones.
175
3.2 Seed Expansion with Urban Dictionary
Urban Dictionary is a web-based dictionary that contains more than seven million definitions until March,
2013
4
. It was intended as a dictionary of slang, cultural words or phrases not typically found in standard
dictionaries, but it is now used to define any word or phrase. For each item in Urban Dictionary, there is
a list of similar words contributed by volunteers. For example, the similar words of ?cooool? are ?cool?,
?awesome?, ?coooool?, et al
5
and the similar words of ?not bad? are ?good?, ?ok? and ?cool?, et al
6
.
These similar words are typically semantically close to and have the same sentiment polarity with the
target word. We conduct preliminary statistic on the items of Urban Dictionary from ?a? to ?z?, and
find that there are total 799,430 items containing similar words and each of them has about 10.27 similar
words on average.
We utilize Urban Dictionary to expand little sentiment seeds for collecting training data for building
the phrase-level sentiment classifier. We manually label the top frequent 500 words from the vocabulary
of SSPE as positive, negative or neutral. After removing the ambiguous ones, we obtain 125 positive, 109
negative and 140 neutral words, which are regarded as the sentiment seeds
7
. Afterwards, we leverage
the similar words from Urban Dictionary to expand the sentiment seeds. We first build a k-nearest
neighbors (KNN) classifier by regarding the sentiment seeds as gold standard. Then, we employ the KNN
classifier on the items of Urban Dictionary containing similar words, and predict a three-dimensional
discrete vector [knn
pos
, knn
neg
, knn
neu
] for each item, reflecting the hits numbers of sentiment seeds
with different sentiment polarity in its similar words. For example, the vector value of ?not bad? is
[10, 0, 0], which means that there are 10 positive seeds, 0 negative seeds and 0 neutral seeds occur in
its similar words. To ensure the quality of the expanded words, we set threshold for each category to
collect the items with high quality as expanded words. Take the positive category as an example, we
keep an item as positive expanded word if it satisfies knn
pos
> knn
neg
+ threshold
pos
and knn
pos
>
knn
neu
+ threshold
pos
simultaneously. We empirically set the thresholds of positive, negative and
neutral as 6,3,2 respectively by balancing the size of expanded words in three categories. After seed
expansion, we collect 1,512 positive, 1,345 negative and 962 neutral words, which are used as the training
data to build the phrase-level sentiment classifier. We also tried the propagation methods to expand the
sentiment seeds, namely iteratively added the similar words of sentiment seeds from Urban Dictionary
into the expanded word collection. However, the quantity of expanded words is less than the KNN-based
results and the quality is relatively poor.
After obtaining the training data and feature representation of phrases, we build the phrase-level clas-
sifier with softmax, whose length is two for the positive vs negative case:
y(w) = softmax(? ? e
i
+ b) (4)
where ? and b are the parameters of classifier, e
i
is the embedding of the current phrase w
i
, y(w) is the
predicted sentiment distribution of item w
i
. We employ the classifier to predict the sentiment distribution
of each phrase in the vocabulary of SSPE, and save the phrases as well as their sentiment probability in
the positive (negative) lexicon if the positive (negative) probability is larger than 0.5.
4 Experiment
In this section, we conduct experiments to evaluate the effectiveness of our sentiment lexicon (TS-Lex)
by applying it in the supervised learning framework for Twitter sentiment classification, as given in
subsection 4.1. We also directly evaluate the quality of SSPE as it forms the fundamental component for
building sentiment lexicon. We use SSPE as the feature for sentiment classification of items in existing
sentiment lexicons, as described in subsection 4.2.
4
http://en.wikipedia.org/wiki/Urban Dictionary
5
http://www.urbandictionary.com/define.php?term=cooool
6
http://www.urbandictionary.com/define.php?term=not+bad
7
We will publish the sentiment seeds later.
176
4.1 Twitter Sentiment Classification
Experiment Setup and Dataset We conduct experiments on the benchmark Twitter sentiment classi-
fication dataset (message-level) from SemEval 2013 (Nakov et al., 2013). The training and development
sets were completely released to task participants. However, we were unable to download all the training
and development sets because some tweets were deleted or not available due to modified authorization
status. The statistic of the positive and negative tweets in our dataset are given in Table 1(b). We train
positive vs negative classifier with LibLinear (Fan et al., 2008) with default settings on the training set,
tune parameters -c on the dev set and evaluate on the test set. The evaluation metric is Macro-F1.
(a) Sentiment Lexicons
Lexicon Positive Negative Total
HL 2,006 4,780 6,786
MPQA 2,301 4,150 6,451
NRC-Emotion 2,231 3,324 5,555
TS-Lex 178,781 168,845 347,626
HashtagLex 216,791 153,869 370,660
Sentiment140Lex 480,008 260,158 740,166
(b) SemEval 2013 Dataset
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 1: Statistic of sentiment lexicons and Twitter sentiment classification datasets.
Results and Analysis We compare TS-Lex with HL
8
(Hu and Liu, 2004), MPQA
9
(Wilson et al.,
2005), NRC-Emotion
10
(Mohammad and Turney, 2012), HashtagLex and Sentiment140Lex
11
(Moham-
mad et al., 2013). The statistics of TS-Lex and other sentiment lexicons are illustrated in Table 1(a). HL,
MPQA and NRC-Emotion are traditional sentiment lexicons with a relative small lexicon size. Hashta-
gLex and Sentiment140Lex are Twitter-specific sentiment lexicons. We can find that, TS-Lex is larger
than the traditional sentiment lexicons.
We evaluate the effectiveness of TS-Lex by applying it as the features for Twitter sentiment classifica-
tion in the supervised learning framework (Pang et al., 2002). We conduct experiments in two settings,
namely only utilizing the lexicon features (Unique) and appending lexicon feature to existing feature
sets (Appended). In the first setting, we design the lexicon features as same as the top-performed Twit-
ter sentiment classification system in SemEval2013
12
(Mohammad et al., 2013). For each sentiment
polarity (positive vs negative), the lexicon features are:
? total count of tokens in the tweet with score greater than 0;
? the sum of the scores for all tokens in the tweet;
? the maximal score;
? the non-zero score of the last token in the tweet;
In the second experiment setting, we append the lexicon features to the existing basic feature. We use
the feature sets of Mohammad et al. (2013) excluding the lexicon feature as the basic feature, including
bag-of-words, pos-tagging, emoticons, hashtags, elongated words, etc. Experiment results of the Unique
features and Appended features from different sentiment lexicons on Twitter sentiment classification are
given in Table 2(a).
From Table 2(a), we can find that TS-Lex yields best performance in both Unique and Appended
feature sets among all sentiment lexicons, including two large-scale Twitter-specific sentiment lexicons.
The reason is that the classifier for building TS-Lex utilize (1) the well developed feature representation
of phrases (SSPE), which captures the semantic and sentiment connections between phrases, and (2) the
enlarged sentiment words through web intelligence as training data. HashtagLex and Sentiment140Lex
8
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html#lexicon
9
http://mpqa.cs.pitt.edu/lexicons/subj lexicon/
10
http://www.saifmohammad.com/WebPages/ResearchInterests.html
11
We utilize the unigram and bigram lexicons from HashtagLex and Sentiment140Lex.
12
http://www.saifmohammad.com/WebPages/Abstracts/NRC-SentimentAnalysis.htm
177
(a)
Lexicon Unique Appended
HL 60.49 79.40
MPQA 59.15 76.54
NRC-Emotion 54.81 76.79
HashtagLex 65.30 76.67
Sentiment140Lex 72.51 80.68
TS-Lex 78.07 82.36
(b)
Lexicon Unique
Seed 57.92
Expand 60.69
Lexicon(seed) 74.64
TS-Lex 78.07
Table 2: Macro-F1 on Twitter sentiment classification with different lexicon features.
only utilize the relations between phrases and hashtag/emoticon seeds, yet do not well capture the con-
nections between phrases. In the Unique setting, the performances of the traditional lexicons (HL, MPQA
and NRC-Emotion) are lower than large-scale Twitter-specific lexicons (HashtagLex, Sentiment140Lex
and our lexicon). The reason is that, tweets have the informal language style and contain slangs and di-
verse multi-word phrases, which are not well covered by the traditional sentiment lexicons with a small
size. After incorporating the lexicon feature of TS-Lex into the top-performs system (Mohammad et al.,
2013), we further improve the macro-F1 from 84.70% to 85.65%.
Effect of Seed Expansion with Urban Dictionary To verify the effectiveness of seed expansion
through Urban Dictionary, we conduct experiments by applying (1) sentiment seeds (Seed), (2) words
after expansion (Expand), (3) sentiment lexicon generated from the classifier only utilizing sentiment
seeds as training data (Lexicon(seed)), (4) the final lexicon (TS-Lex) exploiting the expanded words as
training data to build sentiment classifier, to produce lexicon features, and only use them for Twitter
sentiment classification (Unique). From Table 2(b), we find that the performance of sentiment seeds and
expanded words are relatively poor due to their low coverage. Under this scenario, seed expansion yields
2.77% improvement (from 57.92% to 60.69%) on macro-F1. By utilizing the expanded words as training
data to build the phrase-level sentiment classifier, TS-Lex obtains 3.43% improvements on Twitter senti-
ment classification (from 74.64% to 78.07%), which verifies the effectiveness of seed expansion through
Urban Dictionary. In addition, we find that only using a small number of sentiment seeds as the training
data, we can obtain superior performance (74.64%) than all baseline lexicons. This indicates that the
representation learning approach effectively capture the semantic and sentimental connections between
phrases through SSPE, and leverage them for building the sentiment lexicon.
Effect of ? in SSPE We tune the hyper-parameter ? of SSPE on the development set of SemEval 2013,
and study its influence on the performance of Twitter sentiment classification by applying the generated
lexicon as features. We utilize the expanded words as training data to train softmax and only utilize the
lexicon features (Unique) for Twitter sentiment classification. Experiment results with different ? are
illustrated in Figure 3(a).
From Figure 3(a), we can see that that SSPE performs better when ? is in the range of [0.1, 0.3], which
is dominated by the sentiment information. The model with ? = 1 stands for Skip-Gram model. The
sharp decline at ? = 1 indicates the importance of sentiment information in learning sentiment-specific
phrase embedding for building sentiment lexicon.
Discussion In the experiment, we do not apply TS-Lex into the unsupervised learning framework for
Twitter sentiment classification. The reason is that the lexicon-based unsupervised method typically
require the sentiment lexicon to have high precision, yet our task is to build large-scale lexicon (TS-Lex)
with broad coverage. We leave this as the future work, although we may set higher threshold (e.g. larger
than 0.5) to increase the precision of TS-Lex and loose the recall.
4.2 Evaluation of Different Representation Learning Methods
Experiment Setup and Dataset We conduct sentiment classification of items in two traditional senti-
ment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the
178
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.735
0.74
0.745
0.75
0.755
0.76
0.765
0.77
0.775
0.78
0.785
?
Ma
cro
?F1
 
 
TS?Lex
(a) SSPE with different ? on the development set for Twitter
sentiment classification.
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
C&W ReEmbed(C&W) W2V ReEmbed(W2V) MVSA SSPE
MPQAHL
(b) Sentiment classification of lexicons with different embed-
ding learning algrithms.
Figure 3: Experiment results with different settings.
sentiment-specific phrase embedding (SSPE). We train the positive vs negative classifier with LibLin-
ear (Fan et al., 2008). The evaluation metric is the macro-F1 of 5-fold cross validation. The statistics of
HL and MPQA are listed in Table 1(a).
Baseline Embedding Learning Algorithms We compare SSPE with the following embedding learn-
ing algorithms:
(1) C&W. C&W is one of the most representative embedding learning algorithms (Collobert et al.,
2011) for learning word embedding, which has been proven effective in many NLP tasks.
(2) W2V. Mikolov et al. (2013a) introduce Word2Vec for learning the continuous vectors for words
and phrases. We utilize Skip-Gram as it performs better than CBOW in the experiments.
(3) MVSA. Maas et al. (2011) learn word vectors for sentiment analysis with a probabilistic model of
documents utilizing the sentiment polarity of documents.
(4) ReEmbed. Lebret et al. (2013) learn task-specific embedding from existing embedding and task-
specific corpus. We utilize the training set of Twitter sentiment classification as the labeled corpus to
re-embed words. ReEmbed(C&W) and ReEmbed(W2V) stand for the use of different embedding results
as the reference word embedding.
The embedding results of the baseline algorithms and SSPE are trained with the same dataset and
parameter sets.
Results and Analysis Experiment results of the baseline embedding learning algorithms and SSPE are
given in Figure 3(b). We can see that SSPE yields best performance on both lexicons. The reason is that
SSPE effectively encode the sentiment information of tweets as well as the syntactic contexts of phrases
from massive data into the continuous representation of phrases. The performances of C&W and W2V
are relatively low because they only utilize the syntactic contexts of items, yet ignore the sentiment in-
formation of text, which is crucial for sentiment analysis. ReEmbed(C&W) and ReEmbed(W2V) achieve
better performance than C&W and W2V because the sentiment information of sentences are incorporated
into the continuous representation of phrases. There is a gap between ReEmbed and SSPE because SSPE
leverages more sentiment supervision from massive tweets collected by positive and negative emoticons.
5 Conclusion
In this paper, we propose building large-scale Twitter-specific sentiment lexicon with a representation
learning approach. Our method contains two parts: (1) a representation learning algorithm to effectively
learn the embedding of phrases, which are used as features for classification, (2) a seed expansion al-
gorithm that enlarge a small list of sentiment seeds to obtain training data for building the phrase-level
sentiment classifier. We introduce a tailored neural architecture and integrate the sentiment information
of tweets into its hybrid loss function for learning sentiment-specific phrase embedding (SSPE). We
learn SSPE from the tweets collected by positive and negative emoticons, without any manual annota-
179
tion. To collect more training data for building the phrase-level classifier, we utilize the similar words
from Urban Dictionary to expand a small list of sentiment seeds. The effectiveness of our sentiment
lexicon (TS-Lex) has been verified through applied in the supervised learning framework for Twitter
sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-
Lex outperforms previously introduced sentiment lexicons and further improves the top-perform system
in SemEval 2013 with feature combination. In future work, we plan to apply TS-Lex into the unsuper-
vised learning framework for Twitter sentiment classification.
Acknowledgements
We thank Nan Yang, Yajuan Duan and Yaming Sun for their great help. This research was partly sup-
ported by National Natural Science Foundation of China (No.61133012, No.61273321, No.61300113).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200?2204.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. Journal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspec-
tives. IEEE Trans. Pattern Analysis and Machine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity from twitter. In ICWSM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537.
George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word
observations. ICML.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for amazon: Sentiment extraction from small talk on the web.
Management Science, 53(9):1375?1388.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochas-
tic optimization. The Journal of Machine Learning Research, pages 2121?2159.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classifi-
cation. In Proceedings of the 14th ACM international conference on Information and knowledge management,
pages 617?624. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In
ACL, volume 7, pages 442?431.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and applications for sentiment analysis. Communications of the ACM,
56(4):82?89.
Ming Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. 2013. Unsupervised sentiment analysis with emotional signals.
In Proceedings of the International World Wide Web Conference, pages 607?618.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In ACL, pages 873?882. ACL.
180
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.
Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Annual Meeting of the Association for Computa-
tional Linguistics.
R?emi Lebret, Jo?el Legrand, and Ronan Collobert. 2013. Is deep learning really necessary for word embeddings?
NIPS workshop.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the 50th ACL, pages 410?419. ACL, July.
Dekang Lin. 1994. Principar: an efficient, broad-coverage, principle-based parser. In Proceedings of the 15th
conference on COLING, pages 482?488. Association for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies,
5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of
words and phrases and their compositionality. The Conference on Neural Information Processing Systems.
Saif M Mohammad and Peter D Turney. 2012. Crowdsourcing a word?emotion association lexicon. Computa-
tional Intelligence.
Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-the-art in
sentiment analysis of tweets. Proceedings of the International Workshop on Semantic Evaluation.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceed-
ings of the international workshop on artificial intelligence and statistics, pages 246?252.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic
Evaluation, volume 13.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information
retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 79?86.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In IJCAI, volume 9, pages 1199?1204.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational linguistics, 37(1):9?27.
Delip Rao and Deepak Ravichandran. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675?682.
Association for Computational Linguistics.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng, and C.D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions. In Conference on Empirical Methods in Natural Language
Processing, pages 151?161.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher
Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference
on Empirical Methods in Natural Language Processing, pages 1631?1642.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2014. Radical-enhanced chinese
character embedding. arXiv preprint arXiv:1404.4714.
181
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word
embedding for twitter sentiment classification. In Procedding of the 52th Annual Meeting of Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. Annual Meeting of the Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of
reviews. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, pages 777?785. Association for Computational
Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In AAAI/IAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets in a
two-stage framework. In Proceedings of the 51st ACL, pages 1764?1773. ACL.
182
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 497?507, Dublin, Ireland, August 23-29 2014.
Learning Sense-specific Word Embeddings By Exploiting
Bilingual Resources
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in learning word embeddings with neural network language
models (NNLM). However, the majority of previous NNLMs represent each word with a single
embedding, which fails to capture polysemy. In this paper, we address this problem by represent-
ing words with multiple and sense-specific embeddings, which are learned from bilingual parallel
data. We evaluate our embeddings using the word similarity measurement and show that our ap-
proach is significantly better in capturing the sense-level word similarities. We further feed our
embeddings as features in Chinese named entity recognition and obtain noticeable improvements
against single embeddings.
1 Introduction
Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector rep-
resentations for words. Each dimension of word embedding represents a latent feature of the word, hope-
fully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightfor-
wardly for computing word similarities, which benefits many practical applications (Socher et al., 2011;
Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al.,
2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013).
In recent years, neural network language models (NNLMs) have become popular architectures for
learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most
of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an
attempt to better capture the multiple senses or usages of a word, several multi-prototype models have
been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply
induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed
value. These models still may not capture the real senses of words, because different words may have
different number of senses.
We present a novel and simple method of learning sense-specific word embeddings by using bilingual
parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs.
We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the
source language with different senses is supposed to have different translations in the foreign language.
1
For instance,?? can be translated as investment / overpower / subdue / subjugate / uniform, etc. Among
all of these translations, subdue / overpower / subjugate express the same sense of??, whereas uniform
/ investment express a different sense. Therefore, we could effectively obtain the senses of one word by
clustering its translation words, exhibiting different senses in different clusters.
The created clusters are then projected back into the words in the source language texts, forming a
sense-labeled training data. The sense-labeled data are then trained with recurrent neural network langu-
gae model (RNNLM) (Mikolov, 2012), a kind of NNLM, to obtain sense-specific word embeddings. As
a concrete example, Figure 1 illustrates the process of learning sense-specific embeddings.
?
Email correspondence.
1
In this paper, source language refers to Chinese, whereas foreign language refers to English.
This work is licenced under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
497
? Cluster
Monolingualsense-labeled data
? ?? #2 , ?? ? ?? ?? ?1
?? ? ? ?? #1  ? ?? ?2
? ?? #2 ? ?? ?3
? ?? ? ?? ?? ?? ?  ?? #1 ?4
??5
? Project
? Extract
? RNNLM
Bilingual data(E: English, C: Chinese)
E: ? subdue , conquer or control the opponent .
C: ? ?? , ?? ? ?? ?? ?1
E: The workers wearing the factory 's uniform ? 
C: ?? ? ? ?? ? ?? ? 2
E: She overpowered the burglars .
C: ? ?? ? ?? ?3
E: They wore their priestly vestment in Church .
C: ? ?? ? ?? ?? ?? ? ?? ?4
??5
SL word ??
Translations
subdueuniformoverpowersubjugatevestment??
uniformvestment
overpowersubjugate
subdue
??#1
??#2
(clothes)
(defeat)
Sense-specific word embeddings
< v1#1, v2#1, ..., vN#1 >
< v1#2, v2#2, ..., vN#2 >
?? #1?? #2
Figure 1: An illustration of the proposed method. SL stands for source language.
To evaluate the sense-specific word embeddings we have learned, we manually construct a Chinese
polysemous word similarity dataset that contains 401 pairs of words with human-judged similarities. The
performance of our method on this dataset shows that sense-specific embeddings are significantly better
in capturing the sense-level similarities for polysemous words.
We also evaluate our embeddings by feeding them as features to the task of Chinese named entity
recognition (NER), which is a simple semi-supervised learning mechanism (Turian et al., 2010). In or-
der to use sense-specific embeddings as features, we should discriminate the word senses for the NER
data first. Therefore, we further develop a novel monolingual word sense disambiguation (WSD) algo-
rithm based on the RNNLM we have already trained previously. NER results show that sense-specific
embeddings provide noticeable improvements over traditional single embeddings.
Our contribution in this paper is twofold:
? We propose a novel approach of learning sense-specific word embeddings by utilizing bilingual
parallel data (Section 3). Evaluation on a manually constructed polysemous word similarity dataset
shows that our approach better captures word similarities (Section 5.2).
? To use the sense-specific embeddings in practical applications, we develop a novel WSD algorithm
for monolingual data based on RNNLM (Section 4). Using the algorithm, we feed the sense-specific
embeddings as additional features to NER and achieve significant improvement (Section 5.3).
2 Background: Word Embedding and RNNLM
There has been a line of research on learning word embeddings via NNLMs (Bengio et al., 2003; Mnih
and Hinton, 2008; Mikolov et al., 2013b). NNLMs are language models that exploit neural networks to
make probabilistic predictions of the next word given preceding words. By training NNLMs, we obtain
both high performance language models and word embeddings.
Following Mikolov et al. (2013b), we use the recurrent neural network as the basic framework for train-
ing NNLMs. RNNLM has achieved the state-of-the-art performance in language modeling (Mikolov,
2012) and learned effective word embeddings for several tasks (Mikolov et al., 2013b). The architecture
of RNNLM is shown in Figure 2.
The input layer of RNNLM consists of two components: w(t) and h(t ? 1). w(t) is the one-hot
representation of the word at time step t,
2
h(t ? 1) is the output of hidden layer at the last time step.
Therefore, the input encodes all previous history when predicting the next word at time step t. Compared
2
A feature vector of the same size of the vocabulary, and only one dimension is on.
498
w(t)
y(t)
U
W
V
h(t)
h(t-1)
Figure 2: The basic architecture of RNNLM.
with other feed-forward NNLMs, the RNNLM can theoretically represent longer context patterns. The
output y(t) represents the probability distribution of the next word p(w(t + 1)|w(t),h(t ? 1)). The
output values are computed as follows:
h(t) = f(Uw(t) +Wh(t? 1)) (1)
y(t) = g(Vh(t)) (2)
where f is a sigmoid function and g is a softmax function.
The RNNLM is trained by maximizing the log-likelihood of the training data using stochastic gradi-
ent descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the
gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word.
As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in
natural languages and induce a single embedding for each word. We address this issue and introduce an
effective approach for capturing polysemy in the next section.
3 Sense-specific Word Embedding Learning
In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al.
(1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples
of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract
the translations of the source language words from bilingual data (?). Since there may be multiple
translations for the same sense of a source language word, it is straightforward to cluster the translation
words, exhibiting different senses in different clusters (?).
Once word senses are effectively induced for each word, we are able to form the sense-labeled training
data of RNNLMs by tagging each word occurrence in the source language text with its associated sense
cluster (?). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a
standard manner (?).
3.1 Translation Words Extraction
Given bilingual data after word alignment, we present a way of extracting translation words for source
language words by exploiting the translation probability produced by word alignment models (Brown et
al., 1993; Och and Ney, 2003; Liang et al., 2006).
More formally, we notate the Chinese sentence as c = (c
1
, ..., c
I
) and English sentence as e =
(e
1
, ..., e
J
). The alignment models can be generally factored as:
p(c|e) =
?
a
p(a, c|e) (3)
p(a, c|e) =
?
J
j=1
p
d
(a
j
|a
j?
, j)p
t
(c
j
|e
a
j
) (4)
where a is the alignment specifying the position of an English word aligned to each Chinese word,
p
d
(a
j
|a
j?
, j) is the distortion probability, and p
t
(c
j
|e
a
j
) is the translation probability which we use.
499
SL Word Translation Words Translation Word Clusters Nearest Neighbours
?? investment, overpower,
subdue, subjugate, uniform
investment, uniform ??
dress
,??
policeman uniform
subdue, subjugate, overpower ??
defeat
,??
beat
,??
conquer
? blossom, cost, flower,
spend, take, took
flower, blossom ?
greens
,?
leaf
,??
fruit
take, cost, spend ??
cost
,??
save
,??
rest
? act, code, France,
French, law, method
France, French ?
Germany
,?
Russia
,?
Britain
law, act, code ??
ordinance
,??
bill
,??
rule
method ??
concept
,??
scheme
,??
way
??
lead, leader, leadership
leader, leadership ??
chief
,??
boss
,??
chairman
lead ??
supervise
,??
decision
,??
work
Table 1: Results of our approach on a sample of polysemous words. The second column lists the extracted
translation words of the source language word (Section 3.1). The third column lists the clustering results
using affinity propagation (Section 3.2). The last column lists the nearest neighbour words computed
using the learned sense-specific word embeddings (Section 5.2.2).
In this paper, we use the alignment model proposed by Liang et al. (2006). We utilize the bidirectional
translation probabilities for the extraction of translations, where a foreign language word w
e
is deter-
mined as a translation of source language word w
c
only if both translation probabilities p
t
(w
c
|w
e
) and
p
t
(w
e
|w
c
) exceed some threshold 0 < ? < 1.
The second column of Table 1 presents the extraction results on a sample of source language words
with the corresponding translation words.
3.2 Clustering of Translation Words
For each source language word, its translation words are then clustered so as to separate different senses.
At the clustering time, we first represent each translation word with a feature vector (point), so that
we can measure the similarities between points. Then we perform clustering on these feature vectors,
representing different senses in different clusters.
Different from Apidianaki (2008) who represents all occurrences of the translation words with their
contexts in the foreign language for clustering, we adopt the embeddings of the translation words as
the representations and directly perform clustering on the translation words,
3
rather than the contexts of
occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich
lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation
of the translation words leads to extremely high-efficiency clustering, because the number of translation
words is orders of magnitude less than their occurrences.
Moreover, since the number of senses of different source language words is varied, the commonly-
used k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propaga-
tion (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one
of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the con-
cept of ?message passing?. AP has the major advantage that the number of the resulting clusters is
dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering
approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the
number of resulting clusters automatically without using any partition criterions.
The third column of Table 1 lists the resulting clusters of the translation words for the sampled pol-
ysemous words. We can see that the resulting clusters are meaningful: senses are well represented by
clusters of translation words.
3.3 Cross-lingual Word Sense Projection
The produced clusters are then projected back into the source language to identify word senses.
3
The publicly available word embeddings proposed by Collobert et al. (2011) are used.
500
For each occurrence w
o
of the word w in the source language corpora, we first select the aligned word
with the highest marginal edge posterior (Liang et al., 2006) as its translation. We then identify the sense
of w
o
by computing the similarities of its translation word with each exemplar of the clusters, and select
the one with the maximum similarity. When w
o
is aligned with NULL, we heuristically identify its sense
as the most frequent sense of w that appears in the bilingual dataset.
After projecting the word senses into the source language, we obtain a sense-labeled corpus, which is
used to train the sense-specific word embeddings with RNNLM. The training process is exactly the same
as single embeddings, except that the words in our training corpus has been labeled with senses.
4 Application of Sense-specific Word Embeddings
One of the attractive characteristic of word embeddings is that they can be directly used as word features
in various NLP applications, including NER, chunking, etc. Despite of the usefulness of word embed-
dings on these applications, previous work seldom concerns that words may have multiple senses, which
cannot be effectively represented with single embeddings. In this section, we address this problem by
utilizing sense-specific word embeddings.
We take the task of Chinese NER as a case study. Intuitively, word senses are important in NER. For
instance,? is likely to be an NE of Location when it refers to America. However, when it expresses the
sense of beautiful, it should not be an NE.
Using sense-specific word embedding features for NER is not as straightforward as using single em-
beddings. For each word in the NER data, we first need to determine the correct word sense of it, which
is a typical WSD problem. Then we use the embedding which corresponds to that sense as features.
Here we treat WSD as a sequence labeling problem, and solve it with a very natural algorithm based on
RNNLM we have already trained (Section 3).
4.1 RNNLM-based Word Sense Disambiguation
Given the automatically induced word sense inventories and the RNNLM which has already been trained
on the sense-labeled data of source language, we first develop a greedy decoding algorithm for the
sequential WSD, which works deterministically. Then we improve it using beam search.
Greedy. For word w, we denote the sense-labeled w as w
s
k
, where s
k
represents the k
th
sense of w.
In each step, a single decision is made and the sense of next word (w(t + 1)) which has the maximum
RNNLM output is chosen, given the current (sense-labeled) word w(t)
s
?
and the hidden layer h(t? 1)
at the last time step as input. We simply need to compute a shortlist of y(t) associated with w(t + 1),
that is, y(t)|
w(t+1)
at each step. This process is illustrated in Figure 3.
Beam search. The greedy procedure described above can be improved using a left-to-right beam
search decoding for obtaining a better sequence. The beam-search decoding algorithm keeps B different
sequences of decisions in the agenda, and the sequence with the best overall score is chosen as the final
sense sequence.
Note that the dynamic programming decoding (e.g. viterbi) is not applicable here, because of the
recurrent characteristic of RNNLM. At each step, decisions made by RNNLM depends on all previous
decisions instead of the previous state only, hence markov assumption is not satisfied.
5 Experiments
5.1 Experimental Settings
The Chinese-English parallel datasets we use include LDC03E24, LDC04E12 (1998), the IWSLT 2008
evaluation campaign dataset and the PKU 863 parallel dataset. All corpora are sentence-aligned. After
cleaning and filtering the corpus,
4
we obtain 918,681 pairs of sentences (21.7M words).
In this paper, we use BerkeleyAligner to produce word alignments over the parallel dataset.
5
Berke-
leyAligner also gives translation probabilities and marginal edge posterior probabilities. We adopt the
4
Sentences that are too long (more than 40 words) or too short (less than 10 words) are discarded.
5
code.google.com/p/berkeleyaligner/
501
h(t-1)
U
W
V
h(t)
Next word (sense-labeled)Last word(sense-labeled)
w(t)s*
w(t+1)s1
y(t)|w(t+1)
w(t+1)s2
w(t+1)sK
?
y(t)
?
w(t+1)s*
max
w(t)s* w(t+1)s*
w(t+1)s* w(t+2)s*
w(t+2)s* w(t+3)s*
h(t)
h(t+1)
h(t+2)
h(t-1)
Shortlist? 
Figure 3: Using RNNLM for WSD by sequential labeling (left). Decision at each step of the RNNLM-
based WSD algorithm (right).
scikit-learn tool (Pedregosa et al., 2011) to implement the AP clustering algorithm.
6
The AP algorithm
is not fully automatic in deciding the cluster number. There is a tunable parameter calls preference. A
preference with a larger value encourages more clusters to be produced. We set the preference at the
median value of the input similarity matrix to obtain a moderate number of clusters. The rnnlm toolkit
developed by Mikolov et al. (2011) is used to train RNNLM and obtain word embeddings.
7
We induce
both single and sense-specific embeddings with 50 dimensions. Finally, We obtain embeddings of a
vocabulary of 217K words, with a proportion of 8.4% having multiple sense clusters.
5.2 Evaluation on Word Similarity
Word embeddings can be directly used for computing similarities between words, which benefits many
practical applications. Therefore, we first evaluate our embeddings using a similarity measurement.
Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010):
MaxSim(u, v) = max
1?i?k
u
,1?j?k
v
s(u
i
, v
j
) (5)
AvgSim(u, v) =
1
k
u
?k
v
?
k
u
i=1
?
k
v
j=1
s(u
i
, v
j
) (6)
where k
u
and k
v
are the number of the induced senses for words u and v, respectively. s(?, ?) can be any
standard similarity measure. In this study, we use the cosine similarity.
Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and
Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words,
and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word
similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the
research community, we manually construct a Chinese polysemous word similarity dataset.
5.2.1 Chinese Polysemous Word Similarity Dataset Construction
We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese
knowledge database that maintains comprehensive semantic definitions for each word in Chinese. The
process of the dataset construction includes three steps: (1) Commonly used polysemous words are
extracted according to their sense definitions in HowNet. (2) For each polysemous word, we select
several other words to form word pairs with it. (3) Each word pair is manually annotated with similarity.
In step (1), we mainly took advantage of HowNet for the selection of polysemous words. However,
the synsets defined in HowNet are often too fine-grained and many of them are difficult to distinguish,
6
scikit-learn.org
7
www.fit.vutbr.cz/
?
imikolov/rnnlm/
502
particularly for non-experts. Therefore, we manually discard those words with senses that are hard to
distinguish.
In step (2), for each polysemous word w selected in step 1, we sample several other words to form
word pairs withw. The sampled words can be roughly divided into two categories: related and unrelated.
The related words are sampled manually. They can be the hypernym, hyponym, sibling, (near-)synonym,
antonym, or topically related to one sense of w. The unrelated words are sampled randomly.
In step (3), we ask six graduate students who majored in computational linguistics to assign each word
pair a similarity score. Following the setting of WordSim-353, we restrict the similarity score in the range
(0.0, 10.0). To address the inconsistency of the annotations, we discard those word pairs with a standard
deviation greater than 1.0. We end up with 401 word pairs annotated with acceptable consistency. Unlike
the WordSim-353, in which most of the words are nouns, the words in our dataset are more diverse in
terms of part-of-speech tags.
Table 2 lists a sample of word pairs with annotated similarities from the dataset. The whole evaluation
dataset will be publicly available for the research community.
8
Word Paired word Category Mean.Sim Std.Dev
?? ??conquer synonym 8.60 0.29
??
key point
unrelated 0.12 0.19
? ?enter autonym 7.90 0.97
??
publish
near-synonym 7.86 0.76
? ?plant stem sibling 7.80 0.12
??
cost
topic-related 5.86 0.90
? ??
food
hypernym 6.50 0.71
Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. Mean.Sim
represents the mean similarity of the annotations, Std.Dev represents the standard deviation.
5.2.2 Evaluation Results
Following Zou et al. (2013), we use Spearman?s ? correlation and Kendall?s ? correlation for evaluation.
The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly
outperforms the single-version using either MaxSim or AvgSim measurement.
For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et
al. (2012), which was first presented by Sch?utze (1998). The occurrences of a word are represented by
the average embeddings of its context words. Following Huang et al.?s settings, we use a context window
of size 10 and all occurrences of a word are clustered using the spherical k-means algorithm, where k is
tuned with a development set and finally set to 2.
System
MaxSim AvgSim
? ?100 ? ?100 ? ?100 ? ?100
Ours 55.4 40.9 49.3 35.2
SingleEmb 42.8 30.6 42.8 30.6
Multi-prototype 40.7 29.1 38.3 27.4
Table 3: Spearman?s ? correlation and Kendall?s ? correlation evaluated on the polysemous dataset.
Surprisingly, the multi-prototype method performs even slightly worse than the single-version, which
suggests that learning a fixed number of embeddings for every word may even harm the embedding.
Additionally, the clustering process of the multi-prototype approach suffers from high memory and time
cost, especially for the high-frequency words.
8
ir.hit.edu.cn/
?
jguo
503
To obtain intuitive insight into the superior performance of sense-specific embeddings, we list in the
last column of Table 1 the nearest neighborhoods of the sampled words in the evaluation dataset. The list
shows that we are able to find the different meanings of a word by using sense-specific embeddings.
5.3 Application on Chinese NER
We further apply the sense-specific embeddings as features to Chinese NER. We first perform WSD on
the NER data using the algorithm introduced in Section 4. For beam search decoding, the beam size B
is tuned on a development set and is finally set to 16.
We conduct our experiments on data from People?s Daily (Jan. and Jun. 1998).
9
The original corpus
contains seven NE types.
10
In this study, we select the three most common NE types: Person, Location,
Organization. The data from January are chosen as the training set (37,426 sentences). The first 2,000
sentences from June are chosen as the development set and the next 8,000 sentences as the test set.
CRF models are used in our NER system and are optimized by L2-regularized SGD. We use the
CRFSuite (Okazaki, 2007) because it accepts feature vectors with numerical values. The state-of-the-art
features (Che et al., 2013) are used in our baseline system. For both single and sense-specific embedding
features, we use a window size of 4 (two words before and two words after).
5.3.1 Results
Table 4 demonstrates the performance of NER on the test set. As desired, the single embedding features
improve the performance of our baseline, which were also shown in (Turian et al., 2010). Furthermore,
the sense-specific embeddings outperform the single word embeddings by nearly 1% F-score (88.56 vs.
87.58), which is statistically significant (p-value < 0.01 using one-tail t-test).
System P R F
Baseline 93.27 81.46 86.97
+SingleEmb 93.55 82.32 87.58
+SenseEmb (greedy) 93.38 83.56 88.20
+SenseEmb (beam search) 93.59 84.05 88.56
Table 4: Performance of NER on test data.
According to our hypothesis, the sense-specific embeddings should bring considerable improvements
to the NER of polysemous words. To verify this, we evaluate the per-token accuracy of the polysemous
words in the NER test data. We again adopt HowNet to determine the polysemy. Words that are defined
with multiple senses are selected as test set. Figure 4 shows that the sense-specific embeddings indeed
improve the NE recognition of the polysemous words, whereas the single embeddings even decrease the
accuracy slightly. We also obtain improvements on the NE recognition of the monosemous words, which
provide evidences that more accurate prediction of polysemous words is beneficial for the prediction of
the monosemous words through contextual influence.
6 Related Work
Previous studies have explored the NNLMs, which predict the next word given some history or future
words as context within a neural network architecture. Schwenk and Gauvain (2002), Bengio et al.
(2003), Mnih and Hinton (2007), and Collobert et al. (2011) proposed language models based on feed-
forward neural networks. Mikolov et al. (2010) studied language models based on RNN, which managed
to represent longer history information for word-predicting and demonstrated outstanding performance.
Besides, researchers have also explored the word embeddings learned by NNLMs. Collobert et al.
(2011) used word embeddings as the input of various NLP tasks, including part-of-speech tagging,
chunking, NER, and semantic role labeling. Turian et al. (2010) made a comprehensive comparison
of various types of word embeddings as features for NER and chunking. In addition, word embeddings
9
www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp
10
Person, Location, Organization, Date, Time, Number and Miscellany
504
Polysemous(2) Polysemous(3) Monosemous
per?t
oken
 acc
urac
y
0.60
0.65
0.70
0.75
0.80
0.85
0.90 Baseline +SingleEmb +SenseEmb
Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polyse-
mous(k) represents the set of words that have more than or equal to k senses defined in HowNet.
are shown to capture many relational similarities, which can be recovered by vector arithmetic in the
embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013)
learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to
constrain translational equivalence.
Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of
languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010)
and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each
word. They did this by clustering the contexts of words. These multi-prototype models simply induced
a fixed number of embeddings for every word, regardless of the real sense capacity of the specific word.
There has been a lot of work on using bilingual resources for word sense disambiguation (Gale et
al., 1992; Chan and Ng, 2005). By using aligned bilingual data along with word sense inventories such
as WordNet, training examples for WSD can be automatically gathered. We employ this idea for word
sense induction in our study, which is free of any pre-defined word sense thesaurus.
The most similar work to our sense induction method is Apidianaki (2008). They presented a method
of sense induction by clustering all occurrences of each word?s translation words. In their approach,
occurrences are represented with their contexts. We suggest that clustering contexts suffer from high
memory and time cost, as well as data sparsity. In our method, by clustering the embeddings of transla-
tion words, we induce word senses much more efficiently.
To evaluate word similarity models, researchers often apply a dataset with human-judged similarities
on word pairs, such as WordSim-353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG
(Rubenstein and Goodenough, 1965) and Jin and Wu (2012). For context-based multi-prototype mod-
els, (Huang et al., 2012) constructs a dataset with context-dependent word similarity. To the best of
our knowledge, there is no publicly available datasets for context-unaware polysemous word similarity
evaluation yet. This paper fills this gap.
7 Conclusion
This paper presents a novel and effective approach of producing sense-specific word embeddings by
exploiting bilingual parallel data. The proposed embeddings are expected to capture the multiple senses
of polysemous words. Evaluation on a manually annotated Chinese polysemous word similarity dataset
shows that the sense-specific embeddings significantly outperforms the single embeddings and the multi-
prototype approach.
Another contribution of this study is the development of a beam-search decoding algorithm based on
RNNLM for monolingual WSD. This algorithm bridges the proposed sense-specific embeddings and
practical applications, where no bilingual information is provided. Experiments on Chinese NER show
that the sense-specific embeddings indeed improve the performance, especially for the recognition of the
polysemous words.
505
Acknowledgments
We are grateful to Dr. Zhenghua Li, Yue Zhang, Shiqi Zhao, Meishan Zhang and the anonymous review-
ers for their insightful comments and suggestions. This work was supported by the National Key Basic
Research Program of China via grant 2014CB340503 and 2014CB340505, the National Natural Science
Foundation of China (NSFC) via grant 61370164.
References
Marianna Apidianaki. 2008. Translation-oriented word sense induction based on parallel corpora. In In Proceed-
ings of the 6th Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. The Journal of Machine Learning Research, 3:1137?1155.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In AAAI,
volume 5, pages 1037?1042.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zhendong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116?
131.
Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science,
315(5814):972?976.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierar-
chies via word embeddings. In Proceedings of the 52th Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, Baltimore MD, USA.
William A Gale, Kenneth W Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense
disambiguation methods. In Proceedings of the 4th International Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 101?112.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4: evaluating chinese word similarity. In Proceedings of the
First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,
pages 374?377.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composi-
tion in distributional semantics. CoNLL-2013, pages 114?123.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING 2012, pages 1459?1474, Mumbai, India, December.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference
on Human Language Technology Conference of the North American Chapter of the Association of Computa-
tional Linguistics, pages 104?111.
Tomas Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In Proceedings of Interspeech, pages 1045?1048.
506
Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J
?
Cernock`y. 2011. Rnnlm-recurrent neural
network language modeling toolkit. In Proc. of the 2011 ASRU Workshop, pages 196?201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word
representations. In Proceedings of NAACL-HLT, pages 746?751.
Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Ph. D. thesis, Brno
University of Technology.
George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and
cognitive processes, 6(1):1?28.
Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th international conference on Machine learning, pages 641?648.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in
neural information processing systems, pages 1081?1088.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.
chokkan. org/software/crfsuite.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117.
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connectionist language modeling for large vocabulary continuous
speech recognition. In Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Confer-
ence on, volume 1, pages 765?768.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing
Systems, pages 801?809.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound embedding features for semi-
supervised learning. In Proceedings of NAACL-HLT, pages 563?568.
Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for
phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1393?1398, Seattle, Washington, USA, October.
507
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 530?540, Dublin, Ireland, August 23-29 2014.
Jointly or Separately: Which is Better for
Parsing Heterogeneous Dependencies?
Meishan Zhang
?
, Wanxiang Che
?
, Yanqiu Shao
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Beijing Language and Culture University
yqshao163@163.com
Abstract
For languages such as English, several constituent-to-dependency conversion schemes are pro-
posed to construct corpora for dependency parsing. It is hard to determine which scheme is
better because they reflect different views of dependency analysis. We usually obtain dependen-
cy parsers of different schemes by training with the specific corpus separately. It neglects the
correlations between these schemes, which can potentially benefit the parsers. In this paper, we
study how these correlations influence final dependency parsing performances, by proposing a
joint model which can make full use of the correlations between heterogeneous dependencies,
and finally we can answer the following question: parsing heterogeneous dependencies jointly
or separately, which is better? We conduct experiments with two different schemes on the Penn
Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that joint-
ly parsing heterogeneous dependencies can give improved performances for both schemes over
the individual models.
1 Introduction
Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008;
Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et
al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependen-
cy parser is usually constructed according to a specific constituent-to-dependency conversion scheme.
Several conversion schemes for certain languages have been available. For example, the English lan-
guage has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Ya-
mada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme
(de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues, 2007). There are dif-
ferent conversion schemes for the Chinese Penn Treebank (CTB) as well, including the Zhang scheme
(Zhang and Clark, 2008) and the Stanford scheme (de Marneffe and Manning, 2008). It is hard to
judge which scheme is more superior, because each scheme reflects a specific view of dependency analy-
sis, and also there is another fact that different natural language processing (NLP) applications can prefer
different conversion schemes (Elming et al., 2013).
Traditionally, we get dependency parsers of different schemes by training with the specific corpus
separately. The method neglects the correlations between these schemes, which can potentially help
different dependency parsers. On the one hand, there are many consistent dependencies across heteroge-
neous dependency trees. Some dependency structures remain constant in different conversion schemes.
Taking the Yamada and the Stanford schemes as an example, overall 70.27% of the dependencies are
identical (ignoring the dependency labels), according to our experimental analysis. We show a concrete
example for the two heterogeneous dependency trees in Figure 1, where six of the twelve dependencies
are consistent in the two dependency trees (shown by the solid arcs).
On the other hand, differences between heterogeneous dependencies can possibly boost the ev-
idences of the consistent dependencies. For example in Figure 1, the dependencies ?do
VC
xthink?
?
Corresponding author.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
530
We do n?t think at this point anything need to be said
SUB
ROOT
VMOD
VC
VMOD NMOD
PMOD
SUB
VMOD
VMOD
VMOD
VC
nsubj
aux
neg
root
prep det
pobj
nsubj
ccomp
aux
auxpass
xcomp
Figure 1: An example to show the differences and similarities of two dependency schemes. The above
dependency tree is based on the Yamada scheme, while the below dependency tree is based on the
Stanford scheme. The solid arcs show the consistent dependencies between the two dependency
trees, while the dashed arcs show the differences between the two trees.
and ?We
nsubj
x think? from the two trees can both be potential evidences to support the dependency
?thinkyat?. Another example, the label ?PMOD? from the Yamada scheme and the label ?pobj? from
the Stanford scheme on a same dependency ?atypoint? can make it more reliable than one alone.
In this paper, we investigate the influences of the correlations between different dependency schemes
on parsing performances. We propose a joint model to parse heterogeneous dependencies from two
schemes simultaneously, so that the correlations can be fully used by their interactions in a single model.
Joint models have been widely studied to enhance multiple tasks in NLP community, including joint
word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark,
2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word
segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed
over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies
together. To our knowledge, we are the first work to investigate joint models on parallel tasks.
We exploit a transition-based framework with global learning and beam-search decoding to imple-
ment the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art
transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the
Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. The results
show that our joint model gives improved performances over the individual baseline models for both
schemes on both English and Chinese languages, demonstrating positive effects of the correlations be-
tween the two schemes. We make the source code freely available at http://sourceforge.net/
projects/zpar/,version0.7.
2 Baseline
Traditionally, the dependency parsers of different schemes are trained with their corpus separately, using
a state-of-the-art dependency parsing algorithm (Zhang and Clark, 2008; Huang et al., 2009; Koo and
Collins, 2010; Zhang and McDonald, 2012; Choi and McCallum, 2013). In this work, we exploit a
transition-based arc-standard dependency parsing model combined with global learning and beam-search
decoding as the baseline. which is initially proposed by Huang et al. (2009). In the following, we give a
detailed description of the model.
In a typical transition-based system for dependency parsing, we define a transition state, which consists
of a stack to save partial-parsed trees and a queue to save unprocessed words. The parsing is performed
incrementally via a set of transition actions. The transition actions are used to change contents of the
stack and the queue in a transition state. Initially, a start state has an empty stack and all words of a
sentence in its queue. Then transition actions are applied to the start state, and change states step by step.
Finally, we arrive at an end state with only one parsed tree on the stack and no words in the queue. We
score each state by its features generated from the historical actions.
531
S1
? ? ?
? ? ?
S
0
? ? ?
S
AR(l)
AL(l)
P
R
Q
0
Q
1
? ? ?
Q
SH
(a) Arc-standard dependency parsing model for a single dependency tree
S
a
1
? ? ?
? ? ?
S
a
0
? ? ?
S
a
AR
a
(l)
AL
a
(l)
P
R
a
Q
a
0
Q
a
1
? ? ?
Q
a
SH
a
S
b
1
? ? ?
? ? ?
S
b
0
? ? ?
S
b
AR
b
(l)
AL
b
(l)
P
R
b
Q
b
0
Q
b
1
? ? ?
Q
b
SH
b
G
u
i
d
e
d
a
G
u
i
d
e
d
b
(b) The joint model based on arc-standard dependency parsing for two dependency trees
Figure 2: Illustrations for the baseline dependency parsing model and our proposed joint model.
In the baseline arc-standard transition system, we define four kinds of actions, as shown in Figure 2(a).
They are shift (SH), arc-left with dependency label l (AL(l)), arc-right with dependency label l (AR(l))
and pop-root (PR), respectively. The shift action shifts the first element Q
0
of the queue onto the stack;
the action arc-left with dependency label l builds a left arc between the top element S
0
and the second
top element S
1
on the stack, with the dependency label being specified by l; the action arc-right with
dependency label l builds a right arc between the top element S
0
and the second top element S
1
on the
stack, with the dependency label being specified by l; and the pop-root action defines the root node of a
dependency tree when there is only one element on the stack and no element in the queue.
During decoding, each state may have several actions. We employ a fixed beam to reduce the search
space. The low-score states are pruned from the beam when it is full. The feature templates in our
baseline are shown by Table 1, referring to baseline feature templates. We learn the feature weights by
the averaged percepron algorithm with early-update (Collins and Roark, 2004; Zhang and Clark, 2011).
3 The Proposed Joint Model
The aforementioned baseline model can only handle a single dependency tree. In order to parse multiple
dependency trees for a sentence, we usually use individual dependency parsers. This method is not
able to exploit the correlations across different dependency schemes. The joint model to parse multiple
dependency trees with a single model is an elegant way to exploit these correlations fully. Inspired by
this, we make a novel extension to the baseline arc-standard transition system, arriving at a joint model
to parse two heterogeneous dependency trees for a sentence simultaneously.
In the new transition system, we double the original transition state of one stack and one queue into
two stacks and two queues, as shown by Figure 2(b). We use stacks S
a
and S
b
and queues Q
a
and Q
b
to save partial-parsed dependency trees and unprocessed words for two schemes a and b, respectively.
Similarly, the transition actions are doubled as well. We have eight transition actions, where four of them
are aimed for scheme a, and the other four are aimed for scheme b. The concrete action definitions are
similar to the original actions, except an additional constraint that actions should be operated over the
corresponding stack and queue of scheme a or b.
We assume that the actions to build a specific tree of scheme a are A
a
1
A
a
2
? ? ?A
a
n
, and the actions to
532
Baseline feature templates
Unigram features
S
0
w S
0
t S
0
wt S
1
w S
1
t S
1
wt N
0
w N
0
t N
0
wt N
1
w N
1
t N
1
wt
Bigram features
S
0
w?S
1
w S
0
w?S
1
t S
0
t?S
1
w S
0
t?S
1
t S
0
w?N
0
w S
0
w?N
0
t S
0
t?N
0
w S
0
t?N
0
t
Second-order features
S
0l
w S
0r
w S
0l
t S
0r
t S
0l
l S
0r
l S
1l
w S
1r
w S
1l
t S
1r
t S
1l
l S
1r
l
S
0l2
w S
0r2
w S
0l2
t S
0r2
t S
0l2
l2 S
0r2
l2 S
1l2
w S
1r2
w S
1l2
t S
1r2
t S
1l2
l2 S
1r2
l2
Third-order features
S
0
t?S
0l
t?S
0l2
t S
0
t?S
0r
t?S
0r2
t S
1
t?S
1l
t?S
1l2
t S
1
t?S
1r
t?S
1r2
t
S
0
t?S
1
t?S
0l
t S
0
t?S
1
t?S
0l2
t S
0
t?S
1
t?S
0r
t S
0
t?S
1
t?S
0r2
t
S
0
t?S
1
t?S
1l
t S
0
t?S
1
t?S
1l2
t S
0
t?S
1
t?S
1r
t S
0
t?S
1
t?S
1r2
t
Valancy features
S
0
wv
l
S
0
tv
l
S
0
wv
r
S
0
tv
r
S
1
wv
l
S
1
tv
l
S
1
wv
r
S
1
tv
r
Label set features
S
0
ws
r
S
0
ts
r
S
0
ws
l
S
0
ts
l
S
1
ws
l
S
1
ts
l
Proposed new feature templates for the joint model
Guided head features
S
0
w?h
guide
S
0
t?h
guide
S
0
wt?h
guide
S
1
w?h
guide
S
1
t?h
guide
h
guide
Guided label features
S
0
w?S
0
l
guide
S
0
t?S
0
l
guide
S
0
wt?S
0
l
guide
S
1
w?S
0
l
guide
S
1
t?S
0
l
guide
S
0
l
guide
S
0
w?S
1
l
guide
S
0
t?S
1
l
guide
S
0
wt?S
1
l
guide
S
1
w?S
1
l
guide
S
1
t?S
1
l
guide
S
1
l
guide
Table 1: Feature templates for the baseline and joint models, where w denotes the word; t denotes the
POS tag; v
l
and v
r
denote the left and right valencies; l denotes the dependency label; s
l
and s
r
denotes
the label sets of the left and right children; the subscripts l and r denote the left-most and the right-most
children, respectively; the subscripts l2 and r2 denote the second left-most and the second right-most
children, respectively; h
guide
denotes the head direction of the top two elements on the processing stack
in the other tree; l
guide
denotes the label of the same word in the other tree.
build a specific tree of scheme b for the same sentence are A
b
1
A
b
2
? ? ?A
b
n
. We use ST
a
0
ST
a
1
? ? ? ST
a
n
and
ST
b
0
ST
b
1
? ? ? ST
b
n
to denote the historical states for the two action sequences, respectively. A sequence of
actions should consist of A
a
1
A
a
2
? ? ?A
a
n
and A
b
1
A
b
2
? ? ?A
b
n
in a joint model. However, one question that
needs to be answered is that, for a joint state (ST
a
i
,ST
b
j
), which action should be chosen as the next step
to merge the two action sequences into one sequence, A
a
i+1
or A
b
j+1
? To resolve the problem, we employ
a parameter t to limit the next action in the joint model. When t is above zero, an action for scheme b
can be applied only if the last action of scheme a is t steps in advance. For example, the action sequence
is A
a
1
A
b
1
A
a
2
A
b
2
? ? ?A
a
n
A
b
n
when t = 1. t can be negative as well, denoting the reverse constraints.
In the joint model, we extract features separately for the two dependency schemes. When the next
action is aimed for scheme a, we will extract features from S
a
and Q
a
, according to baseline feature
templates in Table 1. In order to make use of the correlations between the two dependency parsing trees,
we introduce several new feature templates, shown in Table 1 referring to proposed new feature templates
for the joint model. The new features are based on two kinds of atomic features: the guided head h
guide
and the guided dependency label l
guide
. Assuming that the currently processing scheme is a, when the
top two elements (S
a
0
and S
a
1
) have both found their heads in Guided
b
(the partial-parsed trees of scheme
b), we can fire the atomic feature h
guide
, which denotes the arc direction between S
0
and S
1
in Guide
b
(S
x
0
S
1
, S
y
0
S
1
or other). When S
a
0
or S
a
1
has its dependency label in Guided
b
, we can fire the atomic
feature l
guide
, which denotes the dependency label of S
a
0
or S
a
1
in Guided
b
. Similarly we can extract the
h
guide
and l
guide
from Guide
a
when we are processing scheme b. When t is infinite, we always have
533
the two atomic features, because the other tree is already parsed. Thus the proposed new features can be
the most effective when t = ? and t = ??. In other conditions, the other tree may not be ready for
the new feature extracting. Similar to the baseline model, we use the beam-search decoding strategy to
reduce the search space, and use the averaged perceptron with early-update to learn the feature weights.
We are especially interested in two cases of the joint models when t is infinite (t =? and t = ??),
where the tree of one specified scheme is always processed after the other tree is finished, because the
new features can be most effectively exploited according to the above analysis. We assume that the first
and second processing schemes are s
1
and s
2
respectively, to facilitate the below descriptions. We can see
that the joint model behaves similarly to a pipeline reranking model, in optimizing scheme s
1
?s parsing
performances. First we get K-best (K equals the beam size of the joint model) candidates for scheme s
1
,
and then employ additional evidences from scheme s
2
?s result, to rerank the K-best candidates, obtaining
a better result. The joint model also behaves similarly to a pipeline feature-based stacking model (Li et
al., 2012), in optimizing scheme s
2
?s parsing performances. After acquiring the best result of scheme
s
1
, we can use it to generate guided features to parse dependencies of scheme s
2
. Thus additional
information from scheme s
1
can be imported into the parsing model of scheme s
2
. Different with the
pipeline reranking and the feature-based stacking models, we employ a single model to achieve the two
goals, making the interactions between the two schemes be better performed.
4 Experiments
4.1 Experimental Settings
In order to evaluate the baseline and joint models, we conduct experiments on English and Chinese da-
ta. For English, we obtain heterogeneous dependencies by the Yamada and the Stanford schemes,
respectively. We transform the bracket constituent trees of English sentences into the Yamada dependen-
cies with the Penn2Malt tool,
1
and into the Stanford dependencies with the Stanford parser version
3.3.1.
2
Following the standard splitting of PTB, we use sections 2-21 as the training data set, section 22 as
the development data set, and section 23 as the final test data set. For Chinese, we obtain heterogeneous
dependencies by the Zhang and the Stanford schemes, respectively. The Zhang dependencies are
obtained by the Penn2Malt tool using the head rules from Zhang and Clark (2008), while the Stanford
dependencies are obtained by the Stanford parser version 3.3.1 similar to English.
We use predicted POS tags in all the experiments. We utilize a linear-CRF POS tagger to obtain
automatic POS tags for English and Chinese datasets.
3
We use a beam size of 64 to train dependency
parsing models. We train the joint models with the Yamada or Zhang dependencies being handled
on stack S
a
and queue Q
a
, and the Stanford dependencies being handled on stack S
b
and queue Q
b
,
referring to Section 3. We follow the standard measures of dependency parsing to evaluate the baseline
and joint models, including unlabeled attachment score (UAS), labeled attachment score (LAS) and
complete match (CM). We ignore the punctuation words for all these measures.
4.2 Development Results
4.2.1 Baseline
Table 2 at the subtable ?Baseline? shows the baseline results on the development data set. The perfor-
mances of the Yamada scheme are better than those of the Stanford scheme. The UAS and LAS of
the Yamada scheme are 92.83 and 91.73 respectively, while they are 92.85 and 90.49 for the Stanford
scheme respectively. The results demonstrate that parsing the Stanford dependencies is more difficult
than parsing the Yamada dependencies because of the lower performances of the Stanford scheme.
1
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html.
2
The tool is available on http://nlp.stanford.edu/software/lex-parser.shtml. We use three options to
perform the conversion: ?-basic? and ?-keepPunct?, respectively.
3
The tagging accuracies are 97.30% on the English test dataset and 93.68% on the Chinese test dataset. We thank Hao
Zhang for sharing the data used in Martins et al. (2013) and Zhang et al. (2013a).
534
Model
Yamada Stanford
UAS LAS CM UAS LAS CM
Baseline 92.83 91.73 47.35 92.85 90.49 50.06
The joint models,
where the Yamada dependencies are processed with priority
t = 1 92.65 91.55 46.35 93.11 90.75 50.24
t = 2 92.65 91.57 46.71 93.15 90.77 50.59
t = 3 92.82 91.74 47.12 93.19 90.82 50.76
t = 4 92.89 91.78 47.35 93.27 90.93 51.29
t =? 93.04 92.01 48.65 93.52 91.15 52.59
The joint models,
where the Stanford dependencies are processed with priority
t = ?1 92.62 91.54 46.71 93.10 90.70 50.76
t = ?2 92.50 91.41 46.18 93.06 90.74 51.12
t = ?3 92.57 91.42 47.00 93.10 90.68 51.35
t = ?4 92.74 91.60 47.41 93.15 90.72 51.29
t = ?? 93.04 91.95 47.88 93.19 90.91 50.71
Table 2: The main results on the development data set of the baseline and proposed joint models.
4.2.2 Parameter Tuning
The proposed joint model has one parameter t to adjust. The parameter t is used to control the decoding in
a joint model, determining which kind of dependencies should be processed at the next step. In our joint
model, if t is larger than zero, scheme a (the Yamada scheme) should be handled t steps in advance,
while when t is smaller than zero, scheme b (the Stanford scheme) should be handled in advance.
When the value of t is infinite, the dependency tree of one scheme is handled until the dependency tree
of the other scheme is finished for a sentence.
As shown by Table 2, we have two major findings. First, the joint models are slightly better when t is
above zero, by decoding with the Yamada scheme in advance. The phenomenon demonstrates that the
decoding sequence is important in the joint parsing models. Second, no matter when t is above or below
zero, the performances arrive at the peak when t is infinite. One benefit of the joint models is that we
can use the correlations between different dependency trees, through the new features proposed by us.
The new features can be the most effective when t is infinite according to the analysis Section 3. Thus
this finding indicates that the new features are crucial in the joint models, since the ineffective utilization
would decrease the model performances a lot. Actually, when the absolute value of t is small, the features
can sometimes be fired and in some other times are not able to be fired, making the training insufficient
and also inconsistent for certain word-pair dependencies when their distances can differ (when t = 1 for
example, the joint model can fire the new features only if the dependency distance equals 1). This would
make the final model deficient, and can even hurt performances of the Yamada scheme.
According to the results on the development data set, we use the t = ? for the final joint model,
which first finishes the Yamada tree and then the Stanford tree for each sentence. Our final model
achieves increases of 0.21 on UAS and 0.28 on LAS for the Yamada scheme, and increases 0.67 on
UAS and 0.66 on LAS for the Stanford scheme.
4.2.3 Feature Ablation
In order to test the effectiveness of the proposed new features, we conduct a feature ablation experiment.
Table 3 shows the results, where the mark ?/wo? denotes the model without the new features proposed
by us. For the Yamada scheme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new
features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS,
respectively. The results demonstrate the new features are effective in the joint model.
535
Model
Yamada Stanford
UAS LAS CM UAS LAS CM
Our joint model 93.04 92.01 48.65 93.52 91.15 52.59
Our joint model/wo 92.89 91.80 48.25 92.95 90.57 50.62
? -0.15 -0.21 -0.40 -0.57 -0.58 -1.97
Table 3: Feature ablation results.
Model
Yamada Stanford
UAS LAS CM UAS LAS CM
Baseline 92.71 91.67 47.48 92.72 90.61 47.76
Our joint model 92.89 91.86 48.39 93.30
?
91.19
?
50.37
Zhang and Nivre (2011) 92.9 91.8 48.0 ? ? ?
Rush and Petrov (2012) ? ? ? 92.7
?
? ?
Martins et al. (2013) 93.07 ? ? 92.82
?
? ?
Zhang et al. (2013a) 93.50 92.41 ? 93.64
?
91.28
?
?
Zhang and McDonald (2014) 93.57 92.48 ? 93.71
?
/93.01
??
91.37
?
/90.64
??
?
Kong and Smith (2014) ? ? ? 92.20
??
89.67
??
?
Table 4: The final results on the test data set, where the results with mark
?
demonstrates that the p-value
is below 10
?3
using t-test. Our Stanford dependencies are slightly different with previous works, where
the results with mark
?
show the numbers for the Stanford dependencies from Stanford parser version
2.0.5 and the results with mark
??
show the numbers for the Stanford dependencies from Stanford parser
version 3.3.0.
4.3 Final Results
Table 4 shows our final results on the English test dataset. The final joint model achieves better per-
formances than the baseline models for both the Yamada and the Stanford schemes, by increases
of 0.18 on UAS and 0.19 on LAS for the Yamada scheme, and increases of 0.58 on UAS and 0.58
on LAS for the Stanford scheme. The results demonstrate that the interactions between the two de-
pendency schemes are useful, and the joint model is superior to separately trained models in handling
heterogeneous dependencies.
We compare our results with some representative previous work of dependency parsing as well. Zhang
and Nivre (2011) is a feature-rich transition-based dependency parser using the arc-eager transition sys-
tem. Rush and Petrov (2012), Zhang et al. (2013a) and Zhang and McDonald (2014) are state-of-the-art
graph-based dependency parsers. Martins et al. (2013) and Kong and Smith (2014) report their results
with the full TurboParser. TurboParser is also a graph-based dependency parser but its decoding algo-
rithm has major differences with the general MST-style decoding.
4.4 Analysis
To better understand the joint model, we conduct analysis work on the Chinese development dataset.
First, we make a comparison to see whether the consistent dependencies give larger increases by the
joint model. As mentioned before, the consistent dependencies can be supported by different evidences
from heterogeneous dependencies. We compute the proportion of the consistent dependencies (ignoring
the dependency labels) between the Yamada and the Stanford dependencies, finding that 70.27% of
the overall dependencies are consistent. Table 5 shows the comparison results. The joint model shows
improvements for the consistent dependencies. However, it does not always show positive effectiveness
for the inconsistent dependencies. The results support our initial motivation that consistent dependencies
can benefit much in joint models .
We also make a comparison between the baseline and joint models with respect to dependency dis-
tance. We use the F-measure value to evaluate the performances. The dependency distances are normal-
536
Yamada Stanford
Consistent Inconsistent Consistent Inconsistent
UAS LAS UAS LAS UAS LAS UAS LAS
Baseline 93.43 92.39 91.44 90.17 93.74 91.35 90.75 88.47
Our joint model 93.81 92.85 91.21 90.02 94.58 92.15 91.01 88.78
? +0.38 +0.46 -0.23 -0.15 +0.84 +0.80 +0.36 +0.31
Table 5: Performances of the baseline and joint models by whether the dependencies are consistent
across the Yamada and the Stanford schemes, where the bold numbers denote the larger increases by
comparisons of consistent and inconsistent dependencies for each scheme.
1 2 3 4 5 6 7
75
80
85
90
95
F
-
m
e
a
s
u
r
e
(
%
)
Baseline Joint
(a) Yamada
1 2 3 4 5 6 7
65
75
85
95
F
-
m
e
a
s
u
r
e
(
%
)
Baseline Joint
(b) Stanford
Figure 3: F-measures of the two heterogeneous dependencies with respect to dependency distance.
ized to a max value of 7. Figure 3 shows the comparison results. We find that the joint model can achieve
consistent better performances for the dependencies of different dependency distance, demonstrating the
robustness of the joint model in improving parsing performances. The joint model performs slightly
better for long-distance dependencies, which is more obvious for the Stanford scheme.
4.5 Parsing Heterogeneous Chinese Dependencies
Table 6 shows our final results on the Chinese test data set. For Chinese, the joint model achieves better
performances with Stanford dependencies being parsed first. The final joint model achieves better
performances than the baseline models for both the Zhang and the Stanford schemes, by increases
of 1.13 on UAS and 0.99 on LAS for the Zhang scheme, and increases of 0.30 on UAS and 0.36 on
LAS for the Stanford scheme. The results also demonstrate similar conclusions with the experiments
on English dataset.
5 Related Work
Our work is mainly inspired by the work of joint models. There are a number of successful studies
on joint modeling pipelined tasks where one task is a prerequisite step of another task, for example,
the joint model of word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009;
Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011;
Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et
Model
Zhang Stanford
UAS LAS CM UAS LAS CM
Baseline 79.07 76.08 27.96 80.33 75.29 31.14
Our joint model 80.20
?
77.07
?
30.10 80.63 75.65 31.20
Table 6: The final results on the test data set, where the results with mark
?
demonstrates that the p-value
is below 10
?3
using t-test.
537
al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic
analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two
heterogeneous dependency trees simultaneously.
There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to
enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a
specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented
a joint inference framework to combine the parsing results based on two different treebanks. All these
work are case studies of annotation adaptation from different sources, which have been done for Chinese
word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their
work, we study the heterogeneous annotations derived from the same source. We use a unified model to
parsing heterogeneous dependencies together.
Our joint parsing model exploits a transition-based framework with global learning and beam-search
decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang
et al., 2009). The transition-based framework is easily adapted to a number of joint models, including
joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing
(Hatori et al., 2012; Bohnet and Nivre, 2012), and also joint word segmentation, POS-tagging and parsing
(Hatori et al., 2012; Zhang et al., 2013b; Zhang et al., 2014).
6 Conclusions
We studied the effectiveness of the correlations between different constituent-to-dependency schemes
for dependency parsing, by exploiting these information with a joint model to parse two heterogeneous
dependency trees simultaneously. We make a novel extension to a transition-based arc-standard depen-
dency parsing algorithm for the joint model. We evaluate our baseline and joint models on both English
and Chinese datasets, based on the Yamada/Zhang and the Stanford dependency schemes. Final
results demonstrate that the joint model which handles two heterogeneous dependencies can give im-
proved performances for dependencies of both schemes. The source code for the joint model is publicly
available at http://sourceforge.net/projects/zpar/,version0.7.
Acknowledgments
We thank Yue Zhang and the anonymous reviewers for their constructive comments, and grateful-
ly acknowledge the support of the National Basic Research Program (973 Program) of China via
Grant 2014CB340503, the National Natural Science Foundation of China (NSFC) via Grant 61133012,
61170144 and 61370164.
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of the EMNLP-CONLL, pages 1455?1465, Jeju Island,
Korea, July.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich?ard Farkas Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. TACL, 1.
Jinho D. Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In
Proceedings of ACL, pages 1052?1062, August.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the
ACL, pages 111?118, Barcelona, Spain, July.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1?8, Manchester, UK, August.
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi, Hector Martinez Alonso, and Anders
S?gaard. 2013. Down-stream effects of tree-to-dependency conversions. In Proceedings of the NAACL, pages
617?626, Atlanta, Georgia, June.
538
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint POS tagging and
dependency parsing in Chinese. In Proceedings of 5th IJCNLP, pages 1216?1224, Chiang Mai, Thailand,
November.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2012. Incremental joint approach to word
segmentation, POS tagging, and dependency parsing in Chinese. In Proceedings of the 50th ACL, pages 1045?
1053, Jeju Island, Korea, July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In
Proceedings of the EMNLP, pages 1222?1231.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?u. 2008. A cascaded linear model for joint Chinese word
segmentation and part-of-speech tagging. In Proceedings of ACL-08, pages 897?904, Columbus, Ohio, June.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word
segmentation and POS tagging: a case study. In Proceedings of the ACL-IJCNLP, pages 522?530.
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In
Proceedings of NODALIDA 2007, Tartu, Estonia.
Lingpeng Kong and Noah A Smith. 2014. An empirical comparison of parsing methods for stanford dependencies.
arXiv preprint arXiv:1404.4314.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual
Meeting of the ACL, pages 1?11.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara.
2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In
Proceedings of the ACL-IJCNLP, pages 513?521, Suntec, Singapore, August.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for
Chinese POS tagging and dependency parsing. In Proceedings of the EMNLP, pages 1180?1191, Edinburgh,
Scotland, UK., July.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Exploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th ACL, pages 675?684, Jeju Island, Korea, July.
Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proceedings of the 51st ACL, pages 617?622, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, number June, pages 91?98, Morristown, NJ, USA.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932.
Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics,
34(4):513?553.
Alexander M Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Proceed-
ings of the NAACL, pages 498?507.
Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser using a dynamic
parsing strategy. In Proceedings of the 51st ACL, pages 135?144, Sofia, Bulgaria, August.
Weiwei Sun and Xiaojun Wan. 2012. Reducing approximation and estimation errors for Chinese lexical processing
with heterogeneous annotations. In Proceedings of the 50th ACL, pages 232?241, Jeju Island, Korea, July.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT, volume 3.
Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings of EMNLP, pages 562?571, Honolulu, Hawaii, October.
Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single
discriminative model. In Proceedings of the EMNLP, pages 843?852, Cambridge, MA, October.
539
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
Hao Zhang and Ryan McDonald. 2012. Generalized higher-order dependency parsing with cube pruning. In
Proceedings of the EMNLP, pages 320?331.
Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In
Proceedings of ACL. Association for Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of the 49th ACL, pages 188?193, Portland, Oregon, USA, June.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDonald. 2013a. Online learning for inexact hypergraph search.
In Proceedings of the EMNLP, pages 908?913, Seattle, Washington, USA, October. Association for Computa-
tional Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013b. Chinese parsing exploiting characters. In
Proceedings of the 51st ACL, pages 125?134, Sofia, Bulgaria, August.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014. Character-level Chinese Dependency Parsing.
In Proceedings of the 52st ACL.
Guangyou Zhou and Jun Zhao. 2013. Joint inference for heterogeneous dependency parsing. In Proceedings of
the 51st ACL, pages 104?109, Sofia, Bulgaria, August.
540
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 917?927, Dublin, Ireland, August 23-29 2014.
Triple based Background Knowledge Ranking for Document Enrichment
Muyu Zhang, Bing Qin
?
, Ting Liu, Mao Zheng
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{myzhang,qinb,tliu,mzheng}@ir.hit.edu.cn
Abstract
Document enrichment is the task of retrieving additional knowledge from external resource over
what is available through source document. This task is essential because of the phenomenon
that text is generally replete with gaps and ellipses since authors assume a certain amount of
background knowledge. The recovery of these gaps is intuitively useful for better understanding
of document. Conventional document enrichment techniques usually rely on Wikipedia which
has great coverage but less accuracy, or Ontology which has great accuracy but less cover-
age. In this study, we propose a document enrichment framework which automatically extracts
?argument
1
, predicate, argument
2
? triple from any text corpus as background knowledge, so
that to ensure the compatibility with any resource (e.g. news text, ontology, and on-line ency-
clopedia) and improve the enriching accuracy. We first incorporate source document and back-
ground knowledge together into a triple based document-level graph and then propose a global
iterative ranking model to propagate relevance score and select the most relevant knowledge
triple. We evaluate our model as a ranking problem and compute the MAP and P&N score to
validate the ranking result. Our final result, a MAP score of 0.676 and P&20 score of 0.417
outperform a strong baseline based on search engine by 0.182 in MAP and 0.04 in P&20.
1 Introduction
Document enrichment is the task to acquire background knowledge from external resources and recover
the omitted information automatically for certain document. This task is essential because authors usu-
ally omit basic but well-known information to make the document more concise. For example, author
omits ?Baghdad is the captain of Iraqi? in the text of Figure 1 (a), which is well-known to readers. Dur-
ing reading process, these gaps will be automatically plugged effortlessly by the background knowledge
in human brain. However, the situation is different for machine because it lacks the ability to acquire
and select the proper background knowledge, which limits the performances of certain NLP applica-
tions. Document enrichment has been proved helpful in these tasks such as web search (Pantel and
Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity
disambiguation (Bunescu and Pasca, 2006; Sen, 2012).
In the past, there are mainly two kinds of document enrichment researches according to the resource
they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia
as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad
1
in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006;
Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great
success of these methods, there remain a great challenge that not all information in the linked Wiki page
is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad
contains lots of information about city history and culture, which are not quite relevant to the semantic of
context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://en.wikipedia.org/wiki/Baghdad
917
S1: Coalition may never know if   Iraqi   president   Saddam 
Hussein survived a U.S. air strike yesterday.
S2: A B-1 bomber dropped four 2,000-pound bombs on a building 
in a residential area of   Baghdad  . 
S3: They had got an intelligence reports senior officials were 
meeting there, possibly including Saddam Hussein and  his sons .
BaghdadIraqi hasCapital
Saddam Hussein diedIn
Qusay HusseinSaddam Hussein hasChild
Kadhimiya
k1:
k2:
k3:
Global 
Ranking
(a) Source document (b) Top-3 background knowledge
S1: Coalition may never know if   Iraqi   president   Saddam Hussein 
survived a U.S. air strike yesterday.
S2: A B-1 bomber dropped four 2,000-pound bombs on a building 
in a residential area of   Baghdad  . 
S3: They had got an intelligence reports senior officials were 
meeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document (b) Two relevant background knowledge
Iraq
Baghdad
Saddam Hussein
Captain
Died In Kadhimiya
Global 
Ranking
k1:
k2:
Figure 1: An example of document enrichment with background knowledge: (a) source document talking
about a U.S. air strike aiming at Saddam in Baghdad (b) two important relevant information, which is
omitted in source document but acquired by our model and enriched as background knowledge .
problem. Another line of works rely on the Ontologies constructed with supervision or even manually
which have great accuracy but less coverage (Motta et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar
and Salim, 2012). Besides, these methods usually rely on special ontology which is rather difficult to
construct and in turn limits the coverage and application of these methods.
Ideally, we would wish to integrate both coverage and accuracy, where an triple based background
knowledge ranking model may help. Our framework extracts knowledge from any corpus resource in-
cluding WikiPedia to ensure coverage and present knowledge as ?argument
1
, predicate, argument
2
?
triple to reduce noise. This model ranks background knowledge triples according to their relevance to
the source document. The key idea behind the model is that document is constructed by several units of
information, which can be extracted automatically. For every background knowledge b extracted auto-
matically from a relevant corpus, the more units are relevant to b and the more important they are, the
more relevant b becomes to the source document. Thus, we extract both source document information
and background knowledge automatically and present them together in a document-level graph. Then
we propagate the relevance score from the source document information to the background knowledge
during an iterative process. After convergence, we obtain the Top n relevant background knowledge,
rather than retrieving all of them without filtering.
To evaluate our model, we use ACE
2
corpus as source documents and output the ranked list of back-
ground knowledge. Then we train three annotators to check the ranking result and annotating whether
certain knowledge is relevant to corresponding source document separately. We totally annotated more
than 7000 background knowledge by three annotators. We evaluate their annotation consistence by com-
puting the Fleiss
?
Kappa (Fleiss, 1971), a famous criterion in multi-annotator consistence evaluation.
We achieve a Fleiss
?
Kappa of value 0.8066 in best situation and 0.7076 in average, which indicates
the great consistence between three annotators. The ranking result is evaluated with MAP score and
P&N score (Voorhees et al., 2005). We finally achieve aMAP score of 0.676 and P&20 score of 0.417
in Top 20 background knowledge, which are higher by 0.182 and 0.04 than a strong baseline based
on search engine. We also evaluate the effect of the automatically extraction to source document and
background knowledge, which is key to the performance of our method in real application.
2 Triple Graph based Document Representation
We believe that different parts of document are related to each other, rather than isolated. Hence, we
propose a triple graph based document representation to incorporate source document information and
background knowledge. In this presentation, ?argument
1
, predicate, argument
2
? triple serves as node
and the edge between nodes indicates their semantic relevance. In this part, we introduce triple graph
and the way to extract source document information and background knowledge automatically.
2.1 Motivation for triple presentation
Compared to Wiki Page, triple based enrichment helps to reduce noise illustrated in Section 1. Compared
to bag of words, triple based presentation help to reduce ambiguity of single word which is shown in
2
http://catalog.ldc.upenn.edu/LDC2006T06
918
B ar ac k O bam a
H ar v ar d  U niv e r s it y W h it e  H ou s e
(a) Ambiguity (b) Disambiguation
B ar ac k O bam a,  e a r n ,  l a w  d e g r e e
H ar v ar d  U niv e r s it y W h it e  H ou s e
Figure 2: The motivation for the form of triple (a) relevance ambiguity of single word Obama, which
is related to Harvard and White House (b) disambiguation with the help of other triple elements, where
?earn, law degree? help to limit Obama to the graduate of Harvard.
Figure 2. Figre 2 (a) shows that the single word of Obama is related to multiple semantic information
such as Harvard University as a law graduate and White House as the president. After introducing the
information from other elements of the triple, ?earn, law degree? help to disambiguate and limit Obama
to the law graduate of Harvard University only in Figure 2 (b). The form of triple has been used as the
presentation of knowledge in some researches such as knowledge base (Hoffart et al., 2013).
2.2 Nodes in the Graph
There are two kinds of nodes in the triple graph: source document nodes (sd-nodes) and background
knowledge nodes (bk-nodes). Both of them are extracted automatically with Open Information Ex-
traction (Open IE) technology which focuses on extracting assertions from massive corpora without a
pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of
syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs.
There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld,
2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many
NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin
et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous
Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008),
to extract source document information and background knowledge automatically. We use the newest
version of ReVerb (version 1.3) without modification, which is free download on-line
3
.
Source document node (sd-node) Sd-nodes consists of the information extracted from source docu-
ment automatically by open information extraction technology (Banko et al., 2007), especially Reverb,
the famous Open IE system developed by University of Washington (Etzioni et al., 2011). The output
of ReVerb is formed as ?argument
1
, predicate, argument
2
?, which is naturally presented as triple. In
this study, we use ACE corpus as source documents and all sd-nodes are extracted by ReVerb. The setup
of automatic extraction makes our method usable in many real applications. To evaluate the effect of
automatic extraction, we also use the golden annotation within ACE (Doddington et al., 2004) corpus as
source document information and compare the performance that with automatic extraction.
Background knowledge node (bk-node) Bk-nodes consist of the background knowledge extracted
from external corpus resources automatically by Reverb too. We do not rely on certain existed knowl-
edge base and extract background knowledge from external corpus resources for corresponding source
document. This setup makes our methods usable in many real applications. Although we do not rely on
special knowledge base, we do adapt our method for the existed knowledge base such as YAGO (Hoffart
et al., 2013) and compare the performance to evaluate the effect of different knowledge sources.
2.3 Edges in the Graph
The edges between two nodes indicate their semantic relevance, which is evaluated in Section 3.1. There
are two kinds of edges: (1) sd-node to sd-node (2) sd-node to bk-node, both of them are undirected.
Considering all the relevance score originating from sd-nodes, we connect no edge between bk-nodes.
3
http://reverb.cs.washington.edu/
919
Edges between sd-nodes All sd-nodes are extracted from the same document, so they should be related
to each other. We connect each pair of sd-nodes with an edge and set the weight of edge as their semantic
relevance computed in Section 3.1. With this setup, we combine the source document as a whole where
different parts affect each other through the edge.
Edges between sd-node and bk-node The basic idea of our model is to propagate relevance score
from the sd-nodes to bk-nodes. Hence, we connect each pair of sd-node and bk-node with an edge and
set the weight of the edge as their relevance computed in Section 3.1. These edges are all undirected,
which indicates that bk-nodes also affect the relevance score of the sd-nodes during the ranking process.
3 Global Ranking Model
In this study, source document D is presented as the graph of sd-nodes. For every background knowledge
b, the task of evaluating the relevance between b and D is naturally converted into evaluating the relevance
between b and the graph of sd-nodes. So the relevance between b and document D can be computed by
propagating the relevance score from every sd-node of D to b iteratively. After the convergence, the
relevance between b and D can be evaluated by the relevance score of b. Intuitively, three factors affect
their relevance:
? How many sd-nodes is b relevant to ?
? How relevant is b to these sd-nodes?
? How important are these sd-nodes ?
For the first factor, b should be more relevant to source document D if more sd-nodes are relevant
to b. We capture this information by allowing b to receive relevance score from all the sd-nodes. For
the second factor, b should be more relevant to D if more relevant b is to sd-nodes. We consider this
information by evaluating the relevance between b and every sd-node (Section 3.1). For the last factor,
important sd-nodes should have higher impact. We consider this information by evaluating the impor-
tance of sd-nodes and assigning higher initial value to importance ones (Section 3.3). We combine all
factors in the global ranking process to select the top-n relevant background knowledge (Section3.2).
3.1 Relevance Evaluation between Nodes
In this section, we evaluate the semantic relevance between different nodes which is the weight of the
edge between them. We introduce Search Engine as a resource, which has been proven effective in
relevance evaluation (Gligorov et al., 2007). This method is motivated by the phenomenon that the
number of results returned by search engine for query p ? qindicates the relevance between p and q.
However, considering the different popularization of queries, this number alone can not accurately
express their semantic relevance. For example, query car ? automobile gets 294, 300, 000 results,
whereas query car?apple gets 683, 000, 000, which is 2 times higher than the previous one. Obviously,
automobile is more relevant to car rather than Apple. The reason of this phenomenon is that apple
is far more popular than automobile, which increase its possibility of co-occurrence with car. So we
consider the number of results for p?q together with p and q withWebJaccard Coefficient (Bollegala
et al., 2007) to evaluate the relevance between p and q according to Formula 1, where H(p), H(q), and
H(p ? q) indicate the number of results for query p, p, and p ? q.
WebJaccard(p, q) =
{
0 if H(p ? q) ? C
H(p?q)
H(p)+H(q)?H(p?q)
otherwise.
(1)
To convert one ?argument
1
, predicate, argument
2
? triple into query, we use argument
1
?
argument
2
as the query for one triple. We have tried argument
1
? predicate ? argument
2
which
920
is usually very sparse. Besides, the combination of two arguments usually maintain better semantic com-
pleteness of triple compared to other combinations according to our analysis. So this setup aims to bal-
ance completeness and sparseness. Accordingly, two triples are combined as argument
1
?argument
2
?
argument
?
1
? argument
?
2
. Considering the scale and noise in the Web data, it is possible for two words
to appear together accidentally. To reduce the adverse effects attributed to random co-occurrences, we
set 0 to the WebJaccard Coefficient of query p? q, if the number of result is less than a threshold C.
3.2 Iterative Relevance Propagation
Here we propose the relevance propagation based iterative process to evaluate the relevance between cer-
tain background knowledge and source document. Note that standard label propagation mainly focuses
on classification task (Wang and Zhang, 2008). However, we focus on a ranking problem where the best
ranking result is computed during an iterative process in this study. So we make two modifications to
suit the ranking problem better: not reseting the relevance score and introducing the propagation between
source document information during iteration.
Propagation possibility The edge between node
i
and node
j
is weighted by r(i, j) to measure their
relevance. However, r(i, j) cannot completely present the propagation possibility because one node can
be equally relevant to all of its neighbors. Thus, we define p(i, j) based on r(i, j) in formula 2 to indicate
the propagation possibility between node
i
and node
j
.
p(i, j) =
r(i, j)? ?(i, j)
?
k?N
r(k, j)? ?(k, j)
(2)
N is the set of all nodes, ?(i, j) denotes whether an edge exists between node
i
and node
l
in the triple-
graph or not, which indicates whether they may propagate to each other or not. E is the set of edges.
?(i, j) =
{
1 if (i, j) ? E
0 otherwise
(3)
Iterative propagation There are n ? n pairs of nodes, the p(i, j) of them is stored in a matrix P .
we use
~
W = (w
1
, w
2
, ? ? ? , w
n
) to denote the relevance score of all nodes, in which w
i
indicates the
relevance between node
i
and source document D. Here the node
i
can indicate both sd-nodes and bk-
nodes because they are processed during one fellow step. So that we keep updating both sd-nodes and
bk-nodes and do not distinguish them explicitly. The only difference between them is that we initialize
the w
i
of sd-nodes as its importance to D (Section 3.1) while bk-nodes as 0 at the beginning. We use
matrix P together with ?(i, j) to compute the
~
W during a iterative process, where
~
W is updated to
~
W
?
during the end of every iteration. The matrix
~
W
?
is updated according to the following Formula 4:
~
W
?
=
~
W ? P
=
~
W ?
?
?
?
?
p(1, 1) p(1, 2) ? ? ? p(1, n)
p(2, 1) p(2, 2) ? ? ? p(2, n)
? ? ? ? ? ? ? ? ? ? ? ?
p(n, 1) p(n, 2) ? ? ? p(n, n)
?
?
?
?
(4)
each w
i
in
~
W is updated to w
?
i
according to the formula 5, where w
i
is propagated from all the other
w
j
(j 6= i) according to their propagation possibility p(j, i). We also introduce the propagation from
bk-nodes to sd-nodes, where bk-nodes serve as intermediate to help mining latent semantics.
w
?
i
= w
1
? p(1, i) + w
2
? p(2, i) + ? ? ?+ w
n
? p(n, i)
=
?
k?N
w
k
? p(k, i)
=
?
k?N
w
k
?
(
r(i, j)? ?(i, j)
?
k?N
r(k, j)? ?(k, j)
)
(5)
921
3.3 Importance Evaluation for sd-nodes
The main idea of our model is to propagate relevance score from sd-nodes to bk-nodes (Section 3.2).
So the initialization of sd-node is important, which indicates the importance of different source docu-
ment information. This section solves this problem by evaluating the importance of sd-nodes to source
document. We use v
j
to denote the initialization of sd-nodes, which indicates the importance of node
j
(node
j
? set of sd-nodes) to source document. In this section, we propose a modified relevance propa-
gation method to evaluate v
j
for sd-notes. We first construct a triple-graph consisting of sd-nodes only.
Then we initialize the relevance score of sd-nodes according to a simple approach based on text fre-
quency (Kohlsch?utter et al., 2010). We use similar relevance propagation process without resetting the
relevance score at the beginning of every iteration, until a global stable state is achieved. Finally, we
normalize all the relevance scores to get
~
V , which indicates the importance of sd-nodes to the source
document. We return
~
V to the global ranking model (Section 3.2) as part of the input. The initial impor-
tance of bk-nodes is set as 0 at the beginning, which denotes that all bk-nodes are ir-relevant to source
document before the starting of global ranking process.
4 Experiment
We treat our task as a ranking problem, which takes a document as input and output the ranked list of
background knowledge. We evaluate our method as a ranking problem similarly to information retrieval
task and focus on the performances of models with different setups.
4.1 Data Preparation
The experiment data consists of two parts: source document information and corresponding background
knowledge. To select source documents, we use the ACE corpus (Doddington et al., 2004) for 2005 eval-
uation
4
which consists of 599 articles from multiple sources. We use ReVerb to extract these documents
into multi-triples. For background knowledge, we first retrieve relevant web pages with simply term
matching method and then extract these pages with ReVerb into a set of triples serving as background
knowledge. To ensure the quality, we filter them according to the confidence given by ReVerb.
Besides automatic extraction, we also adapt our system to the golden annotation of ACE as source
document information and standard YAGO knowledge base
5
as background knowledge (Hoffart et al.,
2013). We compare its performance with that in fully automatic system and evaluate the effect of auto-
matic extraction. For better comparison with YAGO, we retrieve relevant pages from WikiPedia although
our automatic extraction method is applicable to any corpus resources.
For every outputted list, three trained annotators check the result and decide which background knowl-
edge is relevant to source document. They work separately and check the same list, so that we can e-
valuate their annotation consistence. They totally annotated more than 7000 background knowledge and
achieved a Fleiss
?
Kappa value of 0.8066 in best situation and 0.7076 in average between three anno-
tators, which is a good consistence between multi-annotator (Fleiss, 1971). When collision happened,
we choose the label selected by more annotators.
4.2 Baseline system
Although we treat our task as a ranking problem, it is difficult to apply corresponding methods in tra-
ditional ranking tasks such as information retrieval (IR) (Manning et al., 2008) and entity linking (EL)
(Han et al., 2011; Kataria et al., 2011; Sen, 2012) directly in our task. First, both IR and EL make use of
the link structure between web or Wiki pages. However, our task takes single document as input and no
link exists between documents which makes it difficult to apply IR and EL methods such as page rank
(Page et al., 1999) and collective method (Han et al., 2011; Sen, 2012) in this task directly. Second, EL
usually evaluate the text similarity between certain document and target page in WikiPedia. However,
our task focuses on the ranking of ?argument
1
, predicate, argument
2
? triple, which contains little text
information. Lack of text information also limits the application of corresponding methods in our task.
4
http://catalog.ldc.upenn.edu/LDC2006T06
5
http://www.mpi-inf.mpg.de/yago-naga/yago
922
Setup MAP P&20
Baseline 0.494 0.377
AutoSD + AutoBK + NoInitial 0.504 0.378
AutoSD + AutoBK + WithInitial 0.531 0.406
GoldSD + AutoBK + NoInitial 0.564 0.417
GoldSD + AutoBK + WithInitial 0.553 0.406
GoldSD + YAGO + NoInitial 0.676 0.328
GoldSD + YAGO + WithInitial 0.676 0.328
Table 1: The result of our model in different setups: GoldSD indicates using annotation of ACE corpus as
source document information; YAGO indicates using YAGO knowledge base as background knowledge;
AutoSD and AutoBK means aotomatic extraction to source document and background knowledge; NoIni-
tial and WithInitial means whether using different initial importance to source document information.
For better comparison, we introduce search engine as resource which is proved effective in relevance
evaluation (Gligorov et al., 2007) and propose a search engine based strong baseline. As illustrated
before, the relevance R
i
between background knowledge b
i
and source document D has been converted
into the relevance between b
i
and the triples of D. Hence, we compute R
i
by accumulating all r
ij
, the
relevance scores between b
i
and every sd-node s
j
with the same method in Section 3.1 (R
i
=
?
s
j
?S
r
ij
,
S is the set of sd-nodes). Then we rank all background knowledge according to the value ofR
i
and output
the ranked list as final result. We extract source document and background knowledge automatically in
the baseline system, which makes it applicable in different setups.
4.3 Experiment setup
We evaluate our model in different setups. First, we extract both source document information and
background knowledge automatically. Second, we use golden annotation of ACE as source document
information but extract background knowledge automatically. Third, we use golden annotation of ACE
and introduce standard YAGO as background knowledge. For all of them three, we evaluate the different
performances with and without initial importance of sd-nodes(Section 3.3). We evaluate the performance
with two famous criteria in ranking problem: MAP (Voorhees et al., 2005) requires more accuracy
and focuses on the knowledge in higher position; P&N which require more coverage and pays more
attention to the number of relevant ones in Top N knowledge. Note that we do not evaluate the Recall
performance because there can be millions of background knowledge to be ranked for every document.
It is impossible to check all of them. So we focus on the Top N candidates and evaluate the performance
with MAP and P&N . In this study, we evaluate the Top 20 background knowledge triples which are
most easily to be viewed by users.
4.4 Experiment Result
The performance of our model is shown in Table 1. Our search engine based baseline system achieve
a rather good performance: a MAP value of 0.494 and 0.377 in P&20. At the same time, our model
outperforms the baseline system in almost every setup and evaluation criterion. The best performance of
MAP is achieved by GoldSD+YAGO (0.676), while the best performance of P&20 is achieved by GoldS-
D+AutoBK (0.417). To analyze the result further, we find that the initial importance, automatic extraction
to source document, and to background knowledge have different effect on the final performance.
4.4.1 Effect of automatic extraction to source document
We use ACE corpus as source documents, which contain golden annotation to document information.
So we can evaluate the effect of automatic extraction to source document by comparing the performance
with and without golden annotation. The performance without golden annotation is shown in AutoS-
D+AutoBK of Table 1, while the other one shown in GoldSD+AutoBK. We can find that the performance
of GoldSD+AutoBK is better than that of AutoSD+AutoBK in both MAP and P&20, which indicates that
golden annotation do help to improve the ranking result.
923
We further analyze the result and find an interesting phenomenon: these two systems performs greatly
different with the setup of NoInitial, but equally with the setup of WithInitial, which indicates that the
performance of AutoSD+AutoBK has been improved by evaluating the importance of source document
information (Section 3.3). So we can naturally infer that, with a better importance evaluating method
in AutoSD+AutoBK, we may achieve similar performance compared to that in golden annotation. Note
that, AutoSD+AutoBK is compatible with any corpus which is more useful in real applications.
4.4.2 Effect of automatic extraction to background knowledge
We evaluate the effect of automatic extraction to background knowledge by comparing the performances
between GoldSD+AutoBK and GoldSD+YAGO. In GoldSD+AutoBK, the background knowledge is ex-
tracted automatically with ReVerb, which has greater coverage but less accuracy. In contrast, the GoldS-
D+YAGO make use of YAGO as background knowledge, which is less coverage but better accuracy. This
difference are reflected on the system performance, where GoldSD+YAGO achieves much better result in
MAP, but much worse in P&20. This is partly because that MAP focus on the background knowledge in
higher position which requires more accuracy, while P&20 pays more attention to the number of relevant
background knowledge which require more coverage.
In general, automatic extraction system has better coverage but less accuracy compared to YAGO
based system. However, automatic extraction to background knowledge may help in real applications by
improving coverage greatly. Besides, the loss of accuracy is partly due to the technology of information
extraction which may be improved in the future. In addition, we can also combine these two ways to
acquire background knowledge to balance coverage and accuracy in the future.
4.4.3 Effect of initial importance to source document information
Initial importance to source document information (Section 3.3) is important to the performance of
our models as shown in Table 1. The model AutoSD+AutoBK+WithInitial outperforms the AutoS-
D+AutoBK+NoInitial compared to other setups, which indicates the help of initial importance to the
ranking result. Especially, initial importance to source document information helps most in the set-
up of AutoSD+AutoBK, which is most useful in real applications. So we can naturally infer that, by
proposing better importance evaluating method, we may further improve the performance of AutoS-
D+AutoBK+WithInitial, which will great helpful in the future application of this method.
5 Related Work
Document enrichment focuses on introducing external knowledge into source document. There are main-
ly two kinds of works in this topic according to the resource they relying on. The first line of works make
use of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page (Bunes-
cu and Pasca, 2006; Cucerzan, 2007). In early stage, most researches rely on the similarity between the
context of the mention and the definition of candidate entities by proposing different measuring crite-
ria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones
(Bunescu and Pasca, 2006; Cucerzan, 2007; Zheng et al., 2010; Hoffart et al., 2011; Zhang et al., 2011).
However, these methods mainly rely on text similarity but neglect the internal structure between mention-
s. So another kind of works explore the structure information with collective disambiguation (Kulkarni
et al., 2009; Kataria et al., 2011; Sen, 2012; He et al., 2013). These methods make use of structure infor-
mation within context and resolve different mentions based on the coherence among decisions. Despite
the success, the entity linking methods rely on WikiPedia which has great coverage but less accuracy.
Another line of works try to improve the accuracy of enrichment by introducing ontologies (Motta
et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge
such as WordNet (Nastase et al., 2010) and Mesh (Wang and Lim, 2008). In these studies, resources
usually provides word or phrase semantic information such as synonym (Sun et al., 2011) and antonym
(Sansonnet and Bouchet, 2010). However, these methods rely on special ontologies constructed with
supervision or even manually, which is difficult to expand and in turn limits the application of them.
924
6 Conclusion and Future Work
This study presents a triple based background knowledge ranking model to acquire most relevant back-
ground knowledge to certain source document. We first develop a triple graph based document presen-
tation to combine source document together with the background knowledge. Then we propose a global
iterative ranking model to acquire Top n relevant knowledge, which provide additional information be-
yond the source document. Note that, both source document information and background knowledge
are extracted automatically which is useful in real application. The experiments show that our model
achieves better results over a strong baseline, which indicates the effectiveness of our framework.
Another interesting phenomenon is that YAGO based enrichment model achieved better ranking ac-
curacy, but less coverage compared to automatic extraction model. To combine these two sources of
background knowledge may help to overcome both coverage and accuracy problem. So exploiting prop-
er way to incorporate knowledge base and automatic extraction is an important topic in our future work.
Finally, we believe that this background knowledge based document enriching technology may help in
those semantic based NLP applications such as coherence evaluation, coreference resolution and question
answering. In our future work, we will explore how to make use of these background knowledge in real
applications, hopefully to improve the performance significantly in the future.
Acknowledgements
We thank Muyun Yang and Jianhui Ji for their great help. This work was supported by National Natural
Science Foundation of China(NSFC) via grant 61133012, the National 863 Leading Technology Re-
search Project via grant 2012AA011102 and the National Natural Science Foundation of China Surface
Project via grant 61273321.
References
Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In IJCAI, volume 7, pages 2670?2676.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Measuring semantic similarity between words
using web search engines. www, 7:757?766.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and Kateryna Tymoshenko. 2010. Using background knowledge
to support coreference resolution. In ECAI, volume 10, pages 759?764.
Razvan C Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In
EACL, volume 6, pages 9?16.
Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL,
volume 7, pages 708?716.
George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie Strassel, and Ralph M
Weischedel. 2004. The automatic content extraction (ace) program-tasks, data, and evaluation. In LREC.
Citeseer.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the web. Communications of the ACM, 51(12):68?74.
Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open infor-
mation extraction: The second generation. In Proceedings of the Twenty-Second international joint conference
on Artificial Intelligence-Volume Volume One, pages 3?10. AAAI Press.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.
Samah Fodeh, Bill Punch, and Pang-Ning Tan. 2011. On ontology-driven document clustering using core semantic
features. Knowledge and information systems, 28(2):395?421.
Risto Gligorov, Warner ten Kate, Zharko Aleksovski, and Frank van Harmelen. 2007. Using google distance to
weight approximate ontology matches. In Proceedings of the 16th international conference on World Wide Web,
pages 767?776. ACM.
925
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR conference on Research and development in Information
Retrieval, pages 765?774. ACM.
Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang. 2013. Learning entity repre-
sentation for entity disambiguation. Proc. ACL2013.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782?792. Association
for Computational Linguistics.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: a spatially and
temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28?61.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park, and Xiaohua Zhou. 2009. Exploiting wikipedia as external
knowledge for document clustering. In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 389?396. ACM.
Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi, Prithviraj Sen, and Srinivasan H Sengamedu. 2011.
Entity disambiguation with hierarchical topic models. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 1037?1045. ACM.
Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of the third ACM international conference on Web search and data mining, pages
441?450. ACM.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 457?466. ACM.
Yogan Jaya Kumar and Naomie Salim. 2012. Automatic multi document summarization approaches. Journal of
Computer Science, 8(1).
Thomas Lin, Oren Etzioni, et al. 2010. Identifying functional relations in web text. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276. Association for
Computational Linguistics.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to information retrieval,
volume 1. Cambridge University Press Cambridge.
Enrico Motta, Simon Buckingham Shum, and John Domingue. 2000. Ontology-driven document enrichment:
principles, tools and applications. International Journal of Human-Computer Studies, 52(6):1071?1109.
Vivi Nastase, Michael Strube, Benjamin B?orschinger, C?acilia Zirn, and Anas Elghafari. 2010. Wikinet: A very
large scale multi-lingual concept network. In LREC.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures: Associating web queries with structured entities. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, pages 83?92. Association for Computational Linguistics.
Alexandre Passant. 2007. Using ontologies to strengthen folksonomies and enrich information retrieval in we-
blogs. In Proceedings of International Conference on Weblogs and Social Media.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434. Association
for Computational Linguistics.
Jean-Paul Sansonnet and Franc?ois Bouchet. 2010. Extraction of agent psychological behaviors from glosses of
wordnet personality adjectives. In Proc. of the 8th European Workshop on Multi-Agent Systems (EUMAS10).
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld, and Jesse Davis. 2010. Learning first-order horn clauses
from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,
pages 1088?1098. Association for Computational Linguistics.
926
Prithviraj Sen. 2012. Collective context-aware topic models for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages 729?738. ACM.
Koun-Tem Sun, Yueh-Min Huang, and Ming-Chi Liu. 2011. A wordnet-based near-synonyms and similar-looking
word learning system. Educational Technology & Society, 14(1):121?134.
Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval,
volume 63. MIT press Cambridge.
Xudong Wang and Azman O Lim. 2008. Ieee 802.11 s wireless mesh networks: Framework and challenges. Ad
Hoc Networks, 6(6):970?984.
Fei Wang and Changshui Zhang. 2008. Label propagation through linear neighborhoods. Knowledge and Data
Engineering, IEEE Transactions on, 20(1):55?67.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan. 2011. Entity linking with effective acronym expansion,
instance selection and topic modeling. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pages 483?491. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101?110. ACM.
927
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1360?1369, Dublin, Ireland, August 23-29 2014.
Sentence Compression for Target-Polarity Word Collocation Extraction
Yanyan Zhao
1
, Wanxiang Che
2
, Honglei Guo
3
, Bing Qin
2
, Zhong Su
3
and Ting Liu
2?
1: Department of Media Technology and Art, Harbin Institute of Technology
2: Department of Computer Science and Technology, Harbin Institute of Technology
3: IBM Research-China
{yyzhao, bqin, tliu}@ir.hit.edu.cn, {guohl, suzhong}@cn.ibm.com
Abstract
Target-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarily
on syntactic features to identify the relationships between targets and polarity words. A major
problem of current research is that this task focuses on customer reviews, which are natural or
spontaneous, thus posing a challenge to syntactic parsers. We address this problem by proposing
a framework of adding a sentiment sentence compression (Sent Comp) step before performing
T-P collocation extraction. Sent Comp seeks to remove the unnecessary information for senti-
ment analysis, thereby compressing a complicated sentence into one that is shorter and easier to
parse. We apply a discriminative conditional random field model, with some special sentiment-
related features, in order to automatically compress sentiment sentences. Experiments show that
Sent Comp significantly improves the performance of T-P collocation extraction.
1 Introduction
Sentiment analysis deals with the computational treatment of opinion, sentiment and subjectivity in tex-
t (Pang and Lee, 2008), and has received considerable attention in recent years (Liu, 2012). Target-
Polarity word (T-P) collocation extraction, which aims to extract the collocation of a target and its cor-
responding polarity word in a sentiment sentence, is a basic task in sentiment analysis. For example,
in a sentiment sentence ????????????? (The camera has a novel appearance), ????
(appearance) is the target, and ???? (novel) is the polarity word that modifies ???? (appearance).
According, ???, ??? (?appearance, novel?) is the T-P collocation. Generally, T-P collocation is a
basic and complete sentiment unit, thus is very useful for many sentiment analysis applications.
Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (Ab-
basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation ?Adj
ATT
x Noun?, where the
ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P
collocation ???, ??? (?appearance, novel?) in the above sentiment sentence (Bloom et al., 2007;
Qiu et al., 2011; Xu et al., 2013).
However, one major problem of these approaches is the ?naturalness? of sentiment sentences, that is,
such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge
to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can
further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a)
as an example, because the word ???? (fortunately) is so chatty,1 the parsing result is wrong. Thus,
are unable to extract the T-P collocation ???,?? (?keyboard, good?).
To solve the ?naturalness? problem, we can train a parser on sentiment sentences. Unfortunately, an-
notating such data will cost us a lot of time and effort. Instead, in this paper we produce a sentence
compression model, Sent Comp, which is designed especially to compress complicated sentiment sen-
tences into formal and easier to parse ones, further improving T-P collocation extraction.
?
Correspondence author: tliu@ir.hit.edu.cn
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Note that, in Figure 1, the Chinese word ???? is chatty, although its translated English word ?fortunately? is not. In this
paper, we focus on processing the Chinese data.
1360
?? ?? ?
fortunately keyboard good
SBV
VOB
ROOT
(a) before compression
?? ?
keyboard good
SBV
ROOT
(b) after compression
Figure 1: Parse trees before and after compression.
This idea is motivated by the observation that, current syntactic parsers usually perform accurately
for short, simple and formal sentences, whereas error rates increase for longer, more complex or more
natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing
performance would have a ripple effect over T-P collocation extraction. For example, we can compress
the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part ????
(fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree
is correct, making it easier to accurately extract T-P collocation.
Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining impor-
tant information (usually important grammar structure) (Jing, 2000). For example, the sentence ?Overall,
this is a great camera.? can be compressed into ?This is a camera.? by removing the adverbial ?overall?
and the modifier ?great?. However, the modifier ?great? is a polarity word and very important for sen-
timent analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional
compression models, because it needs to retain the important sentiment information, such as the polarity
word. Hence, using Sent Comp, the above sentence can be compressed into ?This is a great camera.?
We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random
fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other
studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce
some sentiment-related features to retain the sentiment information for Sent Comp.
We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the
sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-the-
art T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese
corpus of four product domains show the effectiveness of our approach.
The main contributions of this paper are as follows:
? We present a framework of using sentiment sentence compression preprocessing step to improve T-
P collocation extraction. This framework can better solve the ?over-natural? problem of sentiment
sentences, which poses a challenge to syntactic parsers. More importantly, the idea of this frame-
work can be applied to some other sentiment analysis tasks that rely heavily on syntactic results.
? We develop a simple yet effective compression model Sent Comp for sentiment sentences. To the
best of our knowledge, this is the first sentiment sentence compression model.
2 Background
For our baseline system, we used the state-of-the-art method to extract T-P collocations introduced by
Qiu et al. (2011), who proposed a double propagation method. This idea is based on the observation
that there is a natural syntactic relationship between polarity words and targets owing to the fact that
polarity words are used to modify targets. Furthermore, they also found that polarity words and targets
themselves have relations in some sentiment sentences (Qiu et al., 2011).
Based on this idea, in the double propagation method, we first used an initial seed polarity word lexicon
and the syntactic relations to extract the targets, which can fall into a new target lexicon. Then we used the
target lexicon and the same syntactic relations to extract the polarity words and to subsequently expand
the polarity word lexicon. This is an iterative procedure, because this method can iteratively produce the
new polarity words and targets back and forth using the syntactic relations.
1361
?? ?? ??
function very powerful
SBV
ADV
ROOT
(a) syntactic structure 1
?? ? ??
powerful function
ATT
RAD
ROOT
(b) syntactic structure 2
?? ? ?? ?
function is powerful
SBV VOB RAD
ROOT
(c) syntactic structure 3
?? ? ?? ?? ??
function and service very powerful
COO
LAD ADV
SBV ROOT
(d) syntactic structure 4
?? ?? ?? ? ??
function very powerful and complete
SBV COO
ADV LAD
ROOT
(e) syntactic structure 5
Figure 2: Example of syntactic structure rules for T-P collocation extraction. We showed five examples
from a total of nine syntactic structures. For each kind of syntactic structure (a) to (e), the target is
shown with a red box and the polarity word is shown with a green box. Syntactic structures (a) to (c)
describe the relations between targets and polarity words. Syntactic structure (d), which is extended
from (a), describes the relation between two targets. Syntactic structure (e), which is also extended from
(a), describes the relation between two polarity words. Similarly, we can summarize the other four rules
extended from (b) and (c) to describe the relations between two targets or two polarity words.
We can see that the syntactic relations are important for this method, and Qiu et al. (2011) proposed
eight rules to describe these relations. However, their work only focused on English sentences, whereas
the relations for Chinese sentences are different. Thus, in accordance with Chinese grammar, we pro-
posed nine syntactic structure rules between target t and polarity word p in a Chinese T-P collocation
?t, p?.
2
The three main rules are shown below and some example rules are illustrated in Figure 2.
Rule 1: t
SBV
x p, the ?subject-verb? structure between t and p, such as the example in Figure 2(a).
Rule 2: p
ATT
x t, that p is an attribute for t, such as the example in Figure 2(b).
Rule 3: t
SBV
x ?
VOB
y p, the ?subject-verb-object? structure between t and p, such as the example in
Figure 2(c). The ? denotes any word.
The other six rules can be extended from the three main rules by obtaining the coordination (COO)
relation of t or p, such as t
SBV
x ?
COO
y p in Figure 2(e). Note that the POS for t should be noun and for p
should be adjective.
As described above, the T-P collocation extraction relies heavily on syntactic parsers. Hence, if we
can use the Sent Comp model to improve the performance of parsers, the performance of T-P collocation
extraction can also be improved accordingly.
3 Sentiment Sentence Compression
3.1 Problem Analysis
First, we conducted an error analysis for the results of current T-P collocation extraction, from which we
observed that the ?naturalness? of sentiment sentences is one of the main problems. For examples:
? Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser.
For example, in the sentence ??????? (fortunately the keyboard is good) shown in Figure 1,
the usage of the chatty word ???? (fortunately) affects the accuracy of the syntactic parser.
2
A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our
dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based
dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05).
1362
?? ?? ?
besides photo good
ADV
POB
ROOT
comp
?? ?
photo good
SBV
ROOT
(a) parse tree 1 before and after compression
?? ? ? ? ?? ??screen for people feel good
SBV
ATT
POBRAD SBV
ROOT
comp
?? ??screen good
SBV
ROOT
(b) parse tree 2 before and after compression
Figure 3: ?Naturalness? problem of sentiment sentences.
? Conjunction word usage: conjunction words are often used in sentiment sentences to show the dis-
course relations between two sentences. However, there are so many conjunction words in Chinese,
some of which can cause errors among parsers. For example, in Figure 3(a), the parse tree of sen-
tence ???????? (besides the photo is good) is wrong because of the usage of the conjunction
word ???? (besides).
? Feeling words/phrase usage: in sentiment sentences, people often use some feeling words/phrase,
such as ??????? (feel like) in Figure 3(b) or ????? (smell like). Given that the current
syntactic parser cannot handle the feeling words/phrases very well, the T-P collocation ???, ?
?? (?screen, good?) in Figure 3(b) cannot be extracted correctly.
To address the ?naturalness? problem, we compressed the sentiment sentences into one that are shorter
and easier to parse. Similar to the examples in Figure 1 and 3, the compressed sentences can be easily
and correctly parsed. The above analysis can be used as the criteria to guide us in compressing sentiment
sentences when annotating, and can also help us exploit more useful features for automatic sentiment
sentence compression.
3.2 Task Definition
We focus on studying the methods for extractive sentence compression.
3
Formally, extractive sentence
compression aims to shorten a sentence x = x
1
? ? ?x
n
into a substring y = y
1
? ? ? y
m
, where y
i
?
{x
1
, ? ? ? , x
n
}, m ? n.
In this paper, similar to Nomoto (2007), we also treated the sentence compression as a sequence
labeling task which can be solved by a CRF model. We assigned a compression tag t
i
to each word x
i
in
an original sentence x, where t
i
= N if x
i
? y, else t
i
= Y.
A first-order linear-chain CRF is used which defines the following conditional probability:
P (t|x) =
1
Z(x)
?
i
M
i
(t
i
, t
i?1
|x) (1)
where x and t are the input and output sequences respectively, Z(x) is the partition function, and M
i
is
the clique potential for edge clique i. Here, we used the CRFsuite toolkit to train the CRF model.
4
3.3 Features
The features for Sent Comp are listed in Table 1. Aside from the basic word (w), POS tag (t) and
their combination context features (01 ? 04), we introduced some sentiment-related features (05 ? 06)
and latent semantic features (07 ? 08) to better handle sentiment analysis data and generalize word
features. Then we added the syntactic parse features (09), which are commonly used in traditional
sentence compression task.
One sentiment-related feature (feeling(?)) indicates whether a word is a feeling word, which is inspired
by the naturalness problem in Figure 3(b). As discussed above, the current parser often produces wrong
parse trees because of these feeling words. Therefore, the feeling words tend to be removed from a
3
Generally, there are two kinds of sentence compression methods: extractive method and abstractive method. Because
abstractive method needs more resource and is more complicated, in this paper, we only focus on extractive approach.
4
www.chokkan.org/software/crfsuite/
1363
Basic Features
01: w
i+k
,?1 ? k ? 1
02: w
i+k?1
? w
i+k
, 0 ? k ? 1
03: t
i+k
,?2 ? k ? 2
04: t
i+k?1
? t
i+k
,?1 ? k ? 2
Sentiment-related Features
05: feeling(w
i
)
06: polarity(w
i
)
Latent Semantic Features
07: suffix(w
i
) if t(w
i
) == n else prefix(w
i
)
08: cluster(w
i
)
Syntactic Features
09: dependency(w
i
)
Table 1: Features of sentiment sentence compression
sentiment sentence for Sent Comp. We can obtain a feeling word lexicon from HowNet,
5
a popular
Chinese sentiment thesaurus, where a feeling word is defined by DEF={perception|??} tag. Finally,
we collected 38 feeling words, such as?? (realize),?? (find), and?? (think).
The other sentiment-related feature (polarity(?)) indicates whether a word is a polarity word. One
of the main differences between a sentiment sentence and a formal sentence is that the former often
contains polarity words. In contrast to the features of feeling(?), polarity words (e.g., ?great? in the
sentence ?Overall, this is a great camera?) tend to be retained, because they are important and special
to sentiment analysis. In this paper, we treat polarity words as important features, considering that they
are often tagged as modifiers and are easily removed by common sentence compression methods. We
can obtain the polarity feature (polarity(?)) from a polarity lexicon, which can also be obtained from
HowNet.
To generalize the words in sentiment sentences, we proposed two kinds of semantic features. The
first one is a suffix or prefix character feature (prefix(?) or suffix(?)). In contrast to English, the suffix
(for noun) or prefix (for non noun) characters of a Chinese word often carry that word?s core semantic
information. For example, ??? (bicycle), ?? (car), and ?? (train) are all various kinds of ?
(vehicle), which is also the suffix of the three words. Given that all of them may become targets, they
tend to be retained in compressed sentences. The verbs, ?? and ??, can be denoted by their prefix
feel (?), and can be removed from original sentences because they are feeling words.
We used word clustering features (cluster(?)) as the other latent semantic feature to further improve
the generalization over common words. Word clustering features contain some semantic information
and have been successfully used in several natural language processing tasks, including NER (Miller et
al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words ??
and ?? (appearance) belong to the same word cluster, although they have a different suffix or prefix.
Both words are important for T-P collocation extraction and should be retained. We used the Brown
word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were
obtained from the fifth edition of Chinese Gigaword (LDC2011T13).
Finally, similar to McDonald (2006), we also added the dependency relation between a word and its
parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence
compression. For example, the ROOT relation typically indicates that the word should not be removed
because it is the main verb of a sentence.
4 Experiments
4.1 Experimental Setup
4.1.1 Corpus
We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3
of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).
6
Table 2 describes the corpus,
5
www.keenage.com
6
www.ir-china.org.cn/coae2008.html
1364
Domain # reviews # sentences # collocations
Camera 138 1,249 1,335
Car 161 1,172 1,312
Notebook 56 623 674
Phone 123 1,350 1,479
All 478 4,394 4,800
Table 2: Corpus statistics for the Chinese corpus of four product domains.
where 4,394 sentiment sentences containing 4,800 T-P collocations are manually found and annotated
from 478 reviews.
We ask annotators to manually compress all the sentiment sentences. Specifically, the annotators
removed some words from a sentiment sentence according to two criteria stated as follows: (1) removing
the word should not change the essential content of the sentence, and (2) removing the word should
not change the sentiment orientation of the sentence. In order to assess the quality of the annotation,
we sampled 500 sentences from this corpus and asked two annotators to perform the annotation. The
resulting word-based Cohen?s kappa (Cohen, 1960) (i.e., a measure of inter-annotator agreement ranging
from zero to one) of 0.7 indicated a good strength of agreement.
4.1.2 Evaluation
Generally, compressions are evaluated using three criteria (McDonald, 2006), namely, grammaticality,
importance, and compression rate. Obviously, the former two are difficult to evaluate objectively. Previ-
ous works used human judgment, which entails a difficult and expensive process. In this paper, similar to
a common sequence labeling task, we simply used the F-score metric of removed words to roughly eval-
uate the performance of sentiment sentence compression. Of course, the final effectiveness of sentence
compression model can be reviewed by the derived T-P collocation extraction task.
For T-P collocation extraction, we applied the traditional P, R and F-score for the final evaluations.
Specially, a fuzzy matching evaluation is adopted for the T-P collocation extraction. That is to say,
given an extracted T-P collocation ?t, p?, whose standard result is ?t
s
, p
s
?, if t is the substring of t
s
, and
meanwhile p is the substring of p
s
, we consider the extracted ?t, p? is a correct T-P collocation.
4.2 Sentiment Sentence Compression Results
Features P(%) R(%) F(%)
Basic (01 ? 04) 76.4 57.4 65.5
+ feeling (05) 75.9 57.6 65.5
+ polarity (06) 76.6 57.6 65.7
+ suffix or prefix (07) 78.4 56.9 66.0
+ cluster (08) 74.9 58.9 65.9
+ dependency (09) 75.3 57.2 65.0
All (01 ? 08) 77.3 59.1 67.0
All - feeling (05) 77.1 58.9 66.8
Table 3: The results of sentiment sentence compression with different features.
Results of Sent Comp with different features are shown in Table 3. All results are reported using five-
fold cross validation. We can see that the performance is improved when we added feeling
7
and polarity
features (05 ? 06) respectively, indicating that the sentiment-related features are useful for sentiment
sentence compression. In addition, the latent semantic features (07 ? 08) are also helpful, especially the
suffix or prefix features, which show better performance than the four other kinds of features.
Nonetheless, the dependency features (09) have a negative on compression performance due to the
specificity of compression for sentiment sentences. That is because the lower dependency parsing per-
formance on sentiment sentences introduces many wrong dependency relations, which counteract the
7
In Table 3, although the performance of adding feeling is comparative to the basic system (Basic (01-04)), the system
without feeling (All - feeling (05), the last line) is worse than the system using all the features (All (01-08)). This can illustrate
the effectiveness of the feeling feature.
1365
Domain Method P(%) R(%) F(%)
no Comp 74.7 58.4 65.6
Camera manual Comp 83.4 62.7 71.6
auto Comp 80.4 62.1 70.1
no Comp 68.2 53.1 59.7
Car manual Comp 76.3 57.7 65.7
auto Comp 72.3 56.1 63.2
no Comp 74.1 56.8 64.3
Notebook manual Comp 82.7 64.5 72.5
auto Comp 79.7 62.8 70.2
no Comp 77.3 60.9 68.1
Phone manual Comp 82.7 65.7 73.2
auto Comp 80.3 63.3 70.8
no Comp 73.7 57.5 64.6
All manual Comp 81.2 62.5 70.6
auto Comp 78.1 60.9 68.4
Table 4: Results on T-P collocation extraction for four product domains.
contribution of the dependency relation features. This is also the reason why we need to compress sen-
timent sentences as the first step for T-P collocation extraction. Finally, when we combine all of useful
features (01 ? 08), the performance achieves the highest score.
It is worth noting that sentiment sentence compression is a new task proposed in this paper. For
simplicity, this paper aims to attempt a simple yet effective sentiment sentence compression model. We
will polish the Sent Comp model in the future work.
4.3 Sent Comp for T-P Collocation Extraction
We designed three comparative systems to demonstrate the effectiveness of Sent Comp for T-P collo-
cation extraction. Note that, Sent Comp is the first step to process the corpus before T-P collocation
extraction. The method for T-P collocation extraction was based on the state-of-the-art method proposed
by Qiu et al. (2011) as described in Section 2.
no Comp - This refers to the system that only uses the T-P collocation extraction method and does not
perform sentence compression as the first step.
manual Comp - This system manually compresses the corpus into a new one as the first step, and then
applies the T-P collocation extraction method on the new compressed corpus.
auto Comp - This system uses Sent Comp as the first step to automatically compress the corpus into a
new one, and then applies the T-P collocation extraction method on the new corpus.
From the descriptions above, we can draw a conclusion that the performance of manual Comp can be
considered as the upper bound for the sentiment sentence compression based T-P collocation extraction
task.
Table 4 shows the experimental results of the three systems on T-P collocation extraction for four prod-
uct domains. Here, manual Comp can significantly (p < 0.01) improved the F-score by approximately
6%,
8
compared with no Comp. This illustrates that the idea of sentiment sentence compression is use-
ful for T-P collocation extraction. Specifically, the proposed method can transform some over-natural
sentences into normal ones, further influencing their final syntactic parsers. Evidently, because the T-P
collocation extraction relies heavily on syntactic features, the more correct syntactic parse trees derived
from the compressed sentences can help to increase the performance of this task.
Compared with no Comp, the auto Comp system also yielded a significantly better results (p < 0.01)
that indicated an improvement of 3.8% in the F-score, despite the fact that the automatic sentence com-
pression model Sent Comp may wrongly compress some sentences. This demonstrates the usefulness
of sentiment sentence compression step in the T-P collocation extraction task and further proves the
effectiveness of our proposed model.
8
We use paired bootstrap resampling significance test (Efron and Tibshirani, 1993).
1366
Moreover, we can observe that the idea of sentence compression and our Sent Comp are useful for
all the four product domains on T-P collocation extraction task, indicating that Sent Comp is domain
adaptive. However, we can find a small gap between auto Comp and manual Comp, which indicates
that the Sent Comp model can still be improved further. In the future, we will explore more effective
sentence compression algorithms to bridge the gap between the two systems.
5 Related Works
5.1 Sentiment Analysis
T-P collocation extraction is a basic task in sentiment analysis. In order to solve this task, most methods
focused on identifying relationships between targets and polarity words. In early studies, researcher-
s recognized the target first, and then chose its polarity word within a window of size k (Hu and Liu,
2004). However, considering that this kind of method is too heuristic, the performance proved to be very
limited. To tackle this problem, many researchers found syntactic patterns that can better describe the
relationships between targets and polarity words. For example, Bloom et al. (2007) constructed a link-
age specification lexicon containing 31 patterns, while Qiu et al. (2011) proposed a double propagation
method that introduced eight heuristic syntactic patterns to extract the collocations. Xu et al. (2013) used
the syntactic patterns to extract the collocation candidates in their two-stage framework.
Based on the above, we can conclude that syntactic features are very important for T-P collocation
extraction. However, the ?naturalness? problem can still seriously affect the performance of syntactic
parser. Once our sentiment sentence compression method can improve the quality of parsing, the perfor-
mance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous
work using a sentence compression model to improve this task.
5.2 Sentence Compression
Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones,
while preserving the essential content (Jing, 2000). There are many applications that can benefit from
a robust compression system, such as summarization systems (Li et al., 2013), semantic role label-
ing (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on.
Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and
Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos,
2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the
syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus,
the compressed tree (after removing constituents from a bad parse) may not produce a good compressed
sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem
by using discriminative models.
Aside from above extractive sentence compression approaches, there is another research line, namely,
abstractive approach, which compresses an original sentence by reordering, substituting, and inserting,
as well as removing (Cohn and Lapata, 2013). This method needs more resource and is more complicat-
ed. Therefore, in this paper, we only focus on extractive approach.
At present, the current sentence compression methods all focus on formal sentences, and few meth-
ods are being proposed to study sentiment sentences. As discussed in the above sections, the current
compression models cannot be directly utilized to T-P collocation extraction owing to the specificity of
sentiment sentences. Therefore, a new compression model for sentiment sentences should be established.
6 Conclusion and Future Work
In this work, we presented a framework that adopted a CRF based sentiment sentence compression mod-
el Sent Comp, as a preprocessing step, to improve the T-P collocation extraction task. Different from
the existing sentence compression models used for formal sentences, Sent Comp incorporated some
sentiment-related features to retain the sentiment information. Experimental results showed that the sys-
tem with the sentence compression step performed better than that without this step, thus demonstrating
the effectiveness of the framework and the compression model Sent Comp.
1367
Generally, the idea of this framework maybe useful for many sentiment analysis tasks that rely heavily
on syntactic results. Thus in the future, we will try to apply the Sent Comp model for these tasks. Besides,
the simplicity and effectiveness of this framework motivates us to pursue the study further. For example,
we will polish the Sent Comp model by exploring more sentiment-related features and exploring other
types of compression models.
Acknowledgments
We thank the anonymous reviewers for their helpful comments. This work was supported by National
Natural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, the Ministry
of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Fundamen-
tal Research Funds for the Central Universities via grant No.HIT.NSRIF.2013090 and IBM Research-
China Joint Research Project.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In HLT-NAACL
2007, pages 308?315.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18(4):467?479, December.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010:
Demonstrations, pages 13?16, Beijing, China, August. Coling 2010 Organizing Committee.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. J. Artif. Intell. Res. (JAIR), 31:399?429.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1):37 ? 46.
Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
Trevor Cohn and Mirella Lapata. 2013. An abstractive approach to sentence compression. ACM Transactions on
Intelligent Systems and Technology, 4(3):1?35.
Adnan Duric and Fei Song. 2012. Feature selection for sentiment analysis based on content and syntax models.
Decis. Support Syst., 53(4):704?711, November.
B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pages 959?967, Columbus, Ohio, June. Association for
Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 885?893, Los Angeles, California, June. Association
for Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.
Association for Computational Linguistics.
1368
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD-2004,
pages 168?177.
Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In IN PROCEEDINGS OF THE 6TH
APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artif. Intell., 139(1):91?107, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio, June. Association for Computational Linguistics.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In In Proc. EACL.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,
pages 337?342, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi Tsujii. 2010. Entity-focused sentence simplification
for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics
(COLING 2010), pages 788?796.
Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Pro-
cessing and Management, 43(6):1571?1587, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational Linguistics, 37(1):9?27.
Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Pro-
ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 290?
297, Stroudsburg, PA, USA. Association for Computational Linguistics.
David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In ACL, pages
344?352. The Association for Computer Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar
and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pages 409?420, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese pinion
analysis evaluation 2008. In The First Chinese Opinion Analysis Evaluation (COAE) 2008.
1369
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180?1191,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Joint Models for Chinese POS Tagging and Dependency Parsing
Zhenghua Li?, Min Zhang?, Wanxiang Che?, Ting Liu?, Wenliang Chen? and Haizhou Li?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{lzh,car,tliu}@ir.hit.edu.cn
?Institute for Infocomm Research, Singapore
{mzhang,wechen,hli}@i2r.a-star.edu.sg
Abstract
Part-of-speech (POS) is an indispensable fea-
ture in dependency parsing. Current research
usually models POS tagging and dependency
parsing independently. This may suffer from
error propagation problem. Our experiments
show that parsing accuracy drops by about
6% when using automatic POS tags instead
of gold ones. To solve this issue, this pa-
per proposes a solution by jointly optimiz-
ing POS tagging and dependency parsing in a
unique model. We design several joint models
and their corresponding decoding algorithms
to incorporate different feature sets. We fur-
ther present an effective pruning strategy to re-
duce the search space of candidate POS tags,
leading to significant improvement of parsing
speed. Experimental results on Chinese Penn
Treebank 5 show that our joint models sig-
nificantly improve the state-of-the-art parsing
accuracy by about 1.5%. Detailed analysis
shows that the joint method is able to choose
such POS tags that are more helpful and dis-
criminative from parsing viewpoint. This is
the fundamental reason of parsing accuracy
improvement.
1 Introduction
In dependency parsing, features consisting of part-
of-speech (POS) tags are very effective, since pure
lexical features lead to severe data sparseness prob-
lem. Typically, POS tagging and dependency pars-
ing are modeled in a pipelined way. However, the
pipelined method is prone to error propagation, es-
pecially for Chinese. Due to the lack of morpholog-
ical features, Chinese POS tagging is even harder
than other languages such as English. The state-of-
the-art accuracy of Chinese POS tagging is about
93.5%, which is much lower than that of English
(about 97% (Collins, 2002)). Our experimental re-
sults show that parsing accuracy decreases by about
6% on Chinese when using automatic POS tagging
results instead of gold ones (see Table 3 in Section
5). Recent research on dependency parsing usually
overlooks this issue by simply adopting gold POS
tags for Chinese data (Duan et al, 2007; Zhang and
Clark, 2008b; Huang and Sagae, 2010). In this pa-
per, we address this issue by jointly optimizing POS
tagging and dependency parsing.
Joint modeling has been a popular and effec-
tive approach to simultaneously solve related tasks.
Recently, many successful joint models have been
proposed, such as joint tokenization and POS tag-
ging (Zhang and Clark, 2008a; Jiang et al, 2008;
Kruengkrai et al, 2009), joint lemmatization and
POS tagging (Toutanova and Cherry, 2009), joint
tokenization and parsing (Cohen and Smith, 2007;
Goldberg and Tsarfaty, 2008), joint named en-
tity recognition and parsing (Finkel and Manning,
2009), joint parsing and semantic role labeling
(SRL) (Li et al, 2010), joint word sense disambigua-
tion and SRL (Che and Liu, 2010), joint tokenization
and machine translation (MT) (Dyer, 2009; Xiao et
al., 2010) and joint parsing and MT (Liu and Liu,
2010). Note that the aforementioned ?parsing? all
refer to constituent parsing.
As far as we know, there are few successful mod-
els for jointly solving dependency parsing and other
tasks. Being facilitated by Conference on Com-
putational Natural Language Learning (CoNLL)
2008 and 2009 shared tasks, several joint models
of dependency parsing and SRL have been pro-
posed. Nevertheless, the top-ranked systems all
adopt pipelined approaches (Surdeanu et al, 2008;
1180
Hajic? et al, 2009). Theoretically, joint modeling
of POS tagging and dependency parsing should be
helpful to the two individual tasks. On the one hand,
syntactic information can help resolve some POS
ambiguities which are difficult to handle for the se-
quential POS tagging models. On the other hand,
more accurate POS tags should further improve de-
pendency parsing.
For joint POS tagging and dependency parsing,
the major issue is to design effective decoding algo-
rithms to capture rich features and efficiently search
out the optimal results from a huge hypothesis
space.1 In this paper, we propose several dynamic
programming (DP) based decoding algorithms for
our joint models by extending existing parsing algo-
rithms. We also present effective pruning techniques
to speed up our decoding algorithms. Experimen-
tal results on Chinese Penn Treebank show that our
joint models can significantly improve the state-of-
the-art parsing accuracy by about 1.5%.
The remainder of this paper is organized as fol-
lows. Section 2 describes the pipelined method, in-
cluding the POS tagging and parsing models. Sec-
tion 3 discusses the joint models and the decod-
ing algorithms, while Section 4 presents the pruning
techniques. Section 5 reports the experimental re-
sults and error analysis. We review previous work
closely related to our method in Section 6, and con-
clude this paper in Section 7.
2 The Baseline Pipelined Method
Given an input sentence x = w1...wn, we denote its
POS tag sequence by t = t1...tn, where ti ? T , 1 ?
i ? n, and T is the POS tag set. A dependency tree
is denoted by d = {(h,m) : 0 ? h ? n, 0 < m ?
n}, where (h,m) represents a dependency wh ?
wm whose head word (or father) is wh and modifier
(or child) is wm. w0 is an artificial root token which
is used to simplify the formalization of the problem.
The pipelined method treats POS tagging and de-
pendency parsing as two cascaded problems. First,
1It should be noted that it is straightforward to simultane-
ously do POS tagging and constituent parsing, as POS tags can
be regarded as non-terminals in the constituent structure (Levy
and Manning, 2003). In addition, Rush et al (2010) describes
an efficient and simple inference algorithm based on dual de-
composition and linear programming relaxation to combine a
lexicalized constituent parser and a trigram POS tagger.
an optimal POS tag sequence t? is determined.
t? = arg max
t
Scorepos(x, t)
Then, an optimal dependency tree d? is determined
based on x and t?.
d? = arg max
d
Scoresyn(x, t?,d)
2.1 POS Tagging
POS tagging is a typical sequence labeling prob-
lem. Many models have been successfully applied
to sequence labeling problems, such as maximum-
entropy (Ratnaparkhi, 1996), conditional random
fields (CRF) (Lafferty et al, 2001) and perceptron
(Collins, 2002). We use perceptron to build our POS
tagging baseline for two reasons. Firstly, as a linear
model, perceptron is simple, fast, and effective. It is
competitive to CRF in tagging accuracy but requires
much less training time (Shen et al, 2007). Sec-
ondly, perceptron has been successfully applied to
dependency parsing as well (Koo and Collins, 2010).
In this paper, perceptron is used in all models includ-
ing the POS tagging model, the dependency parsing
models and the joint models.
In a perceptron, the score of a tag sequence is
Scorepos(x, t) = wpos ? fpos(x, t)
where fpos(x, t) refers to the feature vector andwpos
is the corresponding weight vector.
For POS tagging features, we follow the work of
Zhang and Clark (2008a). Three feature sets are
considered: POS unigram, bigram and trigram fea-
tures. For brevity, we will refer to the three sets as
wi ti, ti?1 ti and ti?2 ti?1 ti.
Given wpos, we adopt the Viterbi algorithm to get
the optimal tagging sequence.
2.2 Dependency Parsing
Recently, graph-based dependency parsing has
gained more and more interest due to its state-of-
the-art accuracy. Graph-based dependency parsing
views the problem as finding the highest scoring tree
from a directed graph. Based on dynamic program-
ming decoding, it can efficiently find an optimal tree
in a huge search space. In a graph-based model, the
1181
score of a dependency tree is factored into scores of
small parts (subtrees).
Scoresyn(x, t,d) = wsyn ? fsyn(x, t,d)
=
?
p?d
Scoresyn(x, t, p)
where p is a scoring part which contains one or more
dependencies in the dependency tree d. Figure 1
shows different types of scoring parts used in current
graph-based models.
h m
dependency
h s
sibling
m g h
grandparent
m
h s
tri-sibling
mth s
grand-sibling
mg
Figure 1: Different types of scoring parts used in current
graph-based models (Koo and Collins, 2010).
Eisner (1996) proposes an O(n3) decoding al-
gorithm for dependency parsing. Based on the al-
gorithm, McDonald et al (2005) propose the first-
order model, in which the scoring parts only con-
tains dependencies. The second-order model of Mc-
Donald and Pereira (2006) incorporates sibling parts
and also needs O(n3) parsing time. The second-
order model of Carreras (2007) incorporates both
sibling and grandparent parts, and needs O(n4)
parsing time. However, the grandparent parts are
restricted to those composed of outermost grand-
children. Koo and Collins (2010) propose efficient
decoding algorithms of O(n4) for third-order mod-
els. In their paper, they implement two versions
of third-order models, Model 1 and Model 2 ac-
cording to their naming. Model 1 incorporates only
grand-sibling parts, while Model 2 incorporates both
grand-sibling and tri-sibling parts. Their experi-
ments on English and Czech show that Model 1 and
Model 2 obtain nearly the same parsing accuracy.
Therefore, we use Model 1 as our third-order model
in this paper.
We use three versions of graph-based dependency
parsing models.
? The first-order model (O1): the same with Mc-
Donald et al (2005).
? The second-order model (O2): the same with
Model 1 in Koo and Collins (2010), but without
using grand-sibling features.2
? The third-order model (O3): the same with
Model 1 in Koo and Collins (2010).
We adopt linear models to define the score of a de-
pendency tree. For the third-order model, the score
of a dependency tree is represented as:
Scoresyn(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s)(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
+
?
{(g,h),(h,s),(h,m)}?d
wgsib ? fgsib(x, t, g, h, s,m)
For the first- and second-order models, the above
formula is modified by deactivating extra parts.
For parsing features, we follow standard prac-
tice for graph-based dependency parsing (McDon-
ald, 2006; Carreras, 2007; Koo and Collins, 2010).
Since these features are highly related with our joint
decoding algorithms, we summarize the features as
follows.
? Dependency Features, fdep(x, t, h,m)
? Unigram Features: whth dir, wmtm dir
? Bigram Features: whth wmtm dir dist
? In Between Features: th tb tm dir dist
? Surrounding Features:
th?1 th th+1 tm?1 tm tm+1 dir dist
? Sibling Features, fsib(x, t, h, s,m)
wh th ws ts wm tm dir
? Grandparent Features, fgrd(x, t, g, h,m)
wg tg wh th wm tm dir gdir
? Grand-sibling Features, fgsib(x, t, g, h, s,m)
wg tg wh th ws ts wm tm dir gdir
2This second-order model incorporates grandparent features
composed of all grandchildren rather than just outermost ones,
and outperforms the one of Carreras (2007) according to the
results in Koo and Collins (2010).
1182
where b denotes an index between h and m; dir
and dist are the direction and distance of (h,m);
gdir is the direction of (g, h). We also use back-
off features by generalizing from very specific fea-
tures over word forms, POS tags, directions and dis-
tances to less sparse features over just POS tags or
considering fewer nodes. To avoid producing too
many sparse features, at most two word forms are
used at the same time in sibling, grandparent and
grand-sibling features, while POS tags are used in-
stead for other nodes; meanwhile, at most four POS
tags are considered at the same time for surrounding
features.
3 Joint Models
In the joint method, we aim to simultaneously solve
the two problems.
(t?, d?) = arg max
t,d
Scorejoint(x, t,d)
Under the linear model, the score of a tagged de-
pendency tree is:
Scorejoint(x, t,d) = Scorepos(x, t)
+ Scoresyn(x, t,d)
= wpos?syn ? fpos?syn(x, t,d)
where fpos?syn(.) means the concatenation of fpos(.)
and fsyn(.). Under the joint model, the weights of
POS and syntactic features, wpos?syn, are simulta-
neously learned. We expect that POS and syntactic
features can interact each other to determine an op-
timal joint result.
Similarly to the baseline dependency parsing
models, we define the first-, second-, and third-order
joint models according to the syntactic features con-
tained in fsyn(.).
In the following, we propose two versions of joint
models which can capture different feature sets and
have different complexity.
3.1 Joint Models of Version 1
The crucial problem for the joint method is to de-
sign effective decoding algorithms to capture rich
features and efficiently search out the optimal re-
sults from a huge hypothesis space. Eisner (2000)
describes a preliminary idea to handle polysemy by
extending parsing algorithms. Based on this idea,
we extend decoding algorithms of McDonald et al
(2005) and Koo and Collins (2010), and propose two
DP based decoding algorithms for our joint models
of version 1.
(b)
(a)
i r r j
r+1 ji ri j
i j
Figure 2: The DP structures and derivations of the first-
order decoding algorithm of joint models of version 1.
We omit symmetric right-headed versions for brevity.
Trapezoids denote incomplete spans. Triangles denote
complete spans. Solid circles denote POS tags of the cor-
responding indices.
The decoding algorithm of O1: As shown in
Figure 2, the first-order joint decoding algorithm
utilizes two types of dynamic programming struc-
tures. (1) Incomplete spans consist of a dependency
and the region between the head and modifier; (2)
Complete spans consist of a headword and its de-
scendants on one side. Each span is recursively cre-
ated by combining two smaller and adjacent spans
in a bottom-up fashion.
The pseudo codes are given in Algorithm 1.
I(i,j)(ti,tj) denotes an incomplete span from i to j
whose boundary POS tags are ti and tj . C(i,j)(ti,tj)
refers to a complete span from i to j whose bound-
ary POS tags are ti and tj . Conversely, I(j,i)(tj ,ti)
andC(j,i)(tj ,ti) represent spans of the other direction.
Note that in these notations the first argument index
always refers to the head of the span.
Line 6 corresponds to the derivation in Figure 2-
(a). Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)}) cap-
tures the joint features invented by this combina-
tion, where p = {(i, j)} means that the newly ob-
served scoring part is the dependency (i, j). The
syntactic features, denoted by fsyn(x, ti, tj , i, j), can
only incorporate syntactic unigram and bigram fea-
tures. The surrounding and in between features
are unavailable, because the context POS tags, such
as tb and ti?1, are not contained in the DP struc-
1183
Algorithm 1 The first-order joint decoding algorithm of version 1
1: ?0 ? i ? n, ti ? T C(i,i)(ti,ti) = 0 ? initialization
2: for w = 1..n do ? span width
3: for i = 0..(n? w) do ? span start index
4: j = i + w ? span end index
5: for (ti, tj) ? T 2 do
6: I(i,j)(ti,tj) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)})}7: I(j,i)(tj ,ti) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(j, i)})}8: C(i,j)(ti,tj) = maxi<r?j maxtr?T {I(i,r)(ti,tr) + C(r,j)(tr,tj) + Scorejoint(x, ti, tr, tj , p = ?)}
9: C(j,i)(tj ,ti) = maxi?r<j maxtr?T {C(r,i)(tr,ti) + I(j,r)(tj ,tr) + Scorejoint(x, ti, tr, tj , p = ?)}
10: end for
11: end for
12: end for
tures. Therefore, we adopt pseudo surrounding
and in between features by simply fixing the con-
text POS tags as the single most likely ones (Mc-
Donald, 2006). Taking the in between features
as an example, we use ti t?b tj dir dist instead,
where t?b is the 1-best tag determined by the base-
line POS tagger. The POS features, denoted by
fpos(x, ti, tr, tr+1, tj), can only incorporate all POS
unigram and bigram features.3 Similarly, we use
pseudo POS trigram features such as t?r?1 tr tr+1.
Line 8 corresponds to the derivation in Figure 2-
(b). Since this combination invents no scoring part
(p = ?), Scorejoint(x, ti, tr, tj , p = ?) is only com-
posed of POS features.4
Line 7 and Line 9 create spans in the opposite di-
rection, which can be analogously illustrated. The
space and time complexity of the algorithm are re-
spectively O(n2q2) and O(n3q4), where q = |T |.5
The decoding algorithm of O2 & O3: Figure
3 illustrates the second- and third-order decoding
algorithm of joint models of version 1. A new
kind of span, named the sibling span, is used to
capture sibling structures. Furthermore, each span
is augmented with a grandparent-index to capture
both grandparent and grand-sibling structures. It is
straightforward to derive the pseudo codes of the al-
3? wr tr if i ?= r; ? wr+1 tr+1 if r + 1 ?= j; ? tr tr+1
if r ?= i or r + 1 ?= j; ? ti tr if r ? 1 = i; ? tr+1 tj if
r + 2 = j. Note that wi ti, wj tj and ti tj (if i = j ? 1) are
not incorporated here to avoid double counting.
4? wr tr if r ?= j;? ti tr if i = r?1;? tr tj if r+1 = j.
Pseudo trigram features can be added accordingly.
5We can reduce the time complexity to O(n3q3) by strictly
adopting the DP structures in the parsing algorithm of Eisner
(1996). However, that may make the algorithm harder to com-
prehend.
ig j g i i ji+1
(a)
g i j g i k i k j
(b)
i k j i ik jr+1r
(c)
g i j g i r i r j
(d)
Figure 3: The DP structures and derivations of the
second- and third-order joint decoding algorithm of ver-
sion 1. For brevity, we elide the right-headed and right-
grandparented versions. Rectangles represent sibling
spans.
i j i r r j
i j i r r+1 j
(b)
(a)
Figure 4: The DP structures and derivations of the first-
order joint decoding algorithm of version 2. We omit the
right-headed version for brevity.
1184
gorithm from Figure 3. We omit them due to space
limitation. Pseudo surrounding, in between and POS
trigram features are used due to the same reason as
above. The space and time complexity of the algo-
rithm are respectively O(n3q3) and O(n4q5).
3.2 Joint Models of Version 2
To further incorporate genuine syntactic surround-
ing and POS trigram features in the DP structures,
we extend the algorithms of joint models of version
1, and propose our joint models of version 2.
The decoding algorithm of O1: Figure 4 illus-
trates the first-order joint decoding algorithm of ver-
sion 2. Compared with the structures in Figure 2,
each span is augmented with the POS tags surround-
ing the boundary indices. These context POS tags
enable Scorejoint(.) in line 6-9 of Algorithm 1 to
capture the syntactic surrounding and POS trigram
features, but also require enumeration of POS tags
over more indices. For brevity, we skip the pseudo
codes which can be easily derived from Algorithm
1. The space and time complexity of the algorithm
are respectively O(n2q6) and O(n3q10).
The decoding algorithm of O2 & O3: Using the
same idea as above, the second- and third-order joint
decoding algorithms of version 2 can be derived
based on Figure 3. Again, we omit both its DP struc-
tures and pseudo codes for the sake of brevity. Its
space and time complexity are respectively O(n3q7)
and O(n4q11).
In between features, which should be regarded as
non-local features in the joint situation, still cannot
be incorporated in our joint models of version 2.
Again, we adopt the pseudo version.
3.3 Comparison
Based on the above illustration, we can see that joint
models of version 1 are more efficient with regard
to the number of POS tags for each word, but fail to
incorporate syntactic surrounding features and POS
trigram features in the DP structures. On the con-
trary, joint models of version 2 can incorporate both
aforementioned feature sets, but have higher com-
plexity. These two versions of models will be thor-
oughly compared in the experiments.
4 Pruning Techniques
In this section, we introduce two pruning strategies
to constrain the search space of our models due to
their high complexity.
4.1 POS Tag Pruning
The time complexity of the joint decoding algorithm
is unbearably high with regard to the number of can-
didate POS tags for each word (q = |T |). We
find that it would be extremely time-consuming even
when we only use two most likely POS tags for each
word (q = 2) even for joint models of version 1.
To deal with this problem, we propose a pruning
method that can effectively reduce the POS tag space
based on a probabilistic tagging model.
We adopt a conditional log-linear model (Lafferty
et al, 2001), which defines a conditional distribution
of a POS tag sequence t given x:
P (t|x) = e
wpos?fpos(x,t)
?
t ewpos?fpos(x,t)
We use the same feature set fpos defined in Sec-
tion 2.1, and adopt the exponentiated gradient algo-
rithm to learn the weight vector wpos (Collins et al,
2008).
The marginal probability of tagging a word wi as
t is
P (ti = t|x) =
?
t:t[i]?t
P (t|x)
which can be efficiently computed using the
forward-backward algorithm.
We define pmaxi(x) to be the highest marginal
probability of tagging the word wi:
pmaxi(x) = maxt?T P (ti = t|x)
We then define the allowable candidate POS tags
of the word wi to be
Ti(x) = {t : t ? T , P (ti = t|x) ? ?t?pmaxi(x)}
where ?t is the pruning threshold. Ti(x) is used to
constrain the POS search space by replacing T in
Algorithm 1.
1185
4.2 Dependency Pruning
The parsing time grows quickly for the second- and
third-order models (both baseline and joint) when
the input sentence gets longer (O(n4)). Follow-
ing Koo and Collins (2010), we eliminate unlikely
dependencies using a form of coarse-to-fine prun-
ing (Charniak and Johnson, 2005; Petrov and Klein,
2007). On the development set, 68.87% of the de-
pendencies are pruned, while the oracle dependency
accuracy is 99.77%. We use 10-fold cross validation
to do pruning on the training set.
5 Experiments
We use the Penn Chinese Treebank 5.1 (CTB5) (Xue
et al, 2005). Following the setup of Duan et al
(2007), Zhang and Clark (2008b) and Huang and
Sagae (2010), we split CTB5 into training (secs 001-
815 and 1001-1136), development (secs 886-931
and 1148-1151), and test (secs 816-885 and 1137-
1147) sets. We use the head-finding rules of Zhang
and Clark (2008b) to turn the bracketed sentences
into dependency structures.
We use the standard tagging accuracy to evalu-
ate POS tagging. For dependency parsing, we use
word accuracy (also known as dependency accu-
racy), root accuracy and complete match rate (all
excluding punctuation) .
For the averaged training, we train each model for
15 iterations and select the parameters that perform
best on the development set.
5.1 Results of POS Tag Pruning
Figure 5 shows the distribution of words with dif-
ferent number of candidate POS tags and the k-best
oracle tagging accuracy under different ?t. To avoid
dealing with words that have many candidate POS
tags, we further apply a hard criterion that the decod-
ing algorithms only consider top k candidate POS
tags.
To find the best ?t, we train and evaluate the
second-order joint model of version 1 on the train-
ing and development sets pruned with different ?t
(top k = 5). We adopt the second-order joint model
of version 1 because of its efficiency compared with
the third-order models and its capability of captur-
ing rich features compared with the first-order mod-
els. The results are shown in Table 1. The model
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 >5
pro
por
tion
 of 
wor
ds (
%)
number of candidate POS tags 
0.1
0.01
0.001
93
94
95
96
97
98
99
100
1 2 3 4 5 ?
k-b
est 
ora
cle 
tagg
ing
 acc
ura
cy (
%)
k
0.1
0.01
0.001
Figure 5: Results of POS tag pruning with different prun-
ing threshold ?t on the development set.
?t word root compl. acc. speed
0.1 81.53 76.88 30.00 94.17 2.5
0.01 81.83 76.62 30.62 93.16 1.2
0.001 81.73 77.38 30.50 93.41 0.5
Table 1: Performance of the second-order joint model of
version 1 with different pruning threshold ?t (top k = 5)
on the development set. ?Acc.? means the tagging accu-
racy. ?Speed? refers to the parsing speed (the number of
sentences processed per second).
with ?t = 0.1 obtains the highest tagging accuracy,
which is much higher than that of both ?t = 0.01
and ?t = 0.001. However, its parsing accuracy
is inferior to the other two. ?t = 0.01 produces
slightly better parsing accuracy than ?t = 0.001,
and is twice faster. Finally, we choose ?t = 0.01
due to the efficiency factor and our priority over the
parsing accuracy.
Then we do experiments to find an optimal top
k. Table 2 shows the results. We decide to choose
k = 3 since it leads to best parsing accuracy.
From Table 1 and 2, we can have an interesting
finding: it seems that the harder we filter the POS
tag space, the higher tagging accuracy we get. In
other words, giving the joint model less flexibility
of choosing POS tags leads to better tagging per-
formance.
Due to time limitation, we do not tune ?t and k for
other joint models. Instead, we simply adopt ?t =
0.01 and top k = 3.
5.2 Final Results
Table 3 shows the final results on the test set. We list
a few state-of-the-art results in the bottom. Duan07
refers to the results of Duan et al (2007). They
enhance the transition-based parsing model with
1186
Syntactic Metrics Tagging Accuracy Parsing Speed
word root compl. all-word known unknown Sent/Sec
Joint Models V2
O3 80.79 75.84 29.11 92.80 93.88 76.80 0.3
O2 80.49 75.49 28.24 92.68 93.77 76.27 0.6
O1 77.37 68.64 23.09 92.96 94.05 76.64 2.0
Joint Models V1
O3 80.69 75.90 29.06 92.89 93.96 76.80 0.5
O2 80.74 75.80 28.24 93.08 94.11 77.53 1.7
O1 77.38 69.69 22.62 93.20 94.23 77.76 8.5
Auto POS
O3 79.29 74.65 27.24
93.51 94.36 80.78
2.0
O2 79.03 74.70 27.19 5.8
O1 75.68 68.06 21.10 17.4
MSTParser2 77.95 72.04 25.50 4.1
MSTParser1 75.84 68.55 21.36 5.2
MaltParser 75.24 65.92 23.19 2.6
Gold POS
O3 86.00 77.59 34.02
100.0 100.0 100.0
-
O2 86.18 78.58 34.07 -
O1 82.24 70.10 26.02 -
MSTParser2 85.24 77.41 33.19 -
MSTParser1 83.04 71.49 27.59 -
MaltParser 82.62 69.34 29.06 -
H&S10 85.20 78.32 33.72 -
Z&C08 single 84.33 76.73 32.79 -
Z&C08 hybrid 85.77 76.26 34.41 -
Duan07 83.88 73.70 32.70 -
Table 3: Final results on the test set. ?Gold POS? means that gold POS tags are used as input by the pipelined parsing
models; while ?Auto POS? means that the POS tags are generated by the baseline POS tagging model.
top k word root compl. acc. speed
2 81.46 76.12 30.50 93.51 2.7
3 82.11 76.75 29.75 93.31 1.7
4 81.75 76.62 30.38 93.25 1.4
5 81.83 76.62 30.62 93.16 1.2
Table 2: Performance of the second-order joint model of
version 1 with different top k (?t = 0.01) on the devel-
opment set.
the beam search. H&S10 refers to the results of
Huang and Sagae (2010). They greatly expand the
search space of the transition-based model by merg-
ing equivalent states with dynamic programming.
Z&C08 refers to the results of Zhang and Clark
(2008b). They use a hybrid model to combine the
advantages of both graph-based and transition-based
models. We also do experiments with two publicly
available and widely-used parsers, MSTParser6 and
MaltParser7. MSTParser1 refers to the first-order
6http://sourceforge.net/projects/mstparser/
7http://maltparser.org/
graph-based model of McDonald et al (2005), while
MSTParser2 is the second-order model of McDon-
ald and Pereira (2006). MaltParser is a transition-
based parsing system. It integrates a number of clas-
sification algorithms and transition strategies. We
adopt the support vector machine classifier and the
arc-standard strategy (Nivre, 2008).
We can see that when using gold tags, our
pipelined second- and third-order parsing models
achieve best parsing accuracy, which is even higher
than the hybrid model of Zhang and Clark (2008b).
It is a little surprising that the second-order model
slightly outperforms the third-order one. This may
be possible, since Koo and Collins (2010) shows that
the third-order model outperforms the second-order
one by only 0.32% on English and 0.07% on Czech.
In addition, we only use basic third-order features.
Both joint models of version 1 and 2 can consis-
tently and significantly improve the parsing accu-
racy by about 1.5% for all first-, second- and third-
order cases. Accidentally, the parsing accuracy of
the second-order joint model of version 2 is lower
1187
error pattern # ? error pattern # ?
DEC ? DEG 237 114 NR ? NN 184 100
NN ? VV 389 73 NN ? NR 106 91
DEG ? DEC 170 39 NN ? JJ 95 70
VV ? NN 453 27 VA ? VV 29 41
P ? VV 52 24 JJ ? NN 126 29
P ? CC 39 13 VV ? VA 67 10
Table 4: Error analysis of POS tagging. # means the
error number of the corresponding pattern made by the
baseline tagging model. ? and ? mean the error number
reduced or increased by the joint model.
than that of its counterparts by about 0.3%. More
experiments and further analysis may be needed to
find out the reason. The two versions of joint models
performs nearly the same, which indicates that using
pseudo surrounding and POS trigram features may
be sufficient for the joint method on this data set.
In summary, we can conclude that the joint frame-
work is certainly helpful for dependency parsing.
It is clearly shown in Table 3 that the joint
method surprisingly hurts the tagging accuracy,
which diverges from our discussion in Section 1.
Some insights into this issue will be given in Sec-
tion 5.3. Moreover, it seems that the more syntac-
tic features the joint method incorporates (from
O1 to O3), the more the tagging accuracy drops.
We suspect that this is because the joint models are
dominated by the syntactic features. Take the first-
order joint model as an example. The dimension of
the syntactic features fsyn is about 3.5 million, while
that of fpos is only about 0.5 million. The gap be-
comes much larger for the second- and third-order
cases.
Comparing the parsing speed, we can find that the
pruning of POS tags is very effective. The second-
order joint model of version 1 can parse 1.7 sen-
tences per second, while the pipelined second-order
parsing model can parse 5.8 sentences per second,
which is rather close considering that there is a fac-
tor of q5.
5.3 Error Analysis
To find out the impact of our joint models on the
individual tasks, we conduct detailed error analy-
sis through comparing the results of the pipelined
second-order parsing model and the second-order
joint model of version 1.
Impact on POS tagging: Table 4 shows how the
joint model changes the quantity of POS tagging er-
ror patterns compared with the pipelined model. An
error pattern ?X ? Y? means that the focus word,
whose true tag is ?X?, is assigned a tag ?Y?. We
choose these patterns with largest reduction or in-
crease in the error number, and rank them in de-
scending order of the variation.
From the left part of Table 4, we can see that
the joint method is clearly better at resolving tag-
ging ambiguities like {VV, NN} and {DEG, DEC}.8
One common characteristic of these ambiguous
pairs is that the local or even whole syntactic struc-
ture will be destructed if the wrong tag is chosen. In
other words, resolving these ambiguities is critical
and helpful from the parsing viewpoint. From an-
other perspective, the joint model is capable of pre-
ferring the right tag with the help of syntactic struc-
tures, which is impossible for the baseline sequential
labeling model.
In contrast, pairs like {NN, NR}, {VV, VA} and
{NN, JJ} only slightly influence the syntactic struc-
ture when mis-tagged. The joint method performs
worse on these ambiguous pairs, as shown in the
right part of Table 4.
Impact on parsing: Table 5 studies the change of
parsing error rates between the pipelined and joint
model on different POS tag patterns. We present the
most typical and prominent patterns in the table, and
rank them in descending order of X?s frequency of
occurrence. We also show the change of proportion
of different patterns, which is consistent with the re-
sults in Table 4.
From the table, we can see the joint model can
achieve a large error reduction (0.8?4.0%) for all
the patterns ?X ? X?. In other words, the joint
model can do better given the correct tags than
the pipelined method.
For all the patterns marked by ?, except for the
ambiguous pair {NN, JJ} (which we find is difficult
to explain even after careful result analysis), the joint
model also reduces the error rates (2.2?15.4%). As
8DEG and DEC are the two POS tags for the frequently used
auxiliary word ??? (de?, of) in Chinese. The associative ???
is tagged as DEG, such as ???/father ? ??/eyes (eyes of
the father)?; while the one in a relative clause is tagged as DEC,
such as ??/he ??/made ? ??/progress (progress that he
made)?.
1188
pattern pipelined jointprop (%) error (%) prop (%) error (%)
NN ? NN 94.6 16.8 -1.1 -1.8
? VV ? 2.9 55.5 -0.5 +15.1
? NR ? 0.8 24.5 +0.7 -2.2
? JJ ? 0.7 17.9 +0.5 +2.1
VV ? VV 89.6 34.2 -0.3 ?4.0
? NN ? 6.6 66.4 -0.4 +0.7
? VA ? 1.0 38.8 +0.1 -15.4
NR ? NR 91.7 15.4 -3.7 -0.8
? NN ? 5.9 21.7 +3.2 -3.7
P ? P 92.8 22.6 +3.4 -3.2
? VV ? 3.0 50.0 -1.4 +10.7
? CC ? 2.3 74.4 -0.7 +21.9
JJ ? JJ 80.5 11.2 -2.8 -2.0
? NN ? 9.8 18.3 +2.2 +1.8
DEG ? DEG 86.5 11.1 +2.8 -3.6
? DEC ? 13.5 61.8 -3.1 +37.4
DEC ? DEC 79.7 17.2 +12.1 -4.0
? DEG ? 20.2 56.5 -9.7 +40.2
Table 5: Comparison of parsing error rates on different
POS tag patterns between the pipelined and joint models.
Given a pattern ?X ? Y?, ?prop? means its proportion in
all occurrence of ?X? (Count(X?Y )Count(X) ), and ?error? refers
to its parsing error rate ( Count(wrongly headed X?Y )Count(X?Y ) ).
The last two columns give the absolute reduction (-) or
increase (+) in proportion and error rate made by the joint
model. ? marks the patterns appearing in the left part of
Table 4, while ? marks those in the right part of Table 4.
discussed earlier, these patterns concern ambiguous
tag pairs which usually play similar roles in syn-
tactic structures. This demonstrates that the joint
model can do better on certain tagging error pat-
terns.
For patterns marked by ?, the error rate of the
joint model usually increases by large margin. How-
ever, the proportion of these patterns is substantially
decreased, since the joint model can better resolve
these ambiguities with the help of syntactic knowl-
edge.
In summary, we can conclude that the joint model
is able to choose such POS tags that are more helpful
and discriminative from parsing viewpoint. This is
the fundamental reason of the parsing performance
improvement.
6 Related Work
Theoretically, Eisner (2000) proposes a preliminary
idea of extending the decoding algorithm for de-
pendency parsing to handle polysemy. Here, word
senses can be understood as POS-tagged words.
Koo and Collins (2010) also briefly discuss that their
third-order decoding algorithm can be modified to
handle word senses using the idea of Eisner (2000).
In his PhD thesis, McDonald (2006) extends his
second-order model with the idea of Eisner (2000)
to study the impact of POS tagging errors on pars-
ing accuracy. To make inference tractable, he uses
top 2 candidate POS tags for each word based on
a maximum entropy tagger, and adopts the single
most likely POS tags for the surrounding and in be-
tween features. He conducts primitive experiments
on English Penn Treebank, and shows that parsing
accuracy can be improved from 91.5% to 91.9%.
However, he finds that the model is unbearably time-
consuming.
7 Conclusions
In this paper, we have systematically investigated
the issue of joint POS tagging and dependency pars-
ing. We propose and compare several joint models
and their corresponding decoding algorithms which
can incorporate different feature sets. We also pro-
pose an effective POS tag pruning method which can
greatly improve the decoding efficiency. The experi-
mental results show that our joint models can signif-
icantly improve the state-of-the-art parsing accuracy
by more than 1.5%. Detailed error analysis shows
that the fundamental reason for the parsing accu-
racy improvement is that the joint method is able to
choose POS tags that are helpful and discriminative
from parsing viewpoint.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 60803093, 60975055, the Natural Scientific
Research Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069) and the Fun-
damental Research Funds for the Central Universi-
ties (HIT.KLOF.2010064).
References
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
1189
EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 161?169.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL 2007, pages 208?217.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 406?
414.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING 1996, pages 340?345.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326?334.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, Ohio, June. Association
for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897?904.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 439?446,
Sapporo, Japan, July. Association for Computational
Linguistics.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117.
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 707?715.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. In Computational Lin-
guistics, volume 34, pages 513?553.
1190
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
760?767, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 486?494.
Xinyan Xiao, Yang Liu, YoungSook Hwang, Qun Liu,
and Shouxun Lin. 2010. Joint tokenization and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1200?1208.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
1191
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 160?170, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Collocation Polarity Disambiguation Using Web-based Pseudo Contexts
Yanyan Zhao, Bing Qin and Ting Liu?
Harbin Institute of Technology, Harbin, China
{yyzhao, bqin, tliu}@ir.hit.edu.cn
Abstract
This paper focuses on the task of colloca-
tion polarity disambiguation. The collocation
refers to a binary tuple of a polarity word and
a target (such as ?long, battery life? or ?long,
startup?), in which the sentiment orientation of
the polarity word (?long?) changes along with
different targets (?battery life? or ?startup?).
To disambiguate a collocation?s polarity, pre-
vious work always turned to investigate the
polarities of its surrounding contexts, and then
assigned the majority polarity to the collo-
cation. However, these contexts are limited,
thus the resulting polarity is insufficient to be
reliable. We therefore propose an unsuper-
vised three-component framework to expand
some pseudo contexts from web, to help dis-
ambiguate a collocation?s polarity.Without us-
ing any additional labeled data, experiments
show that our method is effective.
1 Introduction
In recent years, more attention has been paid to
sentiment analysis as it has been widely used in
various natural language processing applications,
such as question answering (Wiebe et al2003;
Yu and Hatzivassiloglou, 2003), information extrac-
tion (Riloff et al2005) and opinion-oriented sum-
marization (Hu and Liu, 2004; Liu et al2005).
Meanwhile, it also brings us lots of interesting and
challenging research topics, such as subjectivity
analysis (Riloff and Wiebe, 2003), sentiment clas-
sification (Pang et al2002; Kim and Hovy, 2005;
?Correspondence author: tliu@ir.hit.edu.cn
Wilson et al2009; He et al2011), opinion re-
trieval (Zhang et al2007; Zhang and Ye, 2008;
Li et al2010) and so on.
One fundamental task for sentiment analysis is
to determine the semantic orientations of words.
For example, the word ?beautiful? is positive, while
?ugly? is negative. Many researchers have devel-
oped several algorithms for this purpose and gener-
ated large static lexicons of words marked with prior
polarities (Hatzivassiloglou and McKeown, 1997;
Turney et al2003; Esuli, 2008; Mohammad et al
2009; Velikovich et al2010). However, there exist
some polarity-ambiguous words, which can dynam-
ically reflect different polarities along with different
contexts. A typical polarity-ambiguous word ???
(?long? in English) is shown with two example sen-
tences as follows.
1. ????[????]t?[?]p?(Positive)
Translated as: The [battery life]t of this camera
is [long]p. (Positive)
2. ????[????]t?[?]p?(Negative)
Translated as: This camera has [long]p
[startup]t. (Negative)
The phrases marked with p superscript are the
polarity-ambiguous words, and the phrases marked
with t superscript are targets modified by the polar-
ity words. In the above two sentences, the sentiment
orientation of the polarity word ??? (?long? in En-
glish) changes along with different targets. When
modifying the target ?????? (?battery life? in
English), its polarity is positive; and when modify-
ing ?????? (?startup? in English), its polarity is
160
negative. In this paper, we especially define the col-
location as a binary tuple of the polarity-ambiguous
word and its modified target, such as ??,?????
(?long, battery life? in English) or ??,?????
(?long, startup? in English). This paper concentrates
on the task of collocation polarity disambiguation.
This is an important task as the problem of
polarity-ambiguity is frequent. We analyze 4,861
common binary tuples of polarity words and their
modified targets from 478 reviews1, and find that
over 20% of them are the collocations defined in this
paper. Therefore, the task of collocation polarity dis-
ambiguation is worthy of study.
For a sentence s containing such a collocation c,
since the in-sentence features are always ambiguous,
it is difficult to disambiguate the polarity of c by us-
ing them. Thus some previous work turned to in-
vestigate its surrounding contexts? polarities (such
as the sentences before or after s), and then assigned
the majority polarity to the collocation c (Hatzivas-
siloglou and McKeown, 1997; Hu and Liu, 2004;
Kanayama and Nasukawa, 2006). However, since
the amount of contexts from the original review is
very limited, the final resulting polarity for the col-
location c is insufficient to be reliable.
Fortunately, most collocations may appear multi-
ple times, in different forms, both within the same
review and within topically-related reviews. Thus
for a collocation, we can collect large amounts of
contexts from other reviews to improve its polarity
disambiguation. These expanded contexts are called
pseudo contexts in this paper. Some previous work
used the similar methods. For example, Ding (Ding
et al2008) expanded some pseudo contexts from
a topically-related review set. But since the review
set is limited, the expanded contexts are still lim-
ited and unreliable. In order to overcome this prob-
lem, we propose an unsupervised three-component
framework to expand more pseudo contexts from
web for the collocation polarity disambiguation.
Without using any labeled data, experiments on
a Chinese data set from four product domains show
that the three-component framework is feasible and
the web-based pseudo contexts are useful for the
collocation polarity disambiguation. Compared to
other previous work, our method achieves an F1
1The dataset will be introduced in Section 4.1 in detail.
score of 72.02%, which is about 15% higher.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Section
3 shows the proposed approach including three in-
dependent components. Section 4 and 5 presents the
experiments and results. Finally we conclude this
paper in Section 6.
2 Related Work
The key of the collocation polarity disambigua-
tion task is to recognize the polarity word?s sen-
timent orientation of a collocation. There are ba-
sically two types of approaches for word polar-
ity recognition: corpus-based and dictionary-based
approaches. Corpus-based approaches find co-
occurrence patterns of words in the large corpora
to determine the word sentiments, such as the work
in (Hatzivassiloglou and McKeown, 1997; Wiebe,
2000; Riloff and Wiebe, 2003; Turney et al2003;
Kaji and Kitsuregawa, 2007; Velikovich et al
2010). On the other hand, dictionary-based ap-
proaches use synonyms and antonyms in WordNet
to determine word sentiments based on a set of seed
polarity words. Such approaches are studied in (Kim
and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps
et al2004). Overall, most of the above approaches
aim to generate a large static polarity word lexicon
marked with prior polarities.
However, it is not sensible to predict a word?s sen-
timent orientation without considering its context.
In fact, even in the same domain, a word may indi-
cate different polarities depending on what targets it
is applied to, especially for the polarity-ambiguous
words, such as ??? (?long? in English) shown in
Section 1. Based on these, we need to consider both
the polarity words and their modified targets, i.e.,
the collocations mentioned in this paper, rather than
only the polarity words.
To date, the task in this paper is similar with
much previous work. Some researchers exploited
the features of the sentences containing colloca-
tions to help disambiguate the polarity of the
polarity-ambiguous word. For example, Hatzivas-
siloglou (Hatzivassiloglou and McKeown, 1997)
and Kanayama (Kanayama and Nasukawa, 2006)
used conjunction rules to solve this problem from
large domain corpora. Suzuki (Suzuki et al2006)
161
took into account many contextual information of
the word within the sentence, such as exclamation
words, emoticons and so on. However, the experi-
mental results show that these in-sentence features
are not rich enough.
Instead of considering the current sentence alone,
some researchers exploited external information and
evidences in other sentences or other reviews to infer
the collocation?s polarity. For a collocation, Hu (Hu
and Liu, 2004) analyzed its surrounding sentences?
polarities to disambiguate its polarity. Ding (Ding
et al2008) proposed a holistic lexicon-based ap-
proach of using global information to solve this
problem. However, the contexts or evidences from
these two methods are limited and unreliable. Ex-
cept for the above unsupervised methods, some re-
searchers (Wilson et al2005; Wilson et al2009)
proposed supervised methods for this task, which
need large annotated corpora.
In addition, many related works tried to learn
word polarity in a specific domain, but ignored the
problem that even the same word in the same do-
main may indicate different polarities (Jijkoun et al
2010; Bollegala et al2011). And some work (Lu et
al., 2011) combined difference sources of informa-
tion, especially the lexicons and heuristic rules for
this task, but ignored the important role of the con-
text. Besides, there exists some research focusing
on word sense subjectivity disambiguation, which
aims to classify a word sense into subjective or ob-
jective (Wiebe and Mihalcea, 2006; Su and Markert,
2009). Obviously, this task is different from ours.
3 The Proposed Approach
3.1 Overview
The motivation of our approach is to make full use of
web sources to collect more useful pseudo contexts
for a collocation, whose original contexts are lim-
ited or unreliable. The framework of our approach
is illustrated in Figure 1.
In order to disambiguate a collocation?s polarity,
three components are carried out:
1. Query Expansion and Pseudo Context Ac-
quisition: This paper uses the collocation as query.
For a collocation, three heuristic query expansion
strategies are used to generate more flexible queries,
which have the same or completely opposite polar-
A Chinese 
collocation 
in a review
Query 
expansion
Searching
Web 
snippets
Original context 
acquisition
Sentiment 
analysis
Pseudo context 
acquisition
Sentiment 
analysis
Combination Pos/Neg
start
end
Figure 1: The framework of our approach.
ity with this collocation. Searching these queries in
the domain-related websites, lots of snippets can be
acquired. Then we can extract the pseudo contexts
from these snippets.
2. Sentiment Analysis: For both original con-
texts and the expanded pseudo contexts from web, a
simple lexicon-based sentiment computing method
is used to recognize each context?s polarity.
3. Combination: Two strategies are designed to
integrate the polarities of the original and pseudo
contexts, under the assumption that these two kinds
of contexts can be complementary to each other.
It is worth noting that this three-component
framework is flexible and we can try to design dif-
ferent strategies for each component. Next sections
will give a simple example strategy for each compo-
nent to show its feasibility and effectiveness.
3.2 Query Expansion and Pseudo Context
Acquisition
3.2.1 Why Expanding Queries
For a collocation, such as ??,????? (?long,
battery life? in English), the most intuitive query
used for searching is constructed by the form of ?tar-
get + polarity word?, i.e., ????? (battery
life long in English). Even if we search this query
alone, a great many web snippets covering the po-
larity word and target will be retrieved. But why do
we still need to expand the queries?
In fact, for a collocation, though the amount of the
retrieved snippets is large, lots of them cannot pro-
vide accurate pseudo contexts. The reason is that the
162
polarity words in some snippets do not really mod-
ify the targets, such as in the sentence ?The battery
life is short, and finds few buyers for a long time.?
There exist no modifying relation between ?battery
life? and ?long?.
In order to filter these meaningless snippets, we
can simply search with a new query ???????
by surrounding it with quotes (noted as Strategy0).
However, this can drastically decline the amount of
snippets. In addition, as the new query is short, in
many retrieved snippets, there also exist no modify-
ing relations between the polarity words and targets.
As a result, if we just use this query strategy, the ex-
panded pseudo contexts are limited and cannot yield
ideal performance.
Therefore, we need to design some effective
query expansion strategies to ensure that (1) the po-
larity words do modify the targets in the retrieved
web snippets, and (2) the snippets are more enough.
3.2.2 Query Expansion Strategy
We first investigate the modifying relations be-
tween polarity words and the targets, and then con-
struct effective queries.
Observed from previous work (Bloom et al
2007; Kobayashi et al2004; Popescu and Etzioni,
2005), there are two kinds of common relations be-
tween the polarity words and their targets. One is
the ?subject-copula-predicate? relation, such as the
relationship between ?long? and ?battery life? in the
sentence ?The battery life of this camera is long?.
The other is the ?attribute-head? relation, such as
the relationship between them in the sentence ?This
camera has long battery life?.
As a result, three heuristic query expansion strate-
gies are adopted to construct efficient queries for
searching. Take the collocation ??,?????
(?long, battery life? in English) as an example, the
strategies are described as follows.
Strategy1: target + modifier + polarity word:
Such as the query ???????? or ?????
???? (?the battery life is very long? in English).
Different from Strategy0, this strategy adds a mod-
ifier element. It refers to the words that are used to
change the degree of a polarity word, such as ??? or
???? (?very? in English). Due to the usage of the
modifiers, the queries from this strategy can satisfy
the ?subject-copula-predicate? relation.
Strategy2: modifier + polarity word + ?+ tar-
get: Such as the query ????????? or ??
???????? (?very long battery life? in En-
glish). This strategy also uses modifiers to modify
polarity words, and the generated queries can satisfy
the ?attribute-head? relation.
Strategy3: negation word + polarity word +?+
target: Such as the query ????????? or ??
???????? (?not long battery life? in En-
glish). This strategy uses negation words to modify
the polarity words. And the queries from this strat-
egy can satisfy the ?attribute-head? relation. The
only difference is that the polarity of this kind of
queries is opposite to that of the collocation.
Similar to the queries from Strategy0, the queries
generated by Strategy1?3 are all searched with
quotes. In addition, note that the modifier and the
negation word are taken from Modifier Lexicon and
Negation Lexicon introduced in Table 2.
3.2.3 Pseudo Context Acquisition
For each query from Strategy0?3, we search it in
some websites to acquire the related snippets. If we
directly search it using Google without site restric-
tions, it does return all the snippets containing the
query, but lots of them are non-reviews. Further, the
pseudo contexts generated by these non-reviews are
useless or even harmful. To overcome this problem,
the advanced search of Google is used to search the
query within the forum sites of the product domain.
We can flexibly choose several popular forum sites
for each domain. The URLs of the forum sites used
in this paper are listed in Table 1.
Formally, given a collocation c, the expanded
pseudo contexts Conx(c) can be obtained using the
following function:
Conx(c) =
?3
i=0 f(Queryi)
=
?3
i=0
?n
j=1 f(queryij)
(1)
Here, Queryi is the query set generated by the ith
query expansion strategy; queryij is the jth query
generated by the ith strategy. And the parameter n is
the total number of queries from the ith query expan-
sion strategy. From this function, we can collect the
contexts of c by summing up all the pseudo contexts
from every queryij .
163
Domain URL
Camera
http://www.qqdc.com.cn
http://forums.nphoto.net
http://dc.pconline.com.cn
http://photobbs.it168.com
http://club.tech.sina.com.cn/dc
Car
http://bbs.chetx.com
http://bbs.pcauto.com.cn
http://club.autohome.com.cn
http://bbs.cheshi.com
http://www.xcar.com.cn
http://www.autohome.com.cn
Notebook
http://benyouhui.it168.com/index.php
http://nbbbs.zol.com.cn
http://www.ibijiben.com
http://notebook.pconline.com.cn
http://nbbbs.enet.com.cn
Phone
http://bbs.imobile.com.cn
http://sjbbs.zol.com.cn
http://bbs.shouji.com.cn
http://bbs.cnmo.com
http://forum.younet.com
Table 1: The URLs used in context expansion for differ-
ent domains.
In detail, the pseudo context acquisition algorithm
for a collocation c is illustrated in Figure 2. Note
that, the original context acquisition of c can be con-
sidered as a simplified version of the pseudo context
acquisition. That?s because the current review con-
taining c can be considered as only one snippet in
pseudo context acquisition. Thus, we can just carry
out the two steps in (2) of Figure 2 to obtain the orig-
inal contexts.
Analyzing either the pseudo contexts or the orig-
inal contexts, we can find that not all of them are
useful contexts. Thus we will simply filter the noisy
ones by context sentiment computation, and choose
the contexts showing sentiment orientations as the
useful contexts.
3.3 Sentiment Analysis
For both the original and expanded pseudo contexts,
we employ the lexicon-based sentiment computing
method (Hu and Liu, 2004) to compute the polarity
value for each context. This unsupervised approach
is quite straightforward and makes use of the senti-
ment lexicons in Table 2.
The polarity value Polarity(con) for a context con
Algorithm: Pseudo Context Expansion Algorithm
Input: A collocation c and the URL list
Output: The pseudo context set Conx(c)
1. Use Strategy0~3 to expand c and the expanded queries 
are saved as a set Query(c).
2. For any query q Query(c),   acquire its pseudo 
contexts Conx(q) as follows:
(1) search q in the domain-related URL list, the top 100 
retrieved snippets for each URL are collected as Snip(q)
(2) for each snippet sp Snip(q)
find the sentence s containing q
obtain the two sentences before and after s as the 
contexts of q in this sp, noted as Conx(q, sp)
Conx(q) = 
3. Conx(c) =                      =
?
?
U
)(
),(
qSnipsp
spqConx
?
U
)(
)(
cQueryq
qConx
?
U U
)( )(
),(
cQueryq qSnipsp
spqConx
? ?
Figure 2: The algorithm for pseudo context acquisition.
Lexicon Content
Modifier Lexicon
?,??,??,??,?,?,
??,?,??,??,??
(?very? or ?quite? in English)
Negation Lexicon ??,?,??(?no? or ?not? in English)
Positive Lexicon There are 3,730 Chinese wordsare collected from HOWNET1.
Negative Lexicon There are 3,116 Chinese wordsare collected from HOWNET.
1 http://www.keenage.com/html/e index.html.
Table 2: The lexicons used in this paper.
is computed by summing up the polarity values of all
words in con, making use of both the word polarity
defined in the positive and negative lexicons and the
contextual shifters defined in the negation lexicon.
The algorithm is illustrated in Figure 3.
In this algorithm, n is the parameter controlling
the window size within which the negation words
have influence on the polarity words, and here n is
set to 3.
Normally, if the polarity value Polarity(con) is
more than 0, the context con is labeled as positive; if
less than 0, the context is negative. We also consider
the transitional words, such as ???? (?but? in En-
glish). Finally, the contexts with positive/negative
polarities are used as the useful contexts.
164
Domain # of reviews # of c # of single c Sig / All # of multiple c(All) (Sig) (%) / kinds of multiple c
Camera 138 295 183 62.03 112 / 35
Car 161 232 131 56.47 101 / 33
Notebook 56 147 94 63.95 53 / 20
Phone 123 327 192 58.72 135 / 35
Total 478 1001 600 59.94 401 / 123 ? 3.3
Table 3: Statistics for the Chinese collocation corpus.
Algorithm: Sentiment Analysis 
Input: a context con, and three lexicons: Positive_Dic, 
Negative_Dic, Negation_Dic
Output: Polarity value Polarity(con)
1. Segment con into word set W(con)
2. For each word w W(con), compute its polarity value 
Polarity(w) as follows:
(1) if w Positive_Dic, Polarity(w) = 1;
(2) if w Negative_Dic, Polarity(w) = -1;
(3) otherwise, Polarity(w) = 0;
(4) Within the window of n words previous to w, if 
there is a word w? Negation_Dic, 
Polarity(w) = -Polarity(w)
3. Polarity(con)  = 
?
?
?
? )(
)(
conWw
wPolarity
?
?
Figure 3: The algorithm for context polarity computation.
3.4 Combination
After the pseudo context acquisition and polarity
computation, two kinds of effective contexts: orig-
inal contexts and pseudo contexts, and their corre-
sponding polarities can be obtained.
In order to yield a relatively accurate polarity Po-
larity(c) for a collocation c, we exploit the following
combination methods:
1. Majority Voting: Rather than considering the
difference between the two kinds of contexts, this
combination method relies on the polarity tag of
each context. Suppose c has n effective contexts
(including original and pseudo contexts), it can ob-
tain n polarity tags based on the individual sentiment
analysis algorithm. The polarity tag receiving more
votes is chosen as the final polarity of c.
2. Complementation: For a collocation c, we
first employ ?Majority Voting? method just on the
expanded pseudo contexts to obtain the polarity tag.
If the polarity of c cannot be recognized2, the ma-
jority polarity tag voted on the original contexts is
chosen as the final polarity tag.
4 Experimental Setup
4.1 Dataset and Evaluation Metrics
We conduct the experiments on a Chinese colloca-
tion corpus of four product domains, which is from
the Task3 of the Chinese Opinion Analysis Evalua-
tion (COAE)3 (Zhao et al2008). Table 3 describes
the corpus in detail.
From 478 reviews, 1,001 collocations (454 pos-
itive and 547 negative) with polarity-ambiguous
words are found and manually annotated by two an-
notators. Cohen?s kappa (Cohen, 1960), a measure
of inter-annotator agreement ranging from zero to
one, is 0.83, indicating a good strength of agree-
ment 4. In Table 3, Sig of the fourth column denotes
the collocations that appear once in all the domain-
related reviews. And multiple in the last column
denotes the collocations that appear several times.
From Table 3, we can find that among all the re-
views, nearly 60% collocations only appear once.
Even for the multiple collocations, they averagely
appear less than 4 times. Therefore, for a colloca-
tion, if we only consider its original contexts alone
or the expanded pseudo contexts from the domain-
related review set al, the contexts are obviously
limited and unreliable.
Instead of using accuracy, we use precision (P),
recall (R) and F-measure (F1) to measure the perfor-
mance of this task. That?s because two kinds of col-
locations? polarities cannot be disambiguated. One
2The reason will be explained in the last paragraph of Sec-
tion 4.1.
3http://www.ir-china.org.cn/coae2008.html
4A small number of collocations are still difficult to be dis-
ambiguated from contexts.
165
is the sparse collocations, which obtain no effective
contexts. The other is the collocations that acquire
the same amount of positive and negative contexts.
The metrics are defined as follows.
P = correctly disambiguated collocations
disambiguated collocations
(2)
R = correctly disambiguated collocations
all collocations
(3)
F1 = 2PR
P +R
(4)
4.2 System Description
In order to compare our method with previous work,
we build several systems as follows:
NoExp: Following the method proposed by
Hu (Hu and Liu, 2004), without using the expanded
pseudo contexts, we only consider the two original
contexts Senbef and Senaft of a collocation c in the
current review. If Senbef expresses the polarity po-
lar, then Polarity(ac) = polar. Else if Senaft
expresses the polarity polar?, then Polarity(ac) =
polar?. Else, this method cannot disambiguate the
polarity of c. In this method, the transitional words,
such as ???? (?but? in English) are considered.
Expdataset: Following the method proposed by
Ding (Ding et al2008), we solve this task with the
help of the pseudo contexts in the domain-related re-
view dataset. For a collocation c appearing in many
domain-related reviews, this method refers to the po-
larities of the same c in other reviews. The majority
polarity is chosen as final polarity.
Expweb+sig: This method is the same as our
method in this paper, except for (1) not combining
the original contexts, and (2) not using all the three
query expansion strategies, but just using the sin-
gle (abbv. sig) Strategy0. This method expands the
pseudo contexts from the web. The majority polarity
is chosen as the final polarity.
Expweb+exp: This method is the same as our pro-
posed method in this paper, except for not combin-
ing the original contexts. It expands the pseudo con-
texts from the web. And the ?exp? in the subscript
means that this method uses all the query expansion
strategies. The majority polarity of all the pseudo
contexts is chosen as the final polarity.
Expmv/cweb+exp+com: This is the method proposed
in this paper, which combines the original and ex-
panded pseudo contexts. The superscript ?mv/c? is
short for the two combination methods: Majority
Voting and Complementation.
5 Results
5.1 Comparisons among All the Systems
In fact, all the systems shown in Section 4.2 can be
considered as context based methods. The essential
difference among them lies in the contexts they used.
For a collocation, the contexts for NoExp are two
original contexts from the current review. Breaking
down the boundary of the current review,Expdataset
explores the pseudo contexts from other domain-
related reviews. Further, Expweb+sig, Expweb+exp
and Expmv/cweb+exp+com expand the pseudo contexts
from web, which can be considered as a large corpus
and can provide more evidences for the collocation
polarity disambiguation.
System P(%) R(%) F1(%)
NoExp 67.32 41.16 51.08
Expdataset 68.14 47.85 56.22
Expweb+sig 70.00 53.85 60.87
Expweb+exp 74.97 63.14 68.55
Expmvweb+exp+com 75.53 67.83 71.47
Expcweb+exp+com 74.36 69.83 72.02
Table 4: Comparative results for the collocation polarity
disambiguation task.
Table 4 illustrates the comparative results of all
systems for collocation polarity disambiguation. It
can be observed that our system Expmvweb+exp+com
and Expcweb+exp+com outperform all the other sys-
tems. We discuss the experimental results as fol-
lows:
NoExp yields the worst performance, especially
on the recall. The reason is that the original con-
texts used in this system are limited, and some of
them are even noisy. In comparison, Expdataset
adds a post-processing step of expanding pseudo
contexts from the topically-related review dataset,
which achieves a better result with an absolute im-
provement of 5.14% (F1). This suggests that the
contexts expanded from other reviews are helpful in
disambiguating the collocation?s polarity.
166
However, Expdataset is just effective in disam-
biguating the polarity of such a collocation c, which
appears many times in the domain-related reviews.
From Table 3, we can notice that this kind of collo-
cations only accounts for 40% in all the collocations,
and further they appear less than 4 times on average.
Thus, for such a collocation c, the pseudo contexts
expanded from other reviews that contain the same
c are still far from enough, since the review set size
in this system is not very large.
In order to avoid the context limitation problem,
we expand more pseudo contexts from web for each
collocation. We first try to use a simple query
form (Strategy0) for web mining. Table 4 illustrates
that the corresponding system Expweb+sig outper-
forms the system Expdataset. It can demonstrate
that our web mining based pseudo context expan-
sion is useful for disambiguating the collocation?s
polarity, since this system can explore more con-
texts. However, we can find that the performance
is not very ideal. This system can generate some
harmful contexts for the reason of the wrong mod-
ifying relations between polarity words and targets
in the retrieved snippets.
Thus this paper adds three query expansion strate-
gies to generate more and accurate pseudo con-
texts. Table 4 shows that the corresponding sys-
tem Expweb+exp can achieve a better result with F1
= 68.55%, which is significantly (?2 test with p <
0.01) outperforms Expweb+sig. It demonstrates that
the query expansion strategies are useful.
Finally, Table 4 gives the results of our method in
this paper, Expmvweb+exp+com and Expcweb+exp+com,
which combines the original and expanded pseudo
contexts to yield a final polarity. We can ob-
serve that both of these systems outperform the sys-
tem NoExp of just using the original contexts and
the system Expweb+exp of just using the expanded
pseudo contexts. This can illustrate that the two
kinds of contexts are complementary to each other.
In addition, we can also find that the two combi-
nation methods produce similar results. In detail,
Expmvweb+exp+com disambiguates 899 collocations,
679 of them are correct; Expcweb+exp+com disam-
biguates 940 collocations, 699 of them are correct.
We can further find that, although the amount of
original contexts is small, it also plays an important
role in disambiguating the polarities of the collo-
cations that cannot be recognized by the expanded
pseudo contexts.
5.2 The Contributions of the Query Expansion
Strategies
The expanded pseudo contexts from our method can
be partly credited to the query expansion strategies.
Based on this, this section aims to analyze the differ-
ent contributions of the query expansion strategies in
our method.
Strategy P(%) R(%) F1(%) Avg(#)
Strategy0 70.00 53.85 60.87 71
Strategy1 74.14 55.84 63.70 112
Strategy2 61.84 37.56 46.74 26
Strategy3 64.34 33.17 43.77 20
Expweb+exp 74.97 63.14 68.55 229
Table 5: The performance of our method based on each
query expansion strategy for collocation polarity disam-
biguation.
Table 5 provides the performance of our method
based on each query expansion strategy for collo-
cation polarity disambiguation. For each strategy,
?Avg? in Table 5 denotes the average number of
the expanded pseudo contexts for each collocation.
From this table, we can find that the larger the ?Avg?
is, the better (F1) the strategy is. In detail, Strategy1
with the largest ?Avg? has the best performance; and
Strategy3 with the fewest ?Avg? has the worst per-
formance. This can further demonstrate our idea
that more and effective pseudo contexts can improve
the performance of the collocation polarity disam-
biguation task. Expweb+exp integrates all the query
expansion strategies and obtains much more ?Avg?.
Therefore, this can significantly increase the recall
value, and further produce a better result. On the
other hand, the results in Table 5 show that these
heuristic query expansion strategies are effective.
5.3 Deep Experiments in the
Three-Component Framework
In order to do a detailed analysis into our three-
component framework, some deep experiments are
made:
Query Expansion The aim of query expansion
is to retrieve lots of relative snippets, from which
we can extract the useful pseudo contexts. For each
167
Strategy0 Strategy1 Strategy2 Strategy3
(%) (%) (%) (%)
Query Expansion 76.75 94.50 85.50 85.25
Pseudo Context 71.25 73.50 67.50 74.50
Sentiment Analysis 63.00 68.25 59.00 69.75
Table 6: The accuracies of the query expansion, pseudo context and sentiment analysis for each strategy.
snippet, if the polarity word of the collocation does
modify the target, we consider this snippet as a cor-
rect query expansion result.
Pseudo Context For each expanded pseudo con-
text from web, if it shows the same sentiment ori-
entation with the collocation (or opposite with the
collocation?s polarity because of the usage of transi-
tional words), we consider this context as a correct
pseudo context.
Sentiment Analysis For each expanded pseudo
context, if its polarity can be correctly recognized
by the polarity computation method in Figure 3, and
meanwhile it shows the same sentiment orientation
with the collocation, we consider this context as a
correct one.
Table 6 illustrates the accuracy of each experi-
ment for each strategy in detail, where 400 web re-
trieved snippets for Query Expansion and 400 ex-
panded pseudo contexts for Pseudo Context and
Sentiment Analysis are randomly selected and man-
ually evaluated for each strategy.
Seen from Table 6, we can find that:
1. For Query Expansion, all strategies yield good
accuracies except for Strategy0. This can draw a
same conclusion with our analysis in Section 3.2.1.
The queries from Strategy0 are short, thus in many
retrieved snippets, there exist no modifying relations
between the polarity words and targets. Accord-
ingly, the pseudo contexts from these snippets are
incorrect. This can result in the low accuracy of
Strategy0. On the other hand, we can find that the
other three query expansion strategies perform well.
2. Although the final result of our three-
component framework is good, the accuracies of
Pseudo Context and Sentiment Analysis for each
strategy is not very high. This is perhaps caused by
unrefined work on the specific sub-stages. For ex-
ample, we get alhe pseudo contexts using the al-
gorithm in Figure 2. However, in some reviews, the
two sentences before and after the target sentence
have no polarity relation with the target sentence it-
self. This can bring in some noises. On the other
hand, the context polarity computation algorithm in
Figure 3 is just a simple attempt, which is not the
best way to compute the context?s polarity.
In fact, this paper aims to try some simple algo-
rithms for each component to validate the effective-
ness of the three-component framework. We will
polish every component of our framework in future.
6 Conclusion and Future Work
This paper proposes a web-based context expan-
sion framework for collocation polarity disambigua-
tion. The basic assumption of this framework is
that, if a collocation appears in different forms, both
within the same review and within topically-related
reviews, then the large amounts of pseudo contexts
from these reviews can help to disambiguate such
a collocation?s polarity. Based on this assumption,
this framework includes three independent compo-
nents. First, the heuristic query expansion strate-
gies are adopted to expand pseudo contexts from
web; then a simple but effective polarity computa-
tion method is used to recognize the polarities for
both the original contexts and the expanded pseudo
contexts; and finally, we integrate the polarities from
the original and pseudo contexts as the collocation?s
polarity. Without using any additional labeled data,
experiments on a Chinese data set from four product
domains show that the proposed framework outper-
forms other previous work.
This paper can be concluded as follows:
1. A framework including three independent com-
ponents is proposed for collocation polarity
disambiguation. We can try other different al-
gorithms for each component.
2. Web-based pseudo contexts are effective for
disambiguating a collocation?s polarity.
168
3. The query expansion strategies are promising,
which can generate more useful and correct
contexts.
4. The initial contexts from current reviews and
the expanded contexts from web are comple-
mentary to each other.
The immediate extension of our work is to polish
each component of this framework, such as improv-
ing the accuracy of query expansion and pseudo con-
text acquisition, using other effective polarity com-
puting methods for each context and so on. In ad-
dition, we will explore other query expansion strate-
gies to generate more effective contexts.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 61133012, the National ?863? Leading Tech-
nology Research Project via grant 2012AA011102,
the Ministry of Education Research of Social Sci-
ences Youth funded projects via grant 12YJCZH304
and the Fundamental Research Funds for the Central
Universities via grant No.HIT.NSRIF.2013090.
References
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using mul-
tiple sources to construct a sentiment sensitive the-
saurus for cross-domain sentiment classification. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 132?141. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the Conference onWeb Search andWeb
Data Mining (WSDM), pages 231?240.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis. In
Proceedings of the ACM SIGIR Conference on Infor-
mation and Knowledge Management (CIKM), pages
617?624.
A. Esuli. 2008. Automatic generation of lexical re-
sources for opinion mining: models, algorithms and
applications. In ACM SIGIR Forum, volume 42, pages
105?106. ACM.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the eighth conference on European chapter of
the Association for Computational Linguistics, pages
174?181. Association for Computational Linguistics.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123?131, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177. ACM.
V. Jijkoun, M. De Rijke, and W. Weerkamp. 2010. Gen-
erating focused topic-specific sentiment lexicons. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 585?594.
Association for Computational Linguistics.
N. Kaji and M. Kitsuregawa. 2007. Building lexicon
for sentiment analysis from massive collection of html
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075?1083.
Jaap Kamps, Maarten Marx, R. ort. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings of
LREC-2004, pages 1115?1118.
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 355?363. Association for Computational
Linguistics.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of IJCNLP-2005, pages 61?66.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In Proceedings of the Joint
Human Language Technology/North American Chap-
ter of the ACL Conference (HLT-NAACL), pages 200?
207.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi, and Toshikazu Fukushima. 2004. Collecting
evaluative expressions for opinion extraction. In Pro-
ceedings of the International Joint Conference on Nat-
ural Language Processing (IJCNLP), pages 584?589.
169
Binyang Li, Lanjun Zhou, Shi Feng, and Kam-Fai Wong.
2010. A unified graph model for sentence-based opin-
ion retrieval. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
page 1367?1375.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW-2005, pages
342?351.
Y. Lu, M. Castellanos, U. Dayal, and C.X. Zhai. 2011.
Automatic construction of a context-aware sentiment
lexicon: an optimization approach. In Proceedings of
the 20th international conference on World wide web,
pages 347?356. ACM.
S. Mohammad, C. Dunne, and B. Dorr. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2-Volume 2, pages
599?608. Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002, pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003, pages 105?112.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of AAAI-2005,
pages 1106?1111.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the ACL, pages 1?9.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Com-
putational Linguistics and Intelligent Text Processing,
pages 502?513.
P. Turney, M.L. Littman, et al003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems (TOIS), 21(4):315?346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In The 2010 Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 777?785.
Jan Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In Proceedings of the Conference on
Computational Linguistics / Association for Computa-
tional Linguistics (COLING/ACL), pages 1065?1072.
Janyce Wiebe, Eric Breck, and Chris Buckley. 2003.
Recognizing and Organizing Opinions Expressed in
the World Press. In Papers from the AAAI Spring
Symposium on New Directions in Question Answering,
pages 24?26.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI, pages 735?
740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005, pages 347?354.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: an exploration
of features for phrase-level sentiment analysis. Com-
putational Linguistics, 35(3).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003, pages 129?
136.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment
for opinion retrieval. In Proceedings of the ACM Spe-
cial Interest Group on Information Retrieval (SIGIR),
pages 411?419.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opin-
ion retrieval from blogs. In In proceedings of CIKM,
page 831?840.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
170
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 468?478,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Web Search Ranking by Incorporating Structured 
Annotation of Queries* 
 
 
Xiao Ding1, Zhicheng Dou2, Bing Qin1, Ting Liu1, Ji-Rong Wen3 
 
1Research Center for Social Computing and Information Retrieval 
Harbin Institute of Technology, China 
 
2Microsoft Research Asia, Beijing 100190, China 
 
3Renmin University of China, Beijing, China 
1{xding, qinb, tliu}@ir.hit.edu.cn; 
2zhichdou@microsoft.com; 3jirong.wen@gmail.com 
 
 
 
 
 
 
Abstract? 
Web users are increasingly looking for 
structured data, such as lyrics, job, or recipes, 
using unstructured queries on the web. 
However, retrieving relevant results from such 
data is a challenging problem due to the 
unstructured language of the web queries. In 
this paper, we propose a method to improve 
web search ranking by detecting Structured 
Annotation of queries based on top search 
results. In a structured annotation, the original 
query is split into different units that are 
associated with semantic attributes in the 
corresponding domain. We evaluate our 
techniques using real world queries and achieve 
significant improvement. 
1 Introduction 
Search engines are getting more sophisticated by 
utilizing information from multiple diverse sources. 
One such valuable source of information is 
structured and semi-structured data, which is not 
very difficult to access, owing to information 
extraction (Wong et al, 2009; Etzioni et al, 2008; 
Zhai and Liu 2006) and semantic web efforts. 
                                                          
? *Work was done when the first author was visiting Microsoft 
Research Asia 
Driving the web search evolution are the user 
needs. Users usually have a template in mind when 
formulating queries to search for information. 
Agarwal et al, (2010) surveyed a search log of 15 
million queries from a commercial search engine. 
They found that 90% of queries follow certain 
templates. For example, by issuing the query 
?taylor swift lyrics falling in love?, the users are 
actually seeking for the lyrics of the song ?Mary's 
Song (oh my my my)? by artist Taylor Swift. The 
words ?falling in love? are actually part of the 
lyrics they are searching for. However, some top 
search results are irrelevant to the query, although 
they contain all the query terms. For example, the 
first top search result shown in Figure 1(a) does 
not contain the required lyrics. It just contains the 
lyrics of another song of Taylor Swift, rather than 
the song that users are seeking. 
A possible way to solve the above ranking 
problem is to understand the underlying query 
structure. For example, after recognizing that 
?taylor swift? is an artist name and ?falling in love? 
are part of the lyrics, we can improve the ranking 
by comparing the structured query with the 
corresponding structured data in documents 
(shown in Figure 1(b)). Some previous studies 
investigated how to extract structured information 
from user queries, such as query segmentation 
(Bergsma and Wang, 2007). The task of query 
segmentation is to separate the query words into 
468
disjointed segments so that each segment maps to a 
semantic unit (Li et al, 2011). For example, the 
segmentation of the query ?taylor swift lyrics 
falling in love? can be ?taylor swift | lyrics | falling 
in love?. Since query segmentation cannot tell 
?talylor swift? is an artist name and ?falling in love? 
are part of lyrics, it is still difficult for us to judge 
whether each part of the query segmentations 
matches the right field of the documents or not 
(such as judge whether ?talylor swift? matches the 
artist name in the document). Recently, a lot of 
work (Sarkas et al, 2010; Li et al, 2009) proposed 
the task of structured annotation of queries which 
aims to detect the structure of the query and assign 
a specific label to it. However, to our knowledge, 
the previous methods do not exploit an effective 
approach for improving web search ranking by 
incorporating structured annotation of queries. 
In this paper, we investigate the possibility of 
using structured annotation of queries to improve 
web search ranking. Specifically, we propose a 
greedy algorithm which uses the structured data 
(named annotated tokens in Figure 1(b)) extracted 
from the top search results to annotate the latent 
structured semantics in web queries. We then 
compute matching scores between the annotated 
query and the corresponding structured 
information contained in documents. The top 
search results can be re-ranked according to the 
matching scores. However, it is very difficult to 
extract structured data from all of the search results. 
Hence, we propose a relevance feedback based re-
ranking model. We use these structured documents 
whose matching scores are greater than a threshold 
as feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Experiments on a large web search dataset from 
a major commercial search engine show that the F-
Measure of structured annotation generated by our 
approach is as high as 91%. On this dataset, our re-
ranking model using the structured annotations 
significantly outperforms two baselines. 
The main contributions of our work include: 
1. We propose a novel approach to generate 
structured annotation of queries based on top 
search results. 
2. Although structured annotation of queries has 
been studied previously, to the best of our 
knowledge this is the first paper that attempts 
to improve web search ranking by 
incorporating structured annotation of queries. 
The rest of this paper is organized as follows. 
We briefly introduce related work in Section 2. 
Section 3 presents our method for generating 
structured annotation of queries. We then propose 
two novel re-ranking models based on structured 
annotation in Section 4. Section 5 introduces the 
data used in this paper. We report experimental 
results in Section 6. Finally we conclude the work 
in Section 7. 
 
Figure 1. Search results of query ?taylor swift lyrics falling in love? and processing pipeline 
[Taylor Swift, #artist_name, 0.34]
...
[Mary?s Song (oh my my my), #song_name, 0.16]
[Crazier, #song_name, 0.1]
[Jump Then Fall, #song_name, 0.08]
...
[Growing up and falling in love?, #lyrics, 0.16]
[Feel like I?m falling and ?, #lyrics, 0.1]
[I realize your love is the best ?, #lyrics, 0.08]
d1 [Taylor Swift, #artist_name]
[Crazier, #song_name]
[Feel like I?m falling and ?, #lyrics]
d2 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
d3 [Taylor Swift, #artist_name]
[Jump Then Fall, #song_name]
[I realize your love is the best ?, #lyrics]
d4 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
Search Results (a)
Weighted Annotated Tokens (c)Query Structured Annotation Generation (d)Top Results Re-ranking (e)
Annotated Tokens (b)
1.
2.
3.
4.
Query: taylor swift lyrics falling in love
<[taylor swift, #artist_name] lyrics 
[falling in love, #lyrics]>
 
1. 
  
2. 
  
3. 
  
4. 
1.
2.
3.
4.
469
2 Related Work 
There is a great deal of prior research that 
identifies query structured information. We 
summarize this research according to their 
different approaches. 
2.1 Structured Annotation of Queries 
Recently, a lot of work has been done on 
understanding query structure (Sarkas et al, 2010; 
Li et al, 2009; Bendersky et al, 2010). One 
important method is structured annotation of 
queries which aims to detect the structure of the 
query and assign a specific label to it. Li et al, 
(2009) proposed web query tagging and its goal is 
to assign to each query term a specified category, 
roughly corresponding to a list of attributes. A 
semi-supervised Conditional Random Field (CRF) 
is used to capture dependencies between query 
words and to identify the most likely joint 
assignment of words to ?categories.? Comparing 
with previous work, the advantages of our 
approach are on the following aspects. First, we 
generate structured annotation of queries based on 
top search results, not some global knowledge base 
or query logs. Second, they mainly focus on the 
method of generating structured annotation of 
queries, rather than leverage the generated query 
structures to improve web search rankings. In this 
paper, we not only offer a novel solution for 
generating structured annotation of queries, but 
also propose a re-ranking approach to improve 
Web search based on structured annotation of 
queries. Bendersky et al, (2011) also used top 
search results to generate structured annotation of 
queries. However, the annotations in their 
definition are capitalization, POS tags, and 
segmentation indicators, which are different from 
ours. 
2.2 Query Template Generation 
The concept of query template has been discussed 
in a few recent papers (Agarwal et al, 2010; Pasca 
2011; Liu et al, 2011; Szpektor et al, 2011). A 
query template is a sequence of terms, where each 
term could be a word or an attribute. For example, 
<#artist_name lyrics #lyrics> is a query template, 
?#artist_name? and ?#lyrics? are attributes, and 
?lyrics? is a word. Structured annotation of queries 
is different from query template, as a query 
template can instantiate multiple queries while a 
structured annotation only serves for a specific 
query. Unlike query template, our work is ranking-
oriented. We aim to automatically annotate query 
structure based on top search results, and further 
use these structured annotations to re-rank top 
search results for improving search performance. 
2.3 Query Segmentation 
The task of query segmentation is to separate the 
query words into disjointed segments so that each 
segment maps to a semantic unit (Li et al, 2011). 
Query segmentation techniques have been well 
studied in recent literature (Tan and Peng, 2008; 
Yu and Shi, 2009). However, structured annotation 
of queries cannot only separate the query words 
into disjoint segments but can also assign each 
segment a semantic label which can help the search 
engine to judge whether each part of query 
segmentation matches the right field of the 
documents or not. 
2.4 Entity Search 
The problem of entity search has received a great 
deal of attention in recent years (Guo et al, 2009; 
Bron et al, 2010; Cheng et al, 2007). Its goal is to 
answer information needs that focus on entities. 
The problem of structured annotation of queries is 
related to entity search because for some queries, 
structured annotation items are entities or attributes. 
Some existing entity search approaches also 
exploit knowledge from the structure of webpages 
(Zhao et al, 2005). Annotating query structured 
information differs from entity search in the 
following aspects. First, structured annotation 
based ranking is applicable for all queries, rather 
than just entity related queries. Second, the result 
of an entity search is usually a list of entities, their 
attributes, and associated homepages, whereas our 
work uses the structured information from 
webpages to annotate query structured information 
and further leverage structured annotation of 
queries to re-rank top search results. 
Table 1. Example domain schemas 
Domain Schema Example structured annotations 
lyrics #artist_name 
#song_name 
#lyrics 
<lyrics of [hey jude, #song_name] [beatles, 
#artist_name]> 
job #category 
#location 
<[teacher, #category] job in [America, 
#location]> 
recipe  #directions 
#ingredients 
<[baking, # directions] [bread, # 
ingredients] recipe> 
 
470
3 Structured Annotation of Queries  
3.1 Problem Definition 
We start our discussion by defining some basic 
concepts. A token is defined as a sequence of 
words including space, i.e., one or more words. For 
example, the bigram ?taylor swift? can be a single 
token. As our objective is to find structured 
annotation of queries in a specific domain, we 
begin with a definition of domain schema. 
Definition 1 (Domain Schema): For a given 
domain of interest, the domain schema is the set of 
attributes. We denote the domain schema as ? =
{?1, ?2,? , ??}, where each ??  is the name of an 
attribute of the domain. Sample domain schemas 
are shown in Table 1. In contrast to previous 
methods (Agarwal et al, 2010), our definition of 
domain schema does not need attribute values. For 
the sake of simplicity, this paper assumes that 
attributes in domain schema are available. 
However, it is not difficult to pre-specify attributes 
in a specific domain. 
Definition 2 (Annotated Token): An annotated 
token in a specific domain is a pair [?, ?], where v 
is a token and a is a corresponding attribute for v 
in this domain. [hey jude, #song_name] is an 
example of an annotated token for the ?lyrics? 
domain shown in Table 1. The words ?hey jude? 
comprise a token, and its corresponding attribute 
name is #song_name. If a token does not have any 
corresponding attributes, we denote it as free token. 
Definition 3 (Structured Annotation): A 
structured annotation p is a sequence of terms <
?1,?2,?,?? >, where each ?? could be a free token or 
an annotated token, and at least one of the terms is 
an annotated token, i.e., ?? ? [1, ?] for which ?? is 
an annotated token. 
Given the schema for the domain ?lyrics?, 
<[taylor swift, #artist_name] lyrics [falling in love, 
#lyrics]> is a possible structured annotation for the 
query ?taylor swift lyrics falling in love?. In this 
annotation, [taylor swift, #artist_name] and 
[falling in love, #lyrics] are two annotated tokens. 
The word ?lyrics? is a free token. 
Intuitively, a structured annotation corresponds 
to an interpretation of the query as a request for 
some structured information from documents. The 
set of annotated tokens expresses the information 
need of the documents that have been requested. 
The free tokens may provide more diverse 
information. Annotated tokens and free tokens 
together cover all query terms, reflecting the 
complete user intent of the query. 
3.2 Generating Structured Annotation 
In this paper, given a domain schema A, we 
generate structured annotation for a query q based 
on the top search results of q. We propose using 
top search results, rather than some global 
knowledge base or query logs, because: 
(1) Top search results have been proven to be 
a successful technique for query explanation 
(Bendersky et al, 2010). 
(2) We have observed that in most cases, a 
reasonable percentage of the top search results are 
relevant to the query. By aggregating structured 
information from the top search results, we can get 
more query-dependent annotated tokens than using 
global data sources which may contain more noise 
and outdated. 
(3) Our goal for generating structured 
annotation is to improve the ranking quality of 
queries. Using top search results enables 
simultaneous and consistent detection of structured 
information from documents and queries. 
As mentioned in Section 3.1, we generate 
structured annotation of queries based on annotated 
tokens, which are actually structured data (shown 
in Figure 1(b)) embedded in web documents. In 
this paper, we assume that the annotated tokens are 
Algorithm 1: Query Structured Annotation Generation 
Input: a list of weighted annotated tokens T = {t1, ? , tm} ; 
          a query q = ?w1, ? , wn?  where wi ? W; 
a pre-defined threshold score ?. 
Output: a query structured annotation p = <s1, ? , sk>. 
  1: Set p = q = {s1, ?, sn}, where si = wi 
  2: for u = 1 to T.size do 
  3:       compute ?????(?, ??) 
            = ?????(?, ??. ?)  
            = ??. ? ? ???0??<??????(??? , ??. ?), 
            where pij = si,?,sj, s.t. sl ? W for l ? [i, j]. //pij is just 
in the remaining query words 
  4: end for 
  5: find the maximum matching tu with  
            ???? = ??????1?????????(?, ??) 
  6: if ?????(?, ????) > ? then 
  7:      replace si,?,sj in p with [si,?,sj, tmax.a ] 
  8:      remove tmax from T 
9:      n ? n ? (j - i) 
10:      go to step 2 
11: else  
12:      return p 
13: end if 
 
471
available and we mainly focus on how to use these 
annotated tokens from top search results to 
generate structured annotation of queries. The 
approach is comprised of two parts, one for 
weighting annotated tokens and the other for 
generating structured annotation of queries based 
on the weighted annotated tokens. 
Weighting: As shown in Figure 1, annotated 
tokens extracted from top results may be 
inconsistent, and hence some of the extracted 
annotated tokens are less useful or even useless for 
generating structured annotation. 
We assume that a better annotated token should 
be supported by more top results; while a worse 
annotated token may appear in fewer results. 
Hence we aggregate all the annotated tokens 
extracted from top search results, and evaluate the 
importance of each unique one by a ranking-aware 
voting model as follows. For an annotated token [v, 
a], its weight w is defined as: 
                      ? =
1
?
? ??1????                           (1) 
where wj is a voting from document dj, and 
?? = {
? ? ? + 1
?
,             if [?, ?] ? ??
0,                      else        
 
Here, N is the number of top search results and j 
is the ranking position of document dj. We then 
generate a weighted annotated token [v, a, w] for 
each original unique token [v, a]. 
Generating: The process by which we map a 
query q to Structured Annotation is shown in 
Algorithm 1. The algorithm takes as input a list of 
weighted annotated tokens and the query q, and 
outputs the structured annotation of the query q. 
The algorithm first partitions the query q by 
comparing each sub-sequence of the query with all 
the weighted annotated tokens, and find the 
maximum matching annotated token (line 1 to line 
5). Then, if the degree of match is greater than the 
threshold ? which is a pre-defined threshold score 
for fuzzy string matching, the query substring will 
be assigned the attribute label of the maximum 
matching annotated token (line 6 to line 8). The 
algorithm stops when all the weighted annotated 
tokens have been scanned, and outputs the 
structured annotation of the query.  
Note that in some cases, the query may fail to 
exactly match with the annotated tokens, due to 
spelling errors, acronyms or abbreviations in users? 
queries. For example, in the query ?broken and 
beatuful lyrics?, ?broken and beatuful? is a 
misspelling of ?broken and beautiful.? We adopt a 
fuzzy string matching function for comparing a 
sub-sequence string s with a token v: 
          ???(?, ?) = 1 ?
????????????(?,?)
max (|?|,|?|)
                (2) 
where EditDistance(s, v) measures the edit 
distances of two strings, |s| is the length of string s 
and |v| is the length of string v. 
4 Ranking with Structured Annotation 
Given a domain schema ? = {?1, ?2, ? , ??}, and a 
query q, suppose that ? = < ?1,?2,?,?? >  is the 
structured annotation for query q obtained using 
the method introduced in the above sections. p can 
better reflects the user?s real search intent than the 
original q, as it presents the structured semantic 
information needed instead of a simple word string. 
Therefore, a document di can better satisfy a user?s 
information need if it contains corresponding 
structured semantic information in p. Suppose that 
Ti is the set of annotated tokens extracted from 
document di, we compute a re-ranking score, 
denoted by RScore, for document di as follows: 
RScore(q, di) = ?????(?, ??) 
                      = ?????(?, ??) 
                      = ? ? ?????(?? , ?)????1????  
where 
  ?????(?? , ?)= {
???(?? . ?? , ?. ?),        if ?? . ?? = ?. ?
0,                                else
      (3) 
where ??  is an annotated token in p and t is an 
annotated token in di. We use Equation (2) to 
compute the similarity between values in query 
annotated tokens and values in document annotated 
tokens. We propose two re-ranking models, 
namely the conservative re-ranking model, to re-
rank top results based on RScore and relevance 
feedback based re-ranking model. 
4.1 Conservative Re-ranking Model 
A nature way to re-rank top search results is 
according to their RScore. However, we fail to 
obtain annotated tokens from some retrieved 
documents, and hence the RScore of these 
documents are not available. In the conservative 
re-ranking model, we only re-rank search results 
that have an RScore. For example, suppose there 
are five retrieved documents {d1, d2, d3, d4, d5} for 
query q, we can extract structured information 
from document d3 and d4 and RScore(q, d4) > 
RScore(q, d3). Note that we cannot obtain 
472
structured information from d1, d2, and d5.  In the 
conservative re-ranking method, d1, d2, and d5 
retain their original positions; while d3 and d4 will 
be re-ranked according to their RScore. Therefore, 
the final ranking generated by our conservative re-
ranking model should be {d1, d2, d4, d3, d5}, in 
which the documents are re-ranked among the 
affected positions. 
There is also useful information in the 
documents without structured data, such as 
community question answering websites. However, 
in the conservative re-ranking model they will not 
be re-ranked. This may hurt the performance of our 
re-ranking model. One reasonable solution is 
relevance feedback model. 
4.2 Relevance Feedback based Re-ranking 
Model 
The disadvantage of the conservative re-ranking 
model is that it only can re-rank those top search 
results with structured data. To make up its 
limitation, we propose a relevance feedback based 
re-ranking model. The key idea of this model is 
based on the observation that the search results 
with the corrected annotated tokens could give 
implicit feedback information. Hence, we use these 
structured documents whose RScore are greater 
than a threshold ? (empirically set it as 0.6) as 
feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Formally, given a query Q and a document 
collection C, a retrieval system returns a ranked list 
of documents D. Let di denote the i-th ranked 
document in the ranked list. Our goal is to study 
how to use these feedback documents, J ? {d1,?, 
dk}, to effectively re-rank the other r search results: 
U ? {dk+1,?, dk+r}. A general formula of relevance 
feedback model (Salton et al 1990) R is as follows: 
?(??) = (1 ? ?)??(Q) + ???(J)             (4) 
where ? ? [0, 1] is the feedback coefficient, and ?? 
and ?? are two models that map a query and a set 
of relevant documents, respectively, into some 
comparable representations. For example, they can 
be represented as vectors of weighted terms or 
language models. 
In this paper, we explore the problem in the 
language model framework, particularly the KL-
divergence retrieval model and mixture-model 
feedback method (Zhai and Lafferty, 2001), mainly 
because language models deliver state-of-the-art 
retrieval performance and the mixture-model based 
feedback is one of the most effective feedback 
techniques which outperforms Rocchio feedback. 
4.2.1 The KL-Divergence Retrieval Model 
The KL-divergence retrieval model was introduced 
in Lafferty and Zhai, (2001) as a special case of the 
risk minimization retrieval framework and can 
support feedback more naturally. In this model, 
queries and documents are represented by unigram 
language models. Assuming that these language 
models can be appropriately estimated, KL-  
divergence retrieval model measures the relevance 
value of a document D with respect to a query Q 
by computing the negative Kullback-Leibler 
divergence between the query language model ?? 
and the document language model ?? as follows: 
?(?, ?) = ??(??||??) = ?? ?(?|??)???
?(?|??)
?(?|??)
???       (5) 
where V is the set of words in our vocabulary. 
Intuitively, the retrieval performance of the KL-
divergence relies on the estimation of the 
document model ?? and the query model ??.  
For the set of k relevant documents, the 
document model ??  is estimated as ?(w|??) =
1
?
?
?(?,??)
|??|
?
?=1 , where ?(?, ??) is the count of word 
w in the i-th relevant document, and |??| is the total 
number of words in that document. The document 
model ??  needs to be smoothed and an effective 
method is Dirichlet smoothing (Zhai et al, 2001). 
The query model intuitively captures what the 
user is interested in, and thus would affect retrieval 
performance. With feedback documents, ??  is 
estimated by the mixture-model feedback method. 
4.2.2 The Mixture Model Feedback Method 
As the problem definition in Equation (4), the 
query model can be estimated by the original query 
model ?(?|??) =
?(?,?)
|?|
 (where c(w,Q) is the count 
of word w in the query Q, and |Q| is the total 
number of words in the query) and the feedback 
document model. Zhai and Lafferty, (2001) 
proposed a mixture model feedback method to 
estimate the feedback document model. More 
specifically, the model assumes that the feedback 
documents can be generated by a background 
language model ?(?|?) estimated using the whole 
collection and an unknown topic language model 
473
?? to be estimated. Formally, let F ? C be a set of 
feedback documents. In this paper, F is comprised 
of documents that RScore are greater than?. The 
log-likelihood function of the mixture model is: 
???(?|??) = 
      ? ? ?(?, ?)??? log [(1 ? ?)?(?|??) + ??(?|?)]???     (6) 
where ? ? [0,1)  is a mixture noise parameter 
which controls the weight of the background 
model. Given a fixed ?, a standard EM algorithm 
can then be used to estimate ?(?|??), which is 
then interpolated with the original query model 
?(?|Q) to obtain an improved estimation of the  
query model: 
?(?|??) = (1 ? ?)?(?|?) + ??(?|??)         (7) 
 where ? is the feedback coefficient. 
5 Data 
We used a dataset composed of 12,396 queries 
randomly sampled from query logs of a search 
engine. For each query, we retrieved its top 100 
results from a commercial search engine. The 
documents were judged by human editors. A five-
grade (from 0 to 4 meaning from bad to perfect) 
relevance rating was assigned for each document. 
We used a proprietary query domain classifier to 
identify queries in three domains, namely ?lyrics,?  
?recipe,? and ?job,? from the dataset. The statistics 
about these domains are shown in Table 2. To 
investigate how many queries may potentially have 
structured annotations, we manually created 
structured annotations for these queries. The last 
column of Table 2 shows the percentage of queries 
that have structured annotations created by 
annotators. We found that for each domain, there 
was on average more than 90% of queries 
identified by us that had a certain structured 
annotation. This indicates that a large percentage 
of these queries contain structured information, as 
we expected. 
6 Experimental Results 
In this section, we present the structured annotation 
of queries and further re-rank the top search results 
for the three domains introduced in Section 5. We 
used the ranking returned by a commercial search 
engine as our one of the Baselines. Note that as the 
baseline already uses a large number of ranking 
signals, it is very difficult to improve it any further. 
We evaluate the ranking quality using the widely 
used Normalized Discounted Cumulative Gain 
measure (NDCG) (Javelin and Kekalainen., 2000). 
We use the same configuration for NDCG as 
(Burges et al 2005). More specifically, for a given 
query q, the NDCG@K is computed as: 
                        ?? = 
1
??
? (2?(?)?1)??=1
log (1 + ?)
                            (4) 
Mq is a normalization constant (the ideal NDCG) 
so that a perfect ordering would obtain an NDCG 
of 1; and r(j) is the rating score of the j-th  
document in the ranking list.  
6.1 Overall Results 
6.1.1 Quality of Structured Annotation of 
Queries 
We generated the structured annotation of queries 
based on the top 10 search results and used ? =
0.04  for Algorithm 1. We used several existing 
metrics, P (Precision), R (Recall), and F-Measure 
to evaluate the quality of the structured annotation. 
As a query structured annotation may contain more 
than one annotated token, we concluded that the 
 
Figure 2. Ranking Quality (* indicates significant 
improvement) 
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0.62
NDCG@1 NDCG@3 NDCG@5
V
a
lu
e
 o
f 
m
e
a
s
u
r
e
m
e
n
t
Measurement
Seg-Ranker Ori-Ranker Con-Ranker FB-Ranker
*
*
*
*
*
*
Table 3. Quality of Structured Annotation. All the 
improvements are significant (p < 0.05) 
Domain Method Precision Recall F-Measure 
lyrics Baseline 
Our 
90.06% 
95.45% 
84.92% 
89.83% 
87.41% 
92.55% 
job Baseline 
Our 
89.62% 
95.31% 
80.14% 
84.93% 
84.62% 
89.82% 
recipe Baseline 
Our 
83.96% 
89.68% 
84.23% 
88.44% 
84.09% 
89.06% 
All Baseline 
Our 
87.88% 
93.61% 
83.10% 
88.45% 
85.42% 
90.96% 
 
Table 2. Domain queries used in our experiment 
Domain Containing 
Keyword 
Queries 
 
Structured  
Annotation% 
lyrics ?lyrics? 196 95% 
job ?job? 124 92% 
recipe ?recipe? 76   93% 
 
474
annotation was correct only if the entire annotation 
was completely the same as the annotation labeled 
by annotators. Otherwise we treated the structured 
annotation as incorrect. Experimental results for 
the three domains are shown in Table 3. We 
compare our approach with Xiao Li, (2010) 
(denoted as baseline), on the dataset described in 
Section 5. They labeled the semantic structure of 
noun phrase queries based on semi-Markov CRFs. 
Our approach achieves better performance than the 
baseline (about 5.5% significant improvement on 
F-Measure). This indicates that the approach of 
generating structured annotation based on the top 
search results is more effective. With the high-
quality structured annotation of queries in hand, it 
may be possible to obtain better ranking results 
using our proposed re-ranking models. 
6.1.2 Re-ranking Result 
We used the models introduced in Section 4 to re-
rank the top 10 search results, based on structured 
annotation of queries and annotated tokens.  
Recall that our goal is to quantify the 
effectiveness of structured annotation of queries 
for real web search. One dimension is to compare 
with the original search results of a commercial 
search engine (denoted as Ori-Ranker). The other 
is to compare with the query segmentation based 
re-ranking model (denoted as Seg-Ranker; Li et 
al., 2011) which tries to improve web search 
ranking by incorporating query segmentation. Li et 
al., (2011) incorporated query segmentation in the 
BM25, unigram language model and bigram 
language model retrieval framework, and bigram 
language model achieved the best performance. In 
this paper, Seg-Ranker integrates bigram language 
model with query segmentation. 
The ranking results of these models are shown 
in Figure 2. This figure shows that all our two 
rankers significantly outperform the Ori-Ranker? 
the original search results of a commercial search 
engine. This means that using high-quality 
structured annotation does help better 
understanding of user intent. By comparing these 
structured annotations and the annotated tokens in 
documents, we can re-rank the more relevant 
results higher and yield better ranking quality. 
Figure 2 also suggests that structured annotation 
based re-ranking models outperform query 
segmentation based re-ranking model. This is 
mainly because structured annotation can not only 
separate the query words into disjoint segments but 
can also assign each segment a semantic label. 
Taking full advantage of the semantic label can 
lead to better ranking performance. 
Furthermore, Figure 2 shows that FB-Ranker 
outperforms Con-Ranker. The main reason is that 
in Con-Ranker, we can only reasonably re-rank the 
search results with structured data. However, in 
FB-Ranker we can not only re-rank the structured 
search results but also can re-rank other documents 
by incorporating implicit information from those 
structured documents.  
On average, FB-Ranker achieves the best 
ranking performance. Table 4 shows more detailed 
Table 4. Detailed ranking results on three domains. 
All the improvements are significant (p < 0.05) 
Domain Ranking Method NDCG@1 NDCG@3 NDCG@5 
lyrics Seg-Ranker 0.572 0.574 0.575 
Ori-Ranker 
FB-Ranker 
0.621 
0.637 
0.628 
0.639 
0.636 
0.647 
recipe Seg-Ranker 0.629 0.631 0.634 
Ori-Ranker 
FB-Ranker 
0.678 
0.707 
0.687 
0.704 
0.696 
0.709 
job Seg-Ranker 0.438 0.413 0.408 
Ori-Ranker 
FB-Ranker 
0.470 
0.504 
0.453 
0.474 
0.442 
0.459 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9
V
a
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Query structured annotation generation threshold ?
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0 0.02 0. 4 .06 0.08 .1 0.3 0.5 0.7 0.9 perfect
N
D
C
G
@
3
Que tr ctured annotation generation threshold ?
Seg-Ranker Ori-Ranker FB-Ranker
 
                          (a) Quality of re-ranking                                    (b) Quality of query structured annotation 
Figure 3. Quality of re-ranking and quality of query structured annotation with different number of search results 
475
results for the three selected domains. This table 
shows that FB-Ranker consistently outperforms the 
two baseline rankers on these domains. In the 
remaining part of this paper, we will only report 
the results for this ranker, due to space limitations. 
Table 4 also indicates that we can get robust 
ranking improvement in different domains, and we 
will consider applying it to more domains. 
6.2 Experiment with Different Thresholds of 
Query Structured Annotation Algorithm 
As introduced in Algorithm 1, we pre-defined a 
threshold ? for fuzzy string matching. We 
evaluated the quality of re-ranking and query 
structured annotation with different settings for ?. 
The results are shown in Figure 3. We found that: 
(1) When we use ? = 0, which means that the 
structured annotations can be generated no matter 
how small the similarity between the query string 
and a weighted annotated token is, we can get a 
significant NDCG@3 gain of 2.15%. Figure 3(b) 
shows that the precision of the structured 
annotation is lowest when ? = 0 . However, the 
precision is still as high as 0.7375, and the highest 
recall is obtained in this case. This means that the 
quality of the generated structured annotations is 
still reasonable, and hence we can get a ranking 
improvement when ? = 0, as shown in Figure 3(a). 
(2) Figure 3(a) suggests that the quality of re-
ranking increases when the threshold ? increases 
from 0 to 0.05. It then decreases when ? increases 
from 0.06 to 0.5. Comparing these two figures 
shows that the trend of re-ranking performance 
adheres to the quality of the structured annotation. 
The settings for ? dramatically affect the recall and 
precision of the structured annotation; and hence 
the ranking quality is impacted. The larger ? is, the 
lower the recall of the structured annotation is. 
(3) Since the re-ranking performance 
dramatically changes along with the quality of the 
structured annotation, we conducted a re-ranking 
experiment with perfect structured annotations (F-
Measure equal to 1.0). Perfect structured 
annotations mean the annotations created by 
annotators as introduced in Section 5. The results 
are shown in the last bar of Figure 3(a). We did not 
find a large space for ranking improvement. The 
NDCG@3 when using perfect structured 
annotations was 0.606, which is just slightly better 
than our best result (yield when ?=0.05). It 
indicates that our structured annotation generation 
algorithm is already quite effective. 
(4) Figure 3(a) shows that our approach 
outperforms the two baseline approaches with most 
settings for ?. This indicates that our approach is 
relatively stable with different settings for ?. 
6.3 Experiment with Number of Top Search 
Results 
The above experiments are conducted based on the 
top 10 search results. In this section, by adjusting 
the number of top search results, ranging from 2 to 
100, we investigate whether the quality of 
structured annotation of queries and the 
performance of re-ranking are affected by the 
quantity of search results. The results shown in 
Figure 4 indicate that the number of search results 
does affect the quality of structured annotation of 
queries and the performance of re-ranking. 
Structured annotations of queries become better 
when more search results are used from 2 to 20. 
This is because more search results cover more 
websites in our domain list, and hence can generate 
more annotated tokens. More results also provide 
more evidence for voting the importance of 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100
V
a
u
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Number of search results
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
2 3 4 5 6 7 8 9 10 0 30 4 50 60 70 80 90 100
N
D
C
G
@
3
Number of search result
Seg-Ranker Ori-Ranker FB-Ranker
 
                              (a) Quality of re-ranking                                   (b) Quality of query structured annotation 
Figure 4. Quality of re-ranking and quality of query structured annotation with different number of search results 
476
annotated tokens, and hence can improve the 
quality of structured annotation of queries. 
In addition, we also found that structured 
annotation of queries become worse when too 
many lower ranked results are used (e.g, using 
results ranked lower than 20). This is because the 
lower ranked results are less relevant than the 
higher ranked results. They may contain more 
irrelevant or noisy annotated tokens than higher 
ranked documents; and hence using them may 
harm the precision of the structured annotations. 
Figure 4 also indicates that the quality of ranking 
and the accuracy of structured annotations are 
correlated. 
7 Conclusions 
In this paper, we studied the problem of improving 
web search ranking by incorporating structured 
annotation of queries. We proposed a systematic 
solution, first to generate structured annotation of 
queries based on top search results, and then 
launching two structured annotation based re-
ranking models. We performed a large-scale 
evaluation over 12,396 queries from a major search 
engine. The experiment results show that the F-
Measure of query structured annotation generated 
by our approach is as high as 91%. In the same 
dataset, our structured annotation based re-ranking 
model significantly outperforms the original ranker 
? the ranking of a major search engine, with 
improvements 5.2%. 
 
Acknowledgments 
This work was supported by National Natural Science 
Foundation of China (NSFC) via grant 61273321, 
61133012 and the Nation-al 863 Leading Technology 
Research Project via grant 2012AA011102. 
References  
G. Agarwal, G. Kabra, and K. C.-C. Chang. Towards 
rich query interpretation: walking back and forth for 
mining query templates. In Proc. of WWW '10. 
M. Bendersky, W. Bruce Croft and D. A. Smith. Joint 
Annotation of Search Queries, In Proc. of ACL-HLT 
2011. 
M. Bendersky, W. Bruce Croft and D. A. Smith. 
Structural Annotation of Search Queries Using 
Pseudo-Relevance Feedback, In Proc. Of CIKM 2010. 
S. Bergsma and Q. I. Wang. Learning noun phrase 
query segmentation. In Proceedings of EMNLP-
CoNLL'07. 
M. Bron, K. Balog, and M. de Rijke. Ranking related 
entities: components and analyses. In Proc. of 
CIKM ?10. 
C. Buckley. Automatic query expansion using SMART. 
InProc. of TREC-3, pages 69?80, 1995. 
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, 
N. Hamilton, and G. Hullender. Learning to rank 
usinggradient descent. In Proceedings of ICML '05. 
T. Cheng, X. Yan, and K. C.-C. Chang. Supporting 
entity search: a large-scale prototype search engine. 
In Proc. of SIGMOD ?07. 
O. Etzioni, M. Banko, S. Soderland, and D.S. Weld, 
(2008). Open Information Extraction from the Web, 
Communications of the ACM, 51(12): 68-74. 
J. Guo, G. Xu, X. Cheng, and H. Li. Named entity 
recognition in query. In Proc. Of SIGIR? 2009. 
K. Jarvelin and J. Kekalainen. Ir evaluation methods for 
retrieving highly relevant documents. In SIGIR '00. 
J. Lafferty and C. Zhai, Document language models, 
query models, and risk minimization for information 
retrieval, In Proceedings of SIGIR'01, pages 111-119, 
2001. 
V. Lavrenko and W. B. Croft. Relevance based 
language models. In Proc. of SIGIR, pages 120?127, 
2001. 
Y. Li, BJP. Hsu, CX. Zhai and K. Wang. Unsupervised 
Query Segmentation Using Clickthrough for 
Information Retrieval. In Proc. of SIGIR'11. 
X. Li, Y.-Y. Wang, and A. Acero. Extracting structured 
information from user queries with semi-supervised 
conditional random fields. In Proc. of SIGIR'09. 
Y. Liu, X. Ni, J-T. Sun, Z. Chen. Unsupervised 
Transactional Query Classification Based on 
Webpage Form Understanding. In Proc. of CIKM '11. 
Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query 
type identification based on click-through 
information. In LNCS, 2006. 
M. Pasca. Asking What No One Has Asked Before: 
Using Phrase Similarities To Generate Synthetic 
Web Search Queries. In Proc. of CIKM '11. 
G. Salton and C. Buckley. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297, 1990. 
477
N. Sarkas, S. Paparizos, and P. Tsaparas. Structured 
annotations of web queries. In Proc. of SIGMOD'10. 
I. Szpektor, A. Gionis, and Y. Maarek. Improving 
recommendation for long-tail queries via templates. 
In Proc. of WWW '11 
B. Tan and F. Peng. Unsupervised query segmentation 
using generative language models and wikipedia. In 
WWW?08. 
T.-L. Wong, W. Lam, and B. Chen. Mining 
employment market via text block detection and 
adaptive cross-domain information extraction. In 
Proc. SIGIR, pages 283?290, 2009. 
X. Yu and H. Shi. Query segmentation using 
conditional random fields. In Proceedings of KEYS 
'09. 
C. Zhai and J. Lafferty, Model-based feedback in the 
language modeling approach to information 
retrieval , In Proceedings of CIKM'01, pages 403-410, 
2001. 
C. Zhai and J. Lafferty, A study of smoothing methods 
for language models applied to ad hoc information 
retrieval, In Proceedings of SIGIR'01, pages 334-342, 
2001. 
Y. Zhai and B. Liu. Structured data extraction from the 
Web based on partial tree alignment. IEEE Trans. 
Knowl. Data Eng., 18(12):1614?1628, Dec. 2006. 
H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. 
Fully automatic wrapper generation for search 
engines. In Proceedings of WWW ?05. 
S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint 
optimization of wrapper generation and template 
detection. In Proc. of SIGKDD'07. 
478
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 863?868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Microblog Entity Linking by Leveraging Extra Posts
Yuhang Guo, Bing Qin?, Ting Liu , Sheng Li
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{yhguo, bqin?, tliu, sli}@ir.hit.edu.cn
Abstract
Linking name mentions in microblog posts to
a knowledge base, namely microblog entity
linking, is useful for text mining tasks on mi-
croblog. Entity linking in long text has been
well studied in previous works. However few
work has focused on short text such as mi-
croblog post. Microblog posts are short and
noisy. Previous method can extract few fea-
tures from the post context. In this paper we
propose to use extra posts for the microblog
entity linking task. Experimental results show
that our proposed method significantly im-
proves the linking accuracy over traditional
methods by 8.3% and 7.5% respectively.
1 Introduction
Microblogging services (e.g. Twitter) are attracting
millions of users to share and exchange their ideas
and opinions. Millions of new microblog posts are
generated on such open broadcasting platforms ev-
ery day 1. Microblog provides a fruitful and instant
channel of global information publication and acqui-
sition.
A necessary step for the information acquisition
on microblog is to identify which entities a post is
about. Such identification can be challenging be-
cause the entity mention may be ambiguous. Let?s
begin with a real post from Twitter.
(1) No excuse for floods tax, says Abbott
URL
?Corresponding author
1See http://blog.twitter.com/2011/06/ 200-million-tweets-
per-day.html.
This post is about an Australia political lead-
er, Tony Abbot, and his opinion on flood tax
policy. To understand that this post mentions
Tony Abbot is not trivial because the name Ab-
bot can refer to many people and organization-
s. In the Wikipedia page of Abbott, there list-
s more than 20 Abbotts, such as baseball player
Jim Abbott, actor Bud Abbott and company
Abbott Laboratories, etc..
Given a knowledge base (KB) (e.g. Wikipedia),
entity linking is the task to identify the referent KB
entity of a target name mention in plain text. Most
current entity linking techniques are designed for
long text such as news/blog articles (Mihalcea and
Csomai, 2007; Cucerzan, 2007; Milne and Witten,
2008; Han and Sun, 2011; Zhang et al, 2011; Shen
et al, 2012; Kulkarni et al, 2009; Ratinov et al,
2011). Entity linking for microblog posts has not
been well studied.
Comparing with news/blog articles, microblog
posts are:
short each post contains no more than 140 charac-
ters;
fresh the new entity-related content may have not
been included in the knowledge base;
informal acronyms and spoken language writing
style are common.
Due to these properties, few feature can be ex-
tracted from a post. Without enough features, pre-
vious entity linking methods may fail. In order to
overcome the feature sparseness, we turn to another
property of microblog:
863
redundancy For each day, over 340M short mes-
sages are posted in twitter. Similar information
may be posted in different expressions.
For example, we find the following post,
(2) Julia Gillard and Tony Abbott on
the flood levy just after 8.30am on
@612brisbane!
The content of post (2) is highly related to post
(1). In contrast to the confusing post (1), the text
in post (2) explicitly indicates that the Abbott here
refers to the Australian political leader. This inspires
us to bridge the confusing post and the knowledge
base with other posts.
In this paper, we approach the microblog entity
linking by leveraging extra posts. A straightforward
method is to expand the post context with similar
posts, which we call Context-Expansion-based Mi-
croblog Entity Linking (CEMEL). In this method,
we first construct a query with the given post and
then search for it in a collection of posts. From the
search result, we select the most similar posts for the
context expansion. The disambiguation will benefit
from the extra posts if, hopefully, they are related
to the given post in content and include explicit fea-
tures for the disambiguation.
Furthermore, we propose a Graph-based Mi-
croblog Entity Linking (GMEL) method. In contrast
to CEMEL, the extra posts in GMEL are not directly
added into the context. Instead, they are represented
as nodes in a graph, and weighted by their similarity
with the target post. We use an iterative algorithm
in this graph to propagate the entity weights through
the edges between the post nodes.
We conduct experiments on real microblog da-
ta which we harvested from Twitter. Current enti-
ty linking corpus, such as the TAC-KBP data (M-
cNamee and Dang, 2009), mainly focuses on long
text. And few microblog entity linking corpus is
publicly available. In this work, we manually anno-
tated a microblog entity linking corpus. This corpus
inherit the target names from TAC-KBP2009. So it
is comparable with the TAC-KBP2009 corpus.
Experimental results show that the performance
of previous methods drops on microblog posts com-
paring with on long text. Both of CEMEL and
GMEL can significantly improve the performance
over baselines, which means that entity linking sys-
tem on microblog can be improved by leveraging ex-
tra posts. The results also show that GMEL outper-
forms CEMEL significantly.
We summarize our contributions as follows.
? We propose a context-expansion-based and a
graph-based method for microblog entity link-
ing by leveraging extra posts.
? We annotate a microblog entity linking corpus
which is comparable to an existing long text
corpus.
? We show the inefficiency of previous method
on the microblog corpus and our method can
significantly improve the results.
2 Task defination
The microblog entity linking task is that, for a name
mention in a microblog post, the system is to find the
referent entity of the name in a knowledge base, or
return a NIL mark if the entity is absence from the
knowledge base. This definition is close to the en-
tity linking task in the TAC-KBP evaluation (Ji and
Grishman, 2011) except for the context of the target
name is microblog post whereas in TAC-KBP the
context is news article or web log.
Several related tasks have been studied on mi-
croblog posts. In Meij et al (2012)?s work, they
link a post, rather than a name mention in the post,
to relevant Wikipedia concepts. Guo et al (2013a)
and Liu et al (2013) define entity linking as to first
detect all the mentions in a post and then link the
mentions to the knowledge base. In contrast, our
definition (as well as the TAC-KBP definition) fo-
cuses on a concerned name mention across different
posts/documents.
3 Method
A typical entity linking system can be broken down
into two steps:
candidate generation This step narrows down the
candidate entity range from any entity in the
world to a limited set.
candidate ranking This step ranks the candidates
and output the top ranked entity as the result.
864
Figure 1: An example of the GMEL graph. p1 . . . p4 are
post nodes and c1 . . . c3 are candidate entity nodes. Each
post node is connected to the corresponding candidate n-
odes from the knowledge base. The edges between the
nodes are weighted by the similarity between them.
In this paper, we use the candidate generation
method described in Guo et al(2013). For the candi-
date ranking, we use a Vector Space Model (VSM)
and a Learning to Rank (LTR) as baselines. VSM
is an unsupervised method and LTR is a supervised
method. Both of them have achieved the state-of-
the-art performances in the TAC-KBP evaluations.
The major challenge in microblog entity linking
is the lack of context in the post. An ideal solu-
tion is to expand the context with the posts which
contain the same entity. However, automatically
judging whether a name mention in two documents
refers to the same entity, namely cross document co-
reference, is not trivial. Here our solution is to rank
the posts by their possibility of co-reference to the
target one and select the most possible co-referent
posts for the expansion.
CEMEL is based on the assumption that, given a
name and two posts where the name is mentioned,
the higher similarity between the posts the high-
er possibility of their co-reference and that the co-
referent posts may contains useful features for the
disambiguation. However, two literally similar posts
may not be co-referent. If such non co-referent post
is expanded to the context, noises may be included.
Take the following post as an example.
(3) AG Abbott says that bullets have
crossed the border from Mexico to
Texas at least four times. URL
This post is similar to post (1) because they both
contains ?says? and ?URL?. But the Abbott in post
(3) refers to the Texas Attorney General Greg Ab-
bott. In this mean, the expanded context in post (3)
could mislead the disambiguation for post (1). Such
noise can be controlled by setting a strict number of
posts to expand the context or weighting the contri-
bution of this post to the target one.
Our CEMEL method consists of the following
steps: First we construct a query with the terms from
the target post. Second we search for the query in a
microblog post collection using a common informa-
tion retrieval model such as the vector space model.
Note that here we limit the searched posts must con-
tain the target name mention. Then we expand the
target post with top N similar posts and use a typical
entity linking method (such as VSM and LTR) with
the expanded context.
Figure 1 illustrates the graph of GMEL. Each n-
ode of this graph represents an candidate entity (e.g.
c1 . . . c3) or a post of the given target name (e.g.
p1 . . . p4) In this graph, each node represents an en-
tity or a post of the given target name. Between each
pair of post nodes, each pair of entity nodes and each
post node and its candidate entity nodes, there is an
edge. The edge is weighted by the similarity be-
tween the two linked nodes. Entity nodes are labeled
by themselves and candidate nodes are initialized as
unlabeled nodes. For the edges between post node
pairs and entity node pairs, we use cosine similari-
ty. For the edges between a post node and its can-
didate entity nodes, we use the score given by tra-
ditional entity linking methods. We use an iterative
algorithm on this graph to propagate the labels from
the entity nodes to the post nodes. We adapt Label
Propagation (LP) (Zhu and Ghahramani, 2002) and
Modified Adsorption (MAD) (Talukdar and Pereira,
2010) for the iteration over the graph.
4 Experiment
4.1 Data Annotation
Till now, few microblog entity linking data is pub-
licly available. In this work, we manually annotate
a data set on microblog posts2. We collect 15.6 mil-
lion microblog posts in Twitter dated from January
23 to February 8, 2011. In order to compare with ex-
isting entity linking on long text, we select a subset
of target names from TAC-KBP2009 and inherit the
knowledge base in the TAC-KBP evaluation. The
2We published this data so that researchers can reproduce
our results.
865
Figure 2: Percentage of the co-reference posts in the top
N similar posts
Figure 3: Impact of expansion post number in CEMEL
TAC-KBP2009 data set includes 513 target names.
We search for all the target names in the post col-
lection and get 26,643 matches. We randomly sam-
ple 120 posts for each of the top 30 most frequently
matched target names and filter out non-English and
overly short (i.e. less than 3 words) posts. Then
we get 2,258 posts for 25 target names and manual-
ly link the target name mentions in the posts to the
TAC-KBP knowledge base.
In order to evaluate the assumption in CEMEL:
similar posts tend to co-reference, we randomly s-
elect 10 posts for 5 target names respectively and
search for the posts in the post collection. From
the search result of each of the 50 posts, we select
the top 20 posts and manually annotate if they co-
reference with the query post.
4.2 Settings
We generate candidates with the method described
in (Guo et al, 2013b) and use Vector Space Mod-
el (VSM) (Varma et al, 2009) and Learning to Rank
(LTR) (Zheng et al, 2010) as the ranking model. We
Figure 4: Accuracy of GMEL with different rate of extra
post nodes
use Lucene and ListNet with default settings for the
VSM and LTR implementation respectively. We use
bigram feature for VSM and the feature set of (Chen
et al, 2011) for LTR. LTR is evaluated with 10-fold
cross validation. Given a target name, the GMEL
graph includes all the evaluation posts as well as a
set of extra post nodes searched from the post collec-
tion with the query of the target name. We filter out
determiners, interjections, punctuations, emoticon-
s, discourse markers and URLs in the posts with a
twitter part-of-speech tagger (Owoputi et al, 2013).
The similarity between a post and its candidate en-
tities is set with the score given by VSM or LTR
and the similarity between other nodes is set with the
corresponding cosine similarity. We employ junto3
with default settings for the iterative algorithm im-
plementation .
4.3 Results
Figure 2 shows the relationship between similari-
ty and co-reference. From this figure we can see
that the percentage decreases with the growth of N.
When the N is up to 10, about 60% of the similar
posts co-reference with the query post and the de-
crease speed slows down. The Pearson correlation
coefficient between the percentage and the number
of top N is -0.843, which shows a significant corre-
lation between the two variables (with p-value 0.01
under t-test).
Figure 3 shows the impact of the extra post num-
ber for the context expansion in CEMEL. We can see
that the accuracies of VSM and LTR are improved
3See https://github.com/parthatalukdar/junto
866
Figure 5: Label entropy of GMEL with different rate of
extra post nodes
Figure 6: Accuracy of the systems
by CEMEL. The improvements peak with 5-10 ex-
tra posts. Then more extra posts will pull down the
accuracy.
Figure 4 shows the accuracy of GMEL. The x-axis
is the rate of the extra post number over the evalu-
ation post number. We can see that the accuracy of
MAD increases with the number of extra post nodes
at first and then turns to be stable. The accuracy of
LP increases at first and drops when more extra posts
are added into the graph.
Figure 5 shows the information entropy of the la-
bels in LP and MAD. The curves show that the pre-
diction of LP tends to converge into a small number
of labels. This is because LP prefers smoothing la-
belings over the graph (Talukdar and Pereira, 2010).
We also evaluate our baselines on TAC-KBP2009
data set (LTR is trained on TAC-KBP2010 data set).
The accuracy of VSM and LTR are 0.8338 and
0.8372 respectively, which are comparable with the
state-of-the-art result (Hachey et al, 2013).
Figure 6 shows the performances of the systems
on the microblog data. We set the optimal expansion
post number of CEMEL and use MAD algorithm for
GMEL with all searched extra post nodes. From this
figure we can see that the results of VSM and LTR
baselines are comparable and both of them are sig-
nificantly lower than that on TAC-KBP2009 data.
CEMEL improves the VSM and LTR baselines by
4.3% and 2.7% respectively. GMEL improves VSM
and LTR by 8.3% and 7.5% respectively. The results
of GMEL are also significantly better than CEMEL.
All of the improvements are significant under Z-test
with p < 0.05.
5 Conclusion
In this paper we approach microblog entity linking
by leveraging extra posts. We propose a context-
expansion-based and a graph-based method. Exper-
imental results on our data set show that the per-
formance of traditional method drops on the mi-
croblog data. The graph-based method outperform-
s the context-expansion-based method and both of
them significantly improve the accuracy of tradition-
al methods. In the graph-based method the modified
adsorption algorithm performs better than the label
propagation algorithm.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61273321, 61073126, 61133012 and the National
863 Leading Technology Research Project via grant
2012AA011102. We would like to thank to Wanx-
iang Che, Ruiji Fu, Yanyan Zhao, Wei Song and
several anonymous reviewers for their constructive
comments and suggestions.
References
Zheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji.
2011. A toolkit for knowledge base population. In
SIGIR, pages 1267?1268.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Method-
s in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
867
Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013a. To link or not to link? a study on end-to-
end tweet entity linking. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1020?1030, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and Sheng
Li. 2013b. Improving candidate generation for entity
linking. In Elisabeth Mtais, Farid Meziane, Mohamad
Saraee, Vijayan Sugumaran, and Sunil Vadera, edi-
tors, Natural Language Processing and Information
Systems, volume 7934 of Lecture Notes in Computer
Science, pages 225?236. Springer Berlin Heidelberg.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating en-
tity linking with wikipedia. Artificial Intelligence,
194(0):130 ? 150. ?ce:title?Artificial Intelligence,
Wikipedia and Semi-Structured Resources?/ce:title?.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Techologies, pages 945?954, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148?1158, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu
Wei, and Yi Lu. 2013. Entity linking for tweets. In
Proceedings of the 51th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics.
P. McNamee and H.T. Dang. 2009. Overview of
the tac 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference
(TAC2009).
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 563?
572, New York, NY, USA. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
?07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233?242, New York, NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In CIKM ?08: Proceeding of the 17th
ACM conference on Information and knowledge man-
agement, pages 509?518, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In NAACL2013,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computation-
al Linguistics: Human Language Technologies, pages
1375?1384, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-
edge base via semantic knowledge. In Proceedings of
the 21st international conference on World Wide We-
b, WWW ?12, pages 449?458, New York, NY, USA.
ACM.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473?1481, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi,
Praveen Bysani, Santosh GSK, Kiran Kumar N, Kran-
thi Reddy, Karuna Kumar, and Nitin Maganti. 2009.
Iiit hyderabad at tac 2009. In Proceedings of the Sec-
ond Text Analysis Conference (TAC 2009), Gaithers-
burg, Maryland, USA, November.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011. Entity linking with effective acronym expan-
sion, instance selection, and topic modeling. In Toby
Walsh, editor, IJCAI 2011, pages 1909?1914.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In NAACL2010, pages 483?491, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
868
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1224?1234,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Multiple Sources for Open-domain Hypernym Discovery
Ruiji Fu, Bing Qin, Ting Liu?
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{rjfu, bqin, tliu}@ir.hit.edu.cn
Abstract
Hypernym discovery aims to extract such
noun pairs that one noun is a hypernym of
the other. Most previous methods are based
on lexical patterns but perform badly on open-
domain data. Other work extracts hypernym
relations from encyclopedias but has limited
coverage. This paper proposes a simple yet ef-
fective distant supervision framework for Chi-
nese open-domain hypernym discovery. Giv-
en an entity name, we try to discover its hy-
pernyms by leveraging knowledge from mul-
tiple sources, i.e., search engine results, ency-
clopedias, and morphology of the entity name.
First, we extract candidate hypernyms from
the above sources. Then, we apply a statistical
ranking model to select correct hypernyms. A
set of novel features is proposed for the rank-
ing model. We also present a heuristic strate-
gy to build a large-scale noisy training data for
the model without human annotation. Exper-
imental results demonstrate that our approach
outperforms the state-of-the-art methods on a
manually labeled test dataset.
1 Introduction
Hypernym discovery is a task to extract such noun
pairs that one noun is a hypernym of the other (S-
now et al, 2005). A noun H is a hypernym of an-
other noun E if E is an instance or subclass of H. In
other word, H is a semantic class of E. For instance,
?actor? is a hypernym of ?Mel Gibson?; ?dog? is a
hypernym of ?Caucasian sheepdog?; ?medicine? is
a hypernym of ?Aspirin?. Hypernym discovery is
an important subtask of semantic relation extraction
?Email correspondence.
and has many applications in ontology construction
(Suchanek et al, 2008), machine reading (Etzion-
i et al, 2006), question answering (McNamee et al,
2008), and so on.
Some manually constructed thesauri such as
WordNet can also provide some semantic relations
such as hypernyms. However, these thesauri are lim-
ited in its scope and domain, and manual construc-
tion is knowledge-intensive and time-consuming.
Therefore, many researchers try to automatically ex-
tract semantic relations or to construct taxonomies.
Most previous methods on automatic hypernym
discovery are based on lexical patterns and suffer
from the problem that such patterns can only cov-
er a small part of complex linguistic circumstances
(Hearst, 1992; Turney et al, 2003; Zhang et al,
2011). Other work tries to extract hypernym rela-
tions from large-scale encyclopedias like Wikipedia
and achieves high precision (Suchanek et al, 2008;
Hoffart et al, 2012). However, the coverage is limit-
ed since there exist many infrequent and new entities
that are missing in encyclopedias (Lin et al, 2012).
We made similar observation that more than a half
of entities in our data set have no entries in the en-
cyclopedias.
This paper proposes a simple yet effective distan-
t supervision framework for Chinese open-domain
hypernym discovery. Given an entity name, our goal
is to discover its hypernyms by leveraging knowl-
edge from multiple sources. Considering the case
where a person wants to know the meaning of an un-
known entity, he/she may search it in a search engine
and then finds out the answer after going through the
search results. Furthermore, if he/she finds an en-
try about the entity in an authentic web site, such as
Wikipedia, the information will help him/her under-
1224
stand the entity. Also, the morphology of the enti-
ty name can provide supplementary information. In
this paper, we imitate the process. The evidences
from the above sources are integrated in our hyper-
nym discovery model.
Our approach is composed of two major steps:
hypernym candidate extraction and ranking. In the
first step, we collect hypernym candidates from mul-
tiple sources. Given an entity name, we search it in
a search engine and extract high-frequency nouns as
its main candidate hypernyms from the search re-
sults. We also collect the category tags for the entity
from two Chinese encyclopedias and the head word
of the entity as the candidates.
In the second step, we identify correct hypernyms
from the candidates. We view this task as a rank-
ing problem and propose a set of effective features
to build a statistical ranking model. For the param-
eter learning of the model, we also present a heuris-
tic strategy to build a large-scale noisy training data
without human annotation.
Our contributions are as follows:
? We are the first to discover hypernym for Chi-
nese open-domain entities by exploiting mul-
tiple sources. The evidences from different
sources can authenticate and complement each
other to improve both precision and recall.
? We manually annotate a dataset containing
1,879 Chinese entities and their hypernyms,
which will be made publicly available. To the
best of our knowledge, this is the first dataset
for Chinese hypernyms.
? We propose a set of novel and effective fea-
tures for hypernym ranking. Experimental re-
sults show that our method achieves the best
performance.
Furthermore, our approach can be easily ported
from Chinese to English and other languages, except
that a few language dependent features need to be
changed.
The remainder of the paper is organized as fol-
lows: Section 2 discusses the related work. Section
3 introduces our method in detail. Section 4 de-
scribes the experimental setup. Section 5 shows the
experimental results. Conclusion and future work
are presented in Section 6.
2 Related Work
Previous methods for hypernym discovery can be
summarized into two major categories, i.e., pattern-
based methods and encyclopedia-based methods.
Pattern-based methods make use of manually
or automatically constructed patterns to mine hyper-
nym relations from text corpora. The pioneer work
by Hearst (1992) finds that linking two noun phras-
es (NPs) via certain lexical constructions often im-
plies hypernym relations. For example, NP1 is a hy-
pernym of NP2 in the lexical pattern ?such NP1 as
NP2?. Similarly, succeeding researchers follow her
work and use handcrafted patterns to extract hyper-
nym pairs from corpora (Caraballo, 1999; Scott and
Dominic, 2003; Ciaramita and Johnson, 2003; Tur-
ney et al, 2003; Pasca, 2004; Etzioni et al, 2005;
Ritter et al, 2009; Zhang et al, 2011).
Evans (2004) considers the web data as a large
corpus and uses search engines to identify hyper-
nyms based on lexical patterns. Given an arbitrary
document, he takes each capitalized word sequence
as an entity and aims to find its potential hypernyms
through pattern-based web searching. Suppose X is
a capitalized word sequence. Some pattern queries
like ?such as X? are threw into the search engine.
Then, in the retrieved documents, the nouns that im-
mediately precede the pattern are recognized as the
hypernyms of X. This work is most related to ours.
However, the patterns used in his work are too strict
to cover many low-frequency entities, and our ex-
periments show the weakness of the method.
Snow et al (2005) for the first time propose to au-
tomatically extract large numbers of lexico-syntactic
patterns and then detect hypernym relations from
a large newswire corpus. First, they use some
known hypernym-hyponym pairs from WordNet as
seeds and collect many patterns from a syntactical-
ly parsed corpus in a bootstrapping way. Then, they
consider all noun pairs in the same sentence as po-
tential hypernym-hyponym pairs and use a statistical
classifier to recognize the correct ones. All patterns
corresponding to the noun pairs in the corpus are
fed into the classifier as features. Their method re-
lies on accurate syntactic parsers and it is difficult to
guarantee the quality of the automatically extracted
patterns. Our experiments show that their method is
inferior to ours.
1225
Encyclopedia-based methods extract hyper-
nym relations from encyclopedias like Wikipedia
(Suchanek et al, 2008; Hoffart et al, 2012). The
user-labeled information in encyclopedias, such as
category tags in Wikipedia, is often used to derive
hypernym relations.
In the construction of the famous ontology YA-
GO, Suchanek et al (2008) consider the title of each
Wikipedia page as an entity and the corresponding
category tags as its potential hypernyms. They ap-
ply a shallow semantic parser and some rules to dis-
tinguish the correct hypernyms. Heuristically, they
find that if the head of the category tag is a plural
word, the tag is most likely to be a correct hyper-
nym. However, this method cannot be used in Chi-
nese because of the lack of plurality information.
The method of Suchanek et al (2008) cannot han-
dle the case when the entity is absent in Wikipedia.
To solve this problem, Lin et al (2012) connect the
absent entities with the entities present in Wikipedia
sharing common contexts. They utilize the Freebase
semantic types to label the present entities and then
propagate the types to the absent entities. The Free-
base contains most of entities in Wikipedia and as-
signs them semantic types defined in advance. But
there are no such resources in Chinese.
Compared with previous work, our approach tries
to identify hypernyms from multiple sources. The
evidences from different sources can authenticate
and complement each other to improve both preci-
sion and recall. Our experimental results show the
effectiveness of our method.
3 Method
Our method is composed of two steps. First, we col-
lect candidate hypernyms from multiple sources for
a given entity. Then, a statistical model is built for
hypernym ranking based on a set of effective fea-
tures. Besides, we also present a heuristic strategy
to build a large-scale training data.
3.1 Candidate Hypernym Collection from
Multiple Sources
In this work, we collect potential hypernyms from
four sources, i.e., search engine results, two ency-
clopedias, and morphology of the entity name.
We count the co-occurrence frequency between
the target entities and other words in the returned
snippets and titles, and select top N nouns (or noun
phrases) as the main candidates. As the experiments
show, this method can find at least one hypernym
for 86.91% entities when N equals 10 (see Section
5.1). This roughly explains why people often can in-
fer semantic meaning of unknown entities after go-
ing through several search results.
Furthermore, the user-generated encyclopedia
category tags are important clues if the entity exist-
s in a encyclopedia. Thus we add these tags into
the candidates. In this work, we consider two Chi-
nese encyclopedias, Baidubaike and Hudongbaike1,
as hypernym sources.
In addition, the head words of entities are also
their hypernyms sometimes. For example, the head
word of ??2? (Emperor Penguin)? indicates
that it?s a kind of ?? (penguins)?. Thus we put
head words into the hypernym candidates. In Chi-
nese, head words are often laid after their modifiers.
Therefore, we try to segment a given entity. If it can
be segmented and the last word is a noun, we take
the last word as the head word. In our data set, the
head words of 41.35% entities are real hypernyms
(see Section 5.1).
We combine all of these hypernym candidates to-
gether as the input of the second stage. The final
coverage rate reaches 93.24%.
3.2 Hypernym Ranking
After getting the candidate hypernyms, we then
adopt a ranking model to determine the correct hy-
pernym. In this section, we propose several effective
features for the model. The model needs training da-
ta for learning how to rank the data in addition to
parameter setting. Considering that manually anno-
tating a large-scale hypernym dataset is costly and
time-consuming, we present a heuristic strategy to
collect training data. We compare three hypernym
ranking models on this data set, including Support
Vector Machine (SVM) with a linear kernel, SVM
with a radial basis function (RBF) kernel and Logis-
tic Regression (LR).
1Baidubaike (http://baike.baidu.com) and
Hudongbaike (http://www.baike.com) are two largest
Chinese encyclopedias containing more than 6.26 million and
7.87 million entries respectively, while Chinese Wikipedia
contains about 0.72 million entries until September, 2013.
1226
Feature Comment Value Range
Prior the prior probability of a candidate being a potential hypernym [0, 1]
Is Tag
whether a candidate is a category tag in the encyclopedia
page of the entity if it exists
0 or 1
Is Head whether a candidate is the head word of the entity 0 or 1
In Titles
some binary features based on the frequency of occurrence of
a candidate in the document titles in the search results
0 or 1
Synonyms
the ratio of the synonyms of the candidate in the candidate
list of the entity
[0, 1]
Radicals
the ratio of the radicals of characters in a candidate matched
with the last character of the entity
[0, 1]
Source Num the number of sources where the candidate is extracted 1, 2, 3, or 4
Lexicon the hypernym candidate itself and its head word 0 or 1
Table 1: The features for ranking
3.2.1 Features for Ranking
The features for hypernym ranking are shown in
Table 1. We illustrate them in detail in the following.
Hypernym Prior: Intuitively, different words
have different probabilities as hypernyms of some
other words. Some are more probable as hypernyms,
such as animal, plant and fruit. Some other words
such as sun, nature and alias, are not usually used
as hypernyms. Thus we use a prior probability to
express this phenomenon. The assumption is that if
the more frequent that a noun appears as category
tags, the more likely it is a hypernym. We extract
category tags from 2.4 million pages in Baidubaike,
and compute the prior probabilities prior(w) for a
word w being a potential hypernym using Equation
1. countCT (w) denotes the times a word appeared
as a category tag in the encyclopedia pages.
prior(w) =
countCT (w)
?
w? countCT (w
?)
(1)
In Titles: When we enter a query into a search
engine, the engine returns a search result list, which
contains document titles and their snippet text. The
distributions of hypernyms and non-hypernyms in ti-
tles are compared with that in snippets respectively
in our training data. We discover that the average
frequency of occurrence of hypernyms in titles is
15.60 while this number of non-hypernyms is only
5.18, while the difference in snippets is very small
(Table 2). Thus the frequency of candidates in titles
can be used as features. In this work the frequency
Avg. Frequency in
titles snippets
Hypernym 15.60 33.69
Non-Hypernym 5.18 30.61
Table 2: Distributions of candidate hypernyms in titles
and snippets
is divided into three cases: greater than 15.60, less
than 5.18, and between 5.18 and 15.60. Three binary
features are used to represent these cases.
Synonyms: If there exist synonyms of a candi-
date hypernym in the candidate list, the candidate is
probably correct answer. For example, when ???
(medicine)? and ??? (medicine)? both appear in
the candidate list of an entity, the entity is probably
a kind of medicine. We get synonyms of a candidate
from a Chinese semantic thesaurus ? Tongyi Cilin
(Extended) (CilinE for short)2 and compute the s-
core as a feature using Equation 2.
ratiosyn(h, le) =
countsyn(h, le)
len(le)
(2)
Given a hypernym candidate h of an entity e and
the list of all candidates le, we compute the ratio of
the synonyms of h in le. countsyn(h, le) denotes the
count of the synonyms of h in le. len(le) is the total
count of candidates.
2CilinE contains synonym and hypernym relations among
77 thousand words, which is manually organized as a hierarchy
of five levels.
1227
Radicals: Chinese characters are a form of
ideogram. By far, the bulk of Chinese characters
were created by linking together a character with a
related meaning and another character to indicate its
pronunciation. The character with a related meaning
is called radical. Sometimes, it is a important clue to
indicate the semantic class of the whole character.
For example, the radical ??? means insects, so it
hints ?|n (dragonfly)? is a kind of insects. Simi-
larly ??? hints ?nJ (lymphoma)? is a kind of
diseases. Thus we use radicals as a feature the value
of which is computed by using Equation 3.
radical(e, h) =
countRM (e, h)
len(h)
(3)
Here radical(e, h) denotes the ratio of characters
radical-matched with the last character of the entity
e in the hypernym h. countRM (e, h) denotes the
count of the radical-matched characters in h. len(h)
denotes the total count of the characters in h.
3.2.2 Training Data Collection
Now training data is imperative to learn the
weights of the features in Section 3.2.1. Hence, we
propose a heuristic strategy to collect training data
from encyclopedias.
Firstly, we extract a number of open-domain enti-
ties from encyclopedias randomly. Then their hyper-
nym candidates are collected by using the method
proposed in Section 3.1. We select positive training
instances following two principles:
? Principle 1: Among the four sources used for
candidate collection, the more sources from
which the hypernym candidate is extracted, the
more likely it is a correct one.
? Principle 2: The higher the prior of the candi-
date being a hypernym is, the more likely it is a
correct one.
We select the best candidates following Principle
1 and then select the best one in them as a positive
instance following Principle 2. And we select a can-
didate as a negative training instance when it is from
only one source and its prior is the lowest. If there
are synonyms of training instances in the candidates
list, the synonyms are also extended into the training
set.
Domain
# of entities
Dev. Test
Biology 72 351
Health Care 61 291
Food 75 303
Movie 51 204
Industry 56 224
Others 35 136
Total 350 1529
Table 3: The evaluation data
In this way, we collect training data automatically,
which are used to learn the feature weights of the
ranking models.
4 Experimental Setup
In this work, we use Baidu3 search engine, the most
popular search engine for Chinese, and get the top
100 search results for each entity. The Chinese seg-
mentation, POS tagging and dependency parsing is
provided by an open-source Chinese language pro-
cessing platform LTP4 (Che et al, 2010).
4.1 Experimental Data
In our experiments, we prepare open-domain enti-
ties from dictionaries in wide domains, which are
published by a Chinese input method editor soft-
ware Sogou Pinyin5. The domains include biology,
health care, food, movie, industry, and so on. We
sample 1,879 entities from these domain dictionaries
and randomly split them into 1/5 for developmen-
t and 4/5 for test (Table 3). We find that only 865
(46.04%) entities exist in Baidubaike or Hudong-
baike. Then we extract candidate hypernyms for the
entities and ask two annotators to judge each hyper-
nym relation pair true or false manually. A pair (E,
H) is annotated as true if the annotators judge ?E is a
(or a kind of) H? is true. Finally, we get 12.53 candi-
date hypernyms for each entity on average in which
about 2.09 hypernyms are correct. 4,330 hypernym
relation pairs are judged by both the annotators. We
measure the agreement of the judges using the Kap-
pa coefficient (Siegel and Castellan Jr, 1988). The
3http://www.baidu.com
4http://ir.hit.edu.cn/demo/ltp/
5http://pinyin.sogou.com/dict/
1228
0 5 10 15 20
0.2
0.4
0.6
0.8
1.0
Top N
Cove
rage 
Rate
SRNSRN + ET+HW
Figure 1: Effect of candidate hypernym coverage rate
while varying N
Kappa value is 0.79.
Our training data, containing 11,481 positive in-
stances and 18,378 negative ones, is extracted from
Baidubaike and Hudongbaike using the heuristic s-
trategy proposed in Section 3.2.2.
4.2 Experimental Metrics
The evaluation metrics for our task include:
Coverage Rate: We evaluate coverage rate of the
candidate hypernyms. Coverage rate is the number
of entities for which at least one correct hypernym is
found divided by the total number of all entities.
Precision@1: Our method returns a ranked list
of hypernyms for each entity. We evaluate precision
of top-1 hypernyms (the most probable ones) in the
ranked lists, which is the number of correct top-1
hypernyms divided by the number of all entities.
R-precision: It is equivalent to Precision@R
where R is the total number of candidates labeled
as true hypernyms of an entity.
Precision, Recall, and F-score: Besides, we can
convert our ranking models to classification models
by setting thresholds. Varying the thresholds, we can
get different precisions, recalls, and F-scores.
5 Results and Analysis
5.1 The Coverage of Candidate Hypernyms
In this section, we evaluate the coverage rate of the
candidate hypernyms. We check the candidate hy-
pernyms of the whole 1,879 entities in the develop-
ment and test sets and see how many entities we can
collect at least one correct hypernym for.
Source
Coverage
Rate
Avg. #
SR10 0.8691 9.44?
ET 0.3938 3.07
HW 0.4135 0.87?
SR10 + ET 0.8909 12.02
SR10 + HW 0.9117 9.75
ET + HW 0.7073 3.92
SR10 + ET + HW 0.9324 12.53
Table 4: Coverage evaluation of the candidate hypernym
extraction
There are four different sources to collect candi-
dates as described in Section 3.1, which can be di-
vided into three kinds: search results (SR for short),
encyclopedia tags (ET) and head words (HW). For
SR, we select top N frequent nouns (SRN ) in the
search results of an entity as its hypernym candi-
dates. The effect of coverage rate while varying N
is shown in Figure 1. As we can see from the fig-
ure, the coverage rate is improved significantly by
increasing N until N reaches 10. After that, the
improvement becomes slight. When the candidates
from all sources are merged, the coverage rate is fur-
ther improved.
Thus we set N as 10 in the remaining experi-
ments. The detail evaluation is shown in Table 4.
We can see that top 10 frequent nouns in the search
results contain at least one correct hypernym for
86.91% entities in our data set. This coincides with
the intuition that people usually can infer the seman-
tic classes of unknown entities by searching them in
web search engines.
The coverage rate of ET merely reaches 39.38%.
We find the reason is that more than half of the enti-
ties have no encyclopedia pages. The average num-
ber of candidate hypernyms from ET is 3.07. Note
that the number is calculated among all the enti-
ties. We also calculate the average number only for
the present entities in encyclopedias. The number
reaches 6.68. The reason is that for many present en-
tities, the category tags include not only hypernyms
?For some of entities are rare, there may be less than 10
nouns in the search results. So the average count of candidates
is less than 10.
?Not all of the entities can be segmented. We cannot get the
head words of the ones that cannot be segmented.
1229
Method
Present Entities Absent Entities All Entities
P@1 R-Prec P@1 R-Prec P@1 R-Prec
MPattern 0.5542 0.4937 0.4306 0.3638 0.5229 0.4608
MSnow 0.3199 0.2592 0.2827 0.2610 0.3092 0.2597
MPrior 0.7339 0.5483 0.3940 0.3531 0.5494 0.4423
MSVM?linear 0.8569 0.6899 0.6157 0.5837 0.7260 0.6322
MSVM?rbf 0.8484 0.6940 0.6241 0.5901 0.7266 0.6376
MLR 0.8612 0.7052 0.6807 0.6258 0.7632 0.6621
Table 5: Precision@1 and R-Precision results on the test set. Here the present entities mean the entities existing in the
encyclopedias. The absent entities mean the ones not existing in the encyclopedias.
but also related words. For example, ??.|?
% (Bradley Center)? in Baidubaike have 5 tags, i.e.,
?NBA?, ?N? (sports)?, ?N?$? (sports)?, ?;
? (basketball)?, and ?|, (arena)?. Among them,
only ?|, (arena)? is a proper hypernym whereas
the others are some related words indicating mere-
ly thematic vicinity. Comparing the results of SR10
and SR10 + ET, we can see that collecting candidates
from ET can improve coverage, although many in-
correct candidates are added in at the same time.
The HW source provides 0.87 candidates on av-
erage with 41.35% coverage rate. That is to say, for
these entities, people can infer the semantic classes
when they see the surface lexicon.
At last, we combine the candidates from all of the
three sources as the input of the ranking methods.
The coverage rate reaches 93.24%.
We also compare with the manually construct-
ed semantic thesaurus CilinE mentioned in Section
3.2.1. Only 29 entities exist in CilinE (coverage rate
is only 1.54%). That is why we try to automatically
extract hypernym relations.
5.2 Evaluation of the Ranking
5.2.1 Overall Performance Comparison
In this section, we compare our proposed methods
with other methods. Table 5 lists the performance
measured by precision at rank 1 and R-precision of
some key methods. The precision-recall curves of
all the methods are shown in Figure 2. Table 7 lists
the maximum F-scores.
MPattern refers to the pattern-based method of
Hearst (1992). We craft Chinese Hearst-style
patterns (Table 6), in which E represents an entity
and H represents one of its hypernyms. Following
Pattern Translation
E?(??/??) H E is a (a kind of) H
E (!) H E(,) and other H
H (?)(?) E H(,) called E
H (?)(?)X E H(,) such as E
H (?)AO? E H(,) especially E
Table 6: Chinese Hearst-style lexical patterns
Evans (2004), we combine each pattern and each en-
tity and submit them into the Baidu search engine.
For example, for an entity E, we search ?E ??
? (E is a)?, ?E  (E and other)?, and so on. We
select top 100 search results of each query and get
1,285,209 results in all for the entities in the test set.
Then we use the patterns to extract hypernyms from
the search results. The result shows that 508 cor-
rect hypernyms are extracted for 568 entities (1,529
entities in total). Only a small part of the entities
can be extracted hypernyms for. This is mainly be-
cause only a few hypernym relations are expressed
in these fixed patterns in the web, and many ones are
expressed in more flexible manners. The hypernyms
are ranked based on the count of evidences where
the hypernyms are extracted.
MSnow is the method originally proposed by S-
now et al (2005) for English but we adapt it for Chi-
nese. We consider the top 100 search results for each
known hypernym-hyponym pairs as a corpus to ex-
tract lexico-syntactic patterns. Then, an LR classi-
fier is built based on this patterns to recognize hy-
pernym relations. This method considers all noun-
s co-occurred with the focused entity in the same
sentences as candidate hypernyms. So the number
of candidates is huge, which causes inefficiency. In
1230
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Precision?Recall Curves on the Test Set
Recall
Prec
ision
l l l l llll l llll ll l l l l ll l l ll ll lll ll llllll
l
MSnowMPriorMSVM?l inearMSVM?rbfMLRMPatternMHeurist ic
Figure 2: Precision-Recall curves on the test set
our corpus, there are 652,181 candidates for 1,529
entities (426.54 for each entity on average), most of
which are not hypernyms. One possible reason is
that this method relies on an accurate syntactic pars-
er and it is difficult to guarantee the quality of the
automatically extracted patterns. Even worse, the
low quality of the language in the search results may
make this problem more serious.
MPrior refers to the ranking method based on on-
ly the prior of a candidate being a hypernym. As
Table 5 shows, it outperforms MSnow and achieves
comparable results with MPattern on Precision@1
and R-Precision.
Based on the features proposed in Section 3.2.1,
we train several statistical models based on SVM
and LR on the training data. MSVM?linear and
MSVM?rbf refer to the SVM models based on linear
kernels and RBF kernels respectively. MLR refers
to the LR model. The probabilities6 output by the
models are used to rank the candidate hypernyms.
All of the parameters which need to be set in the
models are selected on the development set. Table
5 shows the best models based on each algorithm.
These supervised models outperform the previous
methods. MLR achieves the best performance.
The precision-recall plot of the methods on the
test set is presented in Figure 2. MHeuristic refers
to the heuristic approach, proposed in Section 3.2.2,
to collect training data. Because this method cannot
6The output of an SVM is the distance from the decision
hyper-plane. Sigmoid functions can be used to convert this un-
calibrated distance into a calibrated posterior probability (Platt,
1999).
Method Max. F-score
MPattern 0.2061
MSnow 0.1514
MHeuristic 0.2803
MPrior 0.5591
MSVM?linear 0.5868
MSVM?rbf 0.6014
MLR 0.5998
Table 7: Summary of maximum F-score on the test set
Feature P@1 R-Prec
Max.
F-score
All 0.7632 0.6621 0.5998
? Prior 0.7534 0.6546 0.5837
? Is Tag 0.6965 0.6039 0.5605
? Is Head 0.7018 0.6036 0.5694
? In Titles 0.7436 0.6513 0.5868
? Synonyms 0.7495 0.6493 0.5831
? Radicals 0.7593 0.6584 0.5890
? Source Num 0.7364 0.6556 0.5984
? Lexicon 0.7377 0.6422 0.5851
? Source Info 0.6128 0.5221 0.5459
Table 8: Performance of LR models with different fea-
tures on the test set
provide ranking information, it is not listed in Ta-
ble 5. For fair comparison of R-precision and recall,
we add the extra correct hypernyms from MPattern
and MSnow to the test data set. The models based
on SVM and LR still perform better than the other
methods. MPattern and MSnow suffer from low re-
call and precision. MHeuristic get a high precision
but a low recall, because it can only deal with a part
of entities appearing in encyclopedias. The preci-
sion of MHeuristic reflects the quality of our training
data. We summarize the maximum F-score of dif-
ferent methods in Table 7.
5.2.2 Feature Effect
Table 8 shows the impact of each feature on the
performance of LR models. When we remove any
one of the features, the performance is degraded
more or less. The most effective features are Is Tag
and Is Head. The last line in Table 8 shows the
performance when we remove all features about
the source information, i.e., Is Tag, Is Head, and
1231
Entity
Top-1
Hypernym
Entity
Top-1
Hypernym
??b??(cefoperazone sodium) ??(drug) ?y(bullet tuna) ~a(fish)
???(finger citron rolls) ?(snack) =?(zirconite) ??(ore)
E????(The Avengers) >K(movie) ?|?d?(Felixstowe) l?(port)
@U=(mastigium) ?O(datum) ?!?(coxal cavity) ??(plant)
?UX?=?s
(Ethanolamine phosphotransferase)
)?
(organism)
?u
(coma)
?
(knowledge)
Table 10: Examples of entity-hypernym pairs extracted by MLR
Domain P@1 R-Prec
Max.
F-score
Biology 0.8165 0.7203 0.6424
Health Care 0.7354 0.5962 0.6061
Food 0.7450 0.6634 0.6938
Movie 0.9310 0.8069 0.7031
Industry 0.6286 0.5841 0.4624
Others 0.6324 0.4936 0.4318
Table 9: Performance of MLR in various domains
Source Num. The performance is degraded sharply.
This indicates the importance of the source informa-
tion for hypernym ranking.
5.2.3 The Performance in Each Domain
In this section, we evaluate the performance of
MLR method in various domains. We can see from
Table 9 that the performance in movie domain is best
while the performance in industry domain is worst.
That is because the information about movies is
abundant on the web. Furthermore, most of movies
have encyclopedia pages. It is easy to get the hy-
pernyms. In contrast, the entities in industry domain
are more uncommon. On the whole, our method is
robust for different domains. In Table 10, some in-
stances in various domains are presented.
5.3 Error Analysis
The uncovered entities7 and the false positives8 are
analyzed after the experiments. Some error exam-
ples are shown in Table 10 (in red font).
7Uncovered entities are entities which we do not collect any
correct hypernyms for in the first step.
8False positives are hypernyms ranked at the first places, but
actually are not correct hypernyms.
Uncovered entities: About 34% of the errors are
caused by uncovered entities. It is found that many
of the uncovered entities are rare entities. Nearly
36% of them are very rare and have only less than
100 search results in all. When we can?t get enough
information of an unknown entity from the search
engine, it?s difficult to know its semantic meaning,
such as ?@U= (mastigium)?, ??!? (coxal cav-
ity)?, ??u (coma)?. The identification of their hy-
pernyms requires more human-crafted knowledge.
The ranking models we used are unable to select
them, as the true synonyms are often below rank 10.
False positives: The remained 66% errors are
false positives. They are mainly owing to the
fact that some other related words in the candi-
date lists are more likely hypernyms. For exam-
ple, ?)? (organism)? is wrongly recognized as
the most probable hypernym of ??UX?=
?s (Ethanolamine phosphotransferase)?, because
the entity often co-occurs with word ?)? (organ-
ism)? and the latter is often used as a hypernym of
some other entities. The correct hypernyms actu-
ally are ?s (enzyme)?, ?z??? (chemical sub-
stance)?, and so on.
6 Conclusion
This paper proposes a novel method for finding
hypernyms of Chinese open-domain entities from
multiple sources. We collect candidate hypernyms
with wide coverage from search results, encyclope-
dia category tags and the head word of the entity.
Then, we propose a set of features to build statisti-
cal models to rank the candidate hypernyms on the
training data collected automatically. In our exper-
iments, we show that our method outperforms the
state-of-the-art methods and achieves the best preci-
1232
sion of 76.32% on a manually labeled test dataset.
All of the features which we propose are effective,
especially the features of source information. More-
over, our method works well in various domains, e-
specially in the movie and biology domains. We al-
so conduct detailed analysis to give more insights
on the error distribution. Except some language de-
pendent features, our approach can be easily trans-
fered from Chinese to other languages. For future
work, we would like to explore knowledge from
more sources to enhance our model, such as seman-
tic thesauri and infoboxes in encyclopedias.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61073126 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Zhenghua Li,
Wanxiang Che, Wei Song, Yanyan Zhao, Yuhang
Guo and the anonymous reviewers for insightful
comments and suggestions. Thanks are also due to
our annotators Ni Han and Zhenghua Li.
References
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 120?126,
College Park, Maryland, USA, June.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, China,
August.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 168?
175.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
Oren Etzioni, Michele Banko, and Michael J Cafarella.
2006. Machine reading. In AAAI, volume 6, pages
1517?1519.
Richard Evans. 2004. A framework for named enti-
ty recognition in the open domain. Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2012. Yago2: a spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, pages 1?63.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun
phrase left behind: Detecting and typing unlinkable
entities. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 893?903, Jeju Island, Korea, July.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the Third
International Joint Conference on Natural Language
Processing, pages 799?804.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
John Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in large margin classifiers,
10(3):61?74.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium
on Learning by Reading and Learning to Read, pages
88?93.
Cederberg Scott and Widdows Dominic. 2003. Using lsa
and noun coordination information to improve the pre-
cision and recall of automatic hyponymy extraction. In
Proceedings of the seventh conference on Natural lan-
guage learning at HLT-NAACL 2003-Volume 4, pages
111?118.
Sidney Siegel and N John Castellan Jr. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1297?1304.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Science, Ser-
vices and Agents on the World Wide Web, 6(3):203?
217.
1233
Peter Turney, Michael L Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy
problems. In Proceedings of the International Con-
ference RANLP-2003, pages 482?489.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and Chin-
Yew Lin. 2011. Nonlinear evidence fusion and prop-
agation for hyponymy relation mining. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1159?1168, Portland, Oregon, USA,
June.
1234
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110?120,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Revisiting Embedding Features for Simple Semi-supervised Learning
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in us-
ing continuous word embeddings learned
from unlabeled data as features to improve
supervised NLP systems, which is re-
garded as a simple semi-supervised learn-
ing mechanism. However, fundamen-
tal problems on effectively incorporating
the word embedding features within the
framework of linear models remain. In
this study, we investigate and analyze three
different approaches, including a new pro-
posed distributional prototype approach,
for utilizing the embedding features. The
presented approaches can be integrated
into most of the classical linear models in
NLP. Experiments on the task of named
entity recognition show that each of the
proposed approaches can better utilize the
word embedding features, among which
the distributional prototype approach per-
forms the best. Moreover, the combination
of the approaches provides additive im-
provements, outperforming the dense and
continuous embedding features by nearly
2 points of F1 score.
1 Introduction
Learning generalized representation of words is
an effective way of handling data sparsity caused
by high-dimensional lexical features in NLP sys-
tems, such as named entity recognition (NER)
and dependency parsing. As a typical low-
dimensional and generalized word representa-
tion, Brown clustering of words has been stud-
ied for a long time. For example, Liang (2005)
and Koo et al. (2008) used the Brown cluster
features for semi-supervised learning of various
NLP tasks and achieved significant improvements.
?
Email correspondence.
Recent research has focused on a special fam-
ily of word representations, named ?word embed-
dings?. Word embeddings are conventionally de-
fined as dense, continuous, and low-dimensional
vector representations of words. Word embed-
dings can be learned from large-scale unlabeled
texts through context-predicting models (e.g., neu-
ral network language models) or spectral methods
(e.g., canonical correlation analysis) in an unsu-
pervised setting.
Compared with the so-called one-hot represen-
tation where each word is represented as a sparse
vector of the same size of the vocabulary and only
one dimension is on, word embedding preserves
rich linguistic regularities of words with each di-
mension hopefully representing a latent feature.
Similar words are expected to be distributed close
to one another in the embedding space. Conse-
quently, word embeddings can be beneficial for
a variety of NLP applications in different ways,
among which the most simple and general way is
to be fed as features to enhance existing supervised
NLP systems.
Previous work has demonstrated effectiveness
of the continuous word embedding features in sev-
eral tasks such as chunking and NER using gener-
alized linear models (Turian et al., 2010).
1
How-
ever, there still remain two fundamental problems
that should be addressed:
? Are the continuous embedding features fit for
the generalized linear models that are most
widely adopted in NLP?
? How can the generalized linear models better
utilize the embedding features?
According to the results provided by Turian et
1
Generalized linear models refer to the models that de-
scribe the data as a combination of linear basis functions,
either directly in the input variables space or through some
transformation of the probability distributions (e.g., log-
linear models).
110
al. (2010), the embedding features brought signif-
icantly less improvement than Brown clustering
features. This result is actually not reasonable be-
cause the expressing power of word embeddings
is theoretically stronger than clustering-based rep-
resentations which can be regarded as a kind of
one-hot representation but over a low-dimensional
vocabulary (Bengio et al., 2013).
Wang and Manning (2013) showed that linear
architectures perform better in high-dimensional
discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-
tive in low-dimensional and continuous feature
space. Hence, the previous method that directly
uses the continuous word embeddings as features
in linear models (CRF) is inappropriate. Word
embeddings may be better utilized in the linear
modeling framework by smartly transforming the
embeddings to some relatively higher dimensional
and discrete representations.
Driven by this motivation, we present three
different approaches: binarization (Section 3.2),
clustering (Section 3.3) and a new proposed distri-
butional prototype method (Section 3.4) for better
incorporating the embeddings features. In the bi-
narization approach, we directly binarize the con-
tinuous word embeddings by dimension. In the
clustering approach, we cluster words based on
their embeddings and use the resulting word clus-
ter features instead. In the distributional prototype
approach, we derive task-specific features from
word embeddings by utilizing a set of automati-
cally extracted prototypes for each target label.
We carefully compare and analyze these ap-
proaches in the task of NER. Experimental results
are promising. With each of the three approaches,
we achieve higher performance than directly using
the continuous embedding features, among which
the distributional prototype approach performs the
best. Furthermore, by putting the most effective
two of these features together, we finally outper-
form the continuous embedding features by nearly
2 points of F1 Score (86.21% vs. 88.11%).
The major contribution of this paper is twofold.
(1) We investigate various approaches that can bet-
ter utilize word embeddings for semi-supervised
learning. (2) We propose a novel distributional
prototype approach that shows the great potential
of word embedding features. All the presented ap-
proaches can be easily integrated into most of the
classical linear NLP models.
2 Semi-supervised Learning with Word
Embeddings
Statistical modeling has achieved great success
in most NLP tasks. However, there still remain
some major unsolved problems and challenges,
among which the most widely concerned is the
data sparsity problem. Data sparsity in NLP is
mainly caused by two factors, namely, the lack
of labeled training data and the Zipf distribution
of words. On the one hand, large-scale labeled
training data are typically difficult to obtain, espe-
cially for structure prediction tasks, such as syn-
tactic parsing. Therefore, the supervised mod-
els can only see limited examples and thus make
biased estimation. On the other hand, the nat-
ural language words are Zipf distributed, which
means that most of the words appear a few times
or are completely absent in our texts. For these
low-frequency words, the corresponding parame-
ters usually cannot be fully trained.
More foundationally, the reason for the above
factors lies in the high-dimensional and sparse lex-
ical feature representation, which completely ig-
nores the similarity between features, especially
word features. To overcome this weakness, an ef-
fective way is to learn more generalized represen-
tations of words by exploiting the numerous un-
labeled data, in a semi-supervised manner. After
which, the generalized word representations can
be used as extra features to facilitate the super-
vised systems.
Liang (2005) learned Brown clusters of
words (Brown et al., 1992) from unlabeled data
and use them as features to promote the supervised
NER and Chinese word segmentation. Brown
clusters of words can be seen as a generalized
word representation distributed in a discrete and
low-dimensional vocabulary space. Contextually
similar words are grouped in the same cluster. The
Brown clustering of words was also adopted in de-
pendency parsing (Koo et al., 2008) and POS tag-
ging for online conversational text (Owoputi et al.,
2013), demonstrating significant improvements.
Recently, another kind of word representation
named ?word embeddings? has been widely stud-
ied (Bengio et al., 2003; Mnih and Hinton, 2008).
Using word embeddings, we can evaluate the sim-
ilarity of two words straightforward by comput-
ing the dot-product of two numerical vectors in the
Hilbert space. Two similar words are expected to
111
be distributed close to each other.
2
Word embeddings can be useful as input to an
NLP model (mostly non-linear) or as additional
features to enhance existing systems. Collobert
et al. (2011) used word embeddings as input to a
deep neural network for multi-task learning. De-
spite of the effectiveness, such non-linear models
are hard to build and optimize. Besides, these ar-
chitectures are often specialized for a certain task
and not scalable to general tasks. A simple and
more general way is to feed word embeddings as
augmented features to an existing supervised sys-
tem, which is similar to the semi-supervised learn-
ing with Brown clusters.
As discussed in Section 1, Turian et al. (2010)
is the pioneering work on using word embedding
features for semi-supervised learning. However,
their approach cannot fully exploit the potential
of word embeddings. We revisit this problem
in this study and investigate three different ap-
proaches for better utilizing word embeddings in
semi-supervised learning.
3 Approaches for Utilizing Embedding
Features
3.1 Word Embedding Training
In this paper, we will consider a context-
predicting model, more specifically, the Skip-gram
model (Mikolov et al., 2013a; Mikolov et al.,
2013b) for learning word embeddings, since it is
much more efficient as well as memory-saving
than other approaches.
Let?s denote the embedding matrix to be learned
by C
d?N
, where N is the vocabulary size and d is
the dimension of word embeddings. Each column
of C represents the embedding of a word. The
Skip-gram model takes the current word w as in-
put, and predicts the probability distribution of its
context words within a fixed window size. Con-
cretely, w is first mapped to its embedding v
w
by
selecting the corresponding column vector of C
(or multiplying C with the one-hot vector of w).
The probability of its context word c is then com-
puted using a log-linear function:
P (c|w; ?) =
exp(v
>
c
v
w
)
?
c
?
?V
exp(v
c
?
>
v
w
)
(1)
where V is the vocabulary. The parameters ? are
v
w
i
, v
c
i
for w, c ? V and i = 1, ..., d. Then, the
2
The term similar should be viewed depending on the spe-
cific task.
log-likelihood over the entire training dataset D
can be computed as:
J(?) =
?
(w,c)?D
log p(c|w; ?) (2)
The model can be trained by maximizing J(?).
Here, we suppose that the word embeddings
have already been trained from large-scale unla-
beled texts. We will introduce various approaches
for utilizing the word embeddings as features for
semi-supervised learning. The main idea, as in-
troduced in Section 1, is to transform the continu-
ous word embeddings to some relatively higher di-
mensional and discrete representations. The direct
use of continuous embeddings as features (Turian
et al., 2010) will serve as our baseline setting.
3.2 Binarization of Embeddings
One fairly natural approach for converting the
continuous-valued word embeddings to discrete
values is binarization by dimension.
Formally, we aim to convert the continuous-
valued embedding matrixC
d?N
, to another matrix
M
d?N
which is discrete-valued. There are various
conversion functions. Here, we consider a sim-
ple one. For the i
th
dimension of the word em-
beddings, we divide the corresponding row vector
C
i
into two halves for positive (C
i+
) and nega-
tive (C
i?
), respectively. The conversion function
is then defined as follows:
M
ij
= ?(C
ij
) =
?
?
?
?
?
U
+
, if C
ij
? mean(C
i+
)
B
?
, if C
ij
? mean(C
i?
)
0, otherwise
where mean(v) is the mean value of vector v, U
+
is a string feature which turns on when the value
(C
ij
) falls into the upper part of the positive list.
Similarly, B
?
refers to the bottom part of the neg-
ative list. The insight behind ? is that we only con-
sider the features with strong opinions (i.e., posi-
tive or negative) on each dimension and omit the
values close to zero.
3.3 Clustering of Embeddings
Yu et al. (2013) introduced clustering embeddings
to overcome the disadvantage that word embed-
dings are not suitable for linear models. They sug-
gested that the high-dimensional cluster features
make samples from different classes better sepa-
rated by linear models.
112
In this study, we again investigate this ap-
proach. Concretely, each word is treated as a sin-
gle sample. The batch k-means clustering algo-
rithm (Sculley, 2010) is used,
3
and each cluster
is represented as the mean of the embeddings of
words assigned to it. Similarities between words
and clusters are measured by Euclidean distance.
Moreover, different number of clusters n con-
tain information of different granularities. There-
fore, we combine the cluster features of different
ns to better utilize the embeddings.
3.4 Distributional Prototype Features
We propose a novel kind of embedding features,
named distributional prototype features for su-
pervised models. This is mainly inspired by
prototype-driven learning (Haghighi and Klein,
2006) which was originally introduced as a pri-
marily unsupervised approach for sequence mod-
eling. In prototype-driven learning, a few pro-
totypical examples are specified for each target
label, which can be treated as an injection of
prior knowledge. This sparse prototype informa-
tion is then propagated across an unlabeled corpus
through distributional similarities.
The basic motivation of the distributional pro-
totype features is that similar words are supposed
to be tagged with the same label. This hypothesis
makes great sense in tasks such as NER and POS
tagging. For example, suppose Michael is a pro-
totype of the named entity (NE) type PER. Using
the distributional similarity, we could link similar
words to the same prototypes, so the word David
can be linked to Michael because the two words
have high similarity (exceeds a threshold). Using
this link feature, the model will push David closer
to PER.
To derive the distributional prototype features,
first, we need to construct a few canonical exam-
ples (prototypes) for each target annotation label.
We use the normalized pointwise mutual informa-
tion (NPMI) (Bouma, 2009) between the label and
word, which is a smoothing version of the standard
PMI, to decide the prototypes of each label. Given
the annotated training corpus, the NPMI between
a label and word is computed as follows:
?
n
(label, word) =
?(label, word)
? ln p(label, word)
(3)
3
code.google.com/p/sofia-ml
NE Type Prototypes
B-PER Mark, Michael, David, Paul
I-PER Akram, Ahmed, Khan, Younis
B-ORG Reuters, U.N., Ajax, PSV
I-ORG Newsroom, Inc, Corp, Party
B-LOC U.S., Germany, Britain, Australia
I-LOC States, Republic, Africa, Lanka
B-MISC Russian, German, French, British
I-MISC Cup, Open, League, OPEN
O ., ,, the, to
Table 1: Prototypes extracted from the CoNLL-
2003 NER training data using NPMI.
where,
?(label, word) = ln
p(label, word)
p(label)p(word)
(4)
is the standard PMI.
For each target label l (e.g., PER, ORG, LOC),
we compute the NPMI of l and all words in the
vocabulary, and the top m words are chosen as the
prototypes of l. We should note that the proto-
types are extracted fully automatically, without in-
troducing additional human prior knowledge.
Table 1 shows the top four prototypes extracted
from the NER training corpus of CoNLL-2003
shared task (Tjong Kim Sang and De Meul-
der, 2003), which contains four NE types, namely,
PER, ORG, LOC, and MISC. Non-NEs are denoted
by O. We convert the original annotation to the
standard BIO-style. Thus, the final corpus con-
tains nine labels in total.
Next, we introduce the prototypes as features to
our supervised model. We denote the set of pro-
totypes for all target labels by S
p
. For each proto-
type z ? S
p
, we add a predicate proto = z, which
becomes active at each w if the distributional sim-
ilarity between z and w (DistSim(z, w)) is above
some threshold. DistSim(z, w) can be efficiently
calculated through the cosine similarity of the em-
beddings of z and w. Figure 1 gives an illustra-
tion of the distributional prototype features. Un-
like previous embedding features or Brown clus-
ters, the distributional prototype features are task-
specific because the prototypes of each label are
extracted from the training data.
Moreover, each prototype word is also its own
prototype (since a word has maximum similarity
to itself). Thus, if the prototype is closely related
to a label, all the words that are distributionally
113
i -1x ix1?iy iyO B-LOC
in
/IN
Hague
/NNP
O B-LOC1( , )? ? ?i if y y
( , )
 word = Hague
    pos = NNP
 proto = Britain  B-LOC
 proto = England 
 ...
?
? ?
? ?
? ?? ?
?? ?
? ?
? ?
? ?? ?
i if x y
Figure 1: An example of distributional prototype
features for NER.
similar to that prototype are pushed towards that
label.
4 Supervised Evaluation Task
Various tasks can be considered to compare and
analyze the effectiveness of the above three ap-
proaches. In this study, we partly follow Turian
et al. (2010) and Yu et al. (2013), and take NER as
the supervised evaluation task.
NER identifies and classifies the named entities
such as the names of persons, locations, and orga-
nizations in text. The state-of-the-art systems typ-
ically treat NER as a sequence labeling problem,
where each word is tagged either as a BIO-style
NE or a non-NE category.
Here, we use the linear chain CRF model, which
is most widely used for sequence modeling in the
field of NLP. The CoNLL-2003 shared task dataset
from the Reuters, which was used by Turian et
al. (2010) and Yu et al. (2013), was chosen as
our evaluation dataset. The training set contains
14,987 sentences, the development set contains
3,466 sentences and is used for parameter tuning,
and the test set contains 3,684 sentences.
The baseline features are shown in Table 2.
4.1 Embedding Feature Templates
In this section, we introduce the embedding fea-
tures to the baseline NER system, turning the su-
pervised approach into a semi-supervised one.
Dense embedding features. The dense con-
tinuous embedding features can be fed directly to
the CRF model. These embedding features can
be seen as heterogeneous features from the exist-
ing baseline features, which are discrete. There is
no effective way for dense embedding features to
be combined internally or with other discrete fea-
tures. So we only use the unigram embedding fea-
tures following Turian et al. (2010). Concretely,
the embedding feature template is:
Baseline NER Feature Templates
00: w
i+k
,?2 ? k ? 2
01: w
i+k
? w
i+k+1
,?2 ? k ? 1
02: t
i+k
,?2 ? k ? 2
03: t
i+k
? t
i+k+1
,?2 ? k ? 1
04: chk
i+k
,?2 ? k ? 2
05: chk
i+k
? chk
i+k+1
,?2 ? k ? 1
06: Prefix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
07: Suffix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
08: Type(w
i+k
),?2 ? k ? 2
Unigram Features
y
i
? 00? 08
Bigram Features
y
i?1
? y
i
Table 2: Features used in the NER system. t is
the POS tag. chk is the chunking tag. Prefix
and Suffix are the first and last l characters of a
word. Type indicates if the word is all-capitalized,
is-capitalized, all-digits, etc.
? de
i+k
[d], ?2 ? k ? 2, d ranges over the
dimensions of the dense word embedding de.
Binarized embedding features. The binarized
embedding feature template is similar to the dense
one. The only difference is that the feature val-
ues are discrete and we omit dimensions with zero
value. Therefore, the feature template becomes:
? bi
i+k
[d], ?2 ? k ? 2, where bi
i+k
[d] 6= 0,
d ranges over the dimensions of the binarized
vector bi of word embedding.
In this way, the dimension of the binarized em-
bedding feature space becomes 2 ? d compared
with the originally d of the dense embeddings.
Compound cluster features. The advantage of
the cluster features is that they can be combined
internally or with other features to form compound
features, which can be more discriminative. Fur-
thermore, the number of resulting clusters n can
be tuned, and different ns indicate different granu-
larities. Concretely, the compound cluster feature
template for each specific n is:
? c
i+k
, ?2 ? k ? 2.
? c
i+k
? c
i+k+1
,?2 ? k ? 1.
? c
i?1
? c
i+1
.
Distributional prototype features. The set of
prototypes is again denoted by S
p
, which is de-
114
cided by selecting the topm (NPMI) words as pro-
totypes of each label, where m is tuned on the de-
velopment set. For each word w
i
in a sequence,
we compute the distributional similarity between
w
i
and each prototype in S
p
and select the proto-
types zs that DistSim(z, w) ? ?. We set ? = 0.5
without manual tuning. The distributional proto-
type feature template is then:
? {proto
i+k
=z | DistSim(w
i+k
, z) ? ? & z ?
S
p
}, ?2 ? k ? 2 .
We only use the unigram features, since the
number of active distributional prototype features
varies for different words (positions). Hence,
these features cannot be combined effectively.
4.2 Brown Clustering
Brown clustering has achieved great success in
various NLP applications. At most time, it
provides a strong baseline that is difficult to
beat (Turian et al., 2010). Consequently, in our
study, we conduct comparisons among the embed-
ding features and the Brown clustering features,
along with further investigations of their combina-
tion.
The Brown algorithm is a hierarchical cluster-
ing algorithm which optimizes a class-based bi-
gram language model defined on the word clus-
ters (Brown et al., 1992). The output of the Brown
algorithm is a binary tree, where each word is
uniquely identified by its path from the root. Thus
each word can be represented as a bit-string with
a specific length.
Following the setting of Owoputi et al. (2013),
we will use the prefix features of hierarchical clus-
ters to take advantage of the word similarity in dif-
ferent granularities. Concretely, the Brown cluster
feature template is:
? bc
i+k
, ?2 ? k ? 2.
? prefix (bc
i+k
, p), p ? {2,4,6,...,16}, ?2 ?
k ? 2. prefix takes the p-length prefix of
the Brown cluster coding bc
i+k
.
5 Experiments
5.1 Experimental Setting
We take the English Wikipedia until August 2012
as our unlabeled data to train the word embed-
dings.
4
Little pre-processing is conducted for the
4
download.wikimedia.org.
training of word embeddings. We remove para-
graphs that contain non-roman characters and all
MediaWiki markups. The resulting text is tok-
enized using the Stanford tokenizer,
5
and every
word is converted to lowercase. The final dataset
contains about 30 million sentences and 1.52 bil-
lion words. We use a dictionary that contains
212,779 most common words (frequency ? 80) in
the dataset. An efficient open-source implementa-
tion of the Skip-gram model is adopted.
6
We ap-
ply the negative sampling
7
method for optimiza-
tion, and the asynchronous stochastic gradient de-
scent algorithm (Asynchronous SGD) for parallel
weight updating. In this study, we set the dimen-
sion of the word embeddings to 50. Higher di-
mension is supposed to bring more improvements
in semi-supervised learning, but its comparison is
beyond the scope of this paper.
For the cluster features, we tune the number
of clusters n from 500 to 3000 on the develop-
ment set, and finally use the combination of n =
500, 1000, 1500, 2000, 3000, which achieves the
best results. For the distributional prototype fea-
tures, we use a fixed number of prototype words
(m) for each target label. m is tuned on the devel-
opment set and is finally set to 40.
We induce 1,000 brown clusters of words, the
setting in prior work (Koo et al., 2008; Turian et
al., 2010). The training data of brown clustering is
the same with that of training word embeddings.
5.2 Results
Table 3 shows the performances of NER on the
test dataset. Our baseline is slightly lower than
that of Turian et al. (2010), because they use
the BILOU encoding of NE types which outper-
forms BIO encoding (Ratinov and Roth, 2009).
8
Nonetheless, our conclusions hold. As we can see,
all of the three approaches we investigate in this
study achieve better performance than the direct
use of the dense continuous embedding features.
To our surprise, even the binarized embedding
features (BinarizedEmb) outperform the continu-
ous version (DenseEmb). This provides clear evi-
dence that directly using the dense continuous em-
beddings as features in CRF indeed cannot fully
5
nlp.stanford.edu/software/tokenizer.
shtml.
6
code.google.com/p/word2vec/.
7
More details are analyzed in (Goldberg and Levy, 2014).
8
We use BIO encoding here in order to compare with most
of the reported benchmarks.
115
Setting F1
Baseline 83.43
+DenseEmb? 86.21
+BinarizedEmb 86.75
+ClusterEmb 86.90
+DistPrototype 87.44
+BinarizedEmb+ClusterEmb 87.56
+BinarizedEmb+DistPrototype 87.46
+ClusterEmb+DistPrototype 88.11
+Brown 87.49
+Brown+ClusterEmb 88.17
+Brown+DistPrototype 88.04
+Brown+ClusterEmb+DistPrototype 88.58
Finkel et al. (2005) 86.86
Krishnan and Manning (2006) 87.24
Ando and Zhang (2005) 89.31
Collobert et al. (2011) 88.67
Table 3: The performance of semi-supervised
NER on the CoNLL-2003 test data, using vari-
ous embedding features. ? DenseEmb refers to the
method used by Turian et al. (2010), i.e., the direct
use of the dense and continuous embeddings.
exploit the potential of word embeddings. The
compound cluster features (ClusterEmb) also out-
perform the DenseEmb. The same result is also
shown in (Yu et al., 2013). Further, the distribu-
tional prototype features (DistPrototype) achieve
the best performance among the three approaches
(1.23% higher than DenseEmb).
We should note that the feature templates used
for BinarizedEmb and DistPrototype are merely
unigram features. However, for ClusterEmb, we
form more complex features by combining the
clusters of the context words. We also consider
different number of clusters n, to take advantage
of the different granularities. Consequently, the
dimension of the cluster features is much higher
than that of BinarizedEmb and DistPrototype.
We further combine the proposed features to see
if they are complementary to each other. As shown
in Table 3, the cluster and distributional prototype
features are the most complementary, whereas the
binarized embedding features seem to have large
overlap with the distributional prototype features.
By combining the cluster and distributional pro-
totype features, we further push the performance
to 88.11%, which is nearly two points higher than
the performance of the dense embedding features
(86.21%).
9
We also compare the proposed features with
the Brown cluster features. As shown in Table 3,
the distributional prototype features alone achieve
comparable performance with the Brown clusters.
When the cluster and distributional prototype fea-
tures are used together, we outperform the Brown
clusters. This result is inspiring because we show
that the embedding features indeed have stronger
expressing power than the Brown clusters, as de-
sired. Finally, by combining the Brown cluster
features and the proposed embedding features, the
performance can be improved further (88.58%).
The binarized embedding features are not included
in the final compound features because they are al-
most overlapped with the distributional prototype
features in performance.
We also summarize some of the reported
benchmarks that utilize unlabeled data (with no
gazetteers used), including the Stanford NER tag-
ger (Finkel et al. (2005) and Krishnan and Man-
ning (2006)) with distributional similarity fea-
tures. Ando and Zhang (2005) use unlabeled data
for constructing auxiliary problems that are ex-
pected to capture a good feature representation of
the target problem. Collobert et al. (2011) adjust
the feature embeddings according to the specific
task in a deep neural network architecture. We
can see that both Ando and Zhang (2005) and Col-
lobert et al. (2011) learn task-specific lexical fea-
tures, which is similar to the proposed distribu-
tional prototype method in our study. We suggest
this to be the main reason for the superiority of
these methods.
Another advantage of the proposed discrete fea-
tures over the dense continuous features is tag-
ging efficiency. Table 4 shows the running time
using different kinds of embedding features. We
achieve a significant reduction of the tagging time
per sentence when using the discrete features. This
is mainly due to the dense/sparse battle. Al-
though the dense embedding features are low-
dimensional, the feature vector for each word is
much denser than in the sparse and discrete feature
space. Therefore, we actually need much more
computation during decoding. Similar results can
be observed in the comparison of the DistProto-
type and ClusterEmb features, since the density of
the DistPrototype features is higher. It is possible
9
Statistical significant with p-value < 0.001 by two-tailed
t-test.
116
Setting Time (ms) / sent
Baseline 1.04
+DenseEmb 4.75
+BinarizedEmb 1.25
+ClusterEmb 1.16
+DistPrototype 2.31
Table 4: Running time of different features on a
Intel(R) Xeon(R) E5620 2.40GHz machine.
to accelerate the DistPrototype, by increasing the
threshold of DistSim(z, w). However, this is in-
deed an issue of trade-off between efficiency and
accuracy.
5.3 Analysis
In this section, we conduct analyses to show the
reasons for the improvements.
5.3.1 Rare words
As discussed by Turian et al. (2010), much of the
NER F1 is derived from decisions regarding rare
words. Therefore, in order to show that the three
proposed embedding features have stronger abil-
ity for handling rare words, we first conduct anal-
ysis for the tagging errors of words with differ-
ent frequency in the unlabeled data. We assign the
word frequencies to several buckets, and evaluate
the per-token errors that occurred in each bucket.
Results are shown in Figure 2. In most cases, all
three embedding features result in fewer errors on
rare words than the direct use of dense continuous
embedding features.
Interestingly, we find that for words that are
extremely rare (0?256), the binarized embedding
features incur significantly fewer errors than other
approaches. As we know, the embeddings for the
rare words are close to their initial value, because
they received few updates during training. Hence,
these words are not fully trained. In this case,
we would like to omit these features because their
embeddings are not even trustable. However, all
embedding features that we proposed except Bi-
narizedEmb are unable to handle this.
In order to see how much we have utilized
the embedding features in BinarizedEmb, we cal-
culate the sparsity of the binarized embedding
vectors, i.e., the ratio of zero values in each
vector (Section 3.2). As demonstrated in Fig-
ure 3, the sparsity-frequency curve has good prop-
erties: higher sparsity for very rare words and
very frequent words, while lower sparsity for mid-
frequent words. It indicates that for words that are
very rare or very frequent, BinarizedEmb just omit
most of the features. This is reasonable also for
the very frequent words, since they usually have
rich and diverse context distributions and their
embeddings cannot be well learned by our mod-
els (Huang et al., 2012).
l
l
l
l l
l
l
l
l
l
l
Frequency of word in unlabeled data
Spars
ity
256 1k 4k 16k 64k0
.50
0.55
0.60
0.65
0.70
Figure 3: Sparsity (with confidence interval) of the
binarized embedding vector w.r.t. word frequency
in the unlabeled data.
Figure 2(b) further supports our analysis. Bina-
rizedEmb also reduce much of the errors for the
highly frequent words (32k-64k).
As expected, the distributional prototype fea-
tures produce fewest errors in most cases. The
main reason is that the prototype features are task-
specific. The prototypes are extracted from the
training data and contained indicative information
of the target labels. By contrast, the other em-
bedding features are simply derived from general
word representations and are not specialized for
certain tasks, such as NER.
5.3.2 Linear Separability
Another reason for the superiority of the proposed
embedding features is that the high-dimensional
discrete features are more linear separable than
the low-dimensional continuous embeddings. To
verify the hypothesis, we further carry out experi-
ments to analyze the linear separability of the pro-
posed discrete embedding features against dense
continuous embeddings.
We formalize this problem as a binary classi-
fication task, to determine whether a word is an
NE or not (NE identification). The linear support
vector machine (SVM) is used to build the clas-
sifiers, using different embedding features respec-
117
0?256 256?512 512?1k 1k?2kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
0
50
100
150
200
250 DenseEmbBinarizedEmbClusterEmbDistPrototype
(a)
4k?8k 8k?16k 16k?32k 32k?64kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
40
60
80
100
120 DenseEmbBinarizedEmbClusterEmbDistPrototype
(b)
Figure 2: The number of per-token errors w.r.t. word frequency in the unlabeled data. (a) For rare words
(frequency ? 2k). (b) For frequent words (frequency ? 4k).
Setting Acc. #features
DenseEmb 95.46 250
BinarizedEmb 94.10 500
ClusterEmb 97.57 482,635
DistPrototype 96.09 1,700
DistPrototype-binary 96.82 4,530
Table 5: Performance of the NE/non-NE classi-
fication on the CoNLL-2003 development dataset
using different embedding features.
tively. We use the LIBLINEAR tool (Fan et al.,
2008) as our SVM implementation. The penalty
parameter C is tuned from 0.1 to 1.0 on the devel-
opment dataset. The results are shown in Table 5.
As we can see, NEs and non-NEs can be better
separated using ClusterEmb or DistPrototype fea-
tures. However, the BinarizedEmb features per-
form worse than the direct use of word embedding
features. The reason might be inferred from the
third column of Table 5. As demonstrated in Wang
and Manning (2013), linear models are more ef-
fective in high-dimensional and discrete feature
space. The dimension of the BinarizedEmb fea-
tures remains small (500), which is merely twice
the DenseEmb. By contrast, feature dimensions
are much higher for ClusterEmb and DistProto-
type, leading to better linear separability and thus
can be better utilized by linear models.
We notice that the DistPrototype features per-
form significantly worse than ClusterEmb in NE
identification. As described in Section 3.4, in
previous experiments, we automatically extracted
prototypes for each label, and propagated the in-
formation via distributional similarities. Intu-
itively, the prototypes we used should be more ef-
fective in determining fine-grained NE types than
identifying whether a word is an NE. To verify
this, we extract new prototypes considering only
two labels, namely, NE and non-NE, using the
same metric in Section 3.4. As shown in the last
row of Table 5, higher performance is achieved.
6 Related Studies
Semi-supervised learning with generalized word
representations is a simple and general way of im-
proving supervised NLP systems. One common
approach for inducing generalized word represen-
tations is to use clustering (e.g., Brown clustering)
(Miller et al., 2004; Liang, 2005; Koo et al., 2008;
Huang and Yates, 2009).
Aside from word clustering, word embeddings
have been widely studied. Bengio et al. (2003)
propose a feed-forward neural network based lan-
guage model (NNLM), which uses an embedding
layer to map each word to a dense continuous-
valued and low-dimensional vector (parameters),
and then use these vectors as the input to predict
the probability distribution of the next word. The
NNLM can be seen as a joint learning framework
for language modeling and word representations.
Alternative models for learning word embed-
dings are mostly inspired by the feed-forward
NNLM, including the Hierarchical Log-Bilinear
Model (Mnih and Hinton, 2008), the recurrent
neural network language model (Mikolov, 2012),
the C&W model (Collobert et al., 2011), the log-
linear models such as the CBOW and the Skip-
118
gram model (Mikolov et al., 2013a; Mikolov et
al., 2013b).
Aside from the NNLMs, word embeddings can
also be induced using spectral methods, such as
latent semantic analysis and canonical correlation
analysis (Dhillon et al., 2011). The spectral meth-
ods are generally faster but much more memory-
consuming than NNLMs.
There has been a plenty of work that exploits
word embeddings as features for semi-supervised
learning, most of which take the continuous fea-
tures directly in linear models (Turian et al., 2010;
Guo et al., 2014). Yu et al. (2013) propose com-
pound k-means cluster features based on word em-
beddings. They show that the high-dimensional
discrete cluster features can be better utilized by
linear models such as CRF. Wu et al. (2013) fur-
ther apply the cluster features to transition-based
dependency parsing.
7 Conclusion and Future Work
This paper revisits the problem of semi-supervised
learning with word embeddings. We present three
different approaches for a careful comparison and
analysis. Using any of the three embedding fea-
tures, we obtain higher performance than the di-
rect use of continuous embeddings, among which
the distributional prototype features perform the
best, showing the great potential of word embed-
dings. Moreover, the combination of the proposed
embedding features provides significant additive
improvements.
We give detailed analysis about the experimen-
tal results. Analysis on rare words and linear sep-
arability provides convincing explanations for the
performance of the embedding features.
For future work, we are exploring a novel and a
theoretically more sounding approach of introduc-
ing embedding kernel into the linear models.
Acknowledgments
We are grateful to Mo Yu for the fruitful discus-
sion on the implementation of the cluster-based
embedding features. We also thank Ruiji Fu,
Meishan Zhang, Sendong Zhao and the anony-
mous reviewers for their insightful comments and
suggestions. This work was supported by the
National Key Basic Research Program of China
via grant 2014CB340503 and the National Natu-
ral Science Foundation of China (NSFC) via grant
61133012 and 61370164.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 1?9. Association for Computational Lin-
guistics.
Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A neural probabilistic
language model. The Journal of Machine Learning
Research, 3(Feb):1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 35(8):1798?1828.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31?40.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L. E. On Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (almost)
from scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, volume 24 of NIPS, pages 199?
207.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.?s negative-sampling
word-embedding method. CoRR, abs/1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 497?507, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
119
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
320?327. Association for Computational Linguis-
tics.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
495?503.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 873?882,
Jeju Island, Korea. ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Kathleen McKeown, Johanna D. Moore, Si-
mone Teufel, James Allan, and Sadaoki Furui, edi-
tors, Proc. of ACL-08: HLT, Proc. of ACL-08: HLT,
pages 595?603, Columbus, Ohio. ACL.
Vijay Krishnan and Christopher D Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1121?1128. Association for Compu-
tational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master thesis, Massachusetts Institute
of Technology.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proc. of Workshop at
ICLR, Proc. of Workshop at ICLR, Arizona.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of the NIPS, Proc. of the NIPS, pages
3111?3119, Nevada. MIT Press.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph. d. thesis, Brno Uni-
versity of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
Proc. of the NIPS, Proc. of the NIPS, pages 1081?
1088, Vancouver. MIT Press.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, CoNLL ?09,
pages 147?155, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D Sculley. 2010. Combined regression and ranking.
In Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 979?988. ACM.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Jan Hajic, San-
dra Carberry, and Stephen Clark, editors, Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 384?394, Uppsala, Sweden. ACL.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proc. of the Sixth International Joint Con-
ference on Natural Language Processing, Proc. of
the Sixth International Joint Conference on Natural
Language Processing, pages 1285?1291, Nagoya,
Japan. Asian Federation of Natural Language Pro-
cessing.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dian-
hai Yu, Hua Wu, and Haifeng Wang. 2013. Gener-
alization of words for chinese dependency parsing.
IWPT-2013, page 73.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-
anhai Yu. 2013. Compound embedding features for
semi-supervised learning. In Proc. of the NAACL-
HLT, Proc. of the NAACL-HLT, pages 563?568, At-
lanta. NAACL.
120
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142?146,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improve Statistical Machine Translation with Context-Sensitive
Bilingual Semantic Embedding Model
Haiyang Wu
1
Daxiang Dong
1
Wei He
1
Xiaoguang Hu
1
Dianhai Yu
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,
wu hua,wanghaifeng@baidu.com
tliu@ir.hit.edu.cn
Abstract
We investigate how to improve bilingual
embedding which has been successfully
used as a feature in phrase-based sta-
tistical machine translation (SMT). De-
spite bilingual embedding?s success, the
contextual information, which is of criti-
cal importance to translation quality, was
ignored in previous work. To employ
the contextual information, we propose
a simple and memory-efficient model for
learning bilingual embedding, taking both
the source phrase and context around the
phrase into account. Bilingual translation
scores generated from our proposed bilin-
gual embedding model are used as features
in our SMT system. Experimental results
show that the proposed method achieves
significant improvements on large-scale
Chinese-English translation task.
1 Introduction
In Statistical Machine Translation (SMT) sys-
tem, it is difficult to determine the translation of
some phrases that have ambiguous meanings.For
example, the phrase??? jieguo? can be trans-
lated to either ?results?, ?eventually? or ?fruit?,
depending on the context around it. There are two
reasons for the problem: First, the length of phrase
pairs is restricted due to the limitation of model
size and training data. Another reason is that SMT
systems often fail to use contextual information
in source sentence, therefore, phrase sense disam-
biguation highly depends on the language model
which is trained only on target corpus.
To solve this problem, we present to learn
context-sensitive bilingual semantic embedding.
Our methodology is to train a supervised model
where labels are automatically generated from
phrase-pairs. For each source phrase, the aligned
target phrase is marked as the positive label
whereas other phrases in our phrase table are
treated as negative labels. Different from previ-
ous work in bilingual embedding learning(Zou et
al., 2013; Gao et al., 2014), our framework is a
supervised model that utilizes contextual informa-
tion in source sentence as features and make use
of phrase pairs as weak labels. Bilingual seman-
tic embeddings are trained automatically from our
supervised learning task.
Our learned bilingual semantic embedding
model is used to measure the similarity of phrase
pairs which is treated as a feature in decoding. We
integrate our learned model into a phrase-based
translation system and experimental results indi-
cate that our system significantly outperform the
baseline system. On the NIST08 Chinese-English
translation task, we obtained 0.68 BLEU improve-
ment. We also test our proposed method on much
larger web dataset and obtain 0.49 BLEU im-
provement against the baseline.
2 Related Work
Using vectors to represent word meanings is
the essence of vector space models (VSM). The
representations capture words? semantic and syn-
tactic information which can be used to measure
semantic similarities by computing distance be-
tween the vectors. Although most VSMs represent
one word with only one vector, they fail to cap-
ture homonymy and polysemy of word. Huang
et al. (2012) introduced global document context
and multiple word prototypes which distinguishes
and uses both local and global context via a joint
training objective. Much of the research focus
on the task of inducing representations for sin-
gle languages. Recently, a lot of progress has
142
been made at representation learning for bilin-
gual words. Bilingual word representations have
been presented by Peirsman and Pad?o (2010) and
Sumita (2000). Also unsupervised algorithms
such as LDA and LSA were used by Boyd-Graber
and Resnik (2010), Tam et al. (2007) and Zhao and
Xing (2006). Zou et al. (2013) learn bilingual em-
beddings utilizes word alignments and monolin-
gual embeddings result, Le et al. (2012) and Gao et
al. (2014) used continuous vector to represent the
source language or target language of each phrase,
and then computed translation probability using
vector distance. Vuli?c and Moens (2013) learned
bilingual vector spaces from non-parallel data in-
duced by using a seed lexicon. However, none
of these work considered the word sense disam-
biguation problem which Carpuat and Wu (2007)
proved it is useful for SMT. In this paper, we learn
bilingual semantic embeddings for source content
and target phrase, and incorporate it into a phrase-
based SMT system to improve translation quality.
3 Context-Sensitive Bilingual Semantic
Embedding Model
We propose a simple and memory-efficient
model which embeds both contextual information
of source phrases and aligned phrases in target cor-
pus into low dimension. Our assumption is that
high frequent words are likely to have multiple
word senses; therefore, top frequent words are se-
lected in source corpus. We denote our selected
words as focused phrase. Our goal is to learn a
bilingual embedding model that can capture dis-
criminative contextual information for each fo-
cused phrase. To learn an effective context sensi-
tive bilingual embedding, we extract context fea-
tures nearby a focused phrase that will discrimi-
nate focused phrase?s target translation from other
possible candidates. Our task can be viewed as
a classification problem that each target phrase is
treated as a class. Since target phrases are usu-
ally in very high dimensional space, traditional
linear classification model is not suitable for our
problem. Therefore, we treat our problem as a
ranking problem that can handle large number of
classes and optimize the objectives with scalable
optimizer stochastic gradient descent.
3.1 Bilingual Word Embedding
We apply a linear embedding model for bilin-
gual embedding learning. Cosine similarity be-
tween bilingual embedding representation is con-
sidered as score function. The score function
should be discriminative between target phrases
and other candidate phrases. Our score function
is in the form:
f(x,y; W,U) = cos(W
T
x,U
T
y) (1)
where x is contextual feature vector in source sen-
tence, and y is the representation of target phrase,
W ? R
|X|?k
,U ? R
|Y|?k
are low rank ma-
trix. In our model, we allow y to be bag-of-words
representation. Our embedding model is memory-
efficient in that dimensionality of x and y can be
very large in practical setting. We use |X| and |Y|
means dimensionality of random variable x and y,
then traditional linear model such as max-entropy
model requires memory space of O(|X||Y|). Our
embedding model only requires O(k(|X|+ |Y|))
memory space that can handle large scale vocabu-
lary setting. To score a focused phrase and target
phrase pair with f(x,y), context features are ex-
tracted from nearby window of the focused phrase.
Target words are selected from phrase pairs. Given
a source sentence, embedding of a focused phrase
is estimated from W
T
x and target phrase embed-
ding can be obtained through U
T
y.
3.2 Context Sensitive Features
Context of a focused phrase is extracted from
nearby window, and in our experiment we choose
window size of 6 as a focused phrase?s con-
text. Features are then extracted from the focused
phrase?s context. We demonstrate our feature
extraction and label generation process from the
Chinese-to-English example in figure 1. Window
size in this example is three. Position features
and Part-Of-Speech Tagging features are extracted
from the focused phrase?s context. The word fruit
Figure 1: Feature extraction and label generation
143
is the aligned phrase of our focused phrase and is
treated as positive label. The phrase results is a
randomly selected phrase from phrase table results
of ??. Note that feature window is not well de-
fined near the beginning or the end of a sentence.
To conquer this problem, we add special padding
word to the beginning and the end of a sentence to
augment sentence.
3.3 Parameter Learning
To learn model parameter W and U, we ap-
ply a ranking scheme on candidates selected from
phrase table results of each focused phrase. In par-
ticular, given a focus phrase w, aligned phrase is
treated as positive label whereas phrases extracted
from other candidates in phrase table are treated
as negative label. A max-margin loss is applied in
this ranking setting.
I(?) =
1
m
m
?
i=1
(? ? f(x
i
, y
i
; ?)? f(x
i
, y
?
i
; ?))+
(2)
Where f(x
i
,y
i
) is previously defined, ? =
{W,U} and + means max-margin hinge loss. In
our implementation, a margin of ? = 0.15 is used
during training. Objectives are minimized through
stochastic gradient descent algorithm. For each
randomly selected training example, parameters
are updated through the following form:
? := ?? ?
?l(?)
??
(3)
where ? = {W,U}. Given an instance with pos-
itive and negative label pair {x,y,y
?
}, gradients
of parameter W and U are as follows:
?l(W,U)
?W
= qsx(W
T
x)
T
? pqs
3
x(U
T
y) (4)
?l(W,U)
?U
= qsy(U
T
y)
T
? pqs
3
y(W
T
x) (5)
Where we set p = (W
T
x)
T
(U
T
y), q =
1
||W
T
x||
2
and s =
1
||U
T
y||
2
. To initialize our model param-
eters with strong semantic and syntactic informa-
tion, word vectors are pre-trained independently
on source and target corpus through word2vec
(Mikolov et al., 2013). And the pre-trained word
vectors are treated as initial parameters of our
model. The learned scoring function f(x,y) will
be used during decoding phase as a feature in log-
linear model which we will describe in detail later.
4 Integrating Bilingual Semantic
Embedding into Phrase-Based SMT
Architectures
To incorporate the context-sensitive bilingual
embedding model into the state-of-the-art Phrase-
Based Translation model, we modify the decoding
so that context information is available on every
source phrase. For every phrase in a source sen-
tence, the following tasks are done at every node
in our decoder:
? Get the focused phrase as well as its context in the
source sentence.
? Extract features from the focused phrase?s context.
? Get translation candidate extracted from phrase pairs of
the focused phrase.
? Compute scores for any pair of the focused phrase and
a candidate phrase.
We get the target sub-phrase using word align-
ment of phrase, and we treat NULL as a common
target word if there is no alignment for the focused
phrase. Finally we compute the matching score for
source content and target word using bilingual se-
mantic embedding model. If there are more than
one word in the focus phrase, then we add all score
together. A penalty value will be given if target is
not in translation candidate list. For each phrase in
a given SMT input sentence, the Bilingual Seman-
tic score can be used as an additional feature in
log-linear translation model, in combination with
other typical context-independent SMT bilexicon
probabilities.
5 Experiment
Our experiments are performed using an in-
house phrase-based system with a log-linear
framework. Our system includes a phrase trans-
lation model, an n-gram language model, a lexi-
calized reordering model, a word penalty model
and a phrase penalty model, which is similar to
Moses (Koehn et al., 2007). The evaluation metric
is BLEU (Papineni et al., 2002).
5.1 Data set
We test our approach on LDC corpus first. We
just use a subset of the data available for NIST
OpenMT08 task
1
. The parallel training corpus
1
LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )
144
Method
OpenMT08 WebData
BLEU BLEU
Our Baseline 26.24 29.32
LOC 26.78** 29.62*
LOC+POS 26.82** 29.81*
Table 1: Results of lowercase BLEU on NIST08
task. LOC is the location feature and POS is
the Part-of-Speech feature * or ** equals to sig-
nificantly better than our baseline(? < 0.05 or
? < 0.01, respectively)
contains 1.5M sentence pairs after we filter with
some simple heuristic rules, such as sentence be-
ing too long or containing messy codes. As mono-
lingual corpus, we use the XinHua portion of the
English GigaWord. In monolingual corpus we fil-
ter sentence if it contain more than 100 words
or contain messy codes, Finally, we get mono-
lingual corpus containing 369M words. In order
to test our approach on a more realistic scenario,
we train our models with web data. Sentence
pairs obtained from bilingual website and com-
parable webpage. Monolingual corpus is gained
from some large website such as WiKi. There are
50M sentence pairs and 10B words monolingual
corpus.
5.2 Results and Analysis
For word alignment, we align all of the train-
ing data with GIZA++ (Och and Ney, 2003), us-
ing the grow-diag-final heuristic to improve recall.
For language model, we train a 5-gram modified
Kneser-Ney language model and use Minimum
Error Rate Training (Och, 2003) to tune the SMT.
For both OpenMT08 task and WebData task, we
use NIST06 as the tuning set, and use NIST08 as
the testing set. Our baseline system is a standard
phrase-based SMT system, and a language model
is trained with the target side of bilingual corpus.
Results on Chinese-English translation task are re-
ported in Table 1. Word position features and part-
of-speech tagging features are both useful for our
bilingual semantic embedding learning. Based on
our trained bilingual embedding model, we can
easily compute a translation score between any
bilingual phrase pair. We list some cases in table
2 to show that our bilingual embedding is context
sensitive.
Contextual features extracted from source sen-
tence are strong enough to discriminate different
Source Sentence
4 Nearest Neighbor from
bilingual embedding
??????????
?????????
?????(Investors
can only get down to
business in a stable so-
cial environment)
will be, can only, will, can
??????????
?????????
?????(In compe-
titions, the Chinese Dis-
abled have shown ex-
traordinary athletic abil-
ities)
skills, ability, abilities, tal-
ent
??????????
?????????
????(In the natu-
ral environment of Costa
Rica, grapes do not nor-
mally yield fruit.)
fruit, outcome of, the out-
come, result
? ? ??????
???????(As
a result, Eastern District
Council passed a pro-
posal)
in the end, eventually, as a
result, results
Table 2: Top ranked focused phrases based on
bilingual semantic embedding
word senses. And we also observe from the word
??? jieguo? that Part-Of-Speech Tagging fea-
tures are effective in discriminating target phrases.
6 Conlusion
In this paper, we proposed a context-sensitive
bilingual semantic embedding model to improve
statistical machine translation. Contextual infor-
mation is used in our model for bilingual word
sense disambiguation. We integrated the bilingual
semantic model into the phrase-based SMT sys-
tem. Experimental results show that our method
achieves significant improvements over the base-
line on large scale Chinese-English translation
task. Our model is memory-efficient and practical
for industrial usage that training can be done on
large scale data set with large number of classes.
Prediction time is also negligible with regard to
SMT decoding phase. In the future, we will ex-
plore more features to refine the model and try to
utilize contextual information in target sentences.
Acknowledgments
We thank the three anonymous reviewers for
their valuable comments, and Niu Gang and Wu
Xianchao for discussions. This paper is supported
by 973 program No. 2014CB340505.
145
References
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proc. ACL.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 39?48, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. In Computational Linguistics, Volume 29,
Number 1, March 2003. Computational Linguistics,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 921?929, Los Ange-
les, California, June. Association for Computational
Linguistics.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-lsa based lm adaptation for spoken lan-
guage translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 520?527, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Ivan Vuli?c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 106?116, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
146
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147?152,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Transformation from Discontinuous to Continuous Word Alignment
Improves Translation Quality
Zhongjun He
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
{hezhongjun,wu hua,wanghaifeng}@baidu.com
tliu@ir.hit.edu.cn
Abstract
We present a novel approach to im-
prove word alignment for statistical ma-
chine translation (SMT). Conventional
word alignment methods allow discontin-
uous alignment, meaning that a source
(or target) word links to several target (or
source) words whose positions are dis-
continuous. However, we cannot extrac-
t phrase pairs from this kind of align-
ments as they break the alignment con-
sistency constraint. In this paper, we use
a weighted vote method to transform dis-
continuous word alignment to continuous
alignment, which enables SMT system-
s extract more phrase pairs. We carry
out experiments on large scale Chinese-
to-English and German-to-English trans-
lation tasks. Experimental results show
statistically significant improvements of
BLEU score in both cases over the base-
line systems. Our method produces a gain
of +1.68 BLEU on NIST OpenMT04 for
the phrase-based system, and a gain of
+1.28 BLEU on NIST OpenMT06 for the
hierarchical phrase-based system.
1 Introduction
Word alignment, indicating the correspondence
between the source and target words in bilingual
sentences, plays an important role in statistical
machine translation (SMT). Almost all of the SMT
models, not only phrase-based (Koehn et al.,
2003), but also syntax-based (Chiang, 2005; Liu
et al., 2006; Huang et al., 2006), derive translation
knowledge from large amount bilingual text anno-
tated with word alignment. Therefore, the quality
of the word alignment has big impact on the qual-
ity of translation output.
Word alignments are usually automatically ob-
tained from a large amount of bilingual training
corpus. The most widely used toolkit for word
alignment in SMT community is GIZA++ (Och
and Ney, 2004), which implements the well known
IBM models (Brown et al., 1993) and the HM-
M model (Vogel and Ney, 1996). Koehn et al.
(2003) proposed some heuristic methods (e.g. the
?grow-diag-final? method) to refine word align-
ments trained by GIZA++. Another group of word
alignment methods (Liu et al., 2005; Moore et
al., 2006; Riesa and Marcu, 2010) define feature
functions to describe word alignment. They need
manually aligned bilingual texts to train the mod-
el. However, the manually annotated data is too
expensive to be available for all languages. Al-
though these models reported high accuracy, the
GIZA++ and ?grow-diag-final? method are domi-
nant in practice.
However, automatic word alignments are usu-
ally very noisy. The example in Figure 1 shows
a Chinese and English sentence pair, with word
alignment automatically trained by GIZA++ and
the ?grow-diag-final? method. We find many er-
rors (dashed links) are caused by discontinuous
alignment (formal definition is described in Sec-
tion 2), a source (or target) word linking to sev-
eral discontinuous target (or source) words. This
kind of errors will result in the loss of many use-
ful phrase pairs that are learned based on bilingual
word alignment. Actually, according to the defini-
tion of phrases in a standard phrase-based model,
we cannot extract phrases from the discontinuous
alignment. The reason is that this kind of align-
ment break the alignment consistency constrain-
t (Koehn et al., 2003). For example, the Chi-
147
1{I
meiguo
2
?
shi
3
?
shaoshu
4
A?
jige
5
?
tou
6
e
xia
7
??
fandui
8
?
piao
9

de
10
I[
guojia
11
??
zhiyi
The
1
United
2
States
3
was
4
among
5
the
6
handful
7
of
8
nations
9
that
10
cast
11
a
12
nay
13
note
14
Figure 1: An example of word alignment between a Chinese and English sentence pair. The dashed links
are incorrect alignments.
nese word ?shi
2
?
1
is aligned to the English words
?was
4
? and ?that
10
?. However, these two English
words are discontinuous, and we cannot extract the
phrase pair ?(shi, was)?.
In this paper, we propose a simple weighed vote
method to deal with the discontinuous word align-
ment. Firstly, we split the discontinuous align-
ment into several continuous alignment group-
s, and consider each continuous alignment group
as a bucket. Secondly, we vote for each buck-
et with alignment score measured by word trans-
lation probabilities. Finally, we select the buck-
et with the highest score as the final alignment.
The strength of our method is that we refine word
alignment without using any external knowledge,
as the word translation probabilities can be esti-
mated from the bilingual corpus with the original
word alignment.
We notice that the discontinuous alignment is
helpful for hierarchical phrase-based model, as the
model allows discontinuous phrases. Thus, for
the hierarchical phrase-based model, our method
may lost some discontinuous phrases. To solve
the problem, we keep the original discontinuous
alignment in the training corpus.
We carry out experiment with the state-of-the-
art phrase-based and hierarchical phrase-based
(Chiang, 2005) SMT systems implemented in
Moses (Koehn et al., 2007). Experiments on large
scale Chinese-to-English and German-to-English
translation tasks demonstrate significant improve-
ments in both cases over the baseline systems.
2 The Weighted Vote Method
To refine the discontinuous alignment, we propose
a weighted vote method to transform discontinu-
ous alignment to continuous alignment by discard-
ing noisy links. We split discontinuous alignment
1
The subscript denotes the word position.
into several continuous groups, and select the best
group with the highest score computed by word
translation probabilities as the final alignment.
For further understanding, we first describe
some definitions. Given a word-aligned sentence
pair (F
I
1
, E
J
1
, A), an alignment set A
set
(i) is the
set of target word positions that aligned to the
source word F
i
i
:
A
set
(i) = {j|(i, j) ? A} (1)
For example, in Figure 1, the alignment set
for the Chinese word ?shaoshu
3
? is A
set
(3) =
{5, 7, 8, 10}. We define an alignment s-
pan A
span
(i) as [min(A
set
(i)),max(A
set
(i))].
Thus, the alignment span for the Chinese word
?shaoshu
3
? is A
span
(3) = [5, 10].
The alignment for F
i
i
is discontinuous if there
exist some target words in A
span
(i) linking to an-
other source word, i.e. ?(i
?
, j
?
) ? A, where i
?
6= i,
j
?
? A
span
(i). Otherwise, the alignment is contin-
uous. According to the definition, the alignment
for ?shaoshu
3
? is discontinuous. Because the tar-
get words ?the
6
? and ?nations
9
? in the alignmen-
t span link to another Chinese words ?de
9
? and
?guojia
10
?, respectively. For a target word E
j
j
, the
definition is similar.
If the alignment for F
i
i
is discontinuous, we
can split the alignment span A
span
(i) = [j
1
, j
2
]
into m continuous spans {[j
k
p
, j
k
q
]}, where k =
1, 2, ...,m, and j
k
p
, j
k
q
? [j
1
, j
2
]. Our goal is to se-
lect the best continuous span for the word F
i
i
. To
do this, we score each continuous span with word
translation probabilities:
S([j
k
p
, j
k
q
]) =
q
?
t=p
(Pr(E
j
k
t
|F
i
) + Pr(F
i
|E
j
k
t
))
(2)
where,
Pr(f |e) =
count(f, e)
?
f
?
count(f
?
, e)
(3)
148
am
o
n
g
t
h
e
h
a
n
d
f
u
l
o
f
n
a
t
i
o
n
s
t
h
a
t
?? shaoshu 0.1 0.5 0.2 0.1
Figure 2: An example of weighted voted method
for selecting the best continuous alignment from
the discontinuous alignment. The heavy shading
area is selected as the final alignment.
Pr(e|f) =
count(e, f)
?
e
?
count(f, e
?
)
(4)
The word translation probabilities can be comput-
ed from the bilingual corpus with the initial word
alignment. Finally, we select the span with the
highest score as the final alignment, and discard
all other alignments.
We illustrate our method in Figure 2, which
shows the source word ?shaoshu? and its align-
ment in Figure 1. We split the alignments into
three continuous alignment spans and compute s-
core for each span. Finally, the span with highest
score (heavy shading area) is selected as the final
alignment.
We conduct the procedure for each source and
target word, the improved alignment (solid links)
is shown in Figure 1.
3 Experiment
To demonstrate the effect of the proposed method,
we use the state-of-the-art phrase-based system
and hierarchical phrase-based system implement-
ed in Moses (Koehn et al., 2007). The phrase-
based system uses continuous phrase pair as the
main translation knowledge. While the hierarchi-
cal phrase-based system uses both continuous and
discontinuous phrase pairs, which has an ability to
capture long distance phrase reordering.
we carried out experiments on two translation
tasks: the Chinese-to-English task comes from the
NIST Open MT Evaluation, and the German-to-
English task comes from the Workshop on Ma-
chine Translation (WMT) shared task.
3.1 Training
The training data we used are listed in Table 1. For
the Chinese-English task, the bilingual data are s-
elected from LDC. We used NIST MT03 as the
development set and tested our system on NIST
MT evaluation sets from 2004 to 2008. For the
German-English task, the bilingual data are from
Task Src. Words Tgt. Words
Chinese-to-English 75M 78M
German-to- English 107M 113M
Table 1: Bilingual data for our experiments.
System N04 N05 N06 N08
Baseline 34.53 33.02 30.43 23.29
Refined 36.21 33.99 31.59 24.36
Table 2: Chinese-to-English translation quality of
the phrase-based system.
System W10 W11 W12 W13
Baseline 20.71 20.26 20.52 23.26
Refined 21.46 20.95 21.11 23.77
Table 3: German-to-English translation quality of
the phrase-based system.
the shared translation task 2013. We used WMT08
as the development set and tested our system on
WMT test sets from 2010 to 2013.
The baseline systems are trained on the training
corpus with initial word alignment, which was ob-
tained via GIZA++ and ?grow-diag-final? method.
Based on the initial word alignment, we comput-
ed word translation probabilities and used the pro-
posed method to obtain a refined word alignment.
Then we used the refined word alignment to train
our SMT systems.
The translation results are evaluated by case-
insensitive BLEU-4 (Papineni et al., 2002).
The feature weights of the translation system
are tuned with the standard minimum-error-rate-
training (Och, 2003) to maximize the systems
BLEU score on the development set.
3.2 Results
3.2.1 Phrase-based System
Table 2 shows Chinese-to-English translation
quality of the phrase-based system. We ob-
served that our refined method significantly out-
performed the baseline word alignment on all test
sets. The improvements are ranged from 0.97 to
1.68 BLEU%.
Table 3 shows German-to-English translation
quality of the phrase-based system. The improve-
ments are ranged from 0.51 to 0.75 BLEU%.
These results demonstrate that the proposed
method improves the translation quality for
149
System N04 N05 N06 N08
Baseline 37.33 34.81 32.20 25.33
Refined 37.91 35.36 32.75 25.40
Combined 38.13 35.63 33.48 25.66
Table 4: Chinese-to-English translation quality of
the hierarchical phrase-based system.
System W10 W11 W12 W13
Baseline 21.22 19.77 20.53 23.51
Refined 21.34 20.64 20.88 23.82
Combined 21.65 20.87 21.16 24.04
Table 5: German-to-English translation quality of
the hierarchical phrase-based system.
phrase-based system. The reason is that by dis-
carding noisy word alignments from the discon-
tinuous alignments, the phrase pairs constrained
by the noisy alignments can be extracted. Thus the
system utilized more phrase pairs than the baseline
did.
3.2.2 Hierarchical Phrase-based System
The hierarchical phrase-based system utilizes dis-
continuous phrase pairs for long distance phrase
reordering. Some of the discontinuous phrase
pairs are extracted from the discontinuous align-
ments. By transforming the discontinuous align-
ments to continuous alignments, on the one hand,
we may lost some discontinuous phrase pairs. On
the other hand, we may extract additional contin-
uous and discontinuous phrase pairs as the align-
ment restriction is loose.
See Figure 3 for illustration. From the initial
alignment, we can extract a hierarchical phrase
pair ?(dang X
1
shi, when X
1
)? from the discon-
tinuous alignment of the English word ?when?.
However, the hierarchical phrase pair cannot be
extracted from our refined alignment, because our
method discards the link between the Chinese
word ?dang? and the English word ?when?. In-
stead, we can extract another hierarchical phrase
pair ?(X
1
shi, when X
1
)?.
Does our method still obtain improvements on
the hierarchical phrase-based system? Table 4 and
Table 5 shows Chinese-to-English and German-
to-English translation quality of the hierarchical
phrase-based system, respectively. For Chinese-
to-English translation, the refined alignment ob-
tained improvements ranged from 0.07 to 0.58

dang
?
shigu
u)
fasheng
?
shi
when the accident
happend
Figure 3: Example of word alignment between a
Chinese and English sentence pair. The dashed
initial link is discarded by our method.
BLEU% on the test set ( the row ?Refined?).
While for German-to-English translation, the im-
provements ranged from 0.12 to 0.59 BLEU% on
the test set (the row ?Refined?).
We find that the improvements are less than
that of the phrase-based system. As discussed
above, our method may lost some hierarchical
phrase pairs that extracted from the discontinuous
alignments. To solve the problem, we combine
2
the initial alignments and the refined alignments
to train the SMT system. The results are shown
in the row ?Combined? in Table 4 and Table 5.
For Chinese-to-English translation, we obtained
an improvements of 1.28 BLEU% on NIST06 over
the baseline. While for German-to-English trans-
lation, the greatest improvements is 1.10 BLEU%
on WMT11.
4 Analyses
In order to further study the performance of the
proposed method, we analyze the word alignment
and the phrase table for Chinese-to-English trans-
lation. We find that our method improves the qual-
ity of word alignment. And as a result, more useful
phrase pairs are extracted from the refined word
alignment.
4.1 Word Alignment
The Chinese-to-English training corpus contains
4.5M sentence pairs. By applying GIZA++ and
the ?grow-diag-final? method, we obtained initial
alignments. We find that 4.0M (accounting for
89%) sentence pairs contain discontinuous align-
ments. We then used the proposed method to dis-
card noisy links. By doing this, the total links
between words in the training corpus are reduced
from 99.6M to 78.9M, indicating that 21% links
are discarded.
2
We do not perform combination for phrase-based sys-
tem, because the phrase table extracted from the initial align-
ment is a subset of that extracted from the refined alignment.
150
Alignment Precision Recall AER
Initial 62.94 89.55 26.07
Refined 73.43 87.82 20.01
Table 6: Precision, Recall and AER on Chinese-
to-English alignment.
Alignment StandPhr HierPhr
Initial 29M 86M
Refined 104M 436M
Table 7: The phrase number extracted from the
initial and refined alignment for the hierarchical
phrase-based system on Chinese-to-English trans-
lation. StandPhr is standard phrase, HierPhr is hi-
erarchical phrase.
We evaluated the alignment quality on 200 sen-
tence pairs. Results are shown in Table 6. It is
observed that our method improves the precision
and decreases the AER, while keeping a high re-
call. This means that our method effectively dis-
cards noisy links in the initial word alignments.
4.2 Phrase Table
According to the standard definition of phrase in
SMT, phrase pairs cannot be extracted from the
discontinuous alignments. By transforming dis-
continuous alignments into continuous alignmen-
t, we can extract more phrase pairs. Table 7
shows the number of standard phrases and hier-
archical phrases extracted from the initial and re-
fined word alignments. We find that the number of
both phrases and hierarchical phrases grows heav-
ily. This is because that the word alignment con-
straint for phrase extraction is loosed by removing
noisy links. Although the phrase table becomes
larger, fortunately, there are some methods (John-
son et al., 2007; He et al., 2009) to prune phrase
table without hurting translation quality.
For further illustration, we compare the phrase
pairs extracted from the initial alignment and re-
fined alignment in Figure 1. From the initial align-
ments, we extracted only 3 standard phrase pairs
and no hierarchical phrase pairs (Table 8). After
discarding noisy alignments (dashed links) by us-
ing the proposed method, we extracted 21 standard
phrase pairs and 36 hierarchical phrases. Table 9
and Table 10 show selected phrase pairs and hier-
archical phrase pairs, respectively.
Chinese English
meiguo The United States
guojia nations
piao note
Table 8: Phrase pairs extracted from the initial
alignment of Figure 1.
Chinese English
shi was
fandui piao a nay note
shaoshu jige the handful of
Table 9: Selected phrase pairs extracted from the
refined alignment of Figure 1.
Chinese English
X
1
zhiyi among X
1
X
1
de guojia nations that X
1
X
1
fandui piao X
2
X
2
X
1
a nay note
Table 10: Selected hierarchical phrase pairs ex-
tracted from the refined alignment of Figure 1.
5 Conclusion and Future Work
In this paper, we proposed a novel method to im-
prove word alignment for SMT. The method re-
fines initial word alignments by transforming dis-
continuous alignment to continuous alignment. As
a result, more useful phrase pairs are extracted
from the refined word alignment. Our method is
simple and efficient, since it uses only the word
translation probabilities obtained from the initial
alignments to discard noisy links. Our method
is independent of languages and can be applied
to most SMT models. Experimental results show
significantly improvements for the state-of-the-art
phrase-based and hierarchical phrase-based sys-
tems on all Chinese-to-English and German-to-
English translation tasks.
In the future, we will refine the method by con-
sidering neighbor words and alignments when dis-
carding noisy links.
Acknowlegement
This paper is supported by the 973 program No.
2014CB340505. We would like to thank Xuan Liu
and the anonymous reviewers for their insightful
comments.
151
References
Peter F. Brown, Stephen A. Della Pietra, Vincen-
t J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Dis-
carding monotone composed rule for hierarchical
phrase-based statistical machine translation. In Pro-
ceedings of the 3rd International Universal Commu-
nication Symposium, pages 25?29.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bienni-
al Conference of the Association for Machine Trans-
lation in the Americas.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quali-
ty by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic,
June.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007 demonstration session.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglin-
ear models for word alignment. In Proceedings of
of ACL 2005, pages 459?466, Ann Arbor,Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 609?616.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In In Proceedings of COLING/ACL
2006, pages 513?520, Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. 30:417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search forword alignment. In In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157?166, Uppsala, Swe-
den, July.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836?841, Copen-
hagen, Danmark, August.
152
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Joint Segmentation and Classification Framework
for Sentiment Analysis
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\
, Li Dong
]?
, Ting Liu
\
, Ming Zhou
?
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
]
Beihang University, Beijing, China
\
{dytang, qinb, tliu}@ir.hit.edu.cn
?
{fuwei, mingzhou}@microsoft.com
]
donglixp@gmail.com
Abstract
In this paper, we propose a joint segmenta-
tion and classification framework for sen-
timent analysis. Existing sentiment clas-
sification algorithms typically split a sen-
tence as a word sequence, which does not
effectively handle the inconsistent senti-
ment polarity between a phrase and the
words it contains, such as ?not bad? and
?a great deal of ?. We address this issue
by developing a joint segmentation and
classification framework (JSC), which si-
multaneously conducts sentence segmen-
tation and sentence-level sentiment classi-
fication. Specifically, we use a log-linear
model to score each segmentation candi-
date, and exploit the phrasal information
of top-ranked segmentations as features to
build the sentiment classifier. A marginal
log-likelihood objective function is de-
vised for the segmentation model, which
is optimized for enhancing the sentiment
classification performance. The joint mod-
el is trained only based on the annotat-
ed sentiment polarity of sentences, with-
out any segmentation annotations. Experi-
ments on a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that, our joint model performs com-
parably with the state-of-the-art methods.
1 Introduction
Sentiment classification, which classifies the senti-
ment polarity of a sentence (or document) as posi-
tive or negative, is a major research direction in the
field of sentiment analysis (Pang and Lee, 2008;
Liu, 2012; Feldman, 2013). Majority of existing
approaches follow Pang et al. (2002) and treat sen-
?
This work was partly done when the first and fourth
authors were visiting Microsoft Research.
timent classification as a special case of text cate-
gorization task. Under this perspective, previous
studies typically use pipelined methods with two
steps. They first produce sentence segmentation-
s with separate text analyzers (Choi and Cardie,
2008; Nakagawa et al., 2010; Socher et al., 2013b)
or bag-of-words (Paltoglou and Thelwall, 2010;
Maas et al., 2011). Then, feature learning and sen-
timent classification algorithms take the segmenta-
tion results as inputs to build the sentiment classi-
fier (Socher et al., 2011; Kalchbrenner et al., 2014;
Dong et al., 2014).
The major disadvantage of a pipelined method
is the problem of error propagation, since sen-
tence segmentation errors cannot be corrected by
the sentiment classification model. A typical kind
of error is caused by the polarity inconsistency be-
tween a phrase and the words it contains, such
as ?not bad, bad? and ?a great deal of, great?.
The segmentations based on bag-of-words or syn-
tactic chunkers are not effective enough to han-
dle the polarity inconsistency phenomenons. The
reason lies in that bag-of-words segmentations re-
gard each word as a separate unit, which losses
the word order and does not capture the phrasal
information. The segmentations based on syntac-
tic chunkers typically aim to identify noun group-
s, verb groups or named entities from a sentence.
However, many sentiment indicators are phrases
constituted of adjectives, negations, adverbs or id-
ioms (Liu, 2012; Mohammad et al., 2013a), which
are splitted by syntactic chunkers. Besides, a bet-
ter approach would be to utilize the sentiment in-
formation to improve the segmentor. Accordingly,
the sentiment-specific segmentor will enhance the
performance of sentiment classification in turn.
In this paper, we propose a joint segmentation
and classification framework (JSC) for sentimen-
t analysis, which simultaneous conducts sentence
segmentation and sentence-level sentiment clas-
sification. The framework is illustrated in Fig-
477
Segmentations Input 
that is not bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
Polarity: +1 
-1 
-1 
+1 
+1 
<+1,-1>   NO 
Polarity Update 
<+1,-1>   NO 
<+1,+1>  YES 
<+1,+1>  YES 
SC SEG CG 
Update 
SC 
2.3  
1.6 
0.6 
0.4 
0.6 
0.4 
2.3 
1.6 
SEG 
Rank 
Top K  
Figure 1: The joint segmentation and classification framework (JSC) for sentiment classification. CG
represents the candidate generation model, SC means the sentiment classification model and SEG stands
for the segmentation ranking model. Down Arrow means the use of a specified model, and Up Arrow
indicates the update of a model.
ure 1. We develop (1) a candidate generation mod-
el to generate the segmentation candidates of a
sentence, (2) a segmentation ranking model to s-
core each segmentation candidate of a given sen-
tence, and (3) a classification model to predic-
t the sentiment polarity of each segmentation. The
phrasal information of top-ranked candidates from
the segmentation model are utilized as features to
build the sentiment classifier. In turn, the predict-
ed sentiment polarity of segmentation candidates
from classification model are leveraged to update
the segmentor. We score each segmentation can-
didate with a log-linear model, and optimize the
segmentor with a marginal log-likelihood objec-
tive. We train the joint model from sentences an-
notated only with sentiment polarity, without any
segmentation annotations.
We evaluate the effectiveness of our joint mod-
el on a benchmark Twitter sentiment classifica-
tion dataset in SemEval 2013. Results show that
the joint model performs comparably with state-
of-the-art methods, and consistently outperforms
pipeline methods in various experiment settings.
The main contributions of the work presented in
this paper are as follows.
? To our knowledge, this is the first work that
automatically produces sentence segmenta-
tion for sentiment classification within a joint
framework.
? We show that the joint model yields com-
parable performance with the state-of-the-art
methods on the benchmark Twitter sentiment
classification datasets in SemEval 2013.
2 Related Work
Existing approaches for sentiment classification
are dominated by two mainstream directions.
Lexicon-based approaches (Turney, 2002; Ding
et al., 2008; Taboada et al., 2011; Thelwall et
al., 2012) typically utilize a lexicon of sentiment
words, each of which is annotated with the sen-
timent polarity or sentiment strength. Linguis-
tic rules such as intensifications and negations are
usually incorporated to aggregate the sentimen-
t polarity of sentences (or documents). Corpus-
based methods treat sentiment classification as a
special case of text categorization task (Pang et al.,
2002). They mostly build the sentiment classifier
from sentences (or documents) with manually an-
notated sentiment polarity or distantly-supervised
corpora collected by sentiment signals like emoti-
cons (Go et al., 2009; Pak and Paroubek, 2010;
Kouloumpis et al., 2011; Zhao et al., 2012).
Majority of existing approaches follow Pang et
al. (2002) and employ corpus-based method for
sentiment classification. Pang et al. (2002) pi-
oneer to treat the sentiment classification of re-
views as a special case of text categorization prob-
lem and first investigate machine learning meth-
ods. They employ Naive Bayes, Maximum En-
tropy and Support Vector Machines (SVM) with a
diverse set of features. In their experiments, the
best performance is achieved by SVM with bag-
of-words feature. Under this perspective, many s-
tudies focus on designing or learning effective fea-
tures to obtain better classification performance.
On movie or product reviews, Wang and Man-
ning (2012) present NBSVM, which trades-off
478
between Naive Bayes and NB-feature enhanced
SVM. Kim and Zhai (2009) and Paltoglou and
Thelwall (2010) learn the feature weights by in-
vestigating variants weighting functions from In-
formation Retrieval. Nakagawa et al. (2010) uti-
lize dependency trees, polarity-shifting rules and
conditional random fields (Lafferty et al., 2001)
with hidden variables to compute the documen-
t feature. On Twitter, Mohammad et al. (2013b)
develop a state-of-the-art Twitter sentiment classi-
fier in SemEval 2013, using a variety of sentiment
lexicons and hand-crafted features.
With the revival of deep learning (representa-
tion learning (Hinton and Salakhutdinov, 2006;
Bengio et al., 2013; Jones, 2014)), more recen-
t studies focus on learning the low-dimensional,
dense and real-valued vector as text features for
sentiment classification. Glorot et al. (2011) inves-
tigate Stacked Denoising Autoencoders to learn
document vector for domain adaptation in sen-
timent classification. Yessenalina and Cardie
(2011) represent each word as a matrix and
compose words using iterated matrix multipli-
cation. Socher et al. propose Recursive Au-
toencoder (RAE) (2011), Matrix-Vector Recursive
Neural Network (MV-RNN) (2012) and Recur-
sive Neural Tensor Network (RNTN) (2013b) to
learn the composition of variable-length phrases
based on the representation of its children. To
learn the sentence representation, Kalchbrenner et
al. (2014) exploit Dynamic Convolutional Neu-
ral Network and Le and Mikolov (2014) inves-
tigate Paragraph Vector. To learn word vectors
for sentiment analysis, Maas et al. (2011) propose
a probabilistic document model following Blei et
al. (2003), Labutov and Lipson (2013) re-embed
words from existing word embeddings and Tang
et al. (2014b) develop three neural networks to
learn word vectors from tweets containing posi-
tive/negative emoticons.
Unlike most previous corpus-based algorithms
that build sentiment classifier based on splitting a
sentence as a word sequence, we produce sentence
segmentations automatically within a joint frame-
work, and conduct sentiment classification based
on the segmentation results.
3 The Proposed Approach
In this section, we first give the task definition
of two tasks, namely sentiment classification and
sentence segmentation. Then, we present the
overview of the proposed joint segmentation and
classification model (JSC) for sentiment analysis.
The segmentation candidate generation model and
the segmentation ranking model are described in
Section 4. The details of the sentiment classifica-
tion model are presented in Section 5.
3.1 Task Definition
The task of sentiment classification has been well
formalized in previous studies (Pang and Lee,
2008; Liu, 2012). The objective is to identify the
sentiment polarity of a sentence (or document) as
positive or negative
1
.
The task of sentence segmentation aims to s-
plit a sentence into a sequence of exclusive part-
s, each of which is a basic computational unit of
the sentence. An example is illustrated in Table 1.
The original text ?that is not bad? is segmented
as ?[that] [is] [not bad]?. The segmentation re-
sult is composed of three basic computational u-
nits, namely [that], [is] and [not bad].
Type Sample
Sentence that is not bad
Segmentation [that] [is] [not bad]
Basic units [that], [is], [not bad]
Table 1: Example for sentence segmentation.
3.2 Joint Model (JSC)
The overview of the proposed joint segmentation
and classification model (JSC) for sentiment anal-
ysis is illustrated in Figure 1. The intuitions of the
joint model are two-folds:
? The segmentation results have a strong influ-
ence on the sentiment classification perfor-
mance, since they are the inputs of the sen-
timent classification model.
? The usefulness of a segmentation can be
judged by whether the sentiment classifier
can use it to predict the correct sentence po-
larity.
Based on the mutual influence observation, we
formalize the joint model in Algorithm 1. The in-
puts contain two parts, training data and feature
extractors. Each sentence s
i
in the training data
1
In this paper, the sentiment polarity of a sentence is not
relevant to the target (or aspect) it contains (Hu and Liu, 2004;
Jiang et al., 2011; Mitchell et al., 2013).
479
Algorithm 1 The joint segmentation and classifi-
cation framework (JSC) for sentiment analysis
Input:
training data: T = [s
i
, pol
g
i
], 1 ? i ? |T |
segmentation feature extractor: sfe(?)
classification feature extractor: cfe(?)
Output:
sentiment classifier: SC
segmentation ranking model: SEG
1: Generate segmentation candidates ?
i
for each
sentence s
i
in T , 1 ? i ? |T |
2: Initialize sentiment classifier SC
(0)
based on
cfe(?
ij
), randomize j ? [1, |?
i
|], 1 ? i ?
|T |
3: Randomly initialize the segmentation ranking
model SEG
(0)
4: for r ? 1 ... R do
5: Predict the sentiment polarity pol
i
for ?
i
based on SC
(r?1)
and cfe(?
i?
)
6: Update the segmentation model SEG
(r)
with SEG
(r?1)
and [?
i
, sfe(?
i?
),
pol
i?
, pol
g
i
], 1 ? i ? |T |
7: for i? 1 ... |T | do
8: Calculate the segmentation score for ?
i?
based on SEG
(r)
and sfe(?
i?
)
9: Select the top-ranked K segmentation
candidates ?
i?
from ?
i
10: end for
11: Train the sentiment classifier SC
(r)
with
cfe(?
i?
), 1 ? i ? |T |
12: end for
13: SC? SC
(R)
14: SEG? SEG
(R)
T is annotated only with its gold sentiment po-
larity pol
g
i
, without any segmentation annotation-
s. There are two feature extractors for the task
of sentence segmentation (sfe(?)) and sentiment
classification (cfe(?)), respectively. The output-
s of the joint model are the segmentation ranking
model SEG and the sentiment classifier SC.
In Algorithm 1, we first generate segmentation
candidates ?
i
for each sentence s
i
in the training
set (line 1). Each ?
i
contains no less than one
segmentation candidates. We randomly select one
segmentation result from each ?
i
and utilize their
classification features to initialize the sentimen-
t classifier SC
(0)
(line 2). We randomly initialize
the segmentation model SEG
(0)
(line 3). Subse-
quently, we iteratively train the segmentation mod-
el SEG
(r)
and sentiment classifier SC
(r)
in a join-
t manner (line 4-12). At each iteration, we pre-
dict the sentiment polarity of each segmentation
candidate ?
i?
with the current sentiment classifi-
er SC
(r?1)
(line 5), and then leverage them to up-
date the segmentation model SEG
(r)
(line 6). Af-
terwards, we utilize the recently updated segmen-
tation ranking model SEG
(r)
to update the senti-
ment classifier SC
(r)
(line 7-11). We extract the
segmentation features for each segmentation can-
didate ?
i?
, and employ them to calculate the seg-
mentation score (line 8). The top-ranked K seg-
mentation results ?
i?
of each sentence s
i
is select-
ed (line 9), and further used to train the sentimen-
t classifier SC
(r)
(line 11). Finally, after training
R iterations, we dump the segmentation ranking
model SEG
(R)
and sentiment classifier SC
(R)
in
the last iteration as outputs (line 13-14).
At training time, we train the segmentation
model and classification model from sentences
with manually annotated sentiment polarity. At
prediction time, given a test sentence, we gener-
ate its segmentation candidates, and then calculate
segmentation score for each candidate. Afterward-
s, we select the top-ranked K candidates and vote
their predicted sentiment polarity from sentiment
classifier as the final result.
4 Segmentation Model
In this section, we present details of the segmenta-
tion candidate generation model (Section 4.1), the
segmentation ranking model (Section 4.2) and the
feature description for segmentation ranking mod-
el (Section 4.3).
4.1 Segmentation Candidate Generation
In this subsection, we describe the strategy to gen-
erate segmentation candidates for each sentence.
Since the segmentation results have an exponen-
tial search space in the number of words in a
sentence, we approximate the computation using
beam search with constrains on a phrase table,
which is induced from massive corpora.
Many studies have been previously proposed to
recognize phrases in the text. However, it is out
of scope of this work to compare them. We ex-
ploit a data-driven approach given by Mikolov et
al. (2013), which identifies phrases based on the
occurrence frequency of unigrams and bigrams,
freq(w
i
, w
j
) =
freq(w
i
, w
j
)? ?
freq(w
i
)? freq(w
j
)
(1)
480
where ? is a discounting coefficient that prevents
too many phrases consisting of very infrequen-
t words. We run 2-4 times over the corpora to get
longer phrases containing more words. We em-
pirically set ? as 10 in our experiment. We use
the default frequency threshold (value=5) in the
word2vec toolkit
2
to select bi-terms.
Given a sentence, we initialize the beam of each
index with the current word, and sequentially add
phrases into the beam if the new phrase is con-
tained in the phrase table. At each index of a sen-
tence, we rank the segmentation candidates by the
inverted number of items within a segmentation,
and save the top-ranked N segmentation candi-
dates into the beam. An example of the generated
segmentation candidates is given in Table 2.
Type Sample
Sentence that is not bad
Phrase Table [is not], [not bad], [is not bad]
Segmentations
[that] [is not bad]
[that] [is not] [bad]
[that] [is] [not bad]
[that] [is] [not] [bad]
Table 2: Example for segmentation candidate gen-
eration.
4.2 Segmentation Ranking Model
The objective of the segmentation ranking model
is to assign a scalar to each segmentation candi-
date, which indicates the usefulness of the seg-
mentation result for sentiment classification. In
this subsection, we describe a log-linear model to
calculate the segmentation score. To effectively
train the segmentation ranking model, we devise a
marginal log-likelihood as the optimization objec-
tive.
Given a segmentation candidate ?
ij
of the sen-
tence s
i
, we calculate the segmentation score
for ?
ij
with a log-linear model, as given in Equa-
tion 2.
?
ij
= exp(b+
?
k
sfe
ijk
? w
k
) (2)
where ?
ij
is the segmentation score of ?
ij
; sfe
ijk
is the k-th segmentation feature of ?
ij
; w and b are
the parameters of the segmentation ranking model.
During training, given a sentence s
i
and its gold
sentiment polarity pol
g
i
, the optimization objec-
2
Available at https://code.google.com/p/word2vec/
tive of the segmentation ranking model is to max-
imize the segmentation scores of the hit candi-
dates, whose predicted sentiment polarity equal-
s to the gold polarity of sentence pol
p
i
. The loss
function of the segmentation model is given in E-
quation 3.
loss = ?
|T |
?
i=1
log(
?
j?H
i
?
ij
?
j
?
?A
i
?
ij
?
) + ?||w||
2
2
(3)
where T is the training data; A
i
represents all the
segmentation candidates of sentence s
i
; H
i
mean-
s the hit candidates of s
i
; ? is the weight of the
L2-norm regularization factor. We train the seg-
mentation model with L-BFGS (Liu and Nocedal,
1989), running over the complete training data.
4.3 Feature
We design two kinds of features for sentence seg-
mentation, namely the phrase-embedding feature
and the segmentation-specific feature. The final
feature representation of each segmentation is the
concatenation of these two features. It is worth
noting that, the phrase-embedding feature is used
in both sentence segmentation and sentiment clas-
sification.
Segmentation-Specific Feature We empirically
design four segmentation-specific features to re-
flect the information of each segmentation, as list-
ed in Table 3.
Phrase-Embedding Feature We leverage
phrase embedding to generate the features of
segmentation candidates for both sentence seg-
mentation and sentiment classification. The
reason is that, in both tasks, the basic compu-
tational units of each segmentation candidate
might be words or phrases of variable length.
Under this scenario, phrase embedding is highly
suitable as it is capable to represent phrases with
different length into a consistent distributed vector
space (Mikolov et al., 2013). For each phrase,
phrase embedding is a dense, real-valued and
continuous vector. After the phrase embedding is
trained, the nearest neighbors in the embedding
space are favored to have similar grammatical us-
ages and semantic meanings. The effectiveness of
phrase embedding has been verified for building
large-scale sentiment lexicon (Tang et al., 2014a)
and machine translation (Zhang et al., 2014).
We learn phrase embedding with Skip-Gram
model (Mikolov et al., 2013), which is the state-of-
481
Feature Feature Description
#unit the number of basic computation units in the segmentation candidate
#unit / #word the ratio of units? number in a candidate to the length of original sentence
#word ? #unit the difference between sentence length and the number of basic computational units
#unit > 2 the number of basic component units composed of more than two words
Table 3: Segmentation-specific features for segmentation ranking.
Feature Feature Description
All-Caps the number of words with all characters in upper case
Emoticon the presence of positive (or negative) emoticons, whether the last unit is emoticon
Hashtag the number of hashtag
Elongated units the number of basic computational containing elongated words (with one character
repeated more than two times), such as gooood
Sentiment lexicon the number of sentiment words, the score of last sentiment words, the total sentiment
score and the maximal sentiment score for each lexicon
Negation the number of negations as individual units in a segmentation
Bag-of-Units an extension of bag-of-word for a segmentation
Punctuation the number of contiguous sequences of dot, question mark and exclamation mark.
Cluster the presence of units from each of the 1,000 clusters from Twitter NLP tool (Gimpel
et al., 2011)
Table 4: Classification-specific features for sentiment classification.
the-art phrase embedding learning algorithm. We
compose the representation (or feature) of a seg-
mentation candidate from the embedding of the
basic computational units (words or phrases) it
contains. In this paper, we explore min, max and
average convolution functions, which have been
used as simple and effective methods for composi-
tion learning in vector-based semantics (Mitchell
and Lapata, 2010; Collobert et al., 2011; Socher et
al., 2013a; Shen et al., 2014; Tang et al., 2014b),
to calculate the representation of a segmentation
candidate. The final phrase-embedding feature is
the concatenation of vectors derived from different
convolutional functions, as given in Equation 4,
pf(seg) = [pf
max
(seg), pf
min
(seg), pf
avg
(seg)]
(4)
where pf(seg) is the representation of the given
segmentation; pf
x
(seg) is the result of the con-
volutional function x ? {min,max, avg}. Each
convolutional function pf
x
(?) conducts the matrix-
vector operation of x on the sequence represented
by columns in the lookup table of phrase embed-
ding. The output of pf
x
(?) is calculated as
pf
x
(seg) = ?
x
?L
ph
?
seg
(5)
where ?
x
is the convolutional function of pf
x
;
?L
ph
?
seg
is the concatenated column vectors of
the basic computational units in the segmentation;
L
ph
is the lookup table of phrase embedding.
5 Classification Model
For sentiment classification, we follow the su-
pervised learning framework (Pang et al., 2002)
and build the classifier from sentences with man-
ually labelled sentiment polarity. We extend the
state-of-the-art hand-crafted features in SemEval
2013 (Mohammad et al., 2013b), and design the
classification-specific features for each segmenta-
tion. The detailed feature description is given in
Table 4.
6 Experiment
In this section, we conduct experiments to evaluate
the effectiveness of the joint model. We describe
the experiment settings and the result analysis.
6.1 Dataset and Experiment Settings
We conduct sentiment classification of tweets on a
benchmark Twitter sentiment classification dataset
in SemEval 2013. We run 2-class (positive vs neg-
ative) classification as sentence segmentation has a
great influence on the positive/negative polarity of
tweets due to the polarity inconsistency between a
phrase and its constitutes, such as ?not bad, bad?.
482
We leave 3-class classification (positive, negative,
neutral) and fine-grained classification (very neg-
ative, negative, neutral, positive, very positive) in
the future work.
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 5: Statistics of the SemEval 2013 Twitter
sentiment classification dataset (positive vs nega-
tive).
The statistics of our dataset crawled from Se-
mEval 2013 are given in Table 5. The evalua-
tion metric is the macro-F1 of sentiment classifi-
cation. We train the joint model on the training
set, tune parameters on the dev set and evaluate
on the test set. We train the sentiment classifier
with LibLinear (Fan et al., 2008) and utilize exist-
ing sentiment lexicons
3
to extract classification-
specific features. We randomly crawl 100M tweets
from February 1st, 2013 to April 30th, 2013 with
Twitter API, and use them to learn the phrase em-
bedding with Skip-Gram
4
. The vocabulary size
of the phrase embedding is 926K, from unigram
to 5-gram. The parameter -c in SVM is tuned on
the dev-set in both baseline and our method. We
run the L-BFGS for 50 iterations, and set the reg-
ularization factor ? as 0.003. The beam size N of
the candidate generation model and the top-ranked
segmentation number K are tuned on the dev-set.
6.2 Baseline Methods
We compare the proposed joint model with the fol-
lowing sentiment classification algorithms:
? DistSuper: We collect 10M balanced tweets
selected by positive and negative emoticons
5
as
training data, and build classifier using the Lib-
Linear and ngram features (Go et al., 2009; Zhao
et al., 2012).
? SVM: The n-gram features and Support Vec-
tor Machine are widely-used baseline methods to
build sentiment classifiers (Pang et al., 2002). We
use LibLinear to train the SVM classifier.
3
In this work, we use HL (Hu and Liu, 2004), M-
PQA (Wilson et al., 2005), NRC Emotion Lexicon (Moham-
mad and Turney, 2012), NRC Hashtag Lexicon and Senti-
ment140Lexicon (Mohammad et al., 2013b).
4
https://code.google.com/p/word2vec/
5
We use the emoticons selected by Hu et al. (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
? NBSVM: NBSVM (Wang and Manning,
2012) trades-off between Naive Bayes and NB-
features enhanced SVM. We use NBSVM-bi be-
cause it performs best on sentiment classification
of reviews.
? RAE: Recursive Autoencoder (Socher et al.,
2011) has been proven effective for sentiment clas-
sification by learning sentence representation. We
train the RAE using the pre-trained phrase embed-
ding learned from 100M tweets.
? SentiStrength: Thelwall et al. (2012) build a
lexicon-based classifier which uses linguistic rules
to detect the sentiment strength of tweets.
? SSWE
u
: Tang et al. (2014b) propose to learn
sentiment-specific word embedding (SSWE) from
10M tweets collected by emoticons. They apply
SSWE as features for Twitter sentiment classifica-
tion.
? NRC: NRC builds the state-of-the-art system
in SemEval 2013 Twitter Sentiment Classifica-
tion Track, incorporating diverse sentiment lexi-
cons and hand-crafted features (Mohammad et al.,
2013b). We re-implement this system because the
codes are not publicly available. We do not di-
rectly report their results in the evaluation task,
as our training and development sets are smaller
than their dataset. In NRC + PF, We concatenate
the NRC features and the phrase embeddings fea-
ture (PF), and build the sentiment classifier with
LibLinear.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al., 2013b) be-
cause the tweets in our dataset do not have accu-
rately parsed results. Another reason is that, due to
the differences between domains, the performance
of RNTN trained on movie reviews might be de-
creased if directly applied on the tweets (Xiao et
al., 2013).
6.3 Results and Analysis
Table 6 shows the macro-F1 of the baseline sys-
tems as well as our joint model (JSC) on senti-
ment classification of tweets (positive vs negative).
As is shown in Table 6, distant supervision is
relatively weak because the noisy-labeled tweets
are treated as the gold standard, which decreases
the performance of sentiment classifier. The result
of bag-of-unigram feature (74.50%) is not satisfied
as it losses the word order and does not well cap-
483
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + 5-gram 63.92
SVM + unigram 74.50
SVM + 5-gram 74.97
Recursive Autoencoder 75.42
NBSVM 75.28
SentiStrength 73.23
SSWE
u
84.98
NRC (Top System in SemEval 2013) 84.73
NRC + PF 84.75
JSC 85.51
Table 6: Macro-F1 for positive vs negative classi-
fication of tweets.
ture the semantic meaning of phrases. The integra-
tion of high-order n-ngram (up to 5-gram) does not
achieve significant improvement (+0.47%). The
reason is that, if a sentence contains a bigram ?not
bad?, they will use ?bad? and ?not bad? as par-
allel features, which confuses the sentiment clas-
sification model. NBSVM and Recursive Autoen-
coder perform comparatively and have a big gap
in comparison with JSC. In RAE, the representa-
tion of a sentence is composed from the represen-
tation of words it contains. Accordingly, ?great?
in ?a great deal of ? also contributes to the final
sentence representation via composition function.
JSC automatically conducts sentence segmenta-
tion by considering the sentiment polarity of sen-
tence, and utilize the phrasal information from the
segmentations. Ideally, JSC regards phrases like
?not bad? and ?a great deal of ? as basic compu-
tational units, and yields better classification per-
formance. JSC (85.51%) performs slightly better
than the state-of-the-art systems (SSWE
u
, 84.98%;
NRC+PF, 84.75%), which verifies its effective-
ness.
6.4 Comparing Joint and Pipelined Models
We compare the proposed joint model with
pipelined methods on Twitter sentiment classifi-
cation with different feature sets. Figure 2 gives
the experiment results. The tick [A, B] on x-
axis means the use of A as segmentation feature
and the use of B as classification feature. PF
represents the phrase-embedding feature; SF and
CF stand for the segmentation-specific feature and
classification-specific feature, respectively. We
use the bag-of-word segmentation result to build
sentiment classier in Pipeline 1, and use the seg-
mentation candidate with maximum phrase num-
ber in Pipeline 2.
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
[PF, PF] [PF+SF, PF] [PF, PF+CF] [PF+SF, PF+CF]
Pipeline 1Pipeline 2Joint
Figure 2: Macro-F1 for positive vs negative classi-
fication of tweets with joint and pipelined models.
From Figure 2, we find that the joint model
consistently outperforms pipelined baseline meth-
ods in all feature settings. The reason is that
the pipelined methods suffer from error propaga-
tion, since the errors from linguistic-driven and
bag-of-word segmentations cannot be corrected by
the sentiment classification model. Besides, tra-
ditional segmentors do not update the segmenta-
tion model with the sentiment information of tex-
t. Unlike pipelined methods, the joint model is
capable to address these problems by optimizing
the segmentation model with the classification re-
sults in a joint framework, which yields better
performance on sentiment classification. We also
find that Pipeline 2 always outperforms Pipeline
1, which indicates the usefulness of phrase-based
segmentation for sentiment classification.
6.5 Effect of the beam size N
We investigate the influence of beam size N ,
which is the maximum number of segmentation
candidates of a sentence. In this part, we clamp the
feature set as [PF+SF, PF+CF], and vary the beam
size N in [1,2,4,8,16,32,64]. The experiment re-
sults of macro-F1 on the development set are il-
lustrated in Figure 3 (a). The time cost of each
training iteration is given in Figure 3 (b).
From Figure 3 (a), we can see that when larg-
er beam size is considered, the classification per-
formance is improved. When beam size is 1, the
model stands for the greedy search with the bag-
of-words segmentation. When the beam size is s-
mall, such as 2, beam search losses many phrasal
information of sentences and thus the improve-
ment is not significant. The performance remains
steady when beam size is larger than 16. From
484
1 2 4 8 16 32 640.81
0.82
0.83
0.84
0.85
0.86
Beam Size
Mac
ro?F
1
(a) Macro-F1 score for senti-
ment classification.
1 2 4 8 16 32 640
20
40
60
80
100
120
Beam Size
Run
time
 (Sec
ond)
 
 
(b) Time cost (seconds) of
each training iteration.
Figure 3: Sentiment classification of tweets with
different beam size N .
Figure 3 (b), we can find that the runtime of each
training iteration increases with larger beam size.
It is intuitive as the joint model with larger beam
considers more segmentation results, which in-
creases the training time of the segmentation mod-
el. We set beam size as 16 after parameter learn-
ing.
6.6 Effect of the top-ranked segmentation
number K
We investigate how the top-ranked segmentation
number K affects the performance of sentimen-
t classification. In this part, we set the feature as
[PF+SF, PF+CF], and the beam size as 16. The
results of macro-F1 on the development set are il-
lustrated in Figure 4.
1 3 5 7 9 11 13 150.82
0.83
0.84
0.85
0.86
Top?ranked candidate number
Ma
cro
?F1
Figure 4: Sentiment classification of tweets with
different top-ranked segmentation number K.
From Figure 4, we find that the classification
performance increases with K being larger. The
reason is that when a larger K is used, (1) at train-
ing time, the sentiment classifier is built by using
more phrasal information from multiple segmen-
tations, which benefits from the ensembles; (2) at
test time, the joint model considers several top-
ranked segmentations and get the final sentiment
polarity through voting. The performance remain-
s stable when K is larger than 7, as the phrasal
information has been mostly covered.
7 Conclusion
In this paper, we develop a joint segmentation
and classification framework (JSC) for sentiment
analysis. Unlike existing sentiment classification
algorithms that build sentiment classifier based
on the segmentation results from bag-of-words or
separate segmentors, the proposed joint model si-
multaneously conducts sentence segmentation and
sentiment classification. We introduce a marginal
log-likelihood function to optimize the segmenta-
tion model, and effectively train the joint mod-
el from sentences annotated only with sentiment
polarity, without segmentation annotations of sen-
tences. The effectiveness of the joint model has
been verified by applying it on the benchmark
dataset of Twitter sentiment classification in Se-
mEval 2013. Results show that, the joint model
performs comparably with state-of-the-art meth-
ods, and outperforms pipelined methods in various
settings. In the future, we plan to apply the join-
t model on other domains, such as movie/product
reviews.
Acknowledgements
We thank Nan Yang, Yajuan Duan, Yaming
Sun and Meishan Zhang for their helpful dis-
cussions. We thank the anonymous reviewers
for their insightful comments and feedbacks on
this work. This research was partly supported
by National Natural Science Foundation of Chi-
na (No.61133012, No.61273321, No.61300113).
The contact author of this paper, according to the
meaning given to this role by Harbin Institute of
Technology, is Bing Qin.
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
485
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics.
Nicola Jones. 2014. Computer science: The learning
machines. Nature, 505(7482):146.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A sentence model based on convolu-
tional neural networks. In Procedding of the 52th
Annual Meeting of Association for Computational
Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of CIKM 2009. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of international con-
ference on Machine learning. ACM.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. Proceedings
of International Conference on Machine Learning.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. Conference on Neural Information Processing
Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
Saif M Mohammad and Peter D Turney. 2012. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence.
Saif M Mohammad, Bonnie J Dorr, Graeme Hirst, and
Peter D Turney. 2013a. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
486
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013b. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 786?794.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Georgios Paltoglou and Mike Thelwall. 2010. A s-
tudy of information retrieval weighting schemes for
sentiment analysis. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1386?1395.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
web search. In Proceedings of the companion publi-
cation of the 23rd international conference on World
wide web companion, pages 373?374.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Y Ng. 2013a. Reasoning with neu-
ral tensor networks for knowledge base completion.
The Conference on Neural Information Processing
Systems.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and
Ting Liu. 2014a. Building large-scale twitter-
specific sentiment lexicon : A representation learn-
ing approach. In Proceedings of COLING 2014,
the 25th International Conference on Computation-
al Linguistics, pages 172?182.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014b. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 152?
162, October.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 111?121.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
487
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864?874,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Domain Adaptation for CRF-based Chinese Word Segmentation using
Free Annotations
Yijia Liu ??, Yue Zhang ?, Wanxiang Che ?, Ting Liu ?, Fan Wu ?
?Singapore University of Technology and Design
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{yjliu,car,tliu}@ir.hit.edu.cn {yue zhang,fan wu}@sutd.edu.sg
Abstract
Supervised methods have been the domi-
nant approach for Chinese word segmen-
tation. The performance can drop signif-
icantly when the test domain is different
from the training domain. In this paper,
we study the problem of obtaining par-
tial annotation from freely available data
to help Chinese word segmentation on dif-
ferent domains. Different sources of free
annotations are transformed into a unified
form of partial annotation and a variant
CRF model is used to leverage both fully
and partially annotated data consistently.
Experimental results show that the Chi-
nese word segmentation model benefits
from free partially annotated data. On the
SIGHAN Bakeoff 2010 data, we achieve
results that are competitive to the best re-
ported in the literature.
1 Introduction
Statistical Chinese word segmentation gains high
accuracies on newswire (Xue and Shen, 2003;
Zhang and Clark, 2007; Jiang et al., 2009; Zhao
et al., 2010; Sun and Xu, 2011). However, man-
ually annotated training data mostly come from
the news domain, and the performance can drop
severely when the test data shift from newswire
to blogs, computer forums and Internet literature
(Liu and Zhang, 2012).
Several methods have been proposed for solv-
ing the domain adaptation problem for segmenta-
tion, which include the traditional token- and type-
supervised methods (Song et al., 2012; Zhang et
al., 2014). While token-supervised methods rely
on manually annotated target-domain sentences,
type-supervised methods leverage manually as-
sembled domain-specific lexicons to improve
target-domain segmentation accuracies. Both
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
Figure 1: The segmentation problem, illustrated
using the sentence ??? (Pudong) ?? (devel-
opment) ? (and) ?? (legal) ?? (construc-
tion)?. Possible segmentation labels are drawn un-
der each character, where b, m, e, s stand for the
beginning, middle, end of a multi-character word,
and a single character word, respectively. The path
shows the correct segmentation by choosing one
label for each character.
methods are competitive given the same amount of
annotation effects (Garrette and Baldridge, 2012;
Zhang et al., 2014). However, obtaining manually
annotated data can be expensive.
On the other hand, there are free data which
contain limited but useful segmentation informa-
tion over the Internet, including large-scale un-
labeled data, domain-specific lexicons and semi-
annotated web pages such as Wikipedia. In the
last case, word-boundary information is contained
in hyperlinks and other markup annotations. Such
free data offer a useful alternative for improving
the segmentation performance, especially on do-
mains that are not identical to newswire, and for
which little annotation is available.
In this paper, we investigate techniques for
adopting freely available data to help improve the
performance on Chinese word segmentation. We
propose a simple but robust method for construct-
ing partial segmentation from different sources
of free data, including unlabeled data and the
Wikipedia. There has been work on making use
of both unlabeled data (Sun and Xu, 2011; Wang
et al., 2011) and Wikipedia (Jiang et al., 2013)
864
to improve segmentation. However, no empiri-
cal results have been reported on a unified ap-
proach to deal with different types of free data.
We use a conditional random fields (Lafferty et al.,
2001; Tsuboi et al., 2008) variant that can lever-
age the partial annotations obtained from different
sources of free annotation. Training is achieved by
a modification to the learning objective, incorpo-
rating partial annotation likelihood, so that a single
model can be trained consistently with a mixture
of full and partial annotation.
Experimental results show that our method of
using partially annotated data can consistently im-
proves cross-domain segmentation performance.
We obtain results which are competitive to the
best reported in the literature. Our segmentor
is freely released at https://github.com/
ExpResults/partial-crfsuite.
2 Obtaining Partially Annotated Data
We model the Chinese word segmentation task as
a character sequence tagging problem, which is to
give each character in a sentence a word-boundary
tag (Xue and Shen, 2003). We adopt four tags, b,
m, e and s, which represent the beginning, middle,
end of a multi-character word, and a single char-
acter word, respectively. A manually segmented
sentence can be represented as a tag sequence, as
shown in Figure 1.
We investigate two major sources of freely-
available annotations: lexicons and natural anno-
tation, both with the help of unannotated data.
To make use of the first source of informa-
tion, we incorporate words from a lexicon into
unannotated sentences by matching of character
sequences, resulting in partially annotated sen-
tences, as shown in Figure 2a. In this example,
the word ???? (the Huqi Mountain)? in the
unannotated sentence matches an item in the lex-
icon. As a result, we obtain a partially-annotated
sentence, in which the segmentation ambiguity of
the characters ?? (fox)?, ?? (brandy road)? and
?? (mountain)? are resolved (??? being the be-
ginning, ??? being the middle and ??? being the
end of the same word). At the same time, the seg-
mentation ambiguity of the surrounding characters
?? (at)? and ?? (save)? are reduced (??? be-
ing either a single-character word or the end of
a multi-character word, and ??? being either a
single-character word or the beginning of a multi-
character word).
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
(a) ?? (at) ??? (Huqi Mountain) ? ? (save) ?
? (Biyao)?, where ????? matches a lexicon word.
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
(b) ?? (e.g.)???? (lysozyme)? ??? (lactoferrin)?,
where ?????? is a hyperlink.
Figure 2: Examples of partially annotated data.
The paths show possible correct segmentations.
Natural annotation, which refers to word
boundaries that can be inferred from URLs, fonts
or colors on web pages, also result in partially-
annotated sentences. Taking a web page shown
in Figure 2b for example. It can be inferred from
the URL tags on ?????? that ??? should be
either the beginning of a multi-character word or
a single-character word, and ??? should be either
the end a multi-character word or single-character
word. Similarly, possible tags of the surrounding
character ??? and ??? can also be inferred.
We turn both lexicons and natural annotation
into the same form of partial annotation with
same unresolved ambiguities, as shown in Figure
2, and use them together with available full anno-
tation (Figure 1) as the training data for the seg-
mentor. In this section, we describe in detail how
to obtain partially annotated sentences from each
resource, respectively.
2.1 Lexicons
In this scenario, we assume that there are unla-
beled sentences along with a lexicon for the target
domain. We obtain partially segmented sentences
by extracting word boundaries from the unlabeled
sentences with the help of the lexicon. Previous
matching methods (Wu and Tseng, 1993; Wong
and Chan, 1996) for Chinese word segmentation
largely rely on the lexicons, and are generally con-
sidered being weak in ambiguity resolution (Gao
865
People?s
Daily
?? (saw)?? (Hainan)??? (tourist industry)?? (full)?? (hope)
saw tourist industry in Hainan is full of hope
Wikipedia
??(mainly)?(is)?? (tourist)? (industry)?(and)?? (software)??(industry)
mainly is tourist industry and software industry
(a) Case of incompatible annotation on ????(tourist industry)? between People?s Daily and Wikipedia.
Literature
????? (Shuo Wen Jie Zi, a book)?(segmented)?(annotated)?
the segmented and annotated version of Shuo Wen Jie Zi
Computer
??(each)??(record)?(is)??(splitted)?(into)?? (fields)
each record is splitted into several fields
(b) Similar subsequence ???(field)? is segmented differently under different domains in Wikipedia.
Table 1: Examples natural annotation from Wikipedia. Underline marks annotated words.
et al., 2005). But for obtaining the partial labeled
data with lexicon, the matching method can still be
a solution. Since we do not aim to recognize every
word from sentence, we can select a lexicon with
smaller coverage but less ambiguity to achieve rel-
atively precise matching result.
In this paper, we apply two matching schemes
to the same raw sentences to obtain partially an-
notated sentences. The first is a simple forward-
maximum matching (FMM) scheme, which is
very close to the forward maximum matching al-
gorithm ofWu and Tseng (1993) for Chinese word
segmentation. This scheme scans the input sen-
tence from left to right. At each position, it at-
tempts to find the longest subsequence of Chi-
nese characters that matches a lexicon entry. If
such an entry is found, the subsequence is tagged
with the corresponding tags, and its surrounding
characters are also constrained to a smaller set of
tags. If no subsequence is found in the lexicon, the
character is left with all the possible tags. Taking
the sentence in Figure 2a for example. When the
algorithm scans the second character, ???, and
finds the entry ????? in the lexicon, the sub-
sequence of characters is recognized as a word,
and tagged with b, m and e, respectively. At the
same time, the previous character ??? can be in-
ferred as only end of a multi-character word (e) or
a single-character word (s). The second matching
scheme is backward maximum matching, which
can be treated as the application of FMM on the
reverse of unlabeled sentences using a lexicon of
reversed words.
To mitigate the errors resulting from one single
matching scheme, we combine the two matching
results by agreement. The basic idea is that if a
subsequence of sentence is recognized as word by
multiple matching results, it can be considered as a
more precise annotation. Our algorithm reads par-
tial segmentation by different methods and selects
the subsequences that are identified as word by all
methods as annotated words.
2.2 Natural Annotation
We use the Chinese Wikipedia for natural anno-
tation. Partially annotated sentences are readily
formed in Wikipedia by markup syntax, such as
URLs. However, some subtle issues exist if the
sentences are used directly. One problem is in-
compatibility of segmentation standards between
the annotated training data and Wikipedia. Jiang
et al. (2009) discuss this incompatibility problem
between two corpora ? the CTB and the Peo-
ple?s Daily; the problem is even more severe on
Wikipedia because it can be edited by any user.
Table 1a shows a case of incompatible annota-
tion between the People?s Daily data and natural
annotation in Wikipedia, where the three charac-
ters ????? are segmented differently. Both can
be treated as correct, although they have different
segmentation granularities.
Another problem is the intrinsic ambiguity of
segmentation. The same character sequence can
be segmented into different words under differ-
ent contexts. If the training and test data contain
different contexts, the learned model can give in-
correct results on the test data. This is particu-
larly true across different domains. Table 1b gives
such an example, where the character sequence
???? is segmented differently in two of our test
domains, but both cases exist in Wikipedia.
In summary, Wikipedia introduces both use-
ful information for domain adaptation and harm-
ful noise with negative effects on the model. To
866
achieve better performance of domain adaptation
using Wikipedia, one intuitive approach is to se-
lect more domain-related data and less irrelevant
data to minimize the risks that result from incom-
patible annotation and domain difference.
To this end, we assume that there are some raw
sentences on the target domain, which can be used
to evaluate the relevance between Wikipedia and
target domain test data. We assume that URL-
tagged entries reflect the segmentation standards
of Wikipedia sentence, and use them to match
Wikipedia sentences with the raw target domain
data. If the character sequence of any URL-tagged
entry in a Wikipedia sentence matches the target
domain data, the Wikipedia sentence is selected
for training. Another advantage of such data se-
lection is that the training time consumption can
be reduced by reducing the size of training data.
3 CRF for Word Segmentation
We follow the work of Zhao et al. (2010) and Sun
and Xu (2011), and adopt the Conditional Random
Fields (CRF) model (Lafferty et al., 2001) for the
sequence labeling problem of word segmentation.
Given an input characters sequence, the task is to
assign one segmentation label from {b,m, e, s} on
each character. Let x = (x
1
, x
2
, ..., x
T
) be the
sequence of characters in sentence whose length
is T , and y = (y
1
, y
2
, ..., y
T
) be the correspond-
ing label sequence, where y
i
? Y . The linear-
chain conditional random field for Chinese word
segmentation can be formalized as
p(y|x) =
1
Z
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (1)
where ?
k
are the model parameters, f
k
are the fea-
ture functions and Z is the probability normalizer.
Z =
?
y
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (2)
We follow Sun and Xu (2011) and use the fea-
ture templates shown in Table 2 to model the seg-
mented task. For ith character in the sentence, the
n-gram features represent the surrounding charac-
ters of this character; Type categorizes the charac-
ter it into digit, punctuation, english and other;
Identical indicates whether the input character is
the same with its surrounding characters. This
feature captures repetition patterns such as ??
? (try)? or ??? (stroll)?.
Type Template
unigram C
s
(i? 3 < s < i + 3)
bigram C
s
C
s+1
(i? 3 < s < i + 2)
C
s
C
s+2
(i? 3 < s < i + 1)
type Type(C
i
)
Type(C
s
)Type(C
s+1
)
(i? 1 < s < i + 2)
identical Identical(C
s
, C
s+1
) (i ? 3 <
s < i + 1)
Identical(C
s
, C
s+2
) (i ? 3 <
s < i)
Table 2: Feature templates for the ith character.
For fully-annotated training data, the learning
problem of conditional random fields is to maxi-
mize the log likelihood over all the training data
(Lafferty et al., 2001)
L =
N
?
n=1
log p(y
(n)
|x
(n)
)
Here N is the number of training sentences. Both
the likelihood p(y
(n)
|x
(n)
) and its gradient can be
calculated by performing the forward-backward
algorithm (Baum and Petrie, 1966) on the se-
quence, and several optimization algorithm can be
adopted to learn parameters from data, including
L-BFGS (Liu and Nocedal, 1989) and SGD (Bot-
tou, 1991).
4 Training a CRF with partially
annotated data
For word segmentation with partially annotated
data, some characters in a sentence can have
a definite segmentation label, while some can
have multiple labels with ambiguities remain-
ing. Taking the partially annotated sentence
in Figure 2a for example, the corresponding
potential label sequence for ??????? is
{(e, s), (b), (m), (e), (b, s)}, where the characters
???, ??? and ??? have fixed labels but for ???
and ???, some ambiguities exist. Note that the
full annotation in Figure 1 can be regarded as a
special case of partial annotation, where the num-
ber of potential labels for each character is one.
We follow Tsuboi et al. (2008) and model
marginal probabilities over partially annotated
data. Define the possible labels that correspond
to the partial annotation as L = (L
1
, L
2
, ..., L
T
),
where each L
i
is a non-empty subset of Y that cor-
responds to the set of possible labels for x
i
. Let
867
YL
be the set of all possible label sequences where
?y ? Y
L
, y
i
? L
i
. The marginal probability of
Y
L
can be modeled as
p(Y
L
|x) =
1
Z
?
y?Y
L
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (3)
Defining the unnormalized marginal probability as
Z
Y
L
=
?
y?Y
L
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x),
and the normalizer Z being the same as Equation
2, the log marginal probability of Y
L
over N par-
tially annotated training examples can be formal-
ized as
L
Y
L
=
N
?
n=1
log p(Y
L
|x) =
N
?
n=1
(logZ
Y
L
? logZ)
The gradient of the likelihood can be written as
?L
Y
L
??
k
=
N
?
n=1
T
?
t=1
?
y
Y
L
?L
t
,
y
?
Y
L
?L
t?1
f
k
(y
Y
L
, y
?
Y
L
,x)p
Y
L
(y
Y
L
, y
?
Y
L
|x)
?
N
?
n=1
T
?
t=1
?
y,y
?
f
k
(y, y
?
,x)p(y, y
?
|x)
Both Z
Y
L
and its gradient are similar in form to
Z. By introducing a modification to the forward-
backward algorithm, Z
Y
L
and L
Y
L
can be calcu-
lated. Define the forward variable for partially an-
notated data ?
Y
L
,t
(j) = p
Y
L
(x
?1,...,t?
, y
t
= j). A
modification on the forward algorithm can be for-
malized as
?
Y
L
,t
(j) =
{
0 j /? L
t
?
i?L
t?1
?
t
(j, i, x
t
)?
Y
L
,t?1
(i) j ? L
t
where?
t
(j, i, x) is a potential function that equals
?
k
?
k
f
k
(y
t
= j, y
t?1
= i, x
t
). Similarly, for the
backward variable ?
Y
L
,t
,
?
Y
L
,t
(i) =
{
0 i /? L
t
?
j?L
t+1
?
t
(j, i, x
t+1
)?
Y
L
,t+1
(j) i ? L
t
Z
Y
L
can be calculated by ?
Y
L
(T ),
and p
Y
L
(y, y
?
|x) can be calculated by
?
Y
L
,t?1
(y
?
)?
t
(y, y
?
, x
t
)?
Y
L
,t
(y).
Note that if each element in Y
L
is constrained
to one single label, the CRF model in Equation 3
degrades into Equation 1. So we can train a unified
model with both fully and partially annotated data.
We implement this CRF model based on a open
source toolkit CRFSuite.
1
In our experiments, we
use the L-BFGS (Liu and Nocedal, 1989) algo-
rithm to learn parameters from both fully and par-
tially annotated data.
5 Experiments
We perform our experiments on the domain adap-
tation test data from SIGHANBakeoff 2010 (Zhao
et al., 2010), adapting annotated training sentences
from People?s Daily (PD) (Yu et al., 2001) to
different test domains. The fully annotated data
is selected from the People?s Daily newspaper
in January of 1998, and the four test domains
from the SIGHAN Bakeoff 2010 include finance,
medicine, literature and computer. Sample seg-
mented data in the computer domain from this
bakeoff is used as development set. Statistics of
the data are shown in first half of Table 3. We
use wikidump20140419
2
for the Wikipedia data.
All the traditional Chinese pages in Wikipedia are
converted to simplified Chinese. After filtering
functional pages like redirection and removing du-
plication, 5.45 million sentences are reserved.
For comparison with related work on using a
lexicon to improve segmentation, another set of
test data is chosen for this setting. We use the Chi-
nese Treebank (CTB) as the source domain data,
and Zhuxian (a free Internet novel, also named as
?Jade dynasty?, referred to as ZX henceforth) as
the target domain data.
3
The ZX data are written
in a different style from newswire, and contains
many out-of-vocabulary words. This setting has
been used by Liu and Zhang (2012) and Zhang et
al. (2014) for domain adaptation of segmentation
and POS-tagging. We use the standard training,
development and test split. Statistics of the test
data annotated by Zhang et al. (2014) are shown
in the second half of Table 3.
The data preparation method in Section 2 and
the CRF method in Section 4 are used for all
the experiments. Both recall of out-of-vocabulary
words (R
oov
) and F-score are used to evaluate the
1
http://www.chokkan.org/software/
crfsuite/
2
http://dumps.wikimedia.org/zhwiki/
20140419/
3
Annotated target domain test data and lexicon are avail-
able from http://ir.hit.edu.cn/
?
mszhang/
eacl14mszhang.zip.
868
P
D
?
S
I
G
H
A
N
Data set Train Development Test
PD Computer Finance Medicine Literature Computer
# sent. 19,056 1,000 560 1,308 670 1,329
# words 1,109,734 21,398 33,035 31,499 35,735 35,319
OOV 0.1766 0.0874 0.1102 0.0619 0.1522
C
T
B
5
?
Z
X
Data set Train Development Test Unlabeled
W
i
k
i
p
e
d
i
a
Unlabeled
CTB5 ZX
# sent. 18,086 788 1,394 32,023 5,456,151
# words 493,934 20,393 34,355
OOV 0.1377 0.1550
Table 3: Statistics of data used in this paper.
segmentation performance. There is a mixture of
Chinese characters, English words and numeric
expression in the test data from SIGHAN Bakeoff
2010. To test the influence of Wikipedia data on
Chinese word segmentation alone, we apply reg-
ular expressions to detect English words and nu-
meric expressions, so that they are marked as not
segmented. After performing this preprocessing
step, cleaned test input data are fed to the CRF
model to give a relatively strong baseline.
5.1 Free Lexicons
5.1.1 Obtaining lexicons
For domain adaption from CTB to ZX, we use
a lexicon released by Zhang et al. (2014). The
lexicon is crawled from a online encyclopedia
4
,
and contains the names of 159 characters and ar-
tifacts in the Zhuxian novel. We follow Zhang et
al. (2014) and name it NR for convenience of fur-
ther discussion. The NR lexicon can be treated
as a strongly domain-related, high quality but rel-
atively small lexicon. It?s a typical example of
freely available lexicon over the Internet.
For domain adaptation from PD to medicine and
computer, we collect a list of page titles under
the corresponding categories in Wikipedia. For
medicine, entries under essential medicines, bi-
ological system and diseases are collected. For
computer, entries under computer network, Mi-
crosoft Windows and software widgets are se-
lected. These lexicons are typical freely available
lexicons that we can access to.
5.1.2 Obtaining Unlabeled Sentences
For ZX, partially annotated sentences are obtained
using the NR lexicon and unlabeled ZX sentences
by applying the matching scheme described in
4
http://baike.baidu.com/view/18277.htm
90.1
90.2
90.3
90.4
1 2 4 8 16 32# of sentences * 1000
F sc
ore 
on d
eve
lopm
ent
Figure 3: F-score on the development data when
using different numbers of unlabeled data.
Section 2. The CTB5 training data and the par-
tially annotated data are mixed as the final train-
ing data. Different amounts of unlabeled data are
applied to the development test set, and results are
shown in Figure 3. From this figure we can see
that incorporating 16K sentences gives the high-
est accuracy, and adding more partial labeled data
does not change the accuracy significantly. So for
the ZX experiments, we choose the 16K sentences
as the unlabeled data.
For the medicine and computer experiments, we
selected domain-specific sentences by matching
with the domain-specific lexicons. About 46K out
of the 5.45 million wiki sentences contain subse-
quences in the medicine lexicon and 22K in the
case of the computer domain. We randomly se-
lect 16K sentences as the unlabeled data for each
domain, respectively.
5.1.3 Final results
We incorporate the partially annotated data ob-
tained with the help of lexicon for each of the
test domain. For adaptation from CTB to ZX, we
trained our baseline model on the CTB5 training
data with the feature templates in Table 2. For
adaptation from PD to medicine and computer, we
869
Domain ZX Medicine Computer
F Roov F Roov F Roov
Baseline 87.50 73.65 91.36 72.95 93.16 84.02
Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27
Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63
Zhang et al. (2014) 88.34 - - - - -
Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer
domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
trained our baseline model on the PD training data
with the same feature template setting.
Previous research makes use of a lexicon by
adding lexicon features directly into a model (Sun
and Xu, 2011; Zhang et al., 2014), rather than
transforming them into partially annotated sen-
tences. To make a comparison, we follow Sun and
Xu (2011) and add three lexicon features to repre-
sent whether c
i
is located at the beginning, middle
or the end of a word in the lexicon, respectively.
For each test domain, the lexicon for the lexi-
con feature model consists of the most frequent
words in the source domain training data (about
6.7K for CTB5 and 8K for PD, respectively) and
the domain-specific lexicon we obtained in Sec-
tion 5.1.1.
The results are shown in Table 4, where the first
row shows the performance of the baseline mod-
els and the second row shows the performance
of the model incorporating lexicon feature. The
third row shows our method using partial anno-
tation. On the ZX test set, our method outper-
forms the baseline by more than 3 absolute per-
centage. The model with partially annotated data
performs better than the one with additional lexi-
con features. Similar conclusion is obtained when
adapting from PD to medicine and computer. By
incorporating the partially annotated data, the seg-
mentation of lexicon words, along with the con-
text, is learned.
We also compare our method with the work of
Zhang et al. (2014), who reported results only on
the ZX test data. We use the same lexicon settings.
Our method gives better result than Zhang et al.
(2014), showing that the combination of a lexicon
and unannotated sentence into partially annotated
data can lead to better performance than using a
dictionary alone in type-supervision. Given that
we only explore the use of free resource, combin-
ing a lexicon with unannotated sentences is a bet-
ter option than using the lexicon directly. Zhang
et al.?s concern, on the other hand, is to compare
Method
Com. Dev
F Roov
Baseline 93.56 83.75
Baseline+PA (Random 160K) 94.29 86.58
Baseline+PA (Selected) 95.00 88.28
Table 5: The performance of data selection on the
development set of the computer domain.
type- and token-annotation. Our partial annota-
tion can thus be treated as a compromise to obtain
some pseudo partial token-annotations when full
token annotations are unavailable. Another thing
to note is that the model of Zhang et al. (2014) is
a joint model for segmentation and POS-tagging,
which is generally considered stronger than a sin-
gle segmentation model.
5.2 Free Natural Annotation
When extracting word boundaries from Wikipedia
sentences, we ignore natural annotations on En-
glish words and digits because these words are rec-
ognized by the preprocessor. Following Jiang et
al. (2013), we also recognize a naturally annotated
two-character subsequence as a word.
5.2.1 Effect of data selection
To make better use of more domain-specific data,
and to alleviate noise in partial annotation, we ap-
ply the selection method proposed in Section 2
to the Wikipedia data. On the computer domain
development test data, this selection method re-
sults in 9.4K computer-related sentences with par-
tial annotation. A model is trained with both the
PD training data and the partially annotated com-
puter domain Wikipedia data. For comparison, we
also trained a model with 160K randomly selected
Wikipedia sentences. The experimental result is
shown in Table 5. The model incorporating se-
lected data achieves better performance compared
to the model with randomly sampled data, demon-
strating that data selection is helpful to improving
870
Method
Finance Medicine Literature Computer
Avg-FF Roov F Roov F Roov F Roov
Baseline 95.20 86.90 91.36 72.90 92.27 73.61 93.16 83.48 93.00
Baseline+PA (Ran-
dom 160K)
95.16 87.60 92.41 78.13 92.17 75.30 93.91 83.48 93.41
Baseline+PA
(Selected)
95.54 88.53 92.47 78.28 92.49 76.84 93.93 87.53 93.61
+0.34 +1.11 +0.22 +0.77
Jiang et al. (2013) 93.16 93.34 93.53 91.19 92.80
Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
the domain adaption accuracy.
5.2.2 Final Result
The final results on the four test domains are
shown in Table 6. From this table, we can see
that significant improvements are achieved with
the help of the partially annotated Wikipedia data,
when compared to the baseline. The models
trained with selected partial annotation perform
better than those trained with random partial an-
notation. Our F-scores are competitive to those re-
ported by Jiang et al. (2013). However, since their
model is trained on a different source domain, the
results are not directly comparable.
5.2.3 Analysis
In this section, we study the effect of Wikipedia on
domain adaptation when no data selection is per-
formed, in order to analyze the effect of partially
annotated data. We randomly sample 10K, 20K,
40K, 80K and 160K sentences from the 5.45 mil-
lion Wikipedia sentences, and incorporate them
into the training process, respectively. Five models
are obtained adding the baseline, and we test their
performances on the four test domains. Figure 4
shows the results.
From the figure we can see that for the medicine
and computer domains, where the OOV rate is rel-
atively high, the F-score generally increases when
more data from Wikipedia are used. The trends
of F-score and OOV recall against the volume of
Wikipedia data are almost identical. However, for
the finance and literature domains, which have low
OOV rates, such a relation between data size and
accuracy is not witnessed. For the literature do-
main, even an opposite trends is shown.
We can draw the following conclusions: (1)
Natural annotation on Wikipedia data contributes
to the recognition of OOV words on domain adap-
tation; (2) target domains with more OOV words
benefit more from Wikipedia data. (3) along with
Method
Med. Com.
F F
Baseline 91.36 93.16
Baseline+PA (Lex) 91.68 93.47
Baseline+PA (Natural) 92.47 93.93
Baseline+PA (Lex+Natural) 92.63 94.07
Table 7: Results by combining different sources of
free annotation.
the positive effect on OOV recognition, Wikipedia
data can also introduce noise, and hence data se-
lection can be useful.
5.3 Combining Lexicon and Natural
Annotation
To make the most use of free annotation, we com-
bine available free lexicon and natural annotation
resources by joining the partially annotated sen-
tences derived using each resource, training our
CRF model with these partially annotated sen-
tences and the fully annotated PD sentences. The
tests are performed on medicine and computer do-
mains. Table 7 shows the results, where further
improvements are made on both domains when the
two types of resources are combined.
6 Related Work
There has been a line of research on making use of
unlabeled data for word segmentation. Zhao and
Kit (2008) improve segmentation performance by
mutual information between characters, collected
from large unlabeled data; Li and Sun (2009) use
punctuation information in a large raw corpus to
learn a segmentation model, and achieve better
recognition of OOVwords; Sun and Xu (2011) ex-
plore several statistical features derived from un-
labeled data to help improve character-based word
segmentation. These investigations mainly focus
on in-domain accuracies. Liu and Zhang (2012)
871
0.9516
0.9519
0.9522
0.9525
0 50 100 150
0.8700
0.8725
0.8750
0.8775
(a) Finance
0.9150
0.9175
0.9200
0.9225
0 50 100 150
0.73
0.74
0.75
0.76
0.77
0.78
(b) Medicine
0.9216
0.9219
0.9222
0.9225
0 50 100 150 0.735
0.740
0.745
0.750
(c) Literature
0.934
0.936
0.938
0 50 100 150
0.84
0.85
0.86
0.87
(d) Computer
Figure 4: Performance of the model incorporating difference sizes of Wikipedia data. The solid line
represents the F-score and dashed line represents the recall of OOV words.
study domain adaptation using an unsupervised
self-training method. In contrast to their work,
we make use of not only unlabeled data, but also
leverage any free annotation to achieve better re-
sults for domain adaptation.
There has also been work on making use of a
dictionary and natural annotation for segmenta-
tion. Zhang et al. (2014) study type-supervised do-
main adaptation for Chinese segmentation. They
categorize domain difference into two types: dif-
ferent vocabulary and different POS distributions.
While the first type of difference can be effec-
tively resolved by using lexicon for each domain,
the second type of difference needs to be resolved
by using annotated sentences. They found that
given the same manual annotation time, a com-
bination of the lexicon and sentence is the most
effective. Jiang et al. (2013) use 160K Wikipedia
sentences to improves segmentation accuracies on
several domains. Both Zhang et al. (2014) and
Jiang et al. (2013) work on discriminative mod-
els using the structure perceptron (Collins, 2002),
although they study two different sources of infor-
mation. In contrast to their work, we unify both
types of information under the CRF framework.
CRF has been used for Chinese word segmenta-
tion (Tseng, 2005; Shi and Wang, 2007; Zhao and
Kit, 2008; Wang et al., 2011). However, most pre-
vious work train a CRF by using full annotation
only. In contrast, we study CRF based segmenta-
tion by using both full and partial annotation.
Several other variants of CRF model has been
proposed in the machine learning literature, such
as the generalized expectation method (Mann and
McCallum, 2008), which introduce knowledge by
incorporating a manually annotated feature dis-
tribution into the regularizer, and the JESS-CM
(Suzuki and Isozaki, 2008), which use a EM-like
method to iteratively optimize the parameter on
both the annotated data and unlabeled data. In
contrast, we directly incorporate the likelihood of
partial annotation into the objective function. The
work that is the most similar to ours is Tsuboi et
al. (2008), who modify the CRF learning objec-
tive for partial data. They focus on Japanese lexi-
cal analysis using manually collected partial data,
while we investigate the effect of partial annota-
tion from freely available sources for Chinese seg-
mentation.
7 Conclusion
In this paper, we investigated the problem of do-
main adaptation for word segmentation, by trans-
ferring various sources of free annotations into a
consistent form of partially annotated data and ap-
plying a variant of CRF that can be trained using
fully- and partially-annotated data simultaneously.
We performed a large set of experiments to study
the effectness of free data, finding that they are
useful for improving segmentation accuracy. Ex-
periments also show that proper data selection can
further benefit the model?s performance.
872
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments. This work was supported
by the National Key Basic Research Program of
China via grant 2014CB340503 and the National
Natural Science Foundation of China (NSFC) via
grant 61133012 and 61370164, the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
References
Leonard E Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
markov chains. The annals of mathematical statis-
tics, pages 1554?1563.
L?eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, Nimes, France. EC2.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Comput. Linguist., 31(4):531?574, December.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markovmodels for part-of-speech
tagging with incomplete tag dictionaries. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 821?
831, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 522?530, Suntec, Singapore,
August. Association for Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan L?u, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natu-
ral annotations: Word segmentation as a case study.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 761?769, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Comput. Linguist., 35(4):505?512, December.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745?754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
20th International Joint Conference on Artifical In-
telligence, IJCAI?07, pages 1707?1712, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yan Song, Prescott Klassen, Fei Xia, and Chunyu Kit.
2012. Entropy-based training data selection for do-
main adaptation. In Proceedings of COLING 2012:
Posters, pages 1191?1200, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?
979, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665?673, Columbus, Ohio, June.
Association for Computational Linguistics.
Huihsin Tseng. 2005. A conditional random field word
segmenter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
897?904, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
873
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Pak-kwong Wong and Chorkin Chan. 1996. Chinese
word segmentation based on maximum matching
and word binding force. In Proceedings of the 16th
Conference on Computational Linguistics - Volume
1, COLING ?96, pages 200?203, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: Achievements and
problems. J. Am. Soc. Inf. Sci., 44(9):532?542, Oc-
tober.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing - Volume 17, SIGHAN ?03, pages 176?
179, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 840?
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Type-supervised domain adaptation for
joint segmentation and pos-tagging. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 588?597, Gothenburg, Sweden, April. Asso-
ciation for Computational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In In: The Third International Joint Conference on
Natural Language Processing (IJCNLP-2008.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. 9(2):5:1?5:32,
June.
874
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415?1425,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Using Structured Events to Predict Stock Price Movement:
An Empirical Investigation
Xiao Ding
??
, Yue Zhang
?
, Ting Liu
?
, Junwen Duan
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{xding, tliu, jwduan}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
It has been shown that news events influ-
ence the trends of stock price movements.
However, previous work on news-driven
stock market prediction rely on shallow
features (such as bags-of-words, named
entities and noun phrases), which do not
capture structured entity-relation informa-
tion, and hence cannot represent complete
and exact events. Recent advances in
Open Information Extraction (Open IE)
techniques enable the extraction of struc-
tured events from web-scale data. We
propose to adapt Open IE technology for
event-based stock price movement pre-
diction, extracting structured events from
large-scale public news without manual
efforts. Both linear and nonlinear mod-
els are employed to empirically investigate
the hidden and complex relationships be-
tween events and the stock market. Large-
scale experiments show that the accuracy
of S&P 500 index prediction is 60%, and
that of individual stock prediction can be
over 70%. Our event-based system out-
performs bags-of-words-based baselines,
and previously reported systems trained on
S&P 500 stock historical data.
1 Introduction
Predicting stock price movements is of clear in-
terest to investors, public companies and govern-
ments. There has been a debate on whether the
market can be predicted. The Random Walk The-
ory (Malkiel, 1973) hypothesizes that prices are
determined randomly and hence it is impossible to
outperform the market. However, with advances
of AI, it has been shown empirically that stock
?
This work was done while the first author was visiting
Singapore University of Technology and Design
Figure 1: Example news for Apple Inc. and
Google Inc.
price movement is predictable (Bondt and Thaler,
1985; Jegadeesh, 1990; Lo and MacKinlay, 1990;
Jegadeesh and Titman, 1993). Recent work (Das
and Chen, 2007; Tetlock, 2007; Tetlock et al.,
2008; Si et al., 2013; Xie et al., 2013; Wang and
Hua, 2014) has applied Natural Language Process-
ing (NLP) techniques to help analyze the effect of
web texts on stock market prediction, finding that
events reported in financial news are important ev-
idence to stock price movement prediction.
As news events affect human decisions and the
volatility of stock prices is influenced by human
trading, it is reasonable to say that events can influ-
ence the stock market. Figure 1 shows two pieces
of financial news about Apple Inc. and Google
Inc., respectively. Shares of Apple Inc. fell as trad-
ing began in New York on Thursday morning, the
day after its former CEO Steve Jobs passed away.
Google?s stock fell after grim earnings came out.
Accurate extraction of events from financial news
may play an important role in stock market pre-
diction. However, previous work represents news
documents mainly using simple features, such as
bags-of-words, noun phrases, and named entities
(Lavrenko et al., 2000; Kogan et al., 2009; Luss
and d?Aspremont, 2012; Schumaker and Chen,
2009). With these unstructured features, it is dif-
ficult to capture key events embedded in financial
news, and even more difficult to model the impact
of events on stock market prediction. For exam-
ple, representing the event ?Apple has sued Sam-
sung Electronics for copying ?the look and feel?
1415
of its iPad tablet and iPhone smartphone.? using
term-level features {?Apple?, ?sued?, ?Samsung?,
?Electronics?, ?copying?, ...} alone, it can be dif-
ficult to accurately predict the stock price move-
ments of Apple Inc. and Samsung Inc., respec-
tively, as the unstructured terms cannot indicate
the actor and object of the event.
In this paper, we propose using structured in-
formation to represent events, and develop a pre-
diction model to analyze the relationship between
events and the stock market. The problem is im-
portant because it provides insights into under-
standing the underlying mechanisms of the influ-
ence of events on the stock market. There are two
main challenges to this method. On the one hand,
how to obtain structured event information from
large-scale news streams is a challenging problem.
We propose to apply Open Information Extraction
techniques (Open IE; Banko et al. (2007); Et-
zioni et al. (2011); Fader et al. (2011)), which
do not require predefined event types or manu-
ally labeled corpora. Subsequently, two ontolo-
gies (i.e. VerbNet and WordNet) are used to gen-
eralize structured event features in order to reduce
their sparseness. On the other hand, the problem
of accurately predicting stock price movement us-
ing structured events is challenging, since events
and the stock market can have complex relations,
which can be influenced by hidden factors. In ad-
dition to the commonly used linear models, we
build a deep neural network model, which takes
structured events as input and learn the potential
relationships between events and the stock market.
Experiments on large-scale financial news
datasets from Reuters
1
(106,521 documents)
and Bloomberg
2
(447,145 documents) show that
events are better features for stock market predic-
tion than bags-of-words. In addition, deep neu-
ral networks achieve better performance than lin-
ear models. The accuracy of S&P 500 index pre-
diction by our approach outperforms previous sys-
tems, and the accuracy of individual stock predic-
tion can be over 70% on the large-scale data.
Our system can be regarded as one step towards
building an expert system that exploits rich knowl-
edge for stock market prediction. Our results are
helpful for automatically mining stock price re-
lated news events, and for improving the accuracy
of algorithm trading systems.
1
http://www.reuters.com/
2
http://www.bloomberg.com/
2 Method
2.1 Event Representation
We follow the work of Kim (1993) and design a
structured representation scheme that allows us to
extract events and generalize them. Kim defines
an event as a tuple (O
i
, P , T ), where O
i
? O is
a set of objects, P is a relation over the objects
and T is a time interval. We propose a representa-
tion that further structures the event to have roles
in addition to relations. Each event is composed
of an action P , an actor O
1
that conducted the
action, and an object O
2
on which the action was
performed. Formally, an event is represented as
E = (O
1
, P, O
2
, T ), where P is the action, O
1
is the actor,O
2
is the object and T is the timestamp
(T is mainly used for aligning stock data with
news data). For example, the event ?Sep 3, 2013
- Microsoft agrees to buy Nokia?s mobile phone
business for $7.2 billion.? is modeled as: (Actor =
Microsoft, Action = buy, Object = Nokia?s mobile
phone business, Time = Sep 3, 2013).
Previous work on stock market prediction rep-
resents events as a set of individual terms (Fung
et al., 2002; Fung et al., 2003; Hayo and Ku-
tan, 2004; Feldman et al., 2011). For example,
?Microsoft agrees to buy Nokia?s mobile phone
business for $7.2 billion.? can be represented by
{?Microsoft?, ?agrees?, ?buy?, ?Nokia?s?, ?mo-
bile?, ...} and ?Oracle has filed suit against Google
over its ever-more-popular mobile operating sys-
tem, Android.? can be represented by {?Oracle?,
?has?, ?filed?, ?suit?, ?against?, ?Google?, ...}.
However, terms alone might fail to accurately pre-
dict the stock price movement ofMicrosoft, Nokia,
Oracle and Google, because they cannot indicate
the actor and object of the event. To our knowl-
edge, no effort has been reported in the literature
to empirically investigate structured event repre-
sentations for stock market prediction.
2.2 Event Extraction
A main contribution of our work is to extract and
use structured events instead of bags-of-words in
prediction models. However, structured event ex-
traction can be a costly task, requiring predefined
event types and manual event templates (Ji and Gr-
ishman, 2008; Li et al., 2013). Partly due to this,
the bags-of-words-based document representation
has been the mainstream method for a long time.
To tackle this issue, we resort to Open IE, extract-
ing event tuples from wide-coverage data with-
1416
out requiring any human input (e.g. templates).
Our system is based on the system of Fader et al.
(2011) and the work of Ding et al. (2013); it does
not require predefined target event types and la-
beled training examples. Given a natural language
sentence obtained from news texts, the following
procedure is used to extract structured events:
1. Event Phrase Extraction. We extract the
predicate verb P of a sentence based on
the dependency parser of Zhang and Clark
(2011), and then find the longest sequence of
words P
v
, such that P
v
starts at P and satis-
fies the syntactic and lexical constraints pro-
posed by Fader et al. (2011). The content of
these two constraints are as follows:
? Syntactic constraint: every multi-word
event phrase must begin with a verb, end
with a preposition, and be a contiguous
sequence of words in the sentence.
? Lexical constraint: an event phrase
should appear with at least a minimal
number of distinct argument pairs in a
large corpus.
2. Argument Extraction. For each event
phrase P
v
identified in the step above, we find
the nearest noun phrase O
1
to the left of P
v
in the sentence, and O
1
should contain the
subject of the sentence (if it does not contain
the subject of P
v
, we find the second near-
est noun phrase). Analogously, we find the
nearest noun phrase O
2
to the right of P
v
in
the sentence, and O
2
should contain the ob-
ject of the sentence (if it does not contain the
object of P
v
, we find the second nearest noun
phrase).
An example of the extraction algorithm is as fol-
lows. Consider the sentence,
Instant view: Private sector adds 114,000 jobs
in July: ADP.
The predicate verb is identified as ?adds?, and
its subject and object ?sector? and ?jobs?, respec-
tively. The structured event is extracted as (Private
sector, adds, 114,000 jobs).
2.3 Event Generalization
Our goal is to train a model that is able to make
predictions based on various expressions of the
same event. For example, ?Microsoft swallows
Nokia?s phone business for $7.2 billion? and ?Mi-
crosoft purchases Nokia?s phone business? report
the same event. To improve the accuracy of our
prediction model, we should endow the event ex-
traction algorithm with generalization capacity.
To this end, we leverage knowledge from two
well-known ontologies, WordNet (Miller, 1995)
and VerbNet (Kipper et al., 2006). The pro-
cess of event generalization consists of two steps.
First, we construct a morphological analysis tool
based on the WordNet stemmer to extract lemma
forms of inflected words. For example, in ?In-
stant view: Private sector adds 114,000 jobs in
July.?, the words ?adds? and ?jobs? are trans-
formed to ?add? and ?job?, respectively. Second,
we generalize each verb to its class name in Verb-
Net. For example, ?add? belongs to the multi-
ply class. After generalization, the event (Private
sector, adds, 114,000 jobs) becomes (private sec-
tor, multiply class, 114,000 job). Similar methods
on event generalization have been investigated in
Open IE based event causal prediction (Radinsky
and Horvitz, 2013).
2.4 Prediction Models
1. Linear model. Most previous work uses linear
models to predict the stock market (Fung et al.,
2002; Luss and d?Aspremont, 2012; Schumaker
and Chen, 2009; Kogan et al., 2009; Das and
Chen, 2007; Xie et al., 2013). To make direct com-
parisons, this paper constructs a linear prediction
model by using Support Vector Machines (SVMs),
a state-of-the-art classification model. Given a
training set (d
1
, y
1
), (d
2
, y
2
), ..., (d
N
, y
N
),
where n ? [1, N ], d
n
is a news document and
y
i
? {+1, ?1} is the output class. d
n
can be
news titles, news contents or both. The output
Class +1 represents that the stock price will in-
crease the next day/week/month, and the output
Class -1 represents that the stock price will de-
crease the next day/week/month. The features
can be bag-of-words features or structured event
features. By SVMs, y = argmax{Class +
1, Class ? 1} is determined by the linear func-
tion w ??(d
n
, y
n
), where w is the feature weight
vector, and ?(d
n
, y
n
) is a function that maps d
n
into a M-dimensional feature space. Feature tem-
plates will be discussed in the next subsection.
2. Nonlinear model. Intuitively, the relationship
between events and the stock market may be more
complex than linear, due to hidden and indirect
1417
? 
News documents 
?1 
Class +1 The polarity of the stock price movement is positive 
Class -1 The polarity of the stock price movement is negative 
Input Layer 
Output Layer 
Hidden Layers ? 
? 
?2 ?3 ?M 
Figure 2: Structure of the deep neural network
model
relationships. We exploit a deep neural network
model, the hidden layers of which is useful for
learning such hidden relationships. The structure
of the model with two hidden layers is illustrated
in Figure 2. In all layers, the sigmoid activation
function ? is used.
Let the values of the neurons of the output layer
be y
cls
(cls ? {+1,?1}), its input be net
cls
, and
y
2
be the value vector of the neurons of the last
hidden layer; then:
y
cls
= f(net
cls
) = ?(wcls ? y2) (1)
where wcls is the weight vector between the neu-
ron cls of the output layer and the neurons of the
last hidden layer. In addition,
y
2k
= ?(w
2k ? y1) (k ? [1, |y2|])
y
1j
= ?(w
1j ??(dn)) (j ? [1, |y1|])
(2)
Here y
1
is the value vector of the neu-
rons of the first hidden layer, w
2k =
(w
2k1
, w
2k2
, ..., w
2k|y
1
|
), k ? [1, |y
2
|] and
w
1j = (w1j1, w1j2, ..., w1jM ), j ? [1, |y1|].
w
2kj
is the weight between the kth neuron of
the last hidden layer and the jth neuron of the
first hidden layer; w
1jm
is the weight between
the jth neuron of the first hidden layer and the
mth neuron of the input layer m ? [1, M ]; d
n
is a news document and ?(d
n
) maps d
n
into a
M-dimensional features space. News documents
and features used in the nonlinear model are the
same as those in the linear model, which will be
introduced in details in the next subsection. The
standard back-propagation algorithm (Rumelhart
et al., 1985) is used for supervised training of the
neural network.
train dev test
number of
instances
1425 178 179
number of
events
54776 6457 6593
time inter-
val
02/10/2006
-
18/16/2012
19/06/2012
-
21/02/2013
22/02/2013
-
21/11/2013
Table 1: Dataset splitting
2.5 Feature Representation
In this paper, we use the same features (i.e. docu-
ment representations) in the linear and nonlinear
prediction models, including bags-of-words and
structured events.
(1) Bag-of-words features. We use the clas-
sic ?TFIDF? score for bag-of-words features. Let
L be the vocabulary size derived from the train-
ing data (introduced in the next section), and
freq(t
l
) denote the number of occurrences of
the lth word in the vocabulary in document d.
TF
l
=
1
|d|
freq(t
l
), ?l ? [1 , L], where |d| is
the number of words in the document d (stop
words are removed). TFIDF
l
=
1
|d|
freq(t
l
) ?
log(
N
|{d :freq(t
l
)>0}|
), where N is the number of
documents in the training set. The feature vector
? can be represented as? = (?
1
, ?
2
, ..., ?
M
) =
(TFIDF
1
, TFIDF
2
, ..., TFIDF
M
). The TFIDF
feature representation has been used by most pre-
vious studies on stock market prediction (Kogan et
al., 2009; Luss and d?Aspremont, 2012).
(2) Event features. We represent an event
tuple (O
1
, P, O
2
, T ) by the combination of
elements (except for T) (O
1
, P , O
2
, O
1
+ P ,
P + O
2
, O
1
+ P + O
2
). For example, the
event tuple (Microsoft, buy, Nokia?s mobile phone
business) can be represented as (#arg1=Microsoft,
#action=get class, #arg2=Nokia?s mobile phone
business, #arg1 action=Microsoft get class,
#action arg2=get class Nokia?s mobile phone
business, #arg1 action arg2=Microsoft get class
Nokia?s mobile phone business). Structured
events are more sparse than words, and we reduce
sparseness by two means. First, verb classes
(Section 2.3) are used instead of verbs for P. For
example, ?get class? is used instead of the verb
?buy?. Second, we use back-off features, such
as O
1
+ P (?Microsoft get class?) and P + O
2
(?get class Nokia?s mobile phone business?), to
address the sparseness ofO
1
andO
2
. Note that the
order of O
1
and O
2
is important for our task since
they indicate the actor and object, respectively.
1418
 0.52
 0.53
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
1day 1week 1month
Ac
cu
rac
y
Time span
bow+svmbow+deep neural network
event+svm
event+deep neural network
(a) Accuarcy
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
1day 1week 1month
MC
C
Time span
bow+svmbow+deep neural network
event+svm
event+deep neural network
(b) MCC
Figure 3: Overall development experiment results
3 Experiments
Our experiments are carried out on three differ-
ent time intervals: short term (1 day), medium
term (1 week) and long term (1 month). We test
the influence of events on predicting the polarity
of stock change for each time interval, comparing
the event-based news representation with bag-of-
words-based news representations, and the deep
neural network model with the SVM model.
3.1 Data Description
We use publicly available financial news from
Reuters and Bloomberg over the period from Oc-
tober 2006 to November 2013. This time span
witnesses a severe economic downturn in 2007-
2010, followed by a modest recovery in 2011-
2013. There are 106,521 documents in total
from Reuters News and 447,145 from Bloomberg
News. News titles and contents are extracted from
HTML. The timestamps of the news are also ex-
tracted, for alignment with stock price informa-
tion. The data size is larger than most previous
work in the literature.
We mainly focus on predicting the change of the
Standard & Poor?s 500 stock (S&P 500) index
3
,
obtaining indices and stock price data from Yahoo
Finance. To justify the effectiveness of our predic-
tion model, we also predict price movements of
fifteen individual shares from different sectors in
S&P 500. We automatically align 1,782 instances
of daily trading data with news titles and contents
from the previous day/the day a week before the
stock price data/the day a month before the stock
price data, 4/5 of which are used as the training
3
Standard & Poor?s 500 is a stock market index based
on the market capitalizations of 500 large companies having
common stock listed on the NYSE or NASDAQ.
data, 1/10 for development testing and 1/10 for
testing. As shown in Table 1, the training, devel-
opment and test set are split temporally, with the
data from 02/10/2006 to 18/16/2012 for training,
the data from 19/06/2012 to 21/02/2013 for de-
velopment testing, and the data from 22/02/2013
to 21/11/2013 for testing. There are about 54,776
events in the training set, 6,457 events in the de-
velopment set and 6,593 events in the test set.
3.2 Evaluation Metrics
We use two assessment metrics. First, a standard
and intuitive approach to measuring the perfor-
mance of classifiers is accuracy. However, this
measure is very sensitive to data skew: when a
class has an overwhelmingly high frequency, the
accuracy can be high using a classifier that makes
prediction on the majority class. Previous work
(Xie et al., 2013) uses an additional evaluation
metric, which relies on the Matthews Correlation
Cofficient (MCC) to avoid bias due to data skew
(our data are rather large and not severely skewed,
but we also use MCC for comparison with previ-
ous work). MCC is a single summary value that
incorporates all 4 cells of a 2*2 confusion matrix
(True Positive, False Positive, True Negative and
False Negative, respectively). GivenTP ,TN , FP
and FN :
MCC =
TP ?TN?FP ?FN
?
(TP+FP)(TP+FN )(TN +FP)(TN +FN )
(3)
3.3 Overall Development Results
We evaluate our four prediction methods (i.e.
SVM with bag-of-word features (bow), deep neu-
ral network with bag-of-word features (bow),
1419
1 day 1 week 1 month
1 layer
Accuracy 58.94% 57.73% 55.76%
MCC 0.1249 0.0916 0.0731
2 layers
Accuracy 59.60% 57.73% 56.19%
MCC 0.1683 0.1215 0.0875
Table 2: Different numbers of hidden layers
title content content +
title
bloomberg
title + title
Acc 59.60% 54.65% 56.83% 59.64%
MCC 0.1683 0.0627 0.0852 0.1758
Table 3: Different amounts of data
SVM with event features and deep neural network
with event features) on three time intervals (i.e.
1 day, 1 week and 1 month, respectively) on the
development dataset, and show the results in Fig-
ure 3. We find that:
(1) Structured event is a better choice for rep-
resenting news documents. Given the same pre-
diction model (SVM or deep neural network), the
event-based method achieves consistently better
performance than the bag-of-words-based method
over all three time intervals. This is likely due
to the following two reasons. First, being an ex-
traction of predicate-argument structures, events
carry the most essential information of the docu-
ment. In contrast, bag-of-words can contain more
irrelevant information. Second, structured events
can directly give the actor and object of the action,
which is important for predicting stock market.
(2) The deep neural network model achieves
better performance than the SVM model, partly by
learning hidden relationships between structured
events and stock prices. We give analysis to these
relationships in the next section.
(3) Event information is a good indicator for
short-term volatility of stock prices. As shown in
Figure 3, the performance of daily prediction is
better than weekly and monthly prediction. Our
experimental results confirm the conclusion of
Tetlock, Saar-Tsechansky, and Macskassy (2008)
that there is a one-day delay between the price
response and the information embedded in the
news. In addition, we find that some events may
cause immediate changes of stock prices. For ex-
ample, former Microsoft CEO Steve Ballmer an-
nounced he would step down within 12 months
on 23/08/2013. Within an hour, Microsoft shares
jumped as much as 9 percent. This fact indicates
that it may be possible to predict stock price move-
ment on a shorter time interval than one day. How-
Google Inc.
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
67.86% 0.4642 61.17% 0.2301 55.70% 0.1135
Boeing Company
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
68.75% 0.4339 57.14% 0.1585 56.04% 0.1605
Wal-Mart Stores
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
70.45% 0.4679 62.03% 0.2703 56.04% 0.1605
Table 4: Individual stock prediction results
ever, we cannot access fine-grained stock price
historical data, and this investigation will be left
as future work.
3.4 Experiments with Different Numbers of
Hidden Layers of the Deep Neural
Network Model
Cybenko (1989) states that when every processing
element utilizes the sigmoid activation function,
one hidden layer is enough to solve any discrim-
inant classification problem, and two hidden lay-
ers are capable to parse arbitrary output functions
of input pattern. Here we conduct a development
experiment by different number of hidden layers
for the deep neural network model. As shown in
Table 2, the performance of two hidden layers is
better than one hidden layer, which is consistent
with the experimental results of Sharda and De-
len (2006) on the task of movie box-office predic-
tion. It indicates that more hidden layers can ex-
plain more complex relations (Bengio, 2009). In-
tuitively, three or more hidden layers may achieve
better performance. However, three hidden lay-
ers mean that we construct a five-layer deep neu-
ral network, which is difficult to train (Bengio et
al., 1994). We did not obtain improved accuracies
using three hidden layers, due to diminishing gra-
dients. A deep investigation of this problem is out
of the scope of this paper.
3.5 Experiments with Different Amounts of
Data
We conduct a development experiment by extract-
ing news titles and contents from Reuters and
Bloomberg, respectively. While titles can give the
central information about the news, contents may
provide some background knowledge or details.
Radinsky et al. (2012) argued that news titles are
more helpful for prediction compared to news con-
1420
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  100  200  300  400  500
Ac
cu
rac
y
Company Ranking
Wal-Mart
GoogleBoeing
Nike
Qualcomm
Apache
Starbucks
Avon
Visa
Symantec
Hershey
Mattel
Actavis GannettSanDisk
individual stock
(a) Accuarcy
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  100  200  300  400  500
MC
C
Company Ranking
Wal-MartGoogle
Boeing
Nike Qualcomm
Apache
Starbucks
Avon
Visa
Symantec
Hershey
Mattel
Actavis Gannett
SanDisk
individual stock
(b) MCC
Figure 4: Individual stock prediction experiment results
tents, and this paper mainly uses titles. Here we
design a comparative experiment to analyze the ef-
fectiveness of news titles and contents. First, we
use Reuters news to compare the effectiveness of
news titles and contents, and then add Bloomberg
news titles to investigate whether the amounts of
data matters. Table 3 shows that using only news
titles achieves the best performance. A likely rea-
son is that we may extract some irrelevant events
from news contents.
With the additional Bloomberg data, the results
are not dramatically improved. This is intuitively
because most events are reported by both Reuters
news and Bloomberg news. We randomly se-
lect about 9,000 pieces of news documents from
Reuters and Bloomberg and check the daily over-
lap manually, finding that about 60% of the news
are reported by both Reuters and Bloomberg. The
overlap of important news (news related to S&P
500 companies) is 80% and the overlap of unim-
portant news is 40%.
3.6 Individual Stock Prediction
In addition to predicting the S&P 500 index, we
also investigate the effectiveness of our approach
on the problem of individual stock prediction us-
ing the development dataset. We select three well-
known companies, Google Inc., Boeing Company
and Wal-Mart Stores from three different sec-
tors (i.e. Information Technology, Industrials and
Consumer Staples, respectively) classified by the
Global Industry Classification Standard (GICS).
We use company news, sector news and all news to
predict individual stock price movement, respec-
tively. The experimental results are listed in Ta-
ble 4.
The result of individual stock prediction by us-
ing only company news dramatically outperforms
the result of S&P 500 index prediction. The main
reason is that company-related events can directly
affect the volatility of company shares. There is
a strong correlation between company events and
company shares. Table 4 also shows that the result
of individual stock prediction by using sector news
or all news does not achieve a good performance,
probably because there are many irrelevant events
in all news, which would reduce the performance
of our prediction model.
The fact that the accuracy of these well-known
stocks are higher than the index may be because
there is relatively more news events dedicated to
the relevant companies. To gain a better under-
standing of the behavior of the model on more
individual stocks, we randomly select 15 compa-
nies (i.e. Google Inc., Boeing Company, Wal-Mart
Stores, Nike Inc., QUALCOMM Inc., Apache Cor-
poration, Starbucks Corp., Avon Products, Visa
Inc., Symantec Corp., The Hershey Company,
Mattel Inc., Actavis plc, Gannett Co. and SanDisk
Corporation) from S&P 500 companies. More
specifically, according to the Fortune ranking of
S&P 500 companies
4
, we divide the ranked list
into five parts, and randomly select three compa-
nies from each part. The experimental results are
shown in Figure 4. We find that:
(1) All 15 individual stocks can be predicted
with accuracies above 50%, while 60% of the
stocks can be predicted with accuracies above
60%. It shows that the amount of company-related
events has strong relationship with the volatility of
4
http://money.cnn.com/magazines/fortune/fortune500/.
The amount of company-related news is correlated to the
fortune ranking of companies. However, we find that the
trade volume does not have such a correlation with the
ranking.
1421
S&P 500 Index Prediction
Individual Stock Prediction
Google Inc. Boeing Company Wal-Mart Stores
Accuracy MCC Accuracy MCC Accuracy MCC Accuracy MCC
dev 59.60% 0.1683 67.86% 0.4642 68.75% 0.4339 70.45% 0.4679
test 58.94% 0.1649 66.97% 0.4435 68.03% 0.4018 69.87% 0.4456
Table 5: Final experimental results on the test dataset
company shares.
(2) With decreasing company fortune rankings,
the accuracy and MCC decrease. This is mainly
because there is not as much daily news about low-
ranking companies, and hence one cannot extract
enough structured events to predict the volatility
of these individual stocks.
3.7 Final Results
The final experimental results on the test dataset
are shown in Table 5 (as space is limited, we show
the results on the time interval of one day only).
The experimental results on the development and
test datasets are consistent, which indicate that our
approach has good robustness. The following con-
clusions obtained from development experiments
also hold on the test dataset:
(1) Structured events are more useful represen-
tations compared to bags-of-words for the task of
stock market prediction.
(2) A deep neural network model can be more
accurate on predicting the stock market compared
to the linear model.
(3) Our approach can achieve stable experiment
results on S&P 500 index prediction and individ-
ual stock prediction over a large amount of data
(eight years of stock prices and more than 550,000
pieces of news).
(4) The quality of information is more impor-
tant than the quantity of information on the task
of stock market prediction. That is to say that the
most relevant information (i.e. news title vs news
content, individual company news vs all news) is
better than more, but less relevant information.
3.8 Analysis and Discussion
We use Figure 5 to demonstrate our analysis to
the development experimental result of Google
Inc. stock prediction, which directly shows the
relationship between structured events and the
stock market. The links between each layer show
the magnitudes of feature weights in the model
learned using the training set.
Three events, (Google, says bought stake in,
China?s XunLei), (Google, reveals stake in, Chi-
?1 ?2 ?3 ?4 ?5 ?6 ?7 ?8 ?M ?1: (Google, says bought stake in, China?s XunLei) ?4: (Google, reveals stake in, Chinese social website) ?6: (Capgemini, partners, Google apps software)  ?2: (Oracle, sues, Google) ?5: (Google map, break, privacy law) ?8: (Google, may pull out of, China) 
? 
? 
Figure 5: Prediction of Google Inc. (we only show
structured event features since backoff features are
less informative)
nese social website) and (Capgemini, partners,
Google apps software), have the highest link
weights to the first hidden node (from the left).
These three events indicate that Google constantly
makes new partners and expands its business area.
The first hidden node has high-weight links to
Class +1, showing that Google?s positive coopera-
tion can lead to the rise of its stock price.
Three other events, (Oracle, sues, Google),
(Google map, break, privacy law) and (Google,
may pull out of, China), have high-weight links
to the second hidden node. These three events
show that Google was suffering questions and
challenges, which could affect its reputation and
further pull down its earnings. Correspondingly,
the second hidden node has high-weight links to
Class -1. These suggest that our method can au-
tomatically and directly reveal complex relation-
ships between structured events and the stock mar-
ket, which is very useful for investors, and can fa-
cilitate the research of stock market prediction.
Note that the event features used in our predic-
tion model are generalized based on the algorithm
introduced in Section 2.5. Therefore, though a
specific event in the development test set might
have never happened, its generalized form can be
found in the training set. For example, ?Google
acquired social marketing company Wildfire In-
1422
teractive? is not in the training data, but ?Google
get class? (?get? is the class name of ?acquire?
and ?buy? in VerbNet) can indeed be found in the
training set, such as ?Google bought stake in Xun-
Lei? on 04/01/2007. Hence although the full spe-
cific event feature does not fire, its back-offs fire
for a correct prediction. For simplicity of showing
the event, we did not include back-off features in
Figure 5.
4 Related Work
Stock market prediction has attracted a great deal
of attention across the fields of finance, computer
science and other research communities in the
past. The literature of stock market prediction
was initiated by economists (Keynes, 1937). Sub-
sequently, the influential theory of Efficient Mar-
ket Hypothesis (EMH) (Fama, 1965) was estab-
lished, which states that the price of a security re-
flects all of the information available and that ev-
eryone has a certain degree of access to the infor-
mation. EMH had a significant impact on security
investment, and can serve as the theoretical basis
of event-based stock price movement prediction.
Various studies have found that financial news
can dramatically affect the share price of a se-
curity (Chan, 2003; Tetlock et al., 2008). Cul-
ter et al. (1998) was one of the first to investi-
gate the relationship between news coverage and
stock prices, since which empirical text analysis
technology has been widely used across numerous
disciplines (Lavrenko et al., 2000; Kogan et al.,
2009; Luss and d?Aspremont, 2012). These stud-
ies primarily use bags-of-words to represent finan-
cial news documents. However, as Schumaker and
Chen (2009) and Xie et al. (2013) point out, bag-
of-words features are not the best choice for pre-
dicting stock prices. Schumaker and Chen (2009)
extract noun phrases and named entities to aug-
ment bags-of-words. Xie et al. (2013) explore a
rich feature space that relies on frame semantic
parsing. Wang et al. (2014) use the same fea-
tures as Xie et al. (2013), but they perform non-
parametric kernel density estimation to smooth out
the distribution of features. These can be regarded
as extensions to the bag-of-word method. The
drawback of these approaches, as discussed in the
introduction, is that they do not directly model
events, which have structured information.
There has been efforts to model events more di-
rectly (Fung et al., 2002; Hayo and Kutan, 2005;
Feldman et al., 2011). Fung, Yu, and Lam (2002)
use a normalized word vector-space to model
event. Feldman et al. (2011) extract 9 prede-
fined categories of events based on heuristic rules.
There are two main problems with these efforts.
First, they cannot extract structured event (e.g. the
actor of the event and the object of the event). Sec-
ond, Feldman et al. (2011) can obtain only lim-
ited categories of events, and hence the scalabil-
ity of their work is not strong. In contrast, we
extract structured events by leveraging Open In-
formation Extraction technology (Open IE; Yates
et al. (2007); Etzioni et al. (2011); Faber et al.
(2011)) without predefined event types, which can
effectively solve the two problems above.
Apart from events, sentiment analysis is another
perspective to the problem of stock prediction
(Das and Chen, 2007; Tetlock, 2007; Tetlock et
al., 2008; Bollen et al., 2011; Si et al., 2013). Tet-
lock (2007) examines how qualitative information
(i.e. the fraction of negative words in a particular
news column) is incorporated in aggregate market
valuations. Tetlock, Saar-Tsechansky, and Mac-
skassy (2008) extend that analysis to address the
impact of negative words in all Wall Street Joural
(WSJ) and Dow Jones News Services (DJNS) sto-
ries about individual S&P500 firms from 1980 to
2004. Bollen and Zeng (2011) study whether the
large-scale collective emotion on Twitter is cor-
related with the volatility of Dow Jones Indus-
trial Average (DJIA). From the experimental re-
sults, they find that changes of the public mood
match shifts in the DJIA values that occur 3 to 4
days later. Sentiment-analysis-based stock mar-
ket prediction focuses on investigating the influ-
ence of subjective emotion. However, this paper
puts emphasis on the relationship between objec-
tive events and the stock price movement, and is
orthogonal to the study of subjectivity. As a result,
our model can be combined with the sentiment-
analysis-based method.
5 Conclusion
In this paper, we have presented a framework for
event-based stock price movement prediction. We
extracted structured events from large-scale news
based on Open IE technology and employed both
linear and nonlinear models to empirically investi-
gate the complex relationships between events and
the stock market. Experimental results showed
that events-based document representations are
1423
better than bags-of-words-based methods, and
deep neural networks can model the hidden and in-
directed relationship between events and the stock
market. For further comparisons, we freely release
our data at http://ir.hit.edu.cn/?xding/data.
Acknowledgments
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61202277, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design. We are very grate-
ful to Ji Ma for providing an implementation of the
neural network algorithm.
References
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI,
volume 7, pages 2670?2676.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends
R
? in Machine Learning,
2(1):1?127.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Werner FM Bondt and Richard Thaler. 1985. Does
the stock market overreact? The Journal of finance,
40(3):793?805.
Wesley S Chan. 2003. Stock price reaction to news and
no-news: Drift and reversal after headlines. Journal
of Financial Economics, 70(2):223?260.
David M Cutler, James M Poterba, and Lawrence H
Summers. 1998. What moves stock prices? Bern-
stein, Peter L. and Frank L. Fabozzi, pages 56?63.
George Cybenko. 1989. Approximation by superposi-
tions of a sigmoidal function. Mathematics of con-
trol, signals and systems, 2(4):303?314.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for
amazon: Sentiment extraction from small talk on the
web. Management Science, 53(9):1375?1388.
Xiao Ding, Bing Qin, and Ting Liu. 2013. Building
chinese event type paradigm based on trigger clus-
tering. In Proc. of IJCNLP, pages 311?319, Octo-
ber.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3?10. AAAI Press.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535?1545. Association for Computational
Linguistics.
Eugene F Fama. 1965. The behavior of stock-market
prices. The journal of Business, 38(1):34?105.
Ronen Feldman, Benjamin Rosenfeld, Roy Bar-Haim,
and Moshe Fresko. 2011. The stock sonarsentiment
analysis of stocks based on a hybrid approach. In
Twenty-Third IAAI Conference.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai
Lam. 2002. News sensitive stock trend prediction.
In Advances in Knowledge Discovery and Data Min-
ing, pages 481?493. Springer.
Bernd Hayo and Ali M Kutan. 2005. The impact of
news, oil prices, and global market developments
on russian financial markets1. Economics of Tran-
sition, 13(2):373?393.
Narasimhan Jegadeesh and Sheridan Titman. 1993.
Returns to buying winners and selling losers: Im-
plications for stock market efficiency. The Journal
of Finance, 48(1):65?91.
Narasimhan Jegadeesh. 1990. Evidence of predictable
behavior of security returns. The Journal of Fi-
nance, 45(3):881?898.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In ACL,
pages 254?262.
John Maynard Keynes. 1937. The general theory of
employment. The Quarterly Journal of Economics,
51(2):209?223.
Jaegwon Kim. 1993. Supervenience and mind: Se-
lected philosophical essays. Cambridge University
Press.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending verbnet with novel
verb classes. In Proceedings of LREC, volume 2006,
page 1.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
NAACL, pages 272?280, Boulder, Colorado, June.
Association for Computational Linguistics.
1424
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In KDD-
2000 Workshop on Text Mining, pages 37?44.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. of ACL (Volume 1: Long Papers),
pages 73?82, August.
Andrew W Lo and Archie Craig MacKinlay. 1990.
When are contrarian profits due to stock mar-
ket overreaction? Review of Financial studies,
3(2):175?205.
Ronny Luss and Alexandre d?Aspremont. 2012.
Predicting abnormal returns from news using text
classification. Quantitative Finance, pp.1?14,
doi:10.1080/14697688.2012.672762.
Burton G. Malkiel. 1973. A Random Walk Down Wall
Street. W. W. Norton, New York.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of the
sixth ACM international conference on Web search
and data mining, pages 255?264. ACM.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proc. of WWW, pages
909?918. ACM.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations
by error propagation. Technical report, DTIC Doc-
ument.
Robert P Schumaker and Hsinchun Chen. 2009.
Textual analysis of stock market prediction using
breaking financial news: The azfin text system.
ACM Transactions on Information Systems (TOIS),
27(2):12.
Ramesh Sharda and Dursun Delen. 2006. Predict-
ing box-office success of motion pictures with neu-
ral networks. Expert Systems with Applications,
30(2):243?254.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting
topic based twitter sentiment for stock prediction. In
Proc. of ACL (Volume 2: Short Papers), pages 24?
29, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than words: Quantifying
language to measure firms? fundamentals. The Jour-
nal of Finance, 63(3):1437?1467.
Paul C Tetlock. 2007. Giving content to investor sen-
timent: The role of media in the stock market. The
Journal of Finance, 62(3):1139?1168.
William Yang Wang and Zhenhao Hua. 2014. A
semiparametric gaussian copula regression model
for predicting financial risks from earnings calls. In
Proc. of ACL, June.
Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ?an G. Creamer. 2013. Semantic frames to pre-
dict stock price movement. In Proc. of ACL (Volume
1: Long Papers), pages 873?883, August.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information ex-
traction on the web. In Proc. of NAACL: Demonstra-
tions, pages 25?26. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
1425
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 296?305,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Bootstrapping Events and Relations from Text 
Ting Liu ILS, University at Albany, USA tliu@albany.edu 
Tomek Strzalkowski ILS, University at Albany, USA Polish Academy of Sciences tomek@albany.edu 
Abstract 
In this paper, we describe a new approach to semi-supervised adaptive learning of event extraction from text. Given a set of exam-ples and an un-annotated text corpus, the BEAR system (Bootstrapping Events And Relations) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text, such as events involving multiple entities and their roles. For example, given a series of descriptions of bombing and shooting inci-dents (e.g., in newswire) the system will learn to extract, with a high degree of accu-racy, other attack-type events mentioned elsewhere in text, irrespective of the form of description. A series of evaluations using the ACE data and event set show a signifi-cant performance improvement over our baseline system. 
1 Introduction We constructed a semi-supervised machine learning process that effectively exploits statisti-cal and structural properties of natural language discourse in order to rapidly acquire rules to de-tect mentions of events and other complex rela-tionships in text, extract their key attributes, and construct template-like representations. The learning process exploits descriptive and struc-tural redundancy, which is common in language; it is often critical for achieving successful com-munication despite distractions, different con-texts, or incompatible semantic models between a speaker/writer and a hearer/reader. We also take advantage of the high degree of referential consistency in discourse (e.g., as observed in word sense distribution by (Gale, et al1992), and arguably applicable to larger linguistic units), which enables the reader to efficiently correlate different forms of description across coherent spans of text.  The method we describe here consists of two steps: (1) supervised acquisition of initial extrac-tion rules from an annotated training corpus, and 
(2) self-adapting unsupervised multi-pass boot-strapping by which the system learns new rules as it reads un-annotated text using the rules learnt in the first step and in the subsequent learning passes. When a sufficient quantity and quality of text material is supplied, the system will learn many ways in which a specific class of events can be described. This includes the capability to detect individual event mentions using a system of context-sensitive triggers and to isolate perti-nent attributes such as agent, object, instrument, time, place, etc., as may be specific for each type of event. This method produces an accurate and highly adaptable event extraction that significant-ly outperforms current information extraction techniques both in terms of accuracy and robust-ness, as well as in deployment cost. 2 Learning by bootstrapping  As a semi-supervised machine learning method, bootstrapping can start either with a set of prede-fined rules or patterns, or with a collection of training examples (seeds) annotated by a domain expert on a (small) data set. These are normally related to a target application domain and may be regarded as initial ?teacher instructions? to the learning system. The training set enables the sys-tem to derive initial extraction rules, which are applied to un-annotated text data in order to pro-duce a much larger set of examples. The exam-ples found by the initial rules will occur in a variety of linguistic contexts, and some of these contexts may provide support for creating alter-native extraction rules. When the new rules are subsequently applied to the text corpus, addition-al instances of the target concepts will be identi-fied, some of which will be positive and some not. As this process continues to iterate over, the system acquires more extraction rules, fanning out from the seed set until no new rules can be learned.  Thus defined, bootstrapping has been used in natural language processing research, notably in word sense disambiguation (Yarowsky, 1995). Strzalkowski and Wang (1996) were first to demonstrate that the technique could be applied to adaptive learning of named entity extraction 
296
 Figure 1. Skeletal dependency structure representation of an event mention. 
rules. For example, given a ?na?ve? rule for iden-tifying company names in text, e.g., ?capitalized NP followed by Co.?, their system would first find a large number of (mostly) positive instanc-es of company names, such as ?Henry Kauffman Co.? From the context surrounding each of these instances it would isolate alternative indicators, such as ?the president of?, which is noted to oc-cur in front of many company names, as in ?The president of American Electric Automobile Co. ??. Such alternative indicators give rise to new extraction rules, e.g., ?president of + CNAME?. The new rules find more entities, including com-pany names that do not end with Co., and the process iterates until no further rules are found. The technique achieved a very high performance (95% precision and 90% recall), which encour-aged more research in IE area by using boot-strapping techniques. Using a similar approach, (Thelen and Riloff, 2002) generated new syntac-tic patterns by exploiting the context of known seeds for learning semantic categories.  In Snowball (Agichtein and Gravano, 2000 ) and Yangarber?s IE system (2000), bootstrapping technique was applied for extraction of binary relations, such as Organization-Location, e.g., between Microsoft and Redmond, WA. Then, Xu (2007) extended the method for more complex relations extraction by using sentence syntactic structure and a data driven pattern generation. In this paper, we describe a different approach on building event patterns and adapting to the dif-ferent structures of unseen events. 3 Bootstrapping applied to event learn-ing  Our objective in this project was to expand the bootstrapping technique to learn extraction of events from text, irrespective of their form of description, a property essential for successful adaptability to new domains and text genres. The major challenge in advancing from entities and binary relations to event learning is the complex-ity of structures involved that not only consist of multiple elements but their linguistic context may now extend well beyond a few surrounding words, even past sentence boundaries. These considerations guided the design of the BEAR system (Bootstrapping Events And Relations), which is described in this paper. 3.1 Event representation  An event description can vary from very concise, newswire-style to very rich and complex as may 
be found in essays and other narrative forms. The system needs to recognize any of these forms and to do so we need to distill each description to a basic event pattern. This pattern will capture the heads of key phrases and their dependency struc-ture while suppressing modifiers and certain oth-er non-essential elements. Such skeletal representations cannot be obtained with keyword analysis or linear processing of sentences at word level (e.g., Agichtein and Gravano, 2000), be-cause such methods cannot distinguish a phrase head from its modifier. A shallow dependency parser, such as Minipar (Lin, 1998), that recog-nizes dependency relations between words is quite sufficient for deriving head-modifier rela-tions and thus for construction of event tem-plates. Event templates are obtained by stripping the parse tree of modifiers while preserving the basic dependency structure as shown in Figure 1, which is a stripped down parse tree of, ?Also Monday, Israeli soldiers fired on four diplomatic vehicles in the northern Gaza town of Beit Hanoun, said diplomats? The model proposed here represents a signifi-cant advance over the current methods for rela-tion extraction, such as the SVO model (Yangarber, et al2000) and its extension, e.g., the chain model (Sudo, et al2001) and other related variants (Riloff, 1996) all of which lack the expressive power to accurately recognize and represent complex event descriptions and to sup-port successful machine learning. While Sudo?s subtree model (2003) overcomes some of the limitations of the chain models and is thus con-ceptually closer to our method, it nonetheless lacks efficiency required for practical applica-tions.  We represent complex relations as tree-like structures anchored at an event trigger (which is usually but not necessarily the main verb) with branches extending to the event attributes (which are usually named entities). Unlike the singular concepts (i.e., named entities such as ?person? or 
297
?location?) or linear relations (i.e., tuples such as ?Gates ? CEO ? Microsoft?), an event description consists of elements that form non-linear de-pendencies, which may not be apparent in the word order and therefore require syntactic and semantic analysis to extract. Furthermore, an ar-rangement of these elements in text can vary greatly from one event mention to the next, and there is usually other intervening material in-volved. Consequently, we construe event repre-sentation as a collection of paths linking the trigger to the attributes through the nodes of a parse tree1.  To create an event pattern (which will be part of an extraction rule), we generalize the depend-ency paths that connect the event trigger with each of the event key attributes (the roles). A dependency path consists of lexical and syntactic relations (POS and phrase dependencies), as well as semantic relations, such as entity tags (e.g., Person, Company, etc.) of event roles and word sense designations (based on Wordnet senses) of event triggers. In addition to the trigger-role paths (which we shall call the sub-patterns), an event pattern also contains the following: ? Event Type and Subtype ? which is inher-ited from seed examples; ? Trigger class ? an instance of the trigger must be found in text before any patterns are applied; ? Confidence score ? expected accuracy of the pattern established during training process; ? Context profile ? additional features col-lected from the context surrounding the event description, including references of other types of events near this event, in the same sentence, same paragraph, or ad-jacent paragraphs. We note that the trigger-attribute sub-patterns are defined over phrase structures rather than over linear text, as shown in Figure 2. In order to compose a complete event pattern, sub-patterns are collected across multiple mentions of the same-type event. 
                                                            1 Details of how to derive the skeletal tree representation are described in (Liu, 2009). 2 t ? the type of the event, w_pos ? the lemma of a word and its POS. 3 In this figure we omit the parse tree trimming step which was explained in the previous section. 
3.2 Designating the sense of event triggers  An event trigger may have multiple senses but only one of them is for the event representation. If the correct sense can be determined, we would be able to use its synonyms and hyponym as al-ternative event triggers, thus enabling extraction of more events. This, in turn, requires sense dis-ambiguation to be performed on the event trig-gers. In MUC evaluations, participating groups ( Yangarber and Grishman, 1998) used human experts to decide the correct sense of event trig-gers and then manually added correct synonyms to generalize event patterns. Although accurate, the process is time consuming and not portable to new domains. We developed a new approach for utilizing Wordnet to decide the correct sense of an event trigger. The method is based on the hypothesis that event triggers will share same sense when represent same type of event. For example, when the verbs, attack, assail, strike, gas, bomb, are trigger words of Conflict-Attack event, they share same sense. This process is described in the following steps: 1)  From training corpus, collect all triggers, which specify the lemma, POS tag, the type of event and get alpossible senses of them from Wordnet. 2)  Order the triggers by the trigger frequency TrF(t, w_pos),2 which is calculated by divid-ing number of times each word (w_pos) is used as a trigger for the event of type (t) by the total number of times this word occurs in the training corpus. Clearly, the greater trig-ger frequency of a word, the more discrimi-native it is as a trigger for the given type of event. When the senses of the triggers with high accuracy are defined, they can be the reference for the triggers in low accuracy. 3)  From the top of the trigger list, select the first none-sense defined trigger (Tr1) 4)  Again, beginning from the top of the trigger list, for every trigger Tr2 (other than Tr1), we look for a pair of compatible senses be-tween Tr1 and Tr2. To do so, traverse Syno-nym, Hypernym, and Hyponym links starting from the sense(s) of Tr2 (use either the sense already assigned to Tr2 if has or all its possi-ble senses) and see whether there are paths which can reach the senses of Tr1. If such converging paths exist, the compatible senses                                                             2 t ? the type of the event, w_pos ? the lemma of a word and its POS. 
Attacker:  <N(subj, PER): Attacker> <V(fire): trigger> Place:  <V(fire): trigger> <Prep> <N> <Prep(in)> <N(GPE): Place> Target:  <V(fire): trigger> <Prep(on)> <N(VEH): Target> Time-Within:<N(timex2): Time-Within><SentHead><V(fire): trigger> Figure 2. Trigger-attribute sub-patterns for key roles in a Conflict-Attack event pattern. 
298
are identified and assigned to Tr1 and Tr2 (if Tr2?s sense wasn?t assigned before). Then go back to step 3. However, if no such path ex-ist between Tr1 senses with other triggers senses, the first sense listed in Wordnet will be assigned to Tr1 This algorithm tries to assign the most proper sense to every trigger for one type of event. For example, the sense of fire as trigger of Conflict-Attack event is ?start firing a weapon?; while it is used in Personal-End_Position, its sense is ?ter-minate the employment of?. After the trigger sense is defined, we can expand event triggers by adding their synonyms and hyponyms during the event extraction. 3.3 Deriving initial rules from seed exam-ples  Extraction rules are construed as transformations from the event patterns derived from text onto a formal representation of an event. The initial rules are derived from a manually annotated training text corpus (seed data), supplied as part of an application task. Each rule contains the type of events it extracts, trigger, a list of role sub-patterns, and the confidence score obtained through a validation process (see section 3.6). Figure 3 shows an extraction pattern for the Con-flict-Attack event derived from the training cor-pus (but not validated yet)3.  3.4 Learning through pattern mutation  Given an initial set of extraction rules, a variety of pattern mutation techniques are applied to de-rive new patterns and new rules. This is done by selecting elements of previously learnt patterns, based on the history of partial matches and com-bining them into new patterns. This form of learning, which also includes conditional rule                                                             3 In this figure we omit the parse tree trimming step which was explained in the previous section. 
relaxation, is particularly useful for rapid adapta-tion of extraction capability to slightly altered, partly ungrammatical, or otherwise variant data.  The basic idea is as follows: the patterns ac-quired in prior learning iterations (starting with those obtained from the seed examples) are matched against incoming text to extract new events. Along the way there will be a number of partial matches, i.e., when no existing pattern fully matches a span of text. This may simply mean that no event is present; however, depend-ing upon the degree of the partial match we may also consider that a novel structural variant was found. BEAR would automatically test this hy-pothesis by attempting to construe a new pattern, out of the elements of existing patterns, in order to achieve a full match. If a match is achieved, the new ?mutated? pattern will be added to BEAR learned collection, subject to a validation step. The validation step (discussed later in this paper) is to assure that the added pattern would not introduce an unacceptable drop in overall system precision. Specific pattern mutation tech-niques include the following: ? Adding a role subpattern: When a pattern matches an event mention while there is a sufficient linguistic evidence (e.g., pres-ence of certain types of named entities) that additional roles may be present in text, then appropriate role subpatterns can be "imported" from other, non-matching patterns (Figure 4). ?  Replacing a role subpattern: When a pat-tern matches but for one role, the system can replace this role subpattern by another subpattern for the same role taken from a different pattern for the same event type. ?  Adding or replacing a trigger: When a pattern matches but for the trigger, a new trigger can be added if it either is already present in another pattern for the same event type or the syno-nym/hyponym/hypernym of the trigger (found in section 3.2). We should point out that some of the same ef-fects can be obtained by making patterns more general, i.e., adding "optional" attributes (i.e., optional sub-patterns), etc. Nonetheless, the pat-tern mutation is more efficient because it will automatically learn such generalization on an as-needed basis in an entirely data-driven fashion, while also maintaining high precision of the re-sulting pattern set. It is thus a more general method. Figure 4 illustrated the use of the ele-ments combination technique. In this example, 
 Figure 3. A Conflict-Attack event pattern derived from a positive example in the training corpus 
299
 Figure 5. A new extraction pattern is derived by iden-tifying an alternative trigger for an event. 
Pattern ID: 1286 Type: Conflict   Subtype: Attack Trigger:  attack_N Target:  <N(FAC): Target> <Prep(in)> <N(attack): trigger> Attacker:  <N(PER): Attacker> <V> <N> <Prep> <N> <Prep(in)> <N(attack): trigger> Time-Within: <N(attack): trigger> <E0> <V> <N(timex2): Time-within> Figure 5B. A new pattern is derived for event in Fig 5, with an attack as the trigger. 
Pattern ID: 1207 Type: Conflict    Subtype: Attack Trigger:  bombing_N Target:  <N(bombing): trigger> <Prep(of)> <N(FAC): Target>  Attacker:  <N(PER): Attacker> <V> <N(bombing): trigger>  Time-Within: <N(bombing): trigger> <Prep> <N> <Prep> <N> <E0> <V> <N(timex2): Time-within> Figure 5A. A pattern with the bombing trigger matches the event mention in Fig. 5. 
 Figure 4. Deriving a new pattern by importing a role from another pattern neither of the two existing patterns can fully match the new event description; however, by combining the first pattern with the Place role sub-pattern from the second pattern we obtain a new pattern that fully matches the text. While this adjustment is quite simple, it is nonetheless performed automatically and without any human assistance. The new pattern is then ?learned? by BEAR, subject to a verification step explained in a later section. 3.5 Learning by exploiting structural duali-ty  As the system reads through new text extracting more events using already learnt rules, each ex-tracted event mention is analyzed for presence of alternative trigger elements that can consistently predict the presence of a subset of events that includes the current one. Subsequently, an alter-native sub-pattern structure will be built with branches extending from the new trigger to the already identified attributes, as shown schemati-cally in Figure 5.  In this example, a Conflict-Attack-type event is extracted using a pattern (shown in Figure 5A) anchored at the ?bombing? trigger. Nonetheless, an alternative trigger structure is discovered, which is anchored at ?an attack? NP, as shown on the right side of Figure 5. This ?discovery? is based upon seeing the new trigger repeatedly ? it needs to ?explain? a subset of previously seen events to be adopted. The new trigger will prompt BEAR to derive additional event pat-terns, by computing alternative trigger-attribute paths in the dependency tree. The new pattern 
(shown in Figure 5B) is of course subject to con-fidence validation, after which it will be immedi-ately applied to extract more events.  Another way of getting at this kind of struc-tural duality is to exploit co-referential con-sistency within coherent spans of discourse, e.g., a single news article or a similar document. Such documents may contain references to multiple events, but when the same type of event is men-tioned along with the same attributes, it is more likely than not in reference to the same event.  This hypothesis is a variant of an argument ad-vanced in (Gale, et al2000) that a polysemous word used multiple times within a single docu-ment, is consistently used in the same sense. So if we extract an event mention (of type T) with trigger t in one part of a document, and then find that t occurs in another part of the same docu-ment, then we may assume that this second oc-currence of t has the same sense as the first. Since t is a trigger for an event of type T, we can hypothesize its subsequent occurrences indicate additional mentions of type T events that were not extracted by any of the existing patterns. Our objective is to exploit these unextracted mentions and then automatically generate additional event patterns. Indeed, Ji (2008) showed that trigger co-occurrence helps finding new mentions of the 
300
Pattern ID: -1 Type: Personnel  Subtype: End-Position Trigger: resign_V Person: <N(PER, subj): Person> <V(resign): trigger> Entity: <V(resign):trigger> <E0> <N(ORG): Entity> <N> <V> Figure 7A. A new pattern for End-Position learned by exploiting event co-reference. 
 Figure 7. Two event mentions have different triggers and sub-patterns structures  
 Figure 6. The probability of a sentence containing a mention of the same type of event within a single document same event; however, we found that if using enti-ty co-reference as another factor, more new men-tions could be identified when the trigger has low projected accuracy (Liu, 2009; Yu Hong, et al2011). Our experiments (Figure 64), which com-pared the triggers and the roles across all event mentions within each document on ACE training corpus, showed that when the trigger accuracy is 0.5 or higher, each of its occurrences within the document indicates an event mention of the same type with a very high probability (mostly > 0.9). For triggers with lower accuracy, this high prob-ability is only achieved when the two mentions share at least 60% of their roles, in addition to having a common trigger. Thus our approach uses co-occurrence of both trigger and event ar-gument for detecting new event mentions.  In Figure 7, an End-Position event is extracted from left sentence (L), with ?resign? as the trig-ger and ?Capek? and ?UBS? assigned Person and Entity roles, respectively5. The right sentence (R), taken from the same document, contains the same trigger word, ?resigned? and also the same 
                                                            4 The X-axis is the percentage of entities coreferred between the EVMs (Event mentions) and the SEs (Sentences); while the Y-axis shows the probability that the SE contains a men-tion that is the same type as the EVM. 5 Entity is the employer in the event 
entities, ?Howard G. Capek? and ?UBS?. The projected accuracy of resign_V as an End-Position trigger is 0.88. With 100% argument overlap rate, we estimate the probability that sen-tence R contains an event mention of the same type as sentence L (and in fact co-referential mention) at 97% (We set 80% as the threshold). Thus a new event mention is found and a new pattern for End-Position is automatically derived from R, as shown in Figure 7A. 3.6 Pattern validation  Extraction patterns are validated after each learn-ing cycle against the already annotated data. In the first supervised learning step, patterns accu-racy is tested against the training corpus based on the similarity between the extracted events and human annotated events:  ? A Full match is achieved when the event type is correctly identified and all its roles are correctly matched. A full credit is added to the pattern score. ? A Partial match is achieved when the event type is correctly identified but only a subset of roles is correctly extracted. A partial score, which is the ratio of the matched roles to the whole roles, is add-ed. ? A False Alarm occurs when a wrong type of event is extracted (including when no event is present in text). No credit is add-ed to the pattern score. In the subsequent steps, the validation is ex-tended over parts of the unannotated corpus. In Riloff (1996) and Sudo et al(2001), the pattern accuracy is mainly dependent on its occurrences in the relevant documents6 vs. the whole corpus. However, one document may contain multiple types of events, thus we set a more restricted val-idation measure on new rules: ? Good Match If a new rule ?rediscovers? already extracted events of the same type, then it will be counted as either a Full Match or Partial Match based on previ-ous rules ? Possible Match If an already extracted event of same type of a rule contains same entities and trigger as the candidate extracted by the rule. This candidate is a possible match, so it will get a partial 
                                                            6 If a document contains same type of events extracted from previous steps, the document is a relevant document to the pattern. 
301
Victim pattern: <N(obj, PER): Victim> <V(kill): trigger> (Life-Die) Projected Accuracy: 0.9390243902439024 Number of negative matches: 5 Number of Positive matches: 77  Attacker pattern: <N(subj, PE/PER/ORG): Attacker> <V> <V(use): trigger>  (Conflict-Attack) Projected Accuracy: 0.025210084033613446  Number of negative matches: 116  Number of positive matches: 3  Attacker pattern: <N(subj, GPE/PER): Attacker> <V(attack): trig-ger>  (Conflict-Attack) Projected Accuracy: 0.4166666666666667  Number of negative matches: 7  Number of positive matches: 5 categories of posi-tive matches: GPE: 4  GPE_Nation: 4  PER: 1 PER_Individual: 1 categories of nega-tive matches: GPE: 1  GPE_Nation: 1  PER: 6  PER_Group: 1 PER_Individual: 5 Figure 9. sub-patterns with projected accuracy scores 
Event id: 27 from: sample Projected Accuracy: 0.1765 Adjusted Projected Accuracy: 0.91 Type: Justice Subtype: Arrest-Jail Trigger: capture Person sub-pattern:  <N(obj, PER): Person> <V(capture): trigger> Co-occurrence ratio: {para_Conflict_Demonstrate=100%,  ?} Mutually exclusive ratio: {sent_Conflict_Attack=100%, pa-ra_Conflict_Attack=96.3%,  ?} Figure 8. An Arrest-Jail pattern with context profile information score based on the statistics result from Figure 6. ? False Alarm If a new rule picks up an al-ready extracted event in different type Thus, event patterns are validated for overall expected precision by calculating the ratio of positive matches to all matches against known events. This produces pattern confidence scores, which are used to decide if a pattern is to be learned or not. Learning only the patterns with sufficiently high confidence scores helps to guard the bootstrapping process from spinning off track; nonetheless, the overall objective is to maximize the performance of the resulting set of extraction rules, particularly by expanding its recall rate. For the patterns where the projected accuracy score falls under the cutoff threshold, we may still be able to make some ?repairs? by taking into account their context profile. To do so, we applied a similar approach as (Liao, 2010), which showed that some types of events can appeared frequently with each other. We collected all the matches produced by such a failed pattern and created a list of all other events that occur in their immediate vicinity: in the same sentence, as well as the sentences before and after it7. These other events, of different types and detected by differ-ent patterns, may be seen as co-occurring near the target event: these that co-occur near positive matches of our pattern will be added to the posi-tive context support of this pattern; conversely, events co-occurring near false alarms will be added to the negative context support for this pattern. By collecting such contextual infor-mation, we can find contextually-based indica-tors and non-indicators for occurrence of event mentions. When these extra constraints are in-cluded in a previously failed pattern, its projected 
                                                            7 If a known event is detected in the same sentence (sent_?), the same paragraph (para_?), or an adjacent paragraph (adj_para_...) as the candidate event, it be-comes an element of the pattern context support. 
accuracy is expected to increase, in some cases above the threshold.  For example, the pattern in Figure 8 has an in-itially low projected accuracy score; however, we find that positive matches of this pattern show a very high (100% in fact) degree of correlation with mentions of Demonstrate events. Therefore, limiting the application of this pattern to situa-tions where a Justice-Arrest-Jail event is men-tioned in a nearby text improves its projected accuracy to 91%, which is well above the re-quired threshold.  In addition to the confidence rate of each new pattern, we also calculate projected accuracy of each of the role sub-patterns, because they may be used in the process of detecting new patterns, and it will be necessary to score partial matches, as a function confidence weights for pattern components. To validate a sub-pattern we apply it to the training corpus and calculate its project-ed accuracy score by dividing the number of cor-rectly matched roles by the total number of matches returned. The projected accuracy score will tell us how well a sub-pattern can distin-guish a specific event role from other infor-mation, when used independently from other elements of the complete pattern. Figure 9 shows three sub-pattern examples. The first sub-pattern extracts the Victim role in a Life-Die event with very high projected accuracy. This sub-pattern is also a good candidate for generations of additional patterns for this type of event, a process which we describe in section D. The second sub-pattern was built to extract the Attacker role in Conflict-Attack events, but it has very low projected accuracy. The third one shows another Attacker sub-pattern whose pro-jected accuracy score is 0.417 after the first step 
302
 Figure 10. BEAR cross-validated scores 
Table 1. Sub-patterns whose projected accuracy is significantly increased after noisy samples are removed Sub-patterns Projected Accuracy Additional con-straints Revised Accu-racy Movement-Transport: <N(obj, PER/VEH): Artifact> <V(send): trigger> 0.475 removing PER 0.667 <V(bring): trigger> <N(obj)> <Prep = to> <N(FAC/GPE): Destina-tion> 0.375 removing GPE 1.0 ?    Conflict Attack: <N(PER/ORG/GPE):Attacker><N(attack):trigger> 0.682 removing PER 0.8 <N(subj,GPE/PER):Attacker><V(attack): trigger> 0.417 removing GPE 0.8 <N(obj,VEH/PER/FAC):Target><V(target):trigger> 0.364 removing PER_Individual 0.667 ?     
 Figure 11. BEAR?s unsupervised learning curve. 
in validation process. This is quite low; however, it can be repaired by constraining its entity type to GPE. This is because we note that with a GPE entity, the subpattern is 80% on target, while with PER entity it is 85% a false alarm. After this sub-pattern is restricted to GPE its projected accuracy becomes 0.8. Table 1 lists example sub-patterns for which the projected accuracy increases significantly after adding more constrains. When the projected accuracy of a sub-pattern is improved, all pat-terns containing this sub-pattern will also im-prove their projected accuracy. If the adjusted projected accuracy rises above the predefined threshold, the repaired pattern will be saved.  In the following section, we will discuss the experiments conducted to evaluate the perfor-mance of the techniques underlying BEAR: how effectively it can learn and how accurately it can perform its extraction task. 4 Evaluation  We test the system learning effectiveness by comparing its performance immediately follow-ing the first iteration (i.e., using rules derived from the training data) with its performance after N cycles of unsupervised learning. We split ACE training corpus 8  randomly into 5 folders and trained BEAR on the four folders and evaluated it on the left one. Then, we did 5 fold cross vali-dation. Our experiments showed that BEAR 
                                                            8 ACE training data contains 599 documents from news, weblog, usenet, and conversational telephone speech. Total 33 types of events are defined in ACE corpus.  
reached the best cross-validated score, 66.72%, when pattern accuracy threshold is set at 0.5. The highest score of single run is 67.62%. In the fol-lowing of this section, we will use results of one single run to display the learning behavior of BEAR.  In Figure 10, X-axis shows values of the learning threshold (in descending order), while Y-axis is the average F-score achieved by the automatically learned patterns for all types of events against the test corpus. The red (lower) line represents BEAR?s base run immediately after the first iteration (supervised learning step); the blue (upper) line represents BEAR?s perfor-mance after an additional 10 unsupervised learn-ing cycles9 are completed. We note that the final performance of the bootstrapped system steadily increases as the learning threshold is lowered, peaking at about 0.5 threshold value, and then declines as the threshold value is further de-creased, although it remains solidly above the base run. Analyzing more closely a few selected points on this chart we note, for example, that the base run at threshold of 0 has F-score of 34.5%, which represents 30.42% recall, 40% precision. On the other end of the curve, at the threshold of 0.9, the base run precision is 91.8% but recall at only 21.5%, which produces F-score of 34.8%. It is interesting to observe that at neither of these two extremes the system learning effectiveness is particularly good, and is significantly less than at 
                                                            9 The learning process for one type of event will stop when no new patterns can be generated, so the number of learning cycles for each event type is different. The highest number of learning cycles is 10 and lowest one is 2. 
303
Table 2. BEAR performance following different selections of learning steps  Precision Recall F-score Base1 0.89 0.22 0.35 Base2 0.87 0.28 0.42 All 0.84 0.56 0.67 PMM 0.84 0.48 0.61 CBM 0.86 0.37 0.52  
 Figure 13. Event mention extraction after learning: recall for each type of event 
 Figure 12. Event mention extraction after learning: preci-sion for each type of event 
the median threshold of 0.5 (based on the exper-iments conducted thus far), where the system performance improves from 42% to 66.86% F-score, which represents 83.9% precision and 55.57% recall.   Figure 11 explains BEAR?s learning effec-tiveness at what we determined empirically to be the optimal confidence threshold (0.5) for pattern acquisition. We note that the performance of the system steadily increases until it reaches a plat-eau after about 10 learning cycles.  Figure 12 and Figure 13 show a detailed breakdown of BEAR extraction performance after 10 learning cycles for different types of events. We note that while precision holds steady across the event types, recall levels vary signifi-cantly. The main reason for low recall in some types of events is the failure to find a sufficient number of high-confidence patterns. This may point to limitations of the current pattern discov-ery methods and may require new ways of reach-ing outside of the current feature set. In the previous section we described several learning methods that BEAR uses to discover, validate and adapt new event extraction rules. Some of them work by manipulating already learnt patterns and adapting them to new data in order to create new patterns, and we shall call these pattern-mutation methods (PMM). Other described methods work by exploiting a broader linguistic context in which the events occur, or context-based methods (CBM). CB methods look for structural duality in text surrounding the events and thus discover alternative extraction patterns.  In Table 2, we report the results of running BEAR with each of these two groups of learning methods separately and then in combination to 
see how they contribute to the end performance. Base1 and Base2 showed the result without and with adding trigger synonyms in event extrac-tion. By introducing trigger synonyms, 27% more good events were extracted at the first it-eration and thus, BEAR had more resources to use in the unsupervised learning steps.  The ALL is the combination of PMM and CBM, which demonstrate both methods have the contribution to the final results. Furthermore, as explained before, new extraction rules are learned in each iteration cycle based on what was learned in prior cycles and that new rules are adopted only after they are tested for their pro-jected accuracy (confidence score), so that the overall precision of the resulting rule set is main-tained at a high level relative to the base run. 5 Conclusion and future work  In this paper, we presented a semi-supervised method for learning new event extraction pat-terns from un-annotated text. The techniques de-scribed here add significant new tools that increase capabilities of information extraction technology in general, and more specifically, of systems that are built by purely supervised meth-ods or from manually designed rules. Our eval-uation using ACE dataset demonstrated that that bootstrapping can be effectively applied to learn-ing event extraction rules for 33 different types of events and that the resulting system can out-perform supervised system (base run) significant-ly.  Some follow-up research issues include: ? New techniques are needed to recognize event descriptions that still evade the cur-rent pattern derivation techniques, espe-cially for the events defined in Personnel, Business, and Transactions classes. ? Adapting the bootstrapping method to ex-tract events in a different language, e.g. Chinese or Arabic. ? Expanding this method to extraction of larger ?scenarios?, i.e., groups of correlat-ed events that form coherent ?stories? of-ten described in larger sections of text, e.g., an event and its immediate conse-quences. 
304
References  Agichtein, E. and Gravano, L. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries  Gale, W. A., Church, K. W., and Yarowsky, D. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Lan-guage, 233-237. Harriman, New York: Asso-ciation for Computational Linguistics.  Hong, Y., Zhang, J., Ma, B., Yao, J., Zhou, G., and Zhu, Q,. 2011. Using Cross-Entity Infer-ence to Improve Event Extraction. In Proceed-ings of the Annual Meeting of the Association of Computational Linguistics (ACL 2011). Portland, Oregon, USA. Ji, H. and Grishman, R. 2008. Refining Event Extraction Through Unsupervised Cross-document Inference. In Proceedings of the Annual Meeting of the Association of Compu-tational Linguistics (ACL 2008).Ohio, USA. Liao, S. and Grishman R. 2010. Using Document Level Cross-Event Inference to Improve Event Extraction. In Proc. ACL-2010, pages 789-797, Uppsala, Sweden, July. Lin, D. 1998. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing System, Granada, Spain. Liu Ting. 2009. BEAR: Bootstrap Event and Re-lations from Text. Ph.D. Thesis Riloff, E. 1996. Automatically Generating Ex-traction Patterns from Untagged Text. In Pro-ceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044?1049. The AAAI Press/MIT Press. Sudo, K., Sekine, S., Grishman, R. 2001. Auto-matic Pattern Acquisition for Japanese Infor-mation Extraction. In Proceedings of Human Language Technology Conference (HLT2001). Sudo, K., Sekine, S., Grishman, R. 2003. An im-proved extraction pattern representation model for automatic IE pattern acquisition. Proceed-ings of ACL 2003 , 224 ? 231. Tokyo. Strzalkowski, T., and Wang, J. 1996. A self-learning universal concept spotter. In Proceed-ings of the 16th conference on Computational linguistics - Volume 2, 931-936, Copenhagen, Denmark: Association for Computational Lin-guistics 
Thelen, M., Riloff, E. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10. 214-222. Morristown, NJ: Association for Computational Linguistics    Xu, F., Uszkoreit, H., &amp; Li, H. (2007). A seed-driven bottom-up machine learning framework for extracting relations of various complexity. In Proc. of the 45th Annual Meet-ing of the Association of Comp. Linguistics, pp. 584?591, Prague, Czech Republic. Yangarber, R., and Grishman, R. 1998. NYU: Description of the Proteus/PET System as Used for MUC-7 ST. In Proceedings of the 7th conference on Message understanding. Yangarber, R., Grishman, R., Tapanainen, P., and Huttunen, S.  2000. Unsupervised discov-ery of scenario-level patterns for information extraction. In Proceedings of the Sixth Confer-ence on Applied Natural Language Pro-cessing, (ANLP-NAACL 2000), 282-289  Yarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, 189-196, Cambridge, Massachusetts: Associa-tion for Computational Linguistics    
305
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 588?597,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Type-Supervised Domain Adaptation for Joint Segmentation and
POS-Tagging
Meishan Zhang
?
, Yue Zhang
?
, Wanxiang Che
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
We report an empirical investigation on
type-supervised domain adaptation for
joint Chinese word segmentation and
POS-tagging, making use of domain-
specific tag dictionaries and only un-
labeled target domain data to improve
target-domain accuracies, given a set of
annotated source domain sentences. Pre-
vious work on POS-tagging of other lan-
guages showed that type-supervision can
be a competitive alternative to token-
supervision, while semi-supervised tech-
niques such as label propagation are
important to the effectiveness of type-
supervision. We report similar findings
using a novel approach for joint Chinese
segmentation and POS-tagging, under a
cross-domain setting. With the help of un-
labeled sentences and a lexicon of 3,000
words, we obtain 33% error reduction in
target-domain tagging. In addition, com-
bined type- and token-supervision can lead
to improved cost-effectiveness.
1 Introduction
With accuracies of over 97%, POS-tagging of
WSJ can be treated as a solved problem (Man-
ning, 2011). However, performance is still well
below satisfactory for many other languages and
domains (Petrov et al., 2012; Christodoulopoulos
et al., 2010). There has been a line of research on
using a tag-dictionary for POS-tagging (Merialdo,
1994; Toutanova and Johnson, 2007; Ravi and
Knight, 2009; Garrette and Baldridge, 2012). The
idea is compelling: on the one hand, a list of lex-
icons is often available for special domains, such
as bio-informatics; on the other hand, compiling a
?
Corresponding author.
lexicon of word-tag pairs appears to be less time-
consuming than annotating full sentences.
However, success in type-supervised POS-
tagging turns out to depend on several subtle fac-
tors. For example, recent research has found that
the quality of the tag-dictionary is crucial to the
success of such methods (Banko and Moore, 2004;
Goldberg et al., 2008; Garrette and Baldridge,
2012). Banko and Moore (2004) found that the
accuracies can drop from 96% to 77% when a
hand-crafted tag dictionary is replaced with a raw
tag dictionary gleaned from data, without any hu-
man intervention. These facts indicate that careful
considerations need to be given for effective type-
supervision. In addition, significant manual work
might be required to ensure the quality of lexicons.
To compare type- and token-supervised tagging,
Garrette and Baldridge (2013) performed a set of
experiments by conducting each type of annota-
tion for two hours. They showed that for low-
resource languages, a tag-dictionary can be rea-
sonably effective if label propagation (Talukdar
and Crammer, 2009) and model minimizations
(Ravi and Knight, 2009) are applied to expand and
filter the lexicons. Similar findings were reported
in Garrette et al. (2013).
Do the above findings carry over to the Chi-
nese language? In this paper, we perform an
empirical study on the effects of tag-dictionaries
for domain adaptation of Chinese POS-tagging.
We aim to answer the following research ques-
tions: (a) Is domain adaptation feasible with only
a target-domain lexicon? (b) Can we further im-
prove type-supervised domain adaptation using
unlabeled target-domain sentences? (c) Is craft-
ing a tag dictionary for domain adaptation more
effective than manually annotating target domain
sentences, given similar efforts?
Our investigations are performed under two
Chinese-specific settings. First, unlike low-
resource languages, large amounts of annotation
588
are available for Chinese. For example, the Chi-
nese Treebank (CTB) (Xue et al., 2005) contains
over 50,000 manually tagged news sentences.
Hence rather than studying purely type-supervised
POS-tagging, we make use of CTB as the source
domain, and study domain adaptation to the Inter-
net literature.
Second, one uniqueness of Chinese POS-
tagging, in contrast to the POS-tagging of alpha-
betical languages, is that word segmentation can
be performed jointly to avoid error propagation
(Ng and Low, 2004; Zhang and Clark, 2008; Kru-
engkrai et al., 2009; Zhang and Clark, 2010). We
adopt this approach for a strong baseline. Previous
studies showed that unsupervised domain adap-
tation can give moderate improvements (Liu and
Zhang, 2012). We show that accuracies can be
much more significantly improved by using target-
domain knowledge in the form of lexicons.
Both token-supervised and type-supervised do-
main adaptation rely on a set of source-domain
annotations; while the former makes additional
use of a small set of target annotations, the lat-
ter leverages a target-domain lexicon. We take
a feature-based method, analogous to that of
Daume III (2007), which tunes domain-dependent
versions of features using domain-specific data.
Our method tunes a set of lexicon-based features,
so that domain-dependent models are derived from
inserting domain-specific lexicons.
The conceptually simple method worked highly
effectively on a test set of 1,394 sentences from
the Internet novel ?Zhuxian?. Combined with
the use of unlabeled data, a tag lexicon of 3,000
words gave a 33% error reduction when com-
pared with a strong baseline system trained using
CTB data. We observe that joint use of type- and
token-supervised domain adaptation is more cost-
effective than pure type- or token-supervision.
With 10 hours of annotation, the best error reduc-
tion reaches 47%, with F-score increasing from
80.81% to 89.84%.
2 Baseline
We take as the baseline system a discriminative
joint segmentation and tagging model, proposed
by Zhang and Clark (2010), together with simple
self-training (Liu and Zhang, 2012). While the
baseline discriminative model gives state-of-the-
art joint segmentation and tagging accuracies on
CTB data, the baseline self-training makes use of
unlabeled target domain data to find improved tar-
get domain accuracies over bare CTB training.
2.1 The Baseline Discriminative Chinese
POS-Tagging Model
The baseline discriminative model performs
segmentation and POS-tagging simultaneously.
Given an input sentence c
1
? ? ? c
n
(c
i
refers to the
ith character in the sentence), it operates incre-
mentally, from left to right. At each step, the cur-
rent character can either be appended to the last
word of the existing partial output, or seperated as
the start of a new word with tag p. A beam is used
to maintain the N-best partial results at each step
during decoding. At step i (0 ? i < n), each
item in the beam corresponds to a segmentation
and POS-tagging hypothesis for the first i?1 char-
acters, with the last word being associated with a
POS, but marked as incomplete. When the next
character c
i
is processed, it is combined with all
the partial results from the beam to generate new
partial results, using two types of actions: (1) Ap-
pend, which appends c
i
to the last (partial) word
in a partial result; (2) Separate(p), which makes
the last word in the partial result as completed and
adds c
i
as a new partial word with a POS tag p.
Partial results in the beam are scored globally
over all actions used to build them, so that the N-
best can be put back to the agenda for the next step.
For each action, features are extracted differently.
We use the features from Zhang and Clark (2010).
Discriminative learning with early-update (Collins
and Roark, 2004; Zhang and Clark, 2011) is used
to train the model with beam-search.
2.2 Baseline Unsupervised Adaptation by
Self-Training
A simple unsupervised approach for POS-tagging
with unlabeled data is EM. For a generative model
such as HMM, EM can locally maximize the like-
lihood of training data. Given a good start, EM
can result in a competitive HMM tagging model
(Goldberg et al., 2008).
For discriminative models with source-domain
training examples, an initial model can be trained
using the source-domain data, and self-training
can be applied to find a locally-optimized model
using raw target domain sentences. The training
process is sometimes associated with the EM al-
gorithm. Liu and Zhang (2012) used perplexities
of character trigrams to order unlabeled sentences,
and applied self-training to achieve a 6.3% error
589
Common
Lexicon
Source
Lexicon
Source
Corpus
Training
Model
Target
Sentences
Target
Lexicon
Common
Lexicon
Tagging
Tagging
Results
Training Tagging
Figure 1: Architecture of our lexicon-based model for domain adaptation.
reduction on target-domain data when compared
with source domain training. Their method is sim-
ple to implement, and we take it as our baseline.
3 Type-Supervised Domain Adaptation
To give a formal definition of the domain adap-
tation tasks, we denote by C
s
a set of anno-
tated source-domain sentences, C
t
a set of anno-
tated target-domain sentences, and L
t
an anno-
tated target-domain lexicon. The form of L
t
is a
list of target-domain words, each associated with
a set of POS tags. Token-supervised domain adap-
tation is the task of making use of C
s
and C
t
to
improve target-domain performances, while type-
supervised domain adaptation is to make use of C
s
and L
t
instead for the same purpose.
As described in the introduction, type-
supervised domain adaptation is useful when
annotated sentences are absent, but lexicons are
available. In addition, it is an interesting question
which type of annotation is more cost-effective
when neither is available. We empirically com-
pare the two approaches by proposing a novel
method for type-supervised domain adaptation of
a discriminate tagging model, showing that it can
be a favourable choice in practical situation.
In particular, we split Chinese words into
domain-independent and domain-specific cate-
gories, and define unlexicalized features for
domain-specific words. We train lexicalized
domain-independent and unlexicalized domain-
specific features using the source domain anno-
tated sentences and a source-domain lexicon, and
then apply the resulting model to the target do-
main by replacing the source-domain lexicon with
a target domain lexicon. Combined with unsu-
pervised learning with unlabeled target-domain
of sentences, the conceptually simple method
worked highly effectively. Following Garrette and
Baldridge (2013), we address practical questions
on type-supervised domain adaptation by compar-
ison with token-supervised methods under similar
human annotation efforts.
3.1 System Architecture
Our method is based on the intuition that domain-
specific words of certain types (e.g. proper names)
can behave similarly across domains. For exam-
ple, consider the source-domain sentence ???
?|NR (Jiang Zemin) ??|AD (afterwards) ?
?|VV (visit) ??|NR (Shanghai Automobiles
Corp.)? and the target-domain sentence ??
?|NR (Biyao) ??|AD (afterwards) ??|VV
(arrive) ???|NR (the Bamboo Mountains)?.
???? (Jiang Zemin)? and ??? (Biyao)? are
person names in the two domains, respectively,
whereas ??? (Shanghai Automobiles Corp.)?
and ???? (the Bamboo Mountains)? are loca-
tion names in the two domains, respectively. If the
four words are simply treated as domain-specific
nouns, the two sentences both have the pattern
??domain-NR? AD VV ?domain-NR??, and hence
source domain training data can be useful in train-
ing the distributions of the lexicon-based features
for both domains.
Further, we assume that the syntax structures
and the usage of function words do not vary sig-
nificantly across domains. For example, verbs, ad-
jectives or proper nouns can be different from do-
main to domain, but the subject-verb-object sen-
tence structure does not change. In addition, the
usage of closed-set function words remains sta-
ble across different domains. In the CTB tagset,
closed-set POS tags are the vast majority. Under
this assumption, we introduce a set of unlexical-
ized features into the discriminative model, in or-
der to capture the distributions of domain-specific
dictionary words. Unlexicalized features trained
for source domain words can carry over to the tar-
get domain. The overall architecture of our sys-
590
Action Lexicon Feature templates
Separate in-lex(w
?1
), l(w
?1
) ? in-lex(w
?1
),
in-lex(w
?1
, t
?1
), l(w
?1
) ? in-lex(w
?1
, t
?1
)
Table 1: Dictionary features of the type-
supervised model, where w
?1
and t
?1
denote the
last word and POS tag of a partial result, re-
spectively; l(w) denotes the length of the word
w; in-lex(w, t) denotes whether the word-tag pair
(w, t) is in the lexicon.
tem is shown in Figure 1, where lexicons can be
treated as ?plugins? to the model for different do-
mains, and one model trained from the source do-
main can be applied to many different target do-
mains, as long as a lexicon is available.
The method can be the most effective
when there is a significant amount of domain-
independent words in the data, which provide rich
lexicalized contexts for estimating unlexicalized
features for domain-specific words. For scientific
domains (e.g. the biomedical domain) which
share a significant proportion of common words
with the news domain, and have most domain
specific words being nouns (e.g. ???? (dia-
betes)?), the method can be the most effective.
We choose a comparatively difficult domain pair
(e.g. modern news v.s. ancient style novel),
for which the use of many word types are quite
different. Results on this data can be relatively
more indicative of the usefulness of the method.
3.2 Lexicon-Based Features
Table 1 shows the set of new unlexicalized fea-
tures for the domain-specific lexicons. In addition
to words and POS tags, length information is also
encoded in the features, to capture different dis-
tributions of different word sizes. For example,
a one-character word in the dictionary might not
be identified as confidently using the lexicon as a
three-character word in the dictionary.
To acquire a domain-specific lexicon for the
source domain, we use HowNet (Dong and
Dong, 2006) to classify CTB words into domain-
independent and domain-specific categories. Con-
sisting of semantic information for nearly 100,000
common Chinese words, HowNet can serve as a
resource of domain-independent Chinese words.
We choose out of all words in the source domain
training data those that also occur in HowNet for
domain-independent words, and out of the remain-
ing words those that occur more than 3 times for
words specific to the source domain. We assume
that the domain-independent lexicon applies to all
target domains also. For some target domains,
we can obtain domain-specific terminologies eas-
ily from the Internet. However, this can be a very
small portion depending on the domain. Thus, it
may still be necessary to obtain new lexicons by
manual annotation.
3.3 Lexicon and Self-Training
The lexicon-based features can be combined with
unsupervised learning to further improve target-
domain accuracies. We apply self-training on top
of the lexicon-based features in the following way:
we train a lexicon-based model M using a lexi-
con L
s
of the source domain, and then apply M
together with a target-domain lexicon L
t
to auto-
matically label a set of target domain sentences.
We combine the automatically labeled target sen-
tences with the source-domain training data to ob-
tain an extended set of training data, and train a
final model M
self
, using the lexicon L
s
and L
t
for
source- and target-domain data, respectively.
Different numbers of target domain sentences
can be used for self-training. Liu and Zhang
(2012) showed that an increased amount of tar-
get sentences do not constantly lead to improved
development accuracies. They use character per-
plexity to order target domain sentences, taking
the top K sentences for self-training. They eval-
uate the optimal development accuracies using a
range of different Kvalues, and select the best K
for a final model. This method gave better results
than using sentences in the internet novel in their
original order (Liu and Zhang, 2012). We follow
this method in ranking target domain sentences.
4 Experiments
4.1 Setting
We use annotated sentences from the CTB5 for
source-domain training, splitting the corpus into
training, development and test sections in the same
way as previous work (Kruengkrai et al., 2009;
Zhang and Clark, 2010; Sun, 2011).
Following Liu and Zhang (2012), we use the
free Internet novel ?Zhuxian? (henceforth referred
to as ZX; also known as ?Jade dynasty?) as our tar-
get domain data. The writing style of the novel is
in the literature genre, with the style of Ming and
Qing novels, very different from news in CTB. Ex-
591
CTB sentences ZX sentences
?????????? ??????????????????????
(Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating
?????????????? landscapes. There were haunting ghosts.)
(Lipeng stressed on speeding the reform of official regulations.) ??????????????
?????????????? (No time left. Let me call out Zhuxian, the ancient sword.)
(Chinese chemistry industry increases the pace of opening up.) ???????????????(There came suddenly
a gust of wind, out of which was laughters and magic flashes.)
Table 2: Example sentences from CTB and ZX to illustrate the differences between news and novel.
Data Set Chap. IDs # sents # words
CTB5
Train 1-270, 400-931, 10,086 493,930
1001-1151
Devel 301-325 350 6,821
Test 271-300 348 8,008
ZX
Train 6.6-6.10, 2,373 67,648
7.6-7.10, 19
Devel 6.1-6.5 788 20,393
Test 7.1-7.5 1,394 34,355
Table 3: Corpus statistics.
ample sentences from the two corpora are shown
in Table 2. Liu and Zhang (2012) manually anno-
tated 385 sentences as development and test data,
which we download from their website.
1
These
data follow the same annotation guidelines as the
Chinese Treebank (Xue et al., 2000).
To gain more reliable statistics in our results,
we extend their annotation work to a total 4,555
sentences, covering the sections 6, 7 and 19 of the
novel. The annotation work is based on the auto-
matically labeled sentences by our baseline model
trained with CTB5 corpus. It took an experienced
native speaker 80 hours, about one minute on av-
erage to annotate one sentence. We use chapters
1-5 of section 6 as the development data, chap-
ters 1-5 of section 7 as the test data, and the re-
maining data for target-domain training,
2
in order
to compare type-supervised methods with token-
supervised methods. Under permission from the
author of the novel, we release our annotation for
future reference. Statistics of both the source and
the target domain data are shown in Table 3. The
rest of the novel is treated as unlabeled sentences,
used for type-annotation and self-training.
We perform the standard evaluation, using F-
scores for both the segmentation accuracy and the
1
http://faculty.sutd.edu.sg/?yue zhang/emnlp12yang.zip
2
We only use part of the training sentences in our experi-
ments, and the remaining can be used for further research.
overall segmentation and POS tagging accuracy.
4.2 Baseline Performances
The baseline discriminative model can achieve
state-of-the-art performances on the CTB5, with
a 97.62% segmentation accuracy and a 93.85% on
overall segmentation and tagging accuracy. Using
the CTB model, the performance on ZX drops sig-
nificantly, to a 87.71% segmentation accuracy and
a 80.81% overall accuracy. Applying self-training,
the segmentation and overall F-scores can be im-
proved to 88.62% and 81.94% respectively.
4.3 Development Experiments
In this section, we study type-supervised domain
adaptation by conducting a series of experiments
on the development data, addressing the follow-
ing questions. First, what is the influence of tag-
dictionaries through lexicon-based features? Sec-
ond, what is the effect of type-supervised domain
adaptation in contrast to token-supervised domain
adaptation under the same annotation cost? Third,
what is the interaction between tag-dictionary and
self-training? Finally, what is the combined effect
of type- and token-supervised domain adaptation?
4.3.1 The Influence of The Tag Dictionary
We investigate the effects of two different tag dic-
tionaries. The first dictionary contains names of
characters (e.g. ?? (Guili)) and artifacts (e.g.
swords such as?? (Dragonslayer)) in the novel,
which are obtained from an Internet Encyclope-
dia,
3
and requires little human effort. We ex-
tracted 159 words from this page, verified them,
and put them into a tag dictionary. We associate
every word in this tag dictionary with the POS
?NR (proper noun)?, and name the lexicon by NR.
The second dictionary was constructed man-
ually, by first employing our baseline tagger to
tag the unlabeled ZX sentences automatically,
3
http://baike.baidu.com/view/18277.htm
592
Model
Target-Domain
Cost
Supervised +Self-Training
Resources SEG POS SEG POS ER
Baseline ? 0 89.77 82.92 90.35 83.95 6.03
Type-Supervision
NR(T) 0 89.84 83.91 91.18 85.22 8.14
3K(T) 5h 91.93 86.53 92.86 87.67 8.46
ORACLE(T) ? 93.10 88.87 94.00 89.91 9.34
Token-Supervision
300(S) 5h 92.59 86.86 93.33 87.85 7.53
600(S) 10h 93.19 88.13 93.81 89.01 7.41
900(S) 15h 93.53 88.53 94.15 89.33 6.97
Combined 3K(T) + 300(S) 10h 93.49 88.54 94.00 89.21 5.85
Type- and Token-Supervision 3K(T) + 600(S) 15h 93.98 89.27 94.61 89.87 5.59
Table 4: Development test results, where Cost denotes the cost of type- or token-annotation measured
by person hours, ER denotes the error reductions of overall performances brought by self-training, T
denotes type-annotation and S denotes token-annotation.
and then randomly selecting the words that are
not domain-independent for an experienced native
speaker to annotate. To facilitate comparison with
token-supervision, we spent about 5 person hours
in annotating 3,000 word-tag pairs, at about the
same cost as annotating 300 sentences. Finally we
conjoined the 3,000 word-tag pairs with the NR
lexicon, and name the resulting lexicon by 3K.
For the target domain, we mark the words from
both NR and 3K as the domain-specific lexicons.
In all experiments, we use the same domain-
independent lexicon, which is extracted from the
source domain training data by HowNet matching.
The accuracies are shown in Table 4, where
the NR lexicon improved the overall F-score
slightly over the baseline, and the larger lexicon
3K brought more significant improvements. These
experiments agree with the intuition that the size
and the coverage of the tag dictionary is impor-
tant to the accuracies. To understand the extent to
which a lexicon can improve the accuracies, we
perform an oracle test, in which lexicons in the
gold-standard test outputs are included in the dic-
tionary. The accuracy is 88.87%.
4.3.2 Comparing Type-Supervised and
Token-Supervised Domain Adaptation
Table 4 shows that the accuracy improvement by
3,000 annotated word-tag pairs (86.53%) is close
to that by 300 annotated sentences (86.86%). This
suggest that using our method, type-supervised
domain adaptation can be a competitive choice to
the token-supervised methods.
The fact that the token-supervised model gives
slightly better results than our type-annotation
method under similar efforts can probably be ex-
0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
Token-Supervision with 300(S)
T
y
p
e
-
S
u
p
e
r
v
i
s
i
o
n
w
i
t
h
3
K
(
T
)
Figure 2: Sentence accuracy comparisons for
type- and token-supervision with equal cost.
plained by the nature of domain differences. Texts
in the Internet novel are different with CTB news
in not only the vocabulary, but also POS n-gram
distributions. The latter cannot be transferred from
the source-domain training data directly. Texts
from domains such as modern-style novels and
scientific articles might have more similar POS
distributions to the CTB data, and can potentially
benefit more from pure lexicons. We leave the ver-
ification of this intuition to future work.
4.3.3 Making Use of Unlabeled Sentences
Both type- and token-supervised domain adapta-
tion methods can be further improved via unla-
beled target sentences. We apply self-training to
both methods, and find improved results across the
board in Table 4. The results indicate that unla-
beled data is useful in further improving both type-
and token-supervised domain adaptation.
593
Interestingly, the effects of the two methods
on self-training are slightly different. The er-
ror reduction by self-training improves from 6.0%
(baseline) to averaged 7.3% and 8.6% for token-
and type-supervised adaptation, respectively. The
better effect for the type-supervised method may
result from comparatively more uniform cover-
age of the lexicon on sentences, since the target-
domain lexicon is annotated by selecting words
from much more than 300 sentences.
4.3.4 Combined Model of Type- and
Token-Supervision
Figure 2 shows the F-scores of each development
test sentence by type- and token-supervised do-
main adaptation with 5 person hours, respectively.
It indicates that the two methods make different
types of errors, and can potentially be used jointly
for better improvements. We conduct a set of ex-
periments as shown in Table 4, finding that the
combined type- and token-supervised model with
lexicon 3K and 300 labeled sentences achieves
an overall accuracy of 88.54%, exceeding the ac-
curacies of both the type-supervised model with
lexicon 3K and the token-supervised model with
300 labeled sentences. Similar observation can
be found for the combined model with lexicon 3K
and 600 labeled sentences. If combined with self-
training, the same fact can be observed.
More interestingly, the combined model also
exceeds pure type- and token-supervised mod-
els with the same annotation cost. For exam-
ple, the combined model with 3K and 300 la-
beled sentences gives a better accuracy than the
token-supervised model with 600 sentences, with
or without self-training. Similar observations hold
between the combined model with 3K and 600 la-
beled sentences and the token-supervised model
with 900 sentences. The results suggest that the
most cost-effective approach for domain adapta-
tion can be combined type- and token-supervision:
after annotating a set of raw sentences, one could
stop to annotate some words, rather than continu-
ing sentence annotation.
4.4 Final Results
Table 5 shows the final results on test corpus
within ten person hours? annotation. With five per-
son hours (lexicon 3K), the type-supervised model
gave an error reduction of 32.99% compared with
the baseline. The best result was obtained by the
combined type- and token-supervised model, with
SEG POS ER Time
Baseline 87.71 80.81 0.00 0
Baseline+Self-Training 88.62 81.94 5.89 0
Type-Supervision
NR(T) 88.34 82.54 9.02 0
NR(T)+ Self-Training 89.52 83.93 16.26 0
3K(T) 91.11 86.04 27.25 5h
3K(T)+Self-Training 92.11 87.14 32.99 5h
Token-Supervision
300(S) 92.44 86.87 31.58 5h
300(S)+Self-Training 93.24 87.48 34.76 5h
600(S) 93.09 88.05 37.73 10h
600(S)+Self-Training 93.77 88.78 41.53 10h
Combined Type- and Token-Supervision
3K(T)+300(S) 93.27 89.03 42.83 10h
3K(T)+300(S)+Self-Training 93.98 89.84 47.06 10h
Table 5: Final results on test set within ten per-
son hours? annotation, where ER denotes the over-
all error reductions compared with the baseline
model, Time denotes the cost of type- or token-
annotation measured by person hours, T denotes
type-annotation and S denotes token-annotation.
an error reduction of 47.06%, higher than that the
token-supervised model with the same cost under
the same setting (the model of 600 labeled sen-
tences with an error reduction of 41.53%). The
results confirm that the type-supervised model
is a competitive alternative for joint segmenta-
tion and POS-tagging under the cross-domain set-
ting. Combined type- and token-supervised model
yields better results than single models.
5 Related Work
As mentioned in the introduction, tag dictionaries
have been applied to type-supervised POS tagging
of English (Toutanova and Johnson, 2007; Gold-
water and Griffiths, 2007; Ravi and Knight, 2009;
Garrette and Baldridge, 2012), Hebrew (Goldberg
et al., 2008), Kinyarwanda and Malagasy (Gar-
rette and Baldridge, 2013; Garrette et al., 2013),
and other languages (T?ackstr?om et al., 2013).
These methods assume that lexicon can be ob-
tained by manual annotation or semi-supervised
learning, and use the lexicon to induce tag se-
quences on unlabeled sentences. We study type-
supervised Chinese POS-tagging, but under the
setting of domain adaptation. The problem is
how to leverage a target domain lexicon and an
available annotated resources in a different source
domain to improving POS-tagging. Consistent
594
with Garrette et al. (2013), we also find that the
type-supervised method is a competitive choice to
token-supervised adaptation.
There has been a line of work on using graph-
based label propagation to expand tag-lexicons for
POS-tagging (Subramanya et al., 2010; Das and
Petrov, 2011). Similar methods have been ap-
plied to character-level Chinese tagging (Zeng et
al., 2013). We found that label propagation from
neither the source domain nor auto-labeled target
domain sentences can improve domain adaptation.
The main reason could be significant domain dif-
ferences. Due to space limitations, we omit this
negative result in our experiments.
With respect to domain adaptation, existing
methods can be classified into three categories.
The first category does not explicitly model dif-
ferences between the source and target domains,
but use standard semi-supervised learning meth-
ods with labeled source domain data and unla-
beled target domain data (Dai et al., 2007; Raina
et al., 2007). The baseline self-training ap-
proach (Liu and Zhang, 2012) belongs to this cat-
egory. The second considers the differences in the
two domains in terms of features (Blitzer et al.,
2006; Daume III, 2007), classifying features into
domain-independent source domain and target do-
main groups and training these types consistently.
The third considers differences between the dis-
tributions of instances in the two domains, treat-
ing them differently (Jiang and Zhai, 2007). Our
type-supervised method is closer to the second cat-
egory. However, rather than splitting features into
domain-independent and domain-specific types,
we use domain-specific dictionaries to capture do-
main differences, and train a model on the source
domain only. Our method can be treated as an ap-
proach specific to the POS-tagging task.
With respect to Chinese lexical analysis, lit-
tle previous work has been reported on using a
tag dictionary to improve joint segmentation and
POS-tagging. There has been work on using a
lexicon in improving segmentation in a Chinese
analysis pipeline. Peng et al. (2004) used fea-
tures from a set of Chinese words and characters
to improve CRF-based segmentation; Low et al.
(2005) extracted features based on a Chinese lex-
icon from Peking University to help a maximum
segmentor; Sun (2011) collected 12,992 idioms
from Chinese dictionaries, and used them for rule-
based pre-segmentation; Hatori et al. (2012) col-
lected Chinese words from HowNet and the Chi-
nese Wikipedia to enhance segmentation accura-
cies of their joint dependency parsing systems. In
comparison with their work, our lexicon contain
additional POS information, and are used for word
segmentation and POS-tagging simultaneously. In
addition, we separate domain-dependent lexicons
for the source and target lexicons, and use a novel
framework to perform domain adaptation.
Wang et al. (2011) collect word-tag statistics
from automatically labeled texts, and use them as
features to improve POS-tagging. Their word-tag
statistics can be treated as a type of lexicon. How-
ever, their efforts differ from ours in several as-
pects: (1) they focus on in-domain POS-tagging,
while our concern is cross-domain tagging; (2)
they study POS-tagging on segmented sentences,
while we investigate joint segmentation and POS-
tagging for Chinese; (3) their tag-dictionaries are
not tag-dictionaries literally, but statistics of word-
tag associations.
6 Conclusions
We performed an empirical study on the use of
tag-dictionaries for the domain adaptation of joint
Chinese segmentation and POS-tagging, showing
that type-supervised methods can be a compet-
itive alternative to token-supervised methods
in cost-effectiveness. In addition, combination
of the two methods gives the best cost-effect.
Finally, we release our annotation of over 4,000
sentences in the Internet literature domain on-
line at http://faculty.sutd.edu.sg/
?
yue_zhang/eacl14meishan.zip as a
free resource for Chinese POS-tagging.
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments. We gratefully acknowl-
edge the support of the National Key Basic
Research Program (973 Program) of China via
Grant 2014CB340503 and the National Natural
Science Foundation of China (NSFC) via Grant
61133012 and 61370164, the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the Singaporean Ministration of
Education Tier 2 grant T2MOE201301 and SRG
ISTD 2012 038 from Singapore University of
Technology and Design.
595
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In COLING.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised POS induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575?584, Cambridge, MA, October. Association for
Computational Linguistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring Naive Bayes Classifiers for
Text Classification. In AAAI, pages 540?545.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2006. Hownet And
the Computation of Meaning. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markov models for part-of-speech
tagging with incomplete tag dictionaries. In
EMNLP-CoNLL, pages 821?831.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 138?147, Atlanta, Georgia, June. Association
for Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 583?592, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08: HLT, pages 746?754, Columbus, Ohio, June.
Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045?
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513?521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745?754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Proceeding of CICLing?11.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2).
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277?284, Barcelona, Spain, July. Association
for Computational Linguistics.
596
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of Coling 2004, pages 562?568, Geneva, Switzer-
land, Aug 23?Aug 27. COLING.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y. Ng. 2007. Self-taught learn-
ing: transfer learning from unlabeled data. In ICML,
pages 759?766.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
ACL/IJCNLP, pages 504?512.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
167?176, Cambridge, MA, October. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385?
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, McDon-
ald Ryan, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
In Transactions of the ACL. Association for Compu-
tational Linguistics, March.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In ECML/PKDD (2), pages 442?457.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In NIPS.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Shizhe Huang, and Tony Kroch.
2000. The bracketing guidelines for the chinese
treebank. Technical report, University of Pennsyl-
vania.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint chinese word segmentation and part-
of-speech tagging. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 770?
779, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, pages 888?
896, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843?852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
597
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 246?249,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Semantic Role Labeling with Word Sense
Wanxiang Che, Ting Liu and Yongqiang Li
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, tliu, yqli}@ir.hit.edu.cn
Abstract
Semantic role labeling (SRL) not only needs
lexical and syntactic information, but also
needs word sense information. However, be-
cause of the lack of corpus annotated with
both word senses and semantic roles, there is
few research on using word sense for SRL.
The release of OntoNotes provides an oppor-
tunity for us to study how to use word sense
for SRL. In this paper, we present some novel
word sense features for SRL and find that they
can improve the performance significantly.
1 Introduction
Semantic role labeling (SRL) is a kind of shallow
sentence-level semantic analysis and is becoming a
hot task in natural language processing. SRL aims at
identifying the relations between the predicates in a
sentence and their associated arguments. At present,
the main stream researches are focusing on feature
engineering or combination of multiple results.
Word senses are important information for rec-
ognizing semantic roles. For example, if we know
?cat? is an ?agent? of the predicate ?eat? in a
sentence, we can guess that ?dog? can also be
an ?agent? of ?eat?. Word sense has been suc-
cessfully used in many natural language process-
ing tasks, such as machine translation (Chan et al,
2007; Carpuat and Wu, 2007). CoNLL 2008 shared
task (Surdeanu et al, 2008) first introduced the pred-
icate classification task, which can be regarded as
the predicate sense disambiguation. Meza-Ruiz and
Riedel (2009) has shown that the predicate sense can
improve the final SRL performance. However, there
is few discussion about the concrete influence of all
word senses, i.e. the words besides predicates. The
major reason is lacking the corpus, which is both an-
notated with all word senses and semantic roles.
The release of OntoNotes corpus provides an op-
portunity for us to verify whether all word senses
can help SRL. OntoNotes is a large corpus annotated
with constituency trees (based on Penn Treebank),
predicate argument structures (based on Penn Prop-
Bank) and word senses. It has been used in some
natural language processing tasks, such as joint pars-
ing and named entity recognition (Finkel and Man-
ning, 2009) and word sense disambiguation (Zhong
et al, 2008).
In this paper, we regard the word sense informa-
tion as additional SRL features. We compare three
categories of word sense features (subtree-word re-
lated sense, predicate sense, and sense path) and find
that the subtree-word related sense feature is ineffec-
tive, however, the predicate sense and the sense path
features can improve the SRL performance signifi-
cantly.
2 Data Preparation
In our experiments, we use the OntoNotes Release
2.01 corpus (Hovy et al, 2006). The OntoNotes
project leaders describe it as ?a large, multilingual
richly-annotated corpus constructed at 90% inter-
nanotator agreement.? The corpus has been an-
notated with multiple levels of annotation, includ-
ing constituency trees, predicate argument struc-
ture, word senses, co-reference, and named entities.
For this work, we focus on the constituency trees,
word senses, and predicate argument structures. The
corpus has English and Chinese portions, and we
just use the English portion, which has been split
into seven sections: ABC, CNN, MNB, NBC, PRI,
VOA, and WSJ. These sections represent a mix of
speech and newswire data.
Because we used SRL system based on depen-
dence syntactic trees, we convert the constituency
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2008T04
246
trees into dependence trees with an Constituent-to-
Dependency Conversion Tool2. In addition, we also
convert the OntoNotes sense of each polysemant
into WordNet sense using sense inventory file pro-
vided by OntoNotes 2.0. For an OntoNotes sense
with more than one WordNet sense, we simply use
the foremost (more popular) one.
3 Semantic Role Labeling System
Our baseline is a state-of-the-art SRL system based
on dependency syntactic tree (Che et al, 2009). A
maximum entropy (Berger et al, 1996) classifier is
used to predict the probabilities of a word in the
sentence to be each semantic role. A virtual role
?NULL? (presenting none of roles is assigned) is
added to the roles set, so it does not need seman-
tic role identification stage anymore. For a predi-
cate, two classifiers (one for noun predicates, and
the other for verb predicates) predict probabilities of
each word in a sentence to be each semantic role (in-
cluding virtual role ?NULL?). The features used in
this stage are listed in Table 1.
Feature Description
FirstwordLemma The lemma of the first word in a
subtree
HeadwordLemma The lemma of the head word in
a subtree
HeadwordPOS The POS of the head word in a
subtree
LastwordLemma The lemma of the last word in a
subtree
POSPath The POS path from a word to a
predicate
PathLength The length of a path
Position The relative position of a word
with a predicate
PredicateLemma The lemma of a predicate
RelationPath The dependency relation path
from a word to a predicate
Table 1: Features that are used in SRL.
4 Word Sense for Semantic Role Labeling
From Table 1, we can see that there are lots of lemma
or POS related features. However, the lemma fea-
ture is very sparse and may result in data sparseness
2http://nlp.cs.lth.se/software/treebank converter/
problem. As for the POS, it represents the syntactic
information, but is not enough to distinguish differ-
ent semantic roles. Therefore, we need a kind of new
feature, which is general than the lemma and special
than the POS.
The word sense just satisfies the requirement.
Thus, we will add some new features related with
word sense for SRL. Generally, the original features
can be classified into three categories:
1. Subtree-word related: FirstwordLemma, Last-
wordLemma, HeadwordLemma, and Head-
wordPOS
2. Predicate related: PredicateLemma
3. Word and predicate related: POSPath, Rela-
tionPath, PathLenght, and Position
Correspondingly, we add three categories of word
sense features by replacing Lemma or POS into
Sense, i.e.
1. Subtree-word related sense: FirstwordSense,
LastwordSense, and HeadwordSense
2. Predicate related sense: PredicateSense
3. Word and predicate related sense: SensePath
Three strategies are designed to adopt these
senses:
1. Lemma+Sense: It is the original word
sense representation in OntoNotes, such as
?dog.n.1?. In fact, This is a specialization of
the lemma.
2. Hypernym(n): It is the hypernym of a word
sense, e.g. the hypernym of ?dog.n.1? is ?ca-
nine.n.1?. The n means the level of the hy-
pernym. With the increasing of n, the sense
becomes more and more general. In theory,
however, this strategy may result in inconsis-
tent sense, e.g. word ?dog? and ?canine? have
different hypernyms. The same problem occurs
with Basic Concepts method (Izquierdo et al,
2007).
3. Root Hyper(n): In order to extract more con-
sistent sense, we use the hypernym of a word
sense counting from the root of a sense tree,
e.g. the root hypernym of ?dog.n.1? is ?en-
tity.n.1?. The n means the level of the root hy-
pernym. With the increasing of n, the sense
247
becomes more and more special. Thus, word
?dog? and ?canine? have the same Root Hyper:
?entity?, ?physical entity?, and ?object? with n
= 1, 2, and 3 respectively.
5 Experiments
We will do our experiments on seven of the
OntoNotes English datasets described in Section 2.
For each dataset, we aimed for roughly a 60% train
/ 20% development / 20% test split. See Table 2
for the detailed statistics. In order to examine the
influence of word senses in isolation, we use the hu-
man annotated POS, parse trees, and word senses
provided by OntoNotes. The lemma of each word is
extracted using WordNet tool.
Training Developing Testing
ABC 669 163 138(0001-0040) (0041-0054) (0057-0069)
CNN 1,691 964 1,146(0001-0234) (0235-0331) (0333-0437)
MNB 381 130 125(0001-0015) (0016-0020) (0021-0025)
NBC 351 129 86(0001-0025) (0026-0032) (0033-0039)
PRI 1,205 384 387(0001-0067) (0068-0090) (0091-0112)
VOA 1,238 325 331(0001-0159) (0160-0212) (0213-0264)
WSJ 8,592 2,552 3,432(0020-1446) (1447-1705) (1730-2454)
All 14,127 4,647 5,645
Table 2: Training, developing and testing set sizes for the
seven datasets in sentences. The file ranges (in parenthe-
sis) refer to the numbers within the names of the original
OntoNotes files.
The baseline SRL system without sense informa-
tion is trained with all the training corpus as de-
scribed in Section 3. Its performance on the devel-
opment data is F1 = 85.48%.
Table 3 shows the performance (F1) comparison
on the development data among different sense ex-
tracting strategies with different feature categories.
The numbers are the parameter n used in Hypernym
and Root Hyper strategies.
From Table 3, we can find that:
1. Both of the predicate sense feature and the
sense path feature can improve the performance. For
Subtree-word Predicate Sense
related sense sense path
Lemma+Sense 85.34% 86.16% 85.69%
1 85.41% 86.12% 85.74%
Hypernym(n) 2 85.48% 86.10% 85.74%
3 85.38% 86.10% 85.69%
1 85.35% 86.07% 85.96%
Root Hyper(n) 2 85.45% 86.13% 85.86%
3 85.46% 86.05% 85.91%
Table 3: The performance comparison on the devel-
opment data among different sense extracting strategies
with different feature categories.
the predicate sense feature, we arrive at the same
conclusion with Meza-Ruiz and Riedel (2009). As
for the sense path feature, it is more special than the
POS, therefore, it can enhance the precision.
2. The subtree-word related sense is almost use-
less. The reason is that the original lemma and POS
features have been able to describe the subtree-word
related information. This kind of sense features is
just reduplicate.
3. For different sense feature categories
(columns), the performance is not very seriously af-
fected by different sense extracting strategies (rows).
That is to say, once the sense of a word is disam-
biguated, the sense expressing form is not important
for SRL.
In order to further improve the performance,
we add the predicate sense and the sense path
features simultaneously. Here, we select the
Lemma+Sense strategy for the predicate sense and
the Root Hyper(1) strategy for the sense path. The
final performance achieves F1 = 86.44%, which is
about 1% higher than the baseline (F1 = 85.48%).
Finally, we compare the baseline (without sense)
result with the word sense result on the test data. In
order to see the contribution of correct word senses,
we introduce a simple sense determining strategy,
which use the first (the most popular)WordNet sense
for each word. The final detailed comparison results
are listed in Table 4.
Averagely, both of the methods with the first sense
and the correct sense can perform better than the
baseline. However, the improvement of the method
with the first sense is not significant (?2-test3 with
3http://graphpad.com/quickcalcs/chisquared1.cfm
248
Precision Recall F1
w/o sense 86.25 83.01 84.60
ABC first sense 84.91 81.71 83.28
word sense 87.13 83.40 85.22
w/o sense 86.67 79.97 83.19
CNN first sense 86.94 80.73 83.72
word sense 87.75 80.64 84.05
w/o sense 85.29 81.69 83.45
MNB first sense 85.04 81.85 83.41
word sense 86.96 82.47 84.66
w/o sense 84.49 76.42 80.26
NBC first sense 84.53 76.63 80.38
word sense 86.20 77.44 81.58
w/o sense 86.48 82.29 84.34
PRI first sense 86.82 83.10 84.92
word sense 87.45 83.14 85.24
w/o sense 89.87 86.65 88.23
VOA first sense 90.01 86.60 88.27
word sense 91.35 87.10 89.18
w/o sense 88.38 82.93 85.57
WSJ first sense 88.72 83.29 85.92
word sense 89.25 84.00 86.54
w/o sense 87.85 82.46 85.07
Avg first sense 88.11 82.85 85.40
word sense 88.84 83.37 86.02
Table 4: The testing performance comparison among
the baseline without (w/o) sense information, the method
with the first sense, and the method with the correct word
sense.
? < 0.01). Especially, for some sections, such as
ABC and MNB, it is harmful to the performance. In
contrast, the correct word sense can improve the per-
formance significantly (?2-test with ? < 0.01)and
consistently. These can further prove that the word
sense can enhance the semantic role labeling.
6 Conclusion
This is the first effort to adopt the word sense
features into semantic role labeling. Experiments
show that the subtree-word related sense features
are ineffective, but the predicate sense and the sense
path features can improve the performance signifi-
cantly. In the future, we will use an automatic word
sense disambiguation (WSD) system to obtain word
senses and study the function of WSD for SRL.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144, and Natural Scientific Research
Innovation Foundation in Harbin Institute of Tech-
nology (HIT.NSRIF.2009069).
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of EMNLP/CoNLL-2007, pages
61?72, Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL-2007, pages
33?40, Prague, Czech Republic, June.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL-2009, pages 49?54, Boulder,
Colorado, June.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of NAACL/HLT-2009, pages 326?334, Boul-
der, Colorado, June.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of NAACL/HLT-
2006, pages 57?60, New York City, USA, June.
Rube?n Izquierdo, Armando Sua?rez, and German Rigau.
2007. Exploring the automatic selection of basic level
concepts. In Proceedings of RANLP-2007.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Proceedings of NAACL/HLT-2009,
pages 155?163, Boulder, Colorado, June.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of CoNLL-2008,
pages 159?177, Manchester, England, August.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of EMNLP-2008, pages
1002?1010, Honolulu, Hawaii, October.
249
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 377?380,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generalizing Syntactic Structures for Product Attribute Candidate
Extraction
Yanyan Zhao, Bing Qin, Shen Hu, Ting Liu
Harbin Institute of Technology, Harbin, China
{yyzhao,bqin,shu,tliu}@ir.hit.edu.cn
Abstract
Noun phrases (NP) in a product review are
always considered as the product attribute
candidates in previous work. However, this
method limits the recall of the product at-
tribute extraction. We therefore propose
a novel approach by generalizing syntactic
structures of the product attributes with two
strategies: intuitive heuristics and syntactic
structure similarity. Experiments show that
the proposed approach is effective.
1 Introduction
Product attribute extraction is a fundamental task of
sentiment analysis. It aims to extract the product at-
tributes from a product review, such as ?picture qual-
ity? in the sentence ?The picture quality of Canon is
perfect.? This task is usually performed in two steps:
product attribute candidate extraction and candidate
classification.
Almost all the previous work pays more attention
to the second step, fewer researchers make in-depth
research on the first step. They simply choose the
NPs in a product review as the product attribute can-
didates (Hu and Liu, 2004; Popescu and Etzioni,
2005; Yi et al, 2003). However, this method lim-
its the recall of the product attribute extraction for
two reasons. First, there exist other structures of the
product attributes except NPs. Second, the syntactic
parsing is not perfect, especially for the Non-English
languages, such as Chinese. Experiments on three
Chinese datasets1 show that nearly 15% product at-
tributes are lost, when only using NPs as the can-
didates. Obviously, if using the candidate classifi-
cation techniques on these NP candidates, it would
1It refers to the training data in Section 3.1.
lead to poor performance (especially for recall) for
the final product attribute extraction.
Based on the above discussion, it can be observed
that product attribute candidate extraction is well
worth studying. In this paper, we propose an ap-
proach by generalizing the syntactic structures of the
product attributes to solve this problem. Figure 1
lists some syntactic structure samples from an an-
notated corpus, including the special forms of NPs
in Figure 1(a) and other syntactic structures, such as
VP or IP in Figure 1(b). We can find that the syntac-
tic structures can not only cover more phrase types
besides NP, but also describe the detailed forms of
the product attributes.
NP
NN
??(screen)
NP
NN
??
NP
NN
???(screen    resolution)
NP
QP
CD
?
NP
NN
??(single    track)
NP
ADJP
JJ
?
NP
NN
??(front         seats)
NP
VB
??
NP
NN
??(photographing function)
VP
NP
NN
??
VP
VB
??(screen   display)
IP
(a) syntactic structure samples of NP
(b) syntactic structure samples of other phrases
Figure 1: Syntactic structure samples of the product at-
tributes (acquired by an automatic phrase parser).
In order to exploit more and useful syntactic struc-
tures, two generalization strategies: intuitive heuris-
tics and syntactic structure similarity are used. Ex-
periments on three Chinese domain-specific datasets
show that our approach can significantly improve the
recall of the product attribute candidate extraction,
and furthermore, improve the performance of the fi-
nal product attribute extraction.
377
2 Approach
The standard syntactic structures of the product at-
tributes can be collected from a training set2. Then
a simple method of exact matching can be used to
select the product attribute candidates from the test
set. In particular, for a syntactic structure3 T in
the test set, if T exactly matches with one of the
standard syntactic structures, then its corresponding
string can be treated as a product attribute candidate.
However, this method fails to handle similar syn-
tactic structures, such as the two structures in Fig-
ure 2. Besides, this method treats the syntactic struc-
ture as a whole during exact matching, without con-
sidering any structural information. Therefore, it is
difficult to describe the syntactic structure informa-
tion explicitly. All of these prevent this method from
generalizing unseen data well.
To overcome the above problems, two generaliza-
tion strategies are proposed in this paper. One is to
generalize the syntactic structures with two intuitive
heuristics. The other is to deeply mine the syntactic
structure by decomposing it into several substruc-
tures. Both strategies will be introduced in the fol-
lowing subsections.
2.1 Intuitive Heuristics
Two intuitive heuristics are adopted to generalize the
syntactic structures.
Heu1: For the near-synonymic grammar tags in
syntactic structures, we can generalize them by a
normalized one. Such as the red boxes in Figure 2,
the POSs ?NNS? and ?NN? show the same syntactic
meaning, we can generalize ?NNS? with ?NN?. The
near-synonymic grammar tags are listed in Table 1.
NP
VP NP
VB NNS NP
NN
NP
VP NP
VB NN NN
Heu2
Heu1
Figure 2: Generalizing a syntactic structure with two in-
tuitive heuristics.
Heu2: For the sequence of identical grammar tags
in syntactic structures, we can replace them with
2We use Dan Bikel?s phrase parser for syntactic parsing.
3We simply select the syntactic structures of the strings un-
der three words or four words with ???(?of? in English).
Replaced by Near-synonymic grammar tags
JJ JJR, JJS
NN NNS, NNP, NNPS, CD, NR
RB RBR, RBS
VB VBD, VBG, VBN, VBP, VBZ, VV
S SBAR, SBARQ, SINU, SQ
Table 1: The near-synonymic grammar tags.
one. The reason is that the sequential grammar tags
always describe the same syntactic function as one
grammar tag. Such as the blue circles in Figure 2.
2.2 Syntactic Structure Similarity
The heuristic generalization strategy is too restric-
tive to give a good coverage. Moreover, after this
kind of generalization, the syntactic structure is used
as a whole in exact matching all the same. Thus,
as an alternative to the exact matching, tree kernel
based methods can be used to implicitly explore the
substructures of the syntactic structure in a high-
dimensional space. This kind of methods can di-
rectly calculate the similarity between two substruc-
ture vectors using a kernel function. Tree kernel
based methods are effective in modeling structured
features, which are widely used in many natural
language processing tasks, such as syntactic pars-
ing (Collins and Duffy, 2001) and semantic role la-
beling (Che et al, 2008) and so on.
NP
NN
VP
VB
IP
NP VP
VB
IP
NP
NN
VP
VB NP
NN
VP
VB
IP
NP
NN
VP
IP
NP VP
IP
NP
NN
VP
VB
IP IP
Figure 3: Substructures from a syntactic structure.
In this paper, the syntactic structure for a product
attribute can be decomposed into several substruc-
tures, such as in Figure 3. Correspondingly, the syn-
tactic structure T can be represented by a vector of
integer counts of each substructure type:
?(T ) = (?1(T ), ?2(T ), ..., ?n(T ))
= (# of substructures of type 1,
= # of substructures of type 2,
...,
= # of substructures of type n)
378
After syntactic structure decomposition, we can
count the number of the common substructures as
the similarity between two syntactic structures. The
commonly used convolution tree kernel is applied in
this paper. Its kernel function is defined as follows:
K(T1, T2) = ??(T1),?(T2)?
=
?
i(?i(T1) ? ?i(T2))
Based on these, for a syntactic structure T in the
test set, we can compute the similarity between T
and all the standard syntactic structures by the above
kernel function. A similarity threshold thsim4 is set
to determine whether the string from T is a correct
product attribute candidate.
3 Experiments
3.1 Datasets and Evaluation Metrics
Three domain-specific datasets are used in the ex-
periments, which is from an official Chinese Opin-
ion Analysis Evaluation 2008 (COAE2008) (Zhao et
al., 2008). Table 2 shows the statistics of the three
datasets, each of which is divided into training, de-
velopment and test data in a proportion of 2:1:1.
Domain # of sentences # of standardproduct attributes
Camera 1,780 1,894
Car 2,166 2,504
Phone 2,196 2,293
Table 2: The datasets for three product domains.
Two evaluation metrics, recall and noise ratio, are
designed to evaluate the performance of the prod-
uct attribute candidate extraction. Recall refers to
the proportion of correctly identified attribute candi-
dates in all standard product attributes. Noise ratio
refers to the proportion of incorrectly identified at-
tribute candidates in all candidates.
3.2 Comparative methods
We choose the method, which considers NPs as the
product attribute candidates, as the baseline (shown
as NPs based).
Besides, in order to assess the two generaliza-
tion strategies? effectiveness, four experiments are
designed as follows:
4In the experiments, thsim is set to 0.7, which is tuned on
the development set.
SynStru based: It refers to the syntactic struc-
ture exact matching method, which is implemented
without the two proposed generation strategies.
SynStru h: It refers to the strategy only using the
first generalization.
SynStru kernel: It refers to the strategy only us-
ing the second generalization.
SynStru h+kernel: It refers to the strategy us-
ing both two generalizations, i.e., it refers to our ap-
proach in this paper.
3.3 Results
Table 3 lists the comparative performances on the
test data between our approach and the comparative
methods for product attribute candidate extraction.
Domain Method Recall Noise ratio
Camera
NPs based 81.20% 63.64%
SynStru based 84.80% 67.67%
SynStru h 92.08% 74.74%
SynStru kernel 92.51% 75.92%
SynStru h+kernel 92.72% 76.25%
Car
NPs based 85.25% 69.35%
SynStru based 86.31% 72.66%
SynStru h 93.78% 78.01%
SynStru kernel 94.56% 79.50%
SynStru h+kernel 94.71% 80.44%
Phone
NPs based 84.11% 63.76%
SynStru based 86.26% 67.09%
SynStru h 93.13% 73.62%
SynStru kernel 93.47% 75.11%
SynStru h+kernel 93.63% 75.35%
Table 3: Comparisons between our approach and the
comparative methods for product attribute candidate ex-
traction.
Analyzing the recalls in Table 3, we can find that:
1. The performance of SynStru based method
is better than NPs based method for each domain.
This can illustrate that syntactic structures can cover
more forms of the product attributes. However, the
recall of SynStru based method is not high, either.
2. The two generalization strategies, SynStru h
and SynStru kernel can both significantly improve
the performance for each domain, comparing to the
SynStru based method. This can illustrate that our
two generalization strategies are helpful.
3. Our approach SynStru h+kernel achieves the
best performance. This can illustrate that the two
generalization strategies are complementary to each
379
other. And further, mining and generalizing the syn-
tactic structures is effective for candidate extraction.
However, the noise ratio for each domain is in-
creasing when employing our approach. That?s be-
cause, more kinds of syntactic structures are consid-
ered, more noise is added. However, we can easily
remove the noise in the candidate classification step.
Thus in the next section, we will assess our candi-
date extraction approach by applying it to the prod-
uct attribute extraction task.
4 Application in Product Attribute
Extraction
For the extracted product attribute candidates, we
train a maximum entropy (ME) based binary clas-
sifier to find the correct product attributes. Several
commonly used features are listed in Table 4.
Feature Description
lexical
the words of the product attribute(PA)
the POS for each word of the PA
three words before the PA
three words after the PA
the words? number of the PA
syntactic the syntactic structure of the PA
Is there a stop word in the PA?
binary Is there a polarity word in the PA?
(Y/N) Is there an English word or number in the PA?
Table 4: The feature set for product attribute extraction.
Table 5 shows the product attribute extraction per-
formances on the test data. We can find that the
performance (F1) of our approach is better than
NPs based method for each domain. We discuss the
results as follows:
1. Comparing to the NPs based method, the re-
call of our approach increases a lot for each domain.
This demonstrates that generalized syntactic struc-
tures can cover more forms of product attributes.
2. Comparing to the NPs based method, the pre-
cision of our approach also increases for each do-
main. That?s because syntactic structures are more
specialized than the phrase forms (such as NP, VP)
in the previous work, which can filter some noises
from the phrase(NP) candidates.
5 Conclusion
This paper describes a simple but effective way to
extract the product attribute candidates from product
Domain Method R (%) P (%) F1 (%)
Camera NPs based 59.62 68.38 63.70Our approach 62.96 73.32 67.74
Car NPs based 59.94 64.87 62.31Our approach 67.34 65.90 66.61
Phone NPs based 58.53 71.14 64.22Our approach 67.84 76.13 71.74
Table 5: Comparisons between our approach and the
NPs based method for product attribute extraction.
reviews. The proposed approach is based on deep
analysis into syntactic structures of the product at-
tributes, via intuitive heuristics and syntactic struc-
ture decomposition. Experimental results indicate
that our approach is promising. In future, we will try
more syntactic structure generalization strategies.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, and the ?863?National
High- Tech Research and Development of China via
grant 2008AA01Z144.
References
Wanxiang Che, Min Zhang, AiTi Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a hybrid con-
volution tree kernel for semantic role labeling. ACM
Trans. Asian Lang. Inf. Process., 7(4).
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS, pages 625?632.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI-
2004, pages 755?760.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339?346.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
IEEE International Conference on Data Mining.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
380
Proceedings of NAACL-HLT 2013, pages 52?62,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Named Entity Recognition with Bilingual Constraints
Wanxiang Che? Mengqiu Wang? Christopher D. Manning? Ting Liu?
?{car, tliu}@ir.hit.edu.cn
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?{mengqiu, manning}@stanford.edu
Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Different languages contain complementary
cues about entities, which can be used to im-
prove Named Entity Recognition (NER) sys-
tems. We propose a method that formu-
lates the problem of exploring such signals on
unannotated bilingual text as a simple Inte-
ger Linear Program, which encourages entity
tags to agree via bilingual constraints. Bilin-
gual NER experiments on the large OntoNotes
4.0 Chinese-English corpus show that the pro-
posed method can improve strong baselines
for both Chinese and English. In particular,
Chinese performance improves by over 5%
absolute F1 score. We can then annotate a
large amount of bilingual text (80k sentence
pairs) using our method, and add it as up-
training data to the original monolingual NER
training corpus. The Chinese model retrained
on this new combined dataset outperforms the
strong baseline by over 3% F1 score.
1 Introduction
Named Entity Recognition (NER) is an important
task for many applications, such as information ex-
traction and machine translation. State-of-the-art su-
pervised NER methods require large amounts of an-
notated data, which are difficult and expensive to
produce manually, especially for resource-poor lan-
guages.
A promising approach for improving NER per-
formance without annotating more data is to exploit
unannotated bilingual text (bitext), which are rela-
tively easy to obtain for many language pairs, bor-
rowing from the resources made available by statis-
tical machine translation research.1 Different lan-
guages contain complementary cues about entities.
For example, in Figure 1, the word ?? (Ben)? is
common in Chinese but rarely appears as a trans-
lated foreign name. However, its aligned word on
the English side (?Ben?) provides a strong clue that
this is a person name. Judicious use of this type of
bilingual cues can help to recognize errors a mono-
lingual tagger would make, allowing us to produce
more accurately tagged bitext. Each side of the
tagged bitext can then be used to expand the orig-
inal monolingual training dataset, which may lead
to higher accuracy in the monolingual taggers.
Previous work such as Li et al (2012) and Kim
et al (2012) demonstrated that bilingual corpus an-
notated with NER labels can be used to improve
monolingual tagger performance. But a major draw-
back of their approaches are the need for manual
annotation efforts to create such corpora. To avoid
this requirement, Burkett et al (2010) suggested a
?multi-view? learning scheme based on re-ranking.
Noisy output of a ?strong? tagger is used as training
data to learn parameters of a log-linear re-ranking
model with additional bilingual features, simulated
by a ?weak? tagger. The learned parameters are then
reused with the ?strong? tagger to re-rank its own
outputs for unseen inputs. Designing good ?weak?
taggers so that they complement the ?view? of bilin-
gual features in the log-linear re-ranker is crucial to
the success of this algorithm. Unfortunately there is
no principled way of designing such ?weak? taggers.
In this paper, we would like to explore a conceptu-
ally much simpler idea that can also take advantage
1opus.lingfil.uu.se
52
TheO chairmanO ofO theB?ORG FederalI?ORG ReserveI?ORG isO BenB?PER BernankeI?PER
???B?ORG ??O ?O ?B?PER ???I?PER
Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences.
of the large amount of unannotated bitext, without
complicated machinery. More specifically, we in-
troduce a joint inference method that formulates the
bilingual NER tagging problem as an Integer Linear
Program (ILP) and solves it during decoding. We
propose a set of intuitive and effective bilingual con-
straints that encourage NER results to agree across
the two languages.
Experimental results on the OntoNotes 4.0 named
entity annotated Chinese-English parallel corpus
show that the proposed method can improve the
strong Chinese NER baseline by over 5% F1 score
and also give small improvements over the English
baseline. Moreover, by adding the automatically
tagged data to the original NER training corpus
and retraining the monolingual model using an up-
training regimen (Petrov et al, 2010), we can im-
prove the monolingual Chinese NER performance
by over 3% F1 score.
2 Constraint-based Monolingual NER
NER is a sequence labeling task where we assign
a named entity tag to each word in an input sen-
tence. One commonly used tagging scheme is the
BIO scheme. The tag B-X (Begin) represents the
first word of a named entity of type X, for example,
PER (Person) or LOC (Location). The tag I-X (In-
side) indicates that a word is part of an entity but not
first word. The tag O (Outside) is used for all non-
entity words.2 See Figure 1 for an example tagged
sentence.
Conditional Random Fields (CRF) (Lafferty et al,
2001) is a state-of-the-art sequence labeling model
widely used in NER. A first-order linear-chain CRF
2While the performance of NER is measured at the entity
level (not the tag level).
defines the following conditional probability:
PCRF (y|x) =
1
Z(x)
?
i
Mi(yi, yi?1|x) (1)
where x and y are the input and output sequences,
respectively, Z(x) is the partition function, and Mi
is the clique potential for edge clique i. Decoding
in CRF involves finding the most likely output se-
quence that maximizes this objective, and is com-
monly done by the Viterbi algorithm.
Roth and Yih (2005) proposed an ILP inference
algorithm, which can capture more task-specific and
global constraints than the vanilla Viterbi algorithm.
Our work is inspired by Roth and Yih (2005). But
instead of directly solving the shortest-path problem
in the ILP formulation, we re-define the conditional
probability as:
PMAR(y|x) =
?
i
P (yi|x) (2)
where P (yi|x) is the marginal probability given by
an underlying CRF model computed using forward-
backward inference. Since the early HMM litera-
ture, it has been well known that using the marginal
distributions at each position works well, as opposed
to Viterbi MAP sequence labeling (Me?rialdo, 1994).
Our experimental results also supports this claim, as
we will show in Section 6. Our objective is to find
an optimal NER tag sequence:
y? = argmax
y
PMAR(y|x)
= argmax
y
?
i
logP (yi|x) (3)
Then an ILP can be used to solve the inference
problem as classification problem with constraints.
53
The objective function is:
max
|x|?
i=1
?
y?Y
zyi logP
y
i (4)
where Y is the set of all possible named entity tags.
P yi = P (yi = y|x) is the CRF marginal probabil-
ity that the ith word is tagged with y, and zyi is an
indicator that equals 1 iff the ith word is tagged y;
otherwise, zyi is 0.
If no constraints are identified, then Eq. (4)
achieves maximum when all zyi are assigned to 1,
which violates the condition that each word should
only be assigned a single entity tag. We can express
this with constraints:
?i :
?
y?Y
zyi = 1 (5)
After adding the constraints, the probability of the
sequence is maximized when each word is assigned
the tag with highest probability. However, some in-
valid results may still exist. For example a tag O
may be wrongly followed by a tag I-X, although a
named entity cannot start with I-X. Therefore, we
can add the following constraints:
?i,?X : zB-Xi?1 + zI-Xi?1 ? zI-Xi ? 0 (6)
which specifies that when the ith word is tagged with
I-X (zI-Xi = 1), then the previous word can only be
tagged with B-X or I-X (zB-Xi?1 + zI-Xi?1 ? 1).
3 NER with Bilingual Constraints
This section demonstrates how to jointly perform
NER for two languages with bilingual constraints.
We assume sentences have been aligned into pairs,
and the word alignment between each pair of sen-
tences is also given.
3.1 Hard Bilingual Constraints
We first introduce the simplest hard constraints, i.e.,
each word alignment pair should have the same
named entity tag. For example, in Figure 1, the
Chinese word ????? was aligned with the En-
glish words ?the?, ?Federal? and ?Reserve?. There-
fore, they have the same named entity tags ORG.3
3The prefix B- and I- are ignored.
Similarly, ??? and ?Ben? as well as ????? and
?Bernanke? were all tagged with the tag PER.
The objective function for bilingual NER can be
expressed as follows:
max
|xc|?
i=1
?
y?Y
zyi logP
y
i +
|xe|?
j=1
?
y?Y
zyj logP
y
j (7)
where P yi and P
y
j are the probabilities of the i
th Chi-
nese word and jth English word to be tagged with y,
respectively. xc and xe are respectively the Chinese
and English sentences.
Similar to monolingual constrained NER (Sec-
tion 2), monolingual constraints are added for each
language as shown in Eqs. (8) and (9):
?i :
?
y?Y
zyi = 1;?j :
?
y?Y
zyj = 1 (8)
?i,?X : zB-Xi + zI-Xi ? zB-Xi+1 ? 0 (9)
?j,?X : zB-Xj + zI-Xj ? zB-Xj+1 ? 0
Bilingual constraints are added in Eq. (10):
?(i, j) ? A,?X : zB-Xi + zI-Xi = zB-Xj + zI-Xj (10)
where A = {(i, j)} is the word alignment pair set,
i.e., the ith Chinese word and the jth English word
were aligned together. Chinese word i is tagged with
a named entity type X (zB-Xi + zI-Xi = 1), iff English
word j is tagged with X (zB-Xj +zI-Xj = 1). Therefore,
these hard bilingual constraints guarantee that when
two words are aligned, they are tagged with the same
named entity tag.
However, in practice, aligned word pairs do not
always have the same tag because of the difference
in annotation standards across different languages.
For example, in Figure 2(a), the Chinese word ???
?? is a location. However, it is aligned to the words,
?development? and ?zone?, which are not named en-
tities in English. Word alignment error is another se-
rious problem that can cause violation of hard con-
straints. In Figure 2(b), the English word ?Agency?
is wrongly aligned with the Chinese word ?? (re-
port)?. Thus, these two words cannot be assigned
with the same tag.
To address these two problems, we present a prob-
abilistic model for bilingual NER which can lead to
54
ThisO developmentO zoneO isO locatedO inO . . .
??O ???B?LOC ?O ?O . . .
(a) Inconsistent named entity standards
XinhuaB?ORG NewsI?ORG AgencyI?ORG FebruaryO 16thO
???B?ORG ??B?LOC ???O ?O
(b) Word alignment error
Figure 2: Errors of hard bilingual constraints method.
an optimization problem with two soft bilingual con-
straints:
1) allow word-aligned pairs to have different
named entity tags; 2) consider word alignment prob-
abilities to reduce the influence of wrong word align-
ments.
3.2 Soft Constraints with Tag Uncertainty
The new probabilistic model for bilingual NER is:
P (yc,ye|xc,xe, A) =
P (yc,ye,xc,xe, A)
P (xc,xe, A)
= P (yc,xc,xe, A)
P (xc,xe, A)
? P (ye,xc,xe, A)
P (xc,xe, A)
? P (yc,ye,xc,xe, A)P (xc,xe, A)
P (yc,xc,xe, A)P (ye,xc,xe, A)
(11)
? P (yc|xc)P (ye|xe)
P (yc,ye|A)
P (yc|A)P (ye|A)
(12)
where yc and ye respectively denotes Chinese and
English named entity output sequences. A is the set
of word alignment pairs.
If we assume that named entity tag assignments in
Chinese is only dependent on the observed Chinese
sentence, then we can drop the A and xe term in the
first factor of Eq. (11), and arrive at the first factor of
Eq. (12); similarly we can use the same assumption
to derive the second factor in Eq. (12) for English;
alternatively, if we assume the named entity tag as-
signments are only dependent on the cross-lingual
word associations via word alignment, then we can
drop xc and xe terms in the third factor of Eq. (11)
and arrive at the third factor of Eq. (12). These fac-
tors represent the two major sources of information
in the model: monolingual surface observation, and
cross-lingual word associations.
The first two factors of Eq. (12) can be further
decomposed into the product of probabilities of all
words in each language sentence like Eq. (2).
Assuming that the tags are independent between
different word alignment pairs, then the last factor
of Eq. (12) can be decomposed into:
P (yc,ye|A)
P (yc|A)P (ye|A)
=
?
a?A
P (ycayea)
P (yca)P (yea)
=
?
a?A
?ycyea (13)
where yca and yea respectively denotes Chinese and
English named entity tags in a word alignment pair
a. ?ycye = P (ycye)P (yc)P (ye) is the pointwise mutual infor-
mation (PMI) score between a Chinese named en-
tity tag yc and an English named entity tag ye. If
yc = ye, then the score will be high; otherwise the
score will be low. A number of methods for calculat-
ing the scores are provided at the end of this section.
We use ILP to maximize Eq. (12). The new ob-
jective function is expressed as follow:
max
|xc|?
i=1
?
y?Y
zyi logP
y
i +
|xe|?
j=1
?
y?Y
zyj logP
y
j
+
?
a?A
?
yc?Y
?
ye?Y
zycyea log ?ycyea (14)
where zycyea is an indicator that equals 1 iff the Chi-
nese and English named entity tags are yc and ye
respectively, given a word alignment pair a; other-
wise, zycyea is 0.
Monolingual constraints such as Eqs. (8) and (9)
need to be added. In addition, one and only one pos-
sible named entity tag pair exists for a word align-
ment pair. This condition can be expressed as the
following constraints:
?a ? A :
?
yc?Y
?
ye?Y
zycyea = 1 (15)
When the tag pair of a word alignment pair is de-
termined, the corresponding monolingual named en-
55
tity tags can also be identified. This rule can be ex-
pressed by the following constraints:
?a = (i, j) ? A : zycyea ? zyci , zycyea ? z
ye
j (16)
Thus, if zycyea = 1, then zyci and z
ye
j must be both
equal to 1. Here, the ith Chinese word and the jth
English word are aligned together.
In contrast to hard bilingual constraints, inconsis-
tent named entity tags for an aligned word pair are
allowed in soft bilingual constraints, but are given
lower ?ycye scores.
To calculate the ?ycye score, an annotated bilin-
gual NER corpus is consulted. We count from all
word alignment pairs the number of times yc and ye
occur together (C(ycye)) and separately (C(yc) and
C(ye)). Afterwards, ?ycye is calculated with maxi-
mum likelihood estimation as follows:
?ycye = P (ycye)
P (yc)P (ye)
= N ? C(ycye)
C(yc)C(ye)
(17)
where N is the total number of word alignment
pairs.
However, in this paper, we assume that no named
entity annotated bilingual corpus is available. Thus,
the above method is only used as Oracle. A real-
istic method for calculating the ?ycye score requires
the use of two initial monolingual NER models, such
as baseline CRF, to predict named entity tags for
each language on an unannotated bitext. We count
from this automatically tagged corpus the statistics
mentioned above. This method is henceforth re-
ferred to as Auto.
A simpler approach is to manually set the value
of ?ycye : if yc = ye then we assign a larger value
to ?ycye ; else we assign an ad-hoc smaller value. In
fact, if we set ?ycye = 1 iff yc = ye; otherwise,
?ycye = 0, then the soft constraints backs off to hard
constraints. We refer to this set of soft constraints as
Soft-tag.
3.3 Constraints with Alignment Uncertainty
So far, we assumed that a word alignment set A is
known. In practice, only the word alignment proba-
bility Pa for each word pair a is provided. We can
set a threshold ? for Pa to tune the set A: a ? A
iff Pa ? ?. This condition can be regarded as a
kind of hard word alignment. However, the follow-
ing problem exists: the smaller the ?, the noisier the
word alignments are; the larger the ?, the more pos-
sible word alignments are lost. To ameliorate this
problem, we introduce another set of soft bilingual
constraints.
We can re-express Eq. (13) as follows:
?
a?A
?ycyea =
?
a?A
(?ycyea )Ia (18)
where A is the set of all word pairs between two
languages. Ia = 1 iff Pa ? ?; otherwise, Ia = 0.
We can then replace the hard indicator Ia with
the word alignment probability Pa, Eq. (14) is then
transformed into the following equation:
max
|Wc|?
i
?
y?Y
zyi logP
y
i +
|We|?
j
?
y?Y
zyj logP
y
j
+
?
a?A
?
yc?Y
?
ye?Y
zycyea Pa log ?ycyea (19)
We name the set of constraints above
Soft-align, which has the same constraints
as Soft-tag, i.e., Eqs. (8), (9), (15) and (16).
4 Experimental Setup
We conduct experiments on the latest OntoNotes
4.0 corpus (LDC2011T03). OntoNotes is a large,
manually annotated corpus that contains various text
genres and annotations, such as part-of-speech tags,
named entity labels, syntactic parse trees, predicate-
argument structures and co-references (Hovy et al,
2006). Aside from English, this corpus also con-
tains several Chinese and Arabic corpora. Some of
these corpora contain bilingual parallel documents.
We used the Chinese-English parallel corpus with
named entity labels as our development and test
data. This corpus includes about 400 document pairs
(chtb 0001-0325, ectb 1001-1078). We used odd-
numbered documents as development data and even-
numbered documents as test data. We used all other
portions of the named entity annotated corpus as
training data for the monolingual systems. There
were a total of?660 Chinese documents (?16k sen-
tences) and ?1,400 English documents (?39k sen-
tences). OntoNotes annotates 18 named entity types,
such as person, location, date and money. In this
paper, we selected the four most common named
entity types, i.e., PER (Person), LOC (Location),
56
Chinese NER Templates
00: 1 (class bias param)
01: wi+k,?1 ? k ? 1
02: wi+k?1 ? wi+k, 0 ? k ? 1
03: shape(wi+k),?4 ? k ? 4
04: prefix(wi, k), 1 ? k ? 4
05: prefix(wi?1, k), 1 ? k ? 4
06: suffix(wi, k), 1 ? k ? 4
07: suffix(wi?1, k), 1 ? k ? 4
08: radical(wi, k), 1 ? k ? len(wi)
Unigram Features
yi? 00 ? 08
Bigram Features
yi?1 ? yi? 00 ? 08
Table 1: Basic features of Chinese NER.
ORG (Organization) and GPE (Geo-Political Enti-
ties), and discarded the others.
Since the bilingual corpus is only aligned at the
document level, we performed sentence alignment
using the Champollion Tool Kit (CTK).4 After re-
moving sentences with no aligned sentence, a total
of 8,249 sentence pairs were retained.
We used the BerkeleyAligner,5 to produce
word alignments over the sentence-aligned datasets.
BerkeleyAligner also gives posterior probabilities
Pa for each aligned word pair.
We used the CRF-based Stanford NER tagger (us-
ing Viterbi decoding) as our baseline monolingual
NER tool.6 English features were taken from Finkel
et al (2005). Table 1 lists the basic features of
Chinese NER, where ? means string concatenation
and yi is the named entity tag of the ith word wi.
Moreover, shape(wi) is the shape of wi, such as
date and number. prefix/suffix(wi, k) denotes the
k-characters prefix/suffix of wi. radical(wi, k) de-
notes the radical of the kth Chinese character of wi.7
len(wi) is the number of Chinese characters in wi.
To make the baseline CRF taggers stronger, we
added word clustering features to improve gener-
alization over unseen data for both Chinese and
English. Word clustering features have been suc-
cessfully used in several English tasks, including
4champollion.sourceforge.net
5code.google.com/p/berkeleyaligner
6nlp.stanford.edu/software/CRF-NER.shtml,
which has included our English and Chinese NER implementations.
7The radical of a Chinese character can be found at: www.
unicode.org/charts/unihan.html
NER (Miller et al, 2004) and dependency pars-
ing (Koo et al, 2008). To our knowledge, this work
is the first use of word clustering features for Chi-
nese NER. A C++ implementation of the Brown
word clustering algorithms (Brown et al, 1992) was
used to obtain the word clusters (Liang, 2005).8
Raw text was obtained from the fifth edition of Chi-
nese Gigaword (LDC2011T13). One million para-
graphs from Xinhua news section were randomly
selected, and the Stanford Word Segmenter with
LDC standard was applied to segment Chinese text
into words.9 About 46 million words were obtained
which were clustered into 1,000 word classes.
5 Threshold Tuning
During development, we tuned the word alignment
probability thresholds to find the best value. Figure 3
shows the performance curves.
When the word alignment probability threshold ?
is set to 0.9, the hard bilingual constraints perform
well for both Chinese and English. But as the thresh-
olds value gets smaller, and more noisy word align-
ments are introduced, we see the hard bilingual con-
straints method starts to perform badly.
In Soft-tag setting, where inconsistent tag as-
signments within aligned word pairs are allowed but
penalized, different languages have different optimal
threshold values. For example, Chinese has an opti-
mal threshold of 0.7, whereas English has 0.2. Thus,
the optimal thresholds for different languages need
to be selected with care when Soft-tag is applied
in practice.
Soft-align eliminates the need for careful
tuning of word alignment thresholds, and therefore
can be more easily used in practice. Experimen-
tal results of Soft-align confirms our hypothe-
sis ? the performance of both Chinese and English
NER systems improves with decreasing threshold.
However, we can still improve efficiency by set-
ting a low threshold to prune away very unlikely
word alignments. We set the threshold to 0.1 for
Soft-align to increase speed, and we observed
very minimal performance lost when doing so.
We also found that automatically estimated bilin-
gual tag PMI scores (Auto) gave comparable results
8github.com/percyliang/brown-cluster
9nlp.stanford.edu/software/segmenter.shtml
57
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability
55
60
65
70
75
per
form
anc
e (F
1)
HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)
(a) Chinese
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability
60
65
70
75
80
per
form
anc
e (F
1)
HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)
(b) English
Figure 3: Performance curves of different bilingual constraints methods on development set.
to Oracle. Therefore this technique is effective
for computing the PMI scores, avoiding the need of
manually annotating named entity bilingual corpus.
6 Bilingual NER Results
The main results on Chinese and English test sets
with the optimal word alignment threshold for each
method are shown in Table 2.
The CRF-based Chinese NER with and without
word clustering features are compared here. The
word clustering features significantly (p < 0.01) im-
proved the performance of Chinese NER, 10 giving
us a strong Chinese NER baseline.11 The effective-
ness of word clustering for English NER has been
proved in previous work.
The performance of ILP with only monolingual
constraints is quite comparable with the CRF re-
sults, especially on English. The greater ILP perfor-
mance on English is probably due to more accurate
marginal probabilities estimated by the English CRF
model.
The ILP model with hard bilingual constraints
gives a slight performance improvement on Chi-
nese, but affects performance negatively on English.
Once we introduced tagging uncertainties into the
Soft-tag bilingual constraints, we see a very sig-
10We use paired bootstrap resampling significance test (Efron
and Tibshirani, 1993).
11To the best of our knowledge, there was no performance
report of state-of-the-art NER results on the latest OntoNotes
dataset.
nificant (p < 0.01) performance boost on Chinese.
This method also improves the recall on English,
with a smaller decrease in precision. Overall, it im-
proves English F1 score by about 0.4%, which is un-
fortunately not statistically significant.
Compared with Soft-tag, the final
Soft-align method can further improve
performance on both Chinese and English. This is
likely to be because: 1) Soft-align includes
more word alignment pairs, thereby improving
recall; and 2) uses probabilities to cut wrong
word alignments, thereby improving precision. In
particular, compared with the strong CRF baseline,
the gain on Chinese side is almost 5.5% in absolute
F1 score.
Decoding/inferenc efficiency of different methods
are shown in the last column of Table 2.12 Com-
pared with Viterbi decoding in CRF, monolingual
ILP decoding is about 2.3 times slower. Bilingual
ILP decoding, with either hard or soft constraints, is
significantly slower than the monolingual methods.
The reason is that the number of monolingual ILP
constraints doubles, and there are additionally many
more bilingual constraints. The difference in speed
between the Soft-tag and Soft-align meth-
ods is attributed to the difference in number of word
alignment pairs.
Since each sentence pair can be decoded indepen-
12CPU: Intel Xeon E5-2660 2.20GHz. And the speed cal-
culation of ILP inference methods exclude the time needed to
obtain marginal probabilities from the CRF models.
58
Chinese English Speed
P R F1 P R F1 #sent/s
CRF (No Cluster) 74.74 56.17 64.13 ? ? ? ?
CRF (Word Cluster) 76.90 63.32 69.45 82.95 76.67 79.68 317.3
Monolingual ILP 76.20 63.06 69.01 82.88 76.68 79.66 138.0
Hard 74.38 65.78 69.82 82.66 75.36 78.84 21.1
Soft-tag (Auto) 77.37 71.14 74.13 81.36 78.74 80.03 5.9
Soft-align (Auto) 77.71 72.51 75.02 81.94 78.35 80.10 1.5
Table 2: Results on bilingual parallel test set.
dently, parallelization the decoding process can re-
sult in significant speedup.
7 Semi-supervised NER Results
The above results show the usefulness of our method
in a bilingual setting, where we are presented with
sentence aligned data, and are tagging both lan-
guages at the same time. To have a greater impact
on general monolingual NER systems, we employ
a semi-supervised learning setting. First, we tag a
large amount of unannotated bitext with our bilin-
gual constraint-based NER tagger. Then we mix the
automatically tagged results with the original mono-
lingual Chinese training data to train a new model.
Our bitext is derived from the Chinese-English
part of the Foreign Broadcast Information Service
corpus (FBIS, LDC2003E14). The best perform-
ing bilingual model Soft-align with threshold
? = 0.1 was used under the same experimental set-
ting as described in Section 4
Method #sent P R F1
CRF ?16k 76.90 63.32 69.45
Semi
10k 77.60 66.51 71.62
20k 77.28 67.26 71.92
40k 77.40 67.81 72.29
80k 77.44 68.64 72.77
Table 3: Semi-supervised results on Chinese test set.
Table 3 shows that the performance of the semi-
supervised method improves with more additional
data. We simply appended these data to the orig-
inal training data. We also have done the experi-
ments to down weight the additional training data
by duplicating the original training data. There
was some slight improvements, but not very signif-
icant. Finally, when we add 80k sentences, the F1
score is improved by 3.32%, which is significantly
(p < 0.01) better than the baseline, and most of the
contribution comes from recall improvement.
Before the end of experimental section, let us
summarize the usage of different kinds of data re-
sources used in our experiments, as shown in Ta-
ble 4, where  and ? denote whether the corre-
sponding resources are required. In the bilingual
case, during training, only the monolingual named
entity annotated data (NE-mono) is necessary to
train a monolingual NER tagger. During the test,
unannotated bitext (Bitext) is required by the word
aligner and our bilingual NER tagger. Named entity
annotated bitext (NE-bitext) is used to evaluate our
bilingual model. In the semi-supervised case, be-
sides the original NE-mono data, the Bitext is used
as input to our bilingual NER tagger to product ad-
ditional training data. To evaluate the final NER
model, only NE-mono is needed.
NE-mono Bitext NE-bitext
Bilingual
train  ? ?
test ?  
Semi
train   ?
test  ? ?
Table 4: Summarization of the data resource usage
8 Related Work
Previous work explored the use of bilingual corpora
to improve existing monolingual analyzers. Huang
et al (2009) proposed methods to improve parsing
performance using bilingual parallel corpus. Li et
al. (2012) jointly labeled bilingual named entities
with a cyclic CRF model, where approximate in-
ference was done using loopy belief propagation.
These methods require manually annotated bilingual
59
corpora, which are expensive to construct, and hard
to obtain. Kim et al (2012) proposed a method of
labeling bilingual corpora with named entity labels
automatically based on Wikipedia. However, this
method is restricted to topics covered by Wikipedia.
Similar to our work, Burkett et al (2010) also as-
sumed that annotated bilingual corpora are scarce.
Beyond the difference discussed in Section 1, their
re-ranking strategy may lose the correct named en-
tity results if they are not included in the top-N out-
puts. Furthermore, we consider the word alignment
probabilities in our method which can reduce the in-
fluence of word alignment errors. Finally, we test
our method on a large standard publicly available
corpus (8,249 sentences), while they used a much
smaller (200 sentences) manually annotated bilin-
gual NER corpus for results validation.
In addition to bilingual corpora, bilingual dictio-
naries are also useful resources. Huang and Vo-
gel (2002) and Chen et al (2010) proposed ap-
proaches for extracting bilingual named entity pairs
from unannotated bitext, in which verification is
based on bilingual named entity dictionaries. How-
ever, large-scale bilingual named entity dictionaries
are difficult to obtain for most language pairs.
Yarowsky and Ngai (2001) proposed a projection
method that transforms high-quality analysis results
of one language, such as English, into other lan-
guages on the basis of word alignment. Das and
Petrov (2011) applied the above idea to part-of-
speech tagging with a more complex model. Fu et al
(2011) projected English named entities onto Chi-
nese by carefully designed heuristic rules. Although
this type of method does not require manually an-
notated bilingual corpora or dictionaries, errors in
source language results, wrong word alignments and
inconsistencies between the languages limit applica-
tion of this method.
Constraint-based monolingual methods by using
ILP have been successfully applied to many natural
language processing tasks, such as Semantic Role
Labeling (Punyakanok et al, 2004), Dependency
Parsing (Martins et al, 2009) and Textual Entail-
ment (Berant et al, 2011). Zhuang and Zong (2010)
proposed a joint inference method for bilingual se-
mantic role labeling with ILP. However, their ap-
proach requires training an alignment model with a
manually annotated corpus.
9 Conclusions
We proposed a novel ILP based inference algorithm
with bilingual constraints for NER. This method
can jointly infer bilingual named entities without
using any annotated bilingual corpus. We in-
vestigate various bilingual constraints: hard and
soft constraints. Out empirical study on large-
scale OntoNotes Chinese-English parallel NER data
showed that Soft-align method, which allows
inconsistent named entity tags between two aligned
words and considers word alignment probabilities,
can significantly improve over the performance of
a strong Chinese NER baseline. Our work is the
first to evaluate performance on a large-scale stan-
dard dataset. Finally, we can also improve mono-
lingual Chinese NER performance significantly, by
combining the original monolingual training data
with new data obtained from bitext tagged by our
method. The final ILP-based bilingual NER tag-
ger with soft constraints is publicly available at:
github.com/carfly/bi_ilp
Future work could apply the bilingual constraint-
based method to other tasks, such as part-of-speech
tagging and relation extraction.
Acknowledgments
The authors would like to thank Rob Voigt and the
three anonymous reviewers for their valuable com-
ments and suggestions. We gratefully acknowledge
the support of the National Natural Science Foun-
dation of China (NSFC) via grant 61133012, the
National ?863? Project via grant 2011AA01A207
and 2012AA011102, the Ministry of Education Re-
search of Social Sciences Youth funded projects
via grant 12YJCZH304, the Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
60
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 610?619, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479, December.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 46?54, Uppsala, Sweden, July.
Association for Computational Linguistics.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010.
On jointly recognizing and aligning bilingual named
entities. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 631?639, Uppsala, Sweden, July. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 363?370, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Ruiji Fu, Bing Qin, and Ting Liu. 2011. Generating
chinese named entity data from a parallel corpus. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 264?272, Chiang
Mai, Thailand, November. Asian Federation of Natural
Language Processing.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ?06,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, ICMI 2002,
Washington, DC, USA. IEEE Computer Society.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222?1231, Singapore, August. Association for
Computational Linguistics.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 694?702, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Shankar Kumar. 2005. Minimum bayes-risk techniques
in automatic speech recognition and statistical ma-
chine translation. Ph.D. thesis, Baltimore, MD, USA.
AAI3155633.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei
Huang. 2012. Joint bilingual name tagging for paral-
lel corpora. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM 2012), Honolulu, Hawaii, Octo-
ber.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, MIT.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Bernard Me?rialdo. 1994. Tagging english text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
61
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 337?342, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of Coling
2004, pages 1346?1352, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ?05, pages 736?743, New
York, NY, USA. ACM.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies, NAACL ?01, pages 1?8, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 304?314, Cambridge,
MA, October. Association for Computational Linguis-
tics.
62
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036?1044,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering with Source Language Collocations 
 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Ting Liu1, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu Inc., Beijing, China 
{liuzhanyi, wanghaifeng, wu_hua}@baidu.com  
{tliu, lisheng}@hit.edu.cn 
 
 
 
Abstract 
This paper proposes a novel reordering model 
for statistical machine translation (SMT) by 
means of modeling the translation orders of 
the source language collocations. The model 
is learned from a word-aligned bilingual cor-
pus where the collocated words in source sen-
tences are automatically detected. During 
decoding, the model is employed to softly 
constrain the translation orders of the source 
language collocations, so as to constrain the 
translation orders of those source phrases con-
taining these collocated words. The experi-
mental results show that the proposed method 
significantly improves the translation quality, 
achieving the absolute improvements of 
1.1~1.4 BLEU score over the baseline me-
thods. 
1 Introduction 
Reordering for SMT is first proposed in IBM mod-
els (Brown et al, 1993), usually called IBM con-
straint model, where the movement of words 
during translation is modeled. Soon after, Wu 
(1997) proposed an ITG (Inversion Transduction 
Grammar) model for SMT, called ITG constraint 
model, where the reordering of words or phrases is 
constrained to two kinds: straight and inverted. In 
order to further improve the reordering perfor-
mance, many structure-based methods are pro-
posed, including the reordering model in 
hierarchical phrase-based SMT systems (Chiang, 
2005) and syntax-based SMT systems (Zhang et al, 
2007; Marton and Resnik, 2008; Ge, 2010; Vis-
weswariah et al, 2010). Although the sentence 
structure has been taken into consideration, these 
methods don?t explicitly make use of the strong 
correlations between words, such as collocations, 
which can effectively indicate reordering in the 
target language. 
In this paper, we propose a novel method to im-
prove the reordering for SMT by estimating the 
reordering score of the source-language colloca-
tions (source collocations for short in this paper). 
Given a bilingual corpus, the collocations in the 
source sentence are first detected automatically 
using a monolingual word alignment (MWA) me-
thod without employing additional resources (Liu 
et al, 2009), and then the reordering model based 
on the detected collocations is learned from the 
word-aligned bilingual corpus. The source colloca-
tion based reordering model is integrated into SMT 
systems as an additional feature to softly constrain 
the translation orders of the source collocations in 
the sentence to be translated, so as to constrain the 
translation orders of those source phrases contain-
ing these collocated words. 
This method has two advantages: (1) it can au-
tomatically detect and leverage collocated words in 
a sentence, including long-distance collocated 
words; (2) such a reordering model can be inte-
grated into any SMT systems without resorting to 
any additional resources. 
We implemented the proposed reordering mod-
el in a phrase-based SMT system, and the evalua-
tion results show that our method significantly 
improves translation quality. As compared to the 
baseline systems, an absolute improvement of 
1.1~1.4 BLEU score is achieved.  
1036
The paper is organized as follows: In section 2, 
we describe the motivation to use source colloca-
tions for reordering, and briefly introduces the col-
location extraction method. In section 3, we 
present our reordering model. And then we de-
scribe the experimental results in section 4 and 5. 
In section 6, we describe the related work.  Lastly, 
we conclude in section 7. 
2 Collocation 
A collocation is generally composed of a group of 
words that occur together more often than by 
chance. Collocations effectively reveal the strong 
association among words in a sentence and are 
widely employed in a variety of NLP tasks 
(Mckeown and Radey, 2000).   
Given two words in a collocation, they can be 
translated in the same order as in the source lan-
guage, or in the inverted order. We name the first 
case as straight, and the second inverted. Based on 
the observation that some collocations tend to have 
fixed translation orders such as ??? jin-rong ?fi-
nancial? ??  wei-ji ?crisis?? (financial crisis) 
whose English translation order is usually straight, 
and  ???  fa-lv ?law? ??  fan-wei ?scope?? 
(scope of law) whose English translation order is 
generally inverted, some methods have been pro-
posed to improve the reordering model for SMT 
based on the collocated words crossing the neigh-
boring components (Xiong et al, 2006). We fur-
ther notice that some words are translated in 
different orders when they are collocated with dif-
ferent words. For instance, when ??? chao-liu 
?trend?? is collocated with ??? shi-dai ?times??, 
they are often translated into the ?trend of times?; 
when collocated with ??? li-shi ?history??, the 
translation usually becomes the ?historical trend?. 
Thus, if we can automatically detect the colloca-
tions in the sentence to be translated and their or-
ders in the target language, the reordering 
information of the collocations could be used to 
constrain the reordering of phrases during decod-
ing. Therefore, in this paper, we propose to im-
prove the reordering model for SMT by estimating 
the reordering score based on the translation orders 
of the source collocations. 
In general, the collocations can be automatically 
identified based on syntactic information such as 
dependency trees (Lin, 1998). However these me-
thods may suffer from parsing errors. Moreover, 
for many languages, no valid dependency parser 
exists. Liu et al (2009) proposed to automatically 
detect the collocated words in a sentence with the 
MWA method. The advantage of this method lies 
in that it can identify the collocated words in a sen-
tence without additional resources. In this paper, 
we employ MWA Model l~3 described in Liu et al 
(2009) to detect collocations in sentences, which 
are shown in Eq. (1)~(3). 
?
?
?
l
j
cj jwwtSAp 11 ModelMWA 
)|()|(
 (1) 
?
?
??
l
j
jcj lcjdwwtSAp j12 ModelMWA 
),|()|()|(
 (2) 
?
?
?
?
?
???
l
j
jcj
l
i
ii
lcjdwwt
wnSAp
j
1
1
3 ModelMWA 
),|()|(
)|()|(
 (3) 
Where lwS 1?  is a monolingual sentence; i?  de-
notes the number of words collocating with 
iw ; 
}&],1[|),{( icliciA ii ???  denotes the potentially 
collocated words in S. 
The MWA models measure the collocated 
words under different constraints. MWA Model 1 
only models word collocation probabilities 
)|( jcj wwt
. MWA Model 2 additionally employs 
position collocation probabilities 
),|( lcjd j
. Be-
sides the features in MWA Model 2, MWA Model 
3 also considers fertility probabilities )|( ii wn ? . 
Given a sentence, the optimal collocated words 
can be obtained according to Eq. (4). 
)|(maxarg*  ModelMWA SApA iA?
           (4) 
Given a monolingual word aligned corpus, the 
collocation probabilities can be estimated as fol-
lows. 
2
)|()|(),( ijjiji wwpwwpwwr ??           
(5) 
Where, 
?
?
??
w
j
ji
ji wwcount
wwcountwwp ),(
),()|(
; 
),( ji ww  
denotes the collocated words in the corpus and 
),( ji wwcount
 denotes the co-occurrence frequency. 
1037
3 Reordering Model with Source Lan-
guage Collocations 
In this section, we first describe how to estimate 
the orientation probabilities for a given collocation, 
and then describe the estimation of the reordering 
score during translation. Finally, we describe the 
integration of the reordering model into the SMT 
system. 
3.1 Reordering probability estimation 
Given a source collocation ),( ji ff
 and its corres-
ponding translations 
),( ji aa ee
 in a bilingual sen-
tence pair, the reordering orientation of the 
collocation can be defined as in Eq. (6).  
??
?
????
?????
jiji
jiji
aaji aajiaaji
aajiaajio ji &or& ifinverted
or ifstraight
,,,
(6) 
In our method, only those collocated words in 
source language that are aligned to different target 
words, are taken into consideration, and those be-
ing aligned to the same target word are ignored. 
Given a word-aligned bilingual corpus where 
the collocations in source sentences are detected, 
the probabilities of the translation orientation of 
collocations in the source language can be esti-
mated, as follows: 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,straight(),|straight(
   (7) 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,inverted(),|inverted(
   
(8) 
Here, ),,( ji ffocount
 is collected according to 
the algorithm in Figure 1. 
3.2 Reordering model 
Given a sentence lfF 1?  to be translated, the col-
locations are first detected using the algorithm de-
scribed in Eq. (4). Then the reordering score is 
estimated according to the reordering probability 
weighted by the collocation probability of the col-
located words. Formally, for a generated transla-
tion candidate T , the reordering score is calculated 
as follows. 
),|(log),(),( ,,,),( iiciii i ciaacici ciO ffopffrTFP ??
    (9) 
Input: A word-aligned bilingual corpus where 
the source collocations are detected 
Initialization: 
),,( ji ffocount
=0 
for each sentence pair <F, E> in the corpus do 
for each collocated word pair 
),( ici ff
in F do 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffstraightocount
 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffinvertedocount  
Output: ),,( ji ffocount
 
Figure 1. Algorithm of estimating  
reordering frequency 
Here, 
),( ici ffr
 denotes the collocation probabil-
ity of 
if  and icf
 as shown in Eq. (5). 
In addition to the detected collocated words in 
the sentence, we also consider other possible word 
pairs whose collocation probabilities are higher 
than a given threshold.  Thus, the reordering score 
is further improved according to Eq. (10). 
?
?
??
?
??
????
),(&
)},{(),(
,,,
,,,
),(
)},|(log),(
),|(log),(),(
ji
i
ji
iicii
i
i
ffr
ciji
jiaajiji
ciaaci
ci
ciO
ffopffr
ffopffrTFP
 
(10) 
Where ? and ?  are two interpolation weights. 
?  is the threshold of collocation probability. The 
weights and the threshold can be tuned using a de-
velopment set. 
3.3 Integrated into SMT system 
The SMT systems generally employ the log-linear 
model to integrate various features (Chiang, 2005; 
Koehn et al, 2007). Given an input sentence F, the 
final translation E* with the highest score is chosen 
from candidates, as in Eq. (11). 
}),({maxarg*
1
?
?
? M
m
mmE
FEhE ?
 (11) 
Where hm(E, F) (m=1,...,M) denotes fea-
tures.
m?  is a feature weight. 
Our reordering model can be integrated into the 
system as one feature as shown in (10). 
1038
 Figure 2. An example for reordering 
4 Evaluation of Our Method 
4.1 Implementation 
We implemented our method in a phrase-based 
SMT system (Koehn et al, 2007). Based on the 
GIZA++ package (Och and Ney, 2003), we im-
plemented a MWA tool for collocation detection. 
Thus, given a sentence to be translated, we first 
identify the collocations in the sentence, and then 
estimate the reordering score according to the 
translation hypothesis. For a translation option to 
be expanded, the reordering score inside this 
source phrase is calculated according to their trans-
lation orders of the collocations in the correspond-
ing target phrase. The reordering score crossing the 
current translation option and the covered parts can 
be calculated according to the relative position of 
the collocated words. If the source phrase matched 
by the current translation option is behind the cov-
ered parts in the source sentence, then 
...)|staight(log ?op  is used, otherwise 
...)|inverted(log ?op . For example, in Figure 2, the 
current translation option is (
4332 eeff ? ). The 
collocations related to this translation option are 
),( 31 ff , ),( 32 ff , ),( 53 ff . The reordering scores 
can be estimated as follows: 
),|straight(log),( 3131 ffopffr ? 
),|inverted(log),( 3232 ffopffr ? 
),|inverted(log),( 5353 ffopffr ? 
In order to improve the performance of the de-
coder, we design a heuristic function to estimate 
the future score, as shown in Figure 3. For any un-
covered word and its collocates in the input sen-
tence, if the collocate is uncovered, then the higher 
reordering probability is used. If the collocate has 
been covered, then the reordering orientation can 
Input: Input sentence LfF 1?  
Initialization: Score = 0 
for each uncovered word 
if  do 
for each word
jf
(
icj ?  
or 
??)( , ji ffr
) do 
if 
jf
 is covered then 
if i > j then 
Score+=
),|straight(log)( , jiji ffopffr ?
 
else 
Score+=
),|inverted(log)( , jiji ffopffr ? 
else 
 Score +=
),|(log)(maxarg , jijio ffopffr
 
Output: Score 
Figure 3. Heuristic function for estimating future 
score 
be determined according to the relative positions of 
the words and the corresponding reordering proba-
bility is employed. 
4.2 Settings 
We use the FBIS corpus (LDC2003E14) to train a 
Chinese-to-English phrase-based translation model. 
And the SRI language modeling toolkit (Stolcke, 
2002) is used to train a 5-gram language model on 
the English sentences of FBIS corpus.  
We used the NIST evaluation set of 2002 as the 
development set to tune the feature weights of the 
SMT system and the interpolation parameters, 
based on the minimum error rate training method 
(Och, 2003), and the NIST evaluation sets of 2004 
and 2008 (MT04 and MT08) as the test sets. 
We use BLEU (Papineni et al, 2002) as evalua-
tion metrics. We also calculate the statistical signi-
ficance differences between our methods and the 
baseline method by using the paired bootstrap re-
sample method (Koehn, 2004). 
4.3 Translation results 
We compare the proposed method with various 
reordering methods in previous work. 
Monotone model: no reordering model is used. 
Distortion based reordering (DBR) model: a 
distortion based reordering method (Al-
Onaizan & Papineni, 2006). In this method, the 
distortion cost is defined in terms of words, ra-
ther than phrases. This method considers out-
bound, inbound, and pairwise distortions that  
f1    f2     f3     f4      f5 
e4 
e3 
e2 
e1 
1039
Reorder models MT04 MT08 
Monotone model 26.99 18.30 
DBR model 26.64 17.83 
MSDR model (Baseline) 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
SCBR Model 1 29.21 19.28 
SCBR Model 2 29.44 19.36 
SCBR Model 3 29.50 19.44 
SCBR models (1+2) 29.65 19.57 
SCBR models (1+2+3) 29.75 19.61 
Table 1. Translation results on various reordering models 
 
T1: The two sides are also the basic stand of not relaxed. 
T2: The basic stance of the two sides have not relaxed. 
Reference: The basic stances of both sides did not move. 
Figure 4. Translation example.  (*/*) denotes (pstraight / pinverted)
 are directly estimated by simple counting over 
alignments in the word-aligned bilingual cor-
pus. This method is similar to our proposed 
method. But our method considers the transla-
tion order of the collocated words. 
msd-bidirectional-fe reordering (MSDR or 
Baseline) model: it is one of the reordering 
models in Moses. It considers three different 
orientation types (monotone, swap, and discon-
tinuous) on both source phrases and target 
phrases. And the translation orders of both the 
next phrase and the previous phrase in respect 
to the current phrase are modeled. 
Source collocation based reordering (SCBR) 
model: our proposed method. We investigate 
three reordering models based on the corres-
ponding MWA models and their combinations. 
In SCBR Model i (i=1~3), we use MWA Mod-
el i as described in section 2 to obtain the col-
located words and estimate the reordering 
probabilities according to section 3. 
The experiential results are shown in Table 1. 
The DBR model suffers from serious data sparse-
ness. For example, the reordering cases in the 
trained pairwise distortion model only covered 
32~38% of those in the test sets. So its perfor-
mance is worse than that of the monotone model. 
The MSDR model achieves higher BLEU scores 
than the monotone model and the DBR model. Our 
models further improve the translation quality, 
achieving better performance than the combination 
of MSDR model and DBR model. The results in 
Table 1 show that ?MSDR + SCBR Model 3? per-
forms the best among the SCBR models. This is 
because, as compared to MWA Model 1 and 2, 
MWA Model 3 takes more information into con-
sideration, including not only the co-occurrence 
information of lexical tokens and the position of 
words, but also the fertility of words in a sentence. 
And when the three SCBR models are combined, 
the performance of the SMT system is further im-
proved. As compared to other reordering models, 
our models achieve an absolute improvement of 
0.98~1.19 BLEU score on the test sets, which are 
statistically significant (p < 0.05).  
Figure 4 shows an example: T1 is generated by 
the baseline system and T2 is generated by the sys-
tem where the SCBR models (1+2+3)1 are used.  
                                                          
1 In the remainder of this paper, ?SCBR models? means the 
combination of the SCBR models (1+2+3) unless it is explicit-
ly explained.  
Input:  ??     ?   ??      ??  ?    ?  ??  ??   ? 
shuang-fang    DE    ji-ben       li-chang   ye      dou mei-you song-dong . 
(0.99/0.01) 
both-side       DE     basic          stance  also    both    not        loose     . 
(0.21/0.79) 
(0.95/0.05) 
1040
Reordering models MT04 MT08 
MSDR model 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
CBR model 28.96 18.77 
WCBR model 29.15 19.10 
WCBR+SCBR 
models 
29.87 19.83 
Table 2. Translation results of co-occurrence 
based reordering models 
 CBR model 
SCBR 
Model3 
Consecutive words 77.9% 73.5% 
Interrupted words 74.1% 87.8% 
Total 74.3% 84.9% 
Table 3. Precisions of the reordering models on 
the development set 
The input sentence contains three collocations. The 
collocation (??, ??) is included in the same 
phrase and translated together as a whole. Thus its 
translation is correct in both translations. For the 
other two long-distance collocations (??, ??) 
and (??, ??), their translation orders are not 
correctly handled by the reordering model in the 
baseline system. For the collocation (??, ??), 
since the SCBR models indicate p(o=straight|??, 
??) < p(o=inverted|??, ??), the system fi-
nally generates the translation T2 by constraining 
their translation order with the proposed model. 
5 Collocations vs. Co-occurring Words 
We compared our method with the method that 
models the reordering orientations based on co-
occurring words in the source sentences, rather 
than the collocations.  
5.1 Co-occurrence based reordering model 
We use the similar algorithm described in section 3 
to train the co-occurrence based reordering (CBR) 
model, except that the probability of the reordering 
orientation is estimated on the co-occurring words 
and the relative distance. Given an input sentence 
and a translation candidate, the reordering score is 
estimated as shown in Eq. (12). 
? ??? ),( ,,, ),,|(log),( ji jijiaajiO ffopTFP ji
        (12) 
Here, 
ji??
 is the relative distance of two words 
in the source sentence.  
We also construct the weighted co-occurrence 
based reordering (WCBR) model. In this model, 
the probability of the reordering orientation is ad-
ditionally weighted by the pointwise mutual infor-
mation 2  score of the two words (Manning and 
Sch?tze, 1999), which is estimated as shown in Eq. 
(13). 
? ???
),(
,,,MI ),,|(log),(
),(
ji
jijiaajiji
O
ffopffs
TFP
ji
   (13) 
5.2 Translation results 
Table 2 shows the translation results. It can be seen 
that the performance of the SMT system is im-
proved by integrating the CBR model. The perfor-
mance of the CBR model is also better than that of 
the DBR model. It is because the former is trained 
based on all co-occurring aligned words, while the 
latter only considers the adjacent aligned words. 
When the WCBR model is used, the translation 
quality is further improved. However, its perfor-
mance is still inferior to that of the SCBR models, 
indicating that our method (SCBR models) of 
modeling the translation orders of source colloca-
tions is more effective. Furthermore, we combine 
the weighted co-occurrence based model and our 
method, which outperform all the other models. 
5.3 Result analysis 
Precision of prediction 
First of all, we investigate the performance of 
the reordering models by calculating precisions of 
the translation orders predicted by the reordering 
models. Based on the source sentences and refer-
ence translations of the development set, where the 
source words and target words are automatically 
aligned by the bilingual word alignment method, 
we construct the reference translation orders for 
two words. Against the references, we calculate 
three kinds of precisions as follows: 
|}1|||{|
|}&1{|
,
,,,,
CW ??
???? jio
ooj||iP
ji
aajiji ji
 (14) 
                                                          
2 For occurring words extraction, the window size is set to [-6, 
+6]. 
1041
|}1|||{|
|}&1{|
,
,,,,
IW ??
???? jio
ooj||iP
ji
aajiji ji
 (15) 
 |}{|
|}{|
,
,,,,
total
ji
aajiji
o
ooP ji??
 (16) 
Here, 
jio ,
 denotes the translation order of (
ji ff ,
) 
predicted by the reordering models. If 
)|straight( , ji ffop ?
>
),inverted( ji f|fop ?
, then 
straight, ?jio
, else if 
)|straight( , ji ffop ?
< 
),inverted( ji f|fop ?
, then
inverted, ?jio
. 
ji aajio ,,,
 
denotes the translation order derived from the word 
alignments. If 
ji aajiji oo ,,,, ?
, then the predicted 
translation order is correct, otherwise wrong. 
CWP  
and 
IWP  denote the precisions calculated on the 
consecutive words and the interrupted words in the 
source sentences, respectively. 
totalP  denotes the 
precision on both cases. Here, the CBR model and 
SCBR Model 3 are compared. The results are 
shown in Table 3.  
From the results in Table 3, it can be seen that 
the CBR model has a higher precision on the con-
secutive words than the SCBR model, but lower 
precisions on the interrupted words. It is mainly 
because the CBR model introduces more noise 
when the relative distance of words is set to a large 
number, while the MWA method can effectively 
detect the long-distance collocations in sentences 
(Liu et al, 2009). This explains why the combina-
tion of the two models can obtain the highest 
BLEU score as shown in Table 2. On the whole, 
the SCBR Model 3 achieves higher precision than 
the CBR model. 
Effect of the reordering model 
Then we evaluate the reordering results of the 
generated translations in the test sets. Using the 
above method, we construct the reference transla-
tion orders of collocations in the test sets. For a 
given word pair in a source sentence, if the transla-
tion order in the generated translation is the same 
as that in the reference translations, then it is cor-
rect, otherwise wrong. 
We compare the translations of the baseline me-
thod, the co-occurrence based method, and our me-
thod (SCBR models). The precisions calculated on 
both kinds of words are shown in Table 4. From 
Test sets 
Baseline 
(MSDR) 
MSDR+ 
WCBR 
MSDR+ 
SCBR 
MT04 78.9% 80.8% 82.5% 
MT08 80.7% 83.8% 85.0% 
Table 4. Precisions (total) of the reordering 
models on the test sets 
the results, it can be seen that our method achieves 
higher precisions than both the baseline and the 
method modeling the translation orders of the co-
occurring words. It indicates that the proposed me-
thod effectively constrains the reordering of source 
words during decoding and improves the transla-
tion quality. 
6 Related Work 
Reordering was first proposed in the IBM models 
(Brown et al, 1993), later was named IBM con-
straint by Berger et al (1996). This model treats 
the source word sequence as a coverage set that is 
processed sequentially and a source token is cov-
ered when it is translated into a new target token. 
In 1997, another model called ITG constraint was 
presented, in which the reordering order can be 
hierarchically modeled as straight or inverted for 
two nodes in a binary branching structure (Wu, 
1997). Although the ITG constraint allows more 
flexible reordering during decoding, Zens and Ney 
(2003) showed that the IBM constraint results in 
higher BLEU scores. Our method models the reor-
dering of collocated words in sentences instead of 
all words in IBM models or two neighboring 
blocks in ITG models. 
For phrase-based SMT models, Koehn et al 
(2003) linearly modeled the distance of phrase 
movements, which results in poor global reorder-
ing. More methods are proposed to explicitly mod-
el the movements of phrases (Tillmann, 2004; 
Koehn et al, 2005) or to directly predict the orien-
tations of phrases (Tillmann and Zhang, 2005; 
Zens and Ney, 2006), conditioned on current 
source phrase or target phrase. Hierarchical phrase-
based SMT methods employ SCFG bilingual trans-
lation model and allow flexible reordering (Chiang, 
2005). However, these methods ignored the corre-
lations among words in the source language or in 
the target language. In our method, we automati-
cally detect the collocated words in sentences and 
1042
their translation orders in the target languages, 
which are used to constrain the ordering models 
with the estimated reordering (straight or inverted) 
score. Moreover, our method allows flexible reor-
dering by considering both consecutive words and 
interrupted words. 
In order to further improve translation results, 
many researchers employed syntax-based reorder-
ing methods (Zhang et al, 2007; Marton and Res-
nik, 2008; Ge, 2010; Visweswariah et al, 2010). 
However these methods are subject to parsing er-
rors to a large extent. Our method directly obtains 
collocation information without resorting to any 
linguistic knowledge or tools, therefore is suitable 
for any language pairs. 
In addition, a few models employed the collo-
cation information to improve the performance of 
the ITG constraints (Xiong et al, 2006). Xiong et 
al. used the consecutive co-occurring words as col-
location information to constrain the reordering, 
which did not lead to higher translation quality in 
their experiments. In our method, we first detect 
both consecutive and interrupted collocated words 
in the source sentence, and then estimated the 
reordering score of these collocated words, which 
are used to softly constrain the reordering of source 
phrases. 
7 Conclusions 
We presented a novel model to improve SMT by 
means of modeling the translation orders of source 
collocations. The model was learned from a word-
aligned bilingual corpus where the potentially col-
located words in source sentences were automati-
cally detected by the MWA method. During 
decoding, the model is employed to softly con-
strain the translation orders of the source language 
collocations. Since we only model the reordering 
of collocated words, our methods can partially al-
leviate the data sparseness encountered by other 
methods directly modeling the reordering based on 
source phrases or target phrases. In addition, this 
kind of reordering information can be integrated 
into any SMT systems without resorting to any 
additional resources. 
The experimental results show that the pro-
posed method significantly improves the transla-
tion quality of a phrase based SMT system, 
achieving an absolute improvement of 1.1~1.4 
BLEU score over the baseline methods. 
References 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In 
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL, pp. 529-536. 
Adam L. Berger, Peter F. Brown, Stephen A. Della Pie-
tra, Vincent J. Della Pietra, Andrew S. Kehler, and 
Robert L. Mercer. 1996. Language Translation Appa-
ratus and Method of Using Context-Based Transla-
tion Models. United States Patent, Patent Number 
5510981, April.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Pietra, and Robert. L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parameter 
estimation. Computational Linguistics, 19(2): 263-
311. 
David Chiang. 2005. A Hierarchical Phrase-based Mod-
el for Statistical Machine Translation. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 263-270. 
Niyu Ge. 2010. A Direct Syntax-Driven Reordering 
Model for Phrase-Based Machine Translation. In 
Proceedings of Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the ACL, pp. 849-857. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 388-395. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. In 
Proceedings of the 45th Annual Meeting of the ACL, 
Poster and Demonstration Sessions, pp. 177-180. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of International Workshop on Spoken 
Language Translation. 
1043
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 2009 
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 487-495. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language 
Processing, Cambridge, MA; London, U.K.: Brad-
ford Book & MIT Press. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrased-based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics: Hu-
man Language Technologies, pp. 1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1) : 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Weij-
ing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Christoph Tillmann. 2004. A Unigram Orientation 
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 101-104. 
Christoph Tillmann and Tong Zhang. 2005. A Localized 
Prediction Model for Statistical Machine Translation. 
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pp. 557-564. 
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, 
Vijil Chenthamarakshan, and Nanda Kambhatla. 
2010. Syntax Based Reordering with Automatically 
Derived Rules for Improved Statistical Machine 
Translation. In Proceedings of the 23rd International 
Conference on Computational Linguistics, pp. 1119-
1127. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
the 21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 521-528. 
Richard Zens and Herman Ney. 2003. A Comparative 
Study on Reordering Constraints in Statistical Ma-
chine Translation. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, pp. 192-202. 
Richard Zens and Herman Ney. 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In Proceedings of the Workshop on Statistical 
Machine Translation, pp. 55-63. 
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 
2007. Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT. In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning, pp. 533-540. 
 
 
 
1044
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675?684,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Multiple Treebanks for Parsing with Quasi-synchronous
Grammars
Zhenghua Li, Ting Liu?, Wanxiang Che
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{lzh,tliu,car}@ir.hit.edu.cn
Abstract
We present a simple and effective framework
for exploiting multiple monolingual treebanks
with different annotation guidelines for pars-
ing. Several types of transformation patterns
(TP) are designed to capture the systematic an-
notation inconsistencies among different tree-
banks. Based on such TPs, we design quasi-
synchronous grammar features to augment the
baseline parsing models. Our approach can
significantly advance the state-of-the-art pars-
ing accuracy on two widely used target tree-
banks (Penn Chinese Treebank 5.1 and 6.0)
using the Chinese Dependency Treebank as
the source treebank. The improvements are
respectively 1.37% and 1.10% with automatic
part-of-speech tags. Moreover, an indirect
comparison indicates that our approach also
outperforms previous work based on treebank
conversion.
1 Introduction
The scale of available labeled data significantly af-
fects the performance of statistical data-driven mod-
els. As a structural classification problem that is
more challenging than binary classification and se-
quence labeling problems, syntactic parsing is more
prone to suffer from the data sparseness problem.
However, the heavy cost of treebanking typically
limits one single treebank in both scale and genre.
At present, learning from one single treebank seems
inadequate for further boosting parsing accuracy.1
?Correspondence author: tliu@ir.hit.edu.cn
1Incorporating an increased number of global features, such
as third-order features in graph-based parsers, slightly affects
parsing accuracy (Koo and Collins, 2010; Li et al, 2011).
Treebanks # of Words Grammar
CTB5 0.51 million Phrase structure
CTB6 0.78 million Phrase structure
CDT 1.11 million Dependency structure
Sinica 0.36 million Phrase structure
TCT about 1 million Phrase structure
Table 1: Several publicly available Chinese treebanks.
Therefore, studies have recently resorted to other re-
sources for the enhancement of parsing models, such
as large-scale unlabeled data (Koo et al, 2008; Chen
et al, 2009; Bansal and Klein, 2011; Zhou et al,
2011), and bilingual texts or cross-lingual treebanks
(Burkett and Klein, 2008; Huang et al, 2009; Bur-
kett et al, 2010; Chen et al, 2010).
The existence of multiple monolingual treebanks
opens another door for this issue. For example, ta-
ble 1 lists a few publicly available Chinese treebanks
that are motivated by different linguistic theories or
applications. In the current paper, we utilize the
first three treebanks, i.e., the Chinese Penn Tree-
bank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al, 2005),
and the Chinese Dependency Treebank (CDT) (Liu
et al, 2006). The Sinica treebank (Chen et al, 2003)
and the Tsinghua Chinese Treebank (TCT) (Qiang,
2004) can be similarly exploited with our proposed
approach, which we leave as future work.
Despite the divergence of annotation philosophy,
these treebanks contain rich human knowledge on
the Chinese syntax, thereby having a great deal of
common ground. Therefore, exploiting multiple
treebanks is very attractive for boosting parsing ac-
curacy. Figure 1 gives an example with different an-
675
??1 ??2 ?3 ??4
VV NN CC NN
promote trade and industry
v n c n
OBJ
NMOD
NMOD
VOB COO
LAD
w0
ROOT
ROOT
Figure 1: Example with annotations from CTB5 (upper)
and CDT (under).
notations from CTB5 and CDT.2 This example illus-
trates that the two treebanks annotate coordination
constructions differently. In CTB5, the last noun is
the head, whereas the first noun is the head in CDT.
One natural idea for multiple treebank exploita-
tion is treebank conversion. First, the annotations
in the source treebank are converted into the style
of the target treebank. Then, both the converted
treebank and the target treebank are combined. Fi-
nally, the combined treebank are used to train a
better parser. However, the inconsistencies among
different treebanks are normally nontrivial, which
makes rule-based conversion infeasible. For exam-
ple, a number of inconsistencies between CTB5 and
CDT are lexicon-sensitive, that is, they adopt dif-
ferent annotations for some particular lexicons (or
word senses). Niu et al (2009) use sophisticated
strategies to reduce the noises of the converted tree-
bank after automatic treebank conversion.
The present paper proposes a simple and effective
framework for this problem. The proposed frame-
work avoids directly addressing the difficult anno-
tation transformation problem, but focuses on mod-
eling the annotation inconsistencies using transfor-
mation patterns (TP). The TPs are used to compose
quasi-synchronous grammar (QG) features, such
that the knowledge of the source treebank can in-
spire the target parser to build better trees. We con-
duct extensive experiments using CDT as the source
treebank to enhance two target treebanks (CTB5 and
CTB6). Results show that our approach can signifi-
cantly boost state-of-the-art parsing accuracy. More-
over, an indirect comparison indicates that our ap-
2CTB5 is converted to dependency structures following the
standard practice of dependency parsing (Zhang and Clark,
2008b). Notably, converting a phrase-structure tree into its
dependency-structure counterpart is straightforward and can be
performed by applying heuristic head-finding rules.
proach also outperforms the treebank conversion ap-
proach of Niu et al (2009).
2 Related Work
The present work is primarily inspired by Jiang et
al. (2009) and Smith and Eisner (2009). Jiang et al
(2009) improve the performance of word segmen-
tation and part-of-speech (POS) tagging on CTB5
using another large-scale corpus of different annota-
tion standards (People?s Daily). Their framework is
similar to ours. However, handling syntactic anno-
tation inconsistencies is significantly more challeng-
ing in our case of parsing. Smith and Eisner (2009)
propose effective QG features for parser adaptation
and projection. The first part of their work is closely
connected with our work, but with a few impor-
tant differences. First, they conduct simulated ex-
periments on one treebank by manually creating a
few trivial annotation inconsistencies based on two
heuristic rules. They then focus on better adapting a
parser to a new annotation style with few sentences
of the target style. In contrast, we experiment with
two real large-scale treebanks, and boost the state-
of-the-art parsing accuracy using QG features. Sec-
ond, we explore much richer QG features to fully
exploit the knowledge of the source treebank. These
features are tailored to the dependency parsing prob-
lem. In summary, the present work makes substan-
tial progress in modeling structural annotation in-
consistencies with QG features for parsing.
Previous work on treebank conversion primar-
ily focuses on converting one grammar formalism
of a treebank into another and then conducting a
study on the converted treebank (Collins et al, 1999;
Xia et al, 2008). The work by Niu et al (2009)
is, to our knowledge, the only study to date that
combines the converted treebank with the existing
target treebank. They automatically convert the
dependency-structure CDT into the phrase-structure
style of CTB5 using a statistical constituency parser
trained on CTB5. Their experiments show that
the combined treebank can significantly improve
the performance of constituency parsers. However,
their method requires several sophisticated strate-
gies, such as corpus weighting and score interpo-
lation, to reduce the influence of conversion errors.
Instead of using the noisy converted treebank as ad-
ditional training data, our approach allows the QG-
676
enhanced parsing models to softly learn the system-
atic inconsistencies based on QG features, making
our approach simpler and more robust.
Our approach is also intuitively related to stacked
learning (SL), a machine learning framework that
has recently been applied to dependency parsing
to integrate two main-stream parsing models, i.e.,
graph-based and transition-based models (Nivre and
McDonald, 2008; Martins et al, 2008). However,
the SL framework trains two parsers on the same
treebank and therefore does not need to consider the
problem of annotation inconsistencies.
3 Dependency Parsing
Given an input sentence x = w0w1...wn and its POS
tag sequence t = t0t1...tn, the goal of dependency
parsing is to build a dependency tree as depicted in
Figure 1, denoted by d = {(h,m, l) : 0 ? h ?
n, 0 < m ? n, l ? L}, where (h,m, l) indicates an
directed arc from the head word (also called father)
wh to the modifier (also called child or dependent)
wm with a dependency label l, and L is the label set.
We omit the label l because we focus on unlabeled
dependency parsing in the present paper. The artifi-
cial node w0, which always points to the root of the
sentence, is used to simplify the formalizations.
In the current research, we adopt the graph-based
parsing models for their state-of-the-art performance
in a variety of languages.3 Graph-based models
view the problem as finding the highest scoring tree
from a directed graph. To guarantee the efficiency of
the decoding algorithms, the score of a dependency
tree is factored into the scores of some small parts
(subtrees).
Scorebs(x, t,d) = wbs ? fbs(x, t,d)
=
?
p?d
wpart ? fpart(x, t, p)
where p is a scoring part which contains one or more
dependencies of d, and fbs(.) denotes the basic pars-
ing features, as opposed to the QG features. Figure
2 lists the scoring parts used in our work, where g,
h, m, and s, are word indices.
We implement three parsing models of varying
strengths in capturing features to better understand
the effect of the proposed QG features.
3Our approach can equally be applied to transition-based
parsing models (Yamada and Matsumoto, 2003; Nivre, 2003)
with minor modifications.
dependency sibling grandparent
h
m
h
ms
h
m
g
Figure 2: Scoring parts used in our graph-based parsing
models.
? The first-order model (O1) only incorporates
dependency parts (McDonald et al, 2005), and
requires O(n3) parsing time.
? The second-order model using only sibling
parts (O2sib) includes both dependency and
sibling parts (McDonald and Pereira, 2006),
and needs O(n3) parsing time.
? The second-order model (O2) uses all the
scoring parts in Figure 2 (Koo and Collins,
2010). The time complexity of the decoding
algorithm is O(n4).4
For the O2 model, the score function is rewritten as:
Scorebs(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s),(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
where fdep(.), fsib(.) and fgrd(.) correspond to the
features for the three kinds of scoring parts. We
adopt the standard features following Li et al
(2011). For the O1 and O2sib models, the above
formula is modified by deactivating the extra parts.
4 Dependency Parsing with QG Features
Smith and Eisner (2006) propose the QG for ma-
chine translation (MT) problems, allowing greater
syntactic divergences between the two languages.
Given a source sentence x? and its syntactic tree
d?, a QG defines a monolingual grammar that gen-
erates translations of x?, which can be denoted by
p(x,d,a|x?,d?), where x and d refer to a translation
and its parse, and a is a cross-language alignment.
Under a QG, any portion of d can be aligned to any
4We use the coarse-to-fine strategy to prune the search
space, which largely accelerates the decoding procedure (Koo
and Collins, 2010).
677
hm
h
m
m
h
Consistent: 55.4% Reverse: 8.6%Sibling: 10.0%Grand: 11.7% Reverse-grand: 1.4%( ', , )dep d h m? ?
( ', , , )grd d g h m? ?
( ', , , )sib d h s m? ?
i
m
h
i
h
m
28.2%
i
mh
h
ms
h
ms
6.7%
i
m
h
s
h
s
i
m
6.4%
i
msh
4.9%
s
m
h
4.4%
m
s
h
4.2%
h
m
g
h
m
g
30.1% 6.5%
h
mg
6.2%
h
m
i
g
6.1%
i
m
h
g
m
h
g
5.4% 5.3%
i
h
g
m
Syntactic Structures of the Corresponding Source SideTarget Side
Figure 4: Most frequent transformation patterns (TPs) when using CDT as the source treebank and CTB5 as the
target. A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes
the process by which the left-side subtree is transformed into the right-side structure. Functions ?dep(.), ?sib(.), and
?grd(.) return the specific TP type for a candidate scoring part according to the source tree d?.
Source Parser
ParserS
Target Parser
ParserT
Train
Train
Parse
Target 
Treebank
T={(xj, dj)}j
Source Treebank
S={(xi, di)}i
Parsed 
Treebank
TS={(xj, djS)}j
Target Treebank with 
Source Annotations
T+S={(xj, djS, dj)}j
Out
Figure 3: Framework of our approach.
portion of d?, and the construction of d can be in-
spired by arbitrary substructures of d?. To date, QGs
have been successfully applied to various tasks, such
as word alignment (Smith and Eisner, 2006), ma-
chine translation (Gimpel and Smith, 2011), ques-
tion answering (Wang et al, 2007), and sentence
simplification (Woodsend and Lapata, 2011).
In the present work, we utilize the idea of the QG
for the exploitation of multiple monolingual tree-
banks. The key idea is to let the parse tree of one
style inspire the parsing process of another style.
Different from a MT process, our problem consid-
ers one single sentence (x = x?), and the alignment
a is trivial. Figure 3 shows the framework of our
approach. First, we train a statistical parser on the
source treebank, which is called the source parser.
The source parser is then used to parse the whole tar-
get treebank. At this point, the target treebank con-
tains two sets of annotations, one conforming to the
source style, and the other conforming to the target
style. During both the training and test phases, the
target parser are inspired by the source annotations,
and the score of a target dependency tree becomes
Score(x, t,d?,d) =Scorebs(x, t,d)
+Scoreqg(x, t,d?,d)
The first part corresponds to the baseline model,
whereas the second part is affected by the source tree
d? and can be rewritten as
Scoreqg(x, t,d?,d) = wqg ? fqg(x, t,d?,d)
where fqg(.) denotes the QG features. We expect the
QG features to encourage or penalize certain scor-
ing parts in the target side according to the source
tree d?. Taking Figure 1 as an example, suppose
that the upper structure is the target. The target
parser can raise the score of the candidate depen-
dence ?and? ? ?industry?, because the depen-
678
dency also appears in the source structure, and ev-
idence in the training data shows that both annota-
tion styles handle conjunctions in the same manner.
Similarly, the parser may add weight to ?trade? ?
?industry?, considering that the reverse arc is in
the source structure. Therefore, the QG-enhanced
model must learn the systematic consistencies and
inconsistencies from the training data.
To model such consistency or inconsistency sys-
tematicness, we propose the use of TPs for encoding
the structural correspondence between the source
and target styles. Figure 4 presents the three kinds
of TPs used in our model, which correspond to the
three scoring parts of our parsing models.
Dependency TPs shown in the first row consider
how one dependency in the target side is trans-
formed in the source annotations. We only consider
the five cases shown in the figure. The percentages
in the lower boxes refer to the proportion of the
corresponding pattern, which are counted from the
training data of the target treebank with source anno-
tations T+S . We can see that the noisy source struc-
tures and the gold-standard target structures have
55.4% common dependencies. If the source struc-
ture does not belong to any of the listed five cases,
?dep(d?, h,m) returns ?else? (12.9%). We could
consider more complex structures, such as h being
the grand grand father of m, but statistics show that
more complex transformations become very scarce
in the training data.
For the reason that dependency TPs can only
model how one dependency in the target structure is
transformed, we consider more complex transforma-
tions for the other two kinds of scoring parts of the
target parser, i.e., the sibling and grand TPs shown
in the bottom two rows. We only use high-frequency
TPs of a proportion larger than 1.0%, aggregate oth-
ers as ?else?, which leaves us with 21 sibling TPs
and 22 grand TPs.
Based on these TPs, we propose the QG fea-
tures for enhancing the baseline parsing models,
which are shown in Table 2. The type of the
TP is conjoined with the related words and POS
tags, such that the QG-enhanced parsing models can
make more elaborate decisions based on the context.
Then, the score contributed by the QG features can
be redefined as
Scoreqg(x, t,d?,d) =
?
{(h,m)}?d
wqg-dep ? fqg-dep(x, t,d?, h,m)
+
?
{(h,s),(h,m)}?d
wqg-sib ? fqg-sib(x, t,d?, h, s,m)
+
?
{(g,h),(h,m)}?d
wqg-grd ? fqg-grd(x, t,d?, g, h,m)
which resembles the baseline model and can be nat-
urally handled by the decoding algorithms.
5 Experiments and Analysis
We use the CDT as the source treebank (Liu et
al., 2006). CDT consists of 60,000 sentences from
the People?s Daily in 1990s. For the target tree-
bank, we use two widely used versions of Penn Chi-
nese Treebank, i.e., CTB5 and CTB6, which con-
sist of Xinhua newswire, Hong Kong news and ar-
ticles from Sinarama news magazine (Xue et al,
2005). To facilitate comparison with previous re-
sults, we follow Zhang and Clark (2008b) for data
split and constituency-to-dependency conversion of
CTB5. CTB6 is used as the Chinese data set in the
CoNLL 2009 shared task (Hajic? et al, 2009). There-
fore, we adopt the same setting.
CDT and CTB5/6 adopt different POS tag sets,
and converting from one tag set to another is difficult
(Niu et al, 2009).5 To overcome this problem, we
use the People?s Daily corpus (PD),6 a large-scale
corpus annotated with word segmentation and POS
tags, to train a statistical POS tagger. The tagger
produces a universal layer of POS tags for both the
source and target treebanks. Based on the common
tags, the source parser projects the source annota-
tions into the target treebanks. PD comprises ap-
proximately 300 thousand sentences of with approx-
imately 7 million words from the first half of 1998
of People?s Daily.
Table 3 summarizes the data sets used in the
present work. CTB5X is the same with CTB5 but
follows the data split of Niu et al (2009). We use
CTB5X to compare our approach with their treebank
conversion method (see Table 9).
5The word segmentation standards of the two treebanks also
slightly differs, which are not considered in this work.
6http://icl.pku.edu.cn/icl_groups/
corpustagging.asp
679
fqg-dep(x, t,d?, h,m) fqg-sib(x, t,d?, h, s,m) fqg-grd(x, t,d?, g, h,m)
?dir(h,m) ? dist(h,m) ?dir(h,m) ?dir(h,m) ? dir(g, h)
?dep(d?, h,m) ? th ? tm ?sib(d?, h, s,m) ? th ? ts ? tm ?grd(d?, g, h,m) ? tg ? th ? tm
?dep(d?, h,m) ? wh ? tm ?sib(d?, h, s,m) ? wh ? ts ? tm ?grd(d?, g, h,m) ? wg ? th ? tm
?dep(d?, h,m) ? th ? wm ?sib(d?, h, s,m) ? th ? ws ? tm ?grd(d?, g, h,m) ? tg ? wh ? tm
?dep(d?, h,m) ? wh ? wm ?sib(d?, h, s,m) ? th ? ts ? wm ?grd(d?, g, h,m) ? tg ? th ? wm
?sib(d?, h, s,m) ? ts ? tm ?grd(d?, g, h,m) ? tg ? tm
Table 2: QG features used to enhance the baseline parsing models. dir(h,m) denotes the direction of the dependency
(h,m), whereas dist(h,m) is the distance |h ?m|. ?dir(h,m) ? dist(h,m) indicates that the features listed in the
corresponding column are also conjoined with dir(h,m) ? dist(h,m) to form new features.
Corpus Train Dev Test
PD 281,311 5,000 10,000
CDT 55,500 1,500 3,000
CTB5 16,091 803 1,910
CTB5X 18,104 352 348
CTB6 22,277 1,762 2,556
Table 3: Data used in this work (in sentence number).
We adopt unlabeled attachment score (UAS) as
the primary evaluation metric. We also use Root ac-
curacy (RA) and complete match rate (CM) to give
more insights. All metrics exclude punctuation. We
adopt Dan Bikel?s randomized parsing evaluation
comparator for significance test (Noreen, 1989).7
For all models used in current work (POS tagging
and parsing), we adopt averaged perceptron to train
the feature weights (Collins, 2002). We train each
model for 10 iterations and select the parameters that
perform best on the development set.
5.1 Preliminaries
This subsection describes how we project the source
annotations into the target treebanks. First, we train
a statistical POS tagger on the training set of PD,
which we name TaggerPD .8 The tagging accuracy
on the test set of PD is 98.30%.
We then use TaggerPD to produce POS tags for
all the treebanks (CDT, CTB5, and CTB6).
Based on the common POS tags, we train a
second-order source parser (O2) on CDT, denoted
by ParserCDT . The UAS on CDT-test is 84.45%.
We then use ParserCDT to parse CTB5 and CTB6.
7http://www.cis.upenn.edu/[normal-wave
?
]
dbikel/software.html
8We adopt the Chinese-oriented POS tagging features pro-
posed in Zhang and Clark (2008a).
Models without QG with QG
O2 86.13 86.44 (+0.31, p = 0.06)
O2sib 85.63 86.17 (+0.54, p = 0.003)
O1 83.16 84.40 (+1.24, p < 10?5)
Li11 86.18 ?
Z&N11 86.00 ?
Table 4: Parsing accuracy (UAS) comparison on CTB5-
test with gold-standard POS tags. Li11 refers to the
second-order graph-based model of Li et al (2011),
whereas Z&N11 is the feature-rich transition-based
model of Zhang and Nivre (2011).
At this point, both CTB5 and CTB6 contain depen-
dency structures conforming to the style of CDT.
5.2 CTB5 as the Target Treebank
Table 4 shows the results when the gold-standard
POS tags of CTB5 are adopted by the parsing mod-
els. We aim to analyze the efficacy of QG features
under the ideal scenario wherein the parsing mod-
els suffer from no error propagation of POS tag-
ging. We determine that our baseline O2 model
achieves comparable accuracy with the state-of-the-
art parsers. We also find that QG features can
boost the parsing accuracy by a large margin when
the baseline parser is weak (O1). The improve-
ment shrinks for stronger baselines (O2sib and O2).
This phenomenon is understandable. When gold-
standard POS tags are available, the baseline fea-
tures are very reliable and the QG features becomes
less helpful for more complex models. The p-values
in parentheses present the statistical significance of
the improvements.
We then turn to the more realistic scenario
wherein the gold-standard POS tags of the target
treebank are unavailable. We train a POS tagger on
the training set of CTB5 to produce the automatic
680
Models without QG with QG
O2 79.67 81.04 (+1.37)
O2sib 79.25 80.45 (+1.20)
O1 76.73 79.04 (+2.31)
Li11 joint 80.79 ?
Li11 pipeline 79.29 ?
Table 5: Parsing accuracy (UAS) comparison on CTB5-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p < 10?5).
Setting UAS CM RA
fbs(.) 79.67 26.81 73.82
fqg(.) 79.15 26.34 74.71
fbs(.) + fqg(.) 81.04 29.63 77.17
fbs(.) + fqg-dep(.) 80.82 28.80 76.28
fbs(.) + fqg-sib(.) 80.86 28.48 76.18
fbs(.) + fqg-grd(.) 80.88 28.90 76.34
Table 6: Feature ablation for Parser-O2 on CTB5-test
with automatic POS tags.
POS tags for the development and test sets of CTB5.
The tagging accuracy is 93.88% on the test set. The
automatic POS tags of the training set are produced
using 10-fold cross-validation.9
Table 5 shows the results. We find that QG fea-
tures result in a surprisingly large improvement over
the O1 baseline and can also boost the state-of-
the-art parsing accuracy by a large margin. Li et
al. (2011) show that a joint POS tagging and de-
pendency parsing model can significantly improve
parsing accuracy over a pipeline model. Our QG-
enhanced parser outperforms their best joint model
by 0.25%. Moreover, the QG features can be used to
enhance a joint model and achieve higher accuracy,
which we leave as future work.
5.3 Analysis Using Parser-O2 with AUTO-POS
We then try to gain more insights into the effect of
the QG features through detailed analysis. We se-
lect the state-of-the-art O2 parser and focus on the
realistic scenario with automatic POS tags.
Table 6 compares the efficacy of different feature
sets. The first major row analyzes the efficacy of
9We could use the POS tags produced by TaggerPD in Sec-
tion 5.1, which however would make it difficult to compare our
results with previous ones. Moreover, inferior results may be
gained due to the differences between CTB5 and PD in word
segmentation standards and text sources.
the basic features fbs(.) and the QG features fqg(.).
When using the few QG features in Table 2, the ac-
curacy is very close to that when using the basic
features. Moreover, using both features generates
a large improvement. The second major row com-
pares the efficacy of the three kinds of QG features
corresponding to the three types of scoring parts. We
can see that the three feature sets are similarly effec-
tive and yield comparable accuracies. Combining
these features generate an additional improvement
of approximately 0.2%. These results again demon-
strate that all the proposed QG features are effective.
Figure 5 describes how the performance varies
when the scale of CTB5 and CDT changes. In
the left subfigure, the parsers are trained on part
of the CTB5-train, and ?16? indicates the use of
all the training instances. Meanwhile, the source
parser ParserCDT is trained on the whole CDT-
train. We can see that QG features render larger
improvement when the target treebank is of smaller
scale, which is quite reasonable. More importantly,
the curves indicate that a QG-enhanced parser
trained on a target treebank of 16,000 sentences
may achieve comparable accuracy with a base-
line parser trained on a treebank that is double
the size (32,000), which is very encouraging.
In the right subfigure, the target treebank is
trained on the whole CTB5-train, whereas the source
parser is trained on part of the CDT-train, and ?55.5?
indicates the use of all. The curve clearly demon-
strates that the QG features are more helpful when
the source treebank gets larger, which can be ex-
plained as follows. A larger source treebank can
teach a source parser of higher accuracy; then, the
better source parser can parse the target treebank
more reliably; and finally, the target parser can better
learn the annotation divergences based on QG fea-
tures. These results demonstrate the effectiveness
and stability of our approach.
Table 7 presents the detailed effect of the QG fea-
tures on different dependency patterns. A pattern
?VV ? NN? refers to a right-directed dependency
with the head tagged as ?VV? and the modifier
tagged as ?NN?. whereas ??? means left-directed.
The ?w/o QG? column shows the number of the cor-
responding dependency pattern that appears in the
gold-standard trees but misses in the results of the
baseline parser, whereas the signed figures in the
?+QG? column are the changes made by the QG-
681
71
72
73
74
75
76
77
78
79
80
81
82
1 2 4 8 16
Training Set Size of CTB5
w/o QG
with QG
79.4
79.6
79.8
80
80.2
80.4
80.6
80.8
81
81.2
0 3 6 12 24 55.5
Training Set Size of CDT
with QG
Figure 5: Parsing accuracy (UAS) comparison on CTB5-
test when the scale of CDT and CTB5 varies (thousands
in sentence number).
Dependency w/o QG +QG Descriptions
NN? NN 858 -78 noun modifier or coordinating nouns
VV? VV 777 -41 object clause or coordinating verbs
VV? VV 570 -38 subject clause
VV? NN 509 -79 verb and its object
w0 ? VV 357 -57 verb as sentence root
VV? NN 328 -32 attributive clause
P? VV 278 -37 preposition phrase attachment
VV? DEC 233 -33 attributive clause and auxiliary DE
P? NN 175 -35 preposition and its object
Table 7: Detailed effect of QG features on different de-
pendency patterns.
enhanced parser. We only list the patterns with an
absolute change larger than 30. We find that the QG
features can significantly help a variety of depen-
dency patterns (i.e., reducing the missing number).
5.4 CTB6 as the Target Treebank
We use CTB6 as the target treebank to further verify
the efficacy of our approach. Compared with CTB5,
CTB6 is of larger scale and is converted into de-
pendency structures according to finer-grained head-
finding rules (Hajic? et al, 2009). We directly adopt
the same transformation patterns and features tuned
on CTB5. Table 8 shows results. The improvements
are similar to those on CTB5, demonstrating that our
approach is effective and robust. We list the top three
systems of the CoNLL 2009 shared task in Table 8,
showing that our approach also advances the state-
of-the-art parsing accuracy on this data set.10
10We reproduce their UASs using the data released
by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/
results.php. The parsing accuracies of the top systems may be
underestimated since the accuracy of the provided POS tags in
CoNLL 2009 is only 92.38% on the test set, while the POS tag-
ger used in our experiments reaches 94.08%.
Models without QG with QG
O2 83.23 84.33 (+1.10)
O2sib 82.87 84.11 (+1.37)
O1 80.29 82.76 (+2.47)
Bohnet (2009) 82.68 ?
Che et al (2009) 82.11 ?
Gesmundo et al (2009) 81.70 ?
Table 8: Parsing accuracy (UAS) comparison on CTB6-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p < 10?5).
Models baseline with another treebank
Ours 84.16 86.67 (+2.51)
GP (Niu et al, 2009) 82.42 84.06 (+1.64)
Table 9: Parsing accuracy (UAS) comparison on the test
set of CTB5X. Niu et al (2009) use the maximum en-
tropy inspired generative parser (GP) of Charniak (2000)
as their constituent parser.
5.5 Comparison with Treebank Conversion
As discussed in Section 2, Niu et al (2009) automat-
ically convert the dependency-structure CDT to the
phrase-structure annotation style of CTB5X and use
the converted treebank as additional labeled data.
We convert their phrase-structure results on CTB5X-
test into dependency structures using the same head-
finding rules. To compare with their results, we
run our baseline and QG-enhanced O2 parsers on
CTB5X. Table 9 presents the results.11 The indirect
comparison indicates that our approach can achieve
larger improvement than their treebank conversion
based method.
6 Conclusions
The current paper proposes a simple and effective
framework for exploiting multiple large-scale tree-
banks of different annotation styles. We design
rich TPs to model the annotation inconsistencies and
consequently propose QG features based on these
TPs. Extensive experiments show that our approach
can effectively utilize the syntactic knowledge from
another treebank and significantly improve the state-
of-the-art parsing accuracy.
11We thank the authors for sharing their results. Niu et al
(2009) also use the reranker (RP) of Charniak and Johnson
(2005) as a stronger baseline, but the results are missing. They
find a less improvement on F score with RP than with GP (0.9%
vs. 1.1%). We refer to their Table 5 and 6 for details.
682
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Major Projects via
grant 2011AA01A207, and the National ?863?
Leading Technology Research Project via grant
2012AA011102.
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 693?702, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Bernd Bohnet. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceedings
of the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2009): Shared Task,
pages 67?72, Boulder, Colorado, June. Association for
Computational Linguistics.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 877?886, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 46?54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ANLP?00, pages 132?139.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL 2009: Shared Task, pages 49?
54.
Keh-Jiann Chen, Chi-Ching Luo, Ming-Chung Chang,
Feng-Yi Chen, Chao-Jan Chen, Chu-Ren Huang, and
Zhao-Ming Gao, 2003. Sinica treebank: Design crite-
ria,representational issues and implementation, chap-
ter 13, pages 231?248. Kluwer Academic Publishers.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
570?579, Singapore, August. Association for Compu-
tational Linguistics.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21?29, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micheal Collins, Lance Ramshaw, Jan Hajic, and
Christoph Tillmann. 1999. A statistical parser for
czech. In ACL 1999, pages 505?512.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of CoNLL 2009: Shared Task,
pages 37?42.
Kevin Gimpel and Noah A. Smith. 2011. Quasi-
synchronous phrase dependency grammars for ma-
chine translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 474?485, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222?1231, Singapore, August. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522?530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
683
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models
for chinese pos tagging and dependency parsing. In
EMNLP 2011, pages 1180?1191.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building
a dependency treebank for improving Chinese parser.
In Journal of Chinese Language and Computing, vol-
ume 16, pages 207?224.
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
EMNLP?08, pages 157?166.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46?54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL 2008, pages 950?958.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: An introduction. John Wiley &
Sons, Inc., New York. Book (ISBN 0471611360 ).
Zhou Qiang. 2004. Annotation scheme for chinese tree-
bank. Journal of Chinese Information Processing,
18(4):1?8.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntac-
tic dependencies. In Proceedings on the Workshop
on Statistical Machine Translation, pages 23?30, New
York City, June. Association for Computational Lin-
guistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 822?831, Singapore, August. Association for
Computational Linguistics.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 409?420, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer,
and Dipti Misra. Sharma. 2008. Towards a multi-
representational treebank. In In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1556?1565, Portland, Oregon, USA,
June. Association for Computational Linguistics.
684
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 979?987,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improve SMT Quality with Automatically Extracted Paraphrase Rules 
 
 
Wei He1, Hua Wu2, Haifeng Wang2, Ting Liu1* 
1Research Center for Social Computing and Information 
Retrieval, Harbin Institute of Technology 
{whe,tliu}@ir.hit.edu.cn 
2Baidu 
{wu_hua,wanghaifeng}@baidu.com 
 
 
 
Abstract1 
We propose a novel approach to improve 
SMT via paraphrase rules which are 
automatically extracted from the bilingual 
training data. Without using extra 
paraphrase resources, we acquire the rules 
by comparing the source side of the parallel 
corpus with the target-to-source 
translations of the target side. Besides the 
word and phrase paraphrases, the acquired 
paraphrase rules mainly cover the 
structured paraphrases on the sentence 
level. These rules are employed to enrich 
the SMT inputs for translation quality 
improvement. The experimental results 
show that our proposed approach achieves 
significant improvements of 1.6~3.6 points 
of BLEU in the oral domain and 0.5~1 
points in the news domain. 
1 Introduction 
The translation quality of the SMT system is 
highly related to the coverage of translation models. 
However, no matter how much data is used for 
training, it is still impossible to completely cover 
the unlimited input sentences. This problem is 
more serious for online SMT systems in real-world 
applications. Naturally, a solution to the coverage 
problem is to bridge the gaps between the input 
sentences and the translation models, either from 
the input side, which targets on rewriting the input 
sentences to the MT-favored expressions, or from 
                                                          
This work was done when the first author was visiting Baidu. 
*Correspondence author: tliu@ir.hit.edu.cn 
the side of translation models, which tries to enrich 
the translation models to cover more expressions.  
In recent years, paraphrasing has been proven 
useful for improving SMT quality. The proposed 
methods can be classified into two categories 
according to the paraphrase targets: (1) enrich 
translation models to cover more bilingual 
expressions; (2) paraphrase the input sentences to 
reduce OOVs or generate multiple inputs. In the 
first category, He et al (2011), Bond et al (2008) 
and Nakov (2008) enriched the SMT models via 
paraphrasing the training corpora. Kuhn et al 
(2010) and Max (2010) used paraphrases to 
smooth translation models. For the second 
category, previous studies mainly focus on finding 
translations for unknown terms using phrasal 
paraphrases. Callison-Burch et al (2006) and 
Marton et al (2009) paraphrase unknown terms in 
the input sentences using phrasal paraphrases 
extracted from bilingual and monolingual corpora. 
Mirkin et al (2009) rewrite OOVs with 
entailments and paraphrases acquired from 
WordNet. Onishi et al (2010) and Du et al (2010) 
use phrasal paraphrases to build a word lattice to 
get multiple input candidates. In the above 
methods, only word or phrasal paraphrases are 
used for input sentence rewriting. No structured 
paraphrases on the sentence level have been 
investigated. However, the information in the 
sentence level is very important for disambiguation.  
For example, we can only substitute play with 
drama in a context related to stage or theatre. 
Phrasal paraphrase substitutions can hardly solve 
such kind of problems.  
In this paper, we propose a method that rewrites 
979
the input sentences of the SMT system using 
automatically extracted paraphrase rules which can 
capture structures on sentence level in addition to 
paraphrases on the word or phrase level. Without 
extra paraphrase resources, a novel approach is 
proposed to acquire paraphrase rules from the 
bilingual training corpus based on the results of 
Forward-Translation and Back-Translation. The 
rules target on rewriting the input sentences to an 
MT-favored expression to ensure a better 
translation. The paraphrase rules cover all kinds of 
paraphrases on the word, phrase and sentence 
levels, enabling structure reordering, word or 
phrase insertion, deletion and substitution. The 
experimental results show that our proposed 
approach achieves significant improvements of 
1.6~3.6 points of BLEU in the oral domain and 
0.5~1 points in the news domain. 
The remainder of the paper is organized as 
follows: Section 2 makes a comparison between 
the Forward-Translation and Back-Translation. 
Section 3 introduces our methods that extract 
paraphrase rules from the bilingual corpus of SMT. 
Section 4 describes the strategies for constructing 
word lattice with paraphrase rules. The 
experimental results and some discussions are 
presented in Section 5 and Section 6. Section 7 
compares our work to the previous researches. 
Finally, Section 8 concludes the paper and suggests 
directions for future work. 
2 Forward-Translation vs. Back-
Translation 
The Back-Translation method is mainly used for 
automatic MT evaluation (Rapp 2009). This 
approach is very helpful when no target language 
reference is available. The only requirement is that 
the MT system needs to be bidirectional. The 
procedure includes translating a text into certain 
foreign language with the MT system (Forward-
Translation), and translating it back into the 
original language with the same system (Back-
Translation). Finally the translation quality of 
Back-Translation is evaluated by using the original 
source texts as references. 
Sun et al (2010) reported an interesting 
phenomenon: given a bilingual text, the Back-
Translation results of the target sentences is better 
than the Forward-Translation results of the source 
sentences. Clearly, let (S0, T0) be the initial pair of 
bilingual text. A source-to-target translation system 
SYS_ST and a target-to-source translation system 
SYS_TS are trained using the bilingual corpus. 
?????  is a Forward-Translation function, and 
????? is a function of Back-Translation which can 
be deduced with two rounds of translations: 
????? ? ???_??????_??????. In the first round 
of translation, S0 and T0 are fed into SYS_ST and 
SYS_TS, and we get T1 and S1 as translation results. 
In the second round, we translate S1 back into the 
target side with SYS_ST, and get the translation T2. 
The procedure is illustrated in Figure 1, which can 
also formally be described as: 
1. T1 = FT(S0) = SYS_ST(S0). 
2. T2 = BT(T0), which can be decomposed into 
two steps: S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Using T0 as reference, an interesting result is 
reported in Sun et al (2010) that T2 achieves a 
higher score than T1 in automatic MT evaluation. 
This outcome is important because T2 is translated 
Figure 1: Procedure of Forward-Translation and Back-Translation. 
S0 T0 
S1 T1 
T2 
Source Language Target Language 
Initial Parallel Text 
1st Round Translation 
2nd Round Translation 
Forward- 
Translation
Back- 
Translation 
980
from a machine-generated text S1, but T1 is 
translated from a human-write text S0. Why the 
machine-generated text results in a better 
translation than the human-write text? Two 
possible reasons may explain this phenomenon: (1) 
in the first round of translation T0 ? S1, some 
target word orders are reserved due to the 
reordering failure, and these reserved orders lead to 
a better result in the second round of translation; (2) 
the text generated by an MT system is more likely 
to be matched by the reversed but homologous MT 
system.  
Note that all the texts of S0, S1, S2, T0 and T1 are 
sentence aligned because the initial parallel corpus 
(S0, T0) is aligned in the sentence level. The aligned 
sentence pairs in (S0, S1) can be considered as 
paraphrases. Since S1 has some MT-favored 
structures which may result in a better translation, 
an intuitive idea is whether we can learn these 
structures by comparing S1 with S0. This is the 
main assumption of this paper. Taking (S0, S1) as 
paraphrase resource, we propose a method that 
automatically extracts paraphrase rules to capture 
the MT-favored structures. 
3 Extraction of Paraphrase Rules 
3.1 Definition of Paraphrase Rules 
We define a paraphrase rule as follows: 
1. A paraphrase rule consists of two parts, left-
hand-side (LHS) and right-hand-side (RHS). 
Both of LHS and RHS consist of non-
terminals (slot) and terminals (words). 
2. LHS must start/end with a terminal. 
3. There must be at least one terminal between 
two non-terminals in LHS. 
A paraphrase rule in the format of:  
LHS ? RHS 
which means the words matched by LHS can be 
paraphrased to RHS. Taking Chinese as a case 
study, some examples of paraphrase rules are 
shown in Table 1. 
3.2  Selecting Paraphrase Sentence Pairs 
Following the methods in Section 2, the initial 
bilingual corpus is (S0, T0). We train a source-to-
target PBMT system (SYS_ST) and a target-to-
source PBMT system (SYS_TS) on the parallel 
corpus. Then a Forward-Translation is performed 
on S0 using SYS_ST, and a Back-Translation is 
performed on T0 using SYS_TS and SYS_ST. As 
mentioned above, the detailed procedure is: T1 = 
SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Finally we compute BLEU (Papineni et al 2002) 
score for every sentence in T2 and T1, using the 
corresponding sentence in T0 as reference. If the 
sentence in T2 has a higher BLEU score than the 
aligned sentence in T1, the corresponding sentences 
in S0 and S1 are selected as candidate paraphrase 
sentence pairs, which are used in the following 
steps of paraphrase extractions. 
3.3 Word Alignments Filtering 
We can construct word alignment between S0 and 
S1 through T0. On the initial corpus of (S0, T0), we 
conduct word alignment with Giza++ (Och and 
Ney, 2000) in both directions and then apply the 
grow-diag-final heuristic (Koehn et al, 2005) for 
symmetrization. Because S1 is generated by 
feeding T0 into the PBMT system SYS_TS, the 
word alignment between T0 and S1 can be acquired 
from the verbose information of the decoder. 
The word alignments of S0 and S1 contain noises 
which are produced by either wrong alignment of 
GIZA++ or translation errors of SYS_TS. To ensure 
the alignment quality, we use some heuristics to 
filter the alignment between S0 and S1: 
1. If two identical words are aligned in S0 and 
S1, then remove all the other links to the two 
words. 
No. LHS RHS 
1 ??/ride   X1   ????/bus ??/ride    X1   ??/bus 
2    ?/at   X1  ?/location   ???/turn left  ???/turn left   ?/at   X1  ?/location 
3 ?/NULL   X1    ?/give    ?/me ?/give    ?/me    X1 
4 ?/from  X1  ?/to  X2  ?/need ??/how long??/time 
?/need   ?/spend  ??/how long  ??/time 
?/from X1?/to X2 
Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word 
981
2. Stop words (including some function words 
and punctuations) can only be aligned to 
either stop words or null. 
Figure 2 illustrates an example of using the 
heuristics to filter alignment. 
3.4 Extracting Paraphrase Rules 
From the word-aligned sentence pairs, we then 
extract a set of rules that are consistent with the 
word alignments. We use the rule extracting 
methods of Chiang (2005). Take the sentence pair 
in Figure 2 as an example, two initial phrase pairs 
PP1 = ?? ? ?? ??? ||| ? ? ?? ????  
and  PP2 = ?? ? ? ?? ??? ? ?? ||| ? 
? ?? ? ? ?? ???? are identified, and 
PP1 is contained by PP2, then we could form the 
rule: 
? X1 ? ?? ? ? ? ?? X1
to  have interest  very feel interest  
4 Paraphrasing the Input Sentences 
The extracted paraphrase rules aim to rewrite the 
input sentences to an MT-favored form which may 
lead to a better translation. However, it is risky to 
directly replace the input sentence with a 
paraphrased sentence, since the errors in automatic 
paraphrase substitution may jeopardize the 
translation result seriously. To avoid such damage, 
for a given input sentence, we first transform all 
paraphrase rules that match the input sentences to 
phrasal paraphrases, and then build a word lattice 
for SMT decoder using the phrasal paraphrases. In 
this case, the decoder can search for the best result 
among all the possible paths. 
The input sentences are first segmented into sub-
sentences by punctuations. Then for each sub-
sentence, the matched paraphrase rules are ranked 
according to: (1) the number of matched words; (2) 
the frequency of the paraphrase rule in the training 
data. Actually, the ranking strategy tends to select 
paraphrase rules that have more matched words 
(therefore less ambiguity) and higher frequency 
(therefore more reliable). 
4.1 Applying Paraphrase Rules 
Given an input sentence S and a paraphrase rule R 
<RLHS, RRHS>, if S matches RLHS, then the matched 
part can be replaced by RRHS. An example for 
applying the paraphrase rules is illustrated in 
Figure 3.  
From Figure 3, we can see that the words of 
position 1~3 are replaced to ??? 10 ? ???. 
Actually, only the words at position 3 and 4 are 
paraphrased to the word ????, other words are 
left unchanged. Therefore, we can use a triple, 
<MIN_RP_TEXT, COVER_START, COVER_LEN> 
(<?? , 3, 1> in this example) to denote the 
paraphrase rule, which means the minimal text to 
replace is ????, and the paraphrasing starts at 
position 3 and covers 1 words. 
In this manner, all the paraphrase rules matched 
for a certain sentence can be converted to the 
format of <MIN_RP_TEXT, COVER_START, 
COVER_LEN>, which can also be considered as 
phrasal paraphrases. Then the methods of building 
phrasal paraphrases into word lattice for SMT 
inputs can be used in our approaches. 
??    ??     [10?]   ????
??     [10?]      ?? 
Rule 
LHS:??/ride  X1 ????/bus 
RHS:??/ride  X1  ??/bus 
Figure 3: Example for Applying Paraphrase Rules 
0         1            2                3
welcome  ride     No.10         bus
ride       No.10        bus 
I  very feel interest that N/A  blue   handbag  
I     to   that   N/A  blue  handbag have interest    
?   ?   ?    ??   ?    ?  ??   ???     ? 
?   ?    ?     ?    ??   ???  ?  ??     ? 
Figure 2: Example for Word Alignment 
Filtration 
I     to   that   N/A  blue  handbag have interest    
?   ?    ?     ?    ??   ???  ?  ??     ? 
I  very feel interest that N/A  blue   handbag  
?   ?   ?    ??   ?    ?  ??   ???      ? 
982
4.2 Construction of Paraphrase Lattice 
Given an input sentence, all the matched 
paraphrase rules are converted to phrasal 
paraphrases first. Then we build the phrasal 
paraphrases into word lattices using the methods 
proposed by Du et al (2010). The construction 
process takes advantage of the correspondence 
between detected phrasal paraphrases and positions 
of the original words in the input sentence, and 
then creates extra edges in the lattices to allow the 
decoder to consider paths involving the paraphrase 
words. An example is illustrated in Figure 4: given 
a sequence of words {w1,?,wN} as the input, two 
phrases ? ={?1,??p} and ? = {?1,?, ?q} are 
detected as paraphrases for P1 = {wx,?, wy} (1 ? x 
? y ? N) and P2 = {wm,?,wn} (1 ? m ? n ? N) 
respectively. The following steps are taken to 
transform them into word lattices: 
1. Transform the original source sentence into 
word lattice. N + 1 nodes (?k, 0 ? k ? N) are 
created, and N edges labeled with wi (1 ? i ? 
N) are generated to connect them 
sequentially. 
2. Generate extra nodes and edges for each of 
the paraphrases. Taking ? as an example, 
firstly, p ? 1 nodes are created, and then p 
edges labeled with ?j (1 ? j ? p) are 
generated to connect node ?x-1, p-1 nodes 
and ?y-1. 
Via step 2, word lattices are generated by adding 
new nodes and edges coming from paraphrases. 
4.3  Weight Estimation 
The weights of new edges in the lattices are 
estimated by an empirical method base on ranking 
positions. Following Du et al (2010), supposing 
that E = {e1,?,ek} are a set of new edges 
constructed from k paraphrase rules, which are 
sorted in a descending order. Then the weight for 
an edge ei is calculated as: 
??e?? ? 1? ? ? ???1 ? ? ? ?? where k is a predefined tradeoff parameter between 
decoding speed and the number of potential 
paraphrases being considered. 
5  Experiments 
5.1  Experimental Data 
In our experiments, we used Moses (Koehn et al, 
2007) as the baseline system which can support 
lattice decoding. The alignment was obtained using 
GIZA++ (Och and Ney, 2003) and then we 
symmetrized the word alignment using the grow-
diag-final heuristic. Parameters were tuned using 
Minimum Error Rate Training (Och, 2003). To 
comprehensively evaluate the proposed methods in 
different domains, two groups of experiments were 
carried out, namely, the oral group (Goral) and the 
news group (Gnews). The experiments were 
conducted in both Chinese-English and English-
Chinese directions for the oral group, and Chinese-
English direction for the news group. The English 
sentences were all tokenized and lowercased, and 
the Chinese sentences were segmented into words 
by Language Technology Platform (LTP) 1 . We 
used SRILM2 for the training of language models 
(5-gram in all the experiments). The metrics for 
automatic evaluation were BLEU 3  and TER 4 
(Snover et al, 2005). 
The detailed statistics of the training data in Goral 
are showed in Table 2. For the bilingual corpus, we 
used the BTEC and PIVOT data of IWSLT 2008, 
HIT corpus 5  and other Chinese LDC (CLDC) 
                                                          
1 http://ir.hit.edu.cn/ltp/ 
2 http://www.speech.sri.com/projects/srilm/ 
3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl 
4 http://www.umiacs.umd.edu/~snover/terp/ 
5 The HIT corpus contains the CLDC Olympic corpus (2004-
863-008) and the other HIT corpora available at 
http://mitlab.hit.edu.cn/index.php/resources/29-the-
resource/111-share-bilingual-corpus.html. 
Figure 4: An example of lattice-based 
paraphrases for an input sentence. 
983
corpora, including the Chinese-English Sentence 
Aligned Bilingual Corpus (CLDC-LAC-2003-004) 
and the Chinese-English Parallel Corpora (CLDC-
LAC-2003-006). We trained a Chinese language 
model for the E-C translation on the Chinese part 
of the bi-text. For the English language model of 
C-E translation, an extra corpus named Tanaka was 
used besides the English part of the bilingual 
corpora. For testing and developing, we used six 
Chinese-English development corpora of IWSLT 
2008. The statistics are shown in Table 3.  
In detail, we chose CSTAR03-test and 
IWSLT06-dev as the development set; and used 
IWSLT04-test, IWSLT05-test, IWSLT06-dev and 
IWSLT07-test for testing. For English-Chinese 
evaluation, we used IWSLT English-Chinese MT 
evaluation 2005 as the test set. Due to the lacking 
of development set, we did not tune parameters on 
English-Chinese side, instead, we just used the 
default parameters of Moses. 
In the experiments of the news group, we used 
the Sinorama and FBIS corpora (LDC2005T10 and 
LDC2003E14) for bilingual corpus. After 
tokenization and filtering, this bilingual corpus 
contained 319,694 sentence pairs (7.9M tokens on 
Chinese side and 9.2M tokens on English side). 
We trained a 5-gram language model on the 
English side of the bi-text. The system was tested 
using the Chinese-English MT evaluation sets of 
NIST 2004, NIST 2006 and NIST 2008. For 
development, we used the Chinese-English MT 
evaluation sets of NIST 2002 and NIST 2005. 
Table 4 shows the statistics of test/development 
sets used in the news group. 
5.2 Results 
We extract both Chinese and English rules in Goral, 
and Chinese paraphrase rules in Gnews by 
comparing the results of Forward-Translation and 
Back-Translation as described in Section 3. During 
the extraction, some heuristics are used to ensure 
the quality of paraphrase rules. Take the extraction 
of Chinese paraphrase rules in Goral as a case study. 
Suppose (C0, E0) are the initial bilingual corpus of 
Goral. A Chinese-English and an English-Chinese 
MT system are trained on (C0, E0). We perform 
Back-Translation on E0 (?? ???????? ?? ?? ???????? ?? ??), and 
Forward-Translation on C0 (?? ???????? ?? ??). Suppose e1i and e2i are two aligned sentences in E1 and E2, 
c0i and c1i are the corresponding sentences in C0 
and C1. (c0i, c1i) are selected for the extraction of 
paraphrase rules if two conditions are satisfied: (1) 
BLEU(e2i) ? BLEU(e1i) > ?1, and (2) BLEU(e2i) > 
?2, where BLEU???  is a function for computing 
BLEU score; ?1 and ?2 are thresholds for balancing 
the rules number and the quality of paraphrase 
rules. In our experiment, ?1 and ?2 are empirically 
set to 0.1 and 0.3. 
As a result, we extract 912,625 Chinese and 
1,116,375 English paraphrase rules for Goral, and 
for Gnews the number of Chinese paraphrase rules is 
2,877,960. Then we use the extracted paraphrase 
rules to improve SMT by building word lattices for 
the input sentences. 
The Chinese-English experimental results of 
Goral and Gnews are shown in Table 5 and Table 6, 
respectively. It can be seen that our method 
outperforms the baselines in both oral and news 
domains. Our system gains significant 
improvements of 1.6~3.6 points of BLEU in the 
oral domain, and 0.5~1 points of BLEU in the 
news domain. Figure 5 shows the effect of 
considered paraphrases (k) in the step of building  
Corpus #Sen. pairs #Ch. words #En words
BETC 19,972 174k 190k 
PIVOT 20,000 162k 196k 
HIT 80,868 788k 850k 
CLDC 190,447 1,167k 1,898k 
Tanaka 149,207 - 1,375k 
Table 2: Statistics of training data in Goral 
 Corpus #Sen.  #Ref.  
develop CSTAR03 test set 506 16 IWSLT06 dev set 489 7 
test 
IWSLT04 test set 500 16 
IWSLT05 test set 506 16 
IWSLT06 test set 500 7 
IWSLT07 test set 489 6 
Table 3: Statistics of test/develop sets in Goral 
 Corpus #Sen.  #Ref.  
develop NIST 2002 878 10 NIST 2005 1,082 4 
test 
NIST 2004 1,788 5 
NIST 2006 1,664 4 
NIST 2008 1,357 4 
Table 4: Statistics of test/develop sets in Gnews 
984
word lattices. The result of English-Chinese 
experiments in Goral is shown in Table 7.  
6 Discussion 
We make a detailed analysis on the Chinese-
English translation results that are affected by our 
paraphrase rules. The aim is to investigate what 
kinds of paraphrases have been captured in the 
rules. Firstly the input path is recovered from the 
translation results according to the tracing 
information of the decoder, and therefore we can 
examine which path is selected by the SMT 
decoder from the paraphrase lattice. A human 
annotator is asked to judge whether the recovered 
paraphrase sentence keeps the same meaning as the 
original input. Then the annotator compares the 
baseline translation with the translations proposed 
by our approach. The analysis is carried out on the 
IWSLT 2007 Chinese-English test set, 84 out of 
489 input sentences have been affected by 
paraphrases, and the statistic of human evaluation 
is shown in Table 8.  
It can be seen in Table 8 that the paraphrases 
achieve a relatively high accuracy, 60 (71.4%) 
paraphrased sentences retain the same meaning, 
and the other 24 (28.6%) are incorrect. Among the 
60 correct paraphrases, 36 sentences finally result 
in an improved translation. We further analyze 
these paraphrases and the translation results to 
investigate what kinds of transformation finally 
lead to the translation quality improvement. The 
paraphrase variations can be classified into four 
categories: 
(1) Reordering: The original source sentences 
are reordered to be similar to the order of 
the target language. 
(2) Word substitution: A phrase with multi-
word translations is replaced by a phrase 
with a single-word translation.  
(3) Recovering omitted words: Ellipsis occurs 
frequently in spoken language. Recovering 
the omitted words often leads to a better 
translation. 
(4) Removing redundant words: Mostly, 
translating redundant words may confuse 
the SMT system and would be unnecessary. 
Removing redundant words can mitigate 
this problem. 
44.2?
44.4?
44.6?
44.8?
45.0?
45.2?
45.4?
0 10 20 30 40
BLE
U?s
cor
e?(
%)
Considered?paraprhases?(k)
Figure 5: Effect of considered paraphrases (k) 
on BLEU score
Model BLEU TER iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07
baseline 0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390 
para. improved 0.5712 0.6107 0.2924 0.4193 0.3055 0.2722 0.5374 0.4217 
Model BLEU TER nist 04 nist 06 nist 08 nist 04 nist 06 nist 08 
baseline 0.2795 0.2389 0.1933 0.6554 0.6515 0.6652 
para. improved 0.2891 0.2485 0.1978 0.6451 0.6407 0.6582 
 model IWSLT 2005  BLEU TER 
 baseline 0.4644 0.4164 
 para. improved  0.4853 0.3883 
trans. 
para. improve comparable worsen total
correct 36 20 4 60 
incorrect 1 9 14 24 
Table 8: Human analysis of the paraphrasing 
results in IWSLT 2007 CE translation 
Table 5: Experimental results of Goral in Chinese-English direction 
Table 6: Experimental results of Gnews in Chinese-English direction 
Table 7: Experimental results of Goral in 
English-Chinese direction 
985
Four examples for category (1), (2), (3) and (4) 
are shown in Table 9, respectively. The numbers in 
the second column indicates the number of the 
sentences affected by the rules, among the 36 
sentences with improved paraphrasing and 
translation. A sentence can be classified into 
multiple categories. Except category (2), the other 
three categories cannot be detected by the previous 
approaches, which verify our statement that our 
rules can capture structured paraphrases on the 
sentence level in addition to the paraphrases on the 
word or phrase level. 
Not all the paraphrased results are correct. 
Sometimes an ill paraphrased sentence can produce 
better translations. Take the first line of Table 9 as 
an example, the paraphrased sentence ???/How 
many ??/cigarettes ??/can ??/duty-free ?
/take ?/NULL? is not a fluent Chinese sentence, 
however, the rearranged word order is closer to 
English, which finally results in a much better 
translation. 
7 Related Work 
Previous studies on improving SMT using 
paraphrase rules focus on hand-crafted rules. 
Nakov (2008) employs six rules for paraphrasing 
the training corpus. Bond et al (2008) use 
grammars to paraphrase the source side of training 
data, covering aspects like word order and minor 
lexical variations (tenses etc.) but not content 
words. The paraphrases are added to the source 
side of the corpus and the corresponding target 
sentences are duplicated. 
A disadvantage for hand-crafted paraphrase 
rules is that it is language dependent. In contrast, 
our method that automatically extracted paraphrase 
rules from bilingual corpus is flexible and suitable 
for any language pairs. 
Our work is similar to Sun et al (2010). Both 
tried to capture the MT-favored structures from 
bilingual corpus. However, a clear difference is 
that Sun et al (2010) captures the structures 
implicitly by training an MT system on (S0, S1) and 
?translates? the SMT input to an MT-favored 
expression. Actually, the rewriting process is 
considered as a black box in Sun et al (2010). In 
this paper, the MT-favored expressions are 
captured explicitly by automatically extracted 
paraphrase rules. The advantages of the paraphrase 
rules are: (1) Our method can explicitly capture the 
structure information in the sentence level, 
enabling global reordering, which is impossible in 
Sun et al (2010). (2) For each rule, we can control 
its quality automatically or manually. 
8 Conclusion 
In this paper, we propose a novel method for 
extracting paraphrase rules by comparing the 
source side of bilingual corpus to the target-to-
source translation of the target side. The acquired 
paraphrase rules are employed to enrich the SMT 
inputs, which target on rewriting the input 
sentences to an MT-favored form. The paraphrase 
rules cover all kinds of paraphrases on the word, 
phrase and sentence levels, enabling structure 
reordering, word or phrase insertion, deletion and 
substitution. Experimental results show that the 
paraphrase rules can improve SMT quality in both 
the oral and news domains. The manual 
investigation on oral translation results indicate 
that the paraphrase rules capture four kinds of MT-
favored transformation to ensure translation quality 
improvement. 
Cate. Num Original Sentence/Translation Paraphrased Sentence/Translation 
(1) 11 
??/cigarette ??/can ??/duty-free ?
/take ??/how much ?/N/A ?  
??/how much ??/cigarettes ??/can ??
/duty-free ?/take ?/N/A ? 
what a cigarette can i take duty-free ? how many cigarettes can i take duty-free  one ? 
(2) 18 
?/you  ?/have  ??/how long  ?/N/A  
??/teaching ??/experience ? 
?/you  ?/have  ??/how much  ??/teaching  
??/experience ? 
you have how long teaching experience ? how much teaching experience you have ? 
(3) 10 ??/need  ??/deposit  ?/N/A ? ?/you  ??/need  ??/deposit  ?/N/A ? you need a deposit ? do you need a deposit ? 
(4) 4 
??/ring ?/fall ?/into ???/washbasin 
?/in ?/N/A ?  
ring off into the washbasin is in . 
??/ring  ?/fall  ?/into  ???/washbasin ?
/N/A ? 
ring off into the washbasin . 
Table 9: Examples for classification of paraphrase rules 
986
Acknowledgement 
This work was supported by National Natural 
Science Foundation of China (NSFC) (61073126, 
61133012), 863 High Technology Program 
(2011AA01A207). 
References  
Francis Bond, Eric Nichols, Darren Scott Appling, and 
Michael Paul. 2008. Improving Statistical Machine 
Translation by Paraphrasing the Training Data. In 
Proceedings of the IWSLT, pages 150?157. 
Chris Callison-Burch, Philipp Koehn, and Miles 
Osborne. 2006. Improved Statistical Machine 
Translation Using Paraphrases. In Proceedings of 
NAACL, pages 17-24. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL, pages 263?270. 
Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating 
Translation Using Source Language Paraphrase 
Lattices. In Proceedings of EMNLP, pages 420-429. 
Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011. 
Enriching SMT Training Data via Paraphrasing. In 
Proceedings of IJCNLP, pages 803-810. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
Proceedings of HLT/NAACL, pages 48?54 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
EMNLP, pages 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of IWSLT. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of the ACL Demo and Poster Sessions, 
pages 177?180. 
Roland Kuhn, Boxing Chen, George Foster and Evan 
Stratford. 2010. Phrase Clustering for Smoothing TM 
Probabilities-or, How to Extract Paraphrases from 
Phrase Tables. In Proceedings of COLING, pages 
608?616. 
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 
2009. Improved Statistical Machine Translation 
Using Monolingually-Dervied Paraphrases. In 
Proceedings of EMNLP, pages 381-390. 
Aur?lien Max. 2010. Example-Based Paraphrasing for 
Improved Phrase-Based Statistical Machine 
TranslationIn Proceedings of EMNLP, pages 656-
666. 
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, Idan Szpektor. 2009. 
Source-Language Entailment Modeling for 
Translation Unknown Terms. In Proceedings of ACL, 
pages 791-799. 
Preslav Nakov. 2008. Improved Statistical Machine 
Translation Using Monolingual Paraphrases. In 
Proceedings of ECAI, pages 338-342. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of ACL, 
pages 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL, pages 160-167. 
Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010. 
Paraphrase Lattice for Statistical Machine 
Translation. In Proceedings of ACL, pages 1-5. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
ACL, pages 311-318. 
Reinhard Rapp. 2009. The Back-translation Score: 
Automatic MT Evaluation at the Sentence Level 
without Reference Translations. In Proceedings of 
ACL-IJCNLP, pages 133-136. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, 
John Makhoul, Linnea Micciulla, and Ralph 
Weischedel. 2005. A study of translation error rate 
with targeted human annotation. Technical Report 
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005. 
Yanli Sun, Sharon O?Brien, Minako O?Hagan and Fred 
Hollowood. 2010. A Novel Statistical Pre-Processing 
Model for Rule-Based Machine Translation System. 
In Proceedings of EAMT, 8pp. 
987
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11?16,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparison of Chinese Parsers for Stanford Dependencies
Wanxiang Che?
car@ir.hit.edu.cn
Valentin I. Spitkovsky?
vals@stanford.edu
Ting Liu?
tliu@ir.hit.edu.cn
?School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Stanford dependencies are widely used in nat-
ural language processing as a semantically-
oriented representation, commonly generated
either by (i) converting the output of a con-
stituent parser, or (ii) predicting dependencies
directly. Previous comparisons of the two ap-
proaches for English suggest that starting from
constituents yields higher accuracies. In this
paper, we re-evaluate both methods for Chi-
nese, using more accurate dependency parsers
than in previous work. Our comparison of per-
formance and efficiency across seven popular
open source parsers (four constituent and three
dependency) shows, by contrast, that recent
higher-order graph-based techniques can be
more accurate, though somewhat slower, than
constituent parsers. We demonstrate also that
n-way jackknifing is a useful technique for
producing automatic (rather than gold) part-
of-speech tags to train Chinese dependency
parsers. Finally, we analyze the relations pro-
duced by both kinds of parsing and suggest
which specific parsers to use in practice.
1 Introduction
Stanford dependencies (de Marneffe and Man-
ning, 2008) provide a simple description of rela-
tions between pairs of words in a sentence. This
semantically-oriented representation is intuitive and
easy to apply, requiring little linguistic expertise.
Consequently, Stanford dependencies are widely
used: in biomedical text mining (Kim et al, 2009),
as well as in textual entailment (Androutsopou-
los and Malakasiotis, 2010), information extrac-
tion (Wu and Weld, 2010; Banko et al, 2007) and
sentiment analysis (Meena and Prabhakar, 2007).
In addition to English, there is a Chinese ver-
sion of Stanford dependencies (Chang et al, 2009),
(a) A constituent parse tree.
(b) Stanford dependencies.
Figure 1: A sample Chinese constituent parse tree and its
corresponding Stanford dependencies for the sentence
China (??) encourages (??) private (??)
entrepreneurs (???) to invest (??) in
national (??) infrastructure (??) construction (??).
which is also useful for many applications, such as
Chinese sentiment analysis (Wu et al, 2011; Wu et
al., 2009; Zhuang et al, 2006) and relation extrac-
tion (Huang et al, 2008). Figure 1 shows a sample
constituent parse tree and the corresponding Stan-
ford dependencies for a sentence in Chinese. Al-
though there are several variants of Stanford depen-
dencies for English,1 so far only a basic version (i.e,
dependency tree structures) is available for Chinese.
Stanford dependencies were originally obtained
from constituent trees, using rules (de Marneffe et
al., 2006). But as dependency parsing technolo-
gies mature (Ku?bler et al, 2009), they offer increas-
ingly attractive alternatives that eliminate the need
for an intermediate representation. Cer et al (2010)
reported that Stanford?s implementation (Klein and
Manning, 2003) underperforms other constituent
1nlp.stanford.edu/software/dependencies_manual.pdf
11
Type Parser Version Algorithm URL
Constituent Berkeley 1.1 PCFG code.google.com/p/berkeleyparser
Bikel 1.2 PCFG www.cis.upenn.edu/?dbikel/download.html
Charniak Nov. 2009 PCFG www.cog.brown.edu/?mj/Software.htm
Stanford 2.0 Factored nlp.stanford.edu/software/lex-parser.shtml
Dependency MaltParser 1.6.1 Arc-Eager maltparser.org
Mate 2.0 2nd-order MST code.google.com/p/mate-tools
MSTParser 0.5 MST sourceforge.net/projects/mstparser
Table 1: Basic information for the seven parsers included in our experiments.
parsers, for English, on both accuracy and speed.
Their thorough investigation also showed that con-
stituent parsers systematically outperform parsing
directly to Stanford dependencies. Nevertheless, rel-
ative standings could have changed in recent years:
dependency parsers are now significantly more ac-
curate, thanks to advances like the high-order maxi-
mum spanning tree (MST) model (Koo and Collins,
2010) for graph-based dependency parsing (McDon-
ald and Pereira, 2006). Therefore, we deemed it im-
portant to re-evaluate the performance of constituent
and dependency parsers. But the main purpose of
our work is to apply the more sophisticated depen-
dency parsing algorithms specifically to Chinese.
Number of \in Train Dev Test Total
files 2,083 160 205 2,448
sentences 46,572 2,079 2,796 51,447
tokens 1,039,942 59,955 81,578 1,181,475
Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data.
2 Methodology
We compared seven popular open source constituent
and dependency parsers, focusing on both accuracy
and parsing speed. We hope that our analysis will
help end-users select a suitable method for parsing
to Stanford dependencies in their own applications.
2.1 Parsers
We considered four constituent parsers. They are:
Berkeley (Petrov et al, 2006), Bikel (2004), Char-
niak (2000) and Stanford (Klein and Manning,
2003) chineseFactored, which is also the default
used by Stanford dependencies. The three depen-
dency parsers are: MaltParser (Nivre et al, 2006),
Mate (Bohnet, 2010)2 and MSTParser (McDonald
and Pereira, 2006). Table 1 has more information.
2A second-order MST parser (with the speed optimization).
2.2 Corpus
We used the latest Chinese TreeBank (CTB) 7.0 in
all experiments.3 CTB 7.0 is larger and has more
sources (e.g., web text), compared to previous ver-
sions. We split the data into train/development/test
sets (see Table 2), with gold word segmentation, fol-
lowing the guidelines suggested in documentation.
2.3 Settings
Every parser was run with its own default options.
However, since the default classifier used by Malt-
Parser is libsvm (Chang and Lin, 2011) with a poly-
nomial kernel, it may be too slow for training models
on all of CTB 7.0 training data in acceptable time.
Therefore, we also tested this particular parser with
the faster liblinear (Fan et al, 2008) classifier. All
experiments were performed on a machine with In-
tel?s Xeon E5620 2.40GHz CPU and 24GB RAM.
2.4 Features
Unlike constituent parsers, dependency models re-
quire exogenous part-of-speech (POS) tags, both in
training and in inference. We used the Stanford tag-
ger (Toutanova et al, 2003) v3.1, with the MEMM
model,4 in combination with 10-way jackknifing.5
Word lemmas ? which are generalizations of
words ? are another feature known to be useful
for dependency parsing. Here we lemmatized each
Chinese word down to its last character, since ? in
contrast to English ? a Chinese word?s suffix often
carries that word?s core sense (Tseng et al, 2005).
For example, bicycle (???), car (??) and
train (??) are all various kinds of vehicle (?).
3www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2010T07
4nlp.stanford.edu/software/tagger.shtml
5Training sentences in each fold were tagged using a model
based on the other nine folds; development and test sentences
were tagged using a model based on all ten of the training folds.
12
Dev Test
Type Parser UAS LAS UAS LAS Parsing Time
Constituent Berkeley 82.0 77.0 82.9 77.8 45:56
Bikel 79.4 74.1 80.0 74.3 6,861:31
Charniak 77.8 71.7 78.3 72.3 128:04
Stanford 76.9 71.2 77.3 71.4 330:50
Dependency MaltParser (liblinear) 76.0 71.2 76.3 71.2 0:11
MaltParser (libsvm) 77.3 72.7 78.0 73.1 556:51
Mate (2nd-order) 82.8 78.2 83.1 78.1 87:19
MSTParser (1st-order) 78.8 73.4 78.9 73.1 12:17
Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)
are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen-
eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
3 Results
Table 3 tabulates efficiency and performance for all
parsers; UAS and LAS are unlabeled and labeled at-
tachment scores, respectively ? the standard crite-
ria for evaluating dependencies. They can be com-
puted via a CoNLL-X shared task dependency pars-
ing evaluation tool (without scoring punctuation).6
3.1 Chinese
Mate scored highest, and Berkeley was the most ac-
curate of constituent parsers, slightly behind Mate,
using half of the time. MaltParser (liblinear) was by
far the most efficient but also the least performant; it
scored higher with libsvm but took much more time.
The 1st-order MSTParser was more accurate than
MaltParser (libsvm) ? a result that differs from that
of Cer et al (2010) for English (see ?3.2). The Stan-
ford parser (the default for Stanford dependencies)
was only slightly more accurate than MaltParser (li-
blinear). Bikel?s parser was too slow to be used in
practice; and Charniak?s parser ? which performs
best for English ? did not work well for Chinese.
3.2 English
Our replication of Cer et al?s (2010, Table 1) evalua-
tion revealed a bug: MSTParser normalized all num-
bers to a <num> symbol, which decreased its scores
in the evaluation tool used with Stanford dependen-
cies. After fixing this glitch, MSTParser?s perfor-
mance improved from 78.8 (reported) to 82.5%, thus
making it more accurate than MaltParser (81.1%)
and hence the better dependency parser for English,
consistent with our results for Chinese (see Table 3).
6ilk.uvt.nl/conll/software/eval.pl
Our finding does not contradict the main qualita-
tive result of Cer et al (2010), however, since the
constituent parser of Charniak and Johnson (2005)
still scores substantially higher (89.1%), for English,
compared to all dependency parsers.7 In a separate
experiment (parsing web data),8 we found Mate to
be less accurate than Charniak-Johnson ? and im-
provement from jackknifing smaller ? on English.
4 Analysis
To further compare the constituent and dependency
approaches to generating Stanford dependencies, we
focused on Mate and Berkeley parsers ? the best
of each type. Overall, the difference between their
accuracies is not statistically significant (p > 0.05).9
Table 4 highlights performance (F1 scores) for the
most frequent relation labels. Mate does better on
most relations, noun compound modifiers (nn) and
adjectival modifiers (amod) in particular; and the
Berkeley parser is better at root and dep.10 Mate
seems to excel at short-distance dependencies, pos-
sibly because it uses more local features (even with
a second-order model) than the Berkeley parser,
whose PCFG can capture longer-distance rules.
Since POS-tags are especially informative of Chi-
nese dependencies (Li et al, 2011), we harmonized
training and test data, using 10-way jackknifing (see
?2.4). This method is more robust than training a
7One (small) factor contributing to the difference between
the two languages is that in the Chinese setup we stop with basic
Stanford dependencies ? there is no penalty for further conver-
sion; another is not using discriminative reranking for Chinese.
8sites.google.com/site/sancl2012/home/shared-task
9For LAS, p ? 0.11; and for UAS, p ? 0.25, according to
www.cis.upenn.edu/?dbikel/download/compare.pl
10An unmatched (default) relation (Chang et al, 2009, ?3.1).
13
Relation Count Mate Berkeley
nn 7,783 91.3 89.3
dep 4,651 69.4 70.3
nsubj 4,531 87.1 85.5
advmod 4,028 94.3 93.8
dobj 3,990 86.0 85.0
conj 2,159 76.0 75.8
prep 2,091 94.3 94.1
root 2,079 81.2 82.3
nummod 1,614 97.4 96.7
assmod 1,593 86.3 84.1
assm 1,590 88.9 87.2
pobj 1,532 84.2 82.9
amod 1,440 85.6 81.1
rcmod 1,433 74.0 70.6
cpm 1,371 84.4 83.2
Table 4: Performance (F1 scores) for the fifteen most-
frequent dependency relations in the CTB 7.0 develop-
ment data set attained by both Mate and Berkeley parsers.
parser with gold tags because it improves consis-
tency, particularly for Chinese, where tagging accu-
racies are lower than in English. On development
data, Mate scored worse given gold tags (75.4 versus
78.2%).11 Lemmatization offered additional useful
cues for overcoming data sparseness (77.8 without,
versus 78.2% with lemma features). Unsupervised
word clusters could thus also help (Koo et al, 2008).
5 Discussion
Our results suggest that if accuracy is of primary
concern, then Mate should be preferred;12 however,
Berkeley parser offers a trade-off between accuracy
and speed. If neither parser satisfies the demands
of a practical application (e.g., real-time processing
or bulk-parsing the web), then MaltParser (liblinear)
may be the only viable option. Fortunately, it comes
with much headroom for improving accuracy, in-
cluding a tunable margin parameter C for the classi-
fier, richer feature sets (Zhang and Nivre, 2011) and
ensemble models (Surdeanu and Manning, 2010).
Stanford dependencies are not the only popular
dependency representation. We also considered the
11Berkeley?s performance suffered with jackknifed tags (76.5
versus 77.0%), possibly because it parses and tags better jointly.
12Although Mate?s performance was not significantly better
than Berkeley?s in our setting, it has the potential to tap richer
features and other advantages of dependency parsers (Nivre and
McDonald, 2008) to further boost accuracy, which may be diffi-
cult in the generative framework of a typical constituent parser.
conversion scheme of the Penn2Malt tool,13 used
in a series of CoNLL shared tasks (Buchholz and
Marsi, 2006; Nivre et al, 2007; Surdeanu et al,
2008; Hajic? et al, 2009). However, this tool relies
on function tag information from the CTB in deter-
mining dependency relations. Since these tags usu-
ally cannot be produced by constituent parsers, we
could not, in turn, obtain CoNLL-style dependency
trees from their output. This points to another advan-
tage of dependency parsers: they need only the de-
pendency tree corpus to train and can conveniently
make use of native (unconverted) corpora, such as
the Chinese Dependency Treebank (Liu et al, 2006).
Lastly, we must note that although the Berkeley
parser is on par with Charniak?s (2000) system for
English (Cer et al, 2010, Table 1), its scores for Chi-
nese are substantially higher. There may be subtle
biases in Charniak?s approach (e.g., the conditioning
hierarchy used in smoothing) that could turn out to
be language-specific. The Berkeley parser appears
more general ? without quite as many parameters
or idiosyncratic design decisions ? as evidenced by
a recent application to French (Candito et al, 2010).
6 Conclusion
We compared seven popular open source parsers ?
four constituent and three dependency ? for gen-
erating Stanford dependencies in Chinese. Mate, a
high-order MST dependency parser, with lemmati-
zation and jackknifed POS-tags, appears most accu-
rate; but Berkeley?s faster constituent parser, with
jointly-inferred tags, is statistically no worse. This
outcome is different from English, where constituent
parsers systematically outperform direct methods.
Though Mate scored higher overall, Berkeley?s
parser was better at recovering longer-distance re-
lations, suggesting that a combined approach could
perhaps work better still (Rush et al, 2010, ?4.2).
Acknowledgments
We thank Daniel Cer, for helping us replicate the English ex-
perimental setup and for suggesting that we explore jackknifing
methods, and the anonymous reviewers, for valuable comments.
Supported in part by the National Natural Science Founda-
tion of China (NSFC) via grant 61133012, the National ?863?
Major Project grant 2011AA01A207, and the National ?863?
Leading Technology Research Project grant 2012AA011102.
13w3.msi.vxu.se/?nivre/research/Penn2Malt.html
14
Second author gratefully acknowledges the continued help
and support of his advisor, Dan Jurafsky, and of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program, under the Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions, findings,
and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the views
of DARPA, AFRL, or the US government.
References
Ion Androutsopoulos and Prodromos Malakasiotis. 2010.
A survey of paraphrasing and textual entailment methods.
Journal of Artificial Intelligence Research, 38(1):135?187,
May.
Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt
Broadhead, and Oren Etzioni. 2007. Open information ex-
traction from the web. In Proceedings of the 20th interna-
tional joint conference on Artifical intelligence, IJCAI?07,
pages 2670?2676, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Daniel M. Bikel. 2004. A distributional analysis of a lexi-
calized statistical parsing model. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 182?189,
Barcelona, Spain, July. Association for Computational Lin-
guistics.
Bernd Bohnet. 2010. Top accuracy and fast dependency pars-
ing is not a contradiction. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics (Coling
2010), pages 89?97, Beijing, China, August. Coling 2010
Organizing Committee.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Language
Learning (CoNLL-X), pages 149?164, New York City, June.
Association for Computational Linguistics.
Marie Candito, Joakim Nivre, Pascal Denis, and Enrique Hene-
stroza Anguiano. 2010. Benchmarking of statistical depen-
dency parsers for French. In Coling 2010: Posters, pages
108?116, Beijing, China, August. Coling 2010 Organizing
Committee.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and
Christopher D. Manning. 2010. Parsing to Stanford depen-
dencies: Trade-offs between speed and accuracy. In Pro-
ceedings of the 7th International Conference on Language
Resources and Evaluation (LREC 2010).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A li-
brary for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):27:1?27:27, May.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Discriminative reordering with
Chinese grammatical relations features. In Proceedings of
the Third Workshop on Syntax and Structure in Statistical
Translation, Boulder, Colorado, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 173?180, Ann
Arbor, Michigan, June. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference,
NAACL 2000, pages 132?139, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
COLING Workshop on Cross-framework and Cross-domain
Parser Evaluation.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
Fifth International Conference on Language Resources and
Evaluation (LREC?06).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning
Research, 9:1871?1874, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan S?te?pa?nek,
Pavel Stran?a?k, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages. In
Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Ruihong Huang, Le Sun, and Yuanyong Feng. 2008. Study
of kernel-based methods for Chinese relation extraction. In
Proceedings of the 4th Asia information retrieval conference
on Information retrieval technology, AIRS?08, pages 598?
604, Berlin, Heidelberg. Springer-Verlag.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Jun?ichi Tsujii. 2009. Overview of BioNLP?09
shared task on event extraction. In Proceedings of the Work-
shop on Current Trends in Biomedical Natural Language
Processing: Shared Task, BioNLP ?09, pages 1?9, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics - Volume
1, ACL ?03, pages 423?430, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, ACL
?10, pages 1?11, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple semi-supervised dependency parsing. In Proceedings of
ACL-08: HLT, pages 595?603, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre. 2009.
Dependency Parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
15
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang
Chen, and Haizhou Li. 2011. Joint models for Chinese POS
tagging and dependency parsing. In Proceedings of the 2011
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 1180?1191, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building a de-
pendency treebank for improving Chinese parser. Journal of
Chinese Language and Computing, 16(4).
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proceed-
ings of the 11th Conference of the European Chapter of the
ACL (EACL 2006), pages 81?88.
Arun Meena and T. V. Prabhakar. 2007. Sentence level sen-
timent analysis in the presence of conjuncts using linguistic
analysis. In Proceedings of the 29th European conference on
IR research, ECIR?07, pages 573?580, Berlin, Heidelberg.
Springer-Verlag.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In Proceed-
ings of ACL-08: HLT, pages 950?958, Columbus, Ohio,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency parsing. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages 2216?
2219.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932, Prague, Czech Republic, June.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics, pages
433?440, Sydney, Australia, July. Association for Computa-
tional Linguistics.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and linear
programming relaxations for natural language processing. In
Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1?11, Cambridge,
MA, October. Association for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble
models for dependency parsing: cheap and good? In Hu-
man Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 649?652, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL 2008 shared
task on joint parsing of syntactic and semantic dependen-
cies. In CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning, pages
159?177, Manchester, England, August. Coling 2008 Orga-
nizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 173?180,
Stroudsburg, PA, USA. Association for Computational Lin-
guistics.
Huihsin Tseng, Daniel Jurafsky, and Christopher Manning.
2005. Morphological features help POS tagging of un-
known words across language varieties. In Proceedings of
the fourth SIGHAN bakeoff.
Fei Wu and Daniel S. Weld. 2010. Open information extraction
using Wikipedia. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, ACL
?10, pages 118?127, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009.
Phrase dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 3 - Volume 3, EMNLP
?09, pages 1533?1541, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2011.
Structural opinion mining for graph-based sentiment rep-
resentation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP ?11,
pages 1332?1341, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceedings
of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 188?193, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of the 15th
ACM international conference on Information and knowl-
edge management, CIKM ?06, pages 43?50, New York, NY,
USA. ACM.
16
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 125?134,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Chinese Parsing Exploiting Characters
Meishan Zhang?, Yue Zhang??, Wanxiang Che?, Ting Liu?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
Characters play an important role in the
Chinese language, yet computational pro-
cessing of Chinese has been dominated
by word-based approaches, with leaves in
syntax trees being words. We investigate
Chinese parsing from the character-level,
extending the notion of phrase-structure
trees by annotating internal structures of
words. We demonstrate the importance
of character-level information to Chinese
processing by building a joint segmen-
tation, part-of-speech (POS) tagging and
phrase-structure parsing system that inte-
grates character-structure features. Our
joint system significantly outperforms a
state-of-the-art word-based baseline on the
standard CTB5 test, and gives the best
published results for Chinese parsing.
1 Introduction
Characters play an important role in the Chinese
language. They act as basic phonetic, morpho-
syntactic and semantic units in a Chinese sentence.
Frequently-occurring character sequences that ex-
press certain meanings can be treated as words,
while most Chinese words have syntactic struc-
tures. For example, Figure 1(b) shows the struc-
ture of the word ???? (construction and build-
ing industry)?, where the characters ?? (construc-
tion)? and ?? (building)? form a coordination,
and modify the character ?? (industry)?.
However, computational processing of Chinese
is typically based on words. Words are treated
as the atomic units in syntactic parsing, machine
translation, question answering and other NLP
tasks. Manually annotated corpora, such as the
Chinese Treebank (CTB) (Xue et al, 2005), usu-
ally have words as the basic syntactic elements
?Email correspondence.
?? ???
??
? ??
NR NN
VV
JJ NN
NP NP
NPADJP
NP
VPNP
IP
NP NP
NPADJP
NP
VPNP
IP
NN
?
NR-e
?
NR-b
JJ
?
JJ-s
VV
?
VV-e
?
VV-bNN
?
NN-e
?
NN-m
?
NN-b
NR
?
NR-eNR-b
?
NP NP
NPADJP
NP
VPNP
IP
NN-c
?
NR-i
?
NR-b
JJ-t
?
JJ-b
VV-c
?
VV-i
?
VV-bNR-r
?
NR-iNR-b
?
NR-t
NN-r
?
NN-i
?
NN-i
?
NN-b
NN-c
NN-t
VV-t
NN-t
(a) CTB-style word-based syntax tree for ??? (China) ?
?? (architecture industry) ?? (show) ? (new) ??
(pattern)?.
?? ???
??
? ??
NR NN
VV
JJ NN
NP NP
NPADJP
NP
VPNP
IP
NP NP
NPADJP
NP
VPNP
IP
NN
?
NR-e
?
NR-b
JJ
?
JJ-s
VV
?
VV-e
?
VV-bNN
?
NN-e
?
NN-m
?
NN-b
NR
?
NR-eNR-b
?
NP NP
NPADJP
NP
VPNP
IP
NN-c
?
NN-i
?
NN-b
JJ-t
?
JJ-b
VV-c
?
VV-i
?
VV-bNR-r
?
NR-iNR-b
?
NR-t
NN-r
?
NN-i
?
NN-i
?
NN-b
NN-c
NN-t
VV-t
NN-t
(b) character-level syntax tree with hierarchal word structures
for ?? (middle) ? (nation) ? (construction) ? (building)
? (industry) ? (present) ? (show) ? (new) ? (style) ?
(situation)?.
Figure 1: Word-based and character-level phrase-
structure trees for the sentence ????????
??? (China?s architecture industry shows new
patterns)?, where ?l?, ?r?, ?c? denote the direc-
tions of head characters (see section 2).
(Figure 1(a)). This form of annotation does not
give character-level syntactic structures for words,
a source of linguistic information that is more fun-
damental and less sparse than atomic words.
In this paper, we investigate Chinese syn-
tactic parsing with character-level information
by extending the notation of phrase-structure
125
(constituent) trees, adding recursive structures of
characters for words. We manually annotate the
structures of 37,382 words, which cover the entire
CTB5. Using these annotations, we transform
CTB-style constituent trees into character-level
trees (Figure 1(b)). Our word structure corpus,
together with a set of tools to transform CTB-style
trees into character-level trees, is released at
https://github.com/zhangmeishan/wordstructures.
Our annotation work is in line with the work of
Vadas and Curran (2007) and Li (2011), which
provide extended annotations of Penn Treebank
(PTB) noun phrases and CTB words (on the
morphological level), respectively.
We build a character-based Chinese parsing
model to parse the character-level syntax trees.
Given an input Chinese sentence, our parser pro-
duces its character-level syntax trees (Figure 1(b)).
With richer information than word-level trees, this
form of parse trees can be useful for all the afore-
mentioned Chinese NLP applications.
With regard to task of parsing itself, an impor-
tant advantage of the character-level syntax trees is
that they allow word segmentation, part-of-speech
(POS) tagging and parsing to be performed jointly,
using an efficient CKY-style or shift-reduce algo-
rithm. Luo (2003) exploited this advantage by
adding flat word structures without manually an-
notation to CTB trees, and building a generative
character-based parser. Compared to a pipeline
system, the advantages of a joint system include
reduction of error propagation, and the integration
of segmentation, POS tagging and syntax features.
With hierarchical structures and head character in-
formation, our annotated words are more informa-
tive than flat word structures, and hence can bring
further improvements to phrase-structure parsing.
To analyze word structures in addition to phrase
structures, our character-based parser naturally
performs joint word segmentation, POS tagging
and parsing jointly. Our model is based on the
discriminative shift-reduce parser of Zhang and
Clark (2009; 2011), which is a state-of-the-art
word-based phrase-structure parser for Chinese.
We extend their shift-reduce framework, adding
more transition actions for word segmentation and
POS tagging, and defining novel features that cap-
ture character information. Even when trained
using character-level syntax trees with flat word
structures, our joint parser outperforms a strong
pipelined baseline that consists of a state-of-the-
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(a) subject-predicate.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(b) verb-object.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(c) coordination.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(d) modifier-noun.
Figure 2: Inner word structures of ??? (reper-
tory)?,??? (archaeology)?, ??? (science and
technology)? and ??? (degenerate)?.
art joint segmenter and POS tagger, and our base-
line word-based parser. Our word annotations lead
to further improvements to the joint system, espe-
cially for phrase-structure parsing accuracy.
Our parser work falls in line with recent work
of joint segmentation, POS tagging and parsing
(Hatori et al, 2012; Li and Zhou, 2012; Qian
and Liu, 2012). Compared with related work,
our model gives the best published results for
joint segmentation and POS tagging, as well as
joint phrase-structure parsing on standard CTB5
evaluations. With linear-time complexity, our
parser is highly efficient, processing over 30 sen-
tences per second with a beam size of 16. An
open release of the parser is freely available at
http://sourceforge.net/projects/zpar/, version 0.6.
2 Word Structures and Syntax Trees
The Chinese language is a character-based lan-
guage. Unlike alphabetical languages, Chinese
characters convey meanings, and the meaning of
most Chinese words takes roots in their charac-
ter. For example, the word ???? (computer)? is
composed of the characters ?? (count)?, ?? (cal-
culate)? and ?? (machine)?. An informal name of
?computer? is ????, which is composed of ??
(electronic)? and ?? (brain)?.
Chinese words have internal structures (Xue,
2001; Ma et al, 2012). The way characters inter-
act within words can be similar to the way words
interact within phrases. Figure 2 shows the struc-
tures of the four words ??? (repertory)?, ???
126
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
Figure 3: Character-level word structure of ???
?? (crouching tiger hidden dragon)?.
(archaeology)?, ??? (science and technology)?
and ??? (degenerate)?, which demonstrate
four typical syntactic structures of two-character
words, including subject-predicate, verb-object,
coordination and modifier-noun structures. Multi-
character words can also have recursive syntac-
tic structures. Figure 3 illustrates the structure
of the word ????? (crouching tiger hidden
dragon)?, which is composed of two subwords ??
? (crouching tiger)? and ??? (hidden dragon)?,
both having a modifier-noun structure.
The meaning of characters can be a useful
source of information for computational process-
ing of Chinese, and some recent work has started
to exploit this information. Zhang and Clark
(2010) found that the first character in a Chinese
word is a useful indicator of the word?s POS. They
made use of this information to help joint word
segmentation and POS tagging.
Li (2011) studied the morphological structures
of Chinese words, showing that 35% percent of
the words in CTB5 can be treated as having mor-
phemes. Figure 4(a) illustrates the morphological
structures of the words ? ??? (friends)? and
???? (educational world)?, in which the char-
acters ?? (plural)? and ?? (field)? can be treated
as suffix morphemes. They studied the influence
of such morphology to Chinese dependency pars-
ing (Li and Zhou, 2012).
The aforementioned work explores the influ-
ence of particular types of characters to Chinese
processing, yet not the full potentials of complete
word structures. We take one step further in this
line of work, annotating the full syntactic struc-
tures of 37,382 Chinese words in the form of Fig-
ure 2 and Figure 3. Our annotation covers the
entire vocabulary of CTB5. In addition to dif-
ference in coverage (100% vs 35%), our annota-
tion is structurally more informative than that of
Li (2011), as illustrated in Figure 4(b).
Our annotations are binarized recursive word
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(a) morphological-level word structures, where ?f? de-
notes a special mark for fine-grained words.
NN-c
NN-iNN-b
?(science) ?(technology)
VV-l
VV-iVV-b
?(burn) ?(up)
NN-r
NN-iNN-b
?(repository) ?(saving)
NN-l
VV-iVV-b
?(investigate) ?(ancient)
NN-r
NN-iNN-b
?(bad) ?(kind)
AD-l
AD-iAD-b
?(vain) ?(so)
NN-r
NN-iNN-b
?(crouching) ?(tiger)
NN-r
NN-iNN-i
?(hidden) ?(dragon)
NN-c
VV-r
VV-iVV-b
?(fiercely) ?(sweep)
VV-r
VV-iVV-i
?(thousands) ?(troops)
VV-l
NN-c
NN-iNN-b
?(teach) ?(education)
NN-i
?(field)
NN-r
NN
NN-fNN-f
??(education) ?(field)
NN-c
NN-iNN-b
?(friend) ?(friend)
NN-i
?(plural)
NN-l
NN
NN-fNN-f
??(friend) ?(plural)
(b) character-level word structures.
Figure 4: Comparison between character-level and
morphological-level word structures.
structures. For each word or subword, we spec-
ify its POS and head direction. We use ?l?, ?r?
and ?c? to indicate the ?left?, ?right? and ?coordi-
nation? head directions, respectively. The ?coor-
dination? direction is mostly used in coordination
structures, while a very small number of translit-
eration words, such as ???? (Obama)? and ??
?? (Los Angeles)?, have flat structures, and we
use ?coordination? for their left binarization. For
leaf characters, we follow previous work on word
segmentation (Xue, 2003; Ng and Low, 2004), and
use ?b? and ?i? to indicate the beginning and non-
beginning characters of a word, respectively.
The vast majority of words do not have struc-
tural ambiguities. However, the structures of some
words may vary according to different POS. For
example, ???? means ?dominate? when it is
tagged as a verb, of which the head is the left char-
acter; the same word means ?uniform dress? when
tagged as a noun, of which the head is the right
character. Thus the input of the word structure
annotation is a word together with its POS. The
annotation work was conducted by three persons,
with one person annotating the entire corpus, and
the other two checking the annotations.
Using our annotations, we can extend CTB-
style syntax trees (Figure 1(a)) into character-
level trees (Figure 1(b)). In particular, we mark
the original nodes that represent POS tags in CTB-
style trees with ?-t?, and insert our word structures
as unary subnodes of the ?-t? nodes. For the rest
of the paper, we refer to the ?-t? nodes as full-word
nodes, all nodes above full-word nodes as phrase
127
nodes, and all nodes below full-word nodes as sub-
word nodes.
Our character-level trees contain additional syn-
tactic information, which are potentially useful to
Chinese processing. For example, the head char-
acters of words can be populated up to phrase-
level nodes, and serve as an additional source of
information that is less sparse than head words. In
this paper, we build a parser that yields character-
level trees from raw character sequences. In addi-
tion, we use this parser to study the effects of our
annotations to character-based statistical Chinese
parsing, showing that they are useful in improving
parsing accuracies.
3 Character-based Chinese Parsing
To produce character-level trees for Chinese
NLP tasks, we develop a character-based parsing
model, which can jointly perform word segmen-
tation, POS tagging and phrase-structure parsing.
To our knowledge, this is the first work to develop
a transition-based system that jointly performs the
above three tasks. Trained using annotated word
structures, our parser also analyzes the internal
structures of Chinese words.
Our character-based Chinese parsing model is
based on the work of Zhang and Clark (2009),
which is a transition-based model for lexicalized
constituent parsing. They use a beam-search de-
coder so that the transition action sequence can be
globally optimized. The averaged perceptron with
early-update (Collins and Roark, 2004) is used to
train the model parameters. Their transition sys-
tem contains four kinds of actions: (1) SHIFT,
(2) REDUCE-UNARY, (3) REDUCE-BINARY and
(4) TERMINATE. The system can provide bina-
rzied CFG trees in Chomsky Norm Form, and they
present a reversible conversion procedure to map
arbitrary CFG trees into binarized trees.
In this work, we remain consistent with their
work, using the head-finding rules of Zhang and
Clark (2008), and the same binarization algo-
rithm.1 We apply the same beam-search algorithm
for decoding, and employ the averaged perceptron
with early-update to train our model.
We make two extensions to their work to en-
able joint segmentation, POS tagging and phrase-
structure parsing from the character level. First,
we modify the actions of the transition system for
1We use a left-binarization process for flat word structures
that contain more than two characters.
S2
sta ck
. . .
. . .
q u eu e
Q0 Q1 . . .S1
S1l S1r
. . . . . .
S0
S0l S0r
. . . . . .
Figure 5: A state in a transition-based model.
parsing the inner structures of words. Second, we
extend the feature set for our parsing problem.
3.1 The Transition System
In a transition-based system, an input sentence is
processed in a linear left-to-right pass, and the
output is constructed by a state-transition pro-
cess. We learn a model for scoring the transi-
tion Ai from one state STi to the next STi+1. As
shown in Figure 5, a state ST consists of a stack
S and a queue Q, where S = (? ? ? , S1, S0) con-
tains partially constructed parse trees, and Q =
(Q0, Q1, ? ? ? , Qn?j) = (cj , cj+1, ? ? ? , cn) is the
sequence of input characters that have not been
processed. The candidate transition action A at
each step is defined as follows:
? SHIFT-SEPARATE(t): remove the head
character cj from Q, pushing a subword node
S?
cj
2 onto S, assigning S?.t = t. Note that the
parse tree S0 must correspond to a full-word
or a phrase node, and the character cj is the
first character of the next word. The argu-
ment t denotes the POS of S?.
? SHIFT-APPEND: remove the head character
cj from Q, pushing a subword node S?cj onto
S. cj will eventually be combined with all the
subword nodes on top of S to form a word,
and thus we must have S?.t = S0.t.
? REDUCE-SUBWORD(d): pop the top two
nodes S0 and S1 off S, pushing a new sub-
word node S?S1 S0 onto S. The argument ddenotes the head direction of S?, of which
the value can be ?left?, ?right? or ?coordi-
nation?.3 Both S0 and S1 must be subword
nodes and S?.t = S0.t = S1.t.
2We use this notation for a compact representation of a
tree node, where the numerator represents a father node, and
the denominator represents the children.
3For the head direction ?coordination?, we extract the
head character from the left node.
128
Category Feature templates When to Apply
Structure S0ntl S0nwl S1ntl S1nwl S2ntl S2nwl S3ntl S3nwl, All
features Q0c Q1c Q2c Q3c Q0c ?Q1c Q1c ?Q2c Q2c ?Q3c,
S0ltwl S0rtwl S0utwl S1ltwl S1rtwl S1utwl,
S0nw ? S1nw S0nw ? S1nl S0nl ? S1nw S0nl ? S1nl,
S0nw ?Q0c S0nl ?Q0c S1nw ?Q0c S1nlQ0c,
S0nl ? S1nl ? S2nl S0nw ? S1nl ? S2nl S0nl ? S1nw ? S2nl S0nl ? S1nl ? S2nw,
S0nw ? S1nl ?Q0c S0nl ? S1nw ?Q0c S0nl ? S1nl ?Q0c,
S0ncl S0nct S0nctl S1ncl S1nct S1nctl,
S2ncl S2nct S2nctl S3ncl S3nct S3nctl,
S0nc ? S1nc S0ncl ? S1nl S0nl ? S1ncl S0ncl ? S1ncl,
S0nc ? Q0c S0nl ? Q0c S1nc ? Q0c S1nl ? Q0c,
S0nc ? S1nc ? Q0c S0nc ? S1nc ? Q0c ? Q1c
start(S0w) ? start(S1w) start(S0w) ? end(S1w), REDUCE-SUBWORD
indict(S1wS0w) ? len(S1wS0w) indict(S1wS0w, S0t) ? len(S1wS0w)
String t?1 ? t0 t?2 ? t?1t0 w?1 ? t0 c0 ? t0 start(w?1) ? t0 c?1 ? c0 ? t?1 ? t0, SHIFT-SEPARATE
features w?1 w?2 ? w?1 w?1,where len(w?1) = 1 end(w?1) ? c0, REDUCE-WORD
start(w?1) ? len(w?1) end(w?1) ? len(w?1) start(w?1) ? end(w?1),
w?1 ? c0 end(w?2) ? w?1 start(w?1) ? c0 end(w?2) ? end(w?1),
w?1 ? len(w?2) w?2 ? len(w?1) w?1 ? t?1 w?1 ? t?2 w?1 ? t?1 ? c0,
w?1 ? t?1 ? end(w?2) c?2 ? c?1 ? c0 ? t?1,where len(w?1) = 1 end(w?1) ? t?1,
c ? t?1 ? end(w?1),where c ? w?1 and c 6= end(w?1)
c0 ? t?1 c?1 ? c0 start(w?1) ? c0t?1 c?1 ? c0 ? t?1 SHIFT-APPEND
Table 1: Feature templates for the character-level parser. The function start(?), end(?) and len(?) denote
the first character, the last character and the length of a word, respectively.
? REDUCE-WORD: pop the top node S0 off S,
pushing a full-word node S?S0 onto S. This re-duce action generates a full-word node from
S0, which must be a subword node.
? REDUCE-BINARY(d, l): pop the top two
nodes S0 and S1 off S, pushing a binary
phrase node S?S1 S0 onto S. The argument ldenotes the constituent label of S?, and the ar-
gument d specifies the lexical head direction
of S?, which can be either ?left? or ?right?.
Both S0 and S1 must be a full-word node or
a phrase node.
? REDUCE-UNARY(l): pop the top node S0
off S, pushing a unary phrase node S?S0 onto
S. l denotes the constituent label of S?.
? TERMINATE: mark parsing complete.
Compared to set of actions in our baseline
transition-based phrase-structure parser, we have
made three major changes. First, we split the orig-
inal SHIFT action into SHIFT-SEPARATE(t)
and SHIFT-APPEND, which jointly perform the
word segmentation and POS tagging tasks. Sec-
ond, we add an extra REDUCE-SUBWORD(d) op-
eration, which is used for parsing the inner struc-
tures of words. Third, we add REDUCE-WORD,
which applies a unary rule to mark a completed
subword node as a full-word node. The new node
corresponds to a unary ?-t? node in Figure 1(b).
3.2 Features
Table 1 shows the feature templates of our model.
The feature set consists of two categories: (1)
structure features, which encode the structural in-
formation of subwords, full-words and phrases.
(2) string features, which encode the information
of neighboring characters and words.
For the structure features, the symbols S0, S1,
S2, S3 represent the top four nodes on the stack;
Q0, Q1, Q2, Q3 denote the first four characters
in the queue; S0l, S0r, S0u represent the left,
right child for a binary branching S0, and the sin-
gle child for a unary branching S0, respectively;
S1l, S1r, S1u represent the left, right child for
a binary branching S1, and the single child for
a unary branching S1, respectively; n represents
the type for a node; it is a binary value that indi-
cates whether the node is a subword node; c, w,
t and l represent the head character, word (or sub-
word), POS tag and constituent label of a node, re-
spectively. The structure features are mostly taken
129
from the work of Zhang and Clark (2009). The
feature templates in bold are novel, are designed
to encode head character information. In particu-
lar, the indict function denotes whether a word is
in a tag dictionary, which is collected by extract-
ing all multi-character subwords that occur more
than five times in the training corpus.
For string features, c0, c?1 and c?2 represent
the current character and its previous two charac-
ters, respectively; w?1 and w?2 represent the pre-
vious two words to the current character, respec-
tively; t0, t?1 and t?2 represent the POS tags of
the current word and the previous two words, re-
spectively. The string features are used for word
segmentation and POS tagging, and are adapted
from a state-of-the-art joint segmentation and tag-
ging model (Zhang and Clark, 2010).
In summary, our character-based parser con-
tains the word-based features of constituent parser
presented in Zhang and Clark (2009), the word-
based and shallow character-based features of
joint word segmentation and POS tagging pre-
sented in Zhang and Clark (2010), and addition-
ally the deep character-based features that encode
word structure information, which are the first pre-
sented by this paper.
4 Experiments
4.1 Setting
We conduct our experiments on the CTB5 cor-
pus, using the standard split of data, with sections
1?270,400?931 and 1001?1151 for training, sec-
tions 301?325 for system development, and sec-
tions 271?300 for testing. We apply the same pre-
processing step as Harper and Huang (2011), so
that the non-terminal yield unary chains are col-
lapsed to single unary rules.
Since our model can jointly process word seg-
mentation, POS tagging and phrase-structure pars-
ing, we evaluate our model for the three tasks, re-
spectively. For word segmentation and POS tag-
ging, standard metrics of word precision, recall
and F-score are used, where the tagging accuracy
is the joint accuracy of word segmentation and
POS tagging. For phrase-structure parsing, we
use the standard parseval evaluation metrics on
bracketing precision, recall and F-score. As our
constituent trees are based on characters, we fol-
low previous work and redefine the boundary of
a constituent span by its start and end characters.
In addition, we evaluate the performance of word
6570
7580
8590
95
0 10 20 30 40
64b16b4b1b
(a) Joint segmentation and
POS tagging F-scores.
3040
5060
7080
90
0 10 20 30 40
64b16b4b1b
(b) Joint constituent parsing
F-scores.
Figure 6: Accuracies against the training epoch
for joint segmentation and tagging as well as joint
phrase-structure parsing using beam sizes 1, 4, 16
and 64, respectively.
structures, using the word precision, recall and F-
score metrics. A word structure is correct only if
the word and its internal structure are both correct.
4.2 Development Results
Figure 6 shows the accuracies of our model using
different beam sizes with respect to the training
epoch. The performance of our model increases
as the beam size increases. The amount of in-
creases becomes smaller as the size of the beam
grows larger. Tested using gcc 4.7.2 and Fedora
17 on an Intel Core i5-3470 CPU (3.20GHz), the
decoding speeds are 318.2, 98.0, 30.3 and 7.9 sen-
tences per second with beam size 1, 4, 16 and 64,
respectively. Based on this experiment, we set the
beam size 64 for the rest of our experiments.
The character-level parsing model has the ad-
vantage that deep character information can be ex-
tracted as features for parsing. For example, the
head character of a word is exploited in our model.
We conduct feature ablation experiments to eval-
uate the effectiveness of these features. We find
that the parsing accuracy decreases about 0.6%
when the head character related features (the bold
feature templates in Table 1) are removed, which
demonstrates the usefulness of these features.
4.3 Final Results
In this section, we present the final results of our
model, and compare it to two baseline systems, a
pipelined system and a joint system that is trained
with automatically generated flat words structures.
The baseline pipelined system consists of the
joint segmentation and tagging model proposed by
130
Task P R F
Pipeline Seg 97.35 98.02 97.69
Tag 93.51 94.15 93.83
Parse 81.58 82.95 82.26
Flat word Seg 97.32 98.13 97.73
structures Tag 94.09 94.88 94.48
Parse 83.39 83.84 83.61
Annotated Seg 97.49 98.18 97.84
word structures Tag 94.46 95.14 94.80
Parse 84.42 84.43 84.43
WS 94.02 94.69 94.35
Table 2: Final results on test corpus.
Zhang and Clark (2010), and the phrase-structure
parsing model of Zhang and Clark (2009). Both
models give state-of-the-art performances, and are
freely available.4 The model for joint segmen-
tation and POS tagging is trained with a 16-
beam, since it achieves the best performance. The
phrase-structure parsing model is trained with a
64-beam. We train the parsing model using the
automatically generated POS tags by 10-way jack-
knifing, which gives about 1.5% increases in pars-
ing accuracy when tested on automatic segmented
and POS tagged inputs.
The joint system trained with flat word struc-
tures serves to test the effectiveness of our joint
parsing system over the pipelined baseline, since
flat word structures do not contain additional
sources of information over the baseline. It is also
used to test the usefulness of our annotation in im-
proving parsing accuracy.
Table 2 shows the final results of our model
and the two baseline systems on the test data.
We can see that both character-level joint mod-
els outperform the pipelined system; our model
with annotated word structures gives an improve-
ment of 0.97% in tagging accuracy and 2.17% in
phrase-structure parsing accuracy. The results also
demonstrate that the annotated word structures are
highly effective for syntactic parsing, giving an ab-
solute improvement of 0.82% in phrase-structure
parsing accuracy over the joint model with flat
word structures.
Row ?WS? in Table 2 shows the accuracy of
hierarchical word-structure recovery of our joint
system. This figure can be useful for high-level ap-
plications that make use of character-level trees by
4http://sourceforge.net/projects/zpar/, version 0.5.
our parser, as it reflects the capability of our parser
in analyzing word structures. In particular, the per-
formance of parsing OOV word structure is an im-
portant metric of our parser. The recall of OOV
word structures is 60.43%, while if we do not con-
sider the influences of segmentation and tagging
errors, counting only the correctly segmented and
tagged words, the recall is 87.96%.
4.4 Comparison with Previous Work
In this section, we compare our model to previous
systems on the performance of joint word segmen-
tation and POS tagging, and the performance of
joint phrase-structure parsing.
Table 3 shows the results. Kruengkrai+ ?09
denotes the results of Kruengkrai et al (2009),
which is a lattice-based joint word segmentation
and POS tagging model; Sun ?11 denotes a sub-
word based stacking model for joint segmenta-
tion and POS tagging (Sun, 2011), which uses a
dictionary of idioms; Wang+ ?11 denotes a semi-
supervised model proposed by Wang et al (2011),
which additionally uses the Chinese Gigaword
Corpus; Li ?11 denotes a generative model that
can perform word segmentation, POS tagging and
phrase-structure parsing jointly (Li, 2011); Li+
?12 denotes a unified dependency parsing model
that can perform joint word segmentation, POS
tagging and dependency parsing (Li and Zhou,
2012); Li ?11 and Li+ ?12 exploited annotated
morphological-level word structures for Chinese;
Hatori+ ?12 denotes an incremental joint model
for word segmentation, POS tagging and depen-
dency parsing (Hatori et al, 2012); they use exter-
nal dictionary resources including HowNet Word
List and page names from the Chinese Wikipedia;
Qian+ ?12 denotes a joint segmentation, POS tag-
ging and parsing system using a unified frame-
work for decoding, incorporating a word segmen-
tation model, a POS tagging model and a phrase-
structure parsing model together (Qian and Liu,
2012); their word segmentation model is a combi-
nation of character-based model and word-based
model. Our model achieved the best performance
on both joint segmentation and tagging as well as
joint phrase-structure parsing.
Our final performance on constituent parsing is
by far the best that we are aware of for the Chinese
data, and even better than some state-of-the-art
models with gold segmentation. For example, the
un-lexicalized PCFG model of Petrov and Klein
131
System Seg Tag Parse
Kruengkrai+ ?09 97.87 93.67 ?
Sun ?11 98.17* 94.02* ?
Wang+ ?11 98.11* 94.18* ?
Li ?11 97.3 93.5 79.7
Li+ ?12 97.50 93.31 ?
Hatori+ ?12 98.26* 94.64* ?
Qian+ ?12 97.96 93.81 82.85
Ours pipeline 97.69 93.83 82.26
Ours joint flat 97.73 94.48 83.61
Ours joint annotated 97.84 94.80 84.43
Table 3: Comparisons of our final model with
state-of-the-art systems, where ?*? denotes that
external dictionary or corpus has been used.
(2007) achieves 83.45%5 in parsing accuracy on
the test corpus, and our pipeline constituent pars-
ing model achieves 83.55% with gold segmenta-
tion. They are lower than the performance of our
character-level model, which is 84.43% without
gold segmentation. The main differences between
word-based and character-level parsing models are
that character-level model can exploit character
features. This further demonstrates the effective-
ness of characters in Chinese parsing.
5 Related Work
Recent work on using the internal structure of
words to help Chinese processing gives impor-
tant motivations to our work. Zhao (2009) stud-
ied character-level dependencies for Chinese word
segmentation by formalizing segmentsion task in
a dependency parsing framework. Their results
demonstrate that annotated word dependencies
can be helpful for word segmentation. Li (2011)
pointed out that the word?s internal structure is
very important for Chinese NLP. They annotated
morphological-level word structures, and a unified
generative model was proposed to parse the Chi-
nese morphological and phrase-structures. Li and
Zhou (2012) also exploited the morphological-
level word structures for Chinese dependency
parsing. They proposed a unified transition-based
model to parse the morphological and depen-
dency structures of a Chinese sentence in a unified
framework. The morphological-level word struc-
5We rerun the parser and evaluate it using the publicly-
available code on http://code.google.com/p/berkeleyparser
by ourselves, since we have a preprocessing step for the
CTB5 corpus.
tures concern only prefixes and suffixes, which
cover only 35% of entire words in CTB. Accord-
ing to their results, the final performances of their
model on word segmentation and POS tagging are
below the state-of-the-art joint segmentation and
POS tagging models. Compared to their work,
we consider the character-level word structures
for Chinese parsing, presenting a unified frame-
work for segmentation, POS tagging and phrase-
structure parsing. We can achieve improved seg-
mentation and tagging performance.
Our character-level parsing model is inspired
by the work of Zhang and Clark (2009), which
is a transition-based model with a beam-search
decoder for word-based constituent parsing. Our
work is based on the shift-reduce operations of
their work, while we introduce additional opera-
tions for segmentation and POS tagging. By such
an extension, our model can include all the fea-
tures in their work, together with the features for
segmentation and POS tagging. In addition, we
propose novel features related to word structures
and interactions between word segmentation, POS
tagging and word-based constituent parsing.
Luo (2003) was the first work to introduce the
character-based syntax parsing. They use it as
a joint framework to perform Chinese word seg-
mentation, POS tagging and syntax parsing. They
exploit a generative maximum entropy model for
character-based constituent parsing, and find that
POS information is very useful for Chinese word
segmentation, but high-level syntactic information
seems to have little effect on segmentation. Com-
pared to their work, we use a transition-based dis-
criminative model, which can benefit from large
amounts of flexible features. In addition, in-
stead of using flat structures, we manually anno-
tate hierarchal tree structures of Chinese words
for converting word-based constituent trees into
character-based constituent trees.
Hatori et al (2012) proposed the first joint work
for the word segmentation, POS tagging and de-
pendency parsing. They used a single transition-
based model to perform the three tasks. Their
work demonstrates that a joint model can improve
the performance of the three tasks, particularly
for POS tagging and dependency parsing. Qian
and Liu (2012) proposed a joint decoder for word
segmentation, POS tagging and word-based con-
stituent parsing, although they trained models for
the three tasks separately. They reported better
132
performances when using a joint decoder. In our
work, we employ a single character-based dis-
criminative model to perform segmentation, POS
tagging and phrase-structure parsing jointly, and
study the influence of annotated word structures.
6 Conclusions and Future Work
We studied the internal structures of more than
37,382 Chinese words, analyzing their structures
as the recursive combinations of characters. Using
these word structures, we extended the CTB into
character-level trees, and developed a character-
based parser that builds such trees from raw char-
acter sequences. Our character-based parser per-
forms segmentation, POS tagging and parsing
simultaneously, and significantly outperforms a
pipelined baseline. We make both our annotations
and our parser available online.
In summary, our contributions include:
? We annotated the internal structures of Chi-
nese words, which are potentially useful
to character-based studies of Chinese NLP.
We extend CTB-style constituent trees into
character-level trees using our annotations.
? We developed a character-based parsing
model that can produce our character-level
constituent trees. Our parser jointly performs
word segmentation, POS tagging and syntac-
tic parsing.
? We investigated the effectiveness of our joint
parser over pipelined baseline, and the effec-
tiveness of our annotated word structures in
improving parsing accuracies.
Future work includes investigations of our
parser and annotations on Chinese NLP tasks.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Major Projects
via grant 2011AA01A207, the National ?863?
Leading Technology Research Project via grant
2012AA011102, and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
References
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Mary Harper and Zhongqiang Huang. 2011. Chinese
statistical parsing. Handbook of Natural Language
Processing and Machine Translation.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045?
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513?521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmen-
tation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1405?1414,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Xiaoqiang Luo. 2003. A maximum entropy Chi-
nese character-based parser. In Michael Collins and
Mark Steedman, editors, Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 192?199.
Jianqiang Ma, Chunyu Kit, and Dale Gerdemann.
2012. Semi-automatic annotation of chinese word
structure. In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language
Processing, pages 9?17, Tianjin, China, December.
Association for Computational Linguistics.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277?284, Barcelona, Spain, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
133
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
501?511, Jeju Island, Korea, July. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385?
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 240?247,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nianwen Xue. 2001. Defining and Automatically
Identifying Words in Chinese. Ph.D. thesis, Univer-
sity of Delaware.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, 8(1).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT?09), pages 162?171, Paris, France, October.
Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843?852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 879?887, Athens, Greece,
March. Association for Computational Linguistics.
134
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199?1209,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Semantic Hierarchies via Word Embeddings
Ruiji Fu
?
, Jiang Guo
?
, Bing Qin
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{rjfu, jguo, bqin, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Semantic hierarchy construction aims to
build structures of concepts linked by
hypernym?hyponym (?is-a?) relations. A
major challenge for this task is the
automatic discovery of such relations.
This paper proposes a novel and effec-
tive method for the construction of se-
mantic hierarchies based on word em-
beddings, which can be used to mea-
sure the semantic relationship between
words. We identify whether a candidate
word pair has hypernym?hyponym rela-
tion by using the word-embedding-based
semantic projections between words and
their hypernyms. Our result, an F-score
of 73.74%, outperforms the state-of-the-
art methods on a manually labeled test
dataset. Moreover, combining our method
with a previous manually-built hierarchy
extension method can further improve F-
score to 80.29%.
1 Introduction
Semantic hierarchies are natural ways to orga-
nize knowledge. They are the main components
of ontologies or semantic thesauri (Miller, 1995;
Suchanek et al, 2008). In the WordNet hierar-
chy, senses are organized according to the ?is-a?
relations. For example, ?dog? and ?canine? are
connected by a directed edge. Here, ?canine? is
called a hypernym of ?dog.? Conversely, ?dog?
is a hyponym of ?canine.? As key sources
of knowledge, semantic thesauri and ontologies
can support many natural language processing
applications. However, these semantic resources
are limited in its scope and domain, and their
manual construction is knowledge intensive and
time consuming. Therefore, many researchers
?
Email correspondence.
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite ???medicinal plant
??medicine
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
Figure 1: An example of semantic hierarchy con-
struction.
have attempted to automatically extract semantic
relations or to construct taxonomies.
A major challenge for this task is the auto-
matic discovery of hypernym-hyponym relations.
Fu et al (2013) propose a distant supervision
method to extract hypernyms for entities from
multiple sources. The output of their model is
a list of hypernyms for a given enity (left pan-
el, Figure 1). However, there usually also exists
hypernym?hyponym relations among these hy-
pernyms. For instance, ??? (plant)? and
???? (Ranunculaceae)? are both hyper-
nyms of the entity ??? (aconit),? and ??
? (plant)? is also a hypernym of ????
(Ranunculaceae).? Given a list of hypernyms
of an entity, our goal in the present work is to
construct a semantic hierarchy of these hypernyms
(right panel, Figure 1).
1
Some previous works extend and refine
manually-built semantic hierarchies by using other
resources (e.g., Wikipedia) (Suchanek et al,
2008). However, the coverage is limited by the
scope of the resources. Several other works relied
heavily on lexical patterns, which would suffer
from deficiency because such patterns can only
cover a small proportion of complex linguistic cir-
cumstances (Hearst, 1992; Snow et al, 2005).
1
In this study, we focus on Chinese semantic hierarchy
construction. The proposed method can be easily adapted to
other languages.
1199
Besides, distributional similarity methods (Kotler-
man et al, 2010; Lenci and Benotto, 2012) are
based on the assumption that a term can only be
used in contexts where its hypernyms can be used
and that a term might be used in any contexts
where its hyponyms are used. However, it is not
always rational. Our previous method based on
web mining (Fu et al, 2013) works well for hy-
pernym extraction of entity names, but it is unsuit-
able for semantic hierarchy construction which in-
volves many words with broad semantics. More-
over, all of these methods do not use the word
semantics effectively.
This paper proposes a novel approach for se-
mantic hierarchy construction based on word em-
beddings. Word embeddings, also known as dis-
tributed word representations, typically represent
words with dense, low-dimensional and real-
valued vectors. Word embeddings have been
empirically shown to preserve linguistic regular-
ities, such as the semantic relationship between
words (Mikolov et al, 2013b). For example,
v(king) ? v(queen) ? v(man) ? v(woman),
where v(w) is the embedding of the word w. We
observe that a similar property also applies to the
hypernym?hyponym relationship (Section 3.3),
which is the main inspiration of the present study.
However, we further observe that hypernym?
hyponym relations are more complicated than a
single offset can represent. To address this chal-
lenge, we propose a more sophisticated and gen-
eral method ? learning a linear projection which
maps words to their hypernyms (Section 3.3.1).
Furthermore, we propose a piecewise linear pro-
jection method based on relation clustering to
better model hypernym?hyponym relations (Sec-
tion 3.3.2). Subsequently, we identify whether an
unknown word pair is a hypernym?hyponym re-
lation using the projections (Section 3.4). To the
best of our knowledge, we are the first to apply
word embeddings to this task.
For evaluation, we manually annotate a dataset
containing 418 Chinese entities and their hyper-
nym hierarchies, which is the first dataset for this
task as far as we know. The experimental results
show that our method achieves an F-score of
73.74% which significantly outperforms the pre-
vious state-of-the-art methods. Moreover, com-
bining our method with the manually-built hier-
archy extension method proposed by Suchanek et
al. (2008) can further improve F-score to 80.29%.
2 Background
As main components of ontologies, semantic hi-
erarchies have been studied by many researchers.
Some have established concept hierarchies based
on manually-built semantic resources such as
WordNet (Miller, 1995). Such hierarchies have
good structures and high accuracy, but their cov-
erage is limited to fine-grained concepts (e.g.,
?Ranunculaceae? is not included in Word-
Net.). We have made similar obsevation that about
a half of hypernym?hyponym relations are absent
in a Chinese semantic thesaurus. Therefore, a
broader range of resources is needed to supple-
ment the manually built resources. In the construc-
tion of the famous ontology YAGO, Suchanek et
al. (2008) link the categories in Wikipedia onto
WordNet. However, the coverage is still limited
by the scope of Wikipedia.
Several other methods are based on lexical
patterns. They use manually or automatically
constructed lexical patterns to mine hypernym?
hyponym relations from text corpora. A hierarchy
can then be built based on these pairwise relations.
The pioneer work by Hearst (1992) has found
out that linking two noun phrases (NPs) via cer-
tain lexical constructions often implies hypernym
relations. For example, NP
1
is a hypernym of NP
2
in the lexical pattern ?such NP
1
as NP
2
.? Snow et
al. (2005) propose to automatically extract large
numbers of lexico-syntactic patterns and subse-
quently detect hypernym relations from a large
newswire corpus. Their method relies on accurate
syntactic parsers, and the quality of the automat-
ically extracted patterns is difficult to guarantee.
Generally speaking, these pattern-based methods
often suffer from low recall or precision because
of the coverage or the quality of the patterns.
The distributional methods assume that the con-
texts of hypernyms are broader than the ones of
their hyponyms. For distributional similarity com-
puting, each word is represented as a semantic
vector composed of the pointwise mutual infor-
mation (PMI) with its contexts. Kotlerman et al
(2010) design a directional distributional measure
to infer hypernym?hyponym relations based on
the standard IR Average Precision evaluation mea-
sure. Lenci and Benotto (2012) propose anoth-
er measure focusing on the contexts that hyper-
nyms do not share with their hyponyms. However,
broader semantics may not always infer broader
contexts. For example, for terms ?Obama? and
1200
?American people?, it is hard to say whose
contexts are broader.
Our previous work (Fu et al, 2013) applies a
web mining method to discover the hypernyms of
Chinese entities from multiple sources. We as-
sume that the hypernyms of an entity co-occur
with it frequently. It works well for named enti-
ties. But for class names (e.g., singers in Hong
Kong, tropical fruits) with wider range of mean-
ings, this assumption may fail.
In this paper, we aim to identify hypernym?
hyponym relations using word embeddings, which
have been shown to preserve good properties for
capturing semantic relationship between words.
3 Method
In this section, we first define the task formally.
Then we elaborate on our proposed method com-
posed of three major steps, namely, word embed-
ding training, projection learning, and hypernym?
hyponym relation identification.
3.1 Task Definition
Given a list of hypernyms of an entity, our goal is
to construct a semantic hierarchy on it (Figure 1).
We represent the hierarchy as a directed graph
G, in which the nodes denote the words, and the
edges denote the hypernym?hyponym relations.
Hypernym-hyponym relations are asymmetric and
transitive when words are unambiguous:
? ?x, y ? L : x
H
??y ? ?(y
H
??x)
? ?x, y, z ? L : (x
H
??z ? z
H
??y)? x
H
??y
Here, L denotes the list of hypernyms. x, y and
z denote the hypernyms in L. We use
H
?? to
represent a hypernym?hyponym relation in this
paper. Actually, x, y and z are unambiguous as
the hypernyms of a certain entity. Therefore, G
should be a directed acyclic graph (DAG).
3.2 Word Embedding Training
Various models for learning word embeddings
have been proposed, including neural net lan-
guage models (Bengio et al, 2003; Mnih and
Hinton, 2008; Mikolov et al, 2013b) and spec-
tral models (Dhillon et al, 2011). More recent-
ly, Mikolov et al (2013a) propose two log-linear
models, namely the Skip-gram and CBOW model,
to efficiently induce word embeddings. These two
models can be trained very efficiently on a large-
scale corpus because of their low time complexity.
No. Examples
1
v(?)? v(??) ? v(?)? v(??)
v(shrimp)? v(prawn) ? v(fish)? v(gold fish)
2
v(??)? v(??) ? v(??)? v(??)
v(laborer)? v(carpenter) ? v(actor)? v(clown)
3
v(??)? v(??) 6? v(?)? v(??)
v(laborer)? v(carpenter) 6? v(fish)? v(gold fish)
Table 1: Embedding offsets on a sample of
hypernym?hyponym word pairs.
Additionally, their experiment results have shown
that the Skip-gram model performs best in identi-
fying semantic relationship among words. There-
fore, we employ the Skip-gram model for estimat-
ing word embeddings in this study.
The Skip-gram model adopts log-linear classi-
fiers to predict context words given the current
word w(t) as input. First, w(t) is projected to its
embedding. Then, log-linear classifiers are em-
ployed, taking the embedding as input and pre-
dict w(t)?s context words within a certain range,
e.g. k words in the left and k words in the
right. After maximizing the log-likelihood over
the entire dataset using stochastic gradient descent
(SGD), the embeddings are learned.
3.3 Projection Learning
Mikolov et al (2013b) observe that word em-
beddings preserve interesting linguistic regulari-
ties, capturing a considerable amount of syntac-
tic/semantic relations. Looking at the well-known
example: v(king) ? v(queen) ? v(man) ?
v(woman), it indicates that the embedding offsets
indeed represent the shared semantic relation be-
tween the two word pairs.
We observe that the same property also ap-
plies to some hypernym?hyponym relations. As
a preliminary experiment, we compute the em-
bedding offsets between some randomly sampled
hypernym?hyponym word pairs and measure their
similarities. The results are shown in Table 1.
The first two examples imply that a word can
also be mapped to its hypernym by utilizing word
embedding offsets. However, the offset from
?carpenter? to ?laborer? is distant from
the one from ?gold fish? to ?fish,? indicat-
ing that hypernym?hyponym relations should be
more complicated than a single vector offset can
represent. To verify this hypothesis, we com-
pute the embedding offsets over all hypernym?
1201
???-????sportsman - footballer ??-???staff - civil servant??-??laborer - gardener??-???seaman - navigator??-??actor - singer ??-??actor - protagonist??-??actor - clown
??-??position - headmaster
??-???actor - matador
??-???laborer - temporary worker ??-??laborer - carpenter ??-???position ? consul general
??-??staff - airline hostess??-???staff - salesclerk??-???staff - conductor?-??chicken - cock?-????sheep - small-tail Han sheep?-??sheep - ram ?-??equus - zebra ?-??shrimp - prawn
?-??dog - police dog?-???rabbit - wool rabbit
??-???dolphin - white-flag dolphin ?-??fish - shark ?-???fish - tropical fish?-??fish - gold fish
?-??crab - sea crab
?-??donkey - wild ass
Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute
in some clusters. The left cluster shows some hypernym?hyponym relations about animals. The right
one shows some relations about people?s occupations.
hyponym word pairs in our training data and vi-
sualize them.
2
Figure 2 shows that the relations
are adequately distributed in the clusters, which
implies that hypernym?hyponym relations in-
deed can be decomposed into more fine-grained
relations. Moreover, the relations about animals
are spatially close, but separate from the relations
about people?s occupations.
To address this challenge, we propose to learn
the hypernym?hyponym relations using projection
matrices.
3.3.1 A Uniform Linear Projection
Intuitively, we assume that all words can be pro-
jected to their hypernyms based on a uniform tran-
sition matrix. That is, given a word x and its hy-
pernym y, there exists a matrix ? so that y = ?x.
For simplicity, we use the same symbols as the
words to represent the embedding vectors. Ob-
taining a consistent exact ? for the projection of
all hypernym?hyponym pairs is difficult. Instead,
we can learn an approximate ? using Equation 1
on the training data, which minimizes the mean-
squared error:
?
?
= arg min
?
1
N
?
(x,y)
? ?x? y ?
2
(1)
where N is the number of (x, y) word pairs in
the training data. This is a typical linear regres-
sion problem. The only difference is that our pre-
dictions are multi-dimensional vectors instead of
scalar values. We use SGD for optimization.
2
Principal Component Analysis (PCA) is applied for di-
mensionality reduction.
3.3.2 Piecewise Linear Projections
A uniform linear projection may still be under-
representative for fitting all of the hypernym?
hyponym word pairs, because the relations are
rather diverse, as shown in Figure 2. To better
model the various kinds of hypernym?hyponym
relations, we apply the idea of piecewise linear re-
gression (Ritzema, 1994) in this study.
Specifically, the input space is first segmented
into several regions. That is, all word pairs (x, y)
in the training data are first clustered into sever-
al groups, where word pairs in each group are
expected to exhibit similar hypernym?hyponym
relations. Each word pair (x, y) is represented
with their vector offsets: y ? x for clustering.
The reasons are twofold: (1) Mikolov?s work has
shown that the vector offsets imply a certain lev-
el of semantic relationship. (2) The vector off-
sets distribute in clusters well, and the word pairs
which are close indeed represent similar relations,
as shown in Figure 2.
Then we learn a separate projection for each
cluster, respectively (Equation 2).
?
?
k
= arg min
?
k
1
N
k
?
(x,y)?C
k
? ?
k
x? y ?
2
(2)
where N
k
is the amount of word pairs in the k
th
cluster C
k
.
We use the k-means algorithm for clustering,
where k is tuned on a development dataset.
3.3.3 Training Data
To learn the projection matrices, we extract train-
ing data from a Chinese semantic thesaurus,
Tongyi Cilin (Extended) (CilinE for short) which
1202
?
?
?
?
?
Root
L ev el 1
L ev el 2
L ev el 3
L ev el 4
L ev el 5
?  ob j ect
??  animal
??  insect
- -
??  dragonf ly
B
i
18
A
06@
?? : ?? 
( dragonf ly  :  animal)
?? : ?? 
( dragonf ly  :  insect)
C ilinEh y perny m-h y pony m pairs
S ense C ode:  B i1 8 A0 6 @
S ense C ode:  B i1 8 A
S ense C ode:  B i1 8
S ense C ode:  B i
S ense C ode:  B
?? : ?? 
( insect :  animal)
Figure 3: Hierarchy of CilinE and an Example of
Training Data Generation
contains 100,093 words (Che et al, 2010).
3
CilinE
is organized as a hierarchy of five levels, in which
the words are linked by hypernym?hyponym
relations (right panel, Figure 3). Each word in
CilinE has one or more sense codes (some words
are polysemous) that indicate its position in the hi-
erarchy.
The senses of words in the first level, such as
?? (object)? and ??? (time),? are very gen-
eral. The fourth level only has sense codes without
real words. Therefore, we extract words in the sec-
ond, third and fifth levels to constitute hypernym?
hyponym pairs (left panel, Figure 3).
Note that mapping one hyponym to multi-
ple hypernyms with the same projection (?x is
unique) is difficult. Therefore, the pairs with the
same hyponym but different hypernyms are ex-
pected to be clustered into separate groups. Fig-
ure 3 shows that the word ?dragonfly? in the
fifth level has two hypernyms: ?insect? in the
third level and ?animal? in the second level.
Hence the relations dragonfly
H
?? insect and
dragonfly
H
?? animal should fall into differ-
ent clusters.
In our implementation, we apply this constraint
by simply dividing the training data into two cat-
egories, namely, direct and indirect. Hypernym-
hyponym word pair (x, y) is classified into the di-
rect category, only if there doesn?t exist another
word z in the training data, which is a hypernym of
x and a hyponym of y. Otherwise, (x, y) is classi-
fied into the indirect category. Then, data in these
two categories are clustered separately.
3
www.ltp-cloud.com/download/
x
y
?k
? 
x'
?l
Figure 4: In this example, ?
k
x is located in the
circle with center y and radius ?. So y is consid-
ered as a hypernym of x. Conversely, y is not a
hypernym of x
?
.
x
y
z
x
y
(a) (b)
z
x
y
Figure 5: (a) If d(?
j
y, x) > d(?
k
x, y), we re-
move the path from y to x; (b) if d(?
j
y, x) >
d(?
k
x, z) and d(?
j
y, x) > d(?
i
z, y), we reverse
the path from y to x.
3.4 Hypernym-hyponym Relation
Identification
Upon obtaining the clusters of training data and
the corresponding projections, we can identify
whether two words have a hypernym?hyponym re-
lation. Given two words x and y, we find cluster
C
k
whose center is closest to the offset y ? x, and
obtain the corresponding projection ?
k
. For y to
be considered a hypernym of x, one of the two
conditions below must hold.
Condition 1: The projection ?
k
puts ?
k
x close
enough to y (Figure 4). Formally, the euclidean
distance between ?
k
x and y: d(?
k
x, y) must be
less than a threshold ?.
d(?
k
x, y) =? ?
k
x? y ?
2
< ? (3)
Condition 2: There exists another word z sat-
isfying x
H
??z and z
H
??y. In this case, we use the
transitivity of hypernym?hyponym relations.
Besides, the final hierarchy should be a DAG
as discussed in Section 3.1. However, the pro-
jection method cannot guarantee that theoretical-
ly, because the projections are learned from pair-
wise hypernym?hyponym relations without the w-
hole hierarchy structure. All pairwise hypernym?
hyponym relation identification methods would
suffer from this problem actually. It is an inter-
esting problem how to construct a globally opti-
1203
mal semantic hierarchy conforming to the form
of a DAG. But this is not the focus of this paper.
So if some conflicts occur, that is, a relation cir-
cle exists, we remove or reverse the weakest path
heuristically (Figure 5). If a circle has only two
nodes, we remove the weakest path. If a circle has
more than two nodes, we reverse the weakest path
to form an indirect hypernym?hyponym relation.
4 Experimental Setup
4.1 Experimental Data
In this work, we learn word embeddings from a
Chinese encyclopedia corpus named Baidubaike
4
,
which contains about 30 million sentences (about
780 million words). The Chinese segmentation
is provided by the open-source Chinese language
processing platform LTP
5
(Che et al, 2010).
Then, we employ the Skip-gram method (Section
3.2) to train word embeddings. Finally we obtain
the embedding vectors of 0.56 million words.
The training data for projection learning is
collected from CilinE (Section 3.3.3). We ob-
tain 15,247 word pairs of hypernym?hyponym
relations (9,288 for direct relations and 5,959 for
indirect relations).
For evaluation, we collect the hypernyms for
418 entities, which are selected randomly from
Baidubaike, following Fu et al (2013). We then
ask two annotators to manually label the seman-
tic hierarchies of the correct hypernyms. The final
data set contains 655 unique hypernyms and 1,391
hypernym?hyponym relations among them. We
randomly split the labeled data into 1/5 for de-
velopment and 4/5 for testing (Table 2). The hi-
erarchies are represented as relations of pairwise
words. We measure the inter-annotator agreement
using the kappa coefficient (Siegel and Castel-
lan Jr, 1988). The kappa value is 0.96, which indi-
cates a good strength of agreement.
4.2 Evaluation Metrics
We use precision, recall, and F-score as our met-
rics to evaluate the performances of the methods.
Since hypernym?hyponym relations and its re-
verse (hyponym?hypernym) have one-to-one cor-
respondence, their performances are equal. For
4
Baidubaike (baike.baidu.com) is one of the largest
Chinese encyclopedias containing more than 7.05 million en-
tries as of September, 2013.
5
www.ltp-cloud.com/demo/
Relation
# of word pairs
Dev. Test
hypernym?hyponym 312 1,079
hyponym?hypernym
?
312 1,079
unrelated 1,044 3,250
Total 1,668 5,408
Table 2: The evaluation data.
?
Since hypernym?
hyponym relations and hyponym?hypernym
relations have one-to-one correspondence, their
numbers are the same.
1 5 
10 15 
20 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
1 10 20 30 40 50 60 Indirect 
F1-
Sco
re 
Direct 
Figure 6: Performance on development data w.r.t.
cluster size.
simplicity, we only report the performance of the
former in the experiments.
5 Results and Analysis
5.1 Varying the Amount of Clusters
We first evaluate the effect of different number of
clusters based on the development data. We vary
the numbers of the clusters both for the direct and
indirect training word pairs.
As shown in Figure 6, the performance of clus-
tering is better than non-clustering (when the clus-
ter number is 1), thus providing evidences that
learning piecewise projections based on clustering
is reasonable. We finally set the numbers of the
clusters of direct and indirect to 20 and 5, respec-
tively, where the best performances are achieved
on the development data.
5.2 Comparison with Previous Work
In this section, we compare the proposed method
with previous methods, including manually-built
hierarchy extension, pairwise relation extraction
1204
P(%) R(%) F(%)
M
Wiki+CilinE
92.41 60.61 73.20
M
Pattern
97.47 21.41 35.11
M
Snow
60.88 25.67 36.11
M
balApinc
54.96 53.38 54.16
M
invCL
49.63 62.84 55.46
M
Fu
87.40 48.19 62.13
M
Emb
80.54 67.99 73.74
M
Emb+CilinE
80.59 72.42 76.29
M
Emb+Wiki+CilinE
79.78 80.81 80.29
Table 3: Comparison of the proposed method with
existing methods in the test set.
Pattern Translation
w?[??|??] h w is a [a kind of] h
w [?]? h w[,] and other h
h [?]?[?] w h[,] called w
h [?] [?]? w h[,] such as w
h [?]??? w h[,] especially w
Table 4: Chinese Hearst-style lexical patterns. The
contents in square brackets are omissible.
based on patterns, word distributions, and web
mining (Section 2). Results are shown in Table 3.
5.2.1 Overall Comparison
M
Wiki+CilinE
refers to the manually-built hierar-
chy extension method of Suchanek et al (2008).
In our experiment, we use the category taxonomy
of Chinese Wikipedia
6
to extend CilinE. Table 3
shows that this method achieves a high precision
but also a low recall, mainly because of the limit-
ed scope of Wikipedia.
M
Pattern
refers to the pattern-based method of
Hearst (1992). We extract hypernym?hyponym
relations in the Baidubaike corpus, which is al-
so used to train word embeddings (Section 4.1).
We use the Chinese Hearst-style patterns (Table
4) proposed by Fu et al (2013), in which w rep-
resents a word, and h represents one of its hy-
pernyms. The result shows that only a small part
of the hypernyms can be extracted based on these
patterns because only a few hypernym relations
are expressed in these fixed patterns, and many are
expressed in highly flexible manners.
In the same corpus, we apply the method
M
Snow
originally proposed by Snow et al (2005).
The same training data for projections learn-
6
dumps.wikimedia.org/zhwiki/20131205/
ing from CilinE (Section 3.3.3) is used as
seed hypernym?hyponym pairs. Lexico-syntactic
patterns are extracted from the Baidubaike corpus
by using the seeds. We then develop a logistic re-
gression classifier based on the patterns to recog-
nize hypernym?hyponym relations. This method
relies on an accurate syntactic parser, and the qual-
ity of the automatically extracted patterns is diffi-
cult to guarantee.
We re-implement two previous distribution-
al methods M
balApinc
(Kotlerman et al, 2010)
and M
invCL
(Lenci and Benotto, 2012) in the
Baidubaike corpus. Each word is represented as a
feature vector in which each dimension is the PMI
value of the word and its context words. We com-
pute a score for each word pair and apply a thresh-
old to identify whether it is a hypernym?hyponym
relation.
M
Fu
refers to our previous web mining
method (Fu et al, 2013). This method mines hy-
pernyms of a given word w from multiple sources
and returns a ranked list of the hypernyms. We
select the hypernyms with scores over a threshold
of each word in the test set for evaluation. This
method assumes that frequent co-occurrence of a
noun or noun phrase n in multiple sources with w
indicate possibility of n being a hypernym of w.
The results presented in Fu et al (2013) show that
the method works well when w is an entity, but
not when w is a word with a common semantic
concept. The main reason may be that there are
relatively more introductory pages about entities
than about common words in the Web.
M
Emb
is the proposed method based on word
embeddings. Table 3 shows that the proposed
method achieves a better recall and F-score than
all of the previous methods do. It can significantly
(p < 0.01) improve the F-score over the state-of-
the-art method M
Wiki+CilinE
.
M
Emb
and M
CilinE
can also be combined. The
combination strategy is to simply merge all pos-
itive results from the two methods together, and
then to infer new relations based on the transitiv-
ity of hypernym?hyponym relations. The F-score
is further improved from 73.74% to 76.29%. Note
that, the combined method achieves a 4.43% re-
call improvement over M
Emb
, but the precision is
almost unchanged. The reason is that the infer-
ence based on the relations identified automatical-
ly may lead to error propagation. For example, the
relation x
H
??y is incorrectly identified by M
Emb
.
1205
P(%) R(%) F(%)
M
Wiki+CilinE
80.39 19.29 31.12
M
Emb+CilinE
71.16 52.80 60.62
M
Emb+Wiki+CilinE
69.13 61.65 65.17
Table 5: Performance on the out-of-CilinE data in
the test set.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
l l lllllll
l
l
l ll
l MEmb+Wiki+Ci l inEMEmb+Ci l inEMWiki+Ci l inE
Figure 7: Precision-Recall curves on the out-of-
CilinE data in the test set.
When the relation y
H
??z from M
CilinE
is added, it
will cause a new incorrect relation x
H
??z.
Combining M
Emb
with M
Wiki+CilinE
achieves
a 7% F-score improvement over the best baseline
M
Wiki+CilinE
. Therefore, the proposed method
is complementary to the manually-built hierarchy
extension method (Suchanek et al, 2008).
5.2.2 Comparison on the Out-of-CilinE Data
We are greatly interested in the practical perfor-
mance of the proposed method on the hypernym?
hyponym relations outside of CilinE. We say a
word pair is outside of CilinE, as long as there
is one word in the pair not existing in CilinE. In
our test data, about 62% word pairs are outside
of CilinE. Table 5 shows the performances of the
best baseline method and our method on the out-
of-CilinE data. The method exploiting the tax-
onomy in Wikipedia, M
Wiki+CilinE
, achieves the
highest precision but has a low recall. By con-
trast, our method can discover more hypernym?
hyponym relations with some loss of precision,
thereby achieving a more than 29% F-score im-
provement. The combination of these two meth-
ods achieves a further 4.5% F-score improvement
over M
Emb+CilinE
. Generally speaking, the pro-
posed method greatly improves the recall but dam-
ages the precision.
Actually, we can get different precisions and re-
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( a)  C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( b )  W ik ipedia+ C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( c)  E mb edding
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( d)  E mb edding+ W ik ipedia+ C ilinE
Figure 8: An example for error analysis. The
red paths refer to the relations between the named
entity and its hypernyms extracted using the web
mining method (Fu et al, 2013). The black paths
with hollow arrows denote the relations identified
by the different methods. The boxes with dotted
borders refer to the concepts which are not linked
to correct positions.
calls by adjusting the threshold ? (Equation 3).
Figure 7 shows that M
Emb+CilinE
achieves a high-
er precision than M
Wiki+CilinE
when their recalls
are the same. When they achieve the same preci-
sion, the recall of M
Emb+CilinE
is higher.
5.3 Error Analysis and Discussion
We analyze error cases after experiments. Some
cases are shown in Figure 8. We can see that
there is only one general relation ??? (plant)?
H
?? ??? (organism)? existing in CilinE. Some
fine-grained relations exist in Wikipedia, but the
coverage is limited. Our method based on
word embeddings can discover more hypernym?
hyponym relations than the previous methods can.
When we combine the methods together, we get
the correct hierarchy.
Figure 8 shows that our method loses the
relation ???? (Aconitum)? H?? ????
(Ranunculaceae).? It is because they are
very semantically similar (their cosine similarity
is 0.9038). Their representations are so close to
each other in the embedding space that we have
not find projections suitable for these pairs. The
1206
error statistics show that when the cosine similari-
ties of word pairs are greater than 0.8, the recall is
only 9.5%. This kind of error accounted for about
10.9% among all the errors in our test set. One
possible solution may be adding more data of this
kind to the training set.
6 Related Work
In addition to the works mentioned in Section 2,
we introduce another set of related studies in this
section.
Evans (2004), Ortega-Mendoza et al (2007),
and Sang (2007) consider web data as a large cor-
pus and use search engines to identify hypernyms
based on the lexical patterns of Hearst (1992).
However, the low quality of the sentences in the
search results negatively influence the precision of
hypernym extraction.
Following the method for discovering patterns
automatically (Snow et al, 2005), McNamee et
al. (2008) apply the same method to extract hy-
pernyms of entities in order to improve the perfor-
mance of a question answering system. Ritter et al
(2009) propose a method based on patterns to find
hypernyms on arbitrary noun phrases. They use
a support vector machine classifier to identify the
correct hypernyms from the candidates that match
the patterns. As our experiments show, pattern-
based methods suffer from low recall because of
the low coverage of patterns.
Besides Kotlerman et al (2010) and Lenci and
Benotto (2012), other researchers also propose di-
rectional distributional similarity methods (Weeds
et al, 2004; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor et al, 2007; Clarke, 2009). How-
ever, their basic assumption that a hyponym can
only be used in contexts where its hypernyms can
be used and that a hypernym might be used in all
of the contexts where its hyponyms are used may
not always rational.
Snow et al (2006) provides a global optimiza-
tion scheme for extending WordNet, which is d-
ifferent from the above-mentioned pairwise rela-
tionships identification methods.
Word embeddings have been successfully ap-
plied in many applications, such as in sentiment
analysis (Socher et al, 2011b), paraphrase detec-
tion (Socher et al, 2011a), chunking, and named
entity recognition (Turian et al, 2010; Collobert
et al, 2011). These applications mainly utilize
the representing power of word embeddings to al-
leviate the problem of data sparsity. Mikolov et
al. (2013a) and Mikolov et al (2013b) further ob-
serve that the semantic relationship of words can
be induced by performing simple algebraic oper-
ations with word vectors. Their work indicates
that word embeddings preserve some interesting
linguistic regularities, which might provide sup-
port for many applications. In this paper, we
improve on their work by learning multiple lin-
ear projections in the embedding space, to model
hypernym?hyponym relationships within different
clusters.
7 Conclusion and Future Work
This paper proposes a novel method for seman-
tic hierarchy construction based on word em-
beddings, which are trained using a large-scale
corpus. Using the word embeddings, we learn
the hypernym?hyponym relationship by estimat-
ing projection matrices which map words to their
hypernyms. Further improvements are made us-
ing a cluster-based approach in order to model
the more fine-grained relations. Then we propose
a few simple criteria to identity whether a new
word pair is a hypernym?hyponym relation. Based
on the pairwise hypernym?hyponym relations, we
build semantic hierarchies automatically.
In our experiments, the proposed method signif-
icantly outperforms state-of-the-art methods and
achieves the best F1-score of 73.74% on a manual-
ly labeled test dataset. Further experiments show
that our method is complementary to the previous
manually-built hierarchy extension methods.
For future work, we aim to improve word
embedding learning under the guidance of
hypernym?hyponym relations. By including the
hypernym?hyponym relation constraints while
training word embeddings, we expect to improve
the embeddings such that they become more suit-
able for this task.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61273321 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Shiqi Zhao,
Zhenghua Li, Wei Song and the anonymous re-
viewers for insightful comments and suggestions.
We also thank Xinwei Geng and Hongbo Cai for
their help in the experiments.
1207
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorith-
m for learning directionality of inference rules. In
EMNLP-CoNLL, pages 161?170.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, Chi-
na, August.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112?119. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Richard Evans. 2004. A framework for named entity
recognition in the open domain. Recent Advances in
Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting
multiple sources for open-domain hypernym discov-
ery. In EMNLP, pages 1224?1234.
Maayan Geffet and Ido Dagan. 2005. The distribution-
al inclusion hypotheses and lexical entailment. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 107?114.
Association for Computational Linguistics.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribution-
al similarity for lexical inference. Natural Language
Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 75?79. Association for
Computational Linguistics.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the
Third International Joint Conference on Natural
Language Processing, pages 799?804.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Rosa M Ortega-Mendoza, Luis Villase?nor-Pineda, and
Manuel Montes-y G?omez. 2007. Using lexical
patterns for extracting hyponyms from the web. In
MICAI 2007: Advances in Artificial Intelligence,
pages 904?911. Springer.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
HP Ritzema. 1994. Drainage principles and
applications.
Erik Tjong Kim Sang. 2007. Extracting hypernym
pairs from the web. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 165?168. Associa-
tion for Computational Linguistics.
Sidney Siegel and N John Castellan Jr. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computation-
al Linguistics, pages 801?808, Sydney, Australia,
July. Association for Computational Linguistics.
1208
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Ng. 2011a. Dynam-
ic pooling and unfolding recursive autoencoders for
paraphrase detection. In Advances in Neural Infor-
mation Processing Systems, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th internation-
al conference on Computational Linguistics, page
1015. Association for Computational Linguistics.
1209
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326?1336,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Character-Level Chinese Dependency Parsing
Meishan Zhang
?
, Yue Zhang
?
, Wanxiang Che
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
Recent work on Chinese analysis has led
to large-scale annotations of the internal
structures of words, enabling character-
level analysis of Chinese syntactic struc-
tures. In this paper, we investigate the
problem of character-level Chinese depen-
dency parsing, building dependency trees
over characters. Character-level infor-
mation can benefit downstream applica-
tions by offering flexible granularities for
word segmentation while improving word-
level dependency parsing accuracies. We
present novel adaptations of two ma-
jor shift-reduce dependency parsing algo-
rithms to character-level parsing. Exper-
imental results on the Chinese Treebank
demonstrate improved performances over
word-based parsing methods.
1 Introduction
As a light-weight formalism offering syntactic
information to downstream applications such as
SMT, the dependency grammar has received in-
creasing interest in the syntax parsing commu-
nity (McDonald et al, 2005; Nivre and Nilsson,
2005; Carreras et al, 2006; Duan et al, 2007; Koo
and Collins, 2010; Zhang and Clark, 2008; Nivre,
2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi
and McCallum, 2013). Chinese dependency trees
were conventionally defined over words (Chang et
al., 2009; Li et al, 2012), requiring word segmen-
tation and POS-tagging as pre-processing steps.
Recent work on Chinese analysis has embarked
on investigating the syntactic roles of characters,
leading to large-scale annotations of word internal
structures (Li, 2011; Zhang et al, 2013). Such an-
notations enable dependency parsing on the char-
acter level, building dependency trees over Chi-
nese characters. Figure 1(c) shows an example of
?
Corresponding author.
??? ??? ? ? ??
forestry administration deputy director meeting in make a speech
(a) a word-based dependency tree
? ? ? ? ? ? ? ? ? ?
woods industry office deputy office manager meeting in make speech
(b) a character-level dependency tree by Zhao (2009) with
real intra-word and pseudo inter-word dependencies
? ? ? ? ? ? ? ? ? ?
woods industry office deputy office manager meeting in make speech
(c) a character-level dependency tree investigated in this pa-
per with both real intra- and inter-word dependencies
Figure 1: An example character-level dependency
tree. ????????????? (The deputy
director of forestry administration make a speech
in the meeting)?.
a character-level dependency tree, where the leaf
nodes are Chinese characters.
Character-level dependency parsing is interest-
ing in at least two aspects. First, character-level
trees circumvent the issue that no universal stan-
dard exists for Chinese word segmentation. In the
well-known Chinese word segmentation bakeoff
tasks, for example, different segmentation stan-
dards have been used by different data sets (Emer-
son, 2005). On the other hand, most disagreement
on segmentation standards boils down to disagree-
ment on segmentation granularity. As demon-
strated by Zhao (2009), one can extract both fine-
grained and coarse-grained words from character-
level dependency trees, and hence can adapt to
flexible segmentation standards using this formal-
ism. In Figure 1(c), for example, ???? (deputy
1326
director)? can be segmented as both ?? (deputy)
| ?? (director)? and ???? (deputy direc-
tor)?, but not ?? (deputy) ? (office) | ? (man-
ager)?, by dependency coherence. Chinese lan-
guage processing tasks, such as machine transla-
tion, can benefit from flexible segmentation stan-
dards (Zhang et al, 2008; Chang et al, 2008).
Second, word internal structures can also be
useful for syntactic parsing. Zhang et al (2013)
have shown the usefulness of word structures in
Chinese constituent parsing. Their results on the
Chinese Treebank (CTB) showed that character-
level constituent parsing can bring increased per-
formances even with the pseudo word structures.
They further showed that better performances can
be achieved when manually annotated word struc-
tures are used instead of pseudo structures.
In this paper, we make an investigation of
character-level Chinese dependency parsing using
Zhang et al (2013)?s annotations and based on
a transition-based parsing framework (Zhang and
Clark, 2011). There are two dominant transition-
based dependency parsing systems, namely the
arc-standard and the arc-eager parsers (Nivre,
2008). We study both algorithms for character-
level dependency parsing in order to make a com-
prehensive investigation. For direct comparison
with word-based parsers, we incorporate the tra-
ditional word segmentation, POS-tagging and de-
pendency parsing stages in our joint parsing mod-
els. We make changes to the original transition
systems, and arrive at two novel transition-based
character-level parsers.
We conduct experiments on three data sets, in-
cluding CTB 5.0, CTB 6.0 and CTB 7.0. Exper-
imental results show that the character-level de-
pendency parsing models outperform the word-
based methods on all the data sets. Moreover,
manually annotated intra-word dependencies can
give improved word-level dependency accuracies
than pseudo intra-word dependencies. These re-
sults confirm the usefulness of character-level
syntax for Chinese analysis. The source codes
are freely available at http://sourceforge.
net/projects/zpar/, version 0.7.
2 Character-Level Dependency Tree
Character-level dependencies were first proposed
by Zhao (2009). They show that by annotat-
ing character dependencies within words, one can
adapt to different segmentation standards. The
dependencies they study are restricted to intra-
word characters, as illustrated in Figure 1(b). For
inter-word dependencies, they use a pseudo right-
headed representation.
In this study, we integrate inter-word syntactic
dependencies and intra-word dependencies using
large-scale annotations of word internal structures
by Zhang et al (2013), and study their interac-
tions. We extract unlabeled dependencies from
bracketed word structures according to Zhang et
al.?s head annotations. In Figure 1(c), the depen-
dencies shown by dashed arcs are intra-word de-
pendencies, which reflect the internal word struc-
tures, while the dependencies with solid arcs are
inter-word dependencies, which reflect the syntac-
tic structures between words.
In this formulation, a character-level depen-
dency tree satisfies the same constraints as the
traditional word-based dependency tree for Chi-
nese, including projectivity. We differentiate intra-
word dependencies and inter-word dependencies
by the arc type, so that our work can be com-
pared with conventional word segmentation, POS-
tagging and dependency parsing pipelines under a
canonical segmentation standard.
The character-level dependency trees hold to a
specific word segmentation standard, but are not
limited to it. We can extract finer-grained words
of different granulities from a coarse-grained word
by taking projective subtrees of different sizes. For
example, taking all the intra-word modifier nodes
of ?? (manager)? in Figure 1(c) results in the
word ???? (deputy director)?, while taking the
first modifier node of ?? (manager)? results in the
word ??? (director)?. Note that ??? (deputy
office)? cannot be a word because it does not form
a projective span without ?? (manager)?.
Inner-word dependencies can also bring bene-
fits to parsing word-level dependencies. The head
character can be a less sparse feature compared
to a word. As intra-word dependencies lead to
fine-grained subwords, we can also use these sub-
words for better parsing. In this work, we use
the innermost left/right subwords as atomic fea-
tures. To extract the subwords, we find the inner-
most left/right modifiers of the head character, re-
spectively, and then conjoin them with all their de-
scendant characters to form the smallest left/right
subwords. Figure 2 shows an example, where the
smallest left subword of ???? (chief lawyer)?
is ??? (lawyer)?, and the smallest right subword
1327
? ? ?
big law officer
(a) smallest left subword
? ? ?
agree with law ize
(b) smallest right subword
Figure 2: An example to illustrate the innermost
left/right subwords.
of ???? (legalize)? is ??? (legal)?.
3 Character-Level Dependency Parsing
A transition-based framework with global learn-
ing and beam search decoding (Zhang and Clark,
2011) has been applied to a number of natural lan-
guage processing tasks, including word segmen-
tation, POS-tagging and syntactic parsing (Zhang
and Clark, 2010; Huang and Sagae, 2010; Bohnet
and Nivre, 2012; Zhang et al, 2013). It models
a task incrementally from a start state to an end
state, where each intermediate state during decod-
ing can be regarded as a partial output. A num-
ber of actions are defined so that the state ad-
vances step by step. To learn the model param-
eters, it usually uses the online perceptron algo-
rithm with early-update under the inexact decod-
ing condition (Collins, 2002; Collins and Roark,
2004). Transition-based dependency parsing can
be modeled under this framework, where the state
consists of a stack and a queue, and the set of ac-
tions can be either the arc-eager (Zhang and Clark,
2008) or the arc-standard (Huang et al, 2009)
transition systems.
When the internal structures of words are an-
notated, character-level dependency parsing can
be treated as a special case of word-level depen-
dency parsing, with ?words? being ?characters?.
A big weakness of this approach is that full words
and POS-tags cannot be used for feature engineer-
ing. Both are crucial to well-established features
for word segmentation, POS-tagging and syntactic
parsing. In this section, we introduce novel exten-
sions to the arc-standard and the arc-eager tran-
sition systems, so that word-based and character-
based features can be used simultaneously for
character-level dependency parsing.
3.1 The Arc-Standard Model
The arc-standard model has been applied to joint
segmentation, POS-tagging and dependency pars-
ing (Hatori et al, 2012), but with pseudo word
structures. For unified processing of annotated
word structures and fair comparison between
character-level arc-eager and arc-standard sys-
tems, we define a different arc-standard transition
system, consistent with our character-level arc-
eager system.
In the word-based arc-standard model, the tran-
sition state includes a stack and a queue, where
the stack contains a sequence of partially-parsed
dependency trees, and the queue consists of un-
processed input words. Four actions are defined
for state transition, including arc-left (AL, which
creates a left arc between the top element s
0
and
the second top element s
1
on the stack), arc-right
(AR, which creates a right arc between s
0
and s
1
),
pop-root (PR, which defines the root node of a de-
pendency tree when there is only one element on
the stack and no element in the queue), and the last
shift (SH, which shifts the first element q
0
of the
queue onto the stack).
For character-level dependency parsing, there
are two types of dependencies: inter-word depen-
dencies and intra-word dependencies. To parse
them with both character and word features, we
extend the original transition actions into two cat-
egories, for inter-word dependencies and intra-
word dependencies, respectively. The actions for
inter-word dependencies include inter-word arc-
left (AL
w
), inter-word arc-right (AR
w
), pop-root
(PR) and inter-word shift (SH
w
). Their definitions
are the same as the word-based model, with one
exception that the inter-word shift operation has
a parameter denoting the POS-tag of the incoming
word, so that POS disambiguation is performed by
the SH
w
action.
The actions for intra-word dependencies in-
clude intra-word arc-left (AL
c
), intra-word arc-
right (AR
c
), pop-word (PW) and inter-word shift
(SH
c
). The definitions of AL
c
, AR
c
and SH
c
are
the same as the word-based arc-standard model,
while PW changes the top element on the stack
into a full-word node, which can only take inter-
word dependencies. One thing to note is that, due
to variable word sizes in character-level parsing,
the number of actions can vary between differ-
ent sequences of actions corresponding to differ-
ent analyses. We use the padding method (Zhu
et al, 2013), adding an IDLE action to finished
transition action sequences, for better alignments
between states in the beam.
In the character-level arc-standard transition
1328
step action stack queue dependencies
0 - ? ? ? ? ? ? ?
1 SH
w
(NR) ?/NR ? ? ? ? ? ?
2 SH
c
?/NR ?/NR ? ? ? ? ? ?
3 AL
c
?/NR ? ? ? ? ? A
1
= {?x?}
4 SH
c
?/NR ?/NR ? ? ? ? ? A
1
5 AL
c
?/NR ? ? ? ? ? A
2
= A
1
?
{?x?}
6 PW ???/NR ? ? ? ? ? A
2
7 SH
w
(NN) ???/NR ?/NN ? ? ? ? ? A
2
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
12 PW ???/NR ???/NN ? ? ? ? ? A
i
13 AL
w
???/NN ? ? ? ? ? A
i+1
= A
i
?
{???/NRx???/NN}
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(a) character-level dependency parsing using the arc-standard algorithm
step action stack deque queue dependencies
0 - ? ? ? ? ? ?
1 SH
c
(NR) ? ?/NR ? ? ? ? ? ?
2 AL
c
? ? ?/NR ? ? ? ? A
1
= {?x?}
3 SH
c
? ?/NR ? ? ? ? ? A
1
4 AL
c
? ? ?/NR ? ? ? ? A
2
= A
1
?
{?x?}
5 SH
c
? ?/NR ? ? ? ? ? A
2
6 PW ? ???/NR ? ? ? ? ? A
2
7 SH
w
???/NR ? ? ? ? ? ? A
2
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
13 PW ???/NR ???/NN ? ? ? ? ? A
i
14 AL
w
? ???/NN ? ? ? ? ? A
i+1
= A
i
?
{???/NRx???/NN}
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(b) character-level dependency parsing using the arc-eager algorithm, t = 1
Figure 3: Character-level dependency parsing of the sentence in Figure 1(c).
system, each word is initialized by the action SH
w
with a POS tag, before being incrementally mod-
ified by a sequence of intra-word actions, and fi-
nally being completed by the action PW. The inter-
word actions can be applied when all the elements
on the stack are full-word nodes, while the intra-
word actions can be applied when at least the top
element on the stack is a partial-word node. For
the actions AL
c
and AR
c
to be valid, the top two
elements on the stack are both partial-word nodes.
For the action PW to be valid, only the top ele-
ment on the stack is a partial-word node. Figure
3(a) gives an example action sequence.
There are three types of features. The first two
types are traditionally established features for the
dependency parsing and joint word segmentation
and POS-tagging tasks. We use the features pro-
posed by Hatori et al (2012). The word-level
dependency parsing features are added when the
inter-word actions are applied, and the features
for joint word segmentation and POS-tagging are
added when the actions PW, SH
w
and SH
c
are ap-
plied. Following the work of Hatori et al (2012),
we have a parameter ? to adjust the weights for
joint word segmentation and POS-tagging fea-
tures. We apply word-based dependency pars-
ing features to intra-word dependency parsing as
well, by using subwords (the conjunction of char-
acters spanning the head node) to replace words in
word features. The third type of features is word-
structure features. We extract the head charac-
ter and the smallest subwords containing the head
character from the intra-word dependencies (Sec-
tion 2). Table 1 summarizes the features.
3.2 The Arc-Eager Model
Similar to the arc-standard case, the state of a
word-based arc-eager model consists of a stack
and a queue, where the stack contains a sequence
of partial dependency trees, and the queue con-
sists of unprocessed input words. Unlike the arc-
standard model, which builds dependencies on the
top two elements on the stack, the arc-eager model
builds dependencies between the top element of
the stack and the first element of the queue. Five
actions are defined for state transformation: arc-
left (AL, which creates a left arc between the top
element of the stack s
0
and the first element in
the queue q
0
, while popping s
0
off the stack),
arc-right (AR, which creates a right arc between
1329
Feature templates
Lc, Lct, Rc, Rct, L
lc1
c, L
rc1
c, R
lc1
c,
Lc ?Rc, L
lc1
ct, L
rc1
ct, R
lc1
ct,
Lc ?Rw, Lw ?Rc, Lct ?Rw,
Lwt ?Rc, Lw ?Rct, Lc ?Rwt,
Lc ?Rc ? L
lc1
c, Lc ?Rc ? L
rc1
c,
Lc ?Rc ? L
lc2
c, Lc ?Rc ? L
rc2
c,
Lc ?Rc ?R
lc1
c, Lc ?Rc ?R
lc2
c,
Llsw, Lrsw, Rlsw, Rrsw, Llswt,
Lrswt, Rlswt, Rrswt, Llsw ?Rw,
Lrsw ?Rw, Lw ?Rlsw, Lw ?Rrsw
Table 1: Feature templates encoding intra-word
dependencies. L and R denote the two elements
over which the dependencies are built; the sub-
scripts lc1 and rc1 denote the left-most and right-
most children, respectively; the subscripts lc2 and
rc2 denote the second left-most and second right-
most children, respectively; w denotes the word;
t denotes the POS tag; c denotes the head charac-
ter; lsw and rsw denote the smallest left and right
subwords respectively, as shown in Figure 2.
s
0
and q
0
, while shifting q
0
from the queue onto
the stack), pop-root (PR, which defines the ROOT
node of the dependency tree when there is only
one element on the stack and no element in the
queue), reduce (RD, which pops s
0
off the stack),
and shift (SH, which shifts q
0
onto the stack).
There is no previous work that exploits the
arc-eager algorithm for jointly performing POS-
tagging and dependency parsing. Since the first
element of the queue can be shifted onto the stack
by either SH or AR, it is more difficult to assign
a POS tag to each word by using a single action.
In this work, we make a change to the configu-
ration state, adding a deque between the stack and
the queue to save partial words with intra-word de-
pendencies. We divide the transition actions into
two categories, one for inter-word dependencies
(AR
w
, AL
w
, SH
w
, RD
w
and PR) and the other
for intra-word dependencies (AR
c
, AL
c
, SH
c
, RD
c
and PW), requiring that the intra-word actions be
operated between the deque and the queue, while
the inter-word actions be operated between the
stack and the deque.
For character-level arc-eager dependency pars-
ing, the inter-word actions are the same as the
word-based methods. The actions AL
c
and AR
c
are the same as AL
w
and AR
w
, except that they
operate on characters, but the SH
c
operation has a
parameter to denote the POS tag of a word. The
PW action recognizes a full-word. We also have
an IDLE action, for the same reason as the arc-
standard model.
In the character-level arc-eager transition sys-
tem, a word is formed in a similar way with that
of character-level arc-standard algorithm. Each
word is initialized by the action SH
c
with a POS
tag, and then incrementally changed a sequence of
intra-word actions, before being finalized by the
action PW. All these actions operate between the
queue and deque. For the action PW, only the
first element in the deque (close to the queue) is
a partial-word node. For the actions AR
c
and AL
c
to be valid, the first element in the deque must be
a partial-word node. The action SH
c
have a POS
tag when shifting the first character of a word,but
does not have such a parameter when shifting the
next characters of a word. For the action SH
c
with
a POS tag to be valid, the first element in the deque
must be a full-word node. Different from the arc-
standard model, at any stage we can choose either
the action SH
c
with a POS tag to initialize a new
word on the deque, or the inter-word actions on
the stack. In order to eliminate the ambiguity, we
define a new parameter t to limit the max size of
the deque. If the deque is full with t words, inter-
word actions are performed; otherwise intra-word
actions are performed. All the inter-word actions
must be applied on full-word nodes between the
stack an the deque. Figure 3(b) gives an example
action sequence.
Similar to the arc-standard case, there are three
types of features, with the first two types being
traditionally established features for dependency
parsing and joint word segmentation and POS-
tagging. The dependency parsing features are
taken from the work of Zhang and Nivre (2011),
and the features for joint word segmentation and
POS-tagging are taken from Zhang and Clark
(2010)
1
. The word-level dependency parsing fea-
tures are triggered when the inter-word actions are
applied, while the features of joint word segmenta-
tion and POS-tagging are added when the actions
SH
c
, AR
c
and PW are applied. Again we use a pa-
rameter ? to adjust the weights for joint word seg-
mentation and POS-tagging features. The word-
level features for dependency parsing are applied
to intra-word dependency parsing as well, by us-
ing subwords to replace words. The third type of
features is word-structure features, which are the
1
Since Hatori et al (2012) also use Zhang and Clark
(2010)?s features, the arc-standard and arc-eager character-
level dependency parsing models have the same features for
joint word segmentation and POS-tagging.
1330
CTB50 CTB60 CTB70
Training
#sent 18k 23k 31k
#word 494k 641k 718k
Development
#sent 350 2.1k 10k
#word 6.8k 60k 237k
#oov 553 3.3k 13k
Test
#sent 348 2.8k 10k
#word 8.0k 82k 245k
#oov 278 4.6k 13k
Table 2: Statistics of datasets.
same as those of the character-level arc-standard
model, shown in Table 1.
4 Experiments
4.1 Experimental Settings
We use the Chinese Penn Treebank 5.0, 6.0 and 7.0
to conduct the experiments, splitting the corpora
into training, development and test sets according
to previous work. Three different splitting meth-
ods are used, namely CTB50 by Zhang and Clark
(2010), CTB60 by the official documentation of
CTB 6.0, and CTB70 by Wang et al (2011). The
dataset statistics are shown in Table 2. We use
the head rules of Zhang and Clark (2008) to con-
vert phrase structures into dependency structures.
The intra-word dependencies are extracted from
the annotations of Zhang et al (2013)
2
.
The standard measures of word-level precision,
recall and F1 score are used to evaluate word seg-
mentation, POS-tagging and dependency parsing,
following Hatori et al (2012). In addition, we use
the same measures to evaluate intra-word depen-
dencies, which indicate the performance of pre-
dicting word structures. A word?s structure is cor-
rect only if all the intra-word dependencies are all
correctly recognized.
4.2 Baseline and Proposed Models
For the baseline, we have two different pipeline
models. The first consists of a joint segmentation
and POS-tagging model (Zhang and Clark, 2010)
and a word-based dependency parsing model us-
ing the arc-standard algorithm (Huang et al,
2009). We name this model STD (pipe). The
second consists of the same joint segmentation
and POS-tagging model and a word-based depen-
dency parsing model using the arc-eager algorithm
2
https://github.com/zhangmeishan/
wordstructures; their annotation was conducted
on CTB 5.0, while we made annotations of the remainder of
the CTB 7.0 words. We also make the annotations publicly
available at the same site.
(Zhang and Nivre, 2011). We name this model
EAG (pipe). For the pipeline models, we use a
beam of size 16 for joint segmentation and POS-
tagging, and a beam of size 64 for dependency
parsing, according to previous work.
We study the following character-level depen-
dency parsing models:
? STD (real, pseudo): the arc-standard model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
? STD (pseudo, real): the arc-standard model
with pseudo intra-word dependencies and
real inter-word dependencies;
? STD (real, real): the arc-standard model with
annotated intra-word dependencies and real
inter-word dependencies;
? EAG (real, pseudo): the arc-eager model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
? EAG (pseudo, real): the arc-eager model
with pseudo intra-word dependencies and
real inter-word dependencies;
? EAG (real, real): the arc-eager model with
annotated intra-word dependencies and real
inter-word dependencies.
The annotated intra-word dependencies refer to
the dependencies extracted from annotated word
structures, while the pseudo intra-word depen-
dencies used in the above models are similar
to those of Hatori et al (2012). For a given
word w = c
1
c
2
? ? ? c
m
, the intra-word depen-
dency structure is c
x
1
c
x
2
? ? ?
x
c
m
3
. The real inter-
word dependencies refer to the syntactic word-
level dependencies by head-finding rules from
CTB, while the pseudo inter-word dependencies
refer to the word-level dependencies used by Zhao
(2009) (w
x
1
w
x
2
? ? ?
x
w
n
). The character-level
models with annotated intra-word dependencies
and pseudo inter-word dependencies are compared
with the pipelines on word segmentation and POS-
tagging accuracies, and are compared with the
character-level models with annotated intra-word
dependencies and real inter-word dependencies
on word segmentation, POS-tagging and word-
structure predicating accuracies. All the proposed
3
We also tried similar structures with right arcs, which
gave lower accuracies.
1331
STD (real, real) SEG POS DEP WS
? = 1 95.85 91.60 76.96 95.14
? = 2 96.09 91.89 77.28 95.29
? = 3 96.02 91.84 77.22 95.23
? = 4 96.10 91.96 77.49 95.29
? = 5 96.07 91.90 77.31 95.21
Table 3: Development test results of the character-
level arc-standard model on CTB60.
EAG (real, real) SEG POS DEP WS
? = 1
t = 1 96.00 91.66 74.63 95.49
t = 2 95.93 91.75 76.60 95.37
t = 3 95.93 91.74 76.94 95.36
t = 4 95.91 91.71 76.82 95.33
t = 5 95.95 91.73 76.84 95.40
t = 3
? = 1 95.93 91.74 76.94 95.36
? = 2 96.11 91.99 77.17 95.56
? = 3 96.16 92.01 77.48 95.62
? = 4 96.11 91.93 77.40 95.53
? = 5 96.00 91.84 77.10 95.43
Table 4: Development test results of the character-
level arc-eager model on CTB60.
models use a beam of size 64 after considering
both speeds and accuracies.
4.3 Development Results
Our development tests are designed for two pur-
poses: adjusting the parameters for the two pro-
posed character-level models and testing the effec-
tiveness of the novel word-structure features. Tun-
ing is conducted by maximizing word-level depen-
dency accuracies. All the tests are conducted on
the CTB60 data set.
4.3.1 Parameter Tuning
For the arc-standard model, there is only one pa-
rameter ? that needs tuning. It adjusts the weights
of segmentation and POS-tagging features, be-
cause the number of feature templates is much less
for the two tasks than for parsing. We set the value
of ? to 1 ? ? ? 5, respectively. Table 3 shows the
accuracies on the CTB60 development set. Ac-
cording to the results, we use ? = 4 for our final
character-level arc-standard model.
For the arc-eager model, there are two parame-
ters t and ?. t denotes the deque size of the arc-
eager model, while ? shares the same meaning as
the arc-standard model. We take two steps for pa-
rameter tuning, first adjusting the more crucial pa-
rameter t and then adjusting ? on the best t. Both
parameters are assigned the values of 1 to 5. Ta-
SEG POS DEP WS
STD (real, real) 96.10 91.96 77.49 95.29
STD (real, real)/wo 95.99 91.79 77.19 95.35
? -0.11 -0.17 -0.30 +0.06
EAG (real, real) 96.16 92.01 77.48 95.62
EAG (real, real)/wo 96.09 91.82 77.12 95.56
? -0.07 -0.19 -0.36 -0.06
Table 5: Feature ablation tests for the novel word-
structure features, where ?/wo? denotes the corre-
sponding models without the novel intra-word de-
pendency features.
ble 4 shows the results. According to results, we
set t = 3 and ? = 3 for the final character-level
arc-eager model, respectively.
4.3.2 Effectiveness of Word-Structure
Features
To test the effectiveness of our novel word-
structure features, we conduct feature ablation ex-
periments on the CTB60 development data set for
the proposed arc-standard and arc-eager models,
respectively. Table 5 shows the results. We can
see that both the two models achieve better accu-
racies on word-level dependencies with the novel
word-structure features, while the features do not
affect word-structure predication significantly.
4.4 Final Results
Table 6 shows the final results on the CTB50,
CTB60 and CTB70 data sets, respectively. The
results demonstrate that the character-level depen-
dency parsing models are significantly better than
the corresponding word-based pipeline models,
for both the arc-standard and arc-eager systems.
Similar to the findings of Zhang et al (2013), we
find that the annotated word structures can give
better accuracies than pseudo word structures. An-
other interesting finding is that, although the arc-
eager algorithm achieves lower accuracies in the
word-based pipeline models, it obtains compara-
tive accuracies in the character-level models.
We also compare our results to those of Hatori
et al (2012), which is comparable to STD (pseudo,
real) since similar arc-standard algorithms and
features are used. The major difference is the
set of transition actions. We rerun their system
on the three datasets
4
. As shown in Table 6, our
arc-standard system with pseudo word structures
4
http://triplet.cc/. We use a different
constituent-to-dependency conversion scheme in com-
parison with Hatori et al (2012)?s work.
1332
Model
CTB50 CTB60 CTB70
SEG POS DEP WS SEG POS DEP WS SEG POS DEP WS
The arc-standard models
STD (pipe) 97.53 93.28 79.72 ? 95.32 90.65 75.35 ? 95.23 89.92 73.93 ?
STD (real, pseudo) 97.78 93.74 ? 97.40 95.77
?
91.24
?
? 95.08 95.59
?
90.49
?
? 94.97
STD (pseudo, real) 97.67 94.28
?
81.63
?
? 95.63
?
91.40
?
76.75
?
? 95.53
?
90.75
?
75.63
?
?
STD (real, real) 97.84 94.62
?
82.14
?
97.30 95.56
?
91.39
?
77.09
?
94.80 95.51
?
90.76
?
75.70
?
94.78
Hatori+ ?12 97.75 94.33 81.56 ? 95.26 91.06 75.93 ? 95.27 90.53 74.73 ?
The arc-eager models
EAG (pipe) 97.53 93.28 79.59 ? 95.32 90.65 74.98 ? 95.23 89.92 73.46 ?
EAG (real, pseudo) 97.75 93.88 ? 97.45 95.63
?
91.07
?
? 95.06 95.50
?
90.36
?
? 95.00
EAG (pseudo, real) 97.76 94.36
?
81.70
?
? 95.63
?
91.34
?
76.87
?
? 95.39
?
90.56
?
75.56
?
?
EAG (real, real) 97.84 94.36
?
82.07
?
97.49 95.71
?
91.51
?
76.99
?
95.16 95.47
?
90.72
?
75.76
?
94.94
Table 6: Main results, where the results marked with ? denote that the p-value is less than 0.001 compared
with the pipeline word-based models using pairwise t-test.
brings consistent better accuracies than their work
on all the three data sets.
Both the pipelines and character-level mod-
els with pseudo inter-word dependencies perform
word segmentation and POS-tagging jointly, with-
out using real word-level syntactic information. A
comparison between them (STD/EAG (pipe) vs.
STD/EAG (real, pseudo)) reflects the effectiveness
of annotated intra-word dependencies on segmen-
tation and POS-tagging. We can see that both the
arc-standard and arc-eager models with annotated
intra-word dependencies can improve the segmen-
tation accuracies by 0.3% and the POS-tagging ac-
curacies by 0.5% on average on the three datasets.
Similarly, a comparison between the character-
level models with pseudo inter-word dependen-
cies and the character-level models with real inter-
word dependencies (STD/EAG (real, pseudo) vs.
STD/EAG (real, real)) can reflect the effectiveness
of annotated inter-word structures on morphology
analysis. We can see that improved POS-tagging
accuracies are achieved using the real inter-word
dependencies when jointly performing inner- and
inter-word dependencies. However, we find that
the inter-word dependencies do not help the word-
structure accuracies.
4.5 Analysis
To better understand the character-level parsing
models, we conduct error analysis in this section.
All the experiments are conducted on the CTB60
test data sets. The new advantage of the character-
level models is that one can parse the internal
word structures of intra-word dependencies. Thus
we are interested in their capabilities of predict-
ing word structures. We study the word-structure
accuracies in two aspects, including OOV, word
length, POS tags and the parsing model.
4.5.1 OOV
The word-structure accuracy of OOV words re-
flects a model?s ability of handling unknown
words. The overall recalls of OOV word structures
are 67.98% by STD (real, real) and 69.01% by
EAG (real, real), respectively. We find that most
errors are caused by failures of word segmenta-
tion. We further investigate the accuracies when
words are correctly segmented, where the accura-
cies of OOV word structures are 87.64% by STD
(real, real) and 89.07% by EAG (real, real). The
results demonstrate that the structures of Chinese
words are not difficult to predict, and confirm the
fact that Chinese word structures have some com-
mon syntactic patterns.
4.5.2 Parsing Model
From the above analysis in terms of OOV, word
lengths and POS tags, we can see that the EAG
(real, real) model and the STD (real, real) mod-
els behave similarly on word-structure accuracies.
Here we study the two models more carefully,
comparing their word accuracies sentence by sen-
tence. Figure 4 shows the results, where each
point denotes a sentential comparison between
STD (real, real) and EAG (real, real), the x-axis
denotes the sentential word-structure accuracy of
STD (real, real), and the y-axis denotes that of
EAG (real, real). The points at the diagonal show
the same accuracies by the two models, while oth-
ers show that the two models perform differently
on the corresponding sentences. We can see that
most points are beyond the diagonal line, indicat-
1333
0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
STD (real, real)
E
A
G
(
r
e
a
l
,
r
e
a
l
)
Figure 4: Sentential word-structure accuracies of
STD (real, real) and EAG (real, real).
ing that the two parsing models can be comple-
mentary in parsing intra-word dependencies.
5 Related Work
Zhao (2009) was the first to study character-level
dependencies; they argue that since no consistent
word boundaries exist over Chinese word segmen-
tation, dependency-based representations of word
structures serve as a good alternative for Chinese
word segmentation. Thus their main concern is
to parse intra-word dependencies. In this work,
we extend their formulation, making use of large-
scale annotations of Zhang et al (2013), so that the
syntactic word-level dependencies can be parsed
together with intra-word dependencies.
Hatori et al (2012) proposed a joint model
for Chinese word segmentation, POS-tagging and
dependency parsing, studying the influence of
joint model and character features for parsing,
Their model is extended from the arc-standard
transition-based model, and can be regarded as
an alternative to the arc-standard model of our
work when pseudo intra-word dependencies are
used. Similar work is done by Li and Zhou (2012).
Our proposed arc-standard model is more concise
while obtaining better performance than Hatori et
al. (2012)?s work. With respect to word structures,
real intra-word dependencies are often more com-
plicated, while pseudo word structures cannot be
used to correctly guide segmentation.
Zhao (2009), Hatori et al (2012) and our
work all study character-level dependency pars-
ing. While Zhao (2009) focus on word internal
structures using pseudo inter-word dependencies,
Hatori et al (2012) investigate a joint model using
pseudo intra-word dependencies. We use manual
dependencies for both inner- and inter-word struc-
tures, studying their influences on each other.
Zhang et al (2013) was the first to perform Chi-
nese syntactic parsing over characters. They ex-
tended word-level constituent trees by annotated
word structures, and proposed a transition-based
approach to parse intra-word structures and word-
level constituent structures jointly. For Hebrew,
Tsarfaty and Goldberg (2008) investigated joint
segmentation and parsing over characters using a
graph-based method. Our work is similar in ex-
ploiting character-level syntax. We study the de-
pendency grammar, another popular syntactic rep-
resentation, and propose two novel transition sys-
tems for character-level dependency parsing.
Nivre (2008) gave a systematic description of
the arc-standard and arc-eager algorithms, cur-
rently two popular transition-based parsing meth-
ods for word-level dependency parsing. We extend
both algorithms to character-level joint word seg-
mentation, POS-tagging and dependency parsing.
To our knowledge, we are the first to apply the arc-
eager system to joint models and achieve compar-
ative performances to the arc-standard model.
6 Conclusions
We studied the character-level Chinese depen-
dency parsing, by making novel extensions to
two commonly-used transition-based dependency
parsing algorithms for word-based dependency
parsing. With both pseudo and annotated word
structures, our character-level models obtained
better accuracies than previous work on seg-
mentation, POS-tagging and word-level depen-
dency parsing. We further analyzed some im-
portant factors for intra-word dependencies, and
found that two proposed character-level pars-
ing models are complementary in parsing intra-
word dependencies. We make the source code
publicly available at http://sourceforge.
net/projects/zpar/, version 0.7.
Acknowledgments
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61370164, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design.
1334
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the EMNLP-CONLL, pages 1455?1465,
Jeju Island, Korea, July.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd COLING, number August, pages
89?97.
Xavier Carreras, Mihai Surdeanu, and Llu??s M`arquez.
2006. Projective dependency parsing with per-
ceptron. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 181?185, New York City, June.
Pi-Chuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In ACL Workshop
on Statistical Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, , and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of ACL, pages
1052?1062, August.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the 7th EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, vol-
ume 4701 of Lecture Notes in Computer Science,
pages 559?566.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 123?133.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th ACL,
pages 1045?1053, Jeju Island, Korea, July.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th ACL, pages 1077?1086, Up-
psala, Sweden, July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1222?1231. Asso-
ciation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the ACL, pages 1?11.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454, Jeju Island, Ko-
rea, July.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th
ACL, pages 675?684, Jeju Island, Korea, July.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of the 49th ACL, pages 1405?
1414, Portland, Oregon, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, number
June, pages 91?98, Morristown, NJ, USA.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
ACL.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-based
or morpheme-based? annotation strategies for mod-
ern hebrew clitics. In LREC. European Language
Resources Association.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmenta-
tion and pos tagging with semi-supervised methods
using large auto-analyzed data. In Proceedings of
5th IJCNLP, pages 309?317, Chiang Mai, Thailand,
November.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
Proceedings of EMNLP, pages 562?571, Honolulu,
Hawaii, October.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
EMNLP, pages 843?852, Cambridge, MA, October.
1335
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th ACL, pages 188?193, Port-
land, Oregon, USA, June.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Chinese word segmentation and statistical
machine translation. IEEE Transactions on Signal
Processing, 5(2).
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st ACL, pages 125?134,
Sofia, Bulgaria, August.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the EACL, pages 879?887, Athens, Greece, March.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st ACL, pages 434?443, Sofia, Bulgaria, August.
1336
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555?1565,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Nan Yang
\
, Ming Zhou
?
, Ting Liu
?
, Bing Qin
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
\
University of Science and Technology of China, Hefei, China
{dytang, tliu, qinb}@ir.hit.edu.cn
{fuwei, v-nayang, mingzhou}@microsoft.com
Abstract
We present a method that learns word em-
bedding for Twitter sentiment classifica-
tion in this paper. Most existing algorithm-
s for learning continuous word represen-
tations typically only model the syntactic
context of words but ignore the sentimen-
t of text. This is problematic for senti-
ment analysis as they usually map word-
s with similar syntactic context but oppo-
site sentiment polarity, such as good and
bad, to neighboring word vectors. We
address this issue by learning sentiment-
specific word embedding (SSWE), which
encodes sentiment information in the con-
tinuous representation of words. Specif-
ically, we develop three neural networks
to effectively incorporate the supervision
from sentiment polarity of text (e.g. sen-
tences or tweets) in their loss function-
s. To obtain large scale training corpora,
we learn the sentiment-specific word em-
bedding from massive distant-supervised
tweets collected by positive and negative
emoticons. Experiments on applying SS-
WE to a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that (1) the SSWE feature performs
comparably with hand-crafted features in
the top-performed system; (2) the perfor-
mance is further improved by concatenat-
ing SSWE with existing feature set.
1 Introduction
Twitter sentiment classification has attracted in-
creasing research interest in recent years (Jiang et
al., 2011; Hu et al, 2013). The objective is to clas-
sify the sentiment polarity of a tweet as positive,
?
This work was done when the first and third authors
were visiting Microsoft Research Asia.
negative or neutral. The majority of existing ap-
proaches follow Pang et al (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
designing effective features to obtain better clas-
sification performance. For example, Mohammad
et al (2013) build the top-performed system in the
Twitter sentiment classification track of SemEval
2013 (Nakov et al, 2013), using diverse sentiment
lexicons and a variety of hand-crafted features.
Feature engineering is important but labor-
intensive. It is therefore desirable to discover ex-
planatory factors from the data and make the learn-
ing algorithms less dependent on extensive fea-
ture engineering (Bengio, 2013). For the task of
sentiment classification, an effective feature learn-
ing method is to compose the representation of a
sentence (or document) from the representation-
s of the words or phrases it contains (Socher et
al., 2013b; Yessenalina and Cardie, 2011). Ac-
cordingly, it is a crucial step to learn the word
representation (or word embedding), which is a
dense, low-dimensional and real-valued vector for
a word. Although existing word embedding learn-
ing algorithms (Collobert et al, 2011; Mikolov et
al., 2013) are intuitive choices, they are not effec-
tive enough if directly used for sentiment classi-
fication. The most serious problem is that tradi-
tional methods typically model the syntactic con-
text of words but ignore the sentiment information
of text. As a result, words with opposite polari-
ty, such as good and bad, are mapped into close
vectors. It is meaningful for some tasks such as
pos-tagging (Zheng et al, 2013) as the two words
have similar usages and grammatical roles, but it
becomes a disaster for sentiment analysis as they
have the opposite sentiment polarity.
In this paper, we propose learning sentiment-
specific word embedding (SSWE) for sentiment
analysis. We encode the sentiment information in-
1555
to the continuous representation of words, so that
it is able to separate good and bad to opposite ends
of the spectrum. To this end, we extend the ex-
isting word embedding learning algorithm (Col-
lobert et al, 2011) and develop three neural net-
works to effectively incorporate the supervision
from sentiment polarity of text (e.g. sentences
or tweets) in their loss functions. We learn the
sentiment-specific word embedding from tweet-
s, leveraging massive tweets with emoticons as
distant-supervised corpora without any manual an-
notations. These automatically collected tweet-
s contain noises so they cannot be directly used
as gold training data to build sentiment classifier-
s, but they are effective enough to provide weak-
ly supervised signals for training the sentiment-
specific word embedding.
We apply SSWE as features in a supervised
learning framework for Twitter sentiment classi-
fication, and evaluate it on the benchmark dataset
in SemEval 2013. In the task of predicting posi-
tive/negative polarity of tweets, our method yields
84.89% in macro-F1 by only using SSWE as fea-
ture, which is comparable to the top-performed
system based on hand-crafted features (84.70%).
After concatenating the SSWE feature with ex-
isting feature set, we push the state-of-the-art to
86.58% in macro-F1. The quality of SSWE is al-
so directly evaluated by measuring the word sim-
ilarity in the embedding space for sentiment lexi-
cons. In the accuracy of polarity consistency be-
tween each sentiment word and its top N closest
words, SSWE outperforms existing word embed-
ding learning algorithms.
The major contributions of the work presented
in this paper are as follows.
? We develop three neural networks to learn
sentiment-specific word embedding (SSWE)
from massive distant-supervised tweets with-
out any manual annotations;
? To our knowledge, this is the first work that
exploits word embedding for Twitter senti-
ment classification. We report the results that
the SSWE feature performs comparably with
hand-crafted features in the top-performed
system in SemEval 2013;
? We release the sentiment-specific word em-
bedding learned from 10 million tweets,
which can be adopted off-the-shell in other
sentiment analysis tasks.
2 Related Work
In this section, we present a brief review of the
related work from two perspectives, Twitter senti-
ment classification and learning continuous repre-
sentations for sentiment classification.
2.1 Twitter Sentiment Classification
Twitter sentiment classification, which identifies
the sentiment polarity of short, informal tweets,
has attracted increasing research interest (Jiang et
al., 2011; Hu et al, 2013) in recent years. Gen-
erally, the methods employed in Twitter sentiment
classification follow traditional sentiment classifi-
cation approaches. The lexicon-based approaches
(Turney, 2002; Ding et al, 2008; Taboada et al,
2011; Thelwall et al, 2012) mostly use a dictio-
nary of sentiment words with their associated sen-
timent polarity, and incorporate negation and in-
tensification to compute the sentiment polarity for
each sentence (or document).
The learning based methods for Twitter sen-
timent classification follow Pang et al (2002)?s
work, which treat sentiment classification of texts
as a special case of text categorization issue. Many
studies on Twitter sentiment classification (Pak
and Paroubek, 2010; Davidov et al, 2010; Bar-
bosa and Feng, 2010; Kouloumpis et al, 2011;
Zhao et al, 2012) leverage massive noisy-labeled
tweets selected by positive and negative emoticon-
s as training set and build sentiment classifiers di-
rectly, which is called distant supervision (Go et
al., 2009). Instead of directly using the distant-
supervised data as training set, Liu et al (2012)
adopt the tweets with emoticons to smooth the lan-
guage model and Hu et al (2013) incorporate the
emotional signals into an unsupervised learning
framework for Twitter sentiment classification.
Many existing learning based methods on Twit-
ter sentiment classification focus on feature engi-
neering. The reason is that the performance of sen-
timent classifier being heavily dependent on the
choice of feature representation of tweets. The
most representative system is introduced by Mo-
hammad et al (2013), which is the state-of-the-
art system (the top-performed system in SemEval
2013 Twitter Sentiment Classification Track) by
implementing a number of hand-crafted features.
Unlike the previous studies, we focus on learning
discriminative features automatically from mas-
sive distant-supervised tweets.
1556
2.2 Learning Continuous Representations for
Sentiment Classification
Pang et al (2002) pioneer this field by using bag-
of-word representation, representing each word as
a one-hot vector. It has the same length as the size
of the vocabulary, and only one dimension is 1,
with all others being 0. Under this assumption,
many feature learning algorithms are proposed to
obtain better classification performance (Pang and
Lee, 2008; Liu, 2012; Feldman, 2013). However,
the one-hot word representation cannot sufficient-
ly capture the complex linguistic characteristics of
words.
With the revival of interest in deep learn-
ing (Bengio et al, 2013), incorporating the con-
tinuous representation of a word as features has
been proving effective in a variety of NLP tasks,
such as parsing (Socher et al, 2013a), language
modeling (Bengio et al, 2003; Mnih and Hin-
ton, 2009) and NER (Turian et al, 2010). In the
field of sentiment analysis, Bespalov et al (2011;
2012) initialize the word embedding by Laten-
t Semantic Analysis and further represent each
document as the linear weighted of ngram vec-
tors for sentiment classification. Yessenalina and
Cardie (2011) model each word as a matrix and
combine words using iterated matrix multiplica-
tion. Glorot et al (2011) explore Stacked Denois-
ing Autoencoders for domain adaptation in sen-
timent classification. Socher et al propose Re-
cursive Neural Network (RNN) (2011b), matrix-
vector RNN (2012) and Recursive Neural Tensor
Network (RNTN) (2013b) to learn the composi-
tionality of phrases of any length based on the
representation of each pair of children recursively.
Hermann et al (2013) present Combinatory Cate-
gorial Autoencoders to learn the compositionality
of sentence, which marries the Combinatory Cat-
egorial Grammar with Recursive Autoencoder.
The representation of words heavily relies on
the applications or tasks in which it is used (Lab-
utov and Lipson, 2013). This paper focuses
on learning sentiment-specific word embedding,
which is tailored for sentiment analysis. Un-
like Maas et al (2011) that follow the proba-
bilistic document model (Blei et al, 2003) and
give an sentiment predictor function to each word,
we develop neural networks and map each n-
gram to the sentiment polarity of sentence. Un-
like Socher et al (2011c) that utilize manually
labeled texts to learn the meaning of phrase (or
sentence) through compositionality, we focus on
learning the meaning of word, namely word em-
bedding, from massive distant-supervised tweets.
Unlike Labutov and Lipson (2013) that produce
task-specific embedding from an existing word
embedding, we learn sentiment-specific word em-
bedding from scratch.
3 Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
In this section, we present the details of learn-
ing sentiment-specific word embedding (SSWE)
for Twitter sentiment classification. We pro-
pose incorporating the sentiment information of
sentences to learn continuous representations for
words and phrases. We extend the existing word
embedding learning algorithm (Collobert et al,
2011) and develop three neural networks to learn
SSWE. In the following sections, we introduce the
traditional method before presenting the details of
SSWE learning algorithms. We then describe the
use of SSWE in a supervised learning framework
for Twitter sentiment classification.
3.1 C&W Model
Collobert et al (2011) introduce C&W model to
learn word embedding based on the syntactic con-
texts of words. Given an ngram ?cat chills on a
mat?, C&W replaces the center word with a ran-
dom wordw
r
and derives a corrupted ngram ?cat
chills w
r
a mat?. The training objective is that the
original ngram is expected to obtain a higher lan-
guage model score than the corrupted ngram by a
margin of 1. The ranking objective function can
be optimized by a hinge loss,
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(1)
where t is the original ngram, t
r
is the corrupted
ngram, f
cw
(?) is a one-dimensional scalar repre-
senting the language model score of the input n-
gram. Figure 1(a) illustrates the neural architec-
ture of C&W, which consists of four layers, name-
ly lookup ? linear ? hTanh ? linear (from
bottom to top). The original and corrupted ngram-
s are treated as inputs of the feed-forward neural
network, respectively. The output f
cw
is the lan-
guage model score of the input, which is calculat-
ed as given in Equation 2, where L is the lookup
table of word embedding,w
1
, w
2
, b
1
, b
2
are the pa-
rameters of linear layers.
f
cw
(t) = w
2
(a) + b
2
(2)
1557
so cooool :D 
lookup 
linear 
hTanh 
linear 
softmax 
(a) C&W 
so cooool :D 
(b) SSWEh 
so cooool :D 
(c) SSWEu 
syntactic 
sentiment 
positive 
negative 
Figure 1: The traditional C&W model and our neural networks (SSWE
h
and SSWE
u
) for learning
sentiment-specific word embedding.
a = hTanh(w
1
L
t
+ b
1
) (3)
hTanh(x) =
?
?
?
?
?
?1 if x < ?1
x if ? 1 ? x ? 1
1 if x > 1
(4)
3.2 Sentiment-Specific Word Embedding
Following the traditional C&W model (Collobert
et al, 2011), we incorporate the sentiment infor-
mation into the neural network to learn sentiment-
specific word embedding. We develop three neural
networks with different strategies to integrate the
sentiment information of tweets.
Basic Model 1 (SSWE
h
). As an unsupervised
approach, C&W model does not explicitly capture
the sentiment information of texts. An intuitive
solution to integrate the sentiment information is
predicting the sentiment distribution of text based
on input ngram. We do not utilize the entire sen-
tence as input because the length of different sen-
tences might be variant. We therefore slide the
window of ngram across a sentence, and then pre-
dict the sentiment polarity based on each ngram
with a shared neural network. In the neural net-
work, the distributed representation of higher lay-
er are interpreted as features describing the input.
Thus, we utilize the continuous vector of top layer
to predict the sentiment distribution of text.
Assuming there are K labels, we modify the di-
mension of top layer in C&W model as K and
add a softmax layer upon the top layer. The
neural network (SSWEh) is given in Figure 1(b).
Softmax layer is suitable for this scenario be-
cause its outputs are interpreted as conditional
probabilities. Unlike C&W, SSWE
h
does not gen-
erate any corrupted ngram. Let f
g
(t), where K
denotes the number of sentiment polarity label-
s, be the gold K-dimensional multinomial distri-
bution of input t and
?
k
f
g
k
(t) = 1. For pos-
itive/negative classification, the distribution is of
the form [1,0] for positive and [0,1] for negative.
The cross-entropy error of the softmax layer is :
loss
h
(t) = ?
?
k={0,1}
f
g
k
(t) ? log(f
h
k
(t)) (5)
where f
g
(t) is the gold sentiment distribution and
f
h
(t) is the predicted sentiment distribution.
Basic Model 2 (SSWE
r
). SSWE
h
is trained by
predicting the positive ngram as [1,0] and the neg-
ative ngram as [0,1]. However, the constraint of
SSWE
h
is too strict. The distribution of [0.7,0.3]
can also be interpreted as a positive label because
the positive score is larger than the negative s-
core. Similarly, the distribution of [0.2,0.8] indi-
cates negative polarity. Based on the above obser-
vation, the hard constraints in SSWE
h
should be
relaxed. If the sentiment polarity of a tweet is pos-
itive, the predicted positive score is expected to be
larger than the predicted negative score, and the
exact reverse if the tweet has negative polarity.
We model the relaxed constraint with a rank-
ing objective function and borrow the bottom four
layers from SSWE
h
, namely lookup? linear ?
hTanh ? linear in Figure 1(b), to build the re-
laxed neural network (SSWEr). Compared with
SSWE
h
, the softmax layer is removed because
SSWE
r
does not require probabilistic interpreta-
tion. The hinge loss of SSWE
r
is modeled as de-
1558
scribed below.
loss
r
(t) = max(0, 1? ?
s
(t)f
r
0
(t)
+ ?
s
(t)f
r
1
(t) )
(6)
where f
r
0
is the predicted positive score, f
r
1
is
the predicted negative score, ?
s
(t) is an indicator
function reflecting the sentiment polarity of a sen-
tence,
?
s
(t) =
{
1 if f
g
(t) = [1, 0]
?1 if f
g
(t) = [0, 1]
(7)
Similar with SSWE
h
, SSWE
r
also does not gen-
erate the corrupted ngram.
Unified Model (SSWE
u
). The C&W model
learns word embedding by modeling syntactic
contexts of words but ignoring sentiment infor-
mation. By contrast, SSWE
h
and SSWE
r
learn
sentiment-specific word embedding by integrating
the sentiment polarity of sentences but leaving out
the syntactic contexts of words. We develop a uni-
fied model (SSWEu) in this part, which captures
the sentiment information of sentences as well as
the syntactic contexts of words. SSWE
u
is illus-
trated in Figure 1(c).
Given an original (or corrupted) ngram and
the sentiment polarity of a sentence as the in-
put, SSWE
u
predicts a two-dimensional vector for
each input ngram. The two scalars (f
u
0
, f
u
1
) s-
tand for language model score and sentiment s-
core of the input ngram, respectively. The training
objectives of SSWE
u
are that (1) the original n-
gram should obtain a higher language model score
f
u
0
(t) than the corrupted ngram f
u
0
(t
r
), and (2) the
sentiment score of original ngram f
u
1
(t) should be
more consistent with the gold polarity annotation
of sentence than corrupted ngram f
u
1
(t
r
). The loss
function of SSWE
u
is the linear combination of t-
wo hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(8)
where loss
cw
(t, t
r
) is the syntactic loss as given
in Equation 1, loss
us
(t, t
r
) is the sentiment loss
as described in Equation 9. The hyper-parameter
? weighs the two parts.
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(9)
Model Training. We train sentiment-specific
word embedding from massive distant-supervised
tweets collected with positive and negative emoti-
cons
1
. We crawl tweets from April 1st, 2013 to
April 30th, 2013 with TwitterAPI. We tokenize
each tweet with TwitterNLP (Gimpel et al, 2011),
remove the @user and URLs of each tweet, and fil-
ter the tweets that are too short (< 7 words). Final-
ly, we collect 10M tweets, selected by 5M tweets
with positive emoticons and 5M tweets with nega-
tive emoticons.
We train SSWE
h
, SSWE
r
and SSWE
u
by
taking the derivative of the loss through back-
propagation with respect to the whole set of pa-
rameters (Collobert et al, 2011), and use Ada-
Grad (Duchi et al, 2011) to update the parame-
ters. We empirically set the window size as 3, the
embedding length as 50, the length of hidden lay-
er as 20 and the learning rate of AdaGrad as 0.1
for all baseline and our models. We learn embed-
ding for unigrams, bigrams and trigrams separate-
ly with same neural network and same parameter
setting. The contexts of unigram (bigram/trigram)
are the surrounding unigrams (bigrams/trigrams),
respectively.
3.3 Twitter Sentiment Classification
We apply sentiment-specific word embedding for
Twitter sentiment classification under a supervised
learning framework as in previous work (Pang et
al., 2002). Instead of hand-crafting features, we
incorporate the continuous representation of word-
s and phrases as the feature of a tweet. The senti-
ment classifier is built from tweets with manually
annotated sentiment polarity.
We explore min, average and max convolu-
tional layers (Collobert et al, 2011; Socher et
al., 2011a), which have been used as simple and
effective methods for compositionality learning
in vector-based semantics (Mitchell and Lapata,
2010), to obtain the tweet representation. The re-
sult is the concatenation of vectors derived from
different convolutional layers.
z(tw) = [z
max
(tw), z
min
(tw), z
average
(tw)]
where z(tw) is the representation of tweet tw and
z
x
(tw) is the results of the convolutional layer x ?
{min,max, average}. Each convolutional layer
1
We use the emoticons selected by Hu et al (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
1559
zx
employs the embedding of unigrams, bigrams
and trigrams separately and conducts the matrix-
vector operation of x on the sequence represented
by columns in each lookup table. The output of
z
x
is the concatenation of results obtained from
different lookup tables.
z
x
(tw) = [w
x
?L
uni
?
tw
, w
x
?L
bi
?
tw
, w
x
?L
tri
?
tw
]
where w
x
is the convolutional function of z
x
,
?L?
tw
is the concatenated column vectors of the
words in the tweet. L
uni
, L
bi
and L
tri
are the
lookup tables of the unigram, bigram and trigram
embedding, respectively.
4 Experiment
We conduct experiments to evaluate SSWE by in-
corporating it into a supervised learning frame-
work for Twitter sentiment classification. We also
directly evaluate the effectiveness of the SSWE by
measuring the word similarity in the embedding
space for sentiment lexicons.
4.1 Twitter Sentiment Classification
Experiment Setup and Datasets. We conduct
experiments on the latest Twitter sentiment clas-
sification benchmark dataset in SemEval 2013
(Nakov et al, 2013). The training and develop-
ment sets were completely in full to task partici-
pants. However, we were unable to download all
the training and development sets because some
tweets were deleted or not available due to mod-
ified authorization status. The test set is directly
provided to the participants. The distribution of
our dataset is given in Table 1. We train sentiment
classifier with LibLinear (Fan et al, 2008) on the
training set, tune parameter ?c on the dev set and
evaluate on the test set. Evaluation metric is the
Macro-F1 of positive and negative categories
2
.
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of the SemEval 2013 Twitter
sentiment classification dataset.
2
We investigate 2-class Twitter sentiment classifica-
tion (positive/negative) instead of 3-class Twitter sentiment
classification (positive/negative/neutral) in SemEval2013.
Baseline Methods. We compare our method
with the following sentiment classification algo-
rithms:
(1) DistSuper: We use the 10 million tweets se-
lected by positive and negative emoticons as train-
ing data, and build sentiment classifier with Lib-
Linear and ngram features (Go et al, 2009).
(2) SVM: The ngram features and Support Vec-
tor Machine are widely used baseline methods to
build sentiment classifiers (Pang et al, 2002). Li-
bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,
2012) is a state-of-the-art performer on many sen-
timent classification datasets, which trades-off be-
tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al,
2011c) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. We run RAE with randomly initial-
ized word embedding.
(5) NRC: NRC builds the top-performed system
in SemEval 2013 Twitter sentiment classification
track which incorporates diverse sentiment lexi-
cons and many manually designed features. We
re-implement this system because the codes are
not publicly available
3
. NRC-ngram refers to the
feature set of NRC leaving out ngram features.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al, 2013b) be-
cause we cannot efficiently train the RNTN model.
The reason lies in that the tweets in our dataset do
not have accurately parsed results or fine grained
sentiment labels for phrases. Another reason is
that the RNTN model trained on movie reviews
cannot be directly applied on tweets due to the d-
ifferences between domains (Blitzer et al, 2007).
Results and Analysis. Table 2 shows the macro-
F1 of the baseline systems as well as the SSWE-
based methods on positive/negative sentimen-
t classification of tweets. Distant supervision is
relatively weak because the noisy-labeled tweet-
s are treated as the gold standard, which affects
the performance of classifier. The results of bag-
of-ngram (uni/bi/tri-gram) features are not satis-
fied because the one-hot word representation can-
not capture the latent connections between words.
NBSVM and RAE perform comparably and have
3
For 3-class sentiment classification in SemEval 2013,
our re-implementation of NRC achieved 68.3%, 0.7% low-
er than NRC (69%) due to less training data.
1560
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + uni/bi/tri-gram 63.84
SVM + unigram 74.50
SVM + uni/bi/tri-gram 75.06
NBSVM 75.28
RAE 75.12
NRC (Top System in SemEval) 84.73
NRC - ngram 84.17
SSWE
u
84.98
SSWE
u
+NRC 86.58
SSWE
u
+NRC-ngram 86.48
Table 2: Macro-F1 on positive/negative classifica-
tion of tweets.
a big gap in comparison with the NRC and SSWE-
based methods. The reason is that RAE and NB-
SVM learn the representation of tweets from the
small-scale manually annotated training set, which
cannot well capture the comprehensive linguistic
phenomenons of words.
NRC implements a variety of features and
reaches 84.73% in macro-F1, verifying the impor-
tance of a better feature representation for Twit-
ter sentiment classification. We achieve 84.98%
by using only SSWE
u
as features without borrow-
ing any sentiment lexicons or hand-crafted rules.
The results indicate that SSWE
u
automatically
learns discriminative features from massive tweets
and performs comparable with the state-of-the-art
manually designed features. After concatenating
SSWE
u
with the feature set of NRC, the perfor-
mance is further improved to 86.58%. We also
compare SSWE
u
with the ngram feature by inte-
grating SSWE into NRC-ngram. The concatenated
features SSWE
u
+NRC-ngram (86.48%) outperfor-
m the original feature set of NRC (84.73%).
As a reference, we apply SSWE
u
on subjec-
tive classification of tweets, and obtain 72.17% in
macro-F1 by using only SSWE
u
as feature. Af-
ter combining SSWE
u
with the feature set of NR-
C, we improve NRC from 74.86% to 75.39% for
subjective classification.
Comparision between Different Word Embed-
ding. We compare sentiment-specific word em-
bedding (SSWE
h
, SSWE
r
, SSWE
u
) with base-
line embedding learning algorithms by only us-
ing word embedding as features for Twitter sen-
timent classification. We use the embedding of u-
nigrams, bigrams and trigrams in the experimen-
t. The embeddings of C&W (Collobert et al,
2011), word2vec
4
, WVSA (Maas et al, 2011) and
our models are trained with the same dataset and
same parameter setting. We compare with C&W
and word2vec as they have been proved effective
in many NLP tasks. The trade-off parameter of
ReEmb (Labutov and Lipson, 2013) is tuned on
the development set of SemEval 2013.
Table 3 shows the performance on the pos-
itive/negative classification of tweets
5
. ReEm-
b(C&W) and ReEmb(w2v) stand for the use
of embeddings learned from 10 million distant-
supervised tweets with C&W and word2vec, re-
spectively. Each row of Table 3 represents a word
embedding learning algorithm. Each column s-
tands for a type of embedding used to compose
features of tweets. The column uni+bi denotes the
use of unigram and bigram embedding, and the
column uni+bi+tri indicates the use of unigram,
bigram and trigram embedding.
Embedding unigram uni+bi uni+bi+tri
C&W 74.89 75.24 75.89
Word2vec 73.21 75.07 76.31
ReEmb(C&W) 75.87 ? ?
ReEmb(w2v) 75.21 ? ?
WVSA 77.04 ? ?
SSWE
h
81.33 83.16 83.37
SSWE
r
80.45 81.52 82.60
SSWE
u
83.70 84.70 84.98
Table 3: Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings.
From the first column of Table 3, we can see that
the performance of C&W and word2vec are obvi-
ously lower than sentiment-specific word embed-
dings by only using unigram embedding as fea-
tures. The reason is that C&W and word2vec do
not explicitly exploit the sentiment information of
the text, resulting in that the words with oppo-
site polarity such as good and bad are mapped
to close word vectors. When such word embed-
dings are fed as features to a Twitter sentimen-
t classifier, the discriminative ability of sentiment
words are weakened thus the classification perfor-
mance is affected. Sentiment-specific word em-
4
Available at https://code.google.com/p/word2vec/. We
utilize the Skip-gram model because it performs better than
CBOW in our experiments.
5
MVSA and ReEmb are not suitable for learning bigram
and trigram embedding because their sentiment predictor
functions only utilize the unigram embedding.
1561
beddings (SSWE
h
, SSWE
r
, SSWE
u
) effectively
distinguish words with opposite sentiment polarity
and perform best in three settings. SSWE outper-
forms MVSA by exploiting more contextual infor-
mation in the sentiment predictor function. SSWE
outperforms ReEmb by leveraging more senti-
ment information from massive distant-supervised
tweets. Among three sentiment-specific word em-
beddings, SSWE
u
captures more context informa-
tion and yields best performance. SSWE
h
and
SSWE
r
obtain comparative results.
From each row of Table 3, we can see that the
bigram and trigram embeddings consistently im-
prove the performance of Twitter sentiment classi-
fication. The underlying reason is that a phrase,
which cannot be accurately represented by uni-
gram embedding, is directly encoded into the n-
gram embedding as an idiomatic unit. A typical
case in sentiment analysis is that the composed
phrase and multiword expression may have a dif-
ferent sentiment polarity than the individual word-
s it contains, such as not [bad] and [great] deal
of (the word in the bracket has different sentiment
polarity with the ngram). A very recent study by
Mikolov et al (2013) also verified the effective-
ness of phrase embedding for analogically reason-
ing phrases.
Effect of ? in SSWE
u
We tune the hyper-
parameter ? of SSWE
u
on the development set by
using unigram embedding as features. As given
in Equation 8, ? is the weighting score of syntac-
tic loss of SSWE
u
and trades-off the syntactic and
sentiment losses. SSWE
u
is trained from 10 mil-
lion distant-supervised tweets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
?
Ma
cro
?F1
 
 
SSWEu
Figure 2: Macro-F1 of SSWE
u
on the develop-
ment set of SemEval 2013 with different ?.
Figure 2 shows the macro-F1 of SSWE
u
on pos-
itive/negative classification of tweets with differ-
ent ? on our development set. We can see that
SSWE
u
performs better when ? is in the range
of [0.5, 0.6], which balances the syntactic context
and sentiment information. The model with ?=1
stands for C&W model, which only encodes the
syntactic contexts of words. The sharp decline at
?=1 reflects the importance of sentiment informa-
tion in learning word embedding for Twitter senti-
ment classification.
Effect of Distant-supervised Data in SSWE
u
We investigate how the size of the distant-
supervised data affects the performance of SSWE
u
feature for Twitter sentiment classification. We
vary the number of distant-supervised tweets from
1 million to 12 million, increased by 1 million.
We set the ? of SSWE
u
as 0.5, according to the
experiments shown in Figure 2. Results of posi-
tive/negative classification of tweets on our devel-
opment set are given in Figure 3.
1 2 3 4 5 6 7 8 9 10 11 12
x 106
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
# of distant?supervised tweets
Mac
ro?F
1
 
 
SSWEu
Figure 3: Macro-F1 of SSWE
u
with different size
of distant-supervised data on our development set.
We can see that when more distant-supervised
tweets are added, the accuracy of SSWE
u
con-
sistently improves. The underlying reason is that
when more tweets are incorporated, the word em-
bedding is better estimated as the vocabulary size
is larger and the context and sentiment informa-
tion are richer. When we have 10 million distant-
supervised tweets, the SSWE
u
feature increases
the macro-F1 of positive/negative classification of
tweets to 82.94% on our development set. When
we have more than 10 million tweets, the per-
formance remains stable as the contexts of words
have been mostly covered.
4.2 Word Similarity of Sentiment Lexicons
The quality of SSWE has been implicitly evaluat-
ed when applied in Twitter sentiment classification
in the previous subsection. We explicitly evaluate
it in this section through word similarity in the em-
1562
bedding space for sentiment lexicons. The evalua-
tion metric is the accuracy of polarity consistency
between each sentiment word and its topN closest
words in the sentiment lexicon,
Accuracy =
?
#Lex
i=1
?
N
j=1
?(w
i
, c
ij
)
#Lex?N
(10)
where #Lex is the number of words in the senti-
ment lexicon, w
i
is the i-th word in the lexicon, c
ij
is the j-th closest word tow
i
in the lexicon with co-
sine similarity, ?(w
i
, c
ij
) is an indicator function
that is equal to 1 if w
i
and c
ij
have the same sen-
timent polarity and 0 for the opposite case. The
higher accuracy refers to a better polarity consis-
tency of words in the sentiment lexicon. We set N
as 100 in our experiment.
Experiment Setup and Datasets We utilize
the widely-used sentiment lexicons, namely M-
PQA (Wilson et al, 2005) and HL (Hu and Liu,
2004), to evaluate the quality of word embedding.
For each lexicon, we remove the words that do
not appear in the lookup table of word embedding.
We only use unigram embedding in this section
because these sentiment lexicons do not contain
phrases. The distribution of the lexicons used in
this paper is listed in Table 4.
Lexicon Positive Negative Total
HL 1,331 2,647 3,978
MPQA 1,932 2,817 4,749
Joint 1,051 2,024 3,075
Table 4: Statistics of the sentiment lexicons. Join-
t stands for the words that occur in both HL and
MPQA with the same sentiment polarity.
Results. Table 5 shows our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of
each word. Sentiment-specific word embed-
dings (SSWE
h
, SSWE
r
, SSWE
u
) outperform ex-
isting neural models (C&W, word2vec) by large
margins. SSWE
u
performs best in three lexicon-
s. SSWE
h
and SSWE
r
have comparable perfor-
mances. Experimental results further demonstrate
that sentiment-specific word embeddings are able
to capture the sentiment information of texts and
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural
Embedding HL MPQA Joint
Random 50.00 50.00 50.00
C&W 63.10 58.13 62.58
Word2vec 66.22 60.72 65.59
ReEmb(C&W) 64.81 59.76 64.09
ReEmb(w2v) 67.16 61.81 66.39
WVSA 68.14 64.07 67.12
SSWE
h
74.17 68.36 74.03
SSWE
r
73.65 68.02 73.14
SSWE
u
77.30 71.74 77.33
Table 5: Accuracy of the polarity consistency of
words in different sentiment lexicons.
models like C&W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively.
5 Conclusion
In this paper, we propose learning continuous
word representations as features for Twitter sen-
timent classification under a supervised learning
framework. We show that the word embedding
learned by traditional neural networks are not ef-
fective enough for Twitter sentiment classification.
These methods typically only model the contex-
t information of words so that they cannot dis-
tinguish words with similar context but opposite
sentiment polarity (e.g. good and bad). We learn
sentiment-specific word embedding (SSWE) by
integrating the sentiment information into the loss
functions of three neural networks. We train SS-
WE with massive distant-supervised tweets select-
ed by positive and negative emoticons. The ef-
fectiveness of SSWE has been implicitly evaluat-
ed by using it as features in sentiment classifica-
tion on the benchmark dataset in SemEval 2013,
and explicitly verified by measuring word similar-
ity in the embedding space for sentiment lexicon-
s. Our unified model combining syntactic context
of words and sentiment information of sentences
yields the best performance in both experiments.
Acknowledgments
We thank Yajuan Duan, Shujie Liu, Zhenghua Li,
Li Dong, Hong Sun and Lanjun Zhou for their
great help. This research was partly supported
by National Natural Science Foundation of China
(No.61133012, No.61273321, No.61300113).
1563
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy da-
ta. In Proceedings of International Conference on
Computational Linguistics, pages 36?44.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
Yoshua Bengio. 2013. Deep learning of represen-
tations: Looking forward. arXiv preprint arX-
iv:1305.0445.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375?382.
Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-
oufandeh. 2012. Sentiment classification with su-
pervised sequence embedding. In Machine Learn-
ing and Knowledge Discovery in Databases, pages
159?174. Springer.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Annual Meeting of the Association for
Computational Linguistics, volume 7.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of International Con-
ference on Computational Linguistics, pages 241?
249.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. E-
moticon smoothed language models for twitter sen-
timent analysis. In The Association for the Advance-
ment of Artificial Intelligence.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
1564
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-
do, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositionali-
ty. The Conference on Neural Information Process-
ing Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Andriy Mnih and Geoffrey E Hinton. 2009. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the International Conference on Ma-
chine Learning, pages 129?136.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011c. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Annual Meeting of the
Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmenta-
tion and pos tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 647?657.
1565
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 13?15,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Incremental Structured Prediction Using a Global Learning and
Beam-Search Framework
Yue Zhang
?
, Meishan Zhang
?
, Ting Liu
?
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, tliu}@ir.hit.edu.cn
Abstract
This tutorial discusses a framework for in-
cremental left-to-right structured predica-
tion, which makes use of global discrimi-
native learning and beam-search decoding.
The method has been applied to a wide
range of NLP tasks in recent years, and
achieved competitive accuracies and effi-
ciencies. We give an introduction to the
algorithms and efficient implementations,
and discuss their applications to a range of
NLP tasks.
1 Introduction
This tutorial discusses a framework of online
global discriminative learning and beam-search
decoding for syntactic processing (Zhang and
Clark, 2011b), which has recently been applied
to a wide variety of natural language processing
(NLP) tasks, including word segmentation (Zhang
and Clark, 2007), dependency parsing (Zhang and
Clark, 2008b; Huang and Sagae, 2010; Zhang and
Nivre, 2011; Bohnet and Kuhn, 2012), context
free grammar (CFG) parsing (Collins and Roark,
2004; Zhang and Clark, 2009; Zhu et al., 2013),
combinational categorial grammar (CCG) parsing
(Zhang and Clark, 2011a; Xu et al., 2014) and
machine translation (Liu, 2013), achieving state-
of-the-art accuracies and efficiencies. In addition,
due to its high efficiencies, it has also been ap-
plied to a range of joint structural problems, such
as joint segmentation and POS-tagging (Zhang
and Clark, 2008a; Zhang and Clark, 2010), joint
POS-tagging and dependency parsing (Hatori et
al., 2011; Bohnet and Nivre, 2012), joint mor-
phological analysis, POS-tagging and dependency
parsing (Bohnet et al., 2013), and joint segmenta-
tion, POS-tagging and parsing (Zhang et al., 2013;
Zhang et al., 2014).
In addition to the aforementioned tasks, the
framework can be applied to all structural pre-
diction tasks for which the output can be con-
structed using an incremental process. The advan-
tage of this framework is two-fold. First, beam-
search enables highly efficient decoding, which
typically has linear time complexity, depending on
the incremental process. Second, free from DP-
style constraints and Markov-style independence
assumptions, the framework allows arbitrary fea-
tures to be defined to capture structural patterns.
In addition to feature advantages, the high accura-
cies of this framework are also enabled by direct
interactions between learning and search (Daum?e
III and Marcu, 2005; Huang et al., 2012; Zhang
and Nivre, 2012).
2 Tutorial Overview
In this tutorial, we make an introduction to the
framework, illustrating how it can be applied to
a range of NLP problems, giving theoretical dis-
cussions and demonstrating a software implemen-
tation. We start with a detailed introduction of
the framework, describing the averaged percep-
tron algorithm (Collins, 2002) and its efficient im-
plementation issues (Zhang and Clark, 2007), as
well as beam-search and the early-update strategy
(Collins and Roark, 2004). We then illustrate how
the framework can be applied to NLP tasks, in-
cluding word segmentation, joint segmentation &
POS-tagging, labeled and unlabeled dependency
parsing, joint POS-tagging and dependency pars-
ing, CFG parsing, CCG parsing, and joint segmen-
tation, POS-tagging and parsing. In each case, we
illustrate how the task is turned into an incremen-
tal left-to-right output-building process, and how
rich features are defined to give competitive accu-
racies. These examples can serve as guidance in
applying the framework to other structural predic-
tion tasks.
In the second part of the tutorial, we give
some analysis on why the framework is effective.
We discuss several alternative learning algorithms,
13
and compare beam-search with greedy search on
dependency parsing. We show that accuracy bene-
fits from interaction between learning and search.
Finally, the tutorial concludes with an introduction
to ZPar, an open source toolkit that provides op-
timized C++ implementations of of all the above
tasks.
3 Outline
1 Introduction (0.5 hours)
1.1 An overview of the syntactic processing
framework and its applications
1.2 An introduction to the beam-search
framework and comparison to dynamic
programming
1.3 Algorithm in details
1.3.1 Online discriminative learning using
the perceptron
1.3.2 Beam-search decoding
1.3.3 The integrated framework
2 Applications (1.25 hours)
2.1 Overview
2.2 Word segmentation
2.3 Joint segmentation and POS-tagging
2.4 Dependency parsing
2.5 Context free grammar parsing
2.6 Combinatory categorial grammar pars-
ing
2.7 Joint segmentation, POS-tagging and
parsing
3 Analysis of the framework (0.75 hours)
3.1 The influence of global learning
3.2 The influence of beam-search
3.3 Benefits from the combination
3.4 Related discussions
4 The ZPar software tool (0.5 hours)
4 About the Presenters
Yue Zhang is an Assistant Professor at Singapore
University of Technology and Design (SUTD).
Before joining SUTD in 2012, he worked as a
postdoctoral research associate at University of
Cambridge. He received his PhD and MSc degrees
from University of Oxford, and undergraduate de-
gree from Tsinghua University, China. Dr Zhang?s
research interest includes natural language pars-
ing, natural language generation, machine trans-
lation and machine learning.
Meishan Zhang is a fifth-year Phd candidate at
Research Center for Social Computing and Infor-
mation Retrieval, Harbin Institute of Technology,
China (HIT-SCIR). His research interest includes
Chinese morphological and syntactic parsing, se-
mantic representation and parsing, joint modelling
and machine learning.
Ting Liu is a professor at HIT-SCIR. His re-
search interest includes social computing, infor-
mation retrieval and natural language processing.
References
Bernd Bohnet and Jonas Kuhn. 2012. The best of
bothworlds ? a graph-based completion model for
transition-based parsers. In Proceedings of EACL,
pages 77?87, Avignon, France, April. Association
for Computational Linguistics.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 1455?1465, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Richard Farkas, Filip Ginter, and Jan Hajic. 2013.
Joint morphological and syntactic analysis for richly
inflected languages. Transactions of the Association
for Computational Linguistics, 1:415?428.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL 2004, Main Volume, pages 111?118,
Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8. Association for Computa-
tional Linguistics, July.
Hal Daum?e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In International
Conference on Machine Learning (ICML), Bonn,
Germany.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings
of IJCNLP, pages 1216?1224, Chiang Mai, Thai-
land, November. Asian Federation of Natural Lan-
guage Processing.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010, pages 1077?1086, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
14
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL 2012, pages 142?151, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Yang Liu. 2013. A shift-reduce parsing algorithm for
phrase-based string-to-dependency translation. In
Proceedings of the ACL, pages 1?10, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Wenduan Xu, Yue Zhang, and Stephen Clark. 2014.
Shift-reduce ccg parsing with a dependency model.
In Proceedings of the ACL.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of ACL 2007, pages 840?847, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, pages 888?
896, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2008b. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
Proceedings of EMNLP, pages 562?571, Honolulu,
Hawaii, October. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the chinese treebank using a global
discriminative model. In Proceedings of IWPT?09,
pages 162?171, Paris, France, October. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging us-
ing a single discriminative model. In Proceedings
of EMNLP 2010, pages 843?852, Cambridge, MA,
October. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011a. Shift-reduce
ccg parsing. In Proceedings of ACL 2011, pages
683?692, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011b. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL 2011, pages 188?193, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of ACL 2013.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Character-level chinese dependency
parsing. In Proceedings of the ACL.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of ACL
2013.
15
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407?410,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
HIT-CIR: An Unsupervised WSD System Based on Domain Most
Frequent Sense Estimation
Yuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng Li
Harbin Institute of Technolgy
Harbin, Heilongjiang, PRC
yhguo@ir.hit.edu.cn
Abstract
This paper presents an unsupervised sys-
tem for all-word domain specific word
sense disambiguation task. This system
tags target word with the most frequent
sense which is estimated using a thesaurus
and the word distribution information in
the domain. The thesaurus is automati-
cally constructed from bilingual parallel
corpus using paraphrase technique. The
recall of this system is 43.5% on SemEval-
2 task 17 English data set.
1 Introduction
Tagging polysemous word with its most frequent
sense (MFS) is a popular back-off heuristic in
word sense disambiguation (WSD) systems when
the training data is inadequate. In past evalua-
tions, MFS from WordNet performed even bet-
ter than most of the unsupervised systems (Snyder
and Palmer, 2004; Navigli et al, 2007).
MFS is usually obtained from a large scale
sense tagged corpus, such as SemCor (Miller et al,
1994). However, some polysemous words have
different MFS in different domains. For example,
in the Koeling et al (2005) corpus, target word
coach means ?manager? mostly in the SPORTS
domain but means ?bus? mostly in the FINANCE
domain. So when the MFS is applied to specific
domains, it needs to be re-estimated.
McCarthy et al (2007) proposed an unsuper-
vised predominant word sense acquisition method
which obtains domain specific MFS without sense
tagged corpus. In their method, a thesaurus, in
which words are connected with their distribu-
tional similarity, is constructed from the domain
raw text. Word senses are ranked by their preva-
lence score which is calculated using the thesaurus
and the sense inventory.
In this paper, we propose another way to con-
struct the thesaurus. We use statistical machine
Figure 1: The architecture of HIT-CIR
translation (SMT) techniques to extract paraphrase
pairs from bilingual parallel text. In this way, we
avoid calculating similarities between every pair
of words and could find semantic similar words or
compounds which have dissimilar distributions.
Our system is comprised of two parts: the word
sense ranking part and the word sense tagging part.
Senses are ranked according to their prevalence
score in the target domain, and the predominant
sense is used to tag the occurrences of the target
word in the test data. The architecture of this sys-
tem is shown in Figure 1.
The word sense ranking part includes following
steps.
1. Tag the POS of the background text, count
the word frequency in each POS, and get the
polysemous word list of the POS.
2. Using SMT techniques to extract phrase table
407
Figure 2: Word sense ranking for the noun backbone
from the bilingual corpus. Extract the para-
phrases (called as neighbor words) with the
phrase table for each word in the polysemous
word list.
3. Calculate the prevalence score of each sense
of the target words, rank the senses with the
score and obtain the predominant sense.
We applied our system on the English data set
of SemEval-2 specific domain WSD task. This
task is an all word WSD task in the environ-
mental domain. We employed the domain back-
ground raw text provided by the task organizer as
well as the English WordNet 3.0 (Fellbaum, 1998)
and the English-Spanish parallel corpus from Eu-
roparl (Koehn, 2005).
This paper is organized as follows. Section 2
introduces how to rank word senses. Section 3
presents how to obtain the most related words of
the target words. We describe the system settings
in Section 4 and offer some discussions in Sec-
tion 5.
2 Word Sense Ranking
In our method, word senses are ranked according
to their prevalence score in the specific domain.
According to the assumption of McCarthy et al
(2007), the prevalence score is affected by the fol-
lowing two factors: (1) The relatedness score be-
tween a given sense of the target word and the
target word?s neighbor word. (2) The similarity
between the target word and its neighbor word.
In addition, we add another factor, (3) the impor-
tance of the neighbor word in the specific domain.
In this paper, ?neighbor words? means the words
which are most semantically similar to the target
word.
Figure 2 illustrates the word sense ranking pro-
cess of noun backbone. The contribution of a
neighbor word to a given word sense is measured
by the similarity between them and weighted by
the importance of the neighbor word in the tar-
get domain and the relatedness between the neigh-
bor word and the target word. Sum up the con-
tributions of each neighbor words, and we get the
prevalence score of the word sense.
Formally, the prevalence score of sense s
i
of a
target word w is assigned as follows:
ps(w, s
i
) =
?
n
j
?N
w
rs(w, n
j
) ? ns(s
i
, n
j
) ? dw(n
j
)
(1)
where
ns(s
i
, n
j
) =
sss(s
i
, n
j
)
?
s
i
?
?senses(w)
sss(s
i
?
, n
j
)
, (2)
sss(s
i
, n
j
) = max
s
x
?senses(n
j
)
sss
?
(s
i
, s
x
). (3)
rs(w, n
j
) is the relatedness score between w and
a neighbor word n
j
. N
w
= {n
1
, n
2
, . . . , n
k
}
is the top k relatedness score neighbor word set.
ns(s
i
, n
j
) is the normalized form of the sense sim-
ilarity score between sense s
i
and the neighbor
word n
j
(i.e. sss(s
i
, n
j
)). We define this score
with the maximum WordNet similarity score be-
tween s
i
and the senses of n
j
(i.e. sss
?
(s
i
, n
j
)).
In our system, lesk algorithm is used to measure
the sense similarity score between word senses.
408
Figure 3: Finding the neighbor words of noun backbone
The similarity of this algorithm is the count of
the number of overlap words in the gloss or the
definition of the senses (Banerjee and Pedersen,
2002). The domain importance weight dw(n
j
) is
assigned with the count of n
j
in the domain back-
ground corpus. For the neighbor word that does
not occur in the domain background text, we use
the add-one strategy. We will describe how to ob-
tain n
j
and rs in Section 3.
3 Thesaurus Construction
The neighbor words of the target word as well as
the relatedness score are obtained by extracting
paraphrases from bilingual parallel texts. When
a word is translated from source language to tar-
get language and then translated back to the source
language, the final translation may have the same
meaning to the original word but with different ex-
pressions (e.g. different word or compound). The
translation in the same language could be viewed
as a paraphrase term or, at least, related term of the
original word.
For example, in Figure 3, English noun back-
bone can be translated to columna, columna verte-
bral, pilar and convicciones etc. in Spanish, and
these words also have other relevant translations
in English, such as vertebral column, column, pil-
lar and convictions etc., which are semantically re-
lated to the target word backbone.
We use a statistical machine translation sys-
tem to calculate the translation probability from
English to another language (called as pivot lan-
guage) as well as the translation probability from
that language to English. By multiplying these
two probabilities, we get a paraphrase probabil-
ity. This method was defined in (Bannard and
Callison-Burch, 2005).
In our system, we choose the top k paraphrases
as the neighbor words of the target word, which
have the highest paraphrase probability. Note that
there are two directions of the paraphrase, from
target word to its neighbor word and from the
neighbor word to the target word. We choose
the paraphrase score of the former direction as
the relatedness score (rs). Because the higher
of the score in this direction, the target word is
more likely paraphrased to that neighbor word,
and hence the prevalence of the relevant target
word sense will be higher than other senses. For-
mally, the relatedness score is given by
rs(w, n
j
) =
?
f
p(f |w)p(n
j
|f), (4)
where f is the pivot language word.
We use the English-Spanish parallel text from
Europarl (Koehn, 2005). We choose Spanish as
the pivot language because in the both directions
the BLEU score of the translation between English
and Spanish is relatively higher than other English
and other languages (Koehn, 2005).
4 Data set and System Settings
The organizers of the SemEval-2 specific domain
WSD task provide no training data but raw back-
ground data in the environmental domain. The En-
glish background data is obtained from the offi-
cial web site of World Wide Fund (WWF), Euro-
pean Centre for Nature Conservation (ECNC), Eu-
ropean Commission and the United Nations Eco-
nomic Commission for Europe (UNECE). The
size of the raw text is around 15.5MB after sim-
ple text cleaning. The test data is from WWF and
ECNC, and contains 1398 occurrence of 436 tar-
get words.
For the implementation, we used bpos (Shen et
al., 2007) for the POS tagging. The maximum
409
number of the neighbor word of each target word k
was set to 50. We employed Giza++
1
and Moses
2
to get the phrase table from the bilingual paral-
lel corpus. TheWordNet::Similarity package
3
was
applied for the implement of the lesk word sense
similarity algorithm.
For the target word that is not in the polysemous
word list, we use the MFS from WordNet as the
back-off method.
5 Discussion and Future Work
The recall of our system is 43.5%, which is lower
than that of the MFS baseline, 50.5% (Agirre et
al., 2010). The baseline uses the most frequent
sense from the SemCor corpus (i.e. the MFS of
WordNet). This means that for some target words,
the MFS from SemCor is better than the domain
MFS we estimated in the environmental domain.
In the future, we will analysis errors in detail to
find the effects of the domain on the MFS.
For the domain specific task, it is better to use
parallel text in the domain of the test data in our
method. However, we didn?t find any available
parallel text in the environmental domain yet. In
the future, we will try some parallel corpus acqui-
sition techniques to obtain relevant corpus for en-
vironmental domain for our method.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
1
http://www.fjoch.com/GIZA++.html
2
http://www.statmt.org/moses/
3
http://wn-similarity.sourceforge.net/
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 597?
604, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 419?426, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590, December.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1994. A semantic concordance. In Proc. ARPA
Human Language Technology Workshop ?93, pages
303?308, Princeton, NJ, March. distributed as Hu-
man Language Technology by San Mateo, CA: Mor-
gan Kaufmann Publishers.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30?35, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
410
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 378?384,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 5: Chinese Semantic Dependency Parsing
Wanxiang Che?, Meishan Zhang?, Yanqiu Shao?, Ting Liu?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{car, mszhang, tliu}@ir.hit.edu.cn
?Beijing City University, China
yqshao@bcu.edu.cn
Abstract
The paper presents the SemEval-2012 Shared
Task 5: Chinese Semantic Dependency Pars-
ing. The goal of this task is to identify the de-
pendency structure of Chinese sentences from
the semantic view. We firstly introduce the
motivation of providing Chinese semantic de-
pendency parsing task, and then describe the
task in detail including data preparation, data
format, task evaluation, and so on. Over ten
thousand sentences were labeled for partici-
pants to train and evaluate their systems. At
last, we briefly describe the submitted systems
and analyze these results.
1 Introduction
Semantic analysis is a long-term goal of Natural
Language Processing, and as such, has been re-
searched for several decades. A number of tasks
for encoding semantic information have been devel-
oped over the years, such as entity type recognition
and word sense disambiguation. Recently, sentence-
level semantics ? in particular, semantic role label-
ing ? has received increasing attention. However,
some problems concerning the semantic representa-
tion method used in semantic role labeling continue
to exist (Xue and Palmer, 2005).
1. Semantic role labeling only considers
predicate-argument relations and ignores
the semantic relations between a noun and its
modifier.
2. The meaning of semantic roles is related to spe-
cial predicates. Therefore, there are infinite se-
mantic roles to be learned, as the number of
predicates is not fixed. Although the Prop-
Bank (Xue and Palmer, 2003) normalizes these
semantic roles into certain symbols, such as
Arg0-Arg5, the same symbol can have different
semantic meanings when paired with different
predicates, and thus cannot be learned well.
Semantic dependency parsing is therefore pro-
posed to solve the two problems above for Chinese.
Firstly, the proposed method analyzes all the words?
semantic roles in a sentence and specifies the con-
crete semantic relation of each word pair. After-
ward, this work analyzes and summarizes all the
possible semantic roles, obtaining over 100 of them,
and then uses these semantic roles to specify the se-
mantic relation for each word pair.
Dependency parsing (Ku?bler et al, 2009) is based
on dependency grammar. It has several advantages,
such as concise formalization, easy comprehension,
high efficiency, and so on. Dependency parsing
has been studied intensively in recent decades, with
most related work focusing on syntactic structure.
Many research papers on Chinese linguistics demon-
strate the remarkable difference between semantics
and syntax (Jin, 2001; Zhou and Zhang, 2003).
Chinese is a meaning-combined language with very
flexible syntax, and semantics are more stable than
syntax. The word is the basic unit of semantics,
and the structure and meaning of a sentence consists
mainly of a series of semantic dependencies between
individual words (Li et al, 2003). Thus, a reason-
able endeavor is to exploit dependency parsing for
semantic analysis of Chinese languages. Figure 1
shows an example of Chinese semantic dependency
parsing.
378
??International ??Monetary ??Fund ??organization ??turn down ?for ??global ??economy ??increasing ?of ??prediction
d-genetived-restrictive d-restrictive agent prep-dependd-genetive d-domain aux-depend
d-restrictivecontent
root
Figure 1: An example of Chinese Semantic Dependency Parsing.
Figure 1 shows that Chinese semantic dependency
parsing looks very similar to traditional syntax-
dominated dependency parsing. Below is a compar-
ison between the two tasks, dealing with three main
points:
1. Semantic relations are more fine-grained than
syntactic ones: the syntactic subject can either
be the agent or experiencer, and the syntactic
object can be the content, patient, possession,
and so on. On the whole, the number of seman-
tic relations is at least twice that of syntactic
relations.
2. Semantic dependency parsing builds the depen-
dency structure of a sentence in terms of se-
mantics, and the word pairs of a dependency
should have a direct semantic relation. This
criterion determines many sizeable differences
between semantics and syntax, especially in
phrases formed by ?XP+DEG?, ?XP+DEV?
and prepositional phrases. For example, in ??
? ? ??? (beautiful country), the head of
???? (beautiful) is ???? (country) in se-
mantic dependency parsing, whereas the head
is ??? (de) in syntax dependency parsing.
3. Semantic relations are independent of position.
For example, in ??? ? ??? (the air is
contaminated) and ??? ? ??? (contami-
nate the air), the patient ???? (the air) can be
before or behind a predicate ???? (contami-
nate).
The rest of the paper is organized as follows. Sec-
tion 2 gives a short overview of data annotation.
Section 3 focuses on the task description. Section
4 describes the participant systems. Section 5 com-
pares and analyzes the results. Finally, Section 6
concludes the paper.
2 Data Annotation
2.1 Corpus Section
10,068 sentences were selected from the Penn Chi-
nese Treebank 6.01 (Xue et al, 2005) (1-121, 1001-
1078, 1100-1151) as the raw corpus from which to
create the Chinese Semantic Dependency Parsing
corpus. These sentences were chosen for the anno-
tation for three reasons. First, gold syntactic depen-
dency structures can be of great help in semantic de-
pendency annotation, as syntactic dependency arcs
are often consistent with semantic ones. Second, the
semantic role labels in PropBank2 can be very use-
ful in the present annotation work. Third, the gold
word segmentation and Part-Of-Speech can be used
as the annotation input in this work.
2.2 Semantic Relations
The semantic relations in the prepared Chinese se-
mantic dependency parsing corpus came mostly
from HowNet3 (Dong and Dong, 2006), a fa-
mous Chinese semantic thesaurus. We also referred
to other sources. Aside from the relations from
HowNet, we defined two kinds of new relations: re-
verse relations and indirect relations. When a verb
modifies a noun, the relation between them is a re-
verse relation, and r-XXX is used to indicate this
kind of relation. For instance, in ???????
?? (the little boy who is playing basketball), the se-
mantic relation between the head word ???? (boy)
1http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalog\\Id=LDC2007T36
2http://verbs.colorado.edu/chinese/cpb/
3http://www.keenage.com/
379
and ??? (playing) is the r-agent. When a verbal
noun is the head word, the relation between it and
the modifier is the indirect relation j-XXX. For in-
stance, in ?????? (business management), the
head word is ???? (management) and the modifier
is ???? (business), their relation is j-patient.
Finally, we defined 84 single-level semantic re-
lations. The number of multi-level semantic rela-
tions that actually appear in the labeled corpus in
this work is 39.
Table 1 summarizes all of the semantic relations
used for annotation.
2.3 Annotation Flow
Our corpus annotation flow can be divided into the
following steps.
1. Conversion of the sentences? constituent struc-
tures into dependency structures according to
a set of rules similar with those used by the
syntactic community to find the head of a
phrase (Collins, 1999).
2. Labeling of the semantic relations for each de-
pendency relation according to another set of
rules using the functional tags in the Penn Chi-
nese Treebank and the semantic roles in the
Chinese PropBank.
3. Six human annotators are asked to check and
adjust the structure and semantic relation errors
introduced in Step 2.
The first two steps were performed automatically
using rules. A high accuracy may be achieved with
dependency structures when semantic labels are not
considered. However, accuracy declines remarkably
when the semantic label is considered. Unlabeled
Attachment Score (UAS) and Labeled Attachment
Score (LAS) can be used to evaluate the perfor-
mance of the automatic conversion. Table 2 gives
the detailed results.
UAS LAS
Conversion Result 90.53 57.38
Table 2: Accuracy after conversion from gold ProbBank.
3 Task Description
3.1 Corpus Statistics
We annotated 10,068 sentences from the Penn Chi-
nese TreeBank for Semantic Dependency Parsing,
and these sentences were divided into training, de-
velopment, and test sections. Table 3 gives the de-
tailed statistical information of the three sections.
Data Set CTB files # sent. # words.
1-10; 36-65;81-121; 8301
Training 1001-1078; 250311
1100-1119;
1126-1140
Devel 66-80; 1120-1125 534 15329
Test 11-35; 1141-1151 1233 34311
Total 1-121; 1001-1078 10068 299951
1100-1151
Table 3: Statistics of training, development and test data.
3.2 Data Format
The data format is identical to that of a syntactic de-
pendency parsing shared task. All the sentences are
in one text file, with each sentence separated by a
blank line. Each sentence consists of one or more to-
kens, and each token is represented on one line con-
sisting of 10 fields. Buchholz and Marsi (2006) pro-
vide more detailed information on the format. Fields
are separated from each other by a tab. Only five of
the 10 fields are used: token id, form, pos tagger,
head, and deprel. Head denotes the semantic depen-
dency of each word, and deprel denotes the corre-
sponding semantic relations of the dependency. In
the data, the lemma column is filled with the form
and the cpostag column with the postag. Figure 2
shows an example.
3.3 Evaluation Method
LAS, which is a method widely used in syntactic
dependency parsing, is used to evaluate the perfor-
mance of the semantic dependency parsing system.
LAS is the proportion of ?scoring? tokens assigned
to both the correct head and correct semantic depen-
dency relation. Punctuation is disregarded during
the evaluation process. UAS is another important
indicator, as it reflects the accuracy of the semantic
dependency structure.
380
Main Semantic Roles
Subject Roles agent, experiencer, causer, possessor, existent, whole, relevant
Object Roles isa, content, possession, patient, OfPart, beneficiary, contrast,
partner, basis, cause, cost, scope, concerning
Auxiliary Semantic Roles
Time Roles duration, TimeFin, TimeIni, time, TimeAdv
Location and State Roles LocationFin, LocationIni, LocationThru, StateFin, state,
StateIni, direction, distance, location
Others Verb Modifiers accompaniment, succeeding, frequency, instrument, material,
means, angle, times, sequence, sequence-p, negation, degree,
modal, emphasis, manner, aspect, comment
Attribute Roles
Direct modifiers d-genetive, d-category, d-member, d-domain, d-quantity-p, d-
quantity, d-deno-p, d-deno, d-host, d-TimePhrase, d-LocPhrase,
d-InstPhrase, d-attribute, d-restrictive, d-material, d-content, d-
sequence, d-sequence-p, qp-mod
Verb Phrase r-{Main Semantic Roles}, eg: r-agent, r-patient, r-possessor
Verb Ellipsis c-{Main Semantic Roles}, eg: c-agent, c-content, c-patient
Noun as Predication j-{Main Semantic Roles}, eg: j-agent, j-patient, j-target
Syntactic Roles and Others
Syntactic Roles s-cause, s-concession, s-condition, s-coordinate, s-or, s-
progression, s-besides, s-succession, s-purpose, s-measure, s-
abandonment, s-preference, s-summary, s-recount, s-concerning,
s-result
Others aux-depend, prep-depend, PU, ROOT
Table 1: Semantic Relations defined for Chinese Semantic Dependency Parsing.
ID FORM LEMMA CPOS PPOS FEAT HEAD REL PHEAD PREL
1 ??? ??? NR NR 2 agent
2 ? ? VV VV 0 ROOT
3 ?? ?? NR NR 4 d-genetive
4 ?? ?? NN NN 7 s-coordinate
5 ? ? CC CC 7 aux-depend
6 ?? ?? NR NR 7 d-genetive
7 ?? ?? NN NN 2 content
Figure 2: Data format of the Chinese Semantic Dependency Parsing corpus.
381
4 Participating Systems
Nine organizations were registered to participate in
the Chinese Semantic Dependency Parsing task. Fi-
nally, nine systems were received from five different
participating teams. These systems are as follows:
1. Zhou Qiaoli-1, Zhou Qiaoli-2, Zhou Qiaoli-3
These three systems propose a divide-and-
conquer strategy for semantic dependency
parsing. The Semantic Role (SR) phrases are
identified (Cai et al, 2011) and then replaced
by their head or the SR of the head. The orig-
inal sentence is thus divided into two types of
parts that can be parsed separately. The first
type is SR phrase parsing, and the second in-
volves the replacement of SR phrases with ei-
ther their head or the SR of the head. Finally,
the paper takes a graph-based parser (Li et al,
2011) as the semantic dependency parser for all
parts. These three systems differ in their phrase
identification strategies.
2. NJU-Parser-1, NJU-Parser-2
The NJU-Parser is based on the state-of-the-
art MSTParser (McDonald, 2006). NJU-Parser
applies three methods to enhance semantic de-
pendency parsing. First, sentences are split
into sub-sentences using commas and semi-
colons: (a) sentences are split using only com-
mas and semicolons, as in the primary sys-
tem, and (b) classifiers are used to determine
whether a comma or semicolon should be used
to split the sentence. Second, the last character
in a Chinese word is extracted as the lemma,
since it usually contains the main sense or se-
mantic class. Third, the multilevel-label is in-
troduced into the semantic relation, for exam-
ple, the r-{Main Semantic Roles}, with NJU-
Parser exploiting special strategies to handle it.
However, this third method does not show pos-
itive performance.
3. Zhijun Wu-1
This system extends the second-order of the
MSTParser by adding third-order features, and
then applying this model to Chinese semantic
dependency parsing. In contrast to Koo and
Collins (2010) this system does not implement
the third-order model using dynamic program-
ming, as it requires O(n4) time. It first first ob-
tained the K-best results of second-order mod-
els and then added the third-order features into
the results.
4. ICT-1
The ICT semantic dependency parser employs
a system-combining strategy to obtain the de-
pendency structure and then uses the classifier
from Le Zhang?s Maximum Entropy Model-
ing Toolkit4 to predict the semantic relation for
each dependency. The system-combining strat-
egy involves three steps:
? Parsing each sentence using Nivre?s arc
standard, Nivre?s arc eager (Nivre and
Nilsson, 2005; Nivre, 2008), and Liang?s
dynamic algorithm (Huang and Sagae,
2010);
? Combining parses given by the three
parsers into a weighted directed graph;
? Using the Chu-Liu-Edmonds algorithm to
search for the final parse for each sen-
tence.
5. Giuseppe Attardi-SVM-1-R, Giuseppe Attardi-
SVM-1-rev
We didn?t receive the system description of
these two systems.
5 Results & Analysis
LAS is the main evaluation metric in Chinese Se-
mantic Dependency Parsing, whereas UAS is the
secondary metric. Table 4 shows the results for these
two indicators in all participating systems.
As shown in Table 4, the Zhou Qiaoli-3 system
achieved the best results with LAS of 61.84. The
LAS values of top systems are very closely. We per-
formed significance tests5 for top six results. Table
5 shows the results , from which we can see that
the performances of top five results are comparative
(p > 0.1) and the rank sixth system is significantly
(p < 10?5) worse than top five results.
4http://homepages.inf.ed.ac.uk/s0450736/
maxenttoolkit.html
5http://www.cis.upenn.edu/?dbikel/
download/compare.pl
382
NJU-Parser-2 NJU-Parser-1 Zhijun Wu-1 Zhou Qiaoli-1 Zhou Qiaoli-2
Zhou Qiaoli-3 ? ? ? ? >
NJU-Parser-2 ? ? ? ? >
NJU-Parser-1 ? ? ? ? >
Zhijun Wu-1 ? ? ? ? >
Zhou Qiaoli-1 ? ? ? ? >
Table 5: Significance tests of the top five systems. ? denotes that the two systems are comparable (p > 0.1), and >
means the system of this row is significantly (p < 10?5) better than the system of this column.
System LAS UAS
Zhou Qiaoli-3 61.84 80.60
NJU-Parser-2 61.64 80.29
NJU-Parser-1 61.63 80.35
Zhijun Wu-1 61.58 80.64
Zhou Qiaoli-1 61.15 80.41
Zhou Qiaoli-2 57.55 78.55
ICT-1 56.31 73.20
Giuseppe Attardi-SVM-1-R 44.46 60.83
Giuseppe Attardi-SVM-1-rev 21.86 40.47
Average 54.22 72.82
Table 4: Results of the submitted systems.
The average LAS for all systems was 54.22.
Chinese Semantic Dependency Parsing performed
much more poorly than Chinese Syntactic Depen-
dency Parsing due to the increased complexity
brought about by the greater number of semantic re-
lations compared with syntactic relations, as well as
greater difficulty in classifying semantic relations.
In general, all the systems employed the tradi-
tional syntax-dominated dependency parsing frame-
works. Some new methods were proposed for
this task. Zhou Qiaoli?s systems first identified
the semantic role phrase in a sentence, and then
employed graph-based dependency parsing to ana-
lyze the semantic structure of the sentence. NJU-
Parser first split the sentence into sub-sentences,
then trained and parsed the sentence based on these
sub-sentences; this was shown to perform well. In
addition, ensemble models were also proposed to
solve the task using ICT systems.
6 Conclusion
We described the Chinese Semantic Dependency
Parsing task for SemEval-2012, which is designed to
parse the semantic structures of Chinese sentences.
Nine results were submitted by five organizations,
with the best result garnering an LAS score of 61.84,
which is far below the performance of Chinese Syn-
tax. This demonstrates that further research on the
structure of Chinese Semantics is needed.
In the future, we will check and improve the anno-
tation standards while building a large, high-quality
corpus for further Chinese semantic research.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This work was supported by Na-
tional Natural Science Foundation of China (NSFC)
via grant 61133012 and 61170144, and the Na-
tional ?863? Leading Technology Research Project
via grant 2012AA011102.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Dongfeng Cai, Ling Zhang, Qiaoli Zhou, and Yue Zhao.
2011. A collocation based approach for prepositional
phrase identification. IEEE NLPKE.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Pennsyl-
vania University.
Zhendong Dong and Qiang Dong. 2006. Hownet And the
Computation of Meaning. World Scientific Publishing
Co., Inc., River Edge, NJ, USA.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
383
Uppsala, Sweden, July. Association for Computational
Linguistics.
Guangjin Jin. 2001. Theory of modern Chinese verb se-
mantic computation. Beijing University Press.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the ACL, number July, pages 1?11.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. In Synthesis Lectures on
Human Language Technologies.
Mingqin Li, Juanzi Li, Zhendong Dong, Zuoying Wang,
and Dajin Lu. 2003. Building a large chinese corpus
annotated with semantic dependency. In Proceedings
of the second SIGHAN workshop on Chinese language
processing - Volume 17, SIGHAN ?03, pages 84?91,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1180?
1191, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Nianwen Xue and Martha Palmer. 2003. Annotating
the propositions in the penn chinese treebank. In Pro-
ceedings of the Second SIGHAN Workshop on Chinese
Language Processing.
Nianwen Xue and Martha Palmer. 2005. Automatic se-
mantic role labeling for chinese verbs. In Proceedings
of the 19th International Joint Conference on Artificial
Intelligence.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Guoguang Zhou and Linlin Zhang. 2003. The theory
and method of modern Chinese grammar. Guangdong
Higher Education Press.
384
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 208?212,
Dublin, Ireland, August 23-24, 2014.
Coooolll: A Deep Learning System for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Bing Qin
?
, Ting Liu
?
, Ming Zhou
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we develop a deep learn-
ing system for message-level Twitter sen-
timent classification. Among the 45 sub-
mitted systems including the SemEval
2013 participants, our system (Coooolll)
is ranked 2nd on the Twitter2014 test set
of SemEval 2014 Task 9. Coooolll is
built in a supervised learning framework
by concatenating the sentiment-specific
word embedding (SSWE) features with
the state-of-the-art hand-crafted features.
We develop a neural network with hybrid
loss function
1
to learn SSWE, which en-
codes the sentiment information of tweets
in the continuous representation of words.
To obtain large-scale training corpora, we
train SSWE from 10M tweets collected by
positive and negative emoticons, without
any manual annotation. Our system can
be easily re-implemented with the publicly
available sentiment-specific word embed-
ding.
1 Introduction
Twitter sentiment classification aims to classify
the sentiment polarity of a tweet as positive, nega-
tive or neutral (Jiang et al., 2011; Hu et al., 2013;
Dong et al., 2014). The majority of existing ap-
proaches follow Pang et al. (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
?
This work was partly done when the first author was
visiting Microsoft Research.
1
This is one of the three sentiment-specific word embed-
ding learning algorithms proposed in Tang et al. (2014).
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
designing effective features to obtain better clas-
sification performance (Pang and Lee, 2008; Liu,
2012; Feldman, 2013). For example, Mohammad
et al. (2013) implement diverse sentiment lexicons
and a variety of hand-crafted features. To leverage
massive tweets containing positive and negative e-
moticons for automatically feature learning, Tang
et al. (2014) propose to learn sentiment-specific
word embedding and Kalchbrenner et al. (2014)
model sentence representation with Dynamic Con-
volutional Neural Network.
In this paper, we develop a deep learning sys-
tem for Twitter sentiment classification. First-
ly, we learn sentiment-specific word embedding
(SSWE) (Tang et al., 2014), which encodes the
sentiment information of text into the continuous
representation of words (Mikolov et al., 2013; Sun
et al., 2014). Afterwards, we concatenate the SS-
WE features with the state-of-the-art hand-crafted
features (Mohammad et al., 2013), and build the
sentiment classifier with the benchmark dataset
from SemEval 2013 (Nakov et al., 2013). To
learn SSWE, we develop a tailored neural net-
work, which incorporates the supervision from
sentiment polarity of tweets in the hybrid loss
function. We learn SSWE from tweets, lever-
aging massive tweets with emoticons as distant-
supervised corpora without any manual annota-
tions.
We evaluate the deep learning system on the
test set of Twitter Sentiment Analysis Track in Se-
mEval 2014
2
. Our system (Coooolll) is ranked
2nd on the Twitter2014 test set, along with the
SemEval 2013 participants owning larger train-
ing data than us. The performance of only us-
ing SSWE as features is comparable to the state-
of-the-art hand-crafted features (detailed in Ta-
ble 3), which verifies the effectiveness of the
sentiment-specific word embedding. We release
the sentiment-specific word embedding learned
2
http://alt.qcri.org/semeval2014/task9/
208
Training 
Data 
Learning 
Algorithm 
Feature 
Representation 
Sentiment 
Classifier 
1 
2 
?. 
 
N 
 
N+1 
N+2 
? 
N+K 
STATE 
Feature 
SSWE 
Feature 
all-cap 
emoticon 
? 
?. 
dimension 1 
dimension 2 
dimension N 
elongated 
Massive 
Tweets 
Embedding 
Learning 
Figure 1: Our deep learning system (Coooolll) for
Twitter sentiment classification.
from 10 million tweets, which can be easily used
to re-implement our system and adopted off-the-
shell in other sentiment analysis tasks.
2 A Deep Learning System
In this section, we present the details of our deep
learning system for Twitter sentiment classifica-
tion. As illustrated in Figure 1, Coooolll is a su-
pervised learning method that builds the sentimen-
t classifier from tweets with manually annotated
sentiment polarity. In our system, the feature rep-
resentation of tweet is composed of two parts, the
sentiment-specific word embedding features (SS-
WE features) and the state-of-the-art hand-crafted
features (STATE features). In the following parts,
we introduce the SSWE features and STATE fea-
tures, respectively.
2.1 SSWE Features
In this part, we first describe the neural network
for learning sentiment-specific word embedding.
Then, we generate the SSWE features of a tweet
from the embedding of words it contains.
Our neural network is an extension of the tra-
ditional C&W model (Collobert et al., 2011), as
illustrated in Figure 2. Unlike C&W model that
learns word embedding by only modeling syntac-
tic contexts of words, we develop SSWEu, which
captures the sentiment information of sentences as
well as the syntactic contexts of words. Given an
original (or corrupted) ngram and the sentiment
polarity of a sentence as the input, SSWE
u
predict-
s a two-dimensional vector for each input ngram.
The two scalars (f
u
0
, f
u
1
) stand for language model
score and sentiment score of the input ngram, re-
so cooool :D 
syntactic 
sentiment 
Figure 2: Our neural network (SSWE
u
) for learn-
ing sentiment-specific word embedding.
spectively. The training objectives of SSWE
u
are
that (1) the original ngram should obtain a high-
er language model score f
u
0
(t) than the corrupted
ngram f
u
0
(t
r
), and (2) the sentiment score of orig-
inal ngram f
u
1
(t) should be more consistent with
the gold polarity annotation of sentence than cor-
rupted ngram f
u
1
(t
r
). The loss function of SSWE
u
is the linear combination of two hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(1)
where where t is the original ngram, t
r
is the cor-
rupted ngram which is generated from t with mid-
dle word replaced by a randomly selected one,
loss
cw
(t, t
r
) is the syntactic loss as given in E-
quation 2, loss
us
(t, t
r
) is the sentiment loss as
described in Equation 3. The hyper-parameter ?
weighs the two parts.
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(2)
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(3)
where ?
s
(t) is an indicator function reflecting the
sentiment polarity of a sentence, whose value is 1
if the sentiment polarity of tweet t is positive and
-1 if t?s polarity is negative. We train sentiment-
specific word embedding from 10M tweets col-
lected with positive and negative emoticons (Hu
et al., 2013). The details of training phase are de-
scribed in Tang et al. (2014).
After finish learning SSWE, we explore min,
average and max convolutional layers (Collobert
et al., 2011; Socher et al., 2011; Mitchell and Lap-
ata, 2010), to obtain the tweet representation. The
result is the concatenation of vectors derived from
different convolutional layers.
209
2.2 STATE Features
We re-implement the state-of-the-art hand-crafted
features (Mohammad et al., 2013) for Twitter sen-
timent classification. The STATE features are de-
scribed below.
? All-Caps. The number of words with all char-
acters in upper case.
? Emoticons. We use the presence of positive
(or negative) emoticons and whether the last
unit of a segmentation is emoticon
3
.
? Elongated Units. The number of elongated
words (with one character repeated more than
two times), such as gooood.
? Sentiment Lexicon. We utilize several senti-
ment lexicons
4
to generate features. We ex-
plore the number of sentiment words, the s-
core of last sentiment words, the total senti-
ment score and the maximal sentiment score
for each lexicon.
? Negation. The number of individual nega-
tions
5
within a tweet.
? Punctuation. The number of contiguous se-
quences of dot, question mark and exclama-
tion mark.
? Cluster. The presence of words from each
of the 1,000 clusters from the Twitter NLP
tool (Gimpel et al., 2011).
? Ngrams. The presence of word ngrams (1-4)
and character ngrams (3-5).
3 Experiments
We evaluate our deep learning system by applying
it for Twitter sentiment classification within a su-
pervised learning framework. We conduct exper-
iments on both positive/negative/neutral and posi-
tive/negative classification of tweets.
3
We use the positive and negative emoticons from Sen-
tiStrength, available at http://sentistrength.wlv.ac.uk/.
4
HL (Hu and Liu, 2004), MPQA (Wilson et al., 2005), N-
RC Emotion (Mohammad and Turney, 2013), NRC Hashtag
and Sentiment140Lexicon (Mohammad et al., 2013).
5
http://sentiment.christopherpotts.net/lingstruc.html
3.1 Dataset and Setting
We train the Twitter sentiment classifier on the
benchmark dataset in SemEval 2013 (Nakov et
al., 2013). The training and development sets were
completely in full to task participants of SemEval
2013. However, we were unable to download al-
l the training and development sets because some
tweets were deleted or not available due to modi-
fied authorization status. The distribution of our
dataset is given in Table 1. We train sentimen-
t classifiers with LibLinear (Fan et al., 2008) on
the training set and dev set, and tune parameter
?c,?wi of SVM on the test set of SemEval 2013.
In both experiment settings, the evaluation met-
ric is the macro-F1 of positive and negative class-
es (Nakov et al., 2013).
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of our SemEval 2013 Twitter
sentiment classification dataset.
The test sets of SemEval 2014 is directly pro-
vided to the participants, which is composed of
five parts. The statistic of test sets in SemEval
2014 is given in Table 2.
Positive Negative Neutral Total
T1 427 304 411 1,142
T2 492 394 1,207 2,093
T3 1,572 601 1,640 3,813
T4 982 202 669 1,939
T5 33 40 13 86
Table 2: Statistics of SemEval 2014 Twitter senti-
ment classification test set. T1 is LiveJournal2014,
T2 is SMS2013, T3 is Twitter2013, T4 is Twit-
ter2014, T5 is Twitter2014Sarcasm.
3.2 Results and Analysis
The experiment results of different methods
on positive/negative/neutral and positive/negative
Twitter sentiment classification are listed in Ta-
ble 3. The meanings of T1?T5 in each column are
described in Table 2. SSWE means the approach
that only utilizes the sentiment-specific word em-
bedding as features for Twitter sentiment classi-
fication. In STATE, we only utilize the existing
features (Mohammad et al., 2013) for building the
210
Method
Positive/Negative/Neutral Positive/Negative
T1 T2 T3 T4 T5 T1 T2 T3 T4 T5
SSWE 70.49 64.29 68.69 66.86 50.00 84.51 85.19 85.06 86.14 62.02
Coooolll 72.90 67.68 70.40 70.14 46.66 86.46 85.32 86.01 87.61 56.55
STATE 71.48 65.43 66.18 67.07 44.89 83.96 82.82 84.39 86.16 58.27
W2V 55.19 52.98 52.33 50.58 49.63 68.87 71.89 74.50 71.52 61.60
Top 74.84 70.28 72.12 70.96 58.16 - - - - - - - - - -
Average 63.52 55.63 59.78 60.41 45.44 - - - - - - - - - -
Table 3: Macro-F1 of positive and negative classes in positive/negative/neutral and positive/negative
Twitter sentiment classification on the test sets (T1-T5, detailed in Table 2) of SemEval 2014. The
performances of Coooolll on the Twitter-relevant test sets are bold.
sentiment classifier. In Coooolll, we use the con-
catenation of SSWE features and STATE features.
In W2V, we only use the word embedding learned
from word2vec
6
as features. Top and Average are
the top and average performance of the 45 team-
s of SemEval 2014, including the SemEval 2013
participants who owns larger training data.
On positive/negative/neutral classification of
tweets as listed in Table 3 (left table), we find
that the learned sentiment-specific word embed-
ding features (SSWE) performs comparable with
the state-of-the-art hand-crafted features (STATE),
especially on the Twitter-relevant test sets (T3
and T4)
7
. After feature combination, Coooolll
yields 4.22% and 3.07% improvement by macro-
F1 on T3 and T4,which verifies the effective-
ness of SSWE by learning discriminate features
from massive data for Twitter sentiment classifi-
cation. From the 45 teams, Coooolll gets the Rank
5/2/3/2 on T1-T4 respectively, along with the Se-
mEval 2013 participants owning larger training
data. We also comparing SSWE with the context-
based word embedding (W2V), which don?t cap-
ture the sentiment supervision of tweets. We find
that W2V is not effective enough for Twitter sen-
timent classification as there is a big gap between
W2V and SSWE on T1-T4. The reason is that W2V
does not capture the sentiment information of text,
which is crucial for sentiment analysis tasks and
effectively leveraged for learning the sentiment-
specific word embedding.
We also conduct experiments on the posi-
6
We utilize the Skip-gram model. The embedding is
trained from the 10M tweets collected by positive and neg-
ative emoticons, as same as the training data of SSWE.
7
The result of STATE on T3 is different from the results
reported in Mohammad et al. (2013) and Tang et al. (2014)
because we have different training data with the former and
different -wi of SVM with the latter.
tive/negative classification of tweets. The reason
is that the sentiment-specific word embedding is
learned from the positive/negative supervision of
tweets through emoticons, which is tailored for
positive/negative classification of tweets. From
Table 3 (right table), we find that the performance
of positive/negative Twitter classification is con-
sistent with the performance of 3-class classifica-
tion. SSWE performs comparable to STATE on T3
and T4, and yields better performance (1.62% and
1.45% improvements on T3 and T4, respectively)
through feature combination. SSWE outperform-
s W2V by large margins (more than 10%) on T3
and T4, which further verifies the effectiveness of
sentiment-specific word embedding.
4 Conclusion
We develop a deep learning system (Coooolll) for
message-level Twitter sentiment classification in
this paper. The feature representation of Cooool-
ll is composed of two parts, a state-of-the-art
hand-crafted features and the sentiment-specific
word embedding (SSWE) features. The SSWE
is learned from 10M tweets collected by posi-
tive and negative emoticons, without any manu-
al annotation. The effectiveness of Coooolll has
been verified in both positive/negative/neutral and
positive/negative classification of tweets. Among
45 systems of SemEval 2014 Task 9 subtask(b),
Coooolll yields Rank 2 on the Twitter2014 test set,
along with the SemEval 2013 participants owning
larger training data.
Acknowledgments
We thank Li Dong for helpful discussions. This
work was partly supported by National Natu-
ral Science Foundation of China (No.61133012,
No.61273321, No.61300113).
211
References
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics, pages 655?665.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion, pages 321?327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13, pages
312?320.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2014. Radical-enhanced
chinese character embedding. arXiv preprint arX-
iv:1404.4714.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
212
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242
Manchester, August 2008
A Cascaded Syntactic and Semantic Dependency Parsing System
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
Abstract
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
1 System Architecture
Our CoNLL 2008 Shared Task (Surdeanu et al,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
2 Syntactic Dependency Parsing
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
2.1 Features
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
score(h, c, l) = w ? f(h, c, l) (1)
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)?s:
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word?s
prefix.
2) We add two new features: ?bet-pos-h-same-
num? and ?bet-pos-c-same-num?. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
2.2 Relabeling
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
?I read books in the room.? and ?I read books in
the afternoon.?. It is hard to correctly label the arc
238
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
Table 1: Error Analysis of Each Label
between ?read? and ?in? unless we know the object
of ?in?.
We count the errors of each label, and show the
top ones in Table 1. ?Total? refers to the total num-
ber of the corresponding label in the development
data. The column of ?Mislabeled as? lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
2.3 Relabeling using Maximum Entropy
Classifier
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
?+ dir dist? means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c?s children. ?word c c? rep-
resents form or lemma of one child of the node
c. ?dir c? and ?dist c? represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
Table 2: Relabeling Feature Set (+ dir dist)
3 Semantic Dependency Parsing
3.1 Architecture
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al, 2004).
3.2 Predicate Identification
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it?s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding ?constituent? for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A ?POS pattern? is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
239
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either ?be? or
?get?, or else the relation type is ?APPO?, then the
verb is considered passive, otherwise active.
Also we used some ?combined? features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
3.3 Predicate Classification
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
?Bag of Words?. And an ?ordered? version is in-
troduced where each word is prefixed with a letter
?L?, ?R? or ?T? indicating it?s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with ?L?, ?R? or
?T? indicating the word position joined together,
namely ?Bag of POS (Ordered)?. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely ?Bag of POS
(Numbered)?.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a ?window?, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
3.4 Semantic Role Classification
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al, 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The ?POS
Path? feature consists of POS tags of the words
along the path from a word to the predicate. Other
than ?Up? and ?Down?, the ?Left? and ?Right? di-
rection of the path is added. Similarly, the ?Re-
lation Path? feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): ?Up-
stream paths? are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): ?Familyship rela-
tion? between a word and the predicate, being one
of ?self?, ?child?, ?descendant?, ?parent?, ?ances-
tor?, ?sibling?, and ?not-relative?.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ?p?. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
240
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
3.5 ILP-based Post Inference
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
?NULL? is also added to R, representing ?none of
the roles is assigned?.
For each word w ? W and semantic role label
r ? R we create a binary variable v
wr
? (0, 1),
whose value indicates whether or not the word w
is labeled as label r. p
wr
denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f =
?
w,r
log(p
wr
? v
wr
)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
v
wr
. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
?NULL?), i.e.:
?
r
v
wr
= 1
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
v
wr
= 0, if p
wr
< 0.3 and r 6= ?NULL?
C3: Statistics shows that the most roles (ex-
cept for the virtual role ?NULL?) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles? duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
?
r
v
wr
? 1,
if < p, r > /? {< p, r > |p ? P, r ? R;
d
pr
c
pr
?d
pr
> 0.3 ? d
pr
> 10}
(2)
where P is the set of predicates; c
pr
denotes the
count of words in the training corpus, which are
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
labeled as r ? R for predicate p ? P ; while d
pr
denotes something similar to c
pr
, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
4 Experiments
The original MSTParser
1
is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit
2
. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.5
3
is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
4.1 Syntactic Dependency Parsing
The experiments on development data show that
relabeling process is helpful, which improves the
1
http://sourceforge.net/projects/mstparser
2
http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3
http://sourceforge.net/projects/lpsolve
241
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
Table 4: The performance of predicate identifica-
tion and classification
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
Table 5: Comparison between different post infer-
ence strategies
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
4.2 Semantic Dependency Parsing
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label ?NULL?)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
Table 6: Semantic dependency parsing perfor-
mances
4.3 Overall Performance
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
5 Conclusion and Future Work
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
Acknowledgments
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
?863? National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
References
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
242
Proceedings of the First Workshop on Metaphor in NLP, pages 67?76,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Robust Extraction of Metaphors from Novel Data   Tomek Strzalkowski1, George Aaron Broadwell1, Sarah Taylor2, Laurie Feldman1, Boris Yamrom1, Samira Shaikh1, Ting Liu1, Kit Cho1, Umit Boz1, Ignacio Cases1 and Kyle El-liott3 1State University of New York 2Sarah M. Taylor Consulting LLC 3Plessas Experts University at Albany 121 South Oak St.  Network Inc. Albany NY USA 12222 Falls Church VA USA 22046 Herndon VA 20171 tomek@albany.edu talymail59@gmail.com  kelliot@plessas.net     Abstract 
This article describes our novel approach to the automated detection and analysis of meta-phors in text. We employ robust, quantitative language processing to implement a system prototype combined with sound social science methods for validation. We show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification. We use Topical Structure and Tracking, an Imageability score, and innova-tive methods to build an effective metaphor identification system that is fully automated and performs well over baseline.  1 Introduction The goal of this research is to automatically identi-fy metaphors in textual data.  We have developed a prototype system that can identify metaphors in naturally occurring text and analyze their seman-tics, including the associated affect and force. Met-aphors are mapping systems that allow the semantics of a familiar Source domain to be ap-plied to a Target domain so that new frameworks of reasoning can emerge in the Target domain. Metaphors are pervasive in discourse, used to con-vey meanings indirectly. Thus, they provide criti-cal insights into the preconceptions, assumptions and motivations underlying discourse, especially valuable when studied across cultures. When met-aphors are thoroughly understood within the con-text of a culture, we gain substantial knowledge about cultural values. These insights can help bet-ter shape cross-cultural understanding and facili-
tate discussions and negotiations among different communities.  A longstanding challenge, however, is the large-scale, automated identification of metaphor in vol-umes of data, and especially the interpretation of their complex, underlying semantics.  We propose a data-driven computational ap-proach that can be summarized as follows: Given textual input, we first identify any sentence that contains references to Target concepts in a given Target Domain (Target concepts are elements that belong to a particular domain; for instance ?gov-ernment bureaucracy? is a Target concept in the ?Governance? domain). We then extract a passage of length 2N+1, where N is the number of sentenc-es preceding (or succeeding) the sentence with Target Concept. We employ dependency parsing to determine the syntactic structure of each input sen-tence. Topical structure and imageability analysis are then combined with dependency parsing output to locate the candidate metaphorical expressions within a sentence. For this step, we identify nouns and verbs in the passage (of length 2N+1) and link their occurrences ? including repetitions, pronomi-nal references, synonyms and hyponyms. This linking uncovers the topical structure that holds the narrative together.  We then locate content words that are outside the topical structure and compute their imageability scores. Any nouns or adjectives outside the main topical structure that also have high imageability scores and are dependency-linked in the parse structure to the Target Concept are identified as candidate source relations, i.e., expressions borrowed from a Source domain to describe the Target concept. In addition, any verbs that have a direct dependency on the Target Con-
67
cept are considered as candidate relations. These candidate relations are then used to compute and rank proto-sources. We search for their arguments in a balanced corpus, assumed to represent stand-ard use of the language, and cluster the results. Proto-source clusters and their ranks are exploited to determine whether the candidate relations are metaphorical or literal. Finally, we compute the affect and force associated with the metaphor.    Our approach is shown to work in four lan-guages ? American English, Mexican Spanish, Russian Russian and Iranian Farsi. We detail in this paper the application of our approach to detec-tion of metaphors using specific examples from the ?Governance? domain. However, our approach can be expanded to work on extracting metaphors in any domain, even unspecified ones. We shall brief-ly explain this in Section 5; we defer the details of the expanded version of the algorithm to a separate larger publication. In addition, we shall primarily present examples in English to illustrate details of our algorithms. However, modules for all four lan-guages have the same implementation in our sys-tem.  The rest of the paper is organized as follows: in Section 2, we discuss related research in this field. Section 3 presents our approach in detail; Section 4 describes our evaluation and results. In Section 5 we discuss our conclusions and future directions.  2 Related Work Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their fol-lowers) that generally look at metaphors as abstract language constructs with complex semantic prop-erties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that at-tempt to correlate metaphor semantics with their usage in naturally occurring text but generally lack robust tools to do so; and (3) social science ap-proaches, particularly in psychology and anthro-pology that seek to explain how people deploy and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated exam-ples.     Metaphor study in yet other disciplines has in-cluded cognitive psychologists (e.g., Allbritton, McKoon & Gerrig, 1995) who have focused on the 
way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help out-siders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Polit-ical Science underpinning (Musolff, 2008; Lakoff, 2001).      In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of meta-phors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al(2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example).     Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feld-man & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al(2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial explora-tion and hence, inconclusive. Shutova et al(2010a) employ an unsupervised method of metaphor iden-tification using nouns and verb clustering to auto-matically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quanti-ties and may not be easily generated in different languages.  By contrast, we propose an approach that is fully automated and can be validated using empirical social science methods. Details of our algorithm follow next.  3 Our Approach In this section, we walk through the steps of meta-phor identification in detail. Our overall algorithm 
68
consists of five main steps from obtaining textual input to classification of input as metaphorical or literal.  3.1 Passage Identification The input to our prototype system is a piece of text. This text may be taken from any genre ? news articles, blogs, magazines, official announcements, broadcast transcripts etc.  Given the text, we first identify sentences that contain Target concepts in the domain we are in-terested in. Target concepts are certain keywords that occur within the given domain and represent concepts that may be targets of metaphor. For in-stance, in the ?Governance? domain, concepts such as ?federal bureaucracy? and ?state mandates? serve as Target concepts. We keep a list of Target concepts to search through when analyzing given input. This list can be automatically created by mining Target Concepts from resource such as Wikipedia, given the Target domain, or manually constructed. Space limits the discussion of how such lists may be automatically created; a separate larger publication addresses our approach to this task in greater detail.  In Figure 1, we show a piece of text drawn from a 2008 news article. The sentence in italics con-tains one of our Target concepts: ?federal bureau-cracy?. We extract the sentence containing Target concepts that match any of those in our list, includ-ing N sentences before and N sentences after the sentence if they exist, to yield a passage of at most 2N+1 sentences. For the example shown in Figure 1, the Target concept is ?federal bureaucracy?. In current system prototype, N=2. Hence, we extract two sentences prior to the sentence containing ?federal bureaucracy? (in Figure 1 example, these are omitted for ease of presentation) and two sen-tences following the given sentence.       Once this passage is extracted, we need to de-termine whether a metaphor is present in the mid-dle sentence. To accomplish that, we follow the steps as described in the next section.    
 Figure 1. Excerpt from news article. Passage containing target concept highlighted in italics. The callouts 1, 2 etc., indicate topic chains (see next section).      3.2 Topical Structure and Imageability Anal-ysis Our hypothesis is that metaphorically used terms are typically found outside the topical structure of the text. This is an entirely novel method of effec-tively selecting candidate relations. It draws on  Broadwell et al (2012), who proposed a method to establish the topic chains in discourse as a means of modeling associated socio-linguistic phenomena such as topic control and discourse cohesiveness. We adapted this method to identify and exclude any words that serve to structure the core discus-sion, since the metaphorical words, except in the cases of extended and highly elaborated meta-phors, are not the main subject, and thus unlikely to be repeated or referenced in the context sur-rounding the sentence.  We link the occurrences of each noun and verb in the passage (5 sentence length). Repetitions via synonyms, hyponyms, lexical variants and pronoun references are linked together. These words, as elements of the several topic chains in a text, are then excluded from further consideration. WordNet (Felbaum, 1998) is used to look up synonyms and hyponyms of the remaining content words. We 
These qualities1 have helped him4 navigate the labyrinthine federal bureaucracy in his demand-ing $191,300-a-year job as the top federal offi-cial3 responsible for bolstering airline, border2, port and rail security against a second cata-strophic terrorist attack.  But those same personal qualities1 also explain why the 55-year-old Cabinet officer3 has alienat-ed so many Texans along the U.S.-Mexico bor-der2 with his4 relentless implementation of the Bush administration's hard-nosed approach to immigration enforcement - led by his unyielding push to construct 670 miles of border2 fencing by the end of the year.  Some Texas officials are so exasperated that they say they'll just await the arrival of the next presi-dent before revisiting border enforcement with the federal government.  Copyright 2008. The Houston Chronicle Publishing Company. All Rights Reserved.  
69
illustrate this in Figure 1. We show the two sen-tences that form the latter context in the example passage. We show four of the topic chains discov-ered in this passage. These have been labeled via superscripts in Figure 1. 1 and 2 are the repetitions of word ?qualities? and ?border?. The 3 identifies repetition via lexical variants ?officer? and ?offi-cial? and 4 identifies the pronoun co-references  ?him? and ?his?. We shall exclude these words from consideration when searching for candidate metaphorical relations in the middle sentence of the passage.  To further narrow the pool of candidate relations in this sentence, we compute the imageability scores of the remaining words. The hypothesis is metaphors use highly imageable words to convey their meaning. The use of imageability scores for the primary purpose of metaphor detection distin-guishes our approach from other research on this problem. While Turney et al (2011) explored the use of word concreteness (a concept related but not identical to imageability) in an attempt to disam-biguate between abstract and concrete verb senses, their method was not specifically applied to detec-tion of metaphors; rather it was used to classify verb senses for the purpose of resolving textual entailment. Broadwell et al (2013) present a de-tailed description of our approach and how we use imageability scores to detect metaphors. Our assertion is that any highly imageable word is more likely to be a metaphorical relation. We use the MRCPD (Coltheart 1981, Wilson 1988) expanded lexicon to look up the imageability scores of words not excluded via the topic chains. Although the MRCPD contains data for over 150,000 words, a major limitation of the database for our purposes is that the MRCPD has imageabil-ity ratings (i.e., how easily and quickly the word evokes a mental image) for only ~9,240  (6%) of the total words in its database. To fill this gap, we expanded the MRCPD database by adding imagery ratings for an further 59,989 words. This was done by taking the words for which the MRCPD data-base has an imageability rating and using that word as an index to synsets determined using WordNet (Miller, 1995). The expansion and validation of the expanded MRCPD imageability rating is presented in a separate, future publication.  Words that have an imageability rating lower than an experimentally determined threshold are further excluded from consideration. In the exam-
ple shown in Figure 1, words that have sufficiently high imageability scores are ?labyrinthine?, ?port?, ?rail? and ?airline?. We shall consider them as candidate relations, to be further investigated, as explained in the dependency parsing step described next.   3.3 Relation Extraction Dependency parsing reveals the syntactic structure of the sentence with the Target concept. We use the Stanford parser (Klein and Manning, 2003) for English language data. We identify candidate met-aphorical relations to be any verbs that have the Target concept in direct dependency path (other than auxiliary and modal verbs). We exclude verbs of attitude (?think?, ?say?, ?consider?), since these have been found to be more indicative of metony-my than of metaphor. This list of attitude verbs is automatically derived from WordNet. From the example shown in Figure 1, one of the candidate relations extracted would be the verb ?navigate?.      In addition, we have a list of candidate relations from Step 3.2, which are the highly imageable nouns and adjectives that remain after topical structure analysis. Since ?port?, ?rail? and ?airline? do not have a direct dependency path to our Target concept of ?federal bureaucracy?, we drop these from further consideration. The highly imageable word remaining in this list is ?labyrinthine?.      Thus, two candidate relations are extracted from this passage ? ?navigate? and ?labyrinthine?. We shall now show how we use these to discover pro-to-sources for the potential metaphor.  3.4 Discovery of Proto-sources Once candidate relations are identified, we exam-ine whether the usage of these relations is meta-phorical or literal. To determine this, we search for all uses of these relations in a balanced corpus and examine in which contexts the candidate relations occur. To demonstrate this via our example, we shall consider one of the candidate relations identi-fied in Figure 1 ? ?navigate?; the search method is the same for all candidate relations identified. In the case of the verb ?navigate? we search a bal-anced corpus for the collocated words, that is, those that occur within a 4-word window following the verb, with high mutual information (>3) and occurring together in the corpus with a frequency 
70
at least 3. This search returns a list of words, most-ly nouns in this case, that are the objects of the verb ?navigate?, just as ?federal bureaucracy? is the object in the given example. However, since the search occurs in a balanced corpus, given the parameters we search for, we discover words where the objects are literally navigated. Given these search parameters, the top results we get are generally literal uses of the word ?navigate?. We cluster the resulting literal uses as semantically related words using WordNet and corpus statistics. Each such cluster is an emerging prototype source domain, or a proto-source, for the potential meta-phor. In Figure 2, we show three of the clusters ob-tained when searching for the literal usage of the verb ?navigate?. We use elements of the clusters to give names or label the proto-source domains. WordNet hypernyms or synonyms are used in most cases. The clusters shown in Figure 2 represent three potential source domains for the given exam-ple, the labels ?MAZE?, ?WAY? and ?COURSE? are derived from WordNet.  
 Figure 2. Three of several clusters obtained from bal-anced corpus search for objects of verb ?navigate?.       We rank the clusters according to the combined frequency of cluster elements in the balanced cor-pus. In a similar fashion, clusters are obtained for the candidate relation ?labyrinthine?; however here we search for the nouns modified by the adjective ?labyrinthine?.       
3.5 Estimation of Linguistic Metaphor A ranked list of proto-sources from the previous step serves as evidence for the presence of a meta-phor.   If any Target domain elements are found in the top two ranked clusters, we consider the phrase being investigated to be literal. This eliminates examples where one of the most frequently en-countered sources is within the target domain.  If neither of the top two most frequent clusters contains any elements from the target domain, we then compute the average imageability scores for each cluster from the mean imageability score of the cluster elements. If no cluster has a sufficiently high imageability score (experimentally deter-mined to be >.50 in the current prototype), we again consider the given input to be literal. This step reinforces the claim that metaphors use highly imageable language to convey their meaning. If a proto-source cluster is found to meet both criteria, we consider the given phrase to be metaphorical. For the example shown in Figure 1, our system finds ?navigate the ?federal bureaucracy? to be metaphorical. One of the top Source domains iden-tified for this metaphor is ?MAZE?. Hence the conceptual metaphor output for this example can be: ?FEDERAL BUREAUCRACY IS A MAZE?. Our system can thus classify input sentences as metaphorical or literal by the series of steps out-lined above. In addition, we have modules that can determine a more complex conceptual metaphor, based upon evidence of one or more metaphorical passages as identified above. We do not discuss those modules in this article. Once a metaphor is identified, we compute associated Mappings, Af-fect and Force. 3.6 Mappings In the current prototype system, we assign meta-phors to one of three types of mappings. Propertive mappings ? which state what the domain objects  
1. Proto-source Name: MAZE Proto-source Elements: [mazes, system, net-works] IMG Score: 0.74 2. Proto-source Name: WAY Proto-source Elements: [way, tools] IMG Score: 0.60 3. Proto-source Name: COURSE Proto-source Elements: [course, streams] IMG: 0.55 
Table 1. Algorithm assigns affect of metaphor based upon mappings. 
Rel  < Negative Rel  = Neutral 
Rel ? Positive 
71
are and descriptive features; Agentive mappings ? which describe what the domain elements do to other objects in the same or different domains; and Patientive mappings ? which describe what is done to the objects in these domains. These are broad categories to which relations can, with some ex-ceptions be assigned at the linguistic metaphor lev-el by the parse tag of the relation. Relations that take Target concepts as objects are usually Pa-tientive relations. Similarly, relations that are Agentive take Target concepts as subjects. Proper-tive relations are usually determined by adjectival relations.     Once mappings are assigned, we can use them to group linguistic metaphors. A set of linguistic met-aphors on the same or semantically equivalent Target concepts can be grouped together if the re-lations are all agentive, patientive or propertive. The mapping assigned to set of examples in Figure 3 is Patientive.      One immediate consequence of the proposed approach is the simplicity with which we can rep-resent domains, their elements, and the metaphoric mappings between domains. Regardless of what specific relations may operate within a domain (be it Source or Target), they can be classified into just 3 categories. We are further expanding this module to include semantically richer distinctions within the mappings. This includes the determination of the sub-dimensions of mappings i.e. assigning groups of relations to a semantic category.  3.7 Affect and Force  Affect of a metaphor may be positive, negative or neutral. Our affect estimation module computes an affect score taking into account the relation, Target concept and the subject or object of the relation based on the dependency between relation and Target concept. The algorithm is applied according to the categories shown in Table 1.      The expanded ANEW lexicon (Bradley and Lang, 2010) is used to look up affect scores of words. ANEW assigns scores from 0 (highly nega-tive) to 9 (highly positive); 5 being neutral. We compute the affect of a metaphorical phrase within a sentence by summing the affect scores of the re-lation and its object or subject.  If the relation is agentive, we then look at the object in source do-main that the Target concept is acting upon. If the object (denoted in above table as X) has an affect 
score that is greater than neutral, and the relation itself has an affect score that is greater than neutral, then a POSITIVE affect is assigned to the meta-phor. This is denoted by the cell at the intersection of the row labeled ?Rel > Positive? and the 3rd col-umn in Table 1. Similarly affect for the other map-ping categories can be assigned.   
 Figure 3. Four metaphors for the Target concept ?feder-al bureaucracy?.   We also seek to determine the impact of metaphor on the reader. This is explored using the concept of Force in our system. The force of a metaphor is estimated currently by the commonness of the ex-pression in the given Target domain. We compute the frequency of the relation co-occurring with Target concept in a corpus of documents in the given Target domain. This frequency represents the commonness of expression, which is the in-verse of Force. The more common a metaphorical expression is, the lesser its force.     For the example shown below in Figure 4, the affect is computed to be positive (?navigate? and ?veterans? are both found to have positive affect scores, the relation is patientive). The force of this expression is low, since its commonness is 742 (commonness score > 100 is high commonness, determined experimentally).   
 Figure 4. Example of metaphor with positive affect and low force. 
1. His attorney described him as a family man who was lied to by a friend and who got tangled in federal bureaucracy he knew nothing about. 2. The chart, composed of 207 boxes illustrates the maze of federal bureaucracy that would have been created by then-President Bill Clinton's rela-tion health reform plan in the early 1990s. 3. "Helping my constituents navigate the federal bureaucracy is one of the most important things I can do," said Owens. 4. A Virginia couple has donated $1 million to help start a center at Arkansas State University meant to help wounded veterans navigate the federal bureaucracy as they return to civilian life.  
A Virginia couple has donated $1 million to help start a center at Arkansas State University meant to help wounded veterans navigate the federal bureaucracy as they return to civilian life.  
72
   The focus of this article is the automatic identifi-cation of metaphorical sentences in naturally oc-curring text. Affect and force modules are utilized to understand metaphors in context and contrast them across cultures, if feasible. We defer more detailed discussion of affect and force and their implications to a future, larger article.  4 Evaluation and Results In order to determine the efficacy of our system in classifying metaphors as well as to validate various system modules such as affect and force, we per-formed a series of experiments to collect human validation of metaphors in a large set of examples.  4.1 Experimental Setup  We constructed validation tasks that aimed at per-forming evaluation of linguistic metaphor extrac-tion accuracy. The first task ? Task 1, consists of a series of examples, typically 50, split more or less equally between those proposed by the system to be metaphorical and those proposed to be literal. This task was designed to elicit subject and expert judgments on several aspects related to the pres-ence or absence of linguistic metaphors in text. Subjects are presented with brief passages where a Target concept and a relation are highlighted. They are asked to rank their responses on a 7-point scale for the following questions:  Q1: To what degree does the above passage use metaphor to describe the highlighted concept? Q2: To what degree does this passage convey an idea that is either positive or negative?  Q3: To what degree is it a common way to express this idea?      There are additional questions that ask subjects to judge the imageability and arousal of a given pas-sage, which we do not discuss in this article. Q1 deals with assessing the metaphoricity of the ex-ample, Q2 deals with affect and Q3 deals with force.   Each instance of Task 1 consists of a set of instruc-tions, training examples, and a series of passages to be judged. Instructions provide training examples whose ratings fall at each end the rating continu-um. Following the task, participants take a gram-
mar test to demonstrate native language proficien-cy in the target language. All task instances are then posted on Amazon?s Mechanical Turk. The goal is to collect at least 30 valid judgments per task instance. We typically collect ~50 judgments from Mechanical Turkers, so that after filtering for invalid data which includes turkers selecting items at random, taking too little time to complete the task, grammar test failures, and other inconsistent data, we would still retain 30 valid judgments per passage. In addition to grammar test and time fil-ter, we also inserted instance of known metaphors and known literal passages randomly within the Task. Any turker judgments that classify these known instance incorrectly more than 30% of the total known instance size are discarded.     The valid turker judgments are then converted to a binary judgment for the questions we presented. For example, for question Q1, the anchors to 7-point scale are 0 (none at all i.e. literal) to 7 (highly i.e metaphorical). We take [0, 2] as a literal judg-ment and [4, 6] as metaphorical and take a majority vote. If the majority vote is 3, we discard that pas-sage from our test set, since it is undetermined whether the passage is literal or metaphorical.         We have collected human judgments on hun-dreds of metaphors in all four languages of inter-est. In Section 4.3, we explain our performance and compare our results to baseline where appro-priate.  4.2 Test Reliability The judgments collected from subjects are tested for reliability and validity. Reliability among the raters is computed by measuring intra-class corre-lation (ICC) (McGraw & Wong, 1996; Shrout & Fleiss, 1979). A coefficient value above 0.7 indi-cates strong reliability.  Table 3 shows the current reliability coefficients established for the selected Task 1 questions in all 4 languages. In general, our analyses have shown that with approximately 30 or more subjects we obtain a reliability coefficient of at least 0.7. We note that Russian and Farsi reliability scores are low in some categories, primarily due to lack of sufficient subject rating data. However, reliability of subject ratings for metaphor question (Q1) is sufficiently high in three of the four languages we are interested in.  
73
Dimension English Spanish Russian  Farsi Metaphor .908 .882 .838 .606 Affect  .831 .776 .318 .798 Commonness .744 .753 .753 .618 Table 3. Intraclass correlations for linguistic metaphor assessment by Mechanical Turk subjects (Task 1) 4.3 Results In Table 4, we show our performance at classifying metaphors across four different languages. The baseline in this table assigns all given examples in the test set to be metaphorical. We note that per-formance of the system at the linguistic metaphor level when compared to human gold standard is significantly over baseline for all four languages. The system performances cited in Table 4 validate the system against test sets that contain the distri-bution of metaphorical vs. literal examples as out-lined in Table 5.   English Spanish Russian Farsi Baseline 45.8% 41.7% 56.4% 50% System 71.3% 80% 69.2% 78% Table 4. Performance accuracy of system when com-pared to baseline for linguistic metaphor classification.   English Spanish Russian Farsi Metaphor 50 50 22 25 Literal 59 70 17 25 Total 109 120 39 50 Table 5. Number of metaphorical and literal examples in test sets across all four languages.  Table 6 shows the accuracy in classification by the Affect and Force modules. We note that the low performance of affect and force for languages oth-er than English. Our focus has been on improving NLP tools for Spanish, Russian and Farsi, so that a similar robust performance for those language can be achieved as we can demonstrate in English.  Accuracy English Spanish Russian Farsi Affect  72% 54% 51% 40% Force 67% 50% 33% 66% Table 6. Affect and force performance of system on linguistic metaphor level.  5 Discussion and Future Work In this article, we described in detail our approach to detecting metaphors in text. We have developed 
an automated system that does not require the ex-istence of annotated training data or a knowledge base of predefined metaphors. We have described the various steps for detecting metaphors from re-ceiving an input, to selecting candidate relations, to the discovery of prototypical source domains, and leading to the identification of a metaphor as well as the discovery of the potential source domain being applied in the metaphor. We presented two novel concepts that have heretofore not been fully explored in computational metaphor identification systems. The first is the exclusion of words that form the thread of the discussion in the text, by the application of a Topic Tracking module. The se-cond is the application of Imageability scores in the selection of salient candidate relations.  Our evaluation consists first of validating the eval-uation task itself. Once we ensure that sufficient reliability has been established on the various di-mensions we seek to evaluate ? metaphoricity, af-fect and force ? we compare our system performance to the human gold standard. The per-formance of our system as compared to baseline is quite high, across all four languages of interest when measured against human assessed gold standard.  In this article, we discuss examples of metaphors belonging to a specific Target domain ? ?Govern-ance?. However, we can run our system through data in any domain perform the same kind of met-aphor identification. In cases where the Target do-main is unknown, we plan to use our Topic tracking module to recognize content words that may form part of a metaphorical phrase. This is essentially a process that is the reverse of that de-scribed in Section 3.3. We will find the salient Target concepts where there are directly dependent relations with the imageable verbs or adjectives.  In a separate larger publication, we plan to discuss in detail revisions to our Mapping module as well as the discovery and analyses of more complex conceptual metaphors. Such complex metaphors are based upon evidence from one or more instance of linguistic metaphors. Additional modules would recognize the manifold mappings, affect and force associated with the complex conceptual metaphors.   Acknowledgments This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of 
74
Defense US Army Research Laboratory contract num-ber W911NF-12-C-0024. The U.S. Government is au-thorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.  Disclaimer: The views and conclu-sions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Govern-ment. References  Allbritton, David W., Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-Based Schemas and Text Representations: Making Connections Through Con-ceptual Metaphors, Journal of Experimental Psy-chology: Learning, Memory, and Cognition, Vol. 21, No. 3, pp. 612-625. Baumer, Erik. P.S., White, James., Tomlinson, Bill. 2010. Comparing Semantic Role Labeling with Typed Dependency Parsing in Computational Meta-phor Identification. Proceedings of the NAACL HLT 2010 Second Workshop on Computational Ap-proaches to Linguistic Creativity, pages 14?22, Los Angeles, California, June 2010.  Bradley, M.M. & Lang, P.J. 2010. Affective Norms for English Words (ANEW): Instruction manual and af-fective ratings. Technical Report C-2. University of Florida, Gainesville, FL. Broadwell George A., Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Taylor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu and Nick Webb. 2012. Modeling Socio-Cultural Phenomena in Discourse. Journal of Natural Language Engineer-ing, Cambridge Press. Broadwell, George A., Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Taylor, Samira Shaikh, Ting Liu, Kit Cho, aand Nick Webb. 2013. Using imageability and topic chaining to locate met-aphors in linguistic corpora. in Ariel M. Greenberg, William G. Kennedy, Nathan D. Bos and Stephen Marcus, eds. Proceedings of the 6th International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction SBP 2013. Carbonell, Jaime. 1980. Metaphor: a key to extensible semantic analysis. Proceedings of the 18th Annual Meeting on Association for Computational Linguis-tics. Charteris-Black, Jonathan 2002 Second Language Fig-urative Proficiency: A Comparative Study of Malay and English. Applied Linguistics 23/1: 104-133. 
Coltheart, M. 1981. The MRC Psycholinguistic Data-base. Quarterly Journal of Experimental Psychology, 33A, 497-505. Fass, Dan. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computa-tional Linguistics, Vol 17:49-90 Feldman, J. and S. Narayanan. 2004. Embodied mean-ing in a neural theory of language. Brain and Lan-guage, 89(2):385?392. Fellbaum, C. editor. 1998. WordNet: An Electronic Lexical Database (ISBN: 0-262-06197-X). MIT Press, first edition. Gedigian, M., Bryant, J., Narayanan, S., & Ciric, B. (2006). Catching Metaphors. Proceedings of the Third Workshop on Scalable Natural Language Un-derstanding ScaNaLU 06 (pp. 41-48). Association for Computational Linguistics. Klein, Dan and Manning, Christoper D. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430. Krishnakumaran, S. and X. Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computational Approaches to Fig-urative Language, pages 13?20, Rochester, NY. Lakoff, George and Johnson, Mark. 1980. Metaphors We Live By. University Of Chicago Press. Lakoff, George. 2001. Moral Politics: what Conserva-tives Know that Liberals Don?t. University of Chica-go Press. Malkki, Liisa. 1992. National Geographic: The Rooting of People and the Territorialization of National Iden-tity Among Scholars and Refugees. Society for Cul-tural Anthropology 7(1):24-44 Martin, James. 1988. A Computational Theory of Meta-phor. PH.D. Dissertation McGraw, K. O., & Wong, S. P. 1996. Forming infer-ences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30-46. Musolff, Andreas. 2008. What can Critical Metaphor Analysis Add to the Understanding of Racist Ideolo-gy? Recent Studies of Hitler?s Anti-Semitic Meta-phors, Critical Approaches to Discourse Analysis across Disciplines, http://cadaad.org/ejournal, Vol. 2(2): 1-10. O?Halloran, Kieran. 2007. Critical Discourse Analysis and the Corpus-informed Interpretation of Metaphor at the Register Level. Oxford University Press 
75
Shrout, P. E., & Fleiss, J. L. 1979. Intraclass correla-tions: Uses in assessing rater reliability. Psychologi-cal Bulletin, 86 (2), 420-428. Shutova, E. 2010. Models of Metaphors in NLP. In Proceedings of ACL 2010, Uppsala, Sweden. Shutova, E. and S. Teufel. 2010a. Metaphor corpus an-notated for source - target domain mappings. In Pro-ceedings of LREC 2010, Malta. Shutova, E., T. Van de Cruys and A. Korhonen. 2012. Unsupervised Metaphor Paraphrasing Using a Vector Space Model, In Proceedings of COLING 2012, Mumbai, India Turney, Peter., Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identifi-cation through concrete and abstract context. In Pro-ceedings of EMNLP, pages 680?690, Edinburgh, UK Wilks, Yorick. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cambridge University Press, Cambridge, U.K., 329--348. Wilson, M.D. (1988) The MRC Psycholinguistic Data-base: Machine Readable Dictionary, Version 2. Be-havioural Research Methods, Instruments and Computers, 20(1), 6-11.     
76
Proceedings of the Second Workshop on Metaphor in NLP, pages 42?51,
Baltimore, MD, USA, 26 June 2014. c?2014 Association for Computational Linguistics
Computing Affect in Metaphors 
Tomek Strzalkowski1,2, Samira Shaikh1, Kit Cho1, George Aaron Broadwell1, Laurie Feldman1, Sarah Taylor3, Boris Yamrom1, Ting Liu1, Ignacio Cases1, Yuliya Peshkova1 and Kyle Elliot4 1State University of New York - Univer-sity at Albany  
2Polish Academy of Sciences 3Sarah M. Taylor Consulting LLC 4Plessas Experts Network tomek@albany.edu     Abstract 
This article describes a novel approach to automated determination of affect associ-ated with metaphorical language. Affect in language is understood to mean the at-titude toward a topic that a writer at-tempts to convey to the reader by using a particular metaphor. This affect, which we will classify as positive, negative or neutral with various degrees of intensity, may arise from the target of the meta-phor, from the choice of words used to describe it, or from other elements in its immediate linguistic context. We attempt to capture all these contributing elements in an Affect Calculus and demonstrate experimentally that the resulting method can accurately approximate human judgment. The work reported here is part of a larger effort to develop a highly ac-curate system for identifying, classifying, and comparing metaphors occurring in large volumes of text across four differ-ent languages: English, Spanish, Russian, and Farsi. 1 Introduction We present an approach to identification and val-idation of affect in linguistic metaphors, i.e., metaphorical expressions occurring in written language. Our method is specifically aimed at isolating the affect conveyed in metaphors as opposed to more broad approaches to sentiment classification in the surrounding text. We demonstrate experimentally that our basic Affect Calculus captures metaphor-related affect with a high degree of accuracy when applied to neutral metaphor targets. These are targets that them-selves do not carry any prior valuations. We sub-
sequently expanded and refined this method to properly account for the contribution of the prior affect associated with the target as well as its immediate linguistic context.  2 Metaphor in Language Metaphors are mapping systems that allow the semantics of a familiar Source domain to be ap-plied to a new Target domain so as to invite new frameworks for reasoning (usually by analogy) to emerge in the target domain. The purpose of a metaphor is (a) to simplify or enable reasoning and communication about the target domain that would otherwise be difficult (because of tech-nical complexity) or impossible (due to lack of agreed upon vocabulary) (e.g., Lakoff & John-son, 1980; 2004); or (b) to frame the target do-main in a particular way that enables one form of reasoning while inhibiting another (e.g., Thibodeau & Boroditsky, 2011). The two rea-sons for using metaphors are not necessarily mu-tually exclusive, in other words, (a) and (b) can operate at the same time. The distinction sug-gested above has to do with affect: a metaphor formed through (a) alone is likely to be neutral (e.g., client/server, messenger DNA), while a metaphor formed using (b) is likely to have a polarizing affect (e.g., tax?s burden).  The Source and Target domains that serve as endpoints of a metaphoric mapping can be repre-sented in a variety of ways; however, in a nut-shell they are composed of two kinds of things: concepts and relations. In a Target domain the concepts are typically abstract, disembodied, of-ten fuzzy concepts, such as crime, mercy, or vio-lence, but may also include more concrete, novel, or elaborate concepts such as democracy or eco-nomic inequality. In a Source domain, the con-cepts are typically concrete and physical; howev-er, mapping between two abstract domains is 
42
also possible. (E.g., crime may be both a target and a source domain.)  The relations of interest are those that operate between the concepts within a Source domain and can be ?borrowed? to link concepts within the Target domain, e.g., ?Crime(TARGET) spread to(RELATION) previously safe areas? may be bor-rowing from a DISEASE or a PARASITE source domain.  3 Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approach-es (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that attempt to correlate met-aphor semantics with their usage in naturally oc-curring text but generally lack robust tools to do so; and (3) social science approaches, particular-ly in psychology and anthropology that seek to explain how people produce and understand met-aphors in interaction, but which lack the neces-sary computational tools to work with anything other than relatively isolated examples. In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of meta-phors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the ex-istence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a sys-tem that can recognize metaphor; however their approach is only shown to work in a narrow do-main (The Wall Street Journal, for example).  Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an over-view). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an at-tempt towards computational metaphor identifi-cation. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met-
aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1  workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different ap-proach to metaphor understanding based on lexi-cal semantics and discourse analysis was intro-duced by Strzalkowski et al. (2013). Space con-straints limit our discussion about their work in this article, however in the foregoing, our discus-sion is largely consistent with their framework. 4 Affect in Metaphors Affect in language is understood to mean the atti-tude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008).  It is expressed through multiple means, many of which are unrelated to metaphor. While affect in text is often associated, at least in theory, with a variety of basic emotions (anger, fear, etc.), it is generally possible to classify the set of possible affective states by polarity: positive, negative, and sometimes neutral. Affect is also considered to have a graded strength, sometimes referred to as intensity.  Our approach to affect in metaphor has been vetted not only by our core linguistic team but also by an independent team of linguist-analysts with whom we work to understand metaphor across several language-culture groups. Our re-search continues to show no difficulties in com-prehension or disagreement across languages concerning the concept of linguistic affect, of its application to metaphor, and of its having both polarity and intensity.  5 Related Research: sentiment and af-fect There is a relatively large volume of research on sentiment analysis in language (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; inter alia) that aim at detecting polarity of text, but is not specifically concerned with metaphors. A number of systems were de-veloped to automatically extract writer?s senti-                                                1 The First Workshop on Metaphor in NLP. http://aclweb.org/anthology//W/W13/W13-09.pdf 
43
ment towards specific products or services such as movies or hotels, from online reviews (e.g., Turney, 2002; Pang and Lee, 2008) or social me-dia messages (e.g., Thelwall et al., 2010). None of these techniques has been applied specifically to metaphorical language, and it is unclear if the-se alone would be sufficient due to the relatively complex semantics involved in metaphor inter-pretation. Socher et al. (2013 cite) have recently used recursive neural tensor networks to classify sentences into positive/negative categories. However, the presence of largely negative con-cepts such as ?poverty? in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2.  While presence of affect in metaphorical lan-guage is well documented in linguistic and psy-cholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who pro-posed various models of metaphor affect classifi-cation based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approach-es, which are closely related to sentiment analy-sis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it.  In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the metaphoric expression contribute to its polarity and strength. For exam-ple, we may want to know what is the affect conveyed about the Government as a target con-cept of the metaphor in ?Government regulations are crushing small businesses.? and how it dif-fers in  ?Government programs help to eradicate poverty in rural areas.? or in ?Feds plan to raise the tax on the rich.? In all these examples, there is a subtle interplay between the prior affect as-sociated with certain words (e.g., ?crush?, ?pov-erty?) and the semantic role they occupy in the sentence (e.g., agent vs. patient vs. location, etc.). Our objective is to develop an approach that can better explain such differences. Not sur-prisingly, in one of the target domains we are investigating, the Economic Inequality domain,                                                 2 https://www.cs.york.ac.uk/semeval-2013/task2/ 
there is considerable agreement on the basic atti-tudes across cultures towards the key target con-cepts: poverty is negative, wealth is positive, taxation is largely negative, and so on. This is in a marked contrast with another Target domain, the Governance domain where the target con-cepts tend to be neutral (e.g. bureaucracy, regula-tions etc.) Another important motivation in developing our approach (although not discussed in this pa-per) is to obtain a model of affect that would help to explain empirically why metaphorically rich language is considered highly influential. Persua-sion and influence literature (Soppory and Dillard, 2002) indicates messages containing metaphorical language produce somewhat great-er attitude change than messages that do not. However, some recent studies (e.g., Broadwell et al., 2012) found that lexical models of affect, sentiment, or emotion in language do not corre-late with established measures of influence, con-trary to expectations. Therefore, a different ap-proach to affect is needed based both on lexical and semantic features. We describe this new model below, and show some preliminary results in applications to metaphors interpretation. 6 Basic Affect Calculus The need for a new approach to affect arises from the inability of the current methods of sen-timent analysis to capture the affect that is con-veyed by the metaphor itself, which may be only a part of the overall affect expressed in a text. Affect conveyed in metaphors, while often more polarized than in literal language, is achieved using subtler, less explicit, and more modulated expressions. This presents a challenge for NLP approaches that base affect determination upon the presence of explicit sentiment markers in language that may mask affect arising from a metaphor. This problem becomes more challeng-ing when strong, explicit sentiment markers are present in a surrounding context or when the atti-tude of the speaker/writer towards the target con-cept is considered.  Our initial objective is thus to detect and clas-sify the portion of affect that the speaker/writer is trying to convey by choosing a specific meta-phor. The observables here are the linguistic metaphors that are actually uttered or written; therefore, our method must be able to determine affect present in the linguistic metaphors first and then extrapolate to the conceptual metaphor based on evidence across multiple uses of the 
44
same metaphor. Conceptual metaphors are posit-ed by instances of linguistic metaphors that point to the same source domain. We choose initially to model the speaker/writer perspective; howev-er, it may also be important to determine the ef-fect that a metaphor has on the reader/listener, which we do not address here. Affect in metaphor arises from the juxtaposi-tion of a Source and a Target domain through the relations explicated in linguistic metaphors. The-se relations typically involve one or more predi-cates from the source domain that are applied to a target concept. For example, in ?Government regulations are crushing small businesses.? the relation ?crushing? is borrowed from a concrete source domain (e.g., Physical Burden), and used with an abstract target concept of ?government regulation? which becomes the agentive argu-ment, i.e., crushed(GovReg, X), where X is an optional patientive argument, in this case ?small businesses?. Thus, government regulation is said to be doing something akin to ?crushing?, a harmful and negative activity according to the Affective Norms in English (ANEW) psycholin-guistic database (Bradley and Lang, 1999). Since ?government regulation? is doing something negative, the polarity of affect conveyed about it is also negative. The ANEW lexicon we are us-ing contains ratings of ~100K words. The origi-nal ANEW lexicon by Bradley and Lang was expanded following the work done by Liu et al. (2014) in expanding the MRC imageability lexi-con. While other sources of valence judgments exist such as NRC (Mohammad et al., 2013) and MPQA (Weibe and Cardie, 2005), there are limi-tations ? for instance ? NRC lexicon rates each words on a positive or negative scale, which does not allow for more fine-grained analysis of strength of valence.  Calculation from Table 1 is further general-ized by incorporating the optional second argu-ment of the relation and the role of the target concept (i.e., agentive or patientive). Thus, if X=?small business? as in the example above, the complete relation becomes crushed(GovReg, 
SmBus), which retains negative affect assuming that ?small business? is considered positive or at least neutral, an assessment that needs to be es-tablished independently. The above calculations are captured in the Af-fect Calculus (AC), which was derived from the sociolinguistic models of topical positioning and disagreement in discourse (Broadwell et al., 2013).     The Affect Calculus was conceived as a hypo-thetical model of metaphorical affect, involving the metaphor target, the source relation, as well as the arguments of this relation, one of which is the target itself. The basic version of the AC is shown in Table 1. We should note that the AC allows us to make affect inferences about any of the elements of the metaphoric relation given the values of the remaining elements. We should also note that this calculus does not yet incorpo-rate any discernable prior affect that the target concept itself may carry. When the target con-cept may be considered neutral (as is ?govern-ment regulation? when taken out of context) this table allows us to compute the affect value of any linguistic metaphor containing it. This is un-like the target concepts such as ?poverty? which bring their prior affect into the metaphor. We will return to this issue later. In the Affect Calculus table, Relation denotes a unary or binary predicate (typically a verb, an adjective, or a noun). In the extended version of the AC (Section 6) Relation may also denote a compound consisting of a predicate and one or more satellite arguments, i.e., arguments other than AGENT or PATIENT, such as ORIGIN or DES-TINATION for motion verbs, etc.  7 Extended Affect Calculus The basic Affect Calculus does not incorporate any prior affect that the target concept might bring into a metaphor. This is fine in some do-mains (e.g., Government), where most target concepts may be considered neutral. But in other target domains, such as the Economic Inequality domain, many of the target concepts have a 
Relation type Type 1 (proper-tive) Rel(Target) Type 2 (agentive) Rel (Target, X) Type 3 (patientive) Rel(X, Target) Relation/X  X ? neutral X < neutral X ? neutral X < neutral Positive POSITIVE POSITIVE ? UNSYMP POSITIVE ? SYMPAT Negative NEGATIVE ? UNSYMP ? SYMPAT ? SYMPAT ? SYMPAT Neutral NEUTRAL NEUTRAL ? NEUTRAL NEUTRAL ? NEUTRAL Table 1.  A simple affect calculus specifies affect polarity for linguistic metaphors using a 5-point polar-ity scale [negative < unsympathetic < neutral < sympathetic < positive]. X is the second argument. 
45
strong prior affect in most cultures (e.g., ?pov-erty? is universally considered negative). We thus need to incorporate this prior affect into our calculation whenever an affect-loaded target concept is invoked in a metaphor. Where the basic Affect Calculus simply imposes a context-borne affect upon a neutral target concept, the Advanced Affect Calculus must combine it with the prior affect carried by the target concept, de-pending upon the type of semantic context. As already discussed, we differentiate 3 basic se-mantic contexts (and additional contexts in the extended Affect Calculus discussed in the next section) where the target concept is positioned with respect to other arguments in a metaphorical expression:  ? Propertive context is when a property of a Target is specified (e.g. deep poverty, sea of wealth) ? Agentive context is when the Target appears as an agent of a relation that may involve an-other concept (Argument X) in the patient role (e.g. Government regulations are crush-ing?, Government programs help?) ? Patientive context is when the Target ap-pears in the patient role that involves another concept (possibly implicit, Argument X) in the agent role. (e.g. ?eradicate poverty., ?.navigate government bureaucracy)  Table 1 (in the previous section) specifies how to calculate the affect expressed towards the tar-get depending upon the affect associated with the Relation and the Argument X. In the Advanced Affect Calculus, this table specifies the context-borne affect that interacts with the affect associ-ated with the target. When the target prior affect is unknown or assumed neutral, the AC table is applied directly, as explained previously. When the target has a known polarized affect, either positive or negative, the values in the AC table are used to calculate the final affect by combin-ing the prior affect of the target with an appro-priate value from the table. This is necessary for affect-loaded target concepts such as ?poverty? or ?wealth? that have strong prior affect and can-not be considered neutral.  In order to calculate the combined affect we define two operators ? and ?. These operators form simple polarity algebra shown in Table 2. When the Target is in a Patientive relation, we use ?  to combine its affect with the context val-ue from the AC table; otherwise, we use ? .  In the table for ? operator, we note that combining opposing affects from the Target and the Rela-
tion causes the final affect to be undetermined (UND). In such cases we will take the affect of the stronger element (more polarized score) to prevail. ?  pos neg neu  ?  pos neg neu 
pos pos neg pos pos pos UND pos 
neg neg pos neg neg UND neg neg 
neu pos neg neu neu pos neg neu Table 2: Polarity algebra for extended affect calculus  More specifically, in order to determine the combined polarity score in these cases, we com-pute the distance between each element?s ANEW score and the closest boundary of the neutral range of scores. For example, ANEW scores are assigned on a 10-point continuum (derived from human judgments on 10-point Likert scale) from most negative (0) to most positive (9). Values in the range of 3.0 to 5.0 may be considered neutral (this range can be set differently for target con-cepts and relations): ? Poverty affect score = 1.67 (ANEW) ? 3 (neutral lower) = -1.33 ? Grasp affect score = 5.45 (ANEW) ? 5 (neutral upper)= +0.45 Consider the expression ?poverty?s grasp?. Since poverty is a polarized target concept in Propertive position, we use ? operator to com-bine its affect value with that of Relation (grasp). The result is negative: ? ?Poverty?s grasp? affect score (via AC?) = -1.33 + 0.45 = -0.82 (negative) When the combined score is close to 0 (-0.5 to +0.5) the final affect is neutral. 7.1 Exceptions The above calculus works in a majority of cases, but there are exceptions requiring specialized handling. An incomplete list of these is below (and cases will be added as we encounter them): Reflexive relations. In some cases the target is in the agentive position but semantically it is also a patient, as in ?poverty is spreading?. These cases need to be handled carefully ? although the current AC may be able to handle them in some contexts. When interpreted as an agentive rela-
46
tion, the affect of ?poverty is spreading? comes out as undetermined but would likely be output as negative on the basis of the strong negative affect associated with poverty (vs. weaker posi-tive affect of ?spreading?). When handled as a patientive relation (an unknown force is spread-ing poverty), it comes out clearly and strongly negative. Similarly, ?wealth is declining? is best handled through patientive relation. Therefore, for this AC we will treat intransitive relations as patientive.  Causative relations. Some relations denoted by causative verbs such as ?alleviate?, ?mitigate? or ?ease? appear to presuppose that their patient argument has negative affect, and their positive polarity already incorporates this assumption. Thus, ?alleviate? is best interpreted as ?reduce the negative of?, which inserts an extra negation into the calculation. Without considering this extra negation we would calculate ?alleviate(+) poverty(-)? as negative (doing something posi-tive to a negative concept), which is not the ex-pected reading. Therefore, the proposed special handling is to treat ?alleviate? and similar rela-tions as always producing positive affect when applied to negative targets.  8 Extensions to Basic Affect Calculus The basic model presented in the preceding sec-tion oversimplifies certain more complex cases where the metaphoric relation involves more than 2 arguments. Consequently, we are consid-ering several extensions to the basic Affect Cal-culus as suggested below. The foregoing should be treated as hypotheses subject to validation.  One possible extension involves relations rep-resented by verbs of motion (which is a common source domain) that involve satellite arguments such as ORIGIN and DESTINATION in addition to the main AGENT and PATIENT roles. Any polarity associated with these arguments may impact af-fect directed at the target concept appearing in one of the main role positions. Likewise, we need a mechanism to calculate affect for target concepts found in one of the satellite roles. In ?Federal cuts could push millions into poverty? the relation ?push into? involves three arguments: AGENT (Federal cuts), PATIENT (millions [peo-ple]) and DESTINATION (poverty). In calculating affect towards ?Federal cuts? it is not sufficient to consider the polarity of the predicate ?push? (or ?push into?), but instead one must consider the polarity of ?push into (poverty)? as the compo-site agentive relation involving ?federal cuts?. 
The polarity of this composite, in turn, depends upon the polarity of its destination argument. In other words: polarity(Rel(DEST)) = polarity (DEST) Thus, if ?poverty? is negative, then pushing someone or something into poverty is a harmful relation. Assuming that ?millions [people]? is considered at least neutral, we obtain negative affect for ?Federal cuts? from the basic Affect Calculus table. An analogous situation holds for the ?ORIGIN? argument, with the polarity reversed. Thus: polarity (Rel (ORIGIN)) = ~polarity (ORIGIN) In other words, the act of removing something from a bad place is helpful and positive. For ex-ample, in ?Higher retail wages would lift Ameri-cans out of poverty? the relation compound ?lift out of (poverty)? is considered helpful/positive. Again, once the polarity of the relation com-pound is established, the basic affect calculus applies as usual, thus we obtain positive affect towards ?higher retail wages?. In situations when both arguments are present at the same time and point towards potentially conflicting outcomes, we shall establish a precedence order based on the evidence from human validation data. Another class of multi-argument relations we are considering includes verbs that take an IN-STRUMENT argument, typically signaled by ?with? preposition. In this case, affect inference for the relation compound is postulated as fol-lows: polarity (Rel (INSTR))    = polarity (INSTR) if polarity(INSTR) < neutral   = polarity (Rel) otherwise In other words, using a negative (bad) instru-ment always makes the relation harmful, while using a positive or neutral instrument has no ef-fect on the base predicate polarity.  Other types of multi-argument relations may require similar treatment, and we are currently investigating further possible extensions. In all cases not explicitly covered in the extended Af-fect Calculus, we shall assume the default condi-tion that other satellite arguments (such as TIME, LOCATION, etc.) will have no impact on the po-larity of the source relation compound. In other words: polarity (Rel (s-role)) =default polarity (Rel) 9 Evaluation and Results For an evaluation, our objective is to construct a test that can evaluate the ability of an automated system to correctly identify and classify the af-
47
fect associated with linguistic and conceptual metaphors. A series of naturally occurring text samples containing a linguistic metaphor about a target concept are presented as input to the sys-tem. The system outputs the affect associated with the metaphor, as positive, negative, or neu-tral. The system output is then compared to hu-man generated answer key resulting in an accu-racy score. The evaluation thus consists of two components:  1. Determining the ground truth about affect in test samples;  2. Measuring the automated system?s ability to identify affect correctly.  Step 1 is done using human assessors who judge affect in a series of test samples. Assessors are presented with brief passages where a target concept and a relation are highlighted. They are asked to rank their responses on a 7-point scale for the following questions, among others: ? To what degree does the above passage use metaphor to describe the highlighted concept? ? To what degree does this passage convey an idea that is either positive or negative? It is strictly necessary that input to the system be metaphorical sentences, since affect may be associated with non-metaphoric expressions as well; in fact, some direct expressions may carry stronger affect than subtle and indirect meta-phors. This is why both questions on the survey are necessary: the first focuses the assessor?s at-tention on the highlighted metaphor before ask-ing about affect. If the purpose of the test is to measure the accuracy of assigning affect to a metaphor, then accuracy should be measured against the subset of expressions judged to be metaphorical.  The judgments collected from human asses-sors are tested for reliability and validity. Relia-bility among the raters is computed by measuring intra-class correlation (ICC) (McGraw & Wong, 1996; Shrout & Fleiss, 1979). Typically, a coef-ficient value above 0.7 indicates strong agree-ment. In general, our analyses have shown that we need approximately 30 or more subjects in order to obtain a reliability coefficient of at least 0.7. In addition, certain precautions were taken to ensure quality control in the data. We used the following criteria to discard a subject?s data: (1) completed the task too quickly (i.e., averaged fewer than 10 seconds for each passage); (2) gave the same answer to 85% or more of the test items; (3) did not pass a simple language profi-ciency test; or (4) did not provide correct an-swers to a set of randomly inserted control pas-
sages which have been previously judged by ex-perts to be unequivocally literal or metaphorical. Human judgments are collected using Amazon?s Mechanical Turk services. For each passage in surveys, we would collect at least 30 viable judgments. In addition, we have native language speakers who have been rigorously trained to provide expert judgments on metaphor and affect identification task. Table 3 shows the intra-class correlations for affect determination amongst Mechanical Turk subjects. Experiments were conducted in 4 languages: English, Spanish, Rus-sian, and Farsi.   English Spanish Russian Farsi Metaphor 0.864 0.853 0.916 0.720 Affect 0.924 0.791 0.713 0.797 Table 3: Intra-class correlations for metaphor and affect assessment by Mechanical Turk sub-jects In Figure 1, we present partial evidence that the human assessment collection method cap-tures the phenomenon of affect associated with metaphors. The chart clearly shows that affect tends to be more polarized in metaphors than in literal expressions. The chart is based on more than 11,000 affect judgments for English linguis-tic metaphors and literal expressions about Gov-ernance concepts. We see a highly pronounced tendency towards the polarization of affect (both positive and negative). Ratings of affect (y-axis) in metaphoric expressions (columns 5-7) are judged to be stronger, and in particular more negative than the literal expressions (columns 1-3). A similar trend occurs with other target con-cepts as well as other languages, although the data are less reliable due to smaller test samples. Once an answer key is established using the aforementioned procedures, system accuracy can be determined from a confusion matrix as shown in Table 4. In Table 4, we show system assign-ment of affect versus answer key for English Governance and Economic Inequality target metaphors. Overall accuracy across positive, negative and neutral affect for English test set of 220 samples is 74.5%. Analogous confusion ma-trices have been constructed for Spanish, Russian and Farsi. NLP resources such as parser and lex-icons for the languages other than English are not as robust or well rounded; therefore affect classi-fication accuracy in those languages is impacted.   
48
 Figure 1: Distribution of affect polarity in hu-man judgment of English literal and metaphori-cal expressions from the Governance domain. Metaphoricity of an expression (x-axis) is judged from highly literal (1) to highly metaphorical (7)   Table 5 shows the accuracy of affect detection for expressions that the system determined to be metaphors across all four languages under inves-tigation. Evaluation set for numbers reported in Table 5 contains a total of 526 linguistic meta-phors in these four languages.   English Affect Sample size = 220 System identified as Positive Negative Neutral 
Answ
er Key 
Positive 40 16 3 Nega-tive 12 109 1 Neutral 10 14 15  Table 4: Confusion matrix for affect classifi-cation in English linguistic metaphors in Gov-ernance and Economic Inequality Domain. Accu-racy is 74.5%   English Spanish Russian Farsi 
Accuracy 74.5% 71% 59% 64% Table 5: Performance on affect classification for linguistic metaphors in four languages 10 Conclusion In this paper we presented a new approach to automatic computing of affect in metaphors that exploits both lexical and semantic information in metaphorical expressions. Our method was eval-uated through a series of rigorous experiments 
where more than several dozen of qualified as-sessors judged hundreds of sentences (extracted from online sources) that contained metaphorical expressions. The objective was to capture affect associated with the metaphor itself. Our system can approximate human judgment with accuracy ranging from 59% for Russian to 74% for Eng-lish. These results are quite promising. The dif-ferences are primarily due to varied robustness of the language processing tools (such as parsers and morphological analyzers) that are available for each language. We note that a direct compar-ison to lexical approaches such as described by Kozareva (2013) is not possible at this time due to differences in assessment methodology, alt-hough it remains one of our objectives.  Our next step is to demonstrate that the new way of calculating affect can lead to a reliable model of affective language use that correlates with other established measures of influence.  Acknowledgements Supported by the Intelligence Advanced Re-search Projects Activity (IARPA) via Depart-ment of Defense US Army Research Laboratory contract number W911NF-12-C-0024. The U.S. Government is authorized to reproduce and dis-tribute reprints for Governmental purposes not-withstanding any copyright annotation thereon.  Disclaimer: The views and conclusions con-tained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either ex-pressed or implied, of IARPA, DoD/ARL, or the U.S. Government. References David W. Allbritton, Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-based schemas and text Representations: making connections through conceptual metaphors, Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(3):612-625. Eric P. S. Baumer, James P. White, and Bill Tomlin-son. 2010. Comparing semantic role labeling with typed dependency parsing in computational meta-phor identification. In Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14?22, Los Angeles, California.  Margaret M. Bradley, and Peter Lang. 1999. Affective norms for English words (ANEW): Instruction manual and affective ratings. Technical Report C-2. University of Florida, Gainesville, FL. George Aaron Broadwell, Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Tay-
49
lor, Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb. 2013. Using imageability and topic chain-ing to locate metaphors in linguistic corpora. In Proceedings of International Conference on So-cial Computing, Behavioral-Cultural Modeling, & Prediction, pages 102?109. Washington D.C. George Aaron Broadwell, Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Tay-lor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu and Nick Webb. 2012. Modeling socio-cultural phenomena in discourse. Journal of Natural Lan-guage Engineering, pages 1?45. Cambridge Press. Claudia Caffi, and Richard W. Janney. 1994. Towards a pragmatics of emotive communication. Jour-nal of Pragmatics, 22:325?373. Jaime Carbonell. 1980. Metaphor: A key to extensible semantic analysis. In Proceedings of the 18th An-nual Meeting on Association for Computational Linguistics. Jonathan, Charteris-Black. 2002. Second language figurative proficiency: A comparative study of Malay and English. Applied Linguistics 23(1):104?133. Dan, Fass. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computa-tional Linguistics, 17:49-90 Jerome Feldman, and Srinivas Narayanan. 2004. Em-bodied meaning in a neural theory of language. Brain and Language, 89(2):385?392. Christiane D. Fellbaum. 1998. WordNet: An electron-ic lexical database (1st ed.). MIT Press. Matt Gedigian, John Bryant, Srini Narayanan and Branimir Ciric. 2006. Catching Metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding ScaNaLU 2006, pages 41?48. New York City: NY. Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jau-har, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders and Eduard Hovy. 2013. Identi-fying Metaphorical Word Use with Tree Kernels. In the Proceedings of the First Workshop on Met-aphor in NLP, (NAACL). Atlanta. Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Linguistics, COLING ?04. Zornitsa Kozareva. 2013. Multilingual Affect Polarity and   Valence Prediction in Metaphor-Rich Texts. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013) Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computation-al Approaches to Figurative Language, pages 13?20. Rochester, NY. George Lakoff, and Mark Johnson. 1980. Metaphors we live by. University Of Chicago Press, Chicago, Illinois. 
George, Lakoff. 2001. Moral politics: what conserva-tives know that liberals don?t. University of Chi-cago Press, Chicago, Illinois. Ting Liu, Kit Cho, George Aaron Broadwell, Samira Shaikh, Tomek Strzalkowski, John Lien, Sarah Taylor, Laurie Feldman, Boris Yamrom, Nick Webb, Umit Boz and Ignacio Cases. 2014. Auto-matic Expansion of the MRC Psycholinguistic Da-tabase Imageability Ratings. In Proceedings of 9th Language Resources and Evaluation Conference, (LREC 2014)Reykjavik, Iceland. Liisa, Malkki.  1992. National geographic: The root-ing of people and the territorialization of national identity among scholars and refugees. Society for Cultural Anthropology, 7(1):24?44. James Martin. 1988. A computational theory of meta-phor. Ph.D. Dissertation. Kenneth O. McGraw and S. P. Wong. 1996. Forming inferences about some intraclass correlation coef-ficients. Psychological Methods, 1(1): 30?46. Mohammad, S.M., S. Kiritchenko, and X. Zhu. 2013. NRC-Canada: Building the state-of-the-art insen-timent analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA, June 2013. Michael Mohler, David Bracewell, David Hinote, and Marc Tomlinson. 2013. Semantic signatures for example-based linguistic metaphor detection. In The Proceedings of the First Workshop on Meta-phor in NLP, (NAACL), pages 46?54. Musolff, Andreas. 2008. What can critical metaphor analysis add to the understanding of racist ideolo-gy? Recent studies of Hitler?s anti-semitic meta-phors, critical approaches to discourse analysis across disciplines. Critical Approaches to Dis-course Analysis Across Disciplines, 2(2):1?10. Kieran, O?Halloran. 2007. Critical discourse analysis and the corpus-informed interpretation of meta-phor at the register level. Oxford University Press Charles E. Osgood. 1981. The cognitive dynamics of synaesthesia and metaphor. In Proceedings of the National Symposium for Research in Art. Learn-ing in Art: Representation and Metaphor, pages 56-80. University of Illinois Press. Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135, January. Allan Pavio and Mary Walsh. 1993. Psychological processes in metaphor comprehension and memory. In Andrew Ortony, editor, Meta-phor and thought (2nd ed.). Cambridge: Cambridge University Press. Patrick E Shrout and Joseph L Fleiss. 1979. Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2):420?428. Ekaterina Shutova. 2010. Models of metaphors in NLP. In Proceedings of ACL 2010. Uppsala, Swe-den. 
50
Ekaterina Shutova and Simone Teufel. 2010a. Meta-phor corpus annotated for source - target domain mappings. In Proceedings of Language Resources and Evaluation Conference 2010. Malta. Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages 688?697. Ekaterina Shutova, Tim Van de Cruys, and Anna Korhonen. 2012. Unsupervised metaphor para-phrasing using a vector space model In Proceed-ings of COLING 2012, Mumbai, India Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. 2013. In Proceedings Conference on Empir-ical Methods in Natural Language Processing (EMNLP 2013). Seattle, USA.  Sopory, P. and Dillard, J. P. (2002), The Persuasive Effects of Metaphor: A Meta-Analysis. Human Communication Research, 28: 382?419. doi: 10.1111/j.1468-2958.2002.tb00813.x Gerard Steen. 1994. Understanding metaphor in lit-erature: An empirical approach. London: Long-man. Carlo, Strapparava, and Rada Mihalcea. 2007. Semeval-2007 task 14: Affective text. In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations, pages 70?74. Association for Computational Linguistics. Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases and Kyle Elliott. 2013. Robust extraction of metaphor from novel data. In Proceedings of Workshop on Metaphor in NLP, NAACL. Atlanta. Mike Thelwall, Kevan Buckley, and Georgios Pato-glou. Sentiment in Twitter events. 2011. Journal of the American Society for Information Science and Technology, 62(2):406?418. Paul H. Thibodeau and Lera Boroditsky. 2011. Meta-phors We Think With: The Role of Metaphor in Reasoning. PLoS ONE 6(2): e16782. Peter D, Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised clas-sification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ?02, pages 417?424. Ielka van der Sluis,  and C. Mellish 2008. Toward affective natural language deneration: Empirical investigations. affective language in human and machine. AISB 2008 Proceedings Volume 2. Tony Veale and Guofu Li. 2012. Specifying view-point and information need with affective meta-phors: a system demonstration of the metaphor magnet web app/service. In Proceedings of the ACL 2012 System Demonstrations, ACL ?12, pag-es 7?12. Janyce, Wiebe and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. In Language Resources and Evaluation. 
Yorick, Wilks. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cambridge University Press, Cambridge, U.K., 329?348. Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton. 2013. Automatic Metaphor Detection us-ing Large-Scale Lexical Resources and Conven-tional Metaphor Extraction. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta.  Wiebe, J., Wilson, T., and Cardie, C.: Annotating expressions of opinions and emotions in  lan-guage. Language Resources and Evaluation, 39(2-3), pp. 165-210 (2005). Li Zhang and John Barnden. 2010. Affect and meta-phor sensing in virtual drama. International Journal of Computer Games Technology. Vol. 2010.  
51
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 210?220,
Dublin, Ireland, August 23, 2014.
Discovering Conceptual Metaphors Using Source Domain Spaces 
  Samira Shaikh1, Tomek Strzalkowski1, Kit Cho1, Ting Liu1, George Aaron Broadwell1, Laurie Feldman1, Sarah Taylor2, Boris Yamrom1, Ching-Sheng Lin1, Ning Sa1, Ignacio Cases1, Yuli-ya Peshkova1 and Kyle Elliot3  1State University of New York  ? University at Albany 2Sarah M. Taylor Consulting LLC samirashaikh@gmail.com     
3Plessas Experts Network 
 Abstract This article makes two contributions towards the use of lexical resources and corpora; specifically making use of them for gaining access to and using word associations. The direct application of our approach is for detecting linguistic and conceptual metaphors automatically in text. We describe our method of building conceptual spaces, that is, defining the vocabulary that characterizes a Source Domain (e.g., Disease) of a conceptual metaphor (e.g., Poverty is a Disease). We also describe how these conceptual spaces are used to group linguistic metaphors into conceptual metaphors. Our method works in multiple languages, including English, Spanish, Russian and Farsi. We provide details of how our method can be evaluated and evaluation results that show satisfactory performance across all languages. 1 Introduction Metaphors are communicative devices that are pervasive in discourse. When understood in a cultural context, they provide insights into how a culture views certain salient concepts, typically broad, abstract concepts such as poverty or democracy. In our research, we are focusing on metaphors on targets of governance, economic inequality and democracy, although our approach works for metaphors on any target. Suppose it is found in a culture that its people use metaphors when speaking of poverty; for example, they may talk about ?symptom of poverty? or that ?poverty infects areas of the city?. These expressions are linguistic metaphors that are instances of a broader conceptual metaphor: Poverty is a Disease. Similarly, if it is found that common linguistic metaphors about poverty for peoples of a culture include ?deep hole of poverty? and ?fall into poverty?, it would lead to the conceptual metaphor: Poverty is an Abyss. A communicator wishing to speak of ways to deal with poverty would use metaphors such as ?treat poverty? and ?cure poverty? to make their framing consistent with the conceptual metaphor of Disease, whereas she would use metaphors such as ?lift out of poverty? when speaking to people who are attuned to the Abyss conceptual metaphor. Here Disease and Abyss are source domains, and poverty is the target domain. Relations, like ?symptom of?, ?infect? and ?fall into? from the respective source domains are mapped onto the target domain of poverty. In order to discover conceptual metaphors and group linguistic metaphors together, we make use of corpora to define the conceptual space that characterizes a source domain. We wish to discover the set of relations that are used literally for a given source domain, and would create metaphors if applied to some other target domain. That is, we wish to automatically discover that relations such as ?symptom?, ?infect?, ?treat? and ?cure? characterize the source domain of Disease, for example. To create the conceptual spaces, we employ a fully automated method in which we search a balanced corpus using specific search patterns. Search patterns are so created as to look for co-occurence of                                                 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/  
210
relations with members of a given source domain. Relations could be nouns, verbs, verb phrases and adjectives that are frequently used literally within a source domain. In addition, we calculate the frequency with which relations occur in a given source domain, or Relation Frequency. We then calculate the Inverse Domain Frequency (IDF), a variant of the inverse document frequency measure quite commonly used in field of information retrieval; the IDF captures the degree of distribution of relations across all source domains under consideration. Using these two measures, the relation frequency and inverse domain frequency, we are able to rank relations within a source domain. This ranked list of relations are then used to group linguistic metaphors belonging to the same source domain together. A group of linguistic metaphors so formed is a conceptual metaphor.  2 Related Research Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as de-fined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that attempt to correlate metaphor semantics with their us-age in naturally occurring text but generally lack robust tools to do so; and (3) social science ap-proaches, particularly in psychology and anthropology that seek to explain how people deploy and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated examples.     Metaphor study in yet other disciplines has included cognitive psychologists (e.g., Allbritton, McKoon & Gerrig, 1995) who have focused on the way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on ref-ugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Sci-ence underpinning (Musolff, 2008; Lakoff, 2001).      In computational investigations of metaphor, knowledge-based approaches include MetaBank (Mar-tin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfacto-rily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example).     Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is diffi-cult to produce in large quantities and may not be easily generated in different languages.  More recently, several important approaches to metaphor extraction have emerged from the IARPA Metaphor program, including Broadwell et al (2013), Strzalkowski et al. (2014), Wilks et al (2013), Hovy et al (2013) inter alia. These papers concentrate on the algorithms for detection and classification of individual linguistic metaphors in text rather than formation of conceptual metaphors in a broader cultural context. Taylor et al (2014) outlines the rationale why conceptual level metaphors may provide important insights into cross-cultural contrasts. Our work described here is a first attempt at automatic discovery of conceptual metaphors operating within a culture directly from the linguistic evidence in language. 3 Our Approach The process of discovering conceptual metaphors is necessarily divided into two phases: (1) collecting evidence about potential source domains that may be invoked when metaphorical expressions are used; and (2) building a conceptual space for each sufficiently evidenced source domain so that linguistic metaphors can be accurately classified as instances of appropriate conceptual metaphors. In 
211
this paper, we concentrate on the second phase only. Strzalkowski et al (2013) in their work have described a data-driven linguistic metaphor extraction method and our approach builds upon their work. During the source domain evidencing phase, we established a set of 50 source domains that operate frequently with the target concepts we are focusing on (government, bureaucracy, poverty, wealth, taxation, democracy and elections). These domains were a joint effort of several teams participating in the Metaphor program and we are taking this set as a starting point. These are shown in Table 1.   A_GOD	 ? CONFINEMENT	 ? GAME	 ? MONSTER	 ? PLANT	 ?A_RIGHT	 ? CRIME	 ? GAP	 ? MORAL_DUTY	 ? PORTAL	 ?ABYSS	 ? CROP	 ? GEOGRAPHIC_FEATURE	 ? MOVEMENT	 ? POSITION	 ?AND	 ?CHANGE	 ?OF	 ?	 ?POSITION	 ?ON	 ?A	 ?SCALE	 ?ADDICTION	 ? DARKNESS	 ? GREED	 ? NATURAL_PHYSICAL_FORCE	 ? RACE	 ?ANIMAL	 ? DESTROYER	 ?	 ? HUMAN_BODY	 ? OBESITY	 ? RESOURCE	 ?BATTLE	 ? DISEASE	 ? IMPURITY	 ? PARASITE	 ? STAGE	 ?BLOOD_STREAM	 ? ENERGY	 ? LIGHT	 ? PATHWAY	 ? STRUGGLE	 ?BODY_OF_WATER	 ? ENSLAVEMENT	 ? MACHINE	 ? PHYSICAL_BURDEN	 ? THEFT	 ?BUILDING	 ? FOOD	 ? MAZE	 ? PHYSICAL_HARM	 ? VISION	 ?COMPETITION	 ? FORCEFUL_EXTRACTION	 ? MEDICINE	 ? PHYSICAL_LOCATION	 ? WAR	 ?Table 1. Set of 50 source domains that operate frequently with target concepts being investigated. Only English names are shown for ease of presentation, equivalent sets in Spanish, Russian and Farsi have been created. Some of the domains are self explanatory, while others require a further specification since the labels are sometimes ambiguous. For example, PLANT represents things that grow in the soil, not factories; similarly, BUILDING represents artifacts such as houses or edifices, but not the act of constructing something; RACE refers to a running competition, not skin color, etc.  Consequently, each of these domains need to be seeded with the prototypical representative elements to make the meaning completely clear. This seeding occurs during the first phase of the process when a linguistic expression, such as ?cure poverty? is classified as a linguistic metaphor. This process of classifying ?cure poverty? as metaphorical is described in detail in Strzalkowski et al. (2013). Part of the seeding process is to establish that a source domain different than the target domain (here: poverty) is invoked by the relation (here: cure). To find the source domain where ?cure? is typically used literally, we form a linguistic pattern [cure [OBJ: X/nn]] (derived automatically from the parsed metaphoric expression) which is subsequently run through a balanced language corpus. Arguments matching the variable X are then clustered into semantic categories, using lexical resources such as Wordnet (Felbaum, 1998) and the most frequent and concrete category is selected as a possible source domain (proto-source domain). From the balanced language corpus, it is possible to compute the frequency with which the arguments resulting from search appear with relation (?cure?). We determine concreteness by looking up concreteness score in MRC psycholinguistic database (Coltheart 1981, Wilson 1988). As may be expected, the initial elements of the proto-source obtained from the above patterns will include: disease, cancer, plague, etc. These become the seeds of the source domain DISEASE in our list. The same process was performed for each of the 50 domains listed here, for each of the 4 languages under consideration. Additional Source Domains are continously generated bottom-up fashion by this phase 1 process elaborated above. In Table 2, we show seeds so obtained for a few source domains.    DISEASE	 ? disease,	 ?cancer,	 ?plague	 ?ABYSS	 ? abyss,	 ?chasm,	 ?crevasse	 ?BODY_OF_WATER	 ? ocean,	 ?lake	 ?river,	 ?pond,	 ?sea	 ?PLANT	 ? plant,	 ?tree,	 ?flower,	 ?weed,	 ?shrub,	 ?vegetable	 ?GEOGRAPHIC_FEATURE	 ? land,	 ?land	 ?form,	 ?earth,	 ?mountain,	 ?plateau,	 ?island,	 ?valley	 ?Table 2. Example of seeds corresponding to a few source domains 
212
Once such seeds are obtained, we perform another search through a balanced corpus in the corresponding language to discover relations that characterize the source domains. The purpose of source domain spaces in our research is two-fold: a) to provide a sufficiently complete characterization of a source domain via a list of relations ; and b) such a list of relations should sufficiently distinguish between different source domains. Creating these spaces is phase 2 of the conceptual metaphor discovery process. We search for nouns, verbs and verb phrases, and adjectives that co-occur with seeds of given source domain with sufficiently high frequency and sufficiently high mutual information. Our goal with this process is to approximate normal usage patterns of relations within source domains. The results of balanced corpora search form our conceptual spaces. The balanced corpora we use are English: Corpus of Contemporary American English (Davies, 2008), Spanish:  Corpus del Espa?ol Actual (Davies, 2002), Russian: Russian National Corpus2 and Farsi: Bijankhan Corpus (Oroumchian et al., 2006). In addition to retrieving the relations, we retrieve the frequency with which these relations can be found to co-occur with seeds of a source domain, Relation Frequency (RF). We calculate Inverse Domain Frequency (IDF) of all relations across all 50 source domains using a variant of the inverse document frequency measure. The formula for IDF is as given below:  IDF = log (total number of source domains / total number of source domains a relation appears in)  For example, if a relation such as ?dive into? is found to appear in two source domains, BODY_OF_WATER and GEOGRAPHIC_FEATURE, then the IDF for ?dive into? would be log (50/2). The rank of a relation is computed as the product of RF and IDF. However, computing rank using RF without normalization results in inflated ranks for relations that are quite common across domains even when they do not sufficiently disambiguate between the domains. We assume a normal distribution of frequencies of relations within a source domain and normalize RF by taking its logarithm. We also normalize with respect to seeds within a source domain. If a relation frequency is disproportionately high with a specific seed, we disregard that frequency. For example, one of the seeds for the source domain of BUILDING is ?house?. A search through balanced corpus for nouns adjacent to ?house? revealed a disproportionately large number for ?white?, which is meant to be the White House, and would be disregarded.  In Table 3, we show a few top ranked relations for the source domains DISEASE and BODY_OF_WATER. In columns 1 and 2, we show the source domain and the relation. Column 3 shows the relation frequency and column 4 shows the part of speech of relation (V=verb or verb phrase, N=noun, ADJ=adjective). An RF score of 800 for row 1 indicates that the relation ?diagnose with? appears 800 times with one or more of the seeds we search for source domain DISEASE (?diagnose with cancer?, ?diagnose with disease? and so on. In column 5, we show the position where the relation is commonly found to co-occur with the source domain. For example, ?afflict? in row 2 has a position ?after? which means it appears after DISEASE: ?DISEASE afflict(s)?; whereas row 3 would be read as ?affict with DISEASE? since it appears ?before?. In column 6, we show the normalized RF*IDF score. The highest RF*IDF score for a relation across our spaces is 2.165. From Table 3, we can see that even if  frequency for some relations may be relatively low, their rank would be high if they are strongly associated with a single source domain.    	 ? 1.	 ?Source	 ?Domain	 ? 2.	 ?Relation	 ? 3.	 ?RF	 ? 4.	 ?Type	 ? 5.	 ?Position	 ? 6.	 ?Norm	 ?RF*IDF	 ?1	 ? DISEASE	 ? diagnose	 ?with	 ? 800	 ? V	 ? before	 ? 1.94	 ?2	 ? DISEASE	 ? afflict	 ? 85	 ? V	 ? after	 ? 1.67	 ?3	 ? DISEASE	 ? afflict	 ?with	 ? 33	 ? V	 ? before	 ? 1.52	 ?4	 ? DISEASE	 ? cure	 ?of	 ? 29	 ? N	 ? before	 ? 1.46	 ?5	 ? BODY_OF_WATER	 ? dive	 ?into	 ? 49	 ? V	 ? before	 ? 2.01	 ?6	 ? BODY_OF_WATER	 ? wade	 ?through	 ? 44	 ? V	 ? before	 ? 1.88	 ?7	 ?	 ? BODY_OF_WATER	 ? wade	 ?into	 ? 42	 ? V	 ? before	 ? 1.84	 ?8	 ? BODY_OF_WATER	 ? rinse	 ?in	 ? 41	 ? V	 ? before	 ? 1.80	 ?Table 3. A few top ranking relations for the source domains DISEASE and BODY_OF_WATER. Relations are ranked by their normalized RF*IDF score.                                                 2 http://ruscorpora.ru/en/ 
213
With the conceptual spaces defined in this manner, we can now use them to group linguistic metaphors together. Shaikh et al (2014) have created a repository of thousands of automatically extracted lingusitic metaphors in all four languages, which we are using to create conceptual metaphors. To discover which conceptual metaphors exist within such large sets of linguistic metaphors would be quite challenging, if not impossible, for a human expert. We automatically assign each linguistic metaphor to ranked list of source domains.  Consider the linguistic metaphor ?plunge into poverty?, where the relation is ?plunge into?. We search through our conceptual spaces and retrieve a list of source domains where the relation ?plunge into? may appear. From this list, only the domains that have this relation RF*IDF score higher than a threshold are considered. This threshold is currently assigned to be 0.40, although it is subject to further experimentation. The source domain where the RF*IDF score of ?plunge into? is the highest is chosen as the source domain, along with the next source domains only if the difference in scores is 5% or lower. Tables 4 and 5 depicts this part of algorithm for two relations, ?plunge into? and ?explorar? (from Spanish ? ?explore?). The relation ?plunge into? is thus assigned to BODY_OF_WATER source domain. ?explorar? is assigned to GEOGRAPHIC_FEATURE and BODY_OF_WATER since difference in RF*IDF scores is less than 5%.  Relation	 ? Source	 ?Domains	 ? RF*IDF	 ?	 ? 	 ? Relation	 ? Source	 ?Domains	 ? RF*IDF	 ?
plunge	 ?into	 ?	 ?
BODY_OF_WATER	 ? 1.82	 ? 	 ?
explorar	 ?
GEOGRAPHIC_FEATURE	 ? 0.77	 ?DARKNESS	 ? 1.28	 ? 	 ? BODY_OF_WATER	 ? 0.76	 ?ABYSS	 ? 0.68	 ? 	 ? PHYSICAL_LOCATION	 ? 0.56	 ?WAR	 ? 0.57	 ? 	 ? PATHWAY	 ? 0.56	 ?GEOGRAPHIC_FEATURE	 ? 0.48	 ? 	 ? BUILDING	 ? 0.41	 ?Table 4 and Table 5. Assigning relations of linguistic metaphor to source domains. ?plunge into? is assigned to BODY_OF_WATER; ?explorar? is assigned to GEOGRAPHIC_FEATURE and BODY_OF_WATER Once this process of assigning linguistic metaphors to source domains is accomplished for all linguistic metaphors in our repository, we validate the resulting conceptual metaphors. A small percentage of metaphors cannot be assigned to any of the 50 Source Domains. We explain the validation process in Section 4. In Tables 6 and 7, we show sample conceptual metaphors in English and Spanish. Our validation process revealed an interesting insight regarding forming conceptual metaphor, wherein they should contain relations that are anchors for that given source domain that we shall describe next.  
 Table 6. A conceptual metaphor in English: POVERTY is a BODY_OF_WATER 
214
 Table 7. A conceptual metaphor in Spanish: POVERTY is a DISEASE 3.1 Anchor relations in Conceptual Metaphors When human assessors are presented with a set of linguistic metaphors and the task to assign them into a source domain, some relations will have stronger impact on their decision that others. For example, ?cure? would almost invariably be assigned to DISEASE domain, while ?dive in? would invoke BODY_OF_WATER domain. Other relations, such as ?spread? or ?fall into? are less specific, however, when paired with highly evocative relations above are likely to be classified the same way. Thus, there are two types of metaphorical relations in linguistic metaphors: (1) the highly evocative relations that unambigously point to a specific source domain ? we shall call them anchors; and (2) the relations that are compatible with the anchor but are not anchors themselves. We can add another class: (3) the relations that are not compatible with a given anchor. Thus, a set of linguistic metaphors that provides evidence for a conceptual metaphor should contain at least some anchor relations and the balance of the set may be composed of anchor-compatible relations. Our current hypothesis is that there should be at least one anchor for each 7 anchor compatible relations for a group of linguistic metaphors to provide a sufficient evidence for a conceptual metaphor.  As part of our validation process, we conducted a series of experiments with human assessors. One of the tasks was to assign a single linguistic metaphor to one of 50 source domains. As an illustrative example, we show in Table 8, one linguistic metaphor. When presented with this example, a majority of assessors chose ENEMY source domain, while DISEASE was selected second. Additionally, there was greater variance among their selections, only 31% chose the top source domain of ENEMY.  Subsequently, human assessors were presented a set of linguistic metaphors where at least one anchor relation was present. In this case, the majority of assessors chose the DISEASE source domain. Even though the ?fight against poverty? example was included in the set, the presence of anchors such as ?cure poverty? and ?treat poverty? lead assessors to choose DISEASE source domain. The variance in selection was also less, a 70% majority choosing DISEASE. We show the conceptual metaphor in Table 9.  The	 ?summit	 ?has	 ?proven	 ?that	 ?there	 ?is	 ?a	 ?renewed	 ?appetite	 ?for	 ?the	 ?fight	 ?against	 ?poverty.	 ?	 ?ENEMY:	 ?31%;	 ?DISEASE:	 ?17%;	 ?ANIMAL,	 ?MONSTER,?.<10%	 ?Table 8. A single linguistic metaphor was assigned a varied number of source domains by human assessors.   Of	 ?course,	 ?many	 ?government	 ?programs	 ?aim	 ?to	 ?alleviate	 ?poverty.	 ?We	 ?seek	 ?to	 ?stimulate	 ?true	 ?prosperity	 ?rather	 ?than	 ?simply	 ?treat	 ?poverty.	 ?Unless	 ?the	 ?fight	 ?against	 ?poverty	 ?is	 ?honestly	 ?addressed	 ?by	 ?the	 ?West,	 ?there	 ?will	 ?be	 ?many	 ?more	 ?Afghanistans.	 ?Above	 ?all,	 ?he	 ?knows	 ?that	 ?the	 ?only	 ?way	 ?to	 ?cure	 ?poverty	 ?is	 ?to	 ?grow	 ?the	 ?economy.	 ?	 ?DISEASE:	 ?70%;	 ?ENEMY:	 ?30%	 ?Table 9. A conceptual metaphor containing anchors. When sample metaphor from Table 8 is included in this set, human assessors still choose the source domain to be DISEASE. 
215
4 Evaluation and Results A group of human experts who are native speakers and have been substantively trained to achieve high levels of agreement (0.78 Krippendorf?s alpha (1970) or higher) form our validation team. In addition, we aim to run crowd-sourced experiments on Amazon Mechanical Turk. In Figure 1, we show a web interface we built to present our human assessors. The task shown here is the assignment of a single linguistic metaphor to one of 50 source domains. Then, we present our validation team with conceptual metaphors we created. Each conceptual metaphor is validated by at least two language experts. This interface is shown in Figure 2. These interfaces are carefully created by our team of social scientists and psychologists, designed to elicit proper responses from native speakers of the language.  
 Figure 1. Interface of task where human assessors select source domain for a single linguistic metaphor.  
216
 Figure 2. Interface of task where human assessors select source domains for a conceptual metaphor. Assessors provide their top two choices along with a description detailing how they made their decision.  In Table 10, we show the number of conceptual metaphors currently in the repository and the accuracy of our method across four languages, as computed by using validation data. We show the number of conceptual metaphors present in the Governance target domain (metaphors about government and bureaucracy), Economic Inequality (dealing with metaphors of poverty, wealth and taxation) and Democracy (democracy and elections metaphors). These conceptual metaphors on the three target domains of Governace, Economic Inequality and Democracy, when compared across cultures could provide deep insight about peoples? perceptions regarding salient concepts. We note that Russian and Farsi performance is lower than that in English and Spanish. The size of balanced corpus and accuracy of lexical tools such as stemmers and morphological analyzers affect performance of our algorithm.  The Farsi balanced corpus is relatively small when compared to English balanced corpus. The smaller size affects computation of statistics such as Relation Frequency and subsequently the thresholds of RF*IDF scores. One improvement we are currently investigating is that the thresholds may be set specifically for a language.   	 ? ENGLISH	 ? SPANISH	 ? RUSSIAN	 ? FARSI	 ?#	 ?of	 ?Governance	 ?Conceptual	 ?Metaphors	 ? 27	 ? 7	 ? 8	 ? 7	 ?#	 ?of	 ?Economic	 ?Inequality	 ?Conceptual	 ?Metaphors	 ? 32	 ? 26	 ? 57	 ? 7	 ?#	 ?of	 ?Democracy	 ?	 ?Conceptual	 ?Metaphors	 ? 51	 ? 16	 ? 18	 ? 8	 ?Total	 ?#	 ?of	 ?	 ?Conceptual	 ?Metaphors	 ? 110	 ? 49	 ? 83	 ? 22	 ?Accuracy	 ?(%)	 ?	 ? 85%	 ? 76%	 ? 67%	 ? 62%	 ?Table 10. Number of conceptual metaphors discovered thus far and performance of our approach across four languages. 
217
5 Conclusion and Future Work In this article, we presented our approach towards automatic discovery of conceptual metaphors directly from linguistic evidence in a given language. We make use of corpora in two unique ways: the first is to discover prototypical seeds that form the basis of source domains and second is to create conceptual spaces that allow us to characterize the relations that operate within source domains automatically. In addition, our approach also allows us to distinguish between source domains as necessary. The validation results show that this is indeed a promising first attempt of tackling a challenging research problem.  We note that the assignment of source domains is limited to the set of 50 in our current prototype. This assumes a closed set of 50 source domains, whereas in reality, there might be many others that operate in the realm of metaphors we are investigating. Although additional source domains are continually being discovered in a bottom-up fashion by the linguistic metaphor extraction process, we cannot account for every source domain that may be relevant. One way of overcoming this limitation would be to define a source domain ?OTHER? that would be the all-encompassing domain accounting for any yet undiscovered domains. The details of how it would be represented are still under investigation.  Another potential improvement to our method is to experimentally refine the threshold score of RF*IDF. Through large scale validation experiments, we could learn the optimal thresholds automatically by using machine learning. 6 Acknowledgements This paper is based on work supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Laboratory contract number W911NF-12-C-0024. The U.S. Government is authorized to reproduce and distribute reprints for Governmental pur-poses notwithstanding any copyright annotation thereon.  Disclaimer: The views and conclusions con-tained herein are those of the authors and should not be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Gov-ernment. References David W. Allbritton, Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-based schemas and text Representa-tions: making connections through conceptual metaphors, Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(3):612-625. Jonathan, Charteris-Black. 2002. Second language figurative proficiency: A comparative study of Malay and English. Applied Linguistics 23(1):104?133. George Aaron Broadwell, Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Taylor, Samira Shaikh, Ting Liu and Kit Cho. 2013. Using Imageability and Topic Chaining to Locate Metaphors in Linguis-tic Corpora. In Proceedings of The 2013 International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction (SBP 2013), Washington D.C., USA. Jaime Carbonell. 1980. Metaphor: A key to extensible semantic analysis. In Proceedings of the 18th Annual Meeting on Association for Computational Linguistics. M. Coltheart. 1981. The MRC Psycholinguistic Database. Quarterly Journal of Experimental Psychology, 33A: 497-505. Davies, Mark. 2008-. The Corpus of Contemporary American English: 450 million words, 1990-present. Availa-ble online at http://corpus.byu.edu/coca/. Davies, Mark. 2002-. Corpus del Espa?ol: 100 million words, 1200s-1900s. Available online at http://www.corpusdelespanol.org. Dan, Fass. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computational Linguistics, 17:49-90 Jerome Feldman, and Srinivas Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and Language, 89(2):385?392. 
218
Christiane D. Fellbaum. 1998. WordNet: An electronic lexical database (1st ed.). MIT Press. Matt Gedigian, John Bryant, Srini Narayanan and Branimir Ciric. 2006. Catching Metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding ScaNaLU 2006, pages 41?48. New York City: NY. Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders and Eduard Hovy. 2013. Identifying Metaphorical Word Use with Tree Kernels. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta. Krippendorff, Klaus. 1970. Estimating the reliability, systematic error, and random error of interval da-ta. Educational and Psychological Measurement, 30 (1),61-70. Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Pro-ceedings of the Workshop on Computational Approaches to Figurative Language, pages 13?20. Rochester, NY. George Lakoff, and Mark Johnson. 1980. Metaphors we live by. University Of Chicago Press, Chicago, Illinois. George, Lakoff. 2001. Moral politics: what conservatives know that liberals don?t. University of Chicago Press, Chicago, Illinois. Liisa, Malkki.  1992. National geographic: The rooting of people and the territorialization of national identity among scholars and refugees. Society for Cultural Anthropology, 7(1):24?44. James Martin. 1988. A computational theory of metaphor. Ph.D. Dissertation. Musolff, Andreas. 2008. What can critical metaphor analysis add to the understanding of racist ideology? Recent studies of Hitler?s anti-semitic metaphors, critical approaches to discourse analysis across disciplines. Critical Approaches to Discourse Analysis Across Disciplines, 2(2):1?10. Kieran, O?Halloran. 2007. Critical discourse analysis and the corpus-informed interpretation of metaphor at the register level. Oxford University Press Farhad Oroumchian, Samira Tasharofi, Hadi Amiri, Hossein Hojjat, Fahime Raja. 2006. Creating a Feasible Corpus for Persian POS Tagging.Technical Report, no. TR3/06, University of Wollongong in Dubai. Samira Shaikh, Tomek Strzalkowski, Ting Liu, George Aaron Broadwell, Boris Yamrom, Sarah Taylor, Laurie Feldman, Kit Cho, Umit Boz, Ignacio Cases, Yuliya Peshkova and Ching-Sheng Lin. 2014. A Multi-Cultural Repository of Automatically Discovered Linguistic and Conceptual Metaphors. In Proceedings of the The 9th edition of the Language Resources and Evaluation Conference , Reykjavik, Iceland.  Ekaterina Shutova and Simone Teufel. 2010a. Metaphor corpus annotated for source - target domain mappings. In Proceedings of Language Resources and Evaluation Conference 2010. Malta. Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, ACL ?10, pages 688?697. Ekaterina Shutova, Tim Van de Cruys, and Anna Korhonen. 2012. Unsupervised metaphor paraphrasing using a vector space model In Proceedings of COLING 2012, Mumbai, India Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases and Kyle Elliott. 2013. Robust extraction of metaphor from novel data. In Proceedings of Workshop on Metaphor in NLP, NAACL. Atlanta. Tomek Strzalkowski, Samira Shaikh, Kit Cho, George Aaron Broadwell, Laurie Feldman, Sarah Taylor, Boris Yamrom, Ting Liu, Ignacio Cases, Yuliya Peshkova and Kyle Elliot. 2014. Computing Affect in Metaphors. In Proceedings of the Second Workshop on Metaphor in NLP, Baltimore Maryland.  Sarah Taylor, Laurie Beth Feldman, Kit Cho, Samira Shaikh, Ignacio Cases,Yuliya  Peshkiva, George Aaron Broadwell Ting Liu, Umit Boz, Kyle Elliott. Boris Yamrom, and Tomek Strzalkowski. 2014. Extracting Un-derstanding from automated metaphor identification: Contrasting Concepts of Poverty across Cultures and Languages. AHFE Conference, Cracow, Poland. Yorick, Wilks. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cam-bridge University Press, Cambridge, U.K., 329?348. Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton. 2013. Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta.  
219
Wilson, M. D. 1988. The MRC Psycholinguistic Database: Machine Readable Dictionary, Version 2. Behav-ioural Research Methods, Instruments and Computers, 20(1): 6-11. 
220
