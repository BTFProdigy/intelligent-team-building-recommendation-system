Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the 8th International Conference on Computational Semantics, pages 195?209,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Disambiguating quantifier scope in DTS
Livio Robaldo & Jurij Di Carlo
Dipartimento di Informatica, Universita? di Torino
robaldo@di.unito.it, dicarlo.jurij@educ.di.unito.it
Abstract
This paper proposes an extension of Dependency Tree Semantics (DTS),
an underspecified logic originally proposed in [20], that uniformily im-
plements constraints on Nested Quantification, Island Constraints and
logical Redundancy. Unfortunately, this extension makes the complex-
ity exponential in the number of NPs, in the worst cases. Nevertheless,
we conducted an experiment on the Turin University Treebank [6], a
Treebank of italian sentences annotated in a syntactic dependency for-
mat, whose results seem to indicate that these cases are very rare in
real sentences.
1 Introduction
Quantifier scope ambiguities may engender several interpretations of an NL
sentence. For instance, (1.a) has two readings that, in standard Generalized
Quantifier (GQ) approach, can be represented as in (1.b-c). (1.b) iff a
particular sound was heard by every man., while (1.c) is true iff each man
has heard a (potentially different) sound.
(1) a. Every man heard a mysterious sound.
b. ?
y
(mystSound
?
(y), ?
x
(man
?
(x), heard
?
(x, y)))
c. ?
x
(man
?
(x),?
y
(mystSound
?
(y), heard
?
(x, y)))
In order to deal with quantifier scope ambiguities, two main problems have
to be taken into account. First, the number of available readings tends to
degenerate into a combinatorial explosion when the number of quantifiers
increases. Second, in many real cases the knowledge needed to disambiguate
is not fully available during the processing of the sentence. In such cases,
all readings must be somehow stored, and, afterwards, when new world
knowledge becomes available, sequentially checked in order to remove the
inconsistent ones.
195
In order to provide a flexible solution to semantic ambiguities, Under-
specified formalisms have been recently proposed. In underspecified seman-
tics, semantic ambiguities are seen as instances of unspecified meaning that
can adopt a more restricted sense depending on the preferences grounded
in the syntactic structure, world-knowledge, topic/focus distinctions and so
forth. underspecified logics allow to encapsulate scope ambiguities in a sin-
gle structure. Surveys in underspecified semantics may be found in [8] and
[11].
The first underspecified logic that has been proposed is perhaps the algo-
rithm of Hobbs and Shieber [13], which has subsequently evolved into Quasi
Logical Form [1]. In this proposal, the underspecified representation is a par-
tial formula ? that may contain underspecified terms, called complex terms,
in the form <q, v, r> where q is a GQ, v an individual variable and r, the re-
striction of q, another partial formula. In order to obtain the disambiguated
formulae, complex terms must be solved. This is done by ?pulling out? and
?unstoring? the complex terms one by one. Depending on the order in which
they are solved, different readings are obtained. For instance, the underspec-
ified representation of (1.a) is shown in (2.a). By solving the complex term
<?, x, man
?
(x)>, we get the partially solved formula in (2.b), from which,
by solving the other complex term <?, y, mystSound
?
(x)>, we get reading
(1.a). Conversely, by solving the two complex terms in the opposite order,
we get (1.c).
(2) a. heard
?
(< ?, x, man
?
(x) >, < ?, y, mystSound
?
(x) >)
b. ?
x
(man
?
(x), heard
?
(x, < ?, y, mystSound
?
(x) >))
A more recent approach to underspecified semantics is grounded in dom-
inance constraints between some kind of scope-bearers and some kind of
scope-arguments. Underspecified Discourse Representation Theory [19], Hole
Semantics [5], the approach of [25], Constraint Language for Lambda Struc-
tures (CLLS) [12], and Minimal Recursion Semantics (MRS) [10], belong to
this approach. In MRS, the underspecified formula of (1) is
(3) ?h
0
, {h
1
: ?
x
(h
11
, h
12
), h
2
: ?
y
(h
21
, h
22
), h
3
: man
?
(x), h
4
: mystSound
?
(y),
h
5
: heard
?
(x, y)}, {h
0
=
q
h
5
, h
11
=
q
h
3
, h
21
=
q
h
4
}?
This representation is a triple whose second element is a flat set of labelled
subformulae called EPs. Each label (h
1
, h
2
, etc.) is called an ?handle?. The
EP associated with a quantifier contains a handle for its restriction and one
for its body. The third element of the triple is a set of constraints in the
196
form h
x
=
q
h
y
, specifying that the EP handled by h
y
must occur in the scope
of h
x
. Two disambiguations are then possible in (3): {h
0
= h
1
, h
12
= h
2
,
h
22
= h
5
}, which leads to (1.b), and {h
0
= h
2
, h
22
= h
1
, h
12
= h
5
}, which
leads to (1.c).
A new approach to underspecification of quantifier scope ambiguities has
been recently proposed in [20], and implemented into a new formalism called
Dependency Tree Semantics (DTS). In DTS, disambiguation is performed
by explicitly showing the dependencies between involved sets, i.e. by achiev-
ing a sort of underspecified ?Skolemization?. DTS is presented in detail in
section 3.
Before presenting DTS, however, we will discuss in section 2 three main
linguistic phenomena that reduce the level of ambiguity of NL sentences,
namely Nested Quantification, Island Constraints and logical Redundancy.
Those constraints have to be somehow carried out in an underspecified logic,
in order to avoid readings not available in NL. We will briefly discuss the
alternatives proposed in the aforementioned formalisms and in DTS.
Section 4 contains the core of the research presented here. It proposes
to add in DTS two new constructs that implement the three mentioned
constraints in terms of allowed/disallowed dependencies, in a uniform and
modular way. These constructs, called positive/negative arcs, make DTS ex-
pressively complete, i.e. able to underspecify any possible subset of available
readings. As argued by [11], although expressive completeness is a desider-
able property for underspecified logics, unfortunately it is in trade-off with
spatial/temporal complexity, which, in DTS, turns out to be exponential
in the number d of NPs. The last section, however, presents a small ex-
periment on the Turin University Treebank (TUT) [6], whose results seem
to indicate that, in real cases, the three examined constraints strongly con-
tribute to keep d, and hence the overall computational time, low. We will
then conclude by claiming that the exponential asymptotic behaviour of the
complexity function is a price worth paying for devising a disambiguation
process based on allowed/disallowed dependencies.
2 Constraints on underspecified formulae
As pointed out in the introduction, not all possible scopings correspond to
a possible reading in NL. In this section, we investigate the three main lin-
guistic phenomena involved in the unavailability of certain scope patterns,
namely Nested Quantification, Island Constraints, and logical Redundancy.
Nested Quantification: Several unreasonable readings arise from a con-
197
trast between semantic/syntactic nesting of quantifiers. For instance, it is
awkward to take the sentence in (4.a), where the NP a company occurs in
the syntactic restriction of the quantifier every, as describing the situation
corresponding to the scoping ?most?, in which each representative saw a po-
tentially different set of most samples, and he belongs to a different company
for each sample he saw.
(4) a. [Every
x
representative of [a
y
company]] saw [most
z
samples] [13].
b. see
?
(<?, x, rep
?
of
?
(x,<?, y, comp
?
(y)>)>, <Most, z, sample
?
(z)>)
c. ?h
0
, {h
1
: ?
x
(h
11
, h
12
), h
2
: ?
y
(h
21
, h
22
), h
3
: Most
z
(h
31
, h
32
),
h
4
:rep
?
of
?
(x, y), h
5
: comp
?
(y), h
6
: sample
?
(z), h
7
: saw(x, z)},
{h
0
=
q
h
7
, h
11
=
q
h
4
, h
21
=
q
h
5
, h
31
=
q
h
6
}?
d. ?
y
(comp
?
(y),Most
z
(samp
?
(z),?
x
(rep
?
(x)?of
?
(x, y), see
?
(x, z)))
e. ?
x
(?
y
(comp
?
(y), rep
?
(x)?of
?
(x, y)),Most
z
(samp
?
(z), see
?
(x, z)))
f. Most
z
(samp
?
(z),?
x
(?
y
(comp
?
(y), rep
?
(x)?of
?
(x, y)), see
?
(x, z)))
g. ?
y
(comp
?
(y), ?
x
(rep
?
(x)?of
?
(x, y),Most
z
(samp
?
(z), see
?
(x, z))))
h. Most
z
(samp
?
(z), ?
y
(comp
?
(y), ?
x
(rep
?
(x)?of
?
(x, y), see
?
(x, z)))
In the Hobbs and Shieber algorithm, these readings are forbidden by se-
lecting, at each step, a complex term not included in the restriction of any
other complex term. With this meta-rule, the algorithm, starting from the
underspecified formula in (4.b), computes the five readings in (4.d-h). The
same readings are triggered by the dominance constraints in the MRS rep-
resentation (4.c).
Several authors, e.g. [18], claim that also (4.d) has to be forbidden, in that
if q
1
occurs in the restriction of q
2
, no other quantifier can ?intercalate? be-
tween them in the scope order. In underspecified semantic, this view was
accepted by [25] and [15] among others. The former develops a logical the-
ory that identifies intercalating readings by deriving inconsistencies. The
latter extends [14], which proposes a syntax-semantics interface from LTAG
to MRS-like formulae, with a construct based on quantifier sets that pre-
vents intercalating readings.
Island Constraints: certain syntactic structures are scope-?islands?, i.e.
the scope-bearers occurring therein cannot outscope elements outside the
island. Island constraints have been introduced by [21]. There are basically
198
two kinds of islands: finite clauses and coordinated structures. An example
of the former is shown in (5.a), where ?
x
cannot outscope ?
y
in that a stu-
dent is outside the finite clause where every occurs. In contrast, the scope of
NPs can freely rise over non-finite clauses, as shown in (5.b), where ?
x
?
y
is
available. An example of coordinated structures acting as islands is shown
in (5.c). (5.c) yields two scopings only, ?
x
?
y
?
z
and ?
y
?
z
?
x
, corresponding,
respectively, to a reading where every student reads a own book and a own
paper, and a reading where there is a single a book and a single paper that
have been read by every student.
(5) a. A
y
student said you met every
x
professor. [16]
b. A
y
student wants you to meet every
x
professor. [16]
c. Every
x
student reads a
y
book and a
z
paper. [26]
In underspecified semantic, a standard way to deal with Island constraints of
the first kind introduces special ?blocking? place holders, one for each tensed
clause. The scope of the quantifiers in the clause is required to be always
included in the scope of the place holder. In MRS, the place holder may be a
handle referring to the ?top? of the clause. [14] implements Island constraints
in an MRS-like framework. Similar alternatives have been proposed in [12]
and [16]. The former implements a mechanism very close to [14], while the
latter devises a feature-based framework where a feature MAXS blocks the
scope of the quantifiers occurring in finite clauses
1
.
Island constraints carried by coordinate structures have received less atten-
tion in Underspecification, the most recent exception being perhaps [26]. In
[26], Hole Semantics is extended to properly handle the meaning of sentences
as (5.c).
Logical Redundancy: redundancy may arise when existential and uni-
versal quantifiers occur in the sentence. Existential quantifiers range over a
singleton of individuals, and so they cannot induce variation of their narrow-
scope sets. Analogously, universal quantifiers range over a whole domain
of individuals, which do not vary on the entities in their wide-scope sets.
Therefore, in (6.a-b) the scopings ?
x
?
y
and ?
x
?
y
are respectively equivalent
to ?
y
?
x
and ?
y
?
x
. Therefore, the latter may be taken as redundant. Nev-
ertheless, there is an exception to the rule about universals. It concerns a
1
In [16], also prepositions as of in (4) are associated with a feature MAXS. This allows to
deal with islands and nested quantifiers in terms of the same construct. Redundancy and
Coordination are not considered, but the extension of the logic in that direction seems
obvious.
199
universal having a modifier (which includes another quantifier) in its restric-
tion. For example, in the case of (6.c), there are two possibilities, according
to the intention of referring to ?all friends of all participants? (?
x
?
y
) or to
?all friends of any participant? (?
y
?
x
).
(6) a. A
x
boy read a
y
book.
b. Every
x
man read every
x
book.
c. [Every
x
friend of [every
y
participant]] arrived.
According to [4], analogous considerations hold for any NP that denotes a
principal filter (see [2]): proper names, definites, but also some bare indefi-
nites. We agree with [4]; proper names, as well as singular definites, clearly
denote constant sets
2
, while definites seem to behave exactly like universals.
For example, it is easy to see that (7.b-c) yield the same scopings of (6.b-c)
(7) a. The
x
men read every
y
book.
b. [The
x
friends of [every
y
participant]] arrived.
In underspecified semantics, logical redundancy has recently been investi-
gated by [9] and [17]. The former devises an algorithm that allows to prevent
the generation of redundant readings, while the method presented in the lat-
ter generates all readings, but allows to group them into equivalence classes.
3 Dependency Tree Semantics
In [20], a new underspecified semantic formalism called Dependency Tree
Semantics (DTS) has been proposed. Well-formed structures in DTS are
based on a graph G that represents the predicate-argument relations. The
nodes of G are either predicates or individual variables called discourse ref-
erents. Predicates connect discourse referents via arcs labelled with the
argument-position. Each discourse referent is also associated, via a function
quant, with a GQ, and with a restriction, via a function restr from discourse
referents to subgraphs of G.
In order to make the dependencies among sets of entities explicit, another
kind of arcs is introduced, termed SemDep arcs, and resemble Skolem de-
pendencies. A discourse referent is taken to depend on all discourse referents
it is connected to via a SemDep arc. Moreover, G includes a special element
2
Note that this seems to be true also for personal pronoun (you, she, etc.), singular
demonstratives (this, that, etc.), and singular possessives (his, her, etc.).
200
called Ctx. Ctx refers to the context, i.e. the domain wrt which the final
structure will be evaluated. All discourse referents are linked to Ctx via a
SemDep arc; however, the ones linked to Ctx only are assumed to denote
fixed sets of entities, i.e. to correspond to Skolem constants. The several
readings of a sentence differ in the SemDep arcs only. For instance, both
readings of (1.a) share the structures shown in fig.1.a
3
, but they have two
different sets of SemDep arcs: (1.b) is conveyed by those in fig.1.b; (1.c) by
those in fig.1.c.
hear'
man'
x y
mSound'
1
2
1
restr(x) restr(y)
quant(x)= 8
quant(y)= 9
1
x
1
y
mSound'
1
man'
b)
SemDep
x y
Ctx
a) )
SemDep
x y
Ctx
Figure 1: (a) SDG for sentence (1.a). (b-c) The two disambiguations (1.a)
In order to achieve underspecified semantics, SemDep arcs have to be some-
how added incrementally. In [20], this is done by defining another kind of
arcs, termed SkDep arcs. This paper makes use of other two kinds of arc,
termed positive and negative arcs, and propose them as an alternative of
SkDep arcs.
Before proceeding, it is worth stressing the main linguistic advantage of
DTS, although this article is not devoted to it. DTS licenses particular
readings where two or more sets of entities are introduced at the same level
of scope. I refer to such interpretations with the term ?Independent Set?
(IS) readings. Three basic kinds of IS readings need to be distinguished:
distributive, collective and cumulative IS readings (cf. [22]), respectively
shown in (8.a-c)
(8) a. Two examiners marked six scripts.
b. Three boys lift a piano.
3
DTS structures are usually displayed in a graphical notation where the subgraphs
corresponding to restr?s values are graphically shown separately. SemDep arcs are shown
as dotted arcs; in order to increase readability, transitive SemDep arcs are omitted. Nev-
ertheless, the reader must always keep in mind that they do occur, i.e. that SemDep
describes a transitive relation. Therefore, in fig.1.b, there is an arc y-->Ctx, even if it is
not shown.
201
c. Three monkeys ate all our bananas.
If we allow both NPs in (8.a) to receive wide scope and we interpret the
main verb distributively, we get a reading where there is a set of two exam-
iners and a set of six scripts and each of the two examiners marked each
of the six scripts. (8.b) is an archetypal example of the so-called collective
readings. The sentence may receive an interpretation where the three boys
lift a single piano with a joint effort. Finally, (8.c) has a cumulative in-
terpretation. The sentence says that the union/cumulation of the bananas
singularly eaten by each of the three monkeys includes the set of all our
bananas. Currently, DTS deals with distributive IS readings only, despite
their controversial existence in NL (see [23]), in that they require less formal
effort. The extension of DTS coverage to collective/cumulative IS readings,
which is taken as the object of future work, would require the introduction
of more complex devices (see [3]).
Since DTS?s coverage includes IS readings, the logic accepts more readings
than other standard approaches to NL semantics. Every partial order be-
tween quantifier corresponds to an available reading. Examples are shown
below in fig.2.
4 Positive and negative arcs
[20] defines some constraints to prevent undesiderable readings arising from
Nested Quantification and logical Redundancy (in contrast, Island Con-
straints are not considered). These constraints are defined in terms of ?meta-
rules?, like in Hobbs and Shieber?s algorithm:
(9) a. If either d
1
or d
2
(transitively) occurs in the restriction of a dis-
course referent d, and the other one does not, then the arc d
1
-->d
2
can be inserted only if the graph contains an arc d-->d
2
.
b. If quant(d
1
)=? or quant(d
2
)=?, the arc d
1
-->d
2
cannot be in-
serted. The only exception is when quant(d
1
)=? and d
2
(transi-
tively) occur in the restriction of d
1
.
By applying (9) to the initial DTS representation of (4.a), we get the five
readings in fig.2, corresponding to (4.d-h).
This section illustrates an alternative way of managing incremental inser-
tion of semantic dependencies, where allowed/disallowed dependencies are
explicitly specified in the representation. This is done by inserting additional
202
xy
Ctx
z
Ctx
a)
)b) d) e)
x
y
z
Ctx
x
y
z
Ctx
x
y
z
x
y
Ctx
z
Figure 2: SemDep configurations corresponding to (4.d-h).
arcs termed positive/negative arcs. With these constructs, we can easily pre-
vent redundant readings by adding a negative arc d
1
-->d
2
for each pair of
discourse referents (d
1
, d
2
) such that quant(d
1
)=? or quant(d
2
)=? and the
exception mentioned in (9.b) does not apply; all other arcs are added as
positive arcs.
Nevertheless, how do we achieve (9.a)? Such constraints disallow a set N
of dependencies unless the representation includes a set E of other depen-
dencies. In order to mirror these concepts, we refine the framework in two
ways:
(10) a. We group positive/negative arcs into (positive/negative) sets, and
we impose that all arcs in a set have to be allowed/disallowed
together.
b. We allow each negative set N to be associated with a further set
of arcs E. Those are the arcs that constitute the exception to the
disallowed dependency. In these cases, we will write {N |E}.
In terms of positive and negative sets, the new DTS representation of (4.a)
turns out to be the one in fig.3. Positive and negative sets are separately
shown as members of two families P and N in order to avoid verbose
graphical representations. The positive set {A-->Ctx} compactly refers to
{x-->Ctx, y-->Ctx, z-->Ctx}. Inserting it amounts to connecting all dis-
course referent to Ctx
4
.
{y-->x} and {z-->x} are allowed and so asserted in P . N contains four
negative sets. {x-->y} and {z-->y} have been disallowed because no arc can
enter an existential quantifier, and {x-->z} because no universal quantifier
can lead to a discourse referent outside its restriction. {y-->z} is disallowed
unless the structure contains a link from x to z. This is handled by adding
the negative set with exception {y-->z|x-->z} in N
5
. The reader can verify
4
We remind that only graphs where all discourse referents are linked to Ctx can be
model-theoretically interpreted.
5
Negative sets cannot be included in other negative sets, e.g. we cannot add
{z-->y|x-->y} in N : {z-->y} already occurs therein. However, negative sets can be
203
that all possible combinations of the allowed arcs lead to the five readings
in fig.2.
1
rep-of'
2
y x
restr(x) restr(z)
quant(z)=Mostquant(y)=9
1
1
omp'
z
restr(y)
x
1
x
quant(x)=8
1 2
1
sample'
z
saw'
2
y
1
P = f
,
g
CtxA
f f
,
g
xy
f g
xz
g
N = f f g
zxy z
omp' rep-of'
sample'
gf
,
g
yx
f
,
g
yz
f
,
g
zx
Figure 3: SemDep configurations corresponding to (4.d-h).
In terms of positive/negative arcs, it is rather easy to extend the coverage
of DTS to Island constraints. No discourse referent outside the island may
depend on a discourse referent inside it. A discourse referent inside the
island is, instead, free to depend on any other discourse referent. Hence, if
d
1
occurs in an island and d
2
does not, we assert {d
1
-->d
2
} as positive set
and {d
2
-->d
1
} as negative set. Coordinate NPs are further constrained; if
D ? {d
11
, . . . d
1n
} is the set of discourse referents in a coordination, and
d
1
?D depends on a d
2
/?D, so must any other d?D. This is handled by
inserting a single positive set {d
11
-->d
2
, . . ., d
1n
-->d
2
}. In example (5.c),
then, P would be P ={{y-->x, z-->x}, {A-->Ctx}}. It is easy to see that P
generates the two desired readings only.
5 Expressivity, complexity, and real cases
It is not really necessary to add every positive/negative set, i.e. to specify
every possible pattern of allowed/disallowed dependencies in the represen-
tation. For instance, disallowing a dependency is clearly equivalent to not
allowing it. In fig.3, it is easy to see that the negative sets {x-->y}, {z-->y},
and {x-->z} are actually useless because those arcs cannot be generated by
the sets in P . Therefore, they can be removed from N . Although we defined
algorithms implementing such heuristics, lack of space forbids us to provide
further details.
exceptions of other negative sets: the occurrence of {x-->z} in N does not prevent the
insertion of {y-->z|x-->z}.
204
From the point of view of expressivity, positive/negative arcs make DTS
expressively complete, i.e. able to underspecify any subset of dependencies.
This should be a property of every underspecified logic, as argued by [11],
but, unfortunately, it turns out to be in trade-off with computational com-
plexity. In DTS, in order to represent a subset of readings that have nothing
in common, the only solution is listing
6
all corresponding positive sets. For
instance, the readings of (5.c) have been ?listed? in P ={{y-->x, z-->x},
{A-->Ctx}}. Since the number of partial orders is exponential in the number
of discourse referents (cf. [7]), in the worst cases P has exponential cardi-
nality.
However, we believe that such cases are rare in reality. To provide evi-
dence for this claim, we analyzed the data in TUT. For each sentence we
estimated the number of positive/negative arcs needed to underspecify its
readings. (11) is one of the more complex sentences we found in TUT.
(11) La
x
societa? opera in numerosi
y
altri settori commerciali e indus-
triali, annoverando tra le
z
sue proprieta? una
k
catena di 20
w
su-
permercati, (alcuni
v
) centri turistici e una
p
miniera. (ALB-247)
The
x
society operates in several
y
other commercial and industrial
sectors, including among its
z
properties a
k
chain of 20
w
supermar-
kets, (some
v
) tourism centers and a
p
mine.
(11) contains seven discourse referents (x, y, z, k, w, v, p), so it initially
yields 7*6=42 non-cycling positive arcs, and more than 6 billion partial
orders between discourse referents. However:
- x is a singular definite with no restriction and z a plural possessive:
both of them must enter Ctx only, and no discourse referent can depend
on x.
- k, v, p are existential quantifiers in a coordination: no discourse ref-
erent can depend on them, and any triple of positive sets {k-->d},
{v-->d}, and {p-->d} is replaced by a the positive set {k-->d, v-->d,
p-->d}
7
.
- w belongs to the restriction of k: whenever w depends on a discourse
referent d6=k (or viceversa) so must k.
By applying these constraints, we obtain the following sets:
6
Actually, we must also forbid those positive sets to combine with each other. This
may be simply achieved by marking in some way every positive set we do not want to
combine.
7
Note that v is hidden, since tourism centers is a bare plural.
205
P = f
,
g
CtxA
f
N = f y w
f
,
w
k g
,
wv wp
,
k g
,
f
,
w
,
wv wp
w y
k g
,
f
,
y
,
yv yp
w z
k g
f
,
z
,
zv zp
g
f
,
y
k g
,
yv yp
,
f g
zy
,
f
,
z
k g
,
zv zp
g
The only acceptable reading is the one having all discourse referents linked
to Ctx only. However, in order to identify it, we would need a semantic
knowledge base from which we can infer, for example, that several sectors
cannot depend on his properties, and so the arc y-->z has to be removed
from P .
1715 sentences included in TUT have been analyzed as example (11)
8
. The
results are shown in Table 1. Sentences have been divided into four classes,
depending on the number D of discourse referents they contain. For each
class, the table shows the average of the number of positive/negative arcs
generated by applying the three constraints discussed above.
|D| 0 < |D| ? 2 2 < |D| ? 5 5 < |D| ? 10 10 < |D| ? 20
|Sentences| 462 643 516 94
|Arcs| 1.10 1.85 5.16 12.91
Table 1: TUT - Number of positive/negative arcs per discourse referents
The number of arcs indicated in Table 1 are very low. The reason for this
is the frequent occurrence in the sentences of proper names and definites
with no restriction. According to the discussion above, such NPs must be
linked to Ctx only, but this is already handled by the positive arc A-->Ctx.
In other words, proper names and definites with no restriction introduce
new discourse referents but do not introduce new arcs, thus decreasing the
average values.
More reliable results are reported in Table 2. The table shows the number
of arcs per discourse referents without considering such NPs. The reader
may see that in the worst cases the number of arcs can be still considered
low.
8
We excluded sentences including verbal ellipsis in that, according to [12], they can
engender complex quantifier-scope ambiguities not currently handled in DTS. Anaphora
have been ignored, in that those requiring a referent resolution enter Ctx only, while donkey
sentences, as shown by [24], simply require copies of the referent, with equal dependencies.
206
|D| 0 < |D| ? 2 2 < |D| ? 5 5 < |D| ? 10 10 < |D| ? 20
|Sentences| 985 574 153 3
|Arcs| 1.38 3.39 10.62 36
Table 2: TUT - Number of positive/negative arcs per discourse referents,
without considering proper names and definitives with no restriction.
6 Conclusions
In this paper, we presented an extension of DTS where allowed/disallowed
dependencies are explicitly specified by inserting new arcs called positive/negative
arcs. We believe that positive/negative arcs provide a scalable and modu-
lar solution for the management of constraints coming from heterogeneous
sources. For instance, we showed that positive/negative arcs needed to man-
age Nested Quantification, Island Constraints, and logical Redundancy may
be independently defined. Unfortunately, positive/negative arcs lead to a
computational complexity which is exponential in the worst cases. Never-
theless, we performed a rough experiment on the Turin University Treebank,
whose results indicate that those worst cases are very rare in real sentences.
It seems then that the computational complexity is not really a problem. In
a real system, in case the number of possible readings is too high, the sys-
tem may decide to inspect the context in order to detect allowed/disallowed
dependencies to be removed or added in the representation, thus reducing
the number of readings.
References
[1] Alshawi, H. The Core Language Engine. Mit Press, Cambridge, UK,
1992.
[2] Barwise, J. & Cooper, R. Generalized Quantifiers and Natural Language.
Linguistics and Philosophy, 4(2), 159?219, 1981.
[3] Beck, S. & Sauerland, U. Cumulation is Needed: A Reply to Winter
(2000). Natural Language Semantics, 8(4), 349?371, 2000.
[4] Beghelli F., Ben-Shalom D., Szabolski, A. Variation, Distributivity, and
the Illusion of Branching In A. Szabolcsi (Eds), Ways of Scope Taking,
Kluwer:Dordrecht, 29-69, 2001.
207
[5] Bos, J. Predicate Logic Unplugged. Proceedings of the 10th Amsterdam
Colloquium. Amsterdam, The Netherlands, 133?142, 1996.
[6] Bosco, C. A grammatical relation system for treebank annotation. Ph.D.
thesis, Department of Computer Science, University of Turin, 2004.
[7] Brinkmann, G. and McKay, B.D. Posets on up to 16 Points. Order,
19(2), 147?179, 2002.
[8] Bunt, H. Semantic Underspecification: Which Technique For What Pur-
pose? . In R. Musken and H. Bunt (Eds.), Computing Meaning. Kluwer,
vol. 3, 2003.
[9] Chaves, R.P. Non-Redundant Scope Disambiguation in Underspecified
Semantics. Proc. of the 8th ESSLLI Student Session, 47-58, Vienna,
2003.
[10] Copestake, A, Flickinger, D. and Sag, I.A. Minimal Recursion Seman-
tics. An introduction. Research on Language and Computation, 3(2),
2005.
[11] Ebert, C. Formal Investigations of Underspecified Representations.
Ph.D thesis, Department of Computer Science, King?s College London,
2005.
[12] Egg, M. and Koller, A. and Niehren, J. The Constraint Language for
Lambda Structures. Journal of Logic, Language and Information, 10(4),
2001.
[13] Hobbs, J. R. and Shieber, S. An Algorithm for Generating Quantifier
Scoping. Computational Linguistics: 13:47?63, 1987.
[14] Joshi, A. K. and Kallmeyer, L. Factoring Predicate Argument and Scope
Semantics: Underspecified Semantics with LTAG. Research on Language
and Computation, 1:3?58, 2003.
[15] Joshi, A. K. and Kallmeyer, L. and Romero, M. Flexible Composition
in LTAG: Quantifier Scope and Inverse Linking. In R. Musken and H.
Bunt (Eds.), Computing Meaning. Kluwer, vol. 3, 2003.
[16] Kallmeyer, L. & Romero, M. Scope and Situation Binding in LTAG
using Semantic Unification. Research on Language and Computation,
6(1), 2008.
208
[17] Koller A., Thater S. Towards a redundancy elimination algorithm for
underspecified descriptions. Proc. of the 5th Int. Workshop on Inference
in Computational Semantics (ICoS-5) Buxton, England, 2006.
[18] Park, J. Quantifier Scope and Constituency. Proceedings of the 33rd
Annual Meeting of the ACL. pp.205-212, 1995.
[19] Reyle, U. Dealing with ambiguities by Underspecification: Construction,
Representation and Deduction. Journal of Semantics, 13, 123?179, 1993.
[20] Robaldo, L. Dependency Tree Semantics. Ph.D thesis, Department of
Computer Science, Turin University, Italy, 2007.
[21] Ross, J. R. Constraints on Variables in Syntax. Ph.D thesis, Mas-
sachusetts Institute of Technology, 1967.
[22] Scha, R. Distributive, Collective and Cumulative Quantification. In J.
Groenendijk, M. Stokhof (Eds.), Formal Methods in the Study of Lan-
guage, Part 2, pages 483?512. Mathematisch Centrum, Amsterdam, 1981.
[23] Schein, B. Plurals and Events. MIT Press, Cambridge, MA, USA, 1993.
[24] Steedman, M. The Grammar of Scope, forthcom-
ing. See Surface-Compositional Scope-Alternation With-
out Existential Quantifiers. Draft 5.2, Sept 2007.
ftp://ftp.cogsci.ed.ac.uk/pub/steedman/quantifiers/journal6.pdf.
[25] Willis, A. An Efficient Treatment of Quantification in Underspecified
Semantic Representations. Ph.D thesis, University of York, 2000.
[26] Willis, A. NP Coordination in Underspecified Scope Representations.
Proc. of the 7th Workshop on Computational Semantics, Tilburg, 2007.
209
Quantifiers in Dependency Tree Semantics
Leonardo Lesmo, Livio Robaldo, Jelle Gerbrandy
Dipartimento di Informatica - Universita? di Torino
{lesmo,robaldo,gerbrand }@di. unito. it
Abstract
Dependency Tree Semantics (DTS) is an underspecified formalism for representing
quantifier scope ambiguities in natural language. DTS features a direct interface
with a Dependency grammar and an incremental, constraint-based disambiguation
mechanism. In this paper, we discuss the meaning of quantifier dependency in DTS
by translating its well formed structures into formulae of a Second Order Logic
augmented with Mostowskian generalized quantifiers.
1 Introduction
Dependency Tree Semantics (DTS) is an underspecified formalism for deal-
ing with quantifier scope ambiguity. DTS tries to keep the advantages of
most common underspecification techniques: it has a straightforward syntax-
semantics interface with a Dependency Grammar, just as QLF has [1], and
it allows for monotonically adding constraints to take partial disambiguations
into account, just as in UDRT [12], MRS [3] or CLLS [4]. These features
have been presented in [7] and [8], whereas in [9] DTS is proposed as a
possible underspecified semantic structure of Meaning?Text Theory [10].
This paper discusses a third property of DTS in further depth: the possibility
to represent branching quantifier (BQ) readings. Branching quantification in
DTS has partially been discussed in [7] and [8], in which we compared DTS
with First Order Logic (FOL). However, FOL is limited in that it allows to
represent only standard quantifiers (? and ?); in this paper we compare DTS
with the logic developed in [13] and [14], which is a fragment of Second Or-
der Logic which allows for a representation of branching quantification with
Generalized Quantifiers.
1.1 Intuitions behind Dependency Tree Semantics
The key idea of DTS is to specify quantifier scope by explicitly showing the
dependencies between involved (quantified) groups of entities, i.e. by imple-
menting a sort of ?Skolemization? in the underspecified representation. Well-
formed structures in DTS are based on a simple graph G that represents the
predicate-arguments relations, without any quantification. The nodes of G
are either predicates or discourse referents; each arc connects a predicate with
a discourse referent and is labelled with the number of the predicate argument
position. With each discourse referent we associate a quantifier (given by a
function QUANT from discourse referents to quantifiers) and its restriction,
which is given by a function RESTR that associates a subgraph of G to each
discourse referent. In (1), we show a first simple example
(1) Two students study three theorems
study?
stud?
x y
theor?
1 2
1 1
stud?
x y
theor?
1 1Restr(x)= Restr(y)=
Quant(x)= two Quant(y)= tree
The representation in (1) is still ambiguous; to disambiguate, we need to
specify how the quantifiers depend on each other. This is done by inserting
dotted arcs between discourse referents, named semdep arc. In figure 1.a and
fig 1.b two fully-specified representations of sentence (1) are given. Fig.1.a
shows the reading in which the quantifier ?three? depends on (has scope inside)
the quantifier ?two?. In figure 1.b, the arc linking x to y specifies that the two
students depend on the theorems. In both interpretations, the wide-scope
quantifier is linked to a new node called Ctx ? the context.
But DTS allows for very natural representation of a third reading of sentence
(1): in figure 1.c, both discourse referents are linked to the context. This is
the branching quantifier (BQ) reading. As we will see, the BQ reading is true
only in those models in which we can find a set of two students and a set of
three theorems, for which it holds that each student in the first set studies each
theorem in the second one. In NL, there are many cases in which the correct
study?
stud?
x y
theor?
1 2
1 1
Ctx
study?
stud?
x y
theor?
1 2
1 1
Ctx
study?
stud?
x y
theor?
1 2
1 1
Ctxa) b) c)
Fig. 1. The three readings of sentence (1)
truth conditions can be captured only via a BQ reading; in fact, it is easy to
add some context elements in the sentence in order to force the two involved
sets to be constant; for instance, in (2.i), the involved students and theorems
are explicitly mentioned in two appositions, while in (2.ii) the prepositional
modifier with my sister favours an interpretation in which three persons, two
friends of mine and my sister, went together to three same concerts.
Finally, even if there are not explicit syntactic elements forcing a BQ reading,
in many cases this is done by world knowledge; for example, in (2.iii), world
knowledge seems to render the reading in which two students have seen the
same three drug dealers the most salient; in fact, the presence of drug-dealers
in front of a school is (fortunately) a rare event and this induces to prefer the
reading minimizing the number of involved drug dealers.
(2) (i) Two students, John and Jack, study three theorems: the first three
of the book.
(ii) Two friends of mine went to three concerts with my sister.
(iii) Two students of mine have seen three drug dealers in front of the
school.
Not all possible configurations of semdep arcs are allowed. For instance, a
well-formed DTS cannot contain cycling paths, which would correspond to a
reading in which two sets of entities depend on each other, which is clearly
absurd. Furthermore, there are constraints to reduce the available readings
to those admitted in NL. In this paper, we will focus on the expressivity of
the general formalism, and provide a precise definition of the meaning of all
configurations that respect a minimal set of syntactic constraints, and abstract
from the question whether they correspond to an actual reading in NL. In other
words, in DTS the set of logical admitted readings is kept separate from the
subset of readings admitted in NL, and this paper focus on the former.
1.2 Formalisation: Syntax of DTS
A well-formed structure (wfs) in DTS is a Scoped Dependency Graph (SDG)
as defined below. We take as given a set of predicates pred and a set of
discourse referents D.
Definition 1.1 [Flat Dependency Graphs (FDG)]
A Flat Dependency Graph is a tuple ?N,L,A,Dom, f? s.t.:
- N is a set of nodes {n1, n2, . . . , nk}.
- L is a set of labels {l1, l2, . . ., lm}; in fig.1, L?{1, 2}.
- Dom ? pred?D is a set domain objects: predicates and discourse referents
- f is a function f : N 7? Dom, specifying the node referent, i.e. the domain
object with which the node is associated. In the following, whenever f(n) ?
X, we will say that node n is of type X.
- A is a set of arcs. An arc is a triple (ns, nd, l), where ns, nd ? N , ns is of
type pred, nd is of type D and l ? L.
Without going into further details, we stipulate that Gf is a connected acyclic
graph such that each node of type pred has one node of type D for each of its
places. Note that there can be two different nodes u and v s.t. f(u)=f(v),
i.e. the nodes in N can be seen as occurrences of symbols from Dom.
Definition 1.2 [Scoped Dependency Graph (SDG)]
A Scoped Dependency Graph is a tuple ?Gf , ctx,Q, quant, restr, SemDep? s.t.:
- Gf = ?N,L,A,Dom, f? is an FDG.
- ctx is a special element called the context.
- Q is a set of 2-place Mostowskian quantifiers {every, most, two, . . .} 1
- quant is a total function ND 7? Q, where ND ? N are the nodes of type D
- restr is a function assigning to each d ? ND its restriction, which is a sub-
graph of Gf .
- SemDep is a relation ND ? (ND ? {{ctx}}).
When SemDep(d, d?), we say that d depends on d?. Note that a discourse ref-
erent can depend on more than one other discourse referent. The dependence
relation needs to satisfy the following constraints:
? The transitive closure of SemDep is a partial order on all discourse referents
and ctx, with ctx as its maximal element.
? Let d be a discourse referent, and let R(d) be the smallest set that contains
d, and for which it holds that if d? is in R(d) and d?? occurs in the restriction
of d?, then also d?? ? D. It must hold that:
? If d1 ? R(d), d2 6? R(d), and d1 depends on d2, then also d depends on d2
? If d1 ? R(d), d2 6? R(d), and d2 depends on d1, then also d depends on d1
These last two constraints serve to exclude certain dependency relations that
are ?logically impossible?, and make sure that, for example, a sentence like
?Most representatives of a company took every sample? does not get a reading
in which ?a? depends on (only) ?every? and ?every? depends (only) on ?most?.
2 Branching quantification
Branching quantification was introduced by Henkin [5] in the context of FOL;
Hintikka [6] showed that it can occur also in NL. A great step toward the
definition of a model-theoretic schema for BQ was made by Barwise [2] who
merged Hintikka?s BQ account with the theory of Generalized Quantifiers.
Barwise?s idea was that the truth-conditions of BQ readings are connected
with the monotonicity of the involved quantifiers. He claimed that there is
no uniform schema for BQ: the formulae associated to sentences featuring all
monotone increasing (M?) quantifiers are different from those associated to
sentences featuring all monotone decreasing (M?) quantifiers. According to
Barwise, sentences with mixed quantifiers (some M? and some M?) make no
1 A 2-place Mostowskian Quantifier [11] (see also [13]) is a symbol Q such that, if x is an
individual variable and ?, ? are formulae then Qx(?,?) is also a formula. Semantically, Q
denotes, in every model M with universe A, a function q which takes in input two subsets B
and C of A and returns a truth-value. Mostowskian Quantifiers are cardinality quantifiers,
in the sense that q(B,C) depends only on the cardinalities of the sets (B ? C), (B \ C),
(C \B) and (A \ (B ? C)). Some examples are
? ?Allx(P1(x), P2(x))?M = true iff |(?P1(x) ? ?P2(x)?M )| = 0
? ?Fewx(P1(x), P2(x))?M = true iff |(?P1(x) ? P2(x)?M )| > ?
sense from a linguistic point of view.
On the other hand, Sher [13], [14] observed that since the semantics of
linearly ordered quantification is provided regardless to monotonicity, there
seems to be no methodological reason for imposing further constraints in case
of partially ordered quantification. In other words, even if readings from NL
are not available, this should not exclude their logical interpretation.
Sher specified the semantics of BQ on the basis of a precise definition of
the involved groups, according to so-called maximality conditions; roughly,
her claim is that the interpretation of a BQ reading with quantifiers of any
type corresponds to the one of Barwise for M? quantifiers augmented with
a maximality condition requiring that the involved sets are maximal with
respect to the body of the formula. Consider the two following sentences:
(3) (i) Most of the dots and most of the stars are all connected by lines.
(ii) Few of the dots and few of the stars are all connected by lines.
In Sher?s logic (let us name it L0) sentences in (3) are associated with formulas
of the following form:
(4) ?P1, P2[ C1 : Q1x(dot(x), P1(x))?
C2 : Q2y(star(y), P2(y))?
IN : ?xy[(P1(x) ? P2(y)) ? conn(x, y)]?
Max(?P1, P2?, IN) ]
where Q1 and Q2 are the Mostowskian quantifiers corresponding to the deter-
miners in our example: Q1=Q2=Most for (3.i); and Q1=Q2=Few for (3.ii).
The symbols C1, C2, IN are labels on the subformulae and Max(?P1, P2?, IN)
is an abbreviation for a maximality condition that states that two sets P1 and
P2 are maximal with respect to the formula with label IN , in the sense that
there are no strict supersets of P1 and P2 that satisfy IN . Formally, the max-
imality condition in (4) is the following formula:
Max(?P1, P2?, IN) ?
?P ?1, P ?2[ ?xy[ (P1(x) ? P2(y)) ? (P ?1(x) ? P ?2(y))?
(P ?1(x) ? P ?2(y)) ? conn(x, y) ] ?
?xy[ (P ?1(x) ? P ?2(y)) ? (P1(x) ? P2(y)) ]]
Sher generalizes the schema of (4), so that it applies to any partially ordered
set of arbitrary quantifiers. To achieve this, it is necessary to existentially
quantify n-ary generalized Skolem functions Hi rather than simple sets Pi,
and to assert maximality conditions also on the subformulae with label Ci.
Here, an n-ary Skolem function is just an n + 1-ary relation H ? we will
write H(x1, . . . xn+1) if x1 . . . xn+1 stand in the relation H, but also write
H(x1 . . . xn) for the set of objects xn+1 s.t. H(x1, . . . xn+1). Consider now a
branching reading such as in the following sentence:
(5) Few men inserted a coin in three coffee machines.
Fewx(man?(x)) @@
??Threey(CoffeeMach?(y))
Az(Coin?(z)) Inserted?(x, z, y)
=df ?Hx, Hy, Hz[ Cx: Fewx(man?(x), Hx(x)) &
Cy: Threey(CoffeeMach?(y), Hy(y)) &
Cz: ?xy[(Hx(x)?Hy(y))? Az(coin?(z), Hz(x, y))] &
IN: ?xyz[Hz(x, y, z)? inserted?(x, y, z)] &
Max(?Hx, Hy?, Cz) & Max(?Hz?, IN) ]
In this reading, the quantifier A depends on both Three and Few: there can be
a different coin for every pair of a man and a coffee machine. This is reflected
by the fact that Hz, the Skolem function associated with the quantifier A, is a
2-ary function, while Hx, Hy are 0-ary Skolem functions (that is, predicates).
The formula states that we have to find witnesses Hx, Hy and Hz such that
Hz corresponds to the extension of inserted?, and Hx and Hy are maximal
sets of individuals x and y such that the set of objects z inserted by x in
y, Hz(x, y, z), includes at least one coin; Hx is a set of a ?few men? and Hy
contains ?three coffee machines?. See [14] for the formal details.
3 Nested Quantification
A limitation of Sher?s logic is that it does not handle the case in which one
quantifier occurs in the syntactical restriction of another quantifier. Consider:
(6) Two representatives of three African countries arrive.
rep?
x
af?c?
1
2
1
arrive
y
1
1
of?
y
Restr(x)=
Restr(y)= Quant(x)= two
Quant(y)= tree
af?c?
rep?
x
1
2
1
y
of?
1
In this example, the quantifier Three occurs in the syntactic restriction of
Two. This corresponds to the fact that the discourse referent y occurs in the
graph RESTR(x). This type of reading cannot be directly represented in
Sher?s logic. Therefore, we propose to extend her definitions to accommodate
for these cases as well. Lack of space does not permit us to state the precise
definitions; we will give two examples instead which should illustrate how the
definitions work. Before discussing the three possible disambiguations of (6),
we introduce a new abbreviation to increase readability.
If ? is a well formed formula, x1 . . . xn a sequence of discourse referents, and
S1, . . . , Sn a sequence of predicates, we define:
?S1, . . . , Sn? ?
max
?[x1 . . . xn] ?
Max(?S1, . . . , Sn?,?x1 . . . xn[(S1(x1) ? . . . ? Sn(xn)) ? ?])
We will omit the reference to the variables x1 . . . xn in the notation when this
does not lead to confusion. By using ?
max
, the formula in (5) can be replaced
by the following equivalent
?Hx, Hy, Hz[ Fewx(man?(x), Hx(x)) & Everyy(CoffeeMach?(y), Hy(y)) &
?Hx, Hy??
max
[ Az(coin?(z), Hz(x, y, z))&
?Hz(x, y)??
max
inserted?(x, y, z) ] ]
For representing the restriction of quantifiers in the logic, in addition to the
Skolem functions Hx that represent the body of the quantifiers, we introduce
restriction sets ?x. The three readings of (6) can now be represented as:
x
y
Ctx
?Hx, Hy,?x,?y[ Twox(?x(x), Hx(x))& ?Hx??
max
(arrive?(x)) &
??x??
max
[Threey(?y(x, y), Hy(x, y)) &
??y(x)??
max
(af?c?(y)) &
?Hy(x)??
max
(repr of?(x,y)) ]]
x
y
Ctx
?Hx, Hy,?x,?y[ Threey(?y(y), Hy(y)) & ??y??
max
(af?c?(y)) &
?Hy??
max
[Twox(?x(y, x), Hx(y, x)) &
??x(y)??
max
(repr of?(x,y)) &
?Hx(y)??
max
(arrive?(x)) ]]
x
y
Ctx
?Hx, Hy,?x,?y[ Twox(?x(x), Hx(x)) & Threey(?y(y), Hy(y)) &
??x, Hy??
max
(repr of?(x,y)) & ??y??
max
(af?c?(y)) &
?Hx??
max
(arrive?(x)) ]
Let us shortly discuss each of these readings.
In the first reading, y depends on x, which is reflected in the fact that ?y and
Hy are unary Skolem functions whose values depend on the value for x. The
restriction set of ?three?, ?y(x), is (for each x) the set of all African countries,
while Hy(x) is the set of objects represented by x. Therefore, the restriction
set of ?two?, ?x, is a maximal set of individuals x that represent three African
countries. Two of these individuals must be in Hx ? the set of those that
arrive.
In the second reading, x depends on y. The set ?y consists of all African
countries. The set Hy must contain three of these, and it is required that for
each element y in Hy there are two individuals in the set of all its representa-
tives ?x(y) that are in Hx(y), which consists of all individuals that arrive.
The third formula represents the branching reading of the sentence, in which
the two discourse referents do not depend on each other. This formula states
that there are sets ?x and Hy such that each individual in ?x represents all
elements from Hy (this is expressed by the maximality condition on the pair
(?x, Hy)), and for which it holds that Hy contains three African countries,
and that two of the representatives from ?x must arrive. In the following, we
report a last complex example:
(7) Everyx teacher failed twoy students that studied less than halfz of the
topics in thew program.
The following DTS represents a reading of (7) in which the discourse referent
w depends on both y and z, and y and z depend on x.
x
1 2
failed
y
Restr(x)=
Quant(y)= ?
Quant(x)= ?
2
zof?
stud
1
study
topic
1
1
w
teacher
1
progr
1
12 x
teacher
1 Restr(w)=
w
progr
1
Restr(z)=
1
2
1
of? Restr(y)=
1
2
1
y
topic
w
z
stud study
z
Quant(z)= < 12
Quant(w)= the
x
y
Ctx
w
z
This DTS gets the translation reported below; in this interpretation, the two
students and the program depend on a teacher, while the set of topics depends
both on a program and on a student. In the formula, the pair of students
associated to a teacher x ? Hx has to belong to the set ?y, i.e. the set of
students y such that the set of things studied by y, i.e. Hz(x, y, w), contains
less than half elements of ?z, i.e. the set of topic in Hw(x), i.e. the program
of x.
?Hx, Hy, Hz, Hw,?x,?y,?z,?w[
Everyx(?x(x), Hx(x)) & {?x}?
max
(teacher?(x)) &
{Hx}?
max
[ Thew(?w(x,w), Hw(x,w)) & {?w(x)}?
max
(progr?(w)) &
Twoy(?y(x, y), Hy(x, y))] & {Hy(x)}?
max
(failed?(x, y)) &
{?y(x), Hw(x)}?
max
[Lthz(?z(x, y, w, z), Hz(x, y, w, z)) &
{?z(x, y, w)}?
max
(topic?(z)?of?(z, w)) &
{Hz(x, y, w)}?
max
(stud?(y)?study?(y, z))]]]
4 Conclusions and further works
In this paper, a comparison between Dependency Tree Semantics and Sher?s
work on Branching Quantification and Generalized Quantifiers has been pre-
sented. In particular, we have shown how disambiguated DTS structures can
be related to formulae of an extension of the formalism from [14] to represent
branching quantification. This provides a way to model-theoretically inter-
pret disambiguated DTS structures. Concerning further work, one of the next
steps in research on DTS will be extending its expressivity in order to deal
with cumulativity, which is a topic that has received very little attention in re-
cent studies on underspecification. Cumulative readings arise from a different
kind of branching quantification, as argued in [13], so the step for including
them is more natural in DTS than in other underspecified logics that do not
take BQ into account.
References
[1] Alshawi, H., editor, ?The Core Language Engine,? Mit Press, Cambridge, MA,
1992.
[2] Barwise, J., On branching quantifiers in english, The Journal of Philosophical
Logic (1979), pp. 47?80.
[3] Copestake, A., D. Flickinger and I. Sag, Minimal recursion semantics. an
introduction, Technical report, Manuscript, Stanford University (1999).
[4] Egg, M., A. Koller and J. Niehren, The constraint language for lambda
structures, J. of Logic, Language and Information 10 (2001), pp. 457?485.
[5] Henkin, L., Some remarks on infinitely long formulas, in: Finitistic methods,
Proc. Symphosium of Foundations Math, Warsaw, 1961, pp. 167?183.
[6] Hintikka, J., Quantifiers vs quantification theory, Dialectica (1973), pp. 329?
358.
[7] Lesmo, L. and L. Robaldo, Dependency tree semantics and underspecification,
in: Proc. Int. Conf. On Natural language processing (ICON2004), Hyderabad,
India, 2004.
[8] Lesmo, L. and L. Robaldo, From dependency tree semantics to fol, in: Proc. 6th
Workshop on Computational Semantics (IWCS-6), Tilburg, 2005, pp. 384?386.
[9] Lesmo, L. and L. Robaldo, Underspecification of quantifier scope in mtt, in:
Proc. 2th Int.Conf. on Meaning Text Theory, Moscow, 2005.
[10] Melcuk, I., Semantics and the lexicon in modern linguistics., in: A. Gelbukh,
editor, In Proc. of the 1st International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing), 2000, pp. 6?18.
URL www.CICLing.com
[11] Mostowski, A., On a generalization of quantifiers., Fundamenta Mathematicae
44 (1957), pp. 12?36.
[12] Reyle, U., Dealing with ambiguities by underspecification: Construction,
representation and deduction, Journal of Semantics (1993), pp. 123?179.
[13] Sher, G., Ways of branching quantifiers, Linguistics and Philosophy (1990),
pp. 393?422.
[14] Sher, G., Partially-ordered (branching) generalized quantifiers: a general
definition, The Journal of Philosophical Logic (1997), pp. 1?43.
Refining the Meaning of Sense
Labels in PDTB: ?Concession?
Livio Robaldo
University of Turin (Italy)
email: robaldo@di.unito.it
Eleni Miltsakaki
University of Pennsylvania (USA)
email: elenimi@linc.cis.upenn.edu
Jerry R. Hobbs
University of Southern California (USA)
email: hobbs@isi.edu
Abstract
The most recent release of PDTB 2.0 contains annotations of senses of
connectives. The PDTB 2.0 manual describes the hierarchical set of
senses used in the annotation and offers rough semantic descriptions of
each label. In this paper, we refine the semantics of concession sub-
stantially and offer a formal description of concessive relations and the
associated inferences drawn by the reader, utilizing basic notions from
Hobbs?s logic, including the distinction between causes and causal com-
plexes (Hobbs, 2005). This work is part of a larger project on the se-
mantics of connectives which aims at developing formal descriptions of
discourse relations, useful for processing real data.
207
208 Robaldo, Miltsakaki, and Hobbs
1 Introduction
As the demand for more powerful NLP applications increases, there is also an in-
creasing need to develop algorithms for automated processing of discourse relations
and models for deriving the inferences drawn by the reader. PDTB 2.0 (Prasad et al,
2008), released in January 2008, contains annotations of discourse connectives and
their arguments, attribution, and sense labels giving rough semantic descriptions of
the connectives. The availability of such a richly annotated corpus promises to boost
our understanding of the structure and meaning of discourse and will facilitate the
development of efficient algorithms for identifying discourse connectives and their
arguments.
However, in order to be able to derive appropriate inferences associated with dis-
course relations, we need to develop useful semantic analyses of the meaning of con-
nectives so that they will generate the same range of inferences made by humans. In
this paper we take a first step in that direction, offering a simple formal analysis of
concessive relations, thus refining the semantics of the concessive sense labels used
in PDTB 2.0. Our analysis uses basic notions of causality developed in Hobbs (1998,
2005), capitalizing on the distinction between causes and causal complexes and on
the semantics of defeasible causality. Concessive meaning involves the failure of a
general defeasible causal relation in this specific instance.
The paper is organized as follows. Section 2 gives an overview of the PDTB 2.0,
focusing on the annotation of the senses of connectives, especially ?concession?. In
Section 3, we present an overview of the framework we are adopting for our formal
analysis, namely, Hobbs?s logic of causality, and our basic claims about how the se-
mantics of defeasible causality contributes to the semantics of concession. Section 4
presents the semantic analysis of ?concession?. In Section 5, we report briefly on the
distribution of concessive labels in PDTB 2.0 and conclude in Section 6.
2 Sense labels in PDTB
The Penn Discourse Treebank provides annotations of the argument structure of dis-
course connectives, attribution (e.g., ?ownership? of the relation by the writer or other
individual), and semantic labels for all the annotated connectives (Prasad et al, 2008).
This annotation of discourse connectives and their arguments draws on a lexical ap-
proach to discourse structure (Webber et al, 2003; Webber and Joshi, 2003), viewing
discourse connectives as discourse-level predicates that take two abstract objects such
as events, states, and propositions (Asher, 1993) as their arguments.
Two major types of discourse connectives are annotated in PDTB: a) explicit con-
nectives including subordinate conjunctions, coordinate conjunctions and adverbials,
and b) implicit connectives that are inserted between two adjacent sentences to cap-
ture the meaning of the inferred relation when no explicit connective is present. The
PDTB 2.0 is, to date, the largest annotation effort at the discourse level, including ap-
proximately 40,000 triples in the form (Connective, Arg1, Arg2). Arg2 is the second
argument in the text in the case of coordinating conjunctions, and is the complement
of subordinating conjunctions. In the case of adverbs, Arg2 is the element which the
adverb modifies syntactically. In cases of ambiguity, sense labels indicate the intended
sense in the given context. In all other cases, sense labels provide semantic descrip-
Refining the Meaning of Sense Labels in PDTB: ?Concession? 209
tions of the relations conveyed by the connectives, both explicit and implicit.
The tagset of senses is organized hierarchically (Miltsakaki et al, 2008). The top
level, or class level, has four tags representing four major semantic classes: ?TEMPO-
RAL?, ?CONTINGENCY?, ?COMPARISON? and ?EXPANSION?. For each class,
a second level of types is defined to further refine the semantics of the class levels.
For example, ?CONTINGENCY? has two types ?Cause? (relating two situations via
a direct cause-effect relation) and ?Condition? (relating a hypothetical scenario with
its (possible) consequences). A third level of subtype specifies the semantic contribu-
tion of each argument. For ?CONTINGENCY?, its ?Cause? type has two subtypes ?
?reason? (which applies when the connective indicates that the situation specified in
Arg2 is interpreted as the cause of the situation specified in Arg1, as often with the
connective because) and ?result? (which is used when the connective indicates that
the situation described in Arg2 is interpreted as the result of the situation presented in
Arg1). That is, ?reason? occurs when Arg2 causes Arg1; ?result? occurs when Arg1
causes Arg2.
Connectives can also be used to relate arguments pragmatically as in John is in
the house because the lights are on or If you?re thirsty, there?s beer in the fridge,
where the relation involbes the belief in or the telling of the condition rather than the
condition itself. For these rhetorical or pragmatic uses of connectives, a small set of
pragmatic sense tags has been defined ? specifically, ?Pragmatic Cause?, ?Pragmatic
Condition?, ?Pragmatic Contrast? and ?Pragmatic Concession?.
2.1 ?Concession? in PDTB
?Concession? is a type of the class-level category ?COMPARISON?. The class tag
?COMPARISON? applies when the connective indicates that a discourse relation is
established between Arg1 and Arg2 in order to highlight prominent differences be-
tween the two situations. Semantically, the truth of both arguments is independent of
the connective or the established relation. ?COMPARISON? has two types that further
specify its semantics. In some cases, Arg1 and Arg2 share a predicate or a property
and the difference is highlighted with respect to the values assigned to this property.
This interpretation is tagged with the type ?Contrast?.
There are also cases in which the highlighted differences are related to expectations
raised by one argument which are then denied by the other. This intepretation is
tagged with the type ?Concession?. According to the description in the PDTB 2.0
manual, the type ?Concession? applies when the connective indicates that one of the
arguments describes a situation A which normally causesC, while the other asserts (or
implies) ?C. Alternatively, one argument denotes a fact that triggers a set of potential
consequences, while the other denies one or more of them.
Two ?Concession? subtypes are defined in terms of the argument creating an ex-
pectation and the one denying it. Specifically, when Arg2 creates an expectation that
Arg1 denies, it is tagged as ?expectation?, shown in (1.c-d). When Arg1 creates an
expectation that Arg2 denies, it is tagged as ?contra-expectation?, shown in (1.e-f).
Examples (1.a-b) are made-up sentences we use for explanation and will be discussed
here and in the next section. All other examples are taken from PDTB 2.0. Each dis-
course fragment in (1) distinguishes between a discourse connective (underlined), and
two sentence-arguments: Arg1 (italics) and Arg2 (boldface).
210 Robaldo, Miltsakaki, and Hobbs
(1) a. Although John studied hard, he did not pass the exam. (expectation)
b. Although running is considered healthy, it is not advisable for persons
with heart problems. (expectation)
c. Although they represent only 2% of the population, they control nearly
one-third of discretionary income. (expectation)
d. While acquiring a big brand-name company can be a shortcut to
growth, it can also bring a host of unforeseen problems (expectation)
e. The Texas oilman has acquired a 26.2% stake valued at more than $1.2
billion in an automotive-lighting company, Koito Manufacturing Co.
But he has failed to gain any influence at the company. (contra-
expectation)
f. Mr. Cannell?s allegations of cheating ?are purely without foundation?,
and based on unfair inferences. However the state will begin keeping
closer track of achievement-test preparation booklets next spring..
(contra-expectation)
(1.a) is an example of ?expectation?: Arg2 (John studied hard) creates the expecta-
tion that John passed the exam, which is precisely denied by Arg1. The same holds
for (1.b-d). Note that (1.b), unlike (1.a, c-d), expresses a general concessive relation,
i.e., it does not refer to particular contingent events. (1.e-f) are instances of contra-
expectation, where the expectation is created by Arg1. In (1.e), the fact that the Texas
oilman acquired the indicated stake value creates the expectation that he gained influ-
ence at the company, while, in (1.f), since Mr. Cannell?s allegations of cheating are
purely without foundation (in the speaker?s judgement), we do not expect the state to
start tracking the test preparation.
3 Toward a formal definition of ?Concession?
Based on our analysis of the range of PDTB tokens tagged with a concessive label, we
offer here a more detailed semantic analysis of the meaning of concessive relations.
Since the direction of the concessive relation is not relevant, the argument that creates
the expectation and the argument that denies it are respectively termed as Argcexp and
Argdexp. We claim that a concessive relation arises from a contrast between the effects
of two causal relations cc and cd holding in the domain. c and d stand for ?creates? and
?denies?, respectively. The relation denoted by cc is the causal relation that creates the
expectation, and cd the one that denies it. The effects of these causal relations, as well
as their causes, are taken to be eventualities1.
In this paper, we use the letter e for most eventualities, possibly with some subscript
or superscript.2 We make use of the subscripts x1 and x2, respectively, to distinguish
between the causes and the effects in a causal relation cx. Therefore, the causes in
cc and cd are indicated by ec1 and ed1 respectively, and the effects by ec2 and ed2,
respectively. ec2 is the ?created expectation?; its cause ec1 is conveyed by Argcexp. ed2
is an eventuality that denies ec2, and it is explicitly described in Argdexp. The cause of
1The term ?eventuality? is borrowed from (Bach, 1981). It covers both standard notions of ?state? and
?event?.
2As we will see, also causal relations are eventualities; so the names cc and cd are an exception to this
rule.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 211
ed2, i.e., ed1, is usually unknown. Also ec2 is, in principle, unknown, but in most cases
it can be taken as the negation of ed2.
For instance, in the context of (1.a), the eventuality John studied hard (ec1) creates
the expectation John passed the exam (ec2). Nevertheless, Argdexp says that John did
not pass the exam actually (ed2). The reason of ed2 is unknown and has to be found in
the context. In other words, the context, whether explicit or inferred, should include
another eventuality that caused John?s failure, despite his studying hard. For example,
the next sentence might be John was very tired during the exam (ed1).
In order to formalize this account of concession, we need a defeasible notion of
causality. Many authors propose such an account of causality, e.g. (Achinstein, 1965;
Shoham, 1990; Simon, 1991; Bell, 1999, 2003), and Giunchiglia et al (2004). The ac-
count we use is that of Hobbs (2005). This distinguishes between the monotonic, pre-
cise notion of ?causal complex? and the nonmonotonic, defeasible notion of ?cause?.
The former gives us mathematical rigor; the latter is more useful for everyday rea-
soning and can be characterized in terms of the former. As Hobbs (2005) explains,
when we flip a switch to turn on a light, we say that flipping the switch ?caused? the
light to turn on. But for this to happen, many other factors had to be in place. The
bulb had to be intact, the switch had to be connected to the bulb, the power had to
be on in the city, and so on. The set of all the states and events that have to hold or
happen for an effect e to happen are called the ?causal complex? of e. Thus, the flip-
ping of the switch and the normal states of the bulb, the wiring, and the power supply
would all be in the causal complex for the turning on of the light. In a causal complex,
the majority of participating eventualities are normally true and therefore presumed
to hold. In the light bulb case, unless otherwise indicated, it is normally true that the
bulb is not burnt out, that the wiring is intact, that the power is on in the city, and so
on. But the light switch could be on or off; neither can be presumed. Those eventu-
alities that cannot normally be assumed to be true are identified as causes (cf. Kayser
and Nouioua, 2008). They are useful in planning, because they are often the actions
that the planner or some other agent must perform. They are useful in explanation
and prediction because they frequently constitute the new information. They are less
useful in diagnosis, where the whole causal complex has to be considered.
Note that in practice, we can never specify all the eventualities in a causal complex
for an event. So while the notion of causal complex gives us a precise way of thinking
about causality, it is not adequate for the kind of practical reasoning we do in planning,
explaining, and predicting. For this, we need the defeasible notion of ?cause?.
3.1 Background on Hobbs?s logic
Hobbs (1998) proposed a wide coverage logical framework for natural language based
on the notion of reification. Reification is the action of making states and events first-
class individuals in the logic, so they can be referred to by constants and variables.
We ?reify? eventualities, from the Latin word ?re(s)? for ?thing?: we take them to
be things. The framework distinguishes two parallel sets of predicates: primed and
unprimed. The unprimed predicates are the ordinary predicates we are used to in
logical representations of language. For example, (give a b c) says that a gives b to c.
When we assert this, we are saying that it actually takes place in the real world. The
primed predicate is used to talk about the reified eventualities. The expression (give?
212 Robaldo, Miltsakaki, and Hobbs
e a b c) says that e is a giving event by a of b to c. Eventualities may be possible
or actual. When they are actual, this is simply one of their properties. To say that a
state e actually obtains in the real world or that an event e actually occurs in the real
world, we write (Rexist e). That is, e really exists in the real world. If I want to fly, my
wanting really exists, but my flying does not. This is represented as:3
(Rexist e) ? (want? e I e1) ? (fly? e1 I)
Therefore, contrary to (p x), (p? e x) does not say that e actually occurs, only that if
it did, it would be a ?p? event. The relation between primed and unprimed predicates
is then formalized by the following axiom schema:
(forall (x) (iff (p x) (exists(e) (and(p? e x)(Rexist e)))))
Eventualities can be treated as the objects of human thoughts. Reified eventualities
are inserted as parameters of such predicates as believe, think, want, etc. These predi-
cates can be applied in a recursive fashion. The fact that John believes that Jack wants
to eat an ice cream is represented as an eventuality e such that4
(believe? e John e1) ? (want? e1 Jack e2) ?
(eat? e2 Jack Ic) ? (iceCream? e3 Ic)
In Hobbs?s notation, every relation on eventualities, including logical operators, causal
and temporal relations, and even tense and aspect, may be reified into another eventu-
ality. For instance, by asserting (imply? e e1 e2), we reify the implication from e1 to e2
into an eventuality e. e has to be thought as ?the state holding between e1 and e2 such
that whenever e1 really exists, e2 really exists too?. Negation is represented as (not? e1
e2): e1 is the eventuality of the e2?s not existing. Some problems arise with negation,
in that what is generally negated is an eventuality type rather than an eventuality token
or instance. In order to deal with more general cases of concession, we will refer to
eventualities that are inconsistentwith other ones. Two eventualities e1 and e2 are said
to be inconsistent iff they (respectively) imply two other eventualities e3 and e4 such
that e3 is the negation of e4. The definition is as follows:
(forall (e1 e3)
(iff (inconsistent e1 e2)
(and (eventuality e1) (eventuality e2)
(exists (e3 e4) (and (imply e1 e3)
(imply e2 e4)(not? e3 e4))))))
3.2 Typical elements, eventuality types and tokens
Among the things we can think about are both specific eventualities, like Fido is bark-
ing, and general or abstract types of eventualities, like Dogs bark. We do not want to
treat these as radically different kinds of entities. We would like both, at some level, to
3In order to increase readability, we will often make use of the symbol ? in place of the unprimed
predicate and.
4The formula expresses the de-re reading of the sentence, where e1, e2 , e3 , John, Jack, Ic are first order
constants.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 213
be treated simply as eventualities that can be the content of thoughts. To this end, the
logical framework includes the notion of typical element (from Hobbs (1983, 1995,
1998)). The typical element of a set is the reification of the universally quantified
variable ranging over the elements of the set (cf. McCarthy (1977)). Typical elements
are first-order individuals. The introduction of typical elements arises from the need
to move from the standard set-theoretic notation
s = {x | p(x) }
or its logical equivalent,
(forall (x) (iff (member x s) (p x)))
to a simple statement that p is true of a ?typical element? of s by reifying typical
elements. The principal property of typical elements is that all properties of typical
elements are inherited by the real members of the set.
It is important not to confuse the concept of typical element with the standard con-
cept of ?prototype?, which allows defeasibility, i.e., properties that are not inherited
by all of the real members of the set. Asserting a predicate on a typical element of a
set is logically equivalent to the multiple assertions of that predicate on all elements
of the set. Talking about typical elements of sets of eventualities leads to the distinc-
tion between eventuality types and eventuality tokens. The logic defines the following
concepts, for which we omit formal details5: a) Eventualities types (aka abstract even-
tualities): eventualities that involve at least one typical element among their arguments
or arguments of their arguments (we can call these ?parameters?), b) Partially instan-
tiated eventuality types (aka partial instances): a particular kind of eventuality type
resulting from instantiating some of the parameters of the abstract eventuality either
with real members of their sets or with typical elements of subsets, and c) Eventuality
tokens (aka instances: a particular kind of partially instantiated eventuality type with
no parameters. It is a consequence of universal instantiation that any property that
holds of an eventuality type is true of any partial instance of it.
Hobbs?s logical framework is particularly suitable to the study of the semantics of
discourse connectives, in that it allows focusing on their meaning while leaving under-
specified the details about the eventualities involved. In other words, we can simply
assume the existence of two eventualities e1 and e2 coming from the two arguments
Arg1 and Arg2 respectively. e1 and e2 may be either eventuality tokens, on atomic
arguments, as in (1.a), or eventuality tokens, on collective arguments, as in (1.c), or
(partially instantiated) eventuality types, as in (1.b), or any other kind of eventuality.
The semantics of concession proposed below uniformily applies to all these cases.
3.3 Hobbs?s Account of Causality
The account of causality described above in the introduction is represented in terms
of two predicates: (cause? cx ex1 ex2) and (causalComplex s ex2). cause? says that cx is
the state holding between ex1 and ex2 such that the former is a non-presumable cause
5Actually, ?instance? is slightly more general, since if s is a set, x is its typical element, and y is a
member of s, y is an instance of x, even though it is not an eventuality. Nevertheless, in this paper we
assume ?instances? and ?eventuality tokens? to be synonymous.
214 Robaldo, Miltsakaki, and Hobbs
of the latter. causalComplex says that s is the set of all presumable or non-presumable
eventualities that are involved in causing ex2. Obviously, ex1 belongs to s. Thus, in the
light example, the predicate cause applies to the flipping of the switch, while the states
of the bulb, the wiring, and the power supply would all be in the causal complex s.
Several axioms characterize the predicates cause and causalComplex. Some of them
relate causality with time6, some relate causality with probability, and so on Hobbs
(2005).
It is clear that the theory must not include an axiom stating that, whenever a causal
relation cx and its cause ex1 really exist, the corresponding effect ex2 really exists too.
The inclusion of such an axiom would lead to a non-defeasible causality. Rather, we
need an axiom stating that an effect really exists just in case all the eventualities in its
causal complex really exist:
(forall (s e)
(if (and (causalComplex s e)
(forall (e1) (if (member e1 s) (Rexist e1)))
(Rexist e)))
Nevertheless, as pointed out above, we can never specify all the eventualities in a
causal complex. Even in simple sentences like (1.a), the eventualities in the causal
complex are not easy to list, and the real causes may not coincide with what we think
the causes are in that context. For example, recalling our analysis of (1.a) above:
ec1=?John studied hard?
ec2=?John passed the exam?
ed1=?John was tired during the exam?
ed2=?John did not pass the exam?
cc=?ec1 causes ec2?; cd=?ed1 causes ed2?
One approach at this point would be to say that both ec1 and the negation of ed1
belong to the causal complex of ec2, with ec1 being the non-presumable cause of ec2.
But this would mean that not being tired during exams is a kind of ?precondition? for
passing exams by studying hard, which is obviously false in many contexts. Note,
however, that there is an arbitrary quality to what we designate as being in a causal
complex, because causality forms chains and we can start the chain at any point. John
was tired caused the situation that he did not manage to concentrate, which caused
the situation that he made a lot of errors in the exam, which caused the situation that
the teacher decided to fail him. One could argue that the last of these eventualities is
the real cause of ed2. Similarly, one could argue that ec1 is not the real cause of ec2:
John studied hard causes the situation that he makes few errors in the exam . . . and the
teacher decides not to fail him. The predicate cause is defeasibly transitive, however,
so these considerations do not affect our account of concession. Furthermore, we do
not take the negation of ed1 as necessarily belonging to the causal complex for ec2.
Rather, we claim that ed1, besides being the cause of ed2, is the cause of another
eventuality edp that is inconsistent with an element ecp in the causal complex for ec2.
6As argued also by Giordano and Schwind (2004), the effect caused by an eventuality can take place in
the current or in a subsequent instant.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 215
In (1.a), ecp may be simply John does not have any particular health problem that
jeopardizes his passing the exam. ed1 caused both John?s failure and an health status
that jeopardizes the passing of his exam. This is what we mean here by ?denying of
an expectation?.
In our analysis of concession, we distinguish between abstract causalities like hard
studying causes passing exams, and causality tokens like John?s tiredness caused
John?s failure. Note that asserting (Rexist c) on an abstract causal relation c amounts
to asserting (Rexist c?) for any (partial) instance c? of c. But recall that cause is only
defeasible. Both the abstract causal principle and its partial instance are simplified
stand-ins for rules that involve entire causal complexes, not all of whose elements may
obtain. Thus, just because hard studying causes passing exams, we cannot invariably
conclude that if John really studied, he really passed the exam.
4 The meaning of concessive relations
Our basic claim is that the meaning of concessive relations is triggered by a contrast
between two causal relations cc and cd such that one or more eventualities in the causal
complex of ec2 (the expectation created by cc), is denied by ed2 (the effect of cd). cc,
cd , ed2, and ec1 (the cause in cc) really exist in the world, or are at least believed to
exist by the speaker/writer. Furthermore, all eventualities in the causal complex for
ed2, including the non-presumable cause ed1, which is unknown in many cases, really
exist too. Argcexp conveys ec1, while Argdexp conveys ed2.
We also claim that in all cases of concession it seems that what really creates the
expectation is a causal relation cac that is an abstraction of cc. cc really exists in the
world precisely because cac really exists and cc is a partial instance of it. In other words,
the real existence of cc is inherited from cac. On the other hand, there is not necessarily
an abstract counterpart cad for cd that also really exists in the world. For instance,
in (1.a), it seems that what creates the expectation is the assumption that the causal
relation studying hard causes passing exams (cac) really exists in the context. John?s
hard studying causes John?s passing exams (cc) is just an instance of cac. This instance
really exists in the world too. However, since causality is defeasible, the fact that John
really studied hard (ec1) does not entail the real existence of John really passed the
exam (ec2). In fact, this is precisely denied by Argdexp: John did not pass the exam
(ed2). The cause of John?s failure, e.g., John?s tiredness (ed1), is (or is the cause of
an eventuality edp that is) inconsistent with an element ecp of the causal complex for
(ec2), namely, John does not have any particural health problem that jeopardizes the
passing of his exam. Note that we do not necessarily infer that being tired causes
failing an exam: tiredness was the cause of the failure in this particular scenario only.
Therefore, we assert that cd really exists, but we do not advocate the existence of a
more abstract causal relation cad that really exists too.
To summarize, the semantics of concession we propose is formalized in (2). The
conjuncts (Rexist cc) and (Rexist ed1) have been omitted in (2) because they may be
inferred from (Rexist cac) and (Rexist ed2). sc is the causal complex associated with
cc. ec1 and ed2 are given to us in Argcexp and Argdexp respectively, while all other
eventualities may be inferred by abduction from the contextual knowledge; some hints
about how this may be done are provided in Hobbs (2005).
216 Robaldo, Miltsakaki, and Hobbs
(2) (exist (cc ca c ec1 ec2 cd ed1 ed2 sc ecp edp)
(cause? cc ec1 ec2) ? (cause? cd ed1 ed2) ? (Rexist ca c) ?
(partialInstance cc ca c) ? (Rexist cd) ? (Rexist ec1) ?
(Rexist ed2) ? (cause ed1 edp) ? (Rexist edp) ?
(inconsistent ec2 ed2) ? (causalComplex sc ec2) ?
(memberecp sc) ? (inconsistent edp ecp))
Let us now examine how the semantics given in (2) applies for corpus examples
tagged as ?expectation? or ?contra-expectation?. Let us analyze (1.b) in the light of
the semantics proposed in (2). The abstract causality that creates the expectation (ca
c) is Something that is considered healthy for humans is advisable for them7. This
is partially instantiated in Since running is considered healthy for persons with heart
problems, it is advisable for them (cc). Nevertheless, the fact that running is really
considered healthy in the context (ec1) does not suffice to assert that running is really
advisable for persons with heart problems (ec2). There is a particular reason why
running is not advisable for persons with heart problems (ed2), e.g. their hearts do
not tolerate a heartbeat increase (ed1). Since running causes a heartbeat increase, the
heart can tolerate a heartbeat increase (ecp) is in the causal complex for ec2 and it is
inconsistent with ed2.
Similarly, in (1.c), which is taken from the PDTB, it is true that representing a
low percentage of the population causes controlling low percentage of income (cac).
Therefore, they represent 2% of population (ec1) causes they control low percentage
of income (ec2). Nevertheless, ec2 does not really exists in the context, in that it is
inconsistent with they control nearly one-third of income (ed2). There must be another
reason for why ec2 does not hold. For instance, either they are very rich, or they do
not have as many basic expenses as other people, or a more complex condition. This
unknown cause, i.e. ed1, both makes ed2 true and ec2 false in the context.
The last example highlights the point that finding the eventualities involved in (2) is
strongly dependent upon contextual knowledge. 2% is not taken to be a low percentage
in any context. For instance, 2% mercury in the water may be considered a high
percentage of pollution. Analogously, one third may be considered a high percentage
in that context, especially if compared with 2% of population, but it may be a low or
medium percentage in many other contexts. The analysis of examples (1.d-e) in terms
of the definition in (2) is analogous.
5 A survey of concessive relations in PDTB 2.0
PDTB 2.0 contains 1193 tokens of explicit connectives which are annotated with one
sense tagged as ?Concession?, ?contra-expectation? and ?expectation?. There are also
another 20 tokens that have been annotated with double senses, one of which is the
concessive type or subtypes. Table (1) shows the distribution of concessive labels for
the 1193 tokens. Explicit connectives with a concessive label assigned to less than
10 tokens are grouped under ?other?. The rest of the connectives shown in Table (1)
amount to 98% of all ?contra-expectation? and 95% of all ?expectation? tokens. The
7This is a paraphrase of Something being considered healthy for humans causes it to be advisable for
humans.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 217
Table 1: Concessive labels in PDTB 2.0
CONN ?contra-exp.? ?exp.? ?Concession? Total
although 21 132 1 154 (13%)
but 494 12 2 508 (42.5%)
even if 3 31 1 35 (3%)
even though 15 52 5 72 (6%)
however 70 2 5 77 (6.5%)
nevertheless 19 0 0 19 (1.5%)
nonetheless 17 0 0 17 (1.5%)
still 79 2 1 82 (7%)
though 30 53 1 84 (7%)
while 3 79 1 83 (7%)
yet 32 0 0 32 (2.5%)
other 13 17 0 30 (2.5%)
Total 796 380 17 1193
most common connective annotated with the ?Concession? type or one of its two sub-
types is ?but? with 508 tokens (42% of all concessive labels), followed by ?although?
with 154 tokens (13% of all concesive labels).
We are currently evaluating the robustness of the proposed refined semantics for
concessive labels in PDTB 2.0 starting with the most the most common concessive
connectives. While the validation process for the entire corpus is still work in progress,
preliminary results on 25% of ?but? tokens indicate that the semantics of concession
based on defeasible causality applies straightforwardly to more than 60% of the data.
In future work, we hope to be able to offer a more comprehensive account of all the
concessive labels in PDTB 2.0 including cases of concession in which the created
expectation arises from an implication rather than from a causal relation (about 23%),
as in (3)
(3) Although working for U.S. intelligence, Mr. Noriega was hardly helping the
U.S. exclusively. (expectation)
In (3), it is strange to say that working for U.S. intelligence normally ?causes?
helping U.S. exclusively. Rather, the former seems a kind of necessary condition or
job requirement for the latter: working for U.S. intelligence implies (among other
things) helping U.S. exclusively. Suppose that someone discovers that Mr. Noriega is
not helping the U.S. exclusively. Mr. Noriega is arguably breaking a rule or flauting an
expectation. Therefore, working for U.S. intelligence ?implies? rather than ?causes?
helping U.S. exclusively.
It is unsurprising that there are cases of concession based on implication rather than
causality, because the two concepts are very close to each other. One could think of
implication as a kind of abstract, informational, or ?denatured? causality. Both obey a
kind of (defeasible) modus ponens. When the cause or antecedent happens or holds,
so does the effect or consequent. The other key property of causal complexes is that
218 Robaldo, Miltsakaki, and Hobbs
all the eventualities in it are relevant, in a sense that is made precise in Hobbs (2005).
This notoriously does not hold for material implication, but as many have argued,
it probably does hold for felicitous uses of our everyday notion of implication. In
addition, there are easy conversions between causality and implication. If A causes B,
then the fact that A happens (defeasibly) implies that B happens. If P implies Q in the
everyday sense, then one?s belief in P (defeasibly) causes one?s belief in Q. In fact,
implicational cases of concession could be viewed as instances of metonymy, where
?believe? is the coercion relation, and hence really causal cases of concession.
6 Conclusion
We presented a formal description of the meaning of concession, a substantial refine-
ment of the rough semantics given in the manual of sense annotations of connectives
in PDTB 2.0. Our analysis builds on Hobbs?s logic of defeasible causality enabled
by the crucial distinction between causes and causal complexes. Our basic claim is
that concession is triggered by the contrast between two causal relations. The causal
relation between the content of one argument of the relation and some implicit even-
tuality (the expectation created based on the content of the argument) and the content
of another causal relation, that between the eventuality described in second argument
and its implicit cause. This second causal relation picks an element of the causal com-
plex that we would normally assume to hold and challenges it, hence the notion of
defeasible causality.
This work illustrates the mutual benefit that corpus annotation and formal analysis
can provide to each other. Corpus examples constitute a forcing function on the formal
analysis; definitions must accommodate the complexities one finds in the real world.
On the other hand, all good annotation rests on solid theory, and formal analysis can
help in the adjudication of difficult examples. The particular analysis we give in this
paper for the concession relation can clarify issues that arise in annotation, and can
also form the basis for recognizing these relations using a knowledge-rich inferencing
system.
References
Achinstein, P. (1965). ?Defeasible? Problems. The Journal of Philosophy 62(21),
629?633.
Asher, N. (1993). Reference to Abstract Objects. Kluwer, Dordrecht.
Bach, E. (1981). On Time, Tense, and Aspect: An Essay in English Metaphysics. In
P. Cole (Ed.), Radical Pragmatics, pp. 63?81. Academic Press, New York.
Bell, J. (1999). Primary and secondary events. In M. Thielscher (Ed.), Proc. of the
IJCAI-99 Workshop on Nonmonotonic Reasoning, Action and Change, pp. 65?72.
Bell, J. (2003). A common sense theory of causation. In P. Blackburn, C. Ghidini,
R. Turner, and F. Giunchiglia (Eds.), Modeling and Using Context: Fourth Interna-
tional and Interdisciplinary Conference, Context 2003, Berlin, pp. 40?53. Springer-
Verlag.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 219
Giordano, L. and C. Schwind (2004). Conditional logic of actions and causation.
Artificial Intelligence 157(1?2), 239?279.
Giunchiglia, E., J. Lee, V. Lifschitz, N. McCain, and H. Turner (2004). Nonmonotonic
causal theories. Artificial Intelligence 153(1?2), 49?104.
Hobbs, J. (1983). An Improper Treatment of Quantification in Ordinary English. In
Proc. of the 21st Annual Meeting of the Association for Computational Linguistics,
Cambridge, Massachusetts, pp. 57?63.
Hobbs, J. (1995). Monotone Decreasing Quantifiers in a Scope-Free Logical Form. In
K. van Deemter and S. Peters (Eds.), Semantic Ambiguity and Underspecification,
CSLI Lecture Notes, pp. 55?76. CSLI.
Hobbs, J. (1998). The Logical Notation: Ontological Promiscuity. In Discourse and
Inference, Chapter 2.
Hobbs, J. (2005). Towards a Useful Notion of Causality for Lexical Semantics. Jour-
nal of Semantics 22(2), 181?209.
Kayser, D. and F. Nouioua (2008). From the Description of an Accident to its Causes.
submitted to Artificial Intelligence.
McCarthy, J. (1977). Epistemological Problems of Artificial Intelligence. In Proc.
of International Joint Conference on Artificial Intelligence, Cambridge, Mas-
sachusetts, pp. 1038?1044.
Miltsakaki, E., L. Robaldo, A. Lee, and A. Joshi (2008). Sense Annotation in the
Penn Discourse Treebank. In Proc. of Computational Linguistics and Intelligent
Text Processing, Volume 4919 of LNCS, pp. 275?286. Springer.
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber
(2008). The Penn Discourse Treebank 2.0. In Proc. of the 6th Int. Conf. on Lan-
guage Resources and Evaluation.
Prasad, R., E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, B. Webber, and L. Robaldo
(2008). The Penn Discourse Treebank 2.0. Annotation Manual. Technical Report
IRCS-06-01, IRCS Technical Report, Institute of Research in Cognitive Science,
University of Pennsylvania.
Shoham, Y. (1990). Nonmonotonic reasoning and causation. Cognitive Science 14,
213?252.
Simon, H. (1991). Nonmonotonic reasoning and causation: Comment. Cognitive
Science 49, 517?528.
Webber, B. and A. Joshi (2003). Anchoring a lexicalized tree-adjoining grammar for
discourse. In M. Stede, L. Wanner, and E. Hovy (Eds.), Discourse Relations and
Discourse Markers: Proceedings of the Conference, pp. 86?92.
Webber, B., A. Joshi, M. Stone, and A. Knott (2003). Anaphora and discourse struc-
ture. Computational Linguistics 29(4), 545?587.
On the Maximalization of the Witness sets in
Independent Set readings
Livio Robaldo
Department of Computer Science, University of Turin,
robaldo@di.unito.it
1 Pre - Introduction
Before starting, I would like to ask reader?s opinion about the truth/falsity of certain NL statements. The
statements are about figures depicting dots connected to stars. In the figures, we distinguish between dots
and stars that are connected, i.e. such that every dot is connected with at least one star and every star is
connected with at least one dot, and dots and stars that are totally connected, i.e. such that every dot is
connected to every star. For instance, in (1), the dots d1, d2, and d3 are connected with the stars s1, s2,
and s3 (on the left) while d4 and d5 are totally connected with s4, s5, and s6 (on the right).
(1)
d
1
d
2
d
3
d
4
d
5
s
1
s
2
s
3
s
4
s
5
s
6
given these premises, is it true that in the next figure Less than half of the dots are totally connected with
exactly three stars? (do not read below before answering)
(2)
d
3
s
4
s
5
d
1
d
2
s
1
s
2
s
3
d
4
d
5
d
6
I do think that the answer is yes. The same answer has been given by several friends/colleagues that were
asked to judge the example. In fact, the figure does contain two dots d1 and d2, which are less than half
of all the dots in the figure, and they are both connected with three same stars s1, s2, and s3.
Now, is it true in (3) that Few dots are totally connected with few stars?
(3)
d
3
s
3
s
4
d
1
d
2
s
1
s
2
d
4
d
5
d
6
s
5
s
7
d
7
d
8
s
6
s
8
s
9
d
9
It is somehow harder to provide an answer to this second question. At first sight, it seems the sentence is
false, or at least ?strange?: no English speaker would ever utter that sentence in that context, whatever he
wants to describe.
We are ready now to explore the proposals that aimed at formally defining the truth conditions of
sentences as the two ones above. In the literature, most logical approaches to the problem state that the
255
two sentences are both false in contexts (2) and (3). In (Robaldo, 2009a), drawing from (Sher, 1997), I
proposed a new alternative where they are both evaluated as true. It seems then that neither proposals
is completely satisfatory. The present paper proposes a ?pragmatic? revision of (Robaldo, 2009a) that
achieves ? what are claimed to be ? the proper truth values of such sentences.
2 Introduction
In the Pre-Introduction, it has been asked to judge the truth values of two NL sentences according to their
?Scopeless interpretation?, termed in (Robaldo, 2009a) as ?Independent Set (IS) reading?. In constrast, in
a linear reading one of the sets may vary on the entities in the other one. An example is Each boy ate two
apples, whose preferred reading is a linear reading where Each outscopes Two, i.e. where each boy ate
two different apples. Four kinds of IS readings have been identified in the literature, from (Scha, 1981).
(4) a. Branching Quantifier readings, e.g. Two students of mine have seen three drug-
dealers in front of the school. (Robaldo, 2009a)
b. Collective readings, e.g. Three boys made a chair yesterday. (Nakanishi, 2007)
c. Cumulative readings, e.g. Three boys invited four girls. (Landman, 2000)
d. Cover readings, e.g. Twenty children ate ten pizzas. (Kratzer, 2007)
The preferred reading of (4.a) is the one where there are exactly two1 students and exacly three drug-
dealers and each of the students saw each of the drug-dealers. Note that these are the truth values
assigned to (1)-(3) when dots and stars are asked to be totally connected. (4.b) may be true in case
three boys cooperated in the construction of a single chair. In the preferred reading of (4.c), there are
three boys and four girls such that each of the boys invited at least one girl, and each of the girls was
invited by at least one boy. These are the truth values assigned to (1) when dots and stars are asked to
be connected, possibly not totally. Finally, (4.d) allows for any sharing of ten pizzas between twenty
children. In Cumulative readings, the single actions are carried out by atomic2 individuals only, while in
(4.d) it is likely that the pizzas are shared among subgroups of children. For instance, Three children ate
five pizzas is satisfied by the following extension of ate? (??? is the standard sum operator (Link, 1983)):
(5) ?ate??M ? {?c1?c2?c3, p1?p2?, ?c2?c3, p3?p4?, ?c3, p5?}
In (5), children c1, c2, and c3 (cut into slices and) share pizzas p1 and p2, c2 and c3 (cut into slices and)
share p3 and p4, and c3 also ate pizza p5 on his own.
Branching Quantifier readings have been the more controversial (cf. (Beghelli et al, 1997) and
(Gierasimczuk and Szymanik, 2009)). Many authors claim that those readings are always subcases of
Cumulative readings, and they often co-occur with certain adverbs (May, 1989), (Schein, 1993). In fact,
in the Pre-Introduction, in order to force such a reading on (1)-(3), it was necessary to add the adverb
totally to the verb connected. Collective and Cumulative readings have been largely studied; see (Scha,
1981), (Link, 1983), (Beck and Sauerland, 2000), and (Ben-Avi and Winter, 2003).
However, the focus here is on Cover readings. This paper assumes ? following (van der Does, 1993),
(van der Does and Verkuyl, 1996), (Schwarzschild, 1996), (Kratzer, 2007) ? that they are the IS readings,
of which the three kinds exemplified in (4.a-c) are merely special cases. The name ?Cover readings?
comes from the fact that their truth values are traditionally captured in terms of Covers. A Cover is a
mathematical structure defined with respect to one or more sets. With respect to two sets S1 and S2, a
Cover Cov is formally defined as:
1In (4.a-d) ?two/three/ten/etc.? are interpreted as ?exactly two/three/ten/etc.? as in (Scha, 1981). That is actually a pragmatic
implicature, as noted in (Landman, 2000), pp.224-238.
2In line with (Landman, 2000), pp.129, and (Beck and Sauerland, 2000), def.(3), that explicitly define Cumulative readings
as statements among atomic individuals only.
256
(6) A Cover Cov is a subset of Cov1 ? Cov2, where Cov1 ? ?(S1) and Cov2 ? ?(S2) s.t.
a. ?s1 ? S1, ?cov1 ? Cov1 s.t. s1 ? cov1, and ?s2 ? S2, ?cov2 ? Cov2 s.t. s2 ? cov2.
b. ?cov1 ? Cov1, ?cov2 ? Cov2 s.t. ?cov1, cov2? ? Cov.
c. ?cov2 ? Cov2, ?cov1 ? Cov1 s.t. ?cov1, cov2? ? Cov.
Covers may be denoted by 2-order variables called ?Cover variables?. We may then define a meta-
predicate Cover that, taken a Cover variable C and two unary predicates P1 and P2, asserts that the
extension of the former is a Cover of the extensions of the latter:
(7) Cover(C, P1, P2) ?
?X1X2[C(X1, X2)??x1x2[((x1 ? X1) ? (x2 ? X2))?(P1(x1) ? P2(x2))]] ?
?x1[ P1(x1) ? ?X1X2[ (x1 ? X1) ? C(X1, X2) ] ] ?
?x2[ P2(x2) ? ?X1X2[ (x2 ? X2) ? C(X1, X2) ] ]
Thus, it is possible to decouple the quantifications from the predications. This is done by introducing
two relational variables whose extensions include the atomic individuals involved. Another relational
variable that covers them describes how the actions are actually done. For instance, in (5), in order to
evaluate as true the variant of (4.d), we may introduce three variables P1, P2, and C such that:
?P1?M = {c1, c2, c3} ?P2?M = {p1, p2, p3, p4, p5}
?C?M = { ?c1?c2?c3, p1?p2?, ?c2?c3, p3?p4?, ?c3, p5? }
The above extensions of P1, P2, and C satisfy Cover(C, P1, P2).
Among the Cover approaches mentioned above, an interesting one is (Schwarzschild, 1996).
Schwarzschild discusses numerous NL sentences where the identification of Covers appears to be prag-
matically determined, rather than existentially quantified. In other words, in the formulae the value of
the Cover variables ought to be provided by an assignment g. One of the examples mostly discussed in
(Schwarzschild, 1996) is:
(8) a. The cows and the pigs were separated.
b. The cows and the pigs were separated according to color.
The preferred reading of (8.a) is the one where the cows were separated from the pigs. However, that
is actually an implicature that may be rewritten as in (8.b), where the separation is not done by race.
Examples like (8) are used by (Schwarzschild, 1996) in order to argue against the existence of groups
and the overgeneration of readings, extensively advocated by (Landman, 2000). Schwarzschild claims
that the NP in (8.a) must correspond to a unary predicate whose extension is the set of individual cows
and pigs, while the precise separation is described by a contextually-dependent Cover variable. Similarly,
in (4.c) the Cumulative interpretation is preferred as in real contexts invitations are usually thought as
actions among pairs of persons. But it may be the case that two or more boys collectively invited two
or more girls. On the other hand, in (4.a) the fact that each student saw each drug-dealer seems to be
favoured by the low value of the numerals. If the sentence were Almost all of my students have seen
several drug-dealers in front of the school, the preferred reading appears to be Cumulative.
The next section illustrates a final component needed to build whole formulae for representing Cover
readings. This is the requirement of Maximal participancy of the witness sets, e.g. the Maximal partic-
ipancy of P1 and P2?s extension in the formula representing the meaning of the variant of (4.d). It will
be also shown that there are two possible ways to maximize the witness sets: Locally and Globally. The
former predicts that both examples in (2) and (3) are true, while the latter predicts that they are both false.
257
3 The Maximality requirement
The previous section showed that, for representing IS readings, it is necessary to reify the witness sets
into relational variables as P1 and P2. Separately, the elements of these sets are combined as described
by the Cover variables, in order to assert the predicates on the correct pairs of (possibly plural) individu-
als. Conversely, it is not possible to represent an IS reading by nesting quantifiers into the scope of other
quantifiers, as it is done in the standard Generalized Quantifier (GQ) approach (Keenan and Westersta?hl,
1997), because the set of entities quantified by the narrow-scope quantifier would vary on each entity
quantified by the wide-scope one.
As argued by (van Benthem, 1986), (Kadmon, 1987), (Sher, 1990), (Sher, 1997), (Spaan, 1996), (Steed-
man, 2007), (Robaldo, 2009a), and (Robaldo, 2009b) the relational variables must, however, be Maxi-
mized in order to achieve the proper truth values with any quantifier, regardless to its monotonicity. To
see why, let us consider sentences in (9), taken from (Robaldo, 2009a), that involve a single quantifier.
(9) a. At least two men walk.
b. At most two men walk.
c. Exactly two men walk.
In terms of reified relational variables, it seems that the meaning of (9.a-c) may represented via (10.a-c),
where ?2, ?2, and =2 are, respectively, an M?, an M?, and a non-M Generalized Quantifier.
(10) a. ?P [ ?2x(man?(x), P (x))??x[P (x)?walk?(x)] ]
b. ?P [ ?2x(man?(x), P (x))??x[P (x)?walk?(x)] ]
c. ?P [ =2x(man?(x), P (x))??x[P (x)?walk?(x)] ]
Only (10.a) correctly yields the truth values of the corresponding sentence. To see why, consider a
model in which three men walk. In such a model, (10.a) is true, while (10.b-c) are false. Conversely,
all formulae in (10) evaluate to true, as all of them allow to choose P such that ?P?M is a set of two
walking men. Therefore, we cannot allow a free choice of P . Instead, P must denote the Maximal set of
individuals satisfying the predicates, i.e. the Maximal set of walking men, in (10). This is achieved by
changing (10.b-c) to (11.a-b) respectively.
(11) a. ?P [ ?2x(man?(x), P (x)) ? ?x[P (x)?walk?(x)]?
??P [(?x[P (x)?P ?(x)] ? ?x[P ?(x)?walk?(x)])??x[P ?(x)?P (x)] ] ]
b. ?P [ =2x(man?(x), P (x)) ? ?x[P (x)?walk?(x)]?
??P [(?x[P (x)?P ?(x)] ? ?x[P ?(x)?walk?(x)])??x[P ?(x)?P (x)] ] ]
The clauses ??P [ . . . ] in the second rows are Maximality Conditions asserting the non-existence of a
superset P ? of P that also satisfies the predication. There is a single choice for P in (11.a-b): it must
denote the set of all walking men. Note that, for the sake of uniformity, the Maximality condition may
be added in (10.a) as well: in case of M? quantifiers, it does not affect the truth values.
3.1 Local Maximalization
Let me term the kind of Maximalization done in (11) as Local Maximalization. The Maximality con-
ditions in (11) require the non-existence of a set ?P ??M of walkers that includes ?P?M . In (Robaldo,
2009a) and (Robaldo, 2009b), I proposed a logical framework for representing Branching Quantifier
based on Local Maximalization. For instance, in (Robaldo, 2009a), the two witness sets of students and
drug-dealers in (4.a) are respectively reified into two variables P1 and P2, and the Maximality condi-
tion requires the non-existence of a Cartesian Product ?P ?1?M ? ?P ?2?M , that also satisfies the main
predication and that includes ?P1?M ? ?P2?M :
258
(12) ?P1P2[ =2x(stud?(x), P1(x)) ? =3x(drugD?(y), P2(y)) ?
?xy[(P1(x) ? P2(y))? saw?(x, y)]?
?P ?1P ?2 [ ( ?xy[(P1(x) ? P2(y))?(P
?
1(x) ? P ?2(y))]?
?xy[(P ?1(x) ? P ?2(y))? saw?(x, y)] )?
?xy[(P ?1(x) ? P ?2(y))?(P1(x) ? P2(y))] ] ]
In order to extend (Robaldo, 2009a) to Cover readings, which are assumed to be the most general cases
of IS readings, we cannot simply require the inclusion of ?P1?M??P2?M into the main predicate?s
extension. Rather, we require the inclusion therein of a pragmatically-determined Cover ?C?M,g of
?P1?M and ?P2?M . Furthermore, the (local) Maximality condition must require the non-existence of
a superset of either ?P1?M or ?P2?M whose corresponding Cover is a superset of ?C?M,g that is also
included in the main predicate?s extension. Thus, (4.d) is represented as3:
(13) ?P1P2[ =20x(child?(x), P1(x)) ? =10y(pizza?(y), P2(y)) ?
Cover(C,P1, P2) ? ?xy[C(x, y)? ate?(x, y)]?
?P ?1 [(?x[P1(x)?P
?
1(x)] ? ?C? [Cover(C ?, P ?1, P2) ? ?xy[C(x, y)?C ?(x, y)] ?
?xy[C ?(x, y)?ate?(x, y)]])??x[P ?1(x)?P1(x)] ] ] ?
?P ?2 [(?y[P2(y)?P
?
2(y)] ? ?C? [Cover(C ?, P1, P ?2) ? ?xy[C(x, y)?C ?(x, y)] ?
?xy[C ?(x, y)?ate?(x, y)]])??y[P ?2(y)?P2(y)] ] ] ]
Note that there are two Maximality conditions: ?P ?1 [ . . . ] and ?P ?2 [ . . . ]. In fact, contrary to what is
done with Cartesian Products, in Cover readings P1 and P2 must be Maximized independently, as it is
no longer required that every member of the former is related with every member of the latter. Note
also that the inner Cover variable C ? is existentially quantified. Of course, it would make no sense to
pragmatically interpret it as it is done with C.
3.2 Global Maximalization
The other kind of Maximalization of the witness sets, termed here as ?Global Maximalization? has been
advocated by (Schein, 1993), and formalized in most formal theories of Cumulativity, e.g. (Landman,
2000), (Hackl, 2000), and (Ben-Avi and Winter, 2003). With respect to IS readings involving two witness
sets ?P1?M and ?P2?M , Global Maximalization requires the non-existence of other two witness sets that
also satisfy the predication but that do not necessarily include ?P1?M and ?P2?M . For instance, the
event-based logic defined by (Landman, 2000) represents the Cumulative reading of (4.c) as:
(14) ?e??INVITE: ?x??BOY: |x|=3 ??Ag(e)=x ? ?y??GIRL: |y|=4 ??Th(e)=y ?
|?Ag(?{e ?INVITE: Ag(e)?BOY ? Th(e)?GIRL})| = 3 ?
|?Th(?{e ?INVITE: Ag(e)?BOY ? Th(e)?GIRL})| = 4
Formula in (14) asserts the existence of a plural event e whose Agent is a plural individual made up of
three boys and whose Theme is a plural individual made up of four girls. The two final conjuncts, in
boldface, are Maximality conditions asserted on pragmatic grounds (see footnote 1 above). Taken ex as
the plural sum of all inviting events having a boy as agent and a girl as theme, i.e.
ex=
?{e ?INVITE: Ag(e)?BOY ? Th(e)?GIRL}
the cardinality of its agent ?Ag(ex) is exactly three while the one of its theme ?Th(ex) is exactly four.
Therefore, Landman?s Maximality conditions in (14) do not refer to the same events and actors quantified
in the first row. Rather, they require that the number of the boys who invited a girl in the whole model is
exactly three and the number of girls who were invited by a boy in the whole model is exactly four.
3Without going down into further details, I simply stipulate that the GQs used in the article are Conservative (Barwise and
Cooper, 1981), (Keenan and Stavi, 1986). In other words, for every quantifier Qx, we require ?PBx ?M ? ?PRx ?M .
259
4 Local Maximalization VS Global Maximalization
We are ready now to compare the two kinds of Maximalization. Global Maximalization appears to be
more problematic than Local one. Since Branching Quantifier readings are special cases of Cumulative
readings, and it has been discussed above that many authors, e.g. (Beghelli et al, 1997), argue that this
is even a good reason to avoid an explicit representation of them, sentence (15.a) entails (15.b).
(15) a. Less than half of the dots are totally connected with exactly three stars.
b. Less than half of the dots are connected with exactly three stars.
Nevertheless, Global Maximalization predicts that (15.b) is false in figure (2). The number of all dots
in the model connected to a star is six, while the number of all stars in the model connected to a dot
is five, not exactly three. On the contrary, once the witness sets have been identified as in (16), Local
Maximalization predicts (15.b) as true, in that no other star is connected to a dot occurring in ?P1?M ,
and no other dot is connected to a star occurring in ?P2?M .
(16)
d
3
s
4
s
5
d
1
d
2
s
1
s
2
s
3
d
4
d
5
d
6
kP
1
k
M
kP
2
k
M
Another scenario where Global Maximalization predicts presumably wrong truth values, with respect to
formula (14) and sentence (4.c), is shown in (17):
(17)
g
1
g
3
g
2
b
3
b
1
b
2
g
4
g
5
b
4
In (17), the Cumulative readings of all (18.a-c) appear to be true provided that numerals N are still
interpreted as exactly-N .
(18) a. Three boys invited four girls.
b. One boy invited one girl.
c. Four boys invited five girls.
Global Maximalization states that only (18.c) is true in (17). Local Maximalization evaluates all (18.a-c)
as true; the witness sets are obviously identified.
Landman does not discuss the evaluation of his formulae in contexts like (17). This is done instead
by (Ferreira, 2007) and (Brasoveanu, 2009). However, the latter do not provide strong linguistic moti-
vations: they simply claim that (18.a-b) are false in (17), as the present paper claims they are not. A
comparison between Local and Global Maximalization is found in (Schein, 1993), even if no formaliza-
tion is presented. (Schein, 1993), ?12, reasonably argues, contra (Sher, 1997), that (19.a-b) are false in
contexts like (20) (or (3)), while (19.c) is true. Local Maximalization predicts all (19.a-c) as true.
(19) a. Few dots are totally connected with few stars.
b. Exactly two dots are totally connected with exactly two stars.
c. At least two dots are totally connected with at least two stars.
260
(20)
d
1
d
3
s
1
s
2
s
3
d
2
d
4
d
6
s
4
s
5
s
6
d
5
d
7
s
7
s
8
d
8
From these observations, Schein concludes that (Sher, 1997)?s Local Maximalization, which is defined
for any kind of quantifier, with any monotonicity, is incorrect. A proper semantics for NL quantification
should instead stipulate two different semantics depending on the monotonicity: one for M? quantifiers,
e.g. At least two, and one for M? quantifiers, e.g. Few, and non-M quantifiers, e.g. Exactly two. The
truth conditions of the former should be defined in terms of Local Maximalization, while those of the
latter in terms of Global Maximalization.
While I accept the truth values attested by Schein for sentences (19.a-c) in (20), I do not share his
conclusions. On the one hand, there are several cases, particularly mixed cases, that are quite hard to
reconcile in Schein?s view. An example is the sentence evaluated in (2), which include a M? quantifier
(Less than half) and a non-M one (Exactly three). Global Maximalization, contrary to Local Maximal-
ization, evaluates the sentence as false in (2), as pointed out above. Also (21.a), which includes an M?
quantifier and an M? one (More than half), and sentence (21.b), which is not a mixed case as it includes
two M? quantifiers, seems to be true in (2), contra Schein?s predictions.
(21) a. Less than half of the dots are connected with more than half of the stars.
b. Less than half of the dots are connected with less than five stars.
On the other hand, all sentences in (19.a-c) seems to be true in (22), while in Schein?s view they should
have the same truth values they have in (20).
(22)
d
6
s
5
s
6
d
1
d
2
s
1
s
2
d
7
d
8
d
3
d
4
d
5
s
3
s
4
s
7
s
8
These considerations lead to conclude that the oddity of sentences (19) in contexts (20) or (3) does not
depend on the monotonicity of the quantifiers involved.
The present paper suggests instead that such an oddity stems from Pragmatics. No English speaker would
ever utter those sentences in those contexts, as they would not be informative enough, and so they would
violate a Gricean Maxim. From the examples above, it seems that sentences involving non-M? quanti-
fiers sound odd in contexts where more pairs of witness sets are available. For instance, the reader gets
confused when he tries to evaluate (19.a) in (20), as multiple pairs of (witness) sets of dots and stars are
available, i.e. ?{d1, d2}, {s1, s2}?, ?{d3, d4}, {s3, s4}?, etc., and he does not have enough information
to prefer one of them upon the others. This does not arises in (3) or (22), where the witness sets are
immediatly and uncontroversially identified.
The multiple availability of witness sets does not seem to confuse the reader for sentences involving M?
quantifiers, perhaps because they are simpler to interpret (cf. (Geurts and van der Silk, 2005)). How-
ever, several cognitive experimental results showed that many other factors besides monotonicity, e.g.
expressivity/computability, fuzzyness, the fact that quantifiers are cardinal rather than proportional, etc.,
may affect the accuracy and reaction time of the interpretation of IS readings (cf. (Sanford and Paterson,
1994), (Bott and Rado?, 2009), (Musolino, 2009), and (Szymanik and Zajenkowski, 2009)).
As it is clear to understand, however, extra-linguistic factors seem the ones that mainly affect the inter-
pretation of quantifiers. For instance, in (17), if the boys b1, b2, b3 are friends who decided to go to a
party with some girls, and b4 wants to go there with his girlfriend (g5) only, the witness sets are most
261
likely identified for (18.a-b) rather than for (18.c), as the two groups of persons are not related.
Conversely, if the four boys belong to the same group of friends hanging out together, the identification
of the witness sets most likely fails in (18.a-b). That is probably the assumption done by (Ferreira, 2007)
and (Brasoveanu, 2009) for claiming that sentences like (18.a-b) are false in contexts like (17). Analo-
gously, in the children-pizza example in (4.d), the arrangement of the children among the tables of the
pizzeria, their mutual friendship, and so on, may affect the identification of the witness sets. Similar
discussions may be found in (Fintel, 1994) and (Winter, 2000).
Of course, an exhaustive study of all factors involved in the pragmatic identification of the witness
sets goes much beyond the goal of the present paper. The aim of this paper is to argue that, once witness
sets are identified, Local Maximalization applies to them. In order to formally obtain this result, a final
modification of the formulae is needed: it is necessary to pragmatically interpret the relational variables
denoting the witness sets, besides those denoting the Covers. Formula (13) is then revised as in (23).
(23) =20x(child?(x), P1(x)) ? =10y(pizza?(y), P2(y)) ?
Cover(C,P1, P2) ? ?xy[C(x, y))? ate?(x, y)]?
?P ?1 [(?x[P1(x)?P
?
1(x)] ? ?C? [Cover(C ?, P ?1, P2) ? ?xy[C(x, y)?C ?(x, y)] ?
?xy[C ?(x, y)?ate?(x, y)]])??x[P ?1(x)?P1(x)] ] ] ?
?P ?2 [(?y[P2(y)?P
?
2(y)] ? ?C? [Cover(C ?, P1, P ?2) ? ?xy[C(x, y)?C ?(x, y)] ?
?xy[C ?(x, y)?ate?(x, y)]])??y[P ?2(y)?P2(y)] ] ]
The only difference between (23) and (13) is that the value of P1 and P2 is provided by an assignment g,
as it is done for the Cover variable C. g must obey to all (extra-)linguistic pragmatic constraints briefly
listed above. The reader could start thinking that, in the new version of the formulae, we may avoid
Maximality conditions, either Local or Global. In fact, Maximalization could be simply implemented
as a constraint on the assignment function g. In other words, we could simply impose g to select only
Maximal witness sets. If g is unable to do so, the intepretation fails as in the cases discussed above.
Such a solution has been actually proposed in (Steedman, 2007) and (Brasoveanu, 2009). Conversely,
in (Robaldo, 2009b) I explained that we do need to explicitly represent the Maximality conditions. In
other words, those are not only seen as necessary conditions needed to determine if a sentence is true or
false in a certain context. Rather, in (Robaldo, 2009b), it is extensively argued that they are part of the
knowledge needed to draw the appropriate inferences from the sentences? meaning.
5 Conclusions
This paper compared the two kind of Maximalization proposed in the literature for handling the proper
truth values of Independent Set readings. They have been termed as Local and Global Maximalization.
The former requires the non-existence of any tuple of supersets of the witness sets that also satisfy the
predication. The latter requires the witness sets to be the only tuple of sets that satisfy the predication.
The present paper argues in favour of Local Maximalization, and claims that the motivations that led to
the definition of Global Maximalitation, and its incorporation within most current formal approaches to
NL quantification, do not appear to be justified enough. These claims are supported by showing that, for
many NL sentences, Global Maximalization predicts counter-intuitive truth conditions.
Also several examples are hard to reconcile in a logical framework based on Local Maximalization. It
seems, however, that the oddity of such examples depends upon pragmatic grounds.
Based on these assumptions, the solution presented here still adopts Local Maximalization, but ad-
vocates a pragmatic interpretation of all relational variables. Drawing from (Schwarzschild, 1996), the
present paper evolves the formulae in (Robaldo, 2009a) and (Robaldo, 2009b), making them able to
handle Cover readings, which are assumed to be the more general cases of Independent Set readings.
In the resulting formulae, the witness sets are firstly pragmatically identified, as it is done with Cover
variables, then they are locally Maximized. In other words, Pragmatics is responsible for identifying
both the (atomic) individuals involved, and the way they sub-combine to carry out the singular actions.
262
The result is able to predict the suitable truth values of Cover readings in all examples considered, and
seems to mirror the correct interplay between the Semantics and the Pragmatics of NL quantifiers.
References
Barwise, J. and R. Cooper (1981). Generalized quantifiers and natural language. Linguistics and Philos-
ophy 4(2), 159?219.
Beck, S. and U. Sauerland (2000). Cumulation is needed: A reply to winter (2000). Natural Language
Semantics 8(4), 349?371.
Beghelli, F., D. Ben-Shalom, and A. Szabolski (1997). Variation, distributivity, and the illusion of
branching. In Ways of Scope Taking, pp. 29?69. Dordrecht: Kluwer Academic Publishers.
Ben-Avi, G. and Y. Winter (2003). Monotonicity and collective quantification. Journal of Logic, Lan-
guage and Information 12, 127?151.
Bott, O. and J. Rado? (2009). How to provide exactly one interpretation for every sentence, or what eye
movements reveal about quantifier scope. In The fruits of empirical linguistics. Berlin: de Gruyter.
Brasoveanu, A. (2009). Modified numerals as post-suppositions. In Proc. of the 17th Amsterdam Collo-
quium.
Ferreira, M. (2007). Scope splitting and cumulativity. In Proc. of the Workshop on quantifier modifica-
tion, ESSLLI 2007.
Fintel, K. (1994). Restrictions on quantifiers domains. Amherst, University of Massachusetts.
Geurts, B. and F. van der Silk (2005). Monotonicity and processing load. The Journal of Seman-
tics 22(17).
Gierasimczuk, N. and J. Szymanik (2009). Branching quantification vs. two-way quantification. The
Journal of Semantics. 26(4), 367?392.
Hackl, M. (2000). Comparative quantifiers. Ph. D. thesis, Massachusetts Institute of Technology.
Kadmon, N. (1987). On unique and non-unique reference and asymmetric quantification. Ph. D. thesis,
University of Massachusetts, Amherst.
Keenan, E. and D. Westersta?hl (1997). Generalized quantifiers in linguistics and logic. In Handbook of
Logic and Language, pp. 837?893. Cambridge: MIT Press.
Keenan, E. L. and J. Stavi (1986). A semantic characterization of natural language determiners. Linguis-
tics and Philosophy 9(3), 253?326.
Kratzer, A. (2007). On the plurality of verbs. In Event Structures in Linguistic Form and Interpretation.
Berlin: Mouton de Gruyter.
Landman, F. (2000). Events and Plurality: The Jerusalem Lectures. Kluwer Academic Publishers.
Link, G. (1983). The logical analysis of plurals and mass terms. In Meaning, Use, and Interpretation in
Language, pp. 302?323. Berlin: de Gruyter.
May, R. (1989). Interpreting logical form. Linguistics and Philosophy 12(4), 387?437.
Musolino, J. (2009). The logical syntax of number words: Theory, acquisition and processing. Cogni-
tion 111.
263
Nakanishi, K. (2007). Event quantification and distributivity. In Event Structures in Linguistic Form and
Interpretation. Mouton de Gruyter.
Robaldo, L. (2009a). Independent set readings and generalized quantifiers. The Journal of Philosophical
Logic 39(1), 23?58.
Robaldo, L. (2009b). Interpretation and inference with maximal referential terms. The Journal of Com-
puter and System Sciences. 76(5), 373?388.
Sanford, A. J., M. L. M. and K. Paterson (1994). Psychological studies of quantifiers. The Journal of
Semantics 11(3), 153170.
Scha, R. (1981). Distributive, collective and cumulative quantification. In Formal Methods in the Study
of Language, Part 2. Amsterdam: Mathematisch Centrum.
Schein, B. (1993). Plurals and Events. MIT Press, Cambridge, MA, USA.
Schwarzschild, R. (1996). Pluralities. Dordrecht: Kluwer.
Sher, G. (1990). Ways of branching quantifiers. Linguistics and Philosophy 13, 393?422.
Sher, G. (1997). Partially-ordered (branching) generalized quantifiers: a general definition. The Journal
of Philosophical Logic 26, 1?43.
Spaan, M. (1996). Parallel quantification. In Quantifiers, Logic, and Language, Volume 54, pp. 281?309.
Stanford: CSLI Publications.
Steedman, M. (2007). The Grammar of Scope. forthcoming. See ?Surface-Compositional Scope-
Alternation Without Existential Quantifiers?. Draft 5.2, Sept 2007. Retrieved September 25, 2007
from ftp://ftp.cogsci.ed.ac.uk/pub/steedman/quantifiers/journal6.pdf.
Szymanik, J. and M. Zajenkowski (2009). Comprehension of simple quantifiers empirical evaluation of
a computational model. Cognitive Science: A Multidisciplinary Journal.
van Benthem, J. (1986). Essays in logical semantics. Dordrecht, Reidel.
van der Does, J. (1993). Sums and quantifiers. Linguistics and Philosophy 16, 509?550.
van der Does, J. and H. Verkuyl (1996). The semantics of plural noun phrases. In Quantifiers, Logic and
Language. CSLI.
Winter, Y. (2000). Distributivity and dependency. Natural Language Semantics 8, 27?69.
264
